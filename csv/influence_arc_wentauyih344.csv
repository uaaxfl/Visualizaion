2020.acl-main.745,P19-1444,0,0.579569,"onal classification layers, semantic parsing is highly domain-specific, and the architecture of a neural parser is strongly coupled with the structure of its underlying DB (e.g., systems for SQL-based and other types of DBs use different encoder mod8413 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413–8426 c July 5 - 10, 2020. 2020 Association for Computational Linguistics els). In fact, existing systems have attempted to leverage BERT, but each with their own domainspecific, in-house strategies to encode the structured information in the DB (Guo et al., 2019; Zhang et al., 2019a; Hwang et al., 2019), and importantly, without pretraining representations on structured data. These challenges call for development of general-purpose pretraining approaches tailored to learning representations for both NL utterances and structured DB tables. In this paper we present TA B ERT, a pretraining approach for joint understanding of NL text and (semi-)structured tabular data (§ 3). TA B ERT is built on top of BERT, and jointly learns contextual representations for utterances and the structured schema of DB tables (e.g., a vector for each utterance token and tab"
2020.acl-main.745,D17-1160,0,0.0575742,"of economics). A key challenge in this scenario is understanding the structured schema of DB tables (e.g., the name, data type, and stored values of columns), and more importantly, the alignment between the input text and the schema (e.g., the token “GDP” refers to the Gross Domestic Product column), which is essential for inferring the correct DB query (Berant and Liang, 2014). Neural semantic parsers tailored to this task therefore attempt to learn joint representations of NL utterances and the (semi-)structured schema of DB tables (e.g., representations of its columns or cell values, as in Krishnamurthy et al. (2017); Bogin et al. (2019b); Wang et al. (2019a), inter alia). However, this unique setting poses several challenges in applying pretrained LMs. First, information stored in DB tables exhibit strong underlying structure, while existing LMs (e.g., BERT) are solely trained for encoding free-form text. Second, a DB table could potentially have a large number of rows, and naively encoding all of them using a resource-heavy LM is computationally intractable. Finally, unlike most text-based QA tasks (e.g., SQuAD, Rajpurkar et al. (2016)) which could be formulated as a generic answer span selection proble"
2020.acl-main.745,P17-2031,0,0.0577108,"Missing"
2020.acl-main.745,2021.ccl-1.108,0,0.210001,"Missing"
2020.acl-main.745,K16-1006,0,0.0299363,"s on the challenging weakly-supervised semantic parsing benchmark W IKI TABLE Q UESTIONS, while performing competitively on the text-toSQL dataset S PIDER.1 1 Introduction Recent years have witnessed a rapid advance in the ability to understand and answer questions about free-form natural language (NL) text (Rajpurkar et al., 2016), largely due to large-scale, pretrained language models (LMs) like BERT (Devlin et al., 2019). These models allow us to capture the syntax and semantics of text via representations learned in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019). It is also relatively easy to apply such pretrained LMs to comprehension tasks that are modeled as text span selection problems, where the boundary of an answer span can be predicted using a simple classifier on top of the LM (Joshi et al., 2019). ∗ 1 Work done while at Facebook AI Research. Available at github.com/facebookresearch/TaBERT Wen-tau Yih Sebastian Riedel Facebook AI Research {scottyih,sriedel}@fb.com However, it is less clear how one could pretrain and fine-tune such models for other"
2020.acl-main.745,P15-1142,0,0.77818,"t systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts using B ERT, registering state-of-the-art performance on W IKI TABLE Q UESTIONS, while performing competitively on S PIDER (§ 5). 2 Background Semantic Parsing over Tables Semantic parsing tackles the task of translating an NL utterance u"
2020.acl-main.745,N18-1202,0,0.0314115,"ntic parsing benchmark W IKI TABLE Q UESTIONS, while performing competitively on the text-toSQL dataset S PIDER.1 1 Introduction Recent years have witnessed a rapid advance in the ability to understand and answer questions about free-form natural language (NL) text (Rajpurkar et al., 2016), largely due to large-scale, pretrained language models (LMs) like BERT (Devlin et al., 2019). These models allow us to capture the syntax and semantics of text via representations learned in an unsupervised manner, before fine-tuning the model to downstream tasks (Melamud et al., 2016; McCann et al., 2017; Peters et al., 2018; Liu et al., 2019b; Yang et al., 2019; Goldberg, 2019). It is also relatively easy to apply such pretrained LMs to comprehension tasks that are modeled as text span selection problems, where the boundary of an answer span can be predicted using a simple classifier on top of the LM (Joshi et al., 2019). ∗ 1 Work done while at Facebook AI Research. Available at github.com/facebookresearch/TaBERT Wen-tau Yih Sebastian Riedel Facebook AI Research {scottyih,sriedel}@fb.com However, it is less clear how one could pretrain and fine-tune such models for other QA tasks that involve joint reasoning ove"
2020.acl-main.745,D19-1005,0,0.0239546,"nt representations of columns from an individual table with global information of its linked tables defined by the DB schema (Bogin et al., 2019a; Wang et al., 2019a). TA B ERT could also potentially improve performance of these systems with improved table-level representations. Knowledge-enhanced Pretraining Recent pretraining models have incorporated structured information from knowledge bases (KBs) or other structured semantic annotations into training contextual word representations, either by fusing vector representations of entities and relations on KBs into word representations of LMs (Peters et al., 2019; Zhang et al., 2019b,c), or by encouraging the LM to recover KB entities and relations from text (Sun et al., 2019; Liu et al., 2019a). TA B ERT is broadly relevant to this line in that it also exposes an LM with structured data (i.e., tables), while aiming to learn joint representations for both textual and structured tabular data. 7 Conclusion and Future Work We present TA B ERT, a pretrained encoder for joint understanding of textual and tabular data. We show that semantic parsers using TA B ERT as a general-purpose feature representation layer achieved strong results on two benchmarks. Th"
2020.acl-main.745,Q13-1033,0,0.0779617,"Missing"
2020.acl-main.745,P18-1034,0,0.0191951,"ation. 6 Related Works Semantic Parsing over Tables Tables are important media of world knowledge. Semantic parsers have been adapted to operate over structured DB tables (Wang et al., 2015; Xu et al., 2017; Dong and Lapata, 2018; Yu et al., 2018b; Shi et al., 2018; Wang et al., 2018), and open-domain, semistructured Web tables (Pasupat and Liang, 2015; Sun et al., 2016; Neelakantan et al., 2016). To improve representations of utterances and tables for neural semantic parsing, existing systems have applied pretrained word embeddings (e.g.., GloVe, as in Zhong et al. (2017); Yu et al. (2018a); Sun et al. (2018); Liang et al. (2018)), and BERT-family models for learning joint contextual representations of utterances and tables, but with domain-specific approaches to encode the structured information in tables (Hwang et al., 2019; He et al., 2019; Guo et al., 2019; Zhang et al., 2019a). TA B ERT advances this line of research by presenting a generalpurpose, pretrained encoder over parallel corpora of Web tables and NL context. Another relevant direction is to augment representations of columns from an individual table with global information of its linked tables defined by the DB schema (Bogin et al.,"
2020.acl-main.745,P15-1129,0,0.0386304,"d capture both the general information of the column (via MCP) and its representative cell values related to the utterance (via CVR). Tab. 5 shows ablation results of pretraining TA B ERT with different objectives. We find TA B ERT trained with both MCP and the auxiliary CVR objectives gets a slight advantage, suggesting CVR could potentially lead to 8420 more representative column representations with additional cell information. 6 Related Works Semantic Parsing over Tables Tables are important media of world knowledge. Semantic parsers have been adapted to operate over structured DB tables (Wang et al., 2015; Xu et al., 2017; Dong and Lapata, 2018; Yu et al., 2018b; Shi et al., 2018; Wang et al., 2018), and open-domain, semistructured Web tables (Pasupat and Liang, 2015; Sun et al., 2016; Neelakantan et al., 2016). To improve representations of utterances and tables for neural semantic parsing, existing systems have applied pretrained word embeddings (e.g.., GloVe, as in Zhong et al. (2017); Yu et al. (2018a); Sun et al. (2018); Liang et al. (2018)), and BERT-family models for learning joint contextual representations of utterances and tables, but with domain-specific approaches to encode the str"
2020.acl-main.745,P15-1128,1,0.828901,"t span selection problems, where the boundary of an answer span can be predicted using a simple classifier on top of the LM (Joshi et al., 2019). ∗ 1 Work done while at Facebook AI Research. Available at github.com/facebookresearch/TaBERT Wen-tau Yih Sebastian Riedel Facebook AI Research {scottyih,sriedel}@fb.com However, it is less clear how one could pretrain and fine-tune such models for other QA tasks that involve joint reasoning over both free-form NL text and structured data. One example task is semantic parsing for access to databases (DBs) (Zelle and Mooney, 1996; Berant et al., 2013; Yih et al., 2015), the task of transducing an NL utterance (e.g., “Which country has the largest GDP?”) into a structured query over DB tables (e.g., SQL querying a database of economics). A key challenge in this scenario is understanding the structured schema of DB tables (e.g., the name, data type, and stored values of columns), and more importantly, the alignment between the input text and the schema (e.g., the token “GDP” refers to the Gross Domestic Product column), which is essential for inferring the correct DB query (Berant and Liang, 2014). Neural semantic parsers tailored to this task therefore attem"
2020.acl-main.745,D18-2002,1,0.939848,"ple consists of an utterance (e.g., “What is the total number of languages used in Aruba?”), a DB with one or more tables, and an annotated SQL query, which typically involves joining multiple tables to get the answer (e.g., SELECT COUNT(*) FROM Country JOIN Lang ON Country.Code = Lang.CountryCode WHERE Name = ‘Aruba’). Base Semantic Parser We aim to show TA B ERT could help improve upon an already strong parser. Unfortunately, at the time of writing, none of the top systems on S PIDER were publicly available. To establish a reasonable testbed, we developed our in-house system based on TranX (Yin and Neubig, 2018), an open-source general-purpose semantic parser. TranX translates an NL utterance into an intermediate meaning representation guided by a user-defined grammar. The generated intermediate MR could then be deterministically converted to domain-specific query languages (e.g., SQL). We use TA B ERT as encoder of utterances and table schemas. Specifically, for a given utterance u and a DB with a set of tables T = {Tt }, we first pair u with each table Tt in T as inputs to TA B ERT, which generates |T |sets of table-specific representations of utterances and columns. At each time step, an LSTM deco"
2020.acl-main.745,N18-2093,0,0.382463,"s (§ 3.2). TA B ERT can be plugged into a neural semantic parser as a general-purpose encoder to compute representations for utterances and tables. Our key insight is that although semantic parsers are highly domain-specific, most systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts"
2020.acl-main.745,D18-1193,0,0.54357,"s (§ 3.2). TA B ERT can be plugged into a neural semantic parser as a general-purpose encoder to compute representations for utterances and tables. Our key insight is that although semantic parsers are highly domain-specific, most systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts"
2020.acl-main.745,D18-1425,0,0.333428,"s (§ 3.2). TA B ERT can be plugged into a neural semantic parser as a general-purpose encoder to compute representations for utterances and tables. Our key insight is that although semantic parsers are highly domain-specific, most systems rely on representations of input utterances and the table schemas to facilitate subsequent generation of DB queries, and these representations can be provided by TA B ERT, regardless of the domain of the parsing task. We apply TA B ERT to two different semantic parsing paradigms: (1) a classical supervised learning setting on the S PIDER text-to-SQL dataset (Yu et al., 2018c), where TA B ERT is fine-tuned together with a task-specific parser using parallel NL utterances and labeled DB queries (§ 4.1); and (2) a challenging weakly-supervised learning benchmark W IKI TABLE Q UESTIONS (Pasupat and Liang, 2015), where a system has to infer latent DB queries from its execution results (§ 4.2). We demonstrate TA B ERT is effective in both scenarios, showing that it is a drop-in replacement of a parser’s original encoder for computing contextual representations of NL utterances and DB tables. Specifically, systems augmented with TA B ERT outperforms their counterparts"
2020.acl-main.745,D17-1125,0,0.0664293,"Missing"
2020.acl-main.745,P19-1139,0,0.334363,"n layers, semantic parsing is highly domain-specific, and the architecture of a neural parser is strongly coupled with the structure of its underlying DB (e.g., systems for SQL-based and other types of DBs use different encoder mod8413 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 8413–8426 c July 5 - 10, 2020. 2020 Association for Computational Linguistics els). In fact, existing systems have attempted to leverage BERT, but each with their own domainspecific, in-house strategies to encode the structured information in the DB (Guo et al., 2019; Zhang et al., 2019a; Hwang et al., 2019), and importantly, without pretraining representations on structured data. These challenges call for development of general-purpose pretraining approaches tailored to learning representations for both NL utterances and structured DB tables. In this paper we present TA B ERT, a pretraining approach for joint understanding of NL text and (semi-)structured tabular data (§ 3). TA B ERT is built on top of BERT, and jointly learns contextual representations for utterances and the structured schema of DB tables (e.g., a vector for each utterance token and table column). Specific"
2020.acl-main.745,P14-1133,0,\N,Missing
2020.acl-main.745,D13-1160,0,\N,Missing
2020.acl-main.745,N19-1273,0,\N,Missing
2020.acl-main.745,P19-1448,0,\N,Missing
2020.acl-main.745,N19-1423,0,\N,Missing
2020.acl-main.745,D19-1537,0,\N,Missing
2020.acl-main.745,D19-1391,0,\N,Missing
2020.acl-tutorials.8,P17-1147,0,0.0670989,"and coherent overview of cutting-edge research in this direction.1 We will start by first giving a brief background of open-domain question answering, discussing the basic setup and core technical challenges of the research problem. We aim to give the audience a historical view of how the field has advanced in the past several decades, from highly-modulated pipeline systems in the early days, to modern endto-end training of deep neural networks in the present. We will then discuss modern datasets proposed for open-domain QA (Voorhees et al., 1999; Berant et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017; Dhingra et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), as well as common evaluation metrics and benchmarks. We plan to provide Two-stage retriever-reader approaches. We will start by discussing two-stage retriever-reader frameworks for open-domain QA, pioneered by Chen et al. (2017): a retriever component finding documents that (might) contain an answer from a large collection of documents, followed by a reader component finding the answer in a given paragraph or a document. In this category, the retriever component is usually implemented by traditional sparse vector space metho"
2020.acl-tutorials.8,D13-1160,0,0.115702,"s tutorial, we aim to provide a comprehensive and coherent overview of cutting-edge research in this direction.1 We will start by first giving a brief background of open-domain question answering, discussing the basic setup and core technical challenges of the research problem. We aim to give the audience a historical view of how the field has advanced in the past several decades, from highly-modulated pipeline systems in the early days, to modern endto-end training of deep neural networks in the present. We will then discuss modern datasets proposed for open-domain QA (Voorhees et al., 1999; Berant et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017; Dhingra et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), as well as common evaluation metrics and benchmarks. We plan to provide Two-stage retriever-reader approaches. We will start by discussing two-stage retriever-reader frameworks for open-domain QA, pioneered by Chen et al. (2017): a retriever component finding documents that (might) contain an answer from a large collection of documents, followed by a reader component finding the answer in a given paragraph or a document. In this category, the retriever component is usually implemen"
2020.acl-tutorials.8,2020.emnlp-main.550,1,0.731233,"Missing"
2020.acl-tutorials.8,Q19-1026,0,0.160351,"Missing"
2020.acl-tutorials.8,W02-1033,0,0.164719,"for open-domain QA, which is also the central part of this tutorial. We divide existing models into three main categories: Two-stage retriever-reader approaches, Dense retriever and end-to-end training, and Retriever-free approaches. We will present the logical elements behind different sorts of models and discuss their pros and cons. Open-domain question answering (QA), the task of answering questions using a large collection of documents of diversified topics, has been a longstanding problem in NLP, information retrieval (IR) and related fields (Voorhees et al., 1999; Moldovan et al., 2000; Brill et al., 2002; Ferrucci et al., 2010). Traditional QA systems were usually constructed as a pipeline, consisting of many different components such as question processing, document/passage retrieval, and answer processing. With the rapid development of neural reading comprehension (Chen, 2018), modern open-domain QA systems have been restructured by combining traditional IR techniques and neural reading comprehension models (Chen et al., 2017; Yang et al., 2019; Min et al., 2019a) or even implemented in a fully end-to-end fashion (Lee et al., 2019; Seo et al., 2019; Guu et al., 2020; Roberts et al., 2020)."
2020.acl-tutorials.8,P19-1612,0,0.234296,"ated fields (Voorhees et al., 1999; Moldovan et al., 2000; Brill et al., 2002; Ferrucci et al., 2010). Traditional QA systems were usually constructed as a pipeline, consisting of many different components such as question processing, document/passage retrieval, and answer processing. With the rapid development of neural reading comprehension (Chen, 2018), modern open-domain QA systems have been restructured by combining traditional IR techniques and neural reading comprehension models (Chen et al., 2017; Yang et al., 2019; Min et al., 2019a) or even implemented in a fully end-to-end fashion (Lee et al., 2019; Seo et al., 2019; Guu et al., 2020; Roberts et al., 2020). In this tutorial, we aim to provide a comprehensive and coherent overview of cutting-edge research in this direction.1 We will start by first giving a brief background of open-domain question answering, discussing the basic setup and core technical challenges of the research problem. We aim to give the audience a historical view of how the field has advanced in the past several decades, from highly-modulated pipeline systems in the early days, to modern endto-end training of deep neural networks in the present. We will then discuss m"
2020.acl-tutorials.8,P17-1171,1,0.873972,"documents of diversified topics, has been a longstanding problem in NLP, information retrieval (IR) and related fields (Voorhees et al., 1999; Moldovan et al., 2000; Brill et al., 2002; Ferrucci et al., 2010). Traditional QA systems were usually constructed as a pipeline, consisting of many different components such as question processing, document/passage retrieval, and answer processing. With the rapid development of neural reading comprehension (Chen, 2018), modern open-domain QA systems have been restructured by combining traditional IR techniques and neural reading comprehension models (Chen et al., 2017; Yang et al., 2019; Min et al., 2019a) or even implemented in a fully end-to-end fashion (Lee et al., 2019; Seo et al., 2019; Guu et al., 2020; Roberts et al., 2020). In this tutorial, we aim to provide a comprehensive and coherent overview of cutting-edge research in this direction.1 We will start by first giving a brief background of open-domain question answering, discussing the basic setup and core technical challenges of the research problem. We aim to give the audience a historical view of how the field has advanced in the past several decades, from highly-modulated pipeline systems in"
2020.acl-tutorials.8,P18-1161,0,0.0415332,"an answer from a large collection of documents, followed by a reader component finding the answer in a given paragraph or a document. In this category, the retriever component is usually implemented by traditional sparse vector space methods, such as TF-IDF or BM25 and the reader is implemented by neural reading comprehension models. We will further discuss several challenges and techniques arising in this area, including multi-passage training (Clark and Gardner, 2018; Wang et al., 2019), passage reranking (Wang et al., 2018; Nogueira and Cho, 2019), and denoising distantly-supervised data (Lin et al., 2018). Dense retriever and end-to-end training. The first category mainly employs a non-machine learning model for the retrieval stage. The second category will focus on how to learn the retriever component by replacing traditional IR methods with dense representations, as well as joint training of both components. Learning and searching in dense vector space is challenging, as it usually involves 1 All the tutorial materials will be released at https://github.com/danqi/acl2020-openqa-tutorial. 34 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 34–37 c"
2020.acl-tutorials.8,D18-1455,0,0.0283103,"uss some hybrid approaches for answering open-domain questions using both text and large knowledge bases, such as Freebase (Bollacker et al., 2008) and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and give a critical review on how structured data complements the information from unstructured text. The approaches include (1) how to leverage structured data to guide the retriever or reader stage of existing textual QA systems (Asai et al., 2020; Min et al., 2019b), or (2) how to synthesize information from these two heterogeneous sources and build effective QA models on the combined information (Sun et al., 2018, 2019; Xiong et al., 2019). Finally, we will discuss some important questions, including (1) How much progress have we made compared to the QA systems developed in the last decade? (2) What are the main challenges and limitations of current approaches? (3) How to trade off the efficiency (computational time and memory requirements) and accuracy in the deep learning era? We hope our tutorial will not only serve as a useful resource for the audience to efficiently acquire up-to-date knowledge, but also provide new perspectives to stimulate the advances of open-domain QA research in the next pha"
2020.acl-tutorials.8,2021.ccl-1.108,0,0.155739,"Missing"
2020.acl-tutorials.8,D19-1284,1,0.929569,"been a longstanding problem in NLP, information retrieval (IR) and related fields (Voorhees et al., 1999; Moldovan et al., 2000; Brill et al., 2002; Ferrucci et al., 2010). Traditional QA systems were usually constructed as a pipeline, consisting of many different components such as question processing, document/passage retrieval, and answer processing. With the rapid development of neural reading comprehension (Chen, 2018), modern open-domain QA systems have been restructured by combining traditional IR techniques and neural reading comprehension models (Chen et al., 2017; Yang et al., 2019; Min et al., 2019a) or even implemented in a fully end-to-end fashion (Lee et al., 2019; Seo et al., 2019; Guu et al., 2020; Roberts et al., 2020). In this tutorial, we aim to provide a comprehensive and coherent overview of cutting-edge research in this direction.1 We will start by first giving a brief background of open-domain question answering, discussing the basic setup and core technical challenges of the research problem. We aim to give the audience a historical view of how the field has advanced in the past several decades, from highly-modulated pipeline systems in the early days, to modern endto-end t"
2020.acl-tutorials.8,P00-1071,0,0.0459746,"g-edge models proposed for open-domain QA, which is also the central part of this tutorial. We divide existing models into three main categories: Two-stage retriever-reader approaches, Dense retriever and end-to-end training, and Retriever-free approaches. We will present the logical elements behind different sorts of models and discuss their pros and cons. Open-domain question answering (QA), the task of answering questions using a large collection of documents of diversified topics, has been a longstanding problem in NLP, information retrieval (IR) and related fields (Voorhees et al., 1999; Moldovan et al., 2000; Brill et al., 2002; Ferrucci et al., 2010). Traditional QA systems were usually constructed as a pipeline, consisting of many different components such as question processing, document/passage retrieval, and answer processing. With the rapid development of neural reading comprehension (Chen, 2018), modern open-domain QA systems have been restructured by combining traditional IR techniques and neural reading comprehension models (Chen et al., 2017; Yang et al., 2019; Min et al., 2019a) or even implemented in a fully end-to-end fashion (Lee et al., 2019; Seo et al., 2019; Guu et al., 2020; Rob"
2020.acl-tutorials.8,D19-1599,0,0.313369,"frameworks for open-domain QA, pioneered by Chen et al. (2017): a retriever component finding documents that (might) contain an answer from a large collection of documents, followed by a reader component finding the answer in a given paragraph or a document. In this category, the retriever component is usually implemented by traditional sparse vector space methods, such as TF-IDF or BM25 and the reader is implemented by neural reading comprehension models. We will further discuss several challenges and techniques arising in this area, including multi-passage training (Clark and Gardner, 2018; Wang et al., 2019), passage reranking (Wang et al., 2018; Nogueira and Cho, 2019), and denoising distantly-supervised data (Lin et al., 2018). Dense retriever and end-to-end training. The first category mainly employs a non-machine learning model for the retrieval stage. The second category will focus on how to learn the retriever component by replacing traditional IR methods with dense representations, as well as joint training of both components. Learning and searching in dense vector space is challenging, as it usually involves 1 All the tutorial materials will be released at https://github.com/danqi/acl2020"
2020.acl-tutorials.8,P19-1417,0,0.0160945,"es for answering open-domain questions using both text and large knowledge bases, such as Freebase (Bollacker et al., 2008) and Wikidata (Vrandeˇci´c and Kr¨otzsch, 2014), and give a critical review on how structured data complements the information from unstructured text. The approaches include (1) how to leverage structured data to guide the retriever or reader stage of existing textual QA systems (Asai et al., 2020; Min et al., 2019b), or (2) how to synthesize information from these two heterogeneous sources and build effective QA models on the combined information (Sun et al., 2018, 2019; Xiong et al., 2019). Finally, we will discuss some important questions, including (1) How much progress have we made compared to the QA systems developed in the last decade? (2) What are the main challenges and limitations of current approaches? (3) How to trade off the efficiency (computational time and memory requirements) and accuracy in the deep learning era? We hope our tutorial will not only serve as a useful resource for the audience to efficiently acquire up-to-date knowledge, but also provide new perspectives to stimulate the advances of open-domain QA research in the next phase. 2. Problem definition &"
2020.acl-tutorials.8,N19-4013,0,0.075277,"sified topics, has been a longstanding problem in NLP, information retrieval (IR) and related fields (Voorhees et al., 1999; Moldovan et al., 2000; Brill et al., 2002; Ferrucci et al., 2010). Traditional QA systems were usually constructed as a pipeline, consisting of many different components such as question processing, document/passage retrieval, and answer processing. With the rapid development of neural reading comprehension (Chen, 2018), modern open-domain QA systems have been restructured by combining traditional IR techniques and neural reading comprehension models (Chen et al., 2017; Yang et al., 2019; Min et al., 2019a) or even implemented in a fully end-to-end fashion (Lee et al., 2019; Seo et al., 2019; Guu et al., 2020; Roberts et al., 2020). In this tutorial, we aim to provide a comprehensive and coherent overview of cutting-edge research in this direction.1 We will start by first giving a brief background of open-domain question answering, discussing the basic setup and core technical challenges of the research problem. We aim to give the audience a historical view of how the field has advanced in the past several decades, from highly-modulated pipeline systems in the early days, to"
2020.acl-tutorials.8,D16-1264,0,0.0937292,"provide a comprehensive and coherent overview of cutting-edge research in this direction.1 We will start by first giving a brief background of open-domain question answering, discussing the basic setup and core technical challenges of the research problem. We aim to give the audience a historical view of how the field has advanced in the past several decades, from highly-modulated pipeline systems in the early days, to modern endto-end training of deep neural networks in the present. We will then discuss modern datasets proposed for open-domain QA (Voorhees et al., 1999; Berant et al., 2013; Rajpurkar et al., 2016; Joshi et al., 2017; Dhingra et al., 2017; Dunn et al., 2017; Kwiatkowski et al., 2019), as well as common evaluation metrics and benchmarks. We plan to provide Two-stage retriever-reader approaches. We will start by discussing two-stage retriever-reader frameworks for open-domain QA, pioneered by Chen et al. (2017): a retriever component finding documents that (might) contain an answer from a large collection of documents, followed by a reader component finding the answer in a given paragraph or a document. In this category, the retriever component is usually implemented by traditional spars"
2020.acl-tutorials.8,2020.emnlp-main.437,0,0.199154,"Missing"
2020.acl-tutorials.8,P19-1436,0,0.0834594,"ees et al., 1999; Moldovan et al., 2000; Brill et al., 2002; Ferrucci et al., 2010). Traditional QA systems were usually constructed as a pipeline, consisting of many different components such as question processing, document/passage retrieval, and answer processing. With the rapid development of neural reading comprehension (Chen, 2018), modern open-domain QA systems have been restructured by combining traditional IR techniques and neural reading comprehension models (Chen et al., 2017; Yang et al., 2019; Min et al., 2019a) or even implemented in a fully end-to-end fashion (Lee et al., 2019; Seo et al., 2019; Guu et al., 2020; Roberts et al., 2020). In this tutorial, we aim to provide a comprehensive and coherent overview of cutting-edge research in this direction.1 We will start by first giving a brief background of open-domain question answering, discussing the basic setup and core technical challenges of the research problem. We aim to give the audience a historical view of how the field has advanced in the past several decades, from highly-modulated pipeline systems in the early days, to modern endto-end training of deep neural networks in the present. We will then discuss modern datasets pro"
2020.acl-tutorials.8,D19-1242,0,0.0278281,"Missing"
2020.emnlp-main.522,D13-1160,0,0.315709,"Missing"
2020.emnlp-main.522,P17-1171,0,0.0611096,"Missing"
2020.emnlp-main.522,N19-1423,0,0.252704,"acebookresearch/BLINK/tree/master/elq 1 have only been evaluated on long, well-formed documents like news articles (Ji et al., 2010), but not on short, noisy text. Also, most prior works have focused mainly on improving model prediction accuracy, largely overlooking efficiency. In this work, we propose ELQ, a fast and accurate entity linking system that specifically targets questions. Following the Wikification setup (Ratinov et al., 2011), ELQ aims to identify the mention boundaries of entities in a given question and their corresponding Wikipedia entity. We employ a biencoder based on BERT (Devlin et al., 2019) as shown in Figure 1. The entity encoder computes entity embeddings for all entities in Wikipedia, using their short descriptions. Then, the question encoder derives token-level embeddings for the input question. We detect mention boundaries using these embeddings, and disambiguate each entity mention based on an inner product between the mention embeddings (averaged embedding over mention tokens) and the entity embeddings. Our model ex6433 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6433–6441, c November 16–20, 2020. 2020 Association for Comp"
2020.emnlp-main.522,P19-1335,0,0.0527789,"d for building robust question answering (QA) systems. For instance, the question “when did shaq come to the nba?” can be answered by examining Shaquille O’Neal’s Wikipedia article (Min et al., 2019), or its properties in a knowledge graph (Yih et al., 2015; Yu et al., 2017). However, real-world user questions are invariably noisy and ill-formed, lacking cues provided by casing and punctuation, which prove challenging to current end-to-end entity linking systems (Yang and Chang, 2015; Sorokin and Gurevych, 2018). While recent pre-trained models have proven highly effective for entity linking (Logeswaran et al., 2019; Wu et al., 2020), they are only designed for entity disambiguation and require mention boundaries to be given in the input. Additionally, such systems ∗ Work done while at Facebook AI. Code and data available at https://github.com/ facebookresearch/BLINK/tree/master/elq 1 have only been evaluated on long, well-formed documents like news articles (Ji et al., 2010), but not on short, noisy text. Also, most prior works have focused mainly on improving model prediction accuracy, largely overlooking efficiency. In this work, we propose ELQ, a fast and accurate entity linking system that specifica"
2020.emnlp-main.550,P17-1171,1,0.902365,", p2 , . . . , pM }, where each passage pi can be viewed as a sequence (i) (i) (i) of tokens w1 , w2 , · · · , w|pi |. Given a question q, (i) (i) (i) the task is to find a span ws , ws+1 , · · · , we from one of the passages pi that can answer the question. Notice that to cover a wide variety of domains, the corpus size can easily range from millions of documents (e.g., Wikipedia) to billions (e.g., the Web). As a result, any open-domain QA system needs to include an efficient retriever component that can select a small set of relevant texts, before applying the reader to extract the answer (Chen et al., 2017).4 Formally speaking, a retriever R : (q, C) → CF is a function that takes as input a question q and a corpus C and returns a much smaller filter set of texts CF ⊂ C, where |CF |= k  |C|. For a fixed k, a retriever can be evaluated in isolation on top-k retrieval accuracy, which is the fraction of questions for which CF contains a span that answers the question. 3 Dense Passage Retriever (DPR) We focus our research in this work on improving the retrieval component in open-domain QA. Given a collection of M text passages, the goal of our dense passage retriever (DPR) is to index all the passag"
2020.emnlp-main.550,P17-1147,0,0.14452,"prepended with the title of the Wikipedia article where the passage is from, along with an [SEP] token. 4.2 Question Answering Datasets We use the same five QA datasets and training/dev/testing splitting method as in previous work (Lee et al., 2019). Below we briefly describe each dataset and refer readers to their paper for the details of data preparation. Natural Questions (NQ) (Kwiatkowski et al., 2019) was designed for end-to-end question answering. The questions were mined from real Google search queries and the answers were spans in Wikipedia articles identified by annotators. TriviaQA (Joshi et al., 2017) contains a set of trivia questions with answers that were originally scraped from the Web. WebQuestions (WQ) (Berant et al., 2013) consists of questions selected using Google Suggest API, where the answers are entities in Freebase. ˇ y, CuratedTREC (TREC) (Baudiˇs and Sediv` 2015) sources questions from TREC QA tracks 5 However, Wang et al. (2019) also propose splitting documents into overlapping passages, which we do not find advantageous compared to the non-overlapping version. Dataset Natural Questions TriviaQA WebQuestions CuratedTREC SQuAD Train 79,168 78,785 3,417 1,353 78,713 58,880 60"
2020.emnlp-main.550,N19-1423,0,0.142037,"s distance is equivalent to L2 distance in a transformed space. Inner product search has been widely used and studied, as well as its connection to cosine similarity and L2 distance (Mussmann and Ermon, 2016; Ram and Gray, 2012). As our ablation study finds other similarity functions perform comparably (Section 5.2; Appendix B), we thus choose the simpler inner product function and improve the dense passage retriever by learning better encoders. Encoders Although in principle the question and passage encoders can be implemented by any neural networks, in this work we use two independent BERT (Devlin et al., 2019) networks (base, uncased) and take the representation at the [CLS] token as the output, so d = 768. Inference During inference time, we apply the passage encoder EP to all the passages and index them using FAISS (Johnson et al., 2017) offline. FAISS is an extremely efficient, open-source library for similarity search and clustering of dense vectors, which can easily be applied to billions of vectors. Given a question q at run-time, we derive its embedding vq = EQ (q) and retrieve the top k passages with embeddings closest to vq . 3.2 Training Training the encoders so that the dot-product simil"
2020.emnlp-main.550,K19-1049,0,0.197277,"in a batch of size B. S = QPT is a (B × B) matrix of similarity scores, where each row of which corresponds to a question, paired with B passages. In this way, we reuse computation and effectively train on B 2 (qi , pj ) question/passage pairs in each batch. Any (qi , pj ) pair is a positive example when i = j, and negative otherwise. This creates B training instances in each batch, where there are B − 1 6771 negative passages for each question. The trick of in-batch negatives has been used in the full batch setting (Yih et al., 2011) and more recently for mini-batch (Henderson et al., 2017; Gillick et al., 2019). It has been shown to be an effective strategy for learning a dual-encoder model that boosts the number of training examples. 4 Experimental Setup In this section, we describe the data we used for experiments and the basic setup. 4.1 Wikipedia Data Pre-processing Following (Lee et al., 2019), we use the English Wikipedia dump from Dec. 20, 2018 as the source documents for answering questions. We first apply the pre-processing code released in DrQA (Chen et al., 2017) to extract the clean, text-portion of articles from the Wikipedia dump. This step removes semi-structured data, such as tables,"
2020.emnlp-main.550,P19-1612,0,0.545445,"encodings are also learnable by adjusting the embedding functions, which provides additional flexibility to have a task-specific representation. With special in-memory data structures and indexing schemes, retrieval can be done efficiently using maximum inner product search (MIPS) algorithms (e.g., Shrivastava and Li (2014); Guo et al. (2016)). However, it is generally believed that learning a good dense vector representation needs a large number of labeled pairs of question and contexts. Dense retrieval methods have thus never be shown to outperform TF-IDF/BM25 for opendomain QA before ORQA (Lee et al., 2019), which proposes a sophisticated inverse cloze task (ICT) objective, predicting the blocks that contain the masked sentence, for additional pretraining. The question encoder and the reader model are then finetuned using pairs of questions and answers jointly. Although ORQA successfully demonstrates that dense retrieval can outperform BM25, setting new state-of-the-art results on multiple open-domain 6769 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 6769–6781, c November 16–20, 2020. 2020 Association for Computational Linguistics QA datasets, it"
2020.emnlp-main.550,2020.acl-main.703,0,0.0786025,"simple and yet effective solution that shows stronger empirical performance, without relying on additional pretraining or complex joint training schemes. DPR has also been used as an important module in very recent work. For instance, extending the idea of leveraging hard negatives, Xiong et al. (2020a) use the retrieval model trained in the previous iteration to discover new negatives and construct a different set of examples in each training iteration. Starting from our trained DPR model, they show that the retrieval performance can be further improved. Recent work (Izacard and Grave, 2020; Lewis et al., 2020b) have also shown that DPR can be combined with generation models such as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2019), achieving good performance on open-domain QA and other knowledge-intensive tasks. 8 Conclusion In this work, we demonstrated that dense retrieval can outperform and potentially replace the traditional sparse retrieval component in open-domain question answering. While a simple dual-encoder approach can be made to work surprisingly well, we showed that there are some critical ingredients to training a dense retriever successfully. Moreover, our empirical analysis a"
2020.emnlp-main.550,P18-1161,0,0.0340757,"HNSW index type on CPU, neighbors to store per node = 512, construction time search depth = 200, search depth = 128. score is chosen as the final answer. The passage selection model serves as a reranker through crossattention between the question and the passage. Although cross-attention is not feasible for retrieving relevant passages in a large corpus due to its nondecomposable nature, it has more capacity than the dual-encoder model sim(q, p) as in Eq. (1). Applying it to selecting the passage from a small number of retrieved candidates has been shown to work well (Wang et al., 2019, 2018; Lin et al., 2018). Specifically, let Pi ∈ RL×h (1 ≤ i ≤ k) be a BERT (base, uncased in our experiments) representation for the i-th passage, where L is the maximum length of the passage and h the hidden dimension. The probabilities of a token being the starting/ending positions of an answer span and a passage being selected are defined as:  Pstart,i (s) = softmax Pi wstart s , (3)  Pend,i (t) = softmax Pi wend t , (4)  ˆ |wselected , (5) Pselected (i) = softmax P i ˆ = [P[CLS] , . . . , P[CLS] ] ∈ Rh×k and where P 1 k wstart , wend , wselected ∈ Rh are learnable vectors. We compute a span score of the s-th"
2020.emnlp-main.550,D19-1284,1,0.906045,"lopment set. For experiments on small datasets under the Multi setting, in which using other datasets is allowed, we fine-tune the reader trained on Natural Questions to the target dataset. All experiments were done on eight 32GB GPUs. 6.2 Results Table 4 summarizes our final end-to-end QA results, measured by exact match with the reference answer after minor normalization as in (Chen et al., 2017; Lee et al., 2019). From the table, we can 6775 Training Model NQ TriviaQA WQ TREC SQuAD Single Single Single Single Single Single Single BM25+BERT (Lee et al., 2019) ORQA (Lee et al., 2019) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever (Asai et al., 2020) REALMWiki (Guu et al., 2020) REALMNews (Guu et al., 2020) 26.5 33.3 28.1 34.5 32.6 39.2 40.4 47.1 45.0 50.9 56.0 - 17.7 36.4 36.4 40.2 40.7 21.3 30.1 46.8 42.9 33.2 20.2 56.5 - Single BM25 DPR BM25+DPR 32.6 41.5 39.0 52.4 56.8 57.0 29.9 34.6 35.2 24.9 25.9 28.0 38.1 29.8 36.7 Multi DPR BM25+DPR 41.5 38.8 56.8 57.9 42.4 41.1 49.4 50.6 24.1 35.8 Table 4: End-to-end QA (Exact Match) Accuracy. The first block of results are copied from their cited papers. REALMWiki and REALMNews are the same model but pretrained on Wikipedia a"
2020.emnlp-main.550,D19-1258,0,0.050899,"to be optimal for NQ, and k = 10 leads to only marginal loss in exact match accuracy (40.8 vs. 41.5 EM on NQ), which should be roughly comparable to ORQA’s 5-passage setup. 7 Related Work Passage retrieval has been an important component for open-domain QA (Voorhees, 1999). It not only effectively reduces the search space for answer extraction, but also identifies the support context for users to verify the answer. Strong sparse vector space models like TF-IDF or BM25 have 6776 been used as the standard method applied broadly to various QA tasks (e.g., Chen et al., 2017; Yang et al., 2019a,b; Nie et al., 2019; Min et al., 2019a; Wolfson et al., 2020). Augmenting text-based retrieval with external structured information, such as knowledge graph and Wikipedia hyperlinks, has also been explored recently (Min et al., 2019b; Asai et al., 2020). The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis (Deerwester et al., 1990). Using labeled pairs of queries and documents, discriminatively trained dense encoders have become popular recently (Yih et al., 2011; Huang et al., 2013; Gillick et al., 2019), with applications to cross-lingual document retrieval, a"
2020.emnlp-main.550,D19-1599,0,0.675786,"illion passages in our experiments, described in Section 4.1) and k is usually small, such as 20–100. 3.1 Overview Our dense passage retriever (DPR) uses a dense encoder EP (·) which maps any text passage to a ddimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval. 3 The ideal size and boundary of a text passage are functions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using fixed-length passages performs better in both retrieval and final QA accuracy, as observed by Wang et al. (2019). 4 Exceptions include (Seo et al., 2019) and (Roberts et al., 2020), which retrieves and generates the answers, respectively. 6770 At run-time, DPR applies a different encoder EQ (·) that maps the input question to a d-dimensional vector, and retrieves k passages of which vectors are the closest to the question vector. We define the similarity between the question and the passage using the dot product of their vectors: sim(q, p) = EQ (q) |EP (p). (1) Although more expressive model forms for measuring the similarity between a question and a passage do exist, such as networks consisting of mult"
2020.emnlp-main.550,2020.tacl-1.13,0,0.0179189,"s to only marginal loss in exact match accuracy (40.8 vs. 41.5 EM on NQ), which should be roughly comparable to ORQA’s 5-passage setup. 7 Related Work Passage retrieval has been an important component for open-domain QA (Voorhees, 1999). It not only effectively reduces the search space for answer extraction, but also identifies the support context for users to verify the answer. Strong sparse vector space models like TF-IDF or BM25 have 6776 been used as the standard method applied broadly to various QA tasks (e.g., Chen et al., 2017; Yang et al., 2019a,b; Nie et al., 2019; Min et al., 2019a; Wolfson et al., 2020). Augmenting text-based retrieval with external structured information, such as knowledge graph and Wikipedia hyperlinks, has also been explored recently (Min et al., 2019b; Asai et al., 2020). The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis (Deerwester et al., 1990). Using labeled pairs of queries and documents, discriminatively trained dense encoders have become popular recently (Yih et al., 2011; Huang et al., 2013; Gillick et al., 2019), with applications to cross-lingual document retrieval, ad relevance prediction, Web search and ent"
2020.emnlp-main.550,D16-1264,0,0.127212,"hich we do not find advantageous compared to the non-overlapping version. Dataset Natural Questions TriviaQA WebQuestions CuratedTREC SQuAD Train 79,168 78,785 3,417 1,353 78,713 58,880 60,413 2,474 1,125 70,096 Dev Test 8,757 8,837 361 133 8,886 3,610 11,313 2,032 694 10,570 Table 1: Number of questions in each QA dataset. The two columns of Train denote the original training examples in the dataset and the actual questions used for training DPR after filtering. See text for more details. as well as various Web sources and is intended for open-domain QA from unstructured corpora. SQuAD v1.1 (Rajpurkar et al., 2016) is a popular benchmark dataset for reading comprehension. Annotators were presented with a Wikipedia paragraph, and asked to write questions that could be answered from the given text. Although SQuAD has been used previously for open-domain QA research, it is not ideal because many questions lack context in absence of the provided paragraph. We still include it in our experiments for providing a fair comparison to previous work and we will discuss more in Section 5.1. Selection of positive passages Because only pairs of questions and answers are provided in TREC, WebQuestions and TriviaQA6 ,"
2020.emnlp-main.550,N19-4013,0,0.0550298,"that we found k = 50 to be optimal for NQ, and k = 10 leads to only marginal loss in exact match accuracy (40.8 vs. 41.5 EM on NQ), which should be roughly comparable to ORQA’s 5-passage setup. 7 Related Work Passage retrieval has been an important component for open-domain QA (Voorhees, 1999). It not only effectively reduces the search space for answer extraction, but also identifies the support context for users to verify the answer. Strong sparse vector space models like TF-IDF or BM25 have 6776 been used as the standard method applied broadly to various QA tasks (e.g., Chen et al., 2017; Yang et al., 2019a,b; Nie et al., 2019; Min et al., 2019a; Wolfson et al., 2020). Augmenting text-based retrieval with external structured information, such as knowledge graph and Wikipedia hyperlinks, has also been explored recently (Min et al., 2019b; Asai et al., 2020). The use of dense vector representations for retrieval has a long history since Latent Semantic Analysis (Deerwester et al., 1990). Using labeled pairs of queries and documents, discriminatively trained dense encoders have become popular recently (Yih et al., 2011; Huang et al., 2013; Gillick et al., 2019), with applications to cross-lingual"
2020.emnlp-main.550,W11-0329,1,0.913257,"passage. Let Q and P be the (B ×d) matrix of question and passage embeddings in a batch of size B. S = QPT is a (B × B) matrix of similarity scores, where each row of which corresponds to a question, paired with B passages. In this way, we reuse computation and effectively train on B 2 (qi , pj ) question/passage pairs in each batch. Any (qi , pj ) pair is a positive example when i = j, and negative otherwise. This creates B training instances in each batch, where there are B − 1 6771 negative passages for each question. The trick of in-batch negatives has been used in the full batch setting (Yih et al., 2011) and more recently for mini-batch (Henderson et al., 2017; Gillick et al., 2019). It has been shown to be an effective strategy for learning a dual-encoder model that boosts the number of training examples. 4 Experimental Setup In this section, we describe the data we used for experiments and the basic setup. 4.1 Wikipedia Data Pre-processing Following (Lee et al., 2019), we use the English Wikipedia dump from Dec. 20, 2018 as the source documents for answering questions. We first apply the pre-processing code released in DrQA (Chen et al., 2017) to extract the clean, text-portion of articles"
2020.emnlp-main.550,P19-1436,0,0.304731,"bed in Section 4.1) and k is usually small, such as 20–100. 3.1 Overview Our dense passage retriever (DPR) uses a dense encoder EP (·) which maps any text passage to a ddimensional real-valued vectors and builds an index for all the M passages that we will use for retrieval. 3 The ideal size and boundary of a text passage are functions of both the retriever and reader. We also experimented with natural paragraphs in our preliminary trials and found that using fixed-length passages performs better in both retrieval and final QA accuracy, as observed by Wang et al. (2019). 4 Exceptions include (Seo et al., 2019) and (Roberts et al., 2020), which retrieves and generates the answers, respectively. 6770 At run-time, DPR applies a different encoder EQ (·) that maps the input question to a d-dimensional vector, and retrieves k passages of which vectors are the closest to the question vector. We define the similarity between the question and the passage using the dot product of their vectors: sim(q, p) = EQ (q) |EP (p). (1) Although more expressive model forms for measuring the similarity between a question and a passage do exist, such as networks consisting of multiple layers of cross attentions, the simi"
2020.emnlp-main.550,W02-1033,0,\N,Missing
2020.emnlp-main.550,D13-1160,0,\N,Missing
2020.emnlp-main.550,Q19-1026,0,\N,Missing
2020.emnlp-main.550,2020.emnlp-main.437,0,\N,Missing
2020.emnlp-main.559,Q13-1005,0,0.556787,"ework, MISP (Model-based Interactive Semantic Parsing), which handles uncertainty modeling and natural language generation. We will leverage MISP for user interaction to prove the feasibility of the envisioned methodology. However, existing studies only focus on interacting with users to resolve uncertainties. None of them has fully addressed the crucial problem of how to continually learn from user interaction, which is the technical focus of this study. One form of user interaction explored for learning semantic parsers is asking users to validate the execution results (Clarke et al., 2010; Artzi and Zettlemoyer, 2013; Iyer et al., 2017). While appealing, in practice it may be a difficult task for real users because they would not need to ask the question if they knew the answer in the first place. We instead aim to learn semantic parsers from finegrained interaction where users only need to answer simple questions covered by their background knowledge (Figure 1). However, learning signals from such fine-grained interactions are bound to be sparse because the system needs to avoid asking too many questions and overwhelming the user, which poses a challenge for learning. To tackle the problem, we propose N"
2020.emnlp-main.559,D13-1160,0,0.148718,"Missing"
2020.emnlp-main.559,P18-1124,1,0.934605,"o minimizing privacy risks. The bootstrapping cost could also be significantly reduced because an interactive system needs not to be almost perfectly accurate to be deployed. On the other hand, such interaction opens up the black box and allows users to know more about the reasoning underneath the system and better interpret the final results (Su et al., 2018). A human-in-theloop methodology like this also opens the door for domain adaptation and personalization. This work builds on the recent line of research on interactive semantic parsing (Li and Jagadish, 2014; Chaurasia and Mooney, 2017; Gur et al., 2018; Yao et al., 2019b). Specifically, Yao et al. (2019b) provide a general framework, MISP (Model-based Interactive Semantic Parsing), which handles uncertainty modeling and natural language generation. We will leverage MISP for user interaction to prove the feasibility of the envisioned methodology. However, existing studies only focus on interacting with users to resolve uncertainties. None of them has fully addressed the crucial problem of how to continually learn from user interaction, which is the technical focus of this study. One form of user interaction explored for learning semantic par"
2020.emnlp-main.559,P17-1097,0,0.0426573,"Missing"
2020.emnlp-main.559,P19-1358,0,0.0318114,"tural language feedback. Semantic Machines et al. (2020) constructed a contextual semantic parsing dataset where agents could trigger conversations to handle exceptions such as ambigu6884 ous or incomplete user commands. In this work, we seek to continually improve semantic parsers from such user interaction, a topic that is not carefully studied by the aforementioned work. Interactive Learning from Feedback. Learning interactively from user feedback has been studied in many NLP tasks (Sokolov et al., 2016; Wang et al., 2016, 2017; Nguyen et al., 2017; Gao et al., 2018; Abujabal et al., 2018; Hancock et al., 2019; Kreutzer and Riezler, 2019). Most relevant to us, Hancock et al. (2019) constructed a self-feeding chatbot that improves itself from user satisfied responses and their feedback on unsatisfied ones. In the field of semantic parsing, Clarke et al. (2010); Artzi and Zettlemoyer (2013); Iyer et al. (2017) learned semantic parsers from binary user feedback on whether executing the generated query yields correct results. However, often times (especially in information-seeking scenarios) it may not be very practical to expect end users able to validate the denotation correctness (e.g., consider val"
2020.emnlp-main.559,W10-2903,0,\N,Missing
2020.emnlp-main.559,P16-1152,0,\N,Missing
2020.emnlp-main.559,D16-1258,0,\N,Missing
2020.emnlp-main.559,I17-2030,0,\N,Missing
2020.emnlp-main.559,P18-2008,0,\N,Missing
2020.emnlp-main.559,P19-1029,0,\N,Missing
2020.emnlp-main.559,D19-1537,0,\N,Missing
2020.emnlp-main.713,N16-1181,0,0.0387079,"Missing"
2020.emnlp-main.713,D18-1549,0,0.0223216,"seudo-Decompositions Since subquestions are retrieval-based, they are often not about the same entities as q. Inspired by retrieve-and-edit methods (e.g., Guu et al., 2018), we replace each sub-question entity not in q with an entity from q of the same type (e.g., “Date” or “Location”) if possible.5 This step is important for PseudoD and Seq2Seq (which would learn to hallucinate entities) but not ONUS (which must reconstruct entities in q from its own decomposition, as discussed next). 3.2.3 Unsupervised Decomposition Models Pretraining Pretraining is crucial for unsupervised Seq2Seq methods (Artetxe et al., 2018; Lample et al., 2018), so we initialize all decomposition models (Seq2Seq or ONUS) with the same pretrained weights. We warm-start our pretraining with the pretrained, English Masked Language Model (MLM) from Lample and Conneau (2019), a 12-block transformer (Vaswani et al., 2017). We do MLM finetuning for one epoch on Q and pseudodecompositions D formed via random retrieval, using the final weights to initialize a pretrained encoder-decoder. See Appendix §B.2 for details. Seq2Seq We finetune the pretrained encoderdecoder using maximum likelihood. We stop training based on validation BLEU bet"
2020.emnlp-main.713,P19-1309,0,0.0593854,"rvision, we find it possible to decompose questions in a fully unsupervised way. We propose an algorithm for One-to-N Unsupervised Sequence transduction (ONUS) that learns to map from the distribution of hard questions to that of many simple questions. First, we automatically create a noisy “pseudo-decomposition” for each hard question by using embedding similarity to retrieve sub-question candidates. We mine over 10M possible sub-questions from Common Crawl with a classifier, showcasing the effectiveness of parallel corpus mining, a common approach in machine translation (Xu and Koehn, 2017; Artetxe and Schwenk, 2019), for QA. Second, we train a decomposition model on the mined data with unsupervised sequence-to-sequence learning, allowing ONUS to improve over pseudo-decompositions. As a result, we are able to train a large transformer model to generate decompositions, surpassing the fluency of heuristic/extractive decompositions. Figure 2 overviews our approach to decomposition. We validate ONUS on multi-hop QA, where questions require reasoning over multiple pieces of evidence. We use an off-the-shelf single-hop QA model to answer decomposed sub-questions. Then, we give sub-questions and their answers to"
2020.emnlp-main.713,Q17-1010,0,0.0230205,"y increases the number of single-hop questions (130K → 10.1M) and multi-hop questions (90K → 2.4M), showing the power of parallel corpus mining in QA. 2 3.2.2 Creating Pseudo-Decompositions To create pseudo-decompositions (retrieval-based sub-questions for a given question), we experimented with using a variable number of subquestions N per question (Appendix §A.1), but we found similar QA results with a fixed N = 2, which we use in the remainder for simplicity. Similarity-based Retrieval To retrieve relevant sub-questions, we embed any text t into a vector vt by summing the FastText vectors (Bojanowski et al., 2017)3 for words in t and use cosine as our similarity metric f .4 Let q be a multi-hop question ˆ be the with a pseudo-decomposition (s∗1 , s∗2 ) and v unit vector of v. Since N = 2, Eq. 1 simplifies to: h i ˆ s2 − v ˆq> v ˆs1 + v ˆq> v ˆs>1 v ˆ s2 (s∗1 , s∗2 ) = argmax v {s1 ,s2 }∈S The last term requires O(|S|2 ) comparisons, which is expensive as |S |> 10M. Instead of solving the above equation exactly, we find an approximate 0 , s0 ) by computing over pseudo-decomposition (s 1 2   S 0 = topK{s∈S} v ˆq> v ˆs with K = 1000. We efficiently build S 0 with FAISS (Johnson et al., 2017a). Random Re"
2020.emnlp-main.713,P17-1171,0,0.0844335,"Missing"
2020.emnlp-main.713,P18-1078,0,0.0373823,"hop) 7 (Baseline) 66.7 77.0±.2 63.7 65.2±.2 66.5 67.1±.5 PseudoD Model Architecture Our model takes in a question and several paragraphs to predict the answer. We compute a separate forward pass on each paragraph (with the question). For each paragraph, the model learns to predict the answer span if the paragraph contains the answer and to predict “no answer” otherwise. We treat yes or no predictions as spans within the passage (prepended to each paragraph), as in Nie et al. (2019) on H OTPOT QA. During inference, for the final softmax, we consider all paragraphs as a single chunk. Similar to Clark and Gardner (2018), we subtract a paragraph’s “no answer” logit from the logits of all spans in that paragraph, to reduce or increase span probabilities accordingly. In other words, we compute the probability p(sp ) of each span sp in a paragraph p ∈ {1, . . . , P } using the predicted span logit l(sp ) and “no answer” paragraph logit n(p) with p(sp ) ∝ el(sp )−n(p) . RO BERTA LARGE (Liu et al., 2019) is used as our pretrained model. Seq2Seq Random FastText Random FastText Random FastText 78.4±.2 78.9±.2 77.7±.2 78.9±.2 79.8±.1 80.1±.2 70.9±.2 72.4±.1 69.4±.3 73.1±.2 76.0±.2 76.2±.1 70.7±.4 72.0±.1 70.0±.7 73.0"
2020.emnlp-main.713,N19-1423,0,0.0236315,"set versions: (1) the original version,9 (2) the multi-hop version from Jiang and Bansal (2019a) who created some distractor paragraphs adversarially to test multi-hop reasoning, and (3) the out-of-domain (OOD) version from Min et al. (2019b) who retrieved distractor paragraphs with the same procedure as the original version but excluded the original paragraphs. Main Results Table 1 shows how unsupervised decompositions affect QA. Our RO BERTA baseline does quite well on H OTPOT QA (77.0 F1), in line with Min et al. (2019a) who achieved strong results using a BERT-based version of the model (Devlin et al., 2019). We achieve large gains over the RO BERTA baseline by simply adding sub-questions and sub-answers to the input. Using decompositions from ONUS trained on FastText 9 Test set is private, so we randomly halve the dev set to form validation/held-out dev sets. Our codebase has our splits. 8868 QType Using Decomps. 7 X SQs SAs QA F1 7 7 77.0±.2 Bridge Comp. Inters. 1-hop 80.1±.2 73.8±.4 79.4±.6 73.9±.6 81.7±.4 80.1±.3 82.3±.5 76.9±.6 X X X X 7 Sent. Span Rand. 7 Sent. 80.1±.2 77.8±.3 76.9±.2 76.9±.2 80.2±.1 Table 2: Left: Decompositions improve QA F1 for all 4 H OTPOT QA types. Right (Ablation): Q"
2020.emnlp-main.713,E17-2068,0,0.0335695,"m large training corpora. A larger simple question corpus will also improve the relevance of retrieved simple questions to the hard 8866 question. Thus, we take inspiration from parallel corpus mining in machine translation (Xu and Koehn, 2017; Artetxe and Schwenk, 2019). We use questions from SQ UAD 2 and H OTPOT QA to form our initial corpora S (single-hop questions) and Q (multi-hop questions), respectively, and we augment Q and S by mining more questions from Common Crawl. First, we select sentences that start with “wh”-words or end in “?” Next, we train an efficient, FastText classifier (Joulin et al., 2017) to classify between questions sampled from Common Crawl, SQ UAD 2, and H OTPOT QA (60K in total). Then, we classify our Common Crawl questions, adding those classified as SQ UAD 2 questions to S and those classified as H OTPOT QA questions to Q. Mining greatly increases the number of single-hop questions (130K → 10.1M) and multi-hop questions (90K → 2.4M), showing the power of parallel corpus mining in QA. 2 3.2.2 Creating Pseudo-Decompositions To create pseudo-decompositions (retrieval-based sub-questions for a given question), we experimented with using a variable number of subquestions N p"
2020.emnlp-main.713,P16-1004,0,0.0292627,"COMP RC decomposes a multi-hop question using a heuristic algorithm or not at all. Watson and D E COMP RC use special case handling to decompose different questions, while our algorithm is fully automated and requires little hand-engineering. More traditional, semantic parsing methods map questions to compositional programs, whose subprograms can be viewed as question decompositions in a formal language (Talmor and Berant, 2018; Wolfson et al., 2020). Examples include classical QA systems like SHRDLU (Winograd, 1972) and LUNAR (Woods et al., 1974), as well as neural Seq2Seq semantic parsers (Dong and Lapata, 2016) and neural module networks (Andreas et al., 2015, 2016). Such methods usually require 8871 strong, program-level supervision to generate programs, as in visual QA (Johnson et al., 2017c) and on H OTPOT QA (Jiang and Bansal, 2019b). Some models use other forms of strong supervision, e.g., the sentences needed to answer a question, as annotated by H OTPOT QA. Such an approach is taken by SAE (Tu et al., 2020) and HGN (Fang et al., 2019), whose methods may be combined with ours. Unsupervised decomposition complements strongly and weakly supervised decomposition approaches. Our unsupervised appro"
2020.emnlp-main.713,D18-1091,0,0.0218187,"Q2 What is the name of the variation on a popular anecdote? x “Mrs. Bixby and the Colonel’s Coat” is a short story by Roald Dahl that first appeared in the 1959 issue of Nugget. ˆ more than 250 million A: 4.4 GPT2 NLL Unsupervised Decomposition Model Intrinsic Evaluation of Decompositions We evaluate the quality of decompositions on other metrics aside from downstream QA. To measure the fluency of decompositions, we compute the likelihood of decompositions using the pretrained GPT-2 language model (Radford et al., 2019). We train a BERT BASE classifier on the questionwellformedness dataset of Faruqui and Das (2018), and we use the classifier to estimate the proportion of sub-questions that are well-formed. We measure how abstractive decompositions are by computing (i) the token Levenstein distance between the multihop question and its generated decomposition and (ii) the ratio between the length of the decomposition and the length of the multi-hop question. We compare ONUS to D ECOMP RC (Min et al., 2019b), a supervised+heuristic decomposition method. As shown in Table 4, ONUS decompositions are more natural and well-formed than D ECOMP RC decompositions. As an example, for Table 3 Q3, D ECOMP RC produc"
2020.emnlp-main.713,Q18-1031,0,0.0158481,"h FAISS (Johnson et al., 2017a). Random Retrieval For comparison, we test a random pseudo-decomposition baseline, where we retrieve s1 , . . . , sN by sampling uniformly from S. 2 See Appendix §A.3 for details on question classifier. 300-dim. English Common Crawl vectors: https:// fasttext.cc/docs/en/english-vectors.html 4 We also tried TFIDF and BERT representations but did not see improvements over FastText (see Appendix §A.4). 3 Editing Pseudo-Decompositions Since subquestions are retrieval-based, they are often not about the same entities as q. Inspired by retrieve-and-edit methods (e.g., Guu et al., 2018), we replace each sub-question entity not in q with an entity from q of the same type (e.g., “Date” or “Location”) if possible.5 This step is important for PseudoD and Seq2Seq (which would learn to hallucinate entities) but not ONUS (which must reconstruct entities in q from its own decomposition, as discussed next). 3.2.3 Unsupervised Decomposition Models Pretraining Pretraining is crucial for unsupervised Seq2Seq methods (Artetxe et al., 2018; Lample et al., 2018), so we initialize all decomposition models (Seq2Seq or ONUS) with the same pretrained weights. We warm-start our pretraining with"
2020.emnlp-main.713,P19-1262,0,0.275833,"er decomposed sub-questions. Then, we give sub-questions and their answers to a recomposition model to combine into a final answer. We evaluate on three dev sets for H OTPOT QA, a standard benchmark for multi-hop QA (Yang et al., 2018), including two challenge sets. ONUS proves to be a powerful tool for QA in the following ways. First, QA models that use decompositions outperform a strong RoBERTa baseline (Liu et al., 2019; Min et al., 2019a) by 3.1 points in F1 on the original dev set, 10 points on the out-of-domain dev set from Min et al. (2019b), and 11 points on the multi-hop dev set from Jiang and Bansal (2019a). Our method is competitive with state-of-the-art methods SAE (Tu et al., 2020) and HGN (Fang et al., 2019) that use additional, strong supervision on which sentences are relevant to the question. Second, our analysis shows that sub-questions improve multi-hop QA by using the single-hop QA model to retrieve question-relevant text. Qualitative examples illustrate how the retrieved text adds a level of interpretability to otherwise black-box, neural QA models. Third, ONUS automatically learns to generate useful decompositions for all four question types in H OTPOT QA, highlighting the general"
2020.emnlp-main.713,P19-1484,1,0.847698,") between predicted and gold spans. 3.2 3.2.1 2.1.2 Learning to Decompose With the above pseudo-decompositions, we explore various decomposition methods (details in §3.2.3): PseudoD We use sub-questions from pseudodecompositions directly in downstream QA. Sequence-to-Sequence (Seq2Seq) We train a Seq2Seq model pθ to maximize log pθ (d0 |q). Question Answering Task Unsupervised Decomposition Training Data and Question Mining Supervised decomposition methods are limited by the amount of available human annotation, but our unsupervised method faces no such limitation, similar to unsupervised QA (Lewis et al., 2019). Since we need to train data-hungry Seq2Seq models, we would benefit from large training corpora. A larger simple question corpus will also improve the relevance of retrieved simple questions to the hard 8866 question. Thus, we take inspiration from parallel corpus mining in machine translation (Xu and Koehn, 2017; Artetxe and Schwenk, 2019). We use questions from SQ UAD 2 and H OTPOT QA to form our initial corpora S (single-hop questions) and Q (multi-hop questions), respectively, and we augment Q and S by mining more questions from Common Crawl. First, we select sentences that start with “w"
2020.emnlp-main.713,2021.ccl-1.108,0,0.209817,"Missing"
2020.emnlp-main.713,P19-1416,0,0.0732106,"Albert Camus?” (Petrochuk and Zettlemoyer, 2018). Thus, a promising strategy to answer hard questions is divide-and-conquer: decompose a hard question into simpler sub-questions, answer the sub-questions with a QA system, and recompose the resulting answers into a final answer, as shown in Figure 1. This approach leverages strong performance on simple questions to help answer harder questions (Christiano et al., 2018). Existing work decomposes questions using a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions (Talmor and Berant, 2018; Min et al., 2019b), which each require significant human effort. For example, D ECOMP RC (Min et al., 2019b) decomposes some questions using supervision and other questions using a heuristic algorithm with fine-grained, special case handling based on part8864 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8864–8880, c November 16–20, 2020. 2020 Association for Computational Linguistics ? ? ? Hard Question ? Simple Question ? ? Step 1 Pseudo Decomp. ? ? ? ? Step 2 ONUS ? ? ? ? or Step 2 Seq2Seq ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 2: One-to-N Unsupervised Sequence tra"
2020.emnlp-main.713,D19-1455,0,0.314797,"er decomposed sub-questions. Then, we give sub-questions and their answers to a recomposition model to combine into a final answer. We evaluate on three dev sets for H OTPOT QA, a standard benchmark for multi-hop QA (Yang et al., 2018), including two challenge sets. ONUS proves to be a powerful tool for QA in the following ways. First, QA models that use decompositions outperform a strong RoBERTa baseline (Liu et al., 2019; Min et al., 2019a) by 3.1 points in F1 on the original dev set, 10 points on the out-of-domain dev set from Min et al. (2019b), and 11 points on the multi-hop dev set from Jiang and Bansal (2019a). Our method is competitive with state-of-the-art methods SAE (Tu et al., 2020) and HGN (Fang et al., 2019) that use additional, strong supervision on which sentences are relevant to the question. Second, our analysis shows that sub-questions improve multi-hop QA by using the single-hop QA model to retrieve question-relevant text. Qualitative examples illustrate how the retrieved text adds a level of interpretability to otherwise black-box, neural QA models. Third, ONUS automatically learns to generate useful decompositions for all four question types in H OTPOT QA, highlighting the general"
2020.emnlp-main.713,P19-1613,0,0.175238,"Albert Camus?” (Petrochuk and Zettlemoyer, 2018). Thus, a promising strategy to answer hard questions is divide-and-conquer: decompose a hard question into simpler sub-questions, answer the sub-questions with a QA system, and recompose the resulting answers into a final answer, as shown in Figure 1. This approach leverages strong performance on simple questions to help answer harder questions (Christiano et al., 2018). Existing work decomposes questions using a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions (Talmor and Berant, 2018; Min et al., 2019b), which each require significant human effort. For example, D ECOMP RC (Min et al., 2019b) decomposes some questions using supervision and other questions using a heuristic algorithm with fine-grained, special case handling based on part8864 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8864–8880, c November 16–20, 2020. 2020 Association for Computational Linguistics ? ? ? Hard Question ? Simple Question ? ? Step 1 Pseudo Decomp. ? ? ? ? Step 2 ONUS ? ? ? ? or Step 2 Seq2Seq ? ? ? ? ? ? ? ? ? ? ? ? ? Figure 2: One-to-N Unsupervised Sequence tra"
2020.emnlp-main.713,D19-1258,0,0.0617453,"tune a pretrained model for single-hop QA following prior work from Min et al. (2019b) on H OTPOT QA, as described below.8 7 7 7 (1hop) 7 (Baseline) 66.7 77.0±.2 63.7 65.2±.2 66.5 67.1±.5 PseudoD Model Architecture Our model takes in a question and several paragraphs to predict the answer. We compute a separate forward pass on each paragraph (with the question). For each paragraph, the model learns to predict the answer span if the paragraph contains the answer and to predict “no answer” otherwise. We treat yes or no predictions as spans within the passage (prepended to each paragraph), as in Nie et al. (2019) on H OTPOT QA. During inference, for the final softmax, we consider all paragraphs as a single chunk. Similar to Clark and Gardner (2018), we subtract a paragraph’s “no answer” logit from the logits of all spans in that paragraph, to reduce or increase span probabilities accordingly. In other words, we compute the probability p(sp ) of each span sp in a paragraph p ∈ {1, . . . , P } using the predicted span logit l(sp ) and “no answer” paragraph logit n(p) with p(sp ) ∝ el(sp )−n(p) . RO BERTA LARGE (Liu et al., 2019) is used as our pretrained model. Seq2Seq Random FastText Random FastText Ra"
2020.emnlp-main.713,D17-1061,1,0.84176,"thods to leverage millions of otherwise unusable questions, similar to work on unsupervised QA (Lewis et al., 2019). When decomposition examples exist, supervised and unsupervised learning can be used in tandem to learn from both labeled and unlabeled examples. Such semi-supervised methods outperform supervised learning for tasks like machine translation (Sennrich et al., 2016). Other work on weakly supervised question generation uses a downstream QA model’s accuracy as a signal for learning to generate useful questions. Weakly supervised question generation often uses reinforcement learning (Nogueira and Cho, 2017; Wang and Lake, 2019; Strub et al., 2017; Das et al., 2017; Liang et al., 2018), where an unsupervised initialization can greatly mitigate the issues of exploring from scratch (Jaderberg et al., 2017). 7 Conclusion We proposed a QA system that answers a question via decomposition, without supervised question decompositions, using three stages: (1) decompose a question into many sub-questions using One-toN Unsupervised Sequence transduction (ONUS), (2) answer sub-questions with an off-the-shelf QA system, and (3) recompose sub-answers into a final answer. When evaluated on three H OTPOT QA dev"
2020.emnlp-main.713,D18-1051,0,0.0253497,"Question answering (QA) systems struggle to answer complex questions such as “What profession do H. L. Mencken and Albert Camus have in common?” since the required information is scattered in different places (Yang et al., 2018). However, QA systems accurately answer ∗ journalist KC was a part-time research scientist at Facebook AI Research while working on this paper. 1 Our code, data, and pretrained models are available at https://github.com/facebookresearch/ UnsupervisedDecomposition. simpler, related questions such as “What profession does H. L. Mencken have?” and “Who was Albert Camus?” (Petrochuk and Zettlemoyer, 2018). Thus, a promising strategy to answer hard questions is divide-and-conquer: decompose a hard question into simpler sub-questions, answer the sub-questions with a QA system, and recompose the resulting answers into a final answer, as shown in Figure 1. This approach leverages strong performance on simple questions to help answer harder questions (Christiano et al., 2018). Existing work decomposes questions using a combination of hand-crafted heuristics, rule-based algorithms, and learning from supervised decompositions (Talmor and Berant, 2018; Min et al., 2019b), which each require significan"
2020.emnlp-main.713,P16-1009,0,0.155251,"tart our pretraining with the pretrained, English Masked Language Model (MLM) from Lample and Conneau (2019), a 12-block transformer (Vaswani et al., 2017). We do MLM finetuning for one epoch on Q and pseudodecompositions D formed via random retrieval, using the final weights to initialize a pretrained encoder-decoder. See Appendix §B.2 for details. Seq2Seq We finetune the pretrained encoderdecoder using maximum likelihood. We stop training based on validation BLEU between generated decompositions and pseudo-decompositions. ONUS We finetune the pretrained encoderdecoder with back-translation (Sennrich et al., 2016) and denoising objectives simultaneously, similar to Lample and Conneau (2019) in unsupervised one-to-one translation.6 For denoising, we produce a noisy input d0 by randomly masking, dropping, and locally shuffling tokens in d ∼ D, and we train a model with parameters θ to maximize log pθ (d|d0 ). We likewise maximize log pθ (q|q 0 ) for a noised version q 0 of q ∼ Q. For back-translation, we generate a multihop question qˆ for a decomposition d ∼ D, and we maximize log pθ (d|ˆ q ). Similarly, we maximize ˆ for a model-generated decomposition dˆ log pθ (q|d) of q ∼ Q. To stop training without"
2020.emnlp-main.713,2020.tacl-1.13,0,0.126801,"frames subquestions as extractive spans of a question, learning to predict span-based sub-questions via supervised learning on human annotations. In other cases, D E COMP RC decomposes a multi-hop question using a heuristic algorithm or not at all. Watson and D E COMP RC use special case handling to decompose different questions, while our algorithm is fully automated and requires little hand-engineering. More traditional, semantic parsing methods map questions to compositional programs, whose subprograms can be viewed as question decompositions in a formal language (Talmor and Berant, 2018; Wolfson et al., 2020). Examples include classical QA systems like SHRDLU (Winograd, 1972) and LUNAR (Woods et al., 1974), as well as neural Seq2Seq semantic parsers (Dong and Lapata, 2016) and neural module networks (Andreas et al., 2015, 2016). Such methods usually require 8871 strong, program-level supervision to generate programs, as in visual QA (Johnson et al., 2017c) and on H OTPOT QA (Jiang and Bansal, 2019b). Some models use other forms of strong supervision, e.g., the sentences needed to answer a question, as annotated by H OTPOT QA. Such an approach is taken by SAE (Tu et al., 2020) and HGN (Fang et al.,"
2020.emnlp-main.713,P02-1040,0,\N,Missing
2020.emnlp-main.713,D17-1319,0,\N,Missing
2020.emnlp-main.713,D18-1259,0,\N,Missing
2020.fever-1.5,W18-2501,0,0.0249596,"Missing"
2020.fever-1.5,2020.acl-main.703,0,0.0592685,"Missing"
2020.fever-1.5,N18-1202,0,0.0207222,"now also for fact checking. In order to determine the feasibility of our approach, we start with a human review study where participants are given a claim from FEVER (Thorne et al., 2018a), and are asked to validate the claim using only a BERT language model. We found that users had reasonable success in determining claim validity. Empowered by the results, Introduction Pre-trained language models have recently lead to significant advancements in wide variety of NLP tasks, including question-answering, commonsense reasoning, and semantic relatedness (Devlin et al., 2018; Radford et al., 2019; Peters et al., 2018; Radford et al., 2018). These models are typically trained on documents mined from Wikipedia (among other websites). Recently, a number of works have found that LMs store a surprising amount of world knowledge, focusing particularly on the task of open-domain question answering (Petroni et al., 2019; Roberts et al., 2020). In this paper, we explore whether we can leverage the knowledge in LMs for fact checking. We propose an approach (Fig. 1b) that replaces the document retriever and evidence selector models in traditional fact-checking (Fig. 1a) with a ∗ (b) Our new factchecking pipeline. Wo"
2020.fever-1.5,D19-1250,0,0.0593939,"Missing"
2020.fever-1.5,2020.emnlp-main.437,0,0.051507,"Missing"
2020.fever-1.5,N18-1074,0,0.179116,"Missing"
2020.fever-1.5,W18-5501,0,0.13934,"Missing"
2020.findings-emnlp.232,D19-1223,0,0.0157011,"ccessful application to masked language model pre-training (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), several approaches have been proposed to simplify the model and its training process. We summarize these attempts as follows: Attention layer simplification There are currently two lines of research trying to simplify the multi-head attention layers. The first one focuses on attention matrix sparsification. Notable examples include Star Transformer (Guo et al., 2019), Sparse Transformer (Child et al., 2019), Adaptive Sparse Transformer (Correia et al., 2019; Sukhbaatar et al., 2019), Log-Sparse Transformer (Li et al., 2019) , Reformer (Kitaev et al., 2020) and Longformer (Beltagy et al., 2020). However, due to the insufficient support for sparse tensors from the current deep learning platforms, some 2561 SearchQA EM F1 TriviaQA EM F1 NewsQA EM F1 NaturalQA EM F1 HotpotQA EM F1 N Model 512 Google BERT RoBERTa-2seq RoBERTa-1seq SparseB ERT BlockB ERT n=2 BlockB ERT n=3 74.94 76.12 77.09 73.36 76.68 75.54 80.37 81.74 82.62 79.01 82.33 81.07 70.18 71.92 73.65 68.71 72.36 72.05 75.35 76.79 78.22 73.15 77.53 76.74 51.27 52.45 56.13 51.18 54.66 53.82 6"
2020.findings-emnlp.232,N19-1423,0,0.522195,"et al., 2017), the building block of BERT. As the attention operation is quadratic to the sequence length, this fundamentally limits the maximum length of the input sequence, and thus restricts the model capacity in terms of capturing long-distance dependencies. As a result, downstream tasks have to either truncate their sequences to leading tokens (Nogueira and Cho, 2019) or split their sequences with a sliding window (Joshi et al., 2019a,b). Ad-hoc handling of long sequences is also required in the pre-training stage, such as updating the model using only short sequences in the early stage (Devlin et al., 2019). Common strategies for reducing memory consumption, unfortunately, do not work. For instance, 1 github.com/google-research/bert 2555 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2555–2565 c November 16 - 20, 2020. 2020 Association for Computational Linguistics shrinking the model by lowering the number of layers L, attention heads A, or hidden units H leads to significant performance degradation (Vaswani et al., 2017; Devlin et al., 2019) and does not address the long sequence issue. Alternatively, general low-memory training techniques, such as microbatching ("
2020.findings-emnlp.232,P84-1044,0,0.238639,"Missing"
2020.findings-emnlp.232,D18-1325,0,0.0732224,"Missing"
2020.findings-emnlp.232,P17-1147,0,0.071275,"Missing"
2020.findings-emnlp.232,D19-1588,1,0.912101,"9; Brown et al., 2020), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019), has drastically reshaped the landscape of the natural language processing research. These methods first pre-train a deep model with language model objectives using a large corpus and then fine-tune the model using in-domain supervised data for target applications. Despite its conceptual simplicity, this paradigm has re-established the new state-of-theart baselines across various tasks, such as question answering (Devlin et al., 2019), coreference resolution (Joshi et al., 2019b), relation extraction (Soares et al., 2019) and text retrieval (Lee et al., 2019; Nogueira and Cho, 2019), to name a few. ⇤ This work was partially done when the first author was an intern at Facebook AI. Code is available at https:// github.com/xptree/BlockBERT Building such models in practice, however, is an extremely resource-intensive process. For instance, the training of BERT-family models is notoriously expensive. Devlin et al. (2019) report that it takes four days to pre-train BERT-Base/BERTLarge on 4/16 Cloud TPUs. In order to reduce the pre-training time of RoBERTa to 1 day, Liu et"
2020.findings-emnlp.232,Q19-1026,0,0.0321512,"Missing"
2020.findings-emnlp.232,N19-4009,0,0.0267825,"an see that for BERT-Base, the O(N 2 ) component accounts for 3.66 GB, and the O(N ) component accounts for 4.83 GB. When the sequence length N increases to 1024 (i.e., b = 4), the O(N 2 ) component increases to 7.32 GB, while the O(N ) part is unchanged. 2.3 Techniques for Reducing Traing Memory Observing that activation memory is the training bottleneck, we discuss common memory reduction techniques below. Low Precision (Micikevicius et al., 2017) Low precision is to use half-precision/mixed-precision for training neural networks. This technique has been widely used in Transformer training (Ott et al., 2019; Liu et al., 2019). In this work, we already Microbatching (Huang et al., 2018) Microbatching is to split a batch into small microbatches (which can be fit into memory), and then run forward and backward passes on them separately with gradients for each micro-batch accumulated. Because it runs forward/backward pass multiple times for a single batch, it trades off time for memory. Gradient Checkpointing (Chen et al., 2016) Gradient checkpointing saves memory by only caching activations of a subset of layers. The un-cached activations will be recomputed during backpropagation from the latest ch"
2020.findings-emnlp.232,N18-1202,0,0.120894,"Missing"
2020.findings-emnlp.232,P18-2124,0,0.0588447,"Missing"
2020.findings-emnlp.232,P19-1612,0,0.0297355,"Missing"
2020.findings-emnlp.232,2021.ccl-1.108,0,0.0678144,"Missing"
2020.findings-emnlp.232,A94-1016,0,0.159416,"Missing"
2020.findings-emnlp.232,P19-1279,0,0.0235209,", 2019), XLNet (Yang et al., 2019), RoBERTa (Liu et al., 2019) and ALBERT (Lan et al., 2019), has drastically reshaped the landscape of the natural language processing research. These methods first pre-train a deep model with language model objectives using a large corpus and then fine-tune the model using in-domain supervised data for target applications. Despite its conceptual simplicity, this paradigm has re-established the new state-of-theart baselines across various tasks, such as question answering (Devlin et al., 2019), coreference resolution (Joshi et al., 2019b), relation extraction (Soares et al., 2019) and text retrieval (Lee et al., 2019; Nogueira and Cho, 2019), to name a few. ⇤ This work was partially done when the first author was an intern at Facebook AI. Code is available at https:// github.com/xptree/BlockBERT Building such models in practice, however, is an extremely resource-intensive process. For instance, the training of BERT-family models is notoriously expensive. Devlin et al. (2019) report that it takes four days to pre-train BERT-Base/BERTLarge on 4/16 Cloud TPUs. In order to reduce the pre-training time of RoBERTa to 1 day, Liu et al. (2019) use 1,024 V100 GPUs. One crucial"
2020.findings-emnlp.232,P19-1032,0,0.0408141,"o masked language model pre-training (Devlin et al., 2019; Radford et al., 2019; Yang et al., 2019; Liu et al., 2019; Lan et al., 2019), several approaches have been proposed to simplify the model and its training process. We summarize these attempts as follows: Attention layer simplification There are currently two lines of research trying to simplify the multi-head attention layers. The first one focuses on attention matrix sparsification. Notable examples include Star Transformer (Guo et al., 2019), Sparse Transformer (Child et al., 2019), Adaptive Sparse Transformer (Correia et al., 2019; Sukhbaatar et al., 2019), Log-Sparse Transformer (Li et al., 2019) , Reformer (Kitaev et al., 2020) and Longformer (Beltagy et al., 2020). However, due to the insufficient support for sparse tensors from the current deep learning platforms, some 2561 SearchQA EM F1 TriviaQA EM F1 NewsQA EM F1 NaturalQA EM F1 HotpotQA EM F1 N Model 512 Google BERT RoBERTa-2seq RoBERTa-1seq SparseB ERT BlockB ERT n=2 BlockB ERT n=3 74.94 76.12 77.09 73.36 76.68 75.54 80.37 81.74 82.62 79.01 82.33 81.07 70.18 71.92 73.65 68.71 72.36 72.05 75.35 76.79 78.22 73.15 77.53 76.74 51.27 52.45 56.13 51.18 54.66 53.82 66.25 66.73 70.64 65.47 69."
2020.findings-emnlp.232,W17-2623,0,0.0306333,"ce Prediction) task, and an input sequence is up to N tokens until it reaches a document boundary. A summary of the pre-training performance comparison between BlockB ERT and RoBERTa-1seq is shown in Table 2. Besides memory saving, we also achieve a significant speedup. For example, when N = 1024, BlockB ERT (n = 2) reduces the training time from RoBERTa’s 9.7 days to 7.5 days. 4.2 Fine-tuning Tasks We evaluate BlockB ERT on several question answering tasks, including SQuAD 1.1/2.0 (Rajpurkar et al., 2018) and five other tasks from the MrQA shared task7 — HotpotQA (Yang et al., 2018), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019). Since MrQA does not have an official test set, we follow Joshi et al. (2019a) to split the devel6 We adopt Sparse Transformer implemented by Fairseq, which first computes the N ⇥ N attention matrix, and then masks it to be a sparse one. This implementation cannot avoid the O(N 2 ) attention computation, and thus has a similar training time/memory cost to RoBERTa. 7 mrqa.github.io 2559 N Model Training Time (day) Memory (per GPU, GB) Heads Config. Valid. ppl 512 RoBERTa-1seq BlockB ERT n=2 Bl"
2020.findings-emnlp.232,P19-1580,0,0.0579792,"Missing"
2020.findings-emnlp.232,D18-1259,0,0.0252582,"we drop the NSP (Next Sentence Prediction) task, and an input sequence is up to N tokens until it reaches a document boundary. A summary of the pre-training performance comparison between BlockB ERT and RoBERTa-1seq is shown in Table 2. Besides memory saving, we also achieve a significant speedup. For example, when N = 1024, BlockB ERT (n = 2) reduces the training time from RoBERTa’s 9.7 days to 7.5 days. 4.2 Fine-tuning Tasks We evaluate BlockB ERT on several question answering tasks, including SQuAD 1.1/2.0 (Rajpurkar et al., 2018) and five other tasks from the MrQA shared task7 — HotpotQA (Yang et al., 2018), NewsQA (Trischler et al., 2017), SearchQA (Dunn et al., 2017), TriviaQA (Joshi et al., 2017) and NaturalQA (Kwiatkowski et al., 2019). Since MrQA does not have an official test set, we follow Joshi et al. (2019a) to split the devel6 We adopt Sparse Transformer implemented by Fairseq, which first computes the N ⇥ N attention matrix, and then masks it to be a sparse one. This implementation cannot avoid the O(N 2 ) attention computation, and thus has a similar training time/memory cost to RoBERTa. 7 mrqa.github.io 2559 N Model Training Time (day) Memory (per GPU, GB) Heads Config. Valid. ppl 5"
2021.acl-long.517,D19-1539,0,0.0185336,"SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al., 2017). 5 Interestingly, RoBERTa appears to perform better compared to BERT and ELECTRA. Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020). 6622 Evaluation set → Training set ↓ DRoBERTa EM DBERT F1 EM DBiDAF F1 EM F1 Finetuned model: BERTlarge 6.0 5.40.3 13.5 12.20.1 8.1 7.00.6 14.2 13.60.8 12.6 11.00.9 21.4 19.40.7 BERTfooled (11.3k) BERTrandom (11.3k) SDC (11.3k) 11.02.6 12.41.6 9.10.7 21.03.0 22.12.2 20.40.7 14.63.7 16.43.0 14.01.0 24.74.0 26.22.7 24.60.7 25.16.5 29.63.7 30.11.2 39.16.9 43.74.0 43.81.2 Orig + BERTfooled (34.4k) Orig + BERTrandom (34.4k) Orig + SDC (34.4k) 15.20.8 16.90.5 9.40.6 25.10.6 23.90.5 20.20.5 20.40.4 20.50.6 15.31.0 31.00.4 31.20.9 25.81.1 32.40.6 34.10.4 32."
2021.acl-long.517,2020.tacl-1.43,0,0.310854,"Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6618–6633 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Platform shown to workers generating questions in the ADC setting. generate five questions for each context that they see. Workers are shown similar instructions (with minimal changes), and paid the same base amount. We fine-tune three models (BERT, RoBERTa, and ELECTRA) on resulting datasets and evaluate them on held-out test sets, adversarial test sets from prior work (Bartolo et al., 2020), and 12 MRQA (Fisch et al., 2019) datasets. For all models, we find that while fine-tuning on adversarial data usually leads to better performance on (previously collected) adversarial data, it typically leads to worse performance on a large, diverse collection of out-of-domain datasets (compared to fine-tuning on standard data). We observe a similar pattern when augmenting the existing dataset with the adversarial data. Results on an extensive collection of out-of-domain evaluation sets suggest that ADC training data does not offer clear benefits vis-`a-vis robustness under distribution shif"
2021.acl-long.517,D19-1606,0,0.024551,"Missing"
2021.acl-long.517,N19-1423,0,0.161318,"how much can differences between ADC and SDC be attributed to the way workers behave when attempting to fool models, regardless of whether they are successful? and (iii) what is the impact of training models on adversarial data only, versus using it as a data augmentation strategy? In this paper, we conduct a large-scale randomized controlled study to address these questions. Focusing our study on span-based question answering and a variant of the Natural Questions dataset (NQ; Lee et al., 2019; Karpukhin et al., 2020), we work with two popular pretrained transformer architectures—BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020)— each fine-tuned on 23.1k examples. To eliminate confounding factors when assessing the impact of ADC, we randomly assign the crowdworkers tasked with generating questions to one of three groups: (i) with an incentive to fool the BERT model; (ii) with an incentive to fool the ELECTRA model; and (iii) a standard, non-adversarial setting (no model in the loop). The pool of contexts is the same for each group and each worker is asked to 6618 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joi"
2021.acl-long.517,D19-1461,0,0.252934,"publicly available at https://github.com/facebookresearch/aqa-study. investigated adversarial data collection (ADC), a scheme in which a worker interacts with a model (in real time), attempting to produce examples that elicit incorrect predictions (e.g., Dua et al., 2019; Nie et al., 2020). The hope is that by identifying parts of the input domain where the model fails one might make the model more robust. Researchers have shown that models trained on ADC perform better on such adversarially collected data and that with successive rounds of ADC, crowdworkers are less able to fool the models (Dinan et al., 2019). While adversarial data may indeed provide more challenging benchmarks, the process and its actual benefits vis-a-vis tasks of interest remain poorly understood, raising several key questions: (i) do the resulting models typically generalize better out of distribution compared to standard data collection (SDC)?; (ii) how much can differences between ADC and SDC be attributed to the way workers behave when attempting to fool models, regardless of whether they are successful? and (iii) what is the impact of training models on adversarial data only, versus using it as a data augmentation strateg"
2021.acl-long.517,W17-5401,0,0.0676695,"Missing"
2021.acl-long.517,D19-5801,0,0.0153683,"utational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6618–6633 August 1–6, 2021. ©2021 Association for Computational Linguistics Figure 1: Platform shown to workers generating questions in the ADC setting. generate five questions for each context that they see. Workers are shown similar instructions (with minimal changes), and paid the same base amount. We fine-tune three models (BERT, RoBERTa, and ELECTRA) on resulting datasets and evaluate them on held-out test sets, adversarial test sets from prior work (Bartolo et al., 2020), and 12 MRQA (Fisch et al., 2019) datasets. For all models, we find that while fine-tuning on adversarial data usually leads to better performance on (previously collected) adversarial data, it typically leads to worse performance on a large, diverse collection of out-of-domain datasets (compared to fine-tuning on standard data). We observe a similar pattern when augmenting the existing dataset with the adversarial data. Results on an extensive collection of out-of-domain evaluation sets suggest that ADC training data does not offer clear benefits vis-`a-vis robustness under distribution shift. To study the differences betwee"
2021.acl-long.517,N18-2017,0,0.0527588,"Missing"
2021.acl-long.517,2020.acl-main.244,0,0.0406829,"Missing"
2021.acl-long.517,P17-1147,0,0.0277592,"e better to out-ofdomain adversarial test sets than models fine-tuned on SDC data, confirming the findings by Dinan et al. (2019). Out-of-domain generalization to MRQA We further evaluate these models on 12 out-of-domain datasets used in the 2019 MRQA shared task4 (Table 4 and Appendix Table 7).5 Notably, for BERT, fine-tuning on SDC data leads to significantly better performance (as compared to fine-tuning on 4 The MRQA 2019 shared task includes HotpotQA (Yang et al., 2018a), Natural Questions (Kwiatkowski et al., 2019), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al., 2017). 5 Interestingly, RoBERTa appears to perform better compared to BERT and ELECTRA. Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020). 6622 Evaluation set → Traini"
2021.acl-long.517,2020.emnlp-main.550,1,0.904447,"typically generalize better out of distribution compared to standard data collection (SDC)?; (ii) how much can differences between ADC and SDC be attributed to the way workers behave when attempting to fool models, regardless of whether they are successful? and (iii) what is the impact of training models on adversarial data only, versus using it as a data augmentation strategy? In this paper, we conduct a large-scale randomized controlled study to address these questions. Focusing our study on span-based question answering and a variant of the Natural Questions dataset (NQ; Lee et al., 2019; Karpukhin et al., 2020), we work with two popular pretrained transformer architectures—BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020)— each fine-tuned on 23.1k examples. To eliminate confounding factors when assessing the impact of ADC, we randomly assign the crowdworkers tasked with generating questions to one of three groups: (i) with an incentive to fool the BERT model; (ii) with an incentive to fool the ELECTRA model; and (iii) a standard, non-adversarial setting (no model in the loop). The pool of contexts is the same for each group and each worker is asked to 6618 Proceedings of the 59th"
2021.acl-long.517,2021.ccl-1.108,0,0.0448477,"Missing"
2021.acl-long.517,Q19-1026,0,0.0981119,"We provide all workers with the same base pay and for those assigned to ADC, pay out an additional bonus for each question that fools the QA model. Finally, we field a different set of workers to validate the generated examples. Context passages For context passages, we use the first 100 words of Wikipedia articles. Truncating the articles keeps the task of generating questions from growing unwieldy. These segments typically contain an overview, providing ample material for factoid questions. We restrict the pool of candidate contexts by leveraging a variant of the Natural Questions dataset (Kwiatkowski et al., 2019; Lee et al., 2019). We first keep only a subset of 23.1k question/answer pairs for which the context passages are the first 100 words of Wikipedia articles2 . From these passages, we sample 10k at random for our study. Models in the loop We use BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020) models as our adversarial models in the loop, using the implementations provided by Wolf et al. (2020). We fine-tune these models for span-based question-answering, using the 23.1k training examples (subsampled previously) for 20 epochs, with early-stopping based on word-overlap F13"
2021.acl-long.517,D17-1082,0,0.0295301,"ation to MRQA We further evaluate these models on 12 out-of-domain datasets used in the 2019 MRQA shared task4 (Table 4 and Appendix Table 7).5 Notably, for BERT, fine-tuning on SDC data leads to significantly better performance (as compared to fine-tuning on 4 The MRQA 2019 shared task includes HotpotQA (Yang et al., 2018a), Natural Questions (Kwiatkowski et al., 2019), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al., 2017). 5 Interestingly, RoBERTa appears to perform better compared to BERT and ELECTRA. Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020). 6622 Evaluation set → Training set ↓ DRoBERTa EM DBERT F1 EM DBiDAF F1 EM F1 Finetuned model: BERTlarge 6.0 5.40.3 13.5 12.20.1 8.1 7.00.6 14.2 13.60.8 12.6 11.00.9 21.4 19.40.7 BE"
2021.acl-long.517,P19-1612,0,0.0933511,"e resulting models typically generalize better out of distribution compared to standard data collection (SDC)?; (ii) how much can differences between ADC and SDC be attributed to the way workers behave when attempting to fool models, regardless of whether they are successful? and (iii) what is the impact of training models on adversarial data only, versus using it as a data augmentation strategy? In this paper, we conduct a large-scale randomized controlled study to address these questions. Focusing our study on span-based question answering and a variant of the Natural Questions dataset (NQ; Lee et al., 2019; Karpukhin et al., 2020), we work with two popular pretrained transformer architectures—BERTlarge (Devlin et al., 2019) and ELECTRAlarge (Clark et al., 2020)— each fine-tuned on 23.1k examples. To eliminate confounding factors when assessing the impact of ADC, we randomly assign the crowdworkers tasked with generating questions to one of three groups: (i) with an incentive to fool the BERT model; (ii) with an incentive to fool the ELECTRA model; and (iii) a standard, non-adversarial setting (no model in the loop). The pool of contexts is the same for each group and each worker is asked to 661"
2021.acl-long.517,K17-1034,0,0.0252732,"). Out-of-domain generalization to MRQA We further evaluate these models on 12 out-of-domain datasets used in the 2019 MRQA shared task4 (Table 4 and Appendix Table 7).5 Notably, for BERT, fine-tuning on SDC data leads to significantly better performance (as compared to fine-tuning on 4 The MRQA 2019 shared task includes HotpotQA (Yang et al., 2018a), Natural Questions (Kwiatkowski et al., 2019), SearchQA (Dunn et al., 2017), SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), BioASQ (Tsatsaronis et al., 2015), DROP (Dua et al., 2019), DuoRC (Saha et al., 2018), RelationExtraction (Levy et al., 2017), RACE (Lai et al., 2017), and TextbookQA (Kembhavi et al., 2017). 5 Interestingly, RoBERTa appears to perform better compared to BERT and ELECTRA. Prior works have hypothesized that the bigger size and increased diversity of the pretraining corpus of RoBERTa (compared to those of BERT and ELECTRA) might somehow be responsible for RoBERTa’s better out-of-domain generalization, (Baevski et al., 2019; Hendrycks et al., 2020; Tu et al., 2020). 6622 Evaluation set → Training set ↓ DRoBERTa EM DBERT F1 EM DBiDAF F1 EM F1 Finetuned model: BERTlarge 6.0 5.40.3 13.5 12.20.1 8.1 7.00.6 14.2 13.60.8 12."
2021.acl-long.517,2020.acl-main.441,1,0.892934,"more widely. Despite performing well on independent and identically distributed (i.i.d.) data, these models are liable under plausible domain shifts. With the goal of providing more challenging benchmarks that require this stronger form of generalization, an emerging line of research has 1 Data collected during this study is publicly available at https://github.com/facebookresearch/aqa-study. investigated adversarial data collection (ADC), a scheme in which a worker interacts with a model (in real time), attempting to produce examples that elicit incorrect predictions (e.g., Dua et al., 2019; Nie et al., 2020). The hope is that by identifying parts of the input domain where the model fails one might make the model more robust. Researchers have shown that models trained on ADC perform better on such adversarially collected data and that with successive rounds of ADC, crowdworkers are less able to fool the models (Dinan et al., 2019). While adversarial data may indeed provide more challenging benchmarks, the process and its actual benefits vis-a-vis tasks of interest remain poorly understood, raising several key questions: (i) do the resulting models typically generalize better out of distribution co"
2021.acl-long.517,P16-1144,0,0.0368826,"Missing"
2021.acl-long.517,P19-1472,0,0.0373793,"Missing"
2021.acl-long.529,P17-1171,0,0.409287,"et al., 2020). In both datasets, claims can be verified ∗ Work done while interning with Facebook AI Research. given a single associated table. While highly useful for the development of models, this closed setting is not reflective of real-world fact checking tasks where it is usually not known which table to consult for evidence. Realistic systems must first retrieve evidence from a large data source. That is, realistic systems must operate in an open setting. Here, we investigate fact verification over tables in the open setting. We take inspiration from similar work on unstructured data (Chen et al., 2017; Nie et al., 2019; Karpukhin et al., 2020; Lewis et al., 2020), proposing a two-step model which combines ad-hoc retrieval with a neural reader. Drawing on preliminary work in open question answering over tables (Sun et al., 2016), we perform retrieval based on simple heuristic modeling of individual table cells. We combine this retriever with a RoBERTa-based (Liu et al., 2019) joint reranking-and-verification model, performing 6787 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing"
2021.acl-long.529,2020.emnlp-main.19,0,0.0384075,"se retrieval models with dot-product indexing (Johnson et al., 2017), and increasingly advanced pretrained transformer-models for reading. The development of similarly fast, reliable and learnable indexing techniques for tables as well as text is an important direction for future work. Concurrently with our work, Chen et al. (2020a) have introduced a BERT-based model to perform question answering over open collections of data including tables. Like ours, their model consists of separate retriever- and reader-steps. Their bestperforming reader employs a long-range sparse attention transformer (Ainslie et al., 2020) to jointly summarize all retrieved data. As in our case, their model demonstrates significant improvements from using multiple retrieved tables. 7 Conclusion We have introduced a novel model for fact verification over large collections of tables, along with two strategies for exploiting closed-domain datasets to increase performance. Our approach performs on par with the current closed-domain state of the art, with larger gains the more tables we include. When using an oracle to retrieve a reference table, our approach also represents a new closed-domain state of the art. Finally, we have mad"
2021.acl-long.529,2020.acl-main.398,0,0.061532,"Missing"
2021.acl-long.89,2021.emnlp-main.468,0,0.0319531,"are two representative examples. One further approach is ColBERT-QA (Khattab et al., 2020), which additionally uses a data augmentation strategy closely related to our own approach described in Appendix D. Retrieval does not strictly have to be performed with a model which contains an explicit memory. Large-scale pre-trained models have been shown to store knowledge directly into their parameters. A model which demonstrates this ability is T5 (Raffel et al., 2020) – which we used as a baseline in § 4. Regarding the multi-task aspect of our approach, a related strategy has been demonstrated by Aghajanyan et al. (2021). In this recent work, the authors multi-task train a pre-trained model on around 50 datasets, before performing the final fine-tuning. While they do not focus on retrieval, their results are consistent with ours and show that multi-task training leads to improved performance and increased sample efficiency. On the topic of question answering, Lewis et al. (2021) show in a recent notable paper that, for several popular QA datasets, a portion of questions in the test set has near-duplicates in the training sets, and the same holds true for an even larger set of answers. To our knowledge, simila"
2021.acl-long.89,D11-1072,0,0.130363,"Missing"
2021.acl-long.89,K17-1034,0,0.060288,"AGO2 dataset, due to the nature of the task, pageand passage-level results coincide. ing that the result achieved by Karpukhin et al. (2020) for open-domain question answering also holds for other knowledge-intensive tasks. The results reveal a strong performance for the multi-task model, confirming the hypothesis that a single model can be trained to perform well on a wide variety of retrieval tasks. With the exception of one dataset, the multi-task model achieves the best retrieval performance or is within a few points of the top score. We note that the one exception, the Zero-shot RE task (Levy et al., 2017), is a trivial task in which the query will always contain the title of the page to be retrieved. Indeed, the model specific to this task achieves a near-perfect score (see full results in Appendix A). Another task which stands out for being markedly different in formulation is AIDAYAGO 2 (Hoffart et al., 2011). As shown in Table 3, models that were not trained on AIDA-YAGO 2 do very poorly on it. Entity linking is normally better performed by models which are explicitly designed for it (De Cao et al., 2020). We nevertheless include it to showcase the ability of neural retrievers to adapt to a"
2021.acl-long.89,2020.acl-main.703,1,0.838453,"l-world knowledge in their parameters and thus may skip retrieval (Petroni et al., 2019), they still have limited capacity and suffer from a lack of explainability. 1 The standard retrieval component in many systems (e.g., Thorne et al., 2018; Wang et al., 2018; Chen et al., 2017) has long relied on term-matching methods, such as tf-idf or BM25 (Robertson and Zaragoza, 2009). These methods rely on efficient algorithms and usually perform reasonably well regardless of the problem. In contrast, recent neural retrieval models, such as ICT (Lee et al., 2019), DPR (Karpukhin et al., 2020) and RAG (Lewis et al., 2020b) achieve better results by learning directly from task-specific training data and going beyond simple keyword matching. While task specialisation results in improved task performance, researchers have observed that a retriever trained for one specific domain will typically achieve low out-of-domain performance, and even lower performance on entirely different tasks (Petroni et al., 2020). This has two implications. First, unlike tfidf or BM25, neural retrieval models are unsuitable for low data regimes such as few- and zero-shot settings. Second, task-specific retrievers complicate practical"
2021.acl-long.89,P17-1147,0,0.107479,"Missing"
2021.acl-long.89,2020.emnlp-main.550,1,0.10568,"ave been shown to incorporate real-world knowledge in their parameters and thus may skip retrieval (Petroni et al., 2019), they still have limited capacity and suffer from a lack of explainability. 1 The standard retrieval component in many systems (e.g., Thorne et al., 2018; Wang et al., 2018; Chen et al., 2017) has long relied on term-matching methods, such as tf-idf or BM25 (Robertson and Zaragoza, 2009). These methods rely on efficient algorithms and usually perform reasonably well regardless of the problem. In contrast, recent neural retrieval models, such as ICT (Lee et al., 2019), DPR (Karpukhin et al., 2020) and RAG (Lewis et al., 2020b) achieve better results by learning directly from task-specific training data and going beyond simple keyword matching. While task specialisation results in improved task performance, researchers have observed that a retriever trained for one specific domain will typically achieve low out-of-domain performance, and even lower performance on entirely different tasks (Petroni et al., 2020). This has two implications. First, unlike tfidf or BM25, neural retrieval models are unsuitable for low data regimes such as few- and zero-shot settings. Second, task-specific ret"
2021.acl-long.89,Q19-1026,0,0.078483,"Missing"
2021.acl-long.89,P19-1612,0,0.241772,"-trained neural models have been shown to incorporate real-world knowledge in their parameters and thus may skip retrieval (Petroni et al., 2019), they still have limited capacity and suffer from a lack of explainability. 1 The standard retrieval component in many systems (e.g., Thorne et al., 2018; Wang et al., 2018; Chen et al., 2017) has long relied on term-matching methods, such as tf-idf or BM25 (Robertson and Zaragoza, 2009). These methods rely on efficient algorithms and usually perform reasonably well regardless of the problem. In contrast, recent neural retrieval models, such as ICT (Lee et al., 2019), DPR (Karpukhin et al., 2020) and RAG (Lewis et al., 2020b) achieve better results by learning directly from task-specific training data and going beyond simple keyword matching. While task specialisation results in improved task performance, researchers have observed that a retriever trained for one specific domain will typically achieve low out-of-domain performance, and even lower performance on entirely different tasks (Petroni et al., 2020). This has two implications. First, unlike tfidf or BM25, neural retrieval models are unsuitable for low data regimes such as few- and zero-shot setti"
2021.acl-long.89,2021.eacl-main.86,0,0.0146227,"irectly into their parameters. A model which demonstrates this ability is T5 (Raffel et al., 2020) – which we used as a baseline in § 4. Regarding the multi-task aspect of our approach, a related strategy has been demonstrated by Aghajanyan et al. (2021). In this recent work, the authors multi-task train a pre-trained model on around 50 datasets, before performing the final fine-tuning. While they do not focus on retrieval, their results are consistent with ours and show that multi-task training leads to improved performance and increased sample efficiency. On the topic of question answering, Lewis et al. (2021) show in a recent notable paper that, for several popular QA datasets, a portion of questions in the test set has near-duplicates in the training sets, and the same holds true for an even larger set of answers. To our knowledge, similar analyses have yet to be performed on the other KILT tasks. Finally two entity linkers, GENRE (De Cao et al., 2020) and BLINK (Wu et al., 2020), are worth mentioning. Being trained specifically for entity linking, these models will generally outperform retrieval-based approaches on that task. While they are not comparable to retrieval models and will not general"
2021.acl-long.89,P19-1441,0,0.115595,"d can also undergo further in-domain training for even higher performance. 2 passage is relevant to a given query is determined by the distance of their vectors (Deerwester et al., 1990). Although dense representations do not encode tokens explicitly and can potentially map paraphrases of completely different tokens to close vectors, performance of early dense retrieval methods was often inferior to term-matching approaches, except when large labelled data is available (Yih et al., 2011; Gao et al., 2011; Huang et al., 2013). Thanks to success of large pre-trained models (Devlin et al., 2019; Liu et al., 2019b), however, recent dense retrieval methods have shown to outperform the sparse counterparts, when fine-tuned on a small set of in-domain labelled data (Karpukhin et al., 2020; Lewis et al., 2020b; Xiong et al., 2020). Efficient index and search of dense vectors are made possible by maximum inner product search (MIPS) algorithms (e.g., Shrivastava and Li, 2014; Guo et al., 2016), as well as tools like FAISS (Johnson et al., 2019). Our work is built upon the Dense Passage Retriever (DPR) architecture of Karpukhin et al. (2020), which was initially proposed for the task of open-domain question a"
2021.acl-long.89,2021.ccl-1.108,0,0.078103,"Missing"
2021.acl-long.89,D19-1250,1,0.911005,"Missing"
2021.acl-long.89,W11-0329,1,0.687298,"able BERT checkpoint which, as we will show, can be used by NLP practitioners as a strong out-of-the-box retrieval system, and can also undergo further in-domain training for even higher performance. 2 passage is relevant to a given query is determined by the distance of their vectors (Deerwester et al., 1990). Although dense representations do not encode tokens explicitly and can potentially map paraphrases of completely different tokens to close vectors, performance of early dense retrieval methods was often inferior to term-matching approaches, except when large labelled data is available (Yih et al., 2011; Gao et al., 2011; Huang et al., 2013). Thanks to success of large pre-trained models (Devlin et al., 2019; Liu et al., 2019b), however, recent dense retrieval methods have shown to outperform the sparse counterparts, when fine-tuned on a small set of in-domain labelled data (Karpukhin et al., 2020; Lewis et al., 2020b; Xiong et al., 2020). Efficient index and search of dense vectors are made possible by maximum inner product search (MIPS) algorithms (e.g., Shrivastava and Li, 2014; Guo et al., 2016), as well as tools like FAISS (Johnson et al., 2019). Our work is built upon the Dense Passage"
2021.acl-long.89,N18-1074,0,0.0138947,"nvolve an efficient retrieval component that, given an input query, selects a limited subset of relevant information from a large knowledge source. Sophisticated downstream models then consider the input only in the context of the retrieved information, and perform the final task.1 ∗ Equal Contribution. While large pre-trained neural models have been shown to incorporate real-world knowledge in their parameters and thus may skip retrieval (Petroni et al., 2019), they still have limited capacity and suffer from a lack of explainability. 1 The standard retrieval component in many systems (e.g., Thorne et al., 2018; Wang et al., 2018; Chen et al., 2017) has long relied on term-matching methods, such as tf-idf or BM25 (Robertson and Zaragoza, 2009). These methods rely on efficient algorithms and usually perform reasonably well regardless of the problem. In contrast, recent neural retrieval models, such as ICT (Lee et al., 2019), DPR (Karpukhin et al., 2020) and RAG (Lewis et al., 2020b) achieve better results by learning directly from task-specific training data and going beyond simple keyword matching. While task specialisation results in improved task performance, researchers have observed that a retri"
2021.acl-long.89,D19-1599,0,0.140533,"Missing"
2021.acl-long.89,2020.emnlp-main.519,1,0.797144,"tuning. While they do not focus on retrieval, their results are consistent with ours and show that multi-task training leads to improved performance and increased sample efficiency. On the topic of question answering, Lewis et al. (2021) show in a recent notable paper that, for several popular QA datasets, a portion of questions in the test set has near-duplicates in the training sets, and the same holds true for an even larger set of answers. To our knowledge, similar analyses have yet to be performed on the other KILT tasks. Finally two entity linkers, GENRE (De Cao et al., 2020) and BLINK (Wu et al., 2020), are worth mentioning. Being trained specifically for entity linking, these models will generally outperform retrieval-based approaches on that task. While they are not comparable to retrieval models and will not generally be applicable to information retrieval tasks, we cite them here to provide readers with a fuller context of the existing literature on related tasks. 6 Conclusions We have conducted a large-scale experimental study on knowledge-intensive tasks, and how re1105 trieval models that tackle them seek the required information from knowledge bases like Wikipedia. The study started"
2021.emnlp-main.573,D11-1072,0,0.0914873,"Missing"
2021.emnlp-main.573,2020.tacl-1.5,0,0.0216494,"tion between mask frequency and token frequency for masking policies learned from TQA, Self-supervised Pre-training. Pre-trained lanalong with random masking and SSM for reference. guage models has shown its capability on a wide Mask frequency is computed as the number of oc- variety of NLP tasks. Current self-supervised obcurrences that a token was masked divided by the jectives are mostly heuristic, including masked lannumber of all masked tokens. For random mask- guage modeling (Devlin et al., 2019), span bounding, the datapoints approximate a Zipfian distribu- ary representation learning (Joshi et al., 2020), cortion (Zipf, 1999), with some noise due to random rupted sentence reconstruction (Lewis et al., 2020), sampling of words. Secondly, for SSM, most dat- etc. Raffel et al. (2020) systematically studied the apoints fall on a curve above the random masking self-supervised objectives used in previous literaline, while a small portion of tokens are less likely ture. Related to our goal of exploring pre-training to be masked, formulating line segments in the bot- objectives, ELECTRA (Clark et al., 2020) propose tom area. These observations indicate that SSM a replaced token prediction task which"
2021.emnlp-main.573,P17-1147,0,0.0178085,", whereby learning to unmask spans in context is analogous to memorizing facts about the span. In the absence of such understanding, heuristic policies may be sub-optimal. This motivates us to explore whether automating the discovery of optimal masking policies is possible. We design methods to learn a masking policy with supervised learning (§4.2) or meta-learning (§4.3), and compare downstream task performance using the same protocol in our previous analysis. Notably, we observe that masking policies learned with supervised learning and meta-learning outperforms the SSM policy for TriviaQA (Joshi et al., 2017), and these policies learned from TriviaQA also help improve performance on Web Questions (Berant et al., 2013). We also discuss the pros and cons of learned masking policies, such as downstream task learning efficiency, risks of over-fitting and learning instability. Finally, in hopes to better understand the heuristic and learned masking policies, we provide quantitative analysis on the masks produced by these policies. We visualize the distribution of part-ofspeech tags among masked tokens, and their relation to token frequency in the corpus (§5.3). We find that the masking policies learned"
2021.emnlp-main.573,2020.emnlp-main.493,0,0.0259897,"7 propose to mask n-grams according to Pointwise Mutual Information (PMI). These works typically consider the efficiency of an objective when pretraining from scratch and without preconceived focus on a given problem; while we focus on encoding knowledge or adapting the model during intermediate pre-training with a given task in mind. Domain/Task-specific Pre-training. Gururangan et al. (2020) experiment on four domains (biomedical, computer science, news, reviews) and eight different datasets, where they discover that pre-training with in-domain corpus leads to better downstream performance. Kang et al. (2020) propose to learn a mask generator via reinforcement learning. Closely related to us, Gu et al. (2020) propose task-guided pre-training by learning to predict importance score for each token in pre-train corpus. Vu et al. (2020); Pruksachatkun et al. (2020) studies knowledge transfer from intermediate-task fine-tuning, while we focus on a different problem setting of intermediate pre-training with generic corpus (e.g., Wikipedia). We believe both settings have practical utility in real-world applications. 7 Conclusion Acknowledgments We would like to thank anonymous reviewers for their constru"
2021.emnlp-main.573,K17-1034,0,0.0627149,"Missing"
2021.emnlp-main.573,2020.acl-main.703,0,0.479697,"a language model f (.; θ), parameterized by θ. Formally, given a sequence of tokens x = [x1 , x2 , ..., xm ], g(x; φ) generates a sequence of binary decisions d = [d1 , d2 , ..., dm ], where di = 1 indicates the token xi will be masked. The source sequence for pre-training, x(src) , is formulated by replacing the selected tokens with a special &lt;mask> (src) (src) (src) token, i.e., x(src) = [x1 , x2 , ..., xm ], where (src) (src) xi = xi if di = 0 and xi = &lt;mask> if di = 1. We denote this operation as x(src) = x ⊕ d. The target sequence x(tar) can be either the full original sequence x (BART, Lewis et al. 2020), or the sequence of masked tokens (T5, Raffel et al. 2020). 3 Analysis Setup In this section we introduce the analysis pipeline (§3.1) and downstream datasets we use (§3.2). We defer the details of learned masking policies to §4. 3.1 Experiment Procedure Our goal is to analyze the influence in downstream task performance brought by different masking policies g(.; φ) during intermediate pre-training. Towards this goal, we ensure that the only variable is the masking policy, while all other aspects are controlled, so that the downstream performance reveal the influence we aim to study. We first"
2021.emnlp-main.573,D19-5808,0,0.0472364,"Missing"
2021.emnlp-main.573,2021.ccl-1.108,0,0.0653848,"Missing"
2021.emnlp-main.573,N19-4009,0,0.0645197,"Missing"
2021.emnlp-main.573,D19-1250,0,0.0567999,"Missing"
2021.emnlp-main.573,2020.acl-main.467,0,0.0373087,"Missing"
2021.emnlp-main.573,2020.emnlp-main.437,0,0.0229287,"Missing"
2021.emnlp-main.573,D19-1608,0,0.0402163,"Missing"
2021.emnlp-main.573,D19-1629,0,0.0561887,"Missing"
2021.naacl-main.100,D13-1160,0,0.0513368,"ring inference. 5 Experiments Datasets We use four benchmark open-domain QA datasets following Lee et al. (2019): Natural Questions (NQ) contains real user questions asked on Google searches; we consider questions with short answers up to 5 tokens. T RIVIAQA (Joshi et al., 2017) consists of questions collected from trivia and quiz-league webTraining We obtain top K predictions (pj , sj ) sites; we take questions in an unfiltered setting and of model M for each question qi in its training discard the provided web snippets. set, which we divide into positives, where sj is WebQuestions (W EB Q) (Berant et al., 2013) is a exactly the groundtruth answer, and remaining neg- collection of questions extracted from the Google atives. We train R using mini-batch gradient de- Suggest API, with answers being Freebase entities. scent, where in each iteration, for question q, we CuratedTREC (Baudiš and Šediv`y, 2015) coninclude 1 randomly chosen positive and M − 1 tains curated questions from TREC QA track. 1282 Model NQ T RIVIAQA W EB Q TREC BM25+BERT (Lee et al., 2019) ORQA (Lee et al., 2019) HardEM (Min et al., 2019a) GraphRetriever (Min et al., 2019b) PathRetriever (Asai et al., 2020) REALM (Guu et al., 2020) R"
2021.naacl-main.100,P17-1171,0,0.0191057,"four QA datasets. Open-domain Question Answering (Voorhees et al., Earlier work (Wang et al., 2018c,b) on open1999) (QA) involves answering questions by ex- domain QA have recognized the potential of answer tracting correct answer spans from a large corpus re-ranking, which we continue to observe despite of passages, and is typically accomplished by a recent advances using large pre-trained models like light-weight passage retrieval model followed by a BERT (Devlin et al., 2019). Figure 1 shows the heavier Machine Reading Comprehension (MRC) top-3 predictions of a BERT-based SOTA model model (Chen et al., 2017). The span selection com- (Karpukhin et al., 2020) on a question from Natponents of MRC models are trained on distantly su- ural Questions (NQ) (Kwiatkowski et al., 2019), pervised positive examples (containing the answer “Who was the head of the Soviet Union when it colstring) together with heuristically chosen negative lapsed?&quot; While all predictions are very relevant and examples, typically from upstream retrieval models. refer to Soviet Union heads, Mikhail Gorbachev is This training scheme possibly explains empirical correct and the rest are close false positives. Table 1 ∗ presents accura"
2021.naacl-main.100,N19-1423,0,0.180271,"er reranking successful for span-extraction tasks, even over large pretrained models, and improve the state 1 Introduction of the art on four QA datasets. Open-domain Question Answering (Voorhees et al., Earlier work (Wang et al., 2018c,b) on open1999) (QA) involves answering questions by ex- domain QA have recognized the potential of answer tracting correct answer spans from a large corpus re-ranking, which we continue to observe despite of passages, and is typically accomplished by a recent advances using large pre-trained models like light-weight passage retrieval model followed by a BERT (Devlin et al., 2019). Figure 1 shows the heavier Machine Reading Comprehension (MRC) top-3 predictions of a BERT-based SOTA model model (Chen et al., 2017). The span selection com- (Karpukhin et al., 2020) on a question from Natponents of MRC models are trained on distantly su- ural Questions (NQ) (Kwiatkowski et al., 2019), pervised positive examples (containing the answer “Who was the head of the Soviet Union when it colstring) together with heuristically chosen negative lapsed?&quot; While all predictions are very relevant and examples, typically from upstream retrieval models. refer to Soviet Union heads, Mikhail"
2021.naacl-main.100,Q19-1026,0,0.0199922,"QA have recognized the potential of answer tracting correct answer spans from a large corpus re-ranking, which we continue to observe despite of passages, and is typically accomplished by a recent advances using large pre-trained models like light-weight passage retrieval model followed by a BERT (Devlin et al., 2019). Figure 1 shows the heavier Machine Reading Comprehension (MRC) top-3 predictions of a BERT-based SOTA model model (Chen et al., 2017). The span selection com- (Karpukhin et al., 2020) on a question from Natponents of MRC models are trained on distantly su- ural Questions (NQ) (Kwiatkowski et al., 2019), pervised positive examples (containing the answer “Who was the head of the Soviet Union when it colstring) together with heuristically chosen negative lapsed?&quot; While all predictions are very relevant and examples, typically from upstream retrieval models. refer to Soviet Union heads, Mikhail Gorbachev is This training scheme possibly explains empirical correct and the rest are close false positives. Table 1 ∗ presents accuracies obtained by the same model on Work done while at Facebook AI. 1 github.com/facebookresearch/reconsider four QA datasets, if the answer exactly matches 1280 Proceedin"
2021.naacl-main.100,P19-1612,0,0.0991828,"Missing"
2021.naacl-main.100,P18-1178,0,0.0204178,"020). In this paper, we dence MRC model predictions, and thus, learns to significantly improve MRC model performance by eliminate hard false positives. This can be viewed as a coarse-to-fine approach of training span selec- making re-ranking successful using span-focused tors, with the base MRC model trained on heuris- re-ranking of its highly confident predictions. For Open-domain QA, it is crucial to train MRC tically chosen negatives and the re-ranker trained models to distinguish passage-span pairs containon finer, more subtle negatives. This contrasts with multi-task training approaches (Wang et al., 2018c), ing the answer (positives) from those that do not (negatives). Using negatives that appear as close whose re-scoring gains are limited by training on false positives can produce more robust MRC modthe same data, especially when coupled with large els. However, prior work relies on upstream repre-trained models. Our approach also scales to any number of ranked candidates, unlike previous con- trieval models to supply distantly supervised positives (contain answer string) and negatives (Asai catenation based cross-passage re-ranking methods et al., 2020), that are in-turn trained using heuri"
2021.naacl-main.100,2020.emnlp-main.519,0,0.0755875,"Missing"
2021.naacl-main.100,D19-1284,1,0.833917,"Missing"
2021.naacl-main.100,D16-1264,0,0.208398,"Missing"
2021.naacl-main.432,D19-1565,0,0.0218031,"e sources of data, which improves the robustness of the model to different types of misinformation. Therefore, we carry out experiments to evaluate the generalization ability of U NIFIED M2 representation to unseen misinformation (i) tasks/datasets and (ii) events. The first experiment is about fast adaption ability (few-shot training) to handle a new task/dataset, whereas the second experiment is about the model’s ability to perform well on events unseen during training. 4.1 Unseen Task/Dataset Generalizability Dataset We evaluate using the following four unseen datasets: P ROPAGANDA (Da San Martino et al., 2019), which contains 21,230 propaganda and non-propaganda sentences, with the propaganda sentences annotated by fine-grained propaganda technique labels, such as “Name calling” and “Appeal to fear”; P OLITI FACT (Shu et al., 2019), which contains 91 true and 91 fake news articles collected from PolitiFact’s fact-checking platform; B UZZ F EED (Shu et al., 2019), which contains 120 true and 120 fake news headlines collected from BuzzFeed’s fact-checking platform; and C OVID T WITTER (Alam et al., 2020), which contains 504 COVID-19-related tweets. For our experiment, we use two of the annotations: 1"
2021.naacl-main.432,P19-1441,0,0.0225539,"ion datasets we train on with U NIFIED M2. eralizability of our proposed approach to unseen tasks/datasets and events. This is highly applicable to real-world use cases, where obtaining new misinformation labels is costly and systems often wish to take down misinformation in real time. Our experimental results indicate that our unified representation has better generalization ability over other baselines. are different, we over-sample from the smaller datasets to make the training examples roughly equal. The second step is to fine-tune each taskspecific heads again, similarly to the MT-DNN by Liu et al. (2019a), to obtain the results reported in Table 2 and Table 4. 2 Here, we provide experimental details (dataset, baselines, experimental setups) and results that empirically show the success of the proposed U NIFIED M2 model. U NIFIED M2 In this section, we describe the architecture and the training details for our proposed U NIFIED M2 model. 2.1 Architecture Our proposed model architecture is a hard-parameter sharing multi-task learning model (Ruder, 2017), where a single shared RoBERTa (Liu et al., 2019b) encoder is used across all tasks. RoBERTa is a Transformer encoder pretrained with a masked"
2021.naacl-main.432,2021.ccl-1.108,0,0.0857064,"Missing"
2021.naacl-main.432,I17-2043,0,0.05714,"Missing"
2021.naacl-main.432,D19-1664,0,0.015135,"heads. During inference time, we only use the classification head relevant to the inference-time task. The overall architecture of the model is shown in Figure 1. 2.2 Training 3 3.1 Experiment Misinformation Tasks/Dataset Table 1 lists the four misinformation tasks/datasets we use to train U NIFIED M2. They span various granularities and domains (articles, sentences, headlines and tweets) as well as various objectives (classifying veracity, bias and clickbaity-ness). N EWS B IAS A task to classify whether a given sentence from a news article contains political bias or not. We adapt the BASIL (Fan et al., 2019) dataset, which has bias-span annotations for lexical and informational bias within news articles. Using this dataset, we also include two auxiliary tasks related to political-bias detection: 1) bias type classification – given a biased sentence, the type of the bias (lexical vs informational) is classified; and 2) polarity detection – given a biased sentence, its polarity (positive, negative, neutral) is determined. FAKE N EWS An article-level fake news detection task that leverages the Webis (Potthast et al., 2018) dataset annotated by professional journalists. Our model training process con"
2021.naacl-main.432,P18-1184,0,0.0683527,"Missing"
2021.naacl-main.432,C18-1288,0,0.116892,"ores from leaveone-event-out cross-validation setup for RUMOR task. task-specific models (ST average). As shown in Table 4, our U NIFIED M2 encoder can quickly adapt to new tasks, even with very little in-domain data. While both the single-task models and U NIFIED M2 significantly outperform vanilla RoBERTa, U NIFIED M2 further outperforms the single-task models, indicating that multi-task learning can aid task generalizability. 4.2 Unseen Event Generalizability Dataset We use the previously introduced RU MOR dataset, which includes nine separate events, for this experiment. A group of works (Kochkina et al., 2018; Li et al., 2019; Yu et al., 2020) have used this dataset in a leave-one-event-out crossvalidation setup (eight events for training and one event for testing) to take event generalizability into consideration in their model evaluation. We conduct a supplementary experiment following this evaluation setup for the completeness of our analysis. Experiment First, we train the U NIFIED M2 encoder without RUMOR data, and then fine-tune and evaluate in the leave-one-event-out cross-validation setup. Note that we re-train the U NIFIED M2 encoder to ensure that it has no knowledge of the left-out-even"
2021.naacl-main.432,P19-1113,0,0.0496762,"Missing"
2021.naacl-main.432,D18-1003,0,0.0331863,"Missing"
2021.naacl-main.432,P18-1022,0,0.0234897,"n sentence from a news article contains political bias or not. We adapt the BASIL (Fan et al., 2019) dataset, which has bias-span annotations for lexical and informational bias within news articles. Using this dataset, we also include two auxiliary tasks related to political-bias detection: 1) bias type classification – given a biased sentence, the type of the bias (lexical vs informational) is classified; and 2) polarity detection – given a biased sentence, its polarity (positive, negative, neutral) is determined. FAKE N EWS An article-level fake news detection task that leverages the Webis (Potthast et al., 2018) dataset annotated by professional journalists. Our model training process consists of two steps. The first step is multi-task training of the shared U NIFIED M2 encoder to learn a general misinfor- RUMOR A task to verify the veracity of a rumor mation representation. We jointly optimize for tweet. The PHEME dataset (Zubiaga et al., 2016), all tasks t1 · · · tT by optimizing the sum of their which contains rumor tweets with their correspondtask-specific losses Lt , where Lt refers to the ing reply tweets (social engagement data), is used cross-entropy loss of the task-specific MLP clas- for th"
2021.naacl-main.432,D17-1317,0,0.032803,"Missing"
2021.naacl-main.432,W16-0802,0,0.101982,"Missing"
2021.naacl-main.432,2020.emnlp-main.108,0,0.0654588,"Missing"
2021.naacl-main.432,N18-1074,0,0.0433491,"Missing"
2021.naacl-main.432,P17-2102,0,0.0679395,"Missing"
2021.naacl-main.432,P17-2067,0,0.0819749,"Missing"
2021.naacl-main.432,D19-1471,0,0.0146857,"al., 2018; Nie et al., 2019). Finally, social-data-based approaches use the surrounding social data–such as the credibility of the authors of the information (Long et al., 2017; Kirilin and Strube, 2018; Li et al., 2019) or social engagement data (Derczynski et al., 2017; Ma et al., 2018; Kwon et al., 2013; Volkova et al., 2017). Though prior works have explored multi-task learning within misinformation, they have focused exclusively on one domain. These works try to predict two different labels on the same set of examples from a single (Kochkina et al., 2018) or two closely-related datasets (Wu et al., 2019). In contrast, our proposed approach crosses not just task or dataset boundaries, but also format and domain boundaries. Furthermore, prior works focus on using an auxiliary task to boost the performance of the main task, while we focus on using multitasking to generalize across many domains. Thus, the focus of this work is not the multitask paradigm, but rather the unification of the various domains, using multitasking. 6 Conclusion In this paper, we introduced U NIFIED M2, which unifies multiple domains of misinformation with a single multi-task learning setup. We empirically showed that suc"
C02-1151,W99-0613,0,0.00584324,"ethod and learning algorithm we used. Section 3.2 introduces the idea of using a belief network in search of the best global class labeling and the applied inference algorithm. 3.1 Learning Basic Classifiers Although the labels of entities and relations from a sentence mutually depend on each other, two basic classifiers for entities and relations are first learned, in which a multi-class classifier for E(or R) is learned as a function of all other “known” properties of the observation. The classifier for entities is a named entity classifier, in which the boundary of an entity is predefined (Collins and Singer, 1999). On the other hand, the relation classifier is given a pair of entities, which denote the two arguments of the target relation. Accurate predictions of these two classifiers seem to rely on complicated syntax analysis and semantics related information of the whole sentence. However, we derive weak classifiers by treating these two learning tasks as shallow text processing problems. This strategy has been successfully applied on several NLP tasks, such as information extraction (Califf and Mooney, 1999; Freitag, 2000; Roth and Yih, 2001) and chunking (i.e. shallow paring) (Munoz et al., 1999)."
C02-1151,P99-1042,0,0.00873966,"tion, along with constraints induced among entity types and relations, is used to perform global inference that accounts for the mutual dependencies among the entities. Our preliminary experimental results are promising and show that our global inference approach improves over learning relations and entities separately. 1 Introduction Recognizing and classifying entities and relations in text data is a key task in many NLP problems such as information extraction (IE) (Califf and Mooney, 1999; Freitag, 2000; Roth and Yih, 2001), question answering (QA) (Voorhees, 2000) and story comprehension (Hirschman et al., 1999). In a typical IE application of constructing a jobs database from unstructured text, the system has to extract meaningful entities like title and salary and, ideally, to determine whether the entities are associated with the same position. In a QA system, many questions ask for specific entities involved in some relations. For example, the question “Where was Poe born?” in TREC-9 asks for the location entity in which Poe was born. The question “Who killed Lee Harvey Oswald?” seeks a person entity that has the relation kill with the person Lee Harvey Oswald. In all earlier works we know of, th"
C02-1151,W99-0621,1,0.758572,"ns and Singer, 1999). On the other hand, the relation classifier is given a pair of entities, which denote the two arguments of the target relation. Accurate predictions of these two classifiers seem to rely on complicated syntax analysis and semantics related information of the whole sentence. However, we derive weak classifiers by treating these two learning tasks as shallow text processing problems. This strategy has been successfully applied on several NLP tasks, such as information extraction (Califf and Mooney, 1999; Freitag, 2000; Roth and Yih, 2001) and chunking (i.e. shallow paring) (Munoz et al., 1999). It assumes that the class labels can be decided by local properties, such as the information provided by the words around or inside the target. Examples include the spelling of a word, part-of-speech, and semantic related attributes acquired from external resources such as WordNet. The propositional learner we use is SNoW (Roth, 1998; Carleson et al., 1999) 1 SNoW is a multi-class classifier that is specifically tailored for large scale learning tasks. The learning architecture makes use of a network of linear functions, in which the targets (entity classes or relation classes, in this case)"
C02-1151,voorhees-tice-2000-trec,0,0.0250636,"information in the sentence; this information, along with constraints induced among entity types and relations, is used to perform global inference that accounts for the mutual dependencies among the entities. Our preliminary experimental results are promising and show that our global inference approach improves over learning relations and entities separately. 1 Introduction Recognizing and classifying entities and relations in text data is a key task in many NLP problems such as information extraction (IE) (Califf and Mooney, 1999; Freitag, 2000; Roth and Yih, 2001), question answering (QA) (Voorhees, 2000) and story comprehension (Hirschman et al., 1999). In a typical IE application of constructing a jobs database from unstructured text, the system has to extract meaningful entities like title and salary and, ideally, to determine whether the entities are associated with the same position. In a QA system, many questions ask for specific entities involved in some relations. For example, the question “Where was Poe born?” in TREC-9 asks for the location entity in which Poe was born. The question “Who killed Lee Harvey Oswald?” seeks a person entity that has the relation kill with the person Lee H"
C04-1197,W03-1006,0,0.00848763,"Locative, Temporal or Manner. The PropBank project (Kingsbury and Palmer, 2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assu"
C04-1197,W03-1008,0,0.0133448,"Manner. The PropBank project (Kingsbury and Palmer, 2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assume a full parse as input. SRL"
C04-1197,P02-1031,0,0.0218397,"d their adjuncts, such as Locative, Temporal or Manner. The PropBank project (Kingsbury and Palmer, 2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder"
C04-1197,W04-2416,0,0.0803492,"Missing"
C04-1197,kingsbury-palmer-2002-treebank,0,0.148443,"data provided in CoNLL2004 shared task on semantic role labeling and achieves very competitive results. 1 Introduction Semantic parsing of sentences is believed to be an important task toward natural language understanding, and has immediate applications in tasks such information extraction and question answering. We study semantic role labeling(SRL). For each verb in a sentence, the goal is to identify all constituents that fill a semantic role, and to determine their roles, such as Agent, Patient or Instrument, and their adjuncts, such as Locative, Temporal or Manner. The PropBank project (Kingsbury and Palmer, 2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradh"
C04-1197,J93-2004,0,0.0263009,"mercial numerical packages, this problem can usually be solved very fast in practice. For instance, it only takes about 10 minutes to solve the inference problem for 4305 sentences on a Pentium-III 800 MHz machine in our experiments. Note that ordinary search methods (e.g., beam search) are not necessarily faster than solving an ILP problem and do not guarantee the optimal solution. 5 Experimental Results The system is evaluated on the data provided in the CoNLL-2004 semantic-role labeling shared task which consists of a portion of PropBank corpus. The training set is extracted from TreeBank (Marcus et al., 1993) section 15–18, the development set, used in tuning parameters of the system, from section 20, and the test set from section 21. We first compare this system with the basic tagger that we have, the CSCL shallow parser from (Punyakanok and Roth, 2001), which is equivalent to using the scoring function from the first phase with only the non-overlapping/embedding constraints. In Fβ=1 65.71 65.46 67.13 68.26 set. All results are for overall performance. Without Inference With Inference i=1 M X Rec. 61.50 60.74 64.75 64.93 Table 1: Summary of experiments on the development zji A0 ≥ zmR-A0 Constrain"
C04-1197,N04-1030,0,0.0808165,"2002) provides a large human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assume a full parse as input. SRL is a difficult task, and one cannot expect h"
C04-1197,C02-1151,1,0.61986,"r (2002) and Carreras and M`arquez (2003). 3 System Architecture Our semantic role labeling system consists of two phases. The first phase finds a subset of arguments from all possible candidates. The goal here is to filter out as many as possible false argument candidates, while still maintaining high recall. The second phase focuses on identifying the types of those argument candidates. Since the number of candidates is much fewer, the second phase is able to use SNoW Learning Architecture The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is specifically tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It incorporates several improvements over the basic Winnow multiplicative update rule. In particular, a regularization term is added, which has the effect of trying to separate the data with a thick separator (Grove and Roth, 2001; Hang et al., 2002). In the work presented here we use this regularizat"
C04-1197,W04-2401,1,0.784199,"ez, 2003)). Here, We reformulate the constraints as linear (in)equalities by introducing indicator variables. The optimization problem (Eq. 2) is solved using ILP. 4.2 Using Integer Linear Programming As discussed previously, a collection of potential arguments is not necessarily a valid semantic labeling since it must satisfy all of the constraints. In this context, inference is the process of finding the best (according to Equation 1) valid semantic labels that satisfy all of the specified constraints. We take a similar approach that has been previously used for entity/relation recognition (Roth and Yih, 2004), and model this inference procedure as solving an ILP. An integer linear program(ILP) is basically the same as a linear program. The cost function and the (in)equality constraints are all linear in terms of the variables. The only difference in an ILP is the variables can only take integers as their values. In our inference problem, the variables are in fact binary. A general binary integer programming problem can be stated as follows. Given a cost vector p ∈ <d , a set of variables, z = (z1 , . . . , zd ) and cost matrices C1 ∈ <t1 × <d , C2 ∈ <t2 ×<d , where t1 and t2 are the numbers of ine"
C04-1197,P03-1002,0,0.119163,"human-annotated corpus of semantic verb-argument relations. Specifically, we use the data provided in the CoNLL-2004 shared task of semantic-role labeling (Carreras and M`arquez, 2003) which consists of a portion of the PropBank corpus, allowing us to compare the performance of our approach with other systems. Previous approaches to the SRL task have made use of a full syntactic parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assume a full parse as input. SRL is a difficult task, and one cannot expect high levels of performanc"
C04-1197,W00-0726,0,0.00801379,"c parse of the sentence in order to define argument boundaries and to determine the role labels (Gildea and Palmer, 2002; Chen and Rambow, 2003; Gildea and Hockenmaier, 2003; Pradhan et al., 2003; Pradhan et al., 2004; Surdeanu et al., 2003). In this work, following the CoNLL-2004 shared task definition, we assume that the SRL system takes as input only partial syntactic information, and no external lexico-semantic knowledge bases. Specifically, we assume as input resources a part-of-speech tagger, a shallow parser that can process the input to the level of based chunks and clauses (Tjong Kim Sang and Buchholz, 2000; Tjong Kim Sang and D´ejean, 2001), and a named-entity recognizer (Tjong Kim Sang and De Meulder, 2003). We do not assume a full parse as input. SRL is a difficult task, and one cannot expect high levels of performance from either purely manual classifiers or purely learned classifiers. Rather, supplemental linguistic information must be used to support and correct a learning system. So far, machine learning approaches to SRL have incorporated linguistic information only implicitly, via the classifiers’ features. The key innovation in our approach is the development of a principled method to"
C04-1197,W01-0708,0,0.0170573,"Missing"
C04-1197,W05-0620,0,\N,Missing
C04-1197,W03-0419,0,\N,Missing
D09-1083,P98-2127,0,0.0764279,"framework that learns the term-weighting function. Given the labeled pairs of texts as training data, the learning procedure tunes the model parameters by minimizing the specified loss function of the similarity score. Compared to traditional TFIDF term-weighting schemes, our approach shows a significant improvement on tasks such as judging the quality of query suggestions and filtering irrelevant ads for online advertising. 1 Introduction Measuring the semantic similarity between two texts is an important problem that has many useful applications in both NLP and IR communities. For example, Lin (1998) defined a similarity measure for automatic thesaurus creation from a corpus. Mihalcea et al. (2006) developed several corpus-based and knowledge-based word similarity measures and applied them to a paraphrase recognition task. In the domain of web search, different methods of measuring similarity between short text segments have recently been proposed for solving problems like query suggestion and alternation (Jones et al., 2006; Sahami and Heilman, 2006; Metzler et al., 2007; Yih and Meek, 2007). 793 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, page"
D09-1083,C98-2122,0,\N,Missing
D10-1025,P98-1069,0,0.301965,"Missing"
D10-1025,P08-1088,0,0.0132652,"l., 2009) generalized LDA to tuples of documents from multiple languages. The experiments in section 3 use CL-LSI and an algorithm similar to PLTM as benchmarks. The closest previous work to this paper is the use of Canonical Correlation Analysis (CCA) to find projections for multiple languages whose results are maximally correlated with each other (Vinokourov et al., 2003). PLSA-, LDA-, and CCA-based cross-lingual models have also been trained without the use of parallel or comparable documents, using only knowledge from a translation dictionary to achieve sharing of topics across languages (Haghighi et al., 2008; Jagarlamudi and Daum´e, 2010; Zhang et al., 2010). 252 Such work is complementary to ours and can be used to extend the models to domains lacking parallel documents. Outside of NLP, researchers have designed algorithms to find discriminative projections. We build on the Oriented Principal Component Analysis (OPCA) algorithm (Diamantaras and Kung, 1996), which finds projections that maximize a signal-tonoise ratio (as defined by the user). OPCA has been used to create discriminative features for audio fingerprinting (Burges et al., 2003). 1.2 Structure of paper This paper now presents two alg"
D10-1025,W07-0711,0,0.0109621,"it on the final test set. The regularization γ was tuned for CCA: γ = 10 for Europarl, and γ = 3 for Wikipedia. Figure 3: Mean reciprocal rank versus dimension for Europarl Figure 4: Mean reciprocal rank versus dimension for Wikipedia In the two figures, we evaluate the five projection methods, as well as a word-by-word translation method (denoted by WbW in the graphs). Here “word-by-word” refers to using cosine distance after applying a word-by-word translation model to the Spanish documents. The word-by-word translation model was trained on the Europarl training set, using the WDHMM model (He, 2007), which performs similarly to IBM 258 Model 4. The probability matrix of generating English words from Spanish words was multiplied by each document’s log(tf)-idf vector to produce a translated document vector. We found that multiplying the probability matrix to the log(tf)-idf vector was more accurate on the development set than multiplying the tf vector directly. This vector was either tested as-is, or mapped through LSA learned from the English training set of the corpus. In the figures, the dimensionality of WbW translation refers to the dimensionality of monolingual LSA. The overall order"
D10-1025,D09-1092,0,0.769618,"ors as low-rank Gaussians (Deerwester et al., 1990). Subsequent projection algorithms were based on generative models of individual terms in the documents, including Probabilistic Latent Semantic Analysis (PLSA) (Hofmann, 1999) and Latent Dirichlet Allocation (LDA) (Blei et al., 2003). Work on cross-lingual projections followed a similar pattern of moving from Gaussian models to term-wise generative models. Cross-language Latent Semantic Indexing (CL-LSI) (Dumais et al., 1997) applied LSA to concatenated comparable documents from multiple languages. Similarly, Polylingual Topic Models (PLTM) (Mimno et al., 2009) generalized LDA to tuples of documents from multiple languages. The experiments in section 3 use CL-LSI and an algorithm similar to PLTM as benchmarks. The closest previous work to this paper is the use of Canonical Correlation Analysis (CCA) to find projections for multiple languages whose results are maximally correlated with each other (Vinokourov et al., 2003). PLSA-, LDA-, and CCA-based cross-lingual models have also been trained without the use of parallel or comparable documents, using only knowledge from a translation dictionary to achieve sharing of topics across languages (Haghighi"
D10-1025,J05-4003,0,0.198249,"Missing"
D10-1025,W02-1011,0,0.0142509,"t pairs to have similar vector representations. We evaluate these algorithms on two tasks: parallel document retrieval for Wikipedia and Europarl documents, and cross-lingual text classification on Reuters. The two discriminative variants, OPCA and CPLSA, significantly outperform their corresponding baselines. The largest differences in performance are observed on the task of retrieval when the documents are only comparable and not parallel. The OPCA method is shown to perform best. 1 • Cross-language text categorization — Applications of text categorization, such as sentiment classification (Pang et al., 2002), are now required to run on multiple languages. Categorization is usually trained on the language of the developer: it needs to be easily extended to other languages. Introduction Given the growth of multiple languages on the Internet, Natural Language Processing must operate on dozens of languages. It is becoming critical that computers reach high performance on the following two tasks: • Comparable and parallel document retrieval — Cross-language information retrieval and text categorization have become important with the growth of the Web (Oard and Diekema, 1998). In addition, machine tran"
D10-1025,P99-1067,0,0.492858,"Missing"
D10-1025,P10-1115,0,0.0709661,"Missing"
D10-1025,1998.amta-tutorials.5,0,\N,Missing
D10-1025,C98-1066,0,\N,Missing
D10-1025,W07-0724,0,\N,Missing
D12-1111,N09-1003,0,0.0976078,"Missing"
D12-1111,W02-0908,0,0.478186,"fferentiated. For example, in synonymy, we may wish to distinguish hyponyms and hypernyms. Moreover, Cruse (1986) notes that numerous kinds of antonymy are possible, for example antipodal pairs like “top-bottom” or 1212 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical items in an antonymy relation should lie at opposite ends of an"
D12-1111,P98-2127,0,0.726512,"s of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Existing vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to re"
D12-1111,D08-1103,0,0.794281,"cosine similarities close to one, while antonyms are close to minus one. We derive this representation with the aid of a thesaurus and latent semantic analysis (LSA). Each entry in the thesaurus – a word sense along with its synonyms and antonyms – is treated as a “document,” and the resulting document collection is subjected to LSA. The key contribution of this work is to show how to assign signs to the entries in the co-occurrence matrix on which LSA operates, so as to induce a subspace with the desired property. We evaluate this procedure with the Graduate Record Examination questions of (Mohammed et al., 2008) and find that the method improves on the results of that study. Further improvements result from refining the subspace representation with discriminative training, and augmenting the training data with general newspaper text. Altogether, we improve on the best previous results by 11 points absolute in F measure. 1 Introduction Vector space representations have proven useful across a wide variety of text processing applications ranging from document clustering to search relevance measurement. In these applications, text is Independent of vector-space representations, a number of authors have f"
D12-1111,D10-1025,1,0.744009,"g,jplatt}@microsoft.com Abstract represented as a vector in a multi-dimensional continuous space, and a similarity metric such as cosine similarity can be used to measure the relatedness of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposite"
D12-1111,D09-1001,0,0.0194247,"hods in Natural Language Processing and Computational Natural c Language Learning, pages 1212–1222, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics gradable opposites like “light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical items in an antonymy relation should lie at opposite ends of an axis. However, in contrast to the logical axes used previously, we desire that antonyms should lie at the opposite ends of a sphere lying in a continuous and automatically induced vector space. To generate this vector space, we present a novel method for assigning both negative"
D12-1111,N10-1013,0,0.0107748,"ce representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Existing vector space models typically map synonyms and antonyms to similar word vectors, and thus fail to represent antonymy. We introduce a new vector space"
D12-1111,C02-1061,0,0.0249339,"light-heavy.” Work in this area includes (Turney, 2001; Lin et al., 2003; Turney and Littman, 2005; Turney, 2006; Curran and Moens, 2002; van der Plas and Tiedemann, 2006; Mohammed et al., 2008; Mohammed et al., 2011). Despite the existence of a large amount of related work in the literature, distinguishing synonyms and antonyms is still considered as a difficult open problem in general (Poon and Domingos, 2009). In this paper, we fuse these two strands of research in an attempt to develop a vector space representation in which the synonymy and antonymy are naturally differentiated. We follow Schwab et al. (2002) in requiring a representation in which two lexical items in an antonymy relation should lie at opposite ends of an axis. However, in contrast to the logical axes used previously, we desire that antonyms should lie at the opposite ends of a sphere lying in a continuous and automatically induced vector space. To generate this vector space, we present a novel method for assigning both negative and positive values to the TF-IDF weights used in latent semantic analysis. To determine these signed values, we exploit the information present in a thesaurus. The result is a vector space representation"
D12-1111,J06-3003,0,0.65437,"ed to measure the relatedness of different items. Vector space representations have been used both at the document and word levels. At the document level, they are effective for applications including information retrieval (Salton and McGill, 1983; Deerwester et al., 1990), document clustering (Deerwester et al., 1990; Xu et al., 2003), search relevance measurement (Baeza-Yates and Ribiero-Neto, 1999) and cross-lingual document retrieval (Platt et al., 2010). At the word level, vector representations have been used to measure word similarity (Deerwester et al., 1990; Turney and Littman, 2005; Turney, 2006; Turney, 2001; Lin, 1998; Agirre et al., 2009; Reisinger and Mooney, 2010) and for language modeling (Bellegarda, 2000; Coccaro and Jurafsky, 1998). While quite successful, these applications have typically been consistent with a very general notion of similarity in which basic association is measured, and finer shades of meaning need not be distinguished. For example, latent semantic analysis might assign a high degree of similarity to opposites as well as synonyms (Landauer and Laham, 1998; Landauer, 2002). Existing vector space models typically map synonyms and antonyms to similar word vec"
D12-1111,C08-1114,0,0.0629197,"s (e.g. “WORK” vs. “ACTIVITY FOR FUN”) and then determining the degree of antonymy. Categories are defined by a thesaurus; contrasting categories are found by using affix rules (e.g., un- & dis-) and WordNet antonymy links. Words belonging to contrasting categories are treated as antonyms and the degree of contrast is determined by distributional similarity. Mohammed et al. (2008) also provides a publicly available dataset for detection of antonymy, which we have adopted. This work has been extended in (Mohammed et al., 2011) to include a study of antonymy based on crowd-sourcing experiments. Turney (2008) proposes a unified approach to handling analogies, synonyms, antonyms and associations by transforming the last three cases into cases of analogy. A supervised learning method is then used to solve the resulting analogical problems. This is evaluated on a set of 136 ESL questions. Lin et al. (2003) builds on (Lin, 1998) and identifies antonyms as semantically related words which also happen to be found together in a database in pre-identified phrases indicating opposition. Lin et al. (2003) further note that whereas synonyms will tend to translate to the same word in another language, antonym"
D12-1111,P06-2111,0,0.229758,"Missing"
D12-1111,W11-0329,1,0.747768,"licitly try to achieve such goals. In this section, we see that when supervised training data is available, the projection matrix of LSA can be enhanced through a discriminative training technique explicitly designed to create a representation suited to a specific task. Because LSA is closely related to principle component analysis (PCA), extensions of PCA such as canonical correlation analysis (CCA) and oriented principle component analysis (OPCA) can leverage the labeled data and produce the projection matrix through general eigen-decomposition (Platt et al., 2010). Along this line of work, Yih et al. (2011) proposed a Siamese neural network approach called S2Net, which tunes the projection matrix directly through gradient descent, and has shown to outperform other methods in several tasks. Below we describe briefly this technique and explain how we adopt it for the task of antonym detection. The goal of S2Net is to learn a concept vector representation of the original sparse term vectors. Although such transformation can be non-linear in general, its current design chooses the model form to be a linear projection matrix, which is identical to Word admirable PILSA-Similar Words commendable, credi"
D12-1111,C98-2122,0,\N,Missing
D13-1006,N12-2002,0,0.475923,"g. his/her )”. Although useful for the task at hand, this has counterintuitive consequences: for example, baby may be considered animate or inanimate, and ant is considered inanimate (Ibid., Figure 1). Others have argued that animacy should be captured by a hierarchy or by categories (Aissen, 2003; Silverstein, 1986). For instance, Zaenen et al. (2004) propose three levels of animacy (human, other animate and inanimate), which cover ten categories of noun phrases, with categories like ORG (organization), ANIM (animal) and MAC (intelligent machines such as robots) categorised as other animate. Bowman and Chopra (2012) report results for animacy defined both this way and with the categories collapsed to a binary (animate, inanimate) definition. 55 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 55–60, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2.2 Methods for Animacy Detection Evans and Orˇasan (2000) propose a rule-based system based on the WordNet taxonomy (Fellbaum, 1998). Each synset is ascribed a binary animacy label based on its unique beginner. A given noun is then associated with the fraction of its ani"
D13-1006,W05-0509,0,0.357202,"Missing"
D13-1006,Y09-1024,0,0.511776,"or reflexives, in the NP. In later work, Orˇasan and Evans (2007) extend the algorithm by propagating animacy labels in the WordNet graph using a chi-squared test, and then apply a k-nearest neighbor classifier based on four lexical features. In their work, the only context used was the animacy of the verb in the NP, for heads of subject NPs (e.g., the subject of eat is typically animate). Øvrelid (2009) and Bowman and Chopra (2012) extend this idea by using dependency relations to generate features for their classifier, enabled by corpora created by Zaenen et al. (2004). In another approach, Ji and Lin (2009) apply a simple “relative-pronoun” pattern to the Google n-gram corpus (Brants and Franz, 2006) to assign animacy (see the List model in Section 5 for details). Although the animacy decision is again context-independent, such a list provides a strong baseline and thus benefit applications like anaphora resolution (Lee et al., 2013). 3 The Task We adopt a definition of animacy closest to the binary version in Bowman and Chopra (2012): we define an entity to be animate if it is alive and has the ability to move under its own will. We adopt this simple definition because it fits well with the com"
D13-1006,J13-4004,0,0.244701,"ining accuracy. Animacy detection is a problem whose solution has been shown to be beneficial for a number of syntactic and semantic tasks. We present a state-of-the-art system for this task which uses a number of simple classifiers with heterogeneous data sources in a voting scheme. We show how this framework can give us direct insight into the behavior of the system, allowing us to more easily diagnose sources of error. 1 2 Introduction 2.1 Animacy detection has proven useful for a variety of syntactic and semantic tasks, such as anaphora and coreference resolution (Orˇasan and Evans, 2007; Lee et al., 2013), verb argument disambiguation (Dell’Orletta et al., 2005) and dependency parsing (Øvrelid and Nivre, 2007). Existing approaches for animacy detection typically rely on two types of information: linguistic databases, and syntactic cues observed from the corpus. They usually combine two types of approaches: rule based systems, and machine learning techniques. In this paper we explore a slightly different angle: we wish to design an animacy detector whose decisions are interpretable and correctable, so that downstream semantic modeling systems can revisit those decisions as needed. Thus here, we"
D13-1006,E09-1072,0,0.2741,"macy is then ascribed by applying a series of rules imposing thresholds on those fractions, together with rules (and a gazetteer) to detect names and acronyms, and a rule triggered by the occurrence of who, or reflexives, in the NP. In later work, Orˇasan and Evans (2007) extend the algorithm by propagating animacy labels in the WordNet graph using a chi-squared test, and then apply a k-nearest neighbor classifier based on four lexical features. In their work, the only context used was the animacy of the verb in the NP, for heads of subject NPs (e.g., the subject of eat is typically animate). Øvrelid (2009) and Bowman and Chopra (2012) extend this idea by using dependency relations to generate features for their classifier, enabled by corpora created by Zaenen et al. (2004). In another approach, Ji and Lin (2009) apply a simple “relative-pronoun” pattern to the Google n-gram corpus (Brants and Franz, 2006) to assign animacy (see the List model in Section 5 for details). Although the animacy decision is again context-independent, such a list provides a strong baseline and thus benefit applications like anaphora resolution (Lee et al., 2013). 3 The Task We adopt a definition of animacy closest to"
D13-1006,N12-3006,1,0.886827,"Missing"
D13-1006,D13-1020,1,0.74531,"ts challenges, so that a binary label must be applied depending on the predominant meaning. In “A plate of chicken,” chicken is treated as inanimate since it refers to food. In “Caruso (1873-1921) is considered one of the world’s best opera singers. He...,” although at the time of writing clearly Caruso was not alive, the token is still treated as animate here because the subsequent writing refers to a live person. 4 The Data We used the MC160 dataset, which is a subset of the MCTest dataset and which is composed of 160 grade level reading comprehension stories generated using crowd sourcing (Richardson et al., 2013). Workers were asked to write a short story (typically less than 300 words) with a target audience of 5 to 7 year olds. The available vocabulary was limited to approximately 8000 words, to model the reading ability of a first or second grader. We labeled this data for animacy using the definition given above. The first 100 of the 160 stories were used as the training set, and the remaining 60 were used for the test set. These animacy labels will be made available on the web site for MCTest (Richardson et al., 2013). 5 The Models Since one of our key goals is interpretability we chose to use an"
D13-1006,N12-1077,1,0.830918,"the frequencies with which the relative pronouns who, where, when, and which occur are considered. Any noun followed most frequently by who is classified as Animate, and any other noun in the list is classified as Inanimate. This voter abstains when the noun is not present in the list. Anaphora Design: The WordNet-based approach of Evans and Orˇasan (2000). WordNet: A simple approach using WordNet. This voter chooses Animal or Person if the unique beginner of the first synset of the noun is either of these, and Inanimate otherwise. WordSim: This voter uses the contextual vector space model of Yih and Qazvinian (2012) computed using Wikipedia and LA Times data. It uses short lists of hand-chosen signal words for the categories Animal, Person, and Inanimate to produce a “response” of the word to each category. This response is equal to the maximum cosine similarity in the vector space of the query word to any signal word in the category. The final vote goes to the category with the highest response. Name: We used an in-house named entity tagger. This voter can recognize some inanimate entities such as cities, but does not distinguish between people and animals, and so can only vote Animate, Inanimate or Abs"
D13-1006,W04-0216,0,0.0430802,"Missing"
D13-1167,N09-1003,0,0.101703,"Missing"
D13-1167,D07-1109,0,0.0193715,"e on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word ∗ Work conducted while interning at Microsoft Research. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. For instance, when applied to lexical semantics, synonyms and antonyms may both be assigned high similarity scores (Landauer and Laham, 1998; Landauer, 2002). Asymmetric relations like hyponyms and hypernyms also canno"
D13-1167,N13-1052,0,0.0279693,"ing a supervised model with features based on the frequencies of patterns in the corpus. Similarly, to measure whether two word pairs have the same relation, Zhila et al. (2013) proposed to combine heterogeneous models, which achieved state-of-the-art performance. In comparison, MRLSA models multiple lexical relations holistically. The degree that two words having a particular relation is estimated using the same linear function of the corresponding vectors and matrix. Tensor decomposition generalizes matrix factorization and has been applied to several NLP applications recently. For example, Cohen et al. (2013) proposed an approximation algorithm for PCFG parsing that relies on Kruskal decomposition. Van de Cruys et al. (2013) modeled the composition of subject-verb-object triples using Tucker decomposition, which results in a better similarity measure for transitive phrases. Similar to this construction but used in the community-based question answering (CQA) scenario, Qiu et al. (2013) represented triples of question title, question content and answer as a tensor and applied 3-mode SVD to derive latent semantic representations for question matching. The construction of MRLSA bears some resemblance"
D13-1167,P12-1091,0,0.0139286,"relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word ∗ Work conducted while interning at Microsoft Research. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-"
D13-1167,N13-1089,0,0.0102888,"measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word ∗ Work conducted while interning at Microsoft Research. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. Fo"
D13-1167,S12-1047,0,0.120674,"and is-a. We use the benchmark GRE test of closestopposites (Mohammad et al., 2008) to show that MRLSA performs comparably to PILSA, which was the pervious state-of-the-art approach on this problem, when given the same amount of information. In addition, when other words and relations are available, potentially from additional resources, MRLSA is able to outperform previous methods significantly. We use the is-a relation to demonstrate that MRLSA is capable of handling asymmetric relations. We take the list of word pairs from the Class-Inclusion (i.e., is-a) relations in SemEval-2012 Task 2 (Jurgens et al., 2012), and use our model to measure the degree of two words have this relation. The measures derived from our model correlate with human judgement better than the best system that participated in the task. The rest of this paper is organized as follows. We first survey some related work in Section 2, followed by a more detailed description of LSA and PILSA in Section 3. Our proposed model, MRLSA, is presented in Section 4. Section 5 presents our experimental results. Finally, Section 6 concludes the paper. 1603 2 Related Work MRLSA can be viewed as a model that derives general continuous space repr"
D13-1167,N13-1090,1,0.236592,"ce representation of text is arguably the vector space model (VSM) (Turney and Pantel, 2010). In this representation, each text object can be represented by a high-dimensional sparse vector, such as a term-vector or a document-vector that denotes the statistics of term occurrences (Salton et al., 1975) in a large corpus. The text can also be represented by a low-dimensional dense vector derived by linear projection models like latent semantic analysis (LSA) (Deerwester et al., 1990), by discriminative learning methods like Siamese neural networks (Yih et al., 2011), recurrent neural networks (Mikolov et al., 2013) and recursive neural networks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the corresponding vectors, which has been a standard appr"
D13-1167,D08-1103,0,0.0590309,"ly unseen relations between two words can be discovered, and the information encoded in other relations can influence the construction of the latent representations, and thus potentially improves the overall quality. In addition, the information in different slices can come from heterogeneous sources (conceptually similar to (Riedel et al., 2013)), which not only improves the model, but also extends the word coverage in a reliable way. We provide empirical evidence that MRLSA is effective using two different word relations: antonymy and is-a. We use the benchmark GRE test of closestopposites (Mohammad et al., 2008) to show that MRLSA performs comparably to PILSA, which was the pervious state-of-the-art approach on this problem, when given the same amount of information. In addition, when other words and relations are available, potentially from additional resources, MRLSA is able to outperform previous methods significantly. We use the is-a relation to demonstrate that MRLSA is capable of handling asymmetric relations. We take the list of word pairs from the Class-Inclusion (i.e., is-a) relations in SemEval-2012 Task 2 (Jurgens et al., 2012), and use our model to measure the degree of two words have thi"
D13-1167,D10-1025,1,0.844508,"y a latent square matrix. The degree of two words having a specific relation can then be measured through simple linear algebraic operations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word ∗ Work conducted while interning at Microsoft Research. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-"
D13-1167,P13-2077,0,0.0579411,"Missing"
D13-1167,N10-1013,0,0.0217339,"rks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the corresponding vectors, which has been a standard approach (Agirre et al., 2009; Reisinger and Mooney, 2010; Yih and Qazvinian, 2012). These approaches do not apply directly to the problem of modeling other types of relations. Existing methods that do handle multiple relations often use a model combination scheme to integrate signals from various types of information sources. For instance, morphological variations discovered from the Google n-gram corpus have been combined with information from thesauri and vector-based word relatedness models for detecting antonyms (Mohammad et al., 2008). An alternative approach proposed by Turney (2008) that handles synonyms, antonyms and associations is to use"
D13-1167,N13-1008,0,0.0242638,"nds to the document-term matrix in the original LSA design but for a specific relation. Analogous to LSA, the whole linear transformation mapping is derived through tensor decomposition, which provides a low-rank approximation of the original tensor. As a result, previously unseen relations between two words can be discovered, and the information encoded in other relations can influence the construction of the latent representations, and thus potentially improves the overall quality. In addition, the information in different slices can come from heterogeneous sources (conceptually similar to (Riedel et al., 2013)), which not only improves the model, but also extends the word coverage in a reliable way. We provide empirical evidence that MRLSA is effective using two different word relations: antonymy and is-a. We use the benchmark GRE test of closestopposites (Mohammad et al., 2008) to show that MRLSA performs comparably to PILSA, which was the pervious state-of-the-art approach on this problem, when given the same amount of information. In addition, when other words and relations are available, potentially from additional resources, MRLSA is able to outperform previous methods significantly. We use th"
D13-1167,S12-1055,0,0.0346693,"Missing"
D13-1167,P13-1045,0,0.00638257,"perations. We demonstrate that by integrating multiple relations from both homogeneous and heterogeneous information sources, MRLSA achieves stateof-the-art performance on existing benchmark datasets for two relations, antonymy and is-a. 1 Introduction Continuous semantic space representations have proven successful in a wide variety of NLP and IR applications, such as document clustering (Xu et al., 2003) and cross-lingual document retrieval (Dumais et al., 1997; Platt et al., 2010) at the document level and sentential semantics (Guo and Diab, 2012; Guo and Diab, 2013) and syntactic parsing (Socher et al., 2013) at the sentence level. Such representations also play an important role in applications for lexical semantics, such as word sense disambiguation (Boyd-Graber et al., 2007), measuring word ∗ Work conducted while interning at Microsoft Research. LSA operates by mapping text objects, typically documents and words, to a latent semantic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. For instance, when applied to lexical semantic"
D13-1167,J06-3003,0,0.236427,"Missing"
D13-1167,C08-1114,0,0.0129127,"been a standard approach (Agirre et al., 2009; Reisinger and Mooney, 2010; Yih and Qazvinian, 2012). These approaches do not apply directly to the problem of modeling other types of relations. Existing methods that do handle multiple relations often use a model combination scheme to integrate signals from various types of information sources. For instance, morphological variations discovered from the Google n-gram corpus have been combined with information from thesauri and vector-based word relatedness models for detecting antonyms (Mohammad et al., 2008). An alternative approach proposed by Turney (2008) that handles synonyms, antonyms and associations is to use a uniform approach by first reducing the problem to determining whether two pairs of words can be analogous, and then predicting it using a supervised model with features based on the frequencies of patterns in the corpus. Similarly, to measure whether two word pairs have the same relation, Zhila et al. (2013) proposed to combine heterogeneous models, which achieved state-of-the-art performance. In comparison, MRLSA models multiple lexical relations holistically. The degree that two words having a particular relation is estimated usin"
D13-1167,N13-1134,0,0.0290766,"Missing"
D13-1167,N12-1077,1,0.751379,"r by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the corresponding vectors, which has been a standard approach (Agirre et al., 2009; Reisinger and Mooney, 2010; Yih and Qazvinian, 2012). These approaches do not apply directly to the problem of modeling other types of relations. Existing methods that do handle multiple relations often use a model combination scheme to integrate signals from various types of information sources. For instance, morphological variations discovered from the Google n-gram corpus have been combined with information from thesauri and vector-based word relatedness models for detecting antonyms (Mohammad et al., 2008). An alternative approach proposed by Turney (2008) that handles synonyms, antonyms and associations is to use a uniform approach by firs"
D13-1167,W11-0329,1,0.838089,"pproach. The most commonly used continuous space representation of text is arguably the vector space model (VSM) (Turney and Pantel, 2010). In this representation, each text object can be represented by a high-dimensional sparse vector, such as a term-vector or a document-vector that denotes the statistics of term occurrences (Salton et al., 1975) in a large corpus. The text can also be represented by a low-dimensional dense vector derived by linear projection models like latent semantic analysis (LSA) (Deerwester et al., 1990), by discriminative learning methods like Siamese neural networks (Yih et al., 2011), recurrent neural networks (Mikolov et al., 2013) and recursive neural networks (Socher et al., 2011), or by graphical models such as probabilistic latent semantic analysis (PLSA) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei et al., 2003). As a generalization of LSA, MRLSA is also a linear projection model. However, while the words are represented by vectors as well, multiple relations between words are captured separately by matrices. In the context of lexical semantics, VSMs provide a natural way of measuring semantic word relatedness by computing the distance between the cor"
D13-1167,D12-1111,1,0.65645,"tic space. The proximity of the vectors in this space implies that the original text objects are semantically related. However, one well-known limitation of LSA is that it is unable to differentiate fine-grained relations. For instance, when applied to lexical semantics, synonyms and antonyms may both be assigned high similarity scores (Landauer and Laham, 1998; Landauer, 2002). Asymmetric relations like hyponyms and hypernyms also cannot be differentiated. Although there exists some recent work, such as PILSA which tries to overcome this weakness of LSA by introducing the notion of polarity (Yih et al., 2012). This extension, however, can only handle two opposing relations (e.g., synonyms and antonyms), leaving open the challenge of encoding multiple relations. In this paper, we propose Multi-Relational Latent Semantic Analysis (MRLSA), which strictly generalizes LSA to incorporate information of multiple relations concurrently. Similar to LSA or PILSA when applied to lexical semantics, each word is still mapped to a vector in the latent space. However, when measuring whether two words have a specific relation (e.g., antonymy or is-a), the word vectors will be mapped to a new space according to th"
D13-1167,N13-1120,1,0.770861,"logical variations discovered from the Google n-gram corpus have been combined with information from thesauri and vector-based word relatedness models for detecting antonyms (Mohammad et al., 2008). An alternative approach proposed by Turney (2008) that handles synonyms, antonyms and associations is to use a uniform approach by first reducing the problem to determining whether two pairs of words can be analogous, and then predicting it using a supervised model with features based on the frequencies of patterns in the corpus. Similarly, to measure whether two word pairs have the same relation, Zhila et al. (2013) proposed to combine heterogeneous models, which achieved state-of-the-art performance. In comparison, MRLSA models multiple lexical relations holistically. The degree that two words having a particular relation is estimated using the same linear function of the corresponding vectors and matrix. Tensor decomposition generalizes matrix factorization and has been applied to several NLP applications recently. For example, Cohen et al. (2013) proposed an approximation algorithm for PCFG parsing that relies on Kruskal decomposition. Van de Cruys et al. (2013) modeled the composition of subject-verb"
D14-1165,P07-1073,0,0.07303,"points when used as a subcomponent. 1 Introduction Identifying the relationship between entities from free text, relation extraction is a key task for acquiring new facts to increase the coverage of a structured knowledge base. Given a pre-defined database schema, traditional relation extraction approaches focus on learning a classifier using textual data alone, such as patterns between the occurrences of two entities in documents, to determine whether the entities have a particular relation. Other than using the existing known facts to label the text corpora in a distant supervision setting (Bunescu and Mooney, 2007; Mintz et al., ∗ Work conducted while interning at Microsoft Research. 2009; Riedel et al., 2010; Ritter et al., 2013), an existing knowledge base is typically not involved in the process of relation extraction. However, this paradigm has started to shift recently, as researchers showed that by taking existing facts of a knowledge base as an integral part of relation extraction, the model can leverage richer information and thus yields better performance. For instance, Riedel et al. (2013) borrowed the idea of collective filtering and constructed a matrix where each row is a pair of entities"
D14-1165,N13-1090,1,0.0150514,"this paper, we will use Xk to refer to the k-th slice of the tensor X . Fig. 1 illustrates this representation. 1 This representation can easily be extended for a probabilistic knowledge base by allowing nonnegative real values. χk en χ e1 of the vectors of (e1 , r) and (e2 , r). Later, they proposed a more scalable method called translating embeddings (TransE) (Bordes et al., 2013a). While both entities and relations are still represented by vectors, the score of (e1 , r, e2 ) becomes the negative dissimilarity measure of the corresponding vectors −kei + rk − ej k, motivated by the work in (Mikolov et al., 2013b; Mikolov et al., 2013a). Alternatively, Socher et al. (2013) proposed a Neural Tensor Network (NTN) that represents entities in d-dimensional vectors created separately by averaging pre-trained word vectors, and then learns a d × d × m tensor describing the interactions between these latent components in each of the m relations. All these methods optimize for loss functions that are more directly related to the true objective – the prediction accuracy of correct entity-relation triples, compared to the meansquared reconstruction error in our method. Nevertheless, they typically require much"
D14-1165,D13-1167,1,0.595887,"Missing"
D14-1165,P09-1113,0,0.347175,"wo entities. There are 72 fine types extracted from Freebase assigned to 53,836 entities that are recorded in Freebase. In addition, special types, PER, LOC, ORG and MISC, are assigned to the remaining 26,862 entities based on the predicted NER tags provided by the corpus. A type is considered incompatible to a relation or a surface pattern if in the training data, none of the argument entities of the relation belongs to the type. We use r = 400 and λ = 0.1 in T RESCAL to factorize the tensor. We compare the proposed T RESCAL model to RI13 (Riedel et al., 2013), YA11 (Yao et al., 2011), MI09 (Mintz et al., 2009) and SU12 (Surdeanu et al., 2012)8 . We follow the protocol used in (Riedel et al., 2013) to evaluate the results. Given a relation as query, the top 1,000 entity pairs output by each system are collected and the top 100 ones are judged manually. Besides comparing individual models, we also report the results of combined models. To combine the scores from two models, we simply normalize the scores of entity-relation tuples to zero mean and unit variance and take the average. The results are summarized in Table 3. As can been seen in the table, using T RESCAL alone is not very effective and its"
D14-1165,N13-1008,0,0.861747,"ther than using the existing known facts to label the text corpora in a distant supervision setting (Bunescu and Mooney, 2007; Mintz et al., ∗ Work conducted while interning at Microsoft Research. 2009; Riedel et al., 2010; Ritter et al., 2013), an existing knowledge base is typically not involved in the process of relation extraction. However, this paradigm has started to shift recently, as researchers showed that by taking existing facts of a knowledge base as an integral part of relation extraction, the model can leverage richer information and thus yields better performance. For instance, Riedel et al. (2013) borrowed the idea of collective filtering and constructed a matrix where each row is a pair of entities and each column is a particular relation. For a true entityrelation triple (e1 , r, e2 ), either from the text corpus or from the knowledge base, the corresponding entry in the matrix is 1. A previously unknown fact (i.e., triple) can be discovered through matrix decomposition. This approach can be viewed as creating vector representations of each relation and candidate pair of entities. Because each entity does not have its own representation, relationships of any unpaired entities cannot"
D14-1165,Q13-1030,0,0.0138003,"xtraction is a key task for acquiring new facts to increase the coverage of a structured knowledge base. Given a pre-defined database schema, traditional relation extraction approaches focus on learning a classifier using textual data alone, such as patterns between the occurrences of two entities in documents, to determine whether the entities have a particular relation. Other than using the existing known facts to label the text corpora in a distant supervision setting (Bunescu and Mooney, 2007; Mintz et al., ∗ Work conducted while interning at Microsoft Research. 2009; Riedel et al., 2010; Ritter et al., 2013), an existing knowledge base is typically not involved in the process of relation extraction. However, this paradigm has started to shift recently, as researchers showed that by taking existing facts of a knowledge base as an integral part of relation extraction, the model can leverage richer information and thus yields better performance. For instance, Riedel et al. (2013) borrowed the idea of collective filtering and constructed a matrix where each row is a pair of entities and each column is a particular relation. For a true entityrelation triple (e1 , r, e2 ), either from the text corpus o"
D14-1165,D11-1135,0,0.0269477,"surface pattern between two entities. There are 72 fine types extracted from Freebase assigned to 53,836 entities that are recorded in Freebase. In addition, special types, PER, LOC, ORG and MISC, are assigned to the remaining 26,862 entities based on the predicted NER tags provided by the corpus. A type is considered incompatible to a relation or a surface pattern if in the training data, none of the argument entities of the relation belongs to the type. We use r = 400 and λ = 0.1 in T RESCAL to factorize the tensor. We compare the proposed T RESCAL model to RI13 (Riedel et al., 2013), YA11 (Yao et al., 2011), MI09 (Mintz et al., 2009) and SU12 (Surdeanu et al., 2012)8 . We follow the protocol used in (Riedel et al., 2013) to evaluate the results. Given a relation as query, the top 1,000 entity pairs output by each system are collected and the top 100 ones are judged manually. Besides comparing individual models, we also report the results of combined models. To combine the scores from two models, we simply normalize the scores of entity-relation tuples to zero mean and unit variance and take the average. The results are summarized in Table 3. As can been seen in the table, using T RESCAL alone is"
D14-1165,D12-1042,0,0.0964042,"types extracted from Freebase assigned to 53,836 entities that are recorded in Freebase. In addition, special types, PER, LOC, ORG and MISC, are assigned to the remaining 26,862 entities based on the predicted NER tags provided by the corpus. A type is considered incompatible to a relation or a surface pattern if in the training data, none of the argument entities of the relation belongs to the type. We use r = 400 and λ = 0.1 in T RESCAL to factorize the tensor. We compare the proposed T RESCAL model to RI13 (Riedel et al., 2013), YA11 (Yao et al., 2011), MI09 (Mintz et al., 2009) and SU12 (Surdeanu et al., 2012)8 . We follow the protocol used in (Riedel et al., 2013) to evaluate the results. Given a relation as query, the top 1,000 entity pairs output by each system are collected and the top 100 ones are judged manually. Besides comparing individual models, we also report the results of combined models. To combine the scores from two models, we simply normalize the scores of entity-relation tuples to zero mean and unit variance and take the average. The results are summarized in Table 3. As can been seen in the table, using T RESCAL alone is not very effective and its performance is only compatible t"
D14-1165,D13-1136,0,0.0732934,"ve filtering and constructed a matrix where each row is a pair of entities and each column is a particular relation. For a true entityrelation triple (e1 , r, e2 ), either from the text corpus or from the knowledge base, the corresponding entry in the matrix is 1. A previously unknown fact (i.e., triple) can be discovered through matrix decomposition. This approach can be viewed as creating vector representations of each relation and candidate pair of entities. Because each entity does not have its own representation, relationships of any unpaired entities cannot be discovered. Alternatively, Weston et al. (2013) created two types of embedding – one based on textual similarity and the other based on knowledge base, where the latter maps each entity and relation to the same ddimensional vector space using a model proposed by Bordes et al. (2013a). They also showed that combining these two models results in a significant improvement over the model trained using only textual data. To make such an integrated strategy work, it is important to capture all existing entities and relations, as well as the known facts, from both textual data and large databases. In this paper, we propose a new knowledge base em"
D15-1237,N10-1145,0,0.283244,"re no correct sentences, enabling researchers to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the W IKI QA dataset. 1 Q: How did Seminole war end? A: Ultimately, the Spanish Crown ceded the colony to United States rule. Introduction Answer sentence selection is a crucial subtask of the open-domain question answering (QA) problem, with the goal of extracting answers from a set of pre-selected sentences (Heilman and Smith, 2010; Yao et al., 2013; Severyn and Moschitti, 2013). In order to conduct research on this important problem, Wang et al. (2007) created a dataset, which we refer to by QAS ENT, based on the TREC-QA data. The QAS ENT dataset chose questions in TREC 8-13 QA tracks and selected sentences that share one or more non-stopwords from the questions. Although QAS ENT has since ∗ Work conducted while interning at Microsoft Research. One significant concern with this approach is that the lexical overlap will make sentence selection easier for the QAS ENT dataset and might inflate the performance of existing"
D15-1237,C02-1150,0,0.499864,"ching methods: Word Count and Weighted Word Count. The first method counts the number of non-stopwords in the question that also occur in the answer sentence. The second method re-weights the counts by the IDF values of the question words. We reimplement LCLR (Yih et al., 2013), an answer sentence selection approach that achieves very competitive results on QAS ENT. LCLR sentences that have human annotations. 4 The classifier is trained using a logistic regression model on the UIUC Question Classification Datasets (http: //cogcomp.cs.illinois.edu/Data/QA/QC). The performance is comparable to (Li and Roth, 2002). 2015 Model Word Cnt Wgt Word Cnt LCLR PV CNN PV-Cnt CNN-Cnt QAS ENT W IKI QA MAP MRR MAP MRR 0.5919 0.6095 0.6954 0.5213 0.5590 0.6762 0.6951 0.6662 0.6746 0.7617 0.6023 0.6230 0.7514 0.7633 0.4891 0.5099 0.5993 0.5110 0.6190 0.5976 0.6520 0.4924 0.5132 0.6086 0.5160 0.6281 0.6058 0.6652 Table 4: Baseline results on both QAS ENT and W IKI QA datasets. Questions without correct answers in the candidate sentences are removed in the W IKI QA dataset. The best results are in bold. makes use of rich lexical semantic features, including word/lemma matching, WordNet and vector-space lexical semanti"
D15-1237,D13-1044,0,0.142527,"rs to work on answer triggering, a critical component in any QA system. We compare several systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the W IKI QA dataset. 1 Q: How did Seminole war end? A: Ultimately, the Spanish Crown ceded the colony to United States rule. Introduction Answer sentence selection is a crucial subtask of the open-domain question answering (QA) problem, with the goal of extracting answers from a set of pre-selected sentences (Heilman and Smith, 2010; Yao et al., 2013; Severyn and Moschitti, 2013). In order to conduct research on this important problem, Wang et al. (2007) created a dataset, which we refer to by QAS ENT, based on the TREC-QA data. The QAS ENT dataset chose questions in TREC 8-13 QA tracks and selected sentences that share one or more non-stopwords from the questions. Although QAS ENT has since ∗ Work conducted while interning at Microsoft Research. One significant concern with this approach is that the lexical overlap will make sentence selection easier for the QAS ENT dataset and might inflate the performance of existing systems in more natural settings. For instance,"
D15-1237,D07-1003,0,0.502781,"eral systems on the task of answer sentence selection on both datasets and also describe the performance of a system on the problem of answer triggering using the W IKI QA dataset. 1 Q: How did Seminole war end? A: Ultimately, the Spanish Crown ceded the colony to United States rule. Introduction Answer sentence selection is a crucial subtask of the open-domain question answering (QA) problem, with the goal of extracting answers from a set of pre-selected sentences (Heilman and Smith, 2010; Yao et al., 2013; Severyn and Moschitti, 2013). In order to conduct research on this important problem, Wang et al. (2007) created a dataset, which we refer to by QAS ENT, based on the TREC-QA data. The QAS ENT dataset chose questions in TREC 8-13 QA tracks and selected sentences that share one or more non-stopwords from the questions. Although QAS ENT has since ∗ Work conducted while interning at Microsoft Research. One significant concern with this approach is that the lexical overlap will make sentence selection easier for the QAS ENT dataset and might inflate the performance of existing systems in more natural settings. For instance, Yih et al. (2013) find that simple word matching methods outperform many sop"
D15-1237,N13-1106,0,0.622424,"Missing"
D15-1237,P13-1171,1,0.917497,". In order to conduct research on this important problem, Wang et al. (2007) created a dataset, which we refer to by QAS ENT, based on the TREC-QA data. The QAS ENT dataset chose questions in TREC 8-13 QA tracks and selected sentences that share one or more non-stopwords from the questions. Although QAS ENT has since ∗ Work conducted while interning at Microsoft Research. One significant concern with this approach is that the lexical overlap will make sentence selection easier for the QAS ENT dataset and might inflate the performance of existing systems in more natural settings. For instance, Yih et al. (2013) find that simple word matching methods outperform many sophisticated approaches on the dataset. We explore this possibility in Section 3. A second, more subtle challenge for question answering is that it normally assumes that there is at least one correct answer for each question in the candidate sentences. During the data construction procedures, all the questions without correct answers are manually discarded.1 We address a new challenge of answer triggering, an important component in QA systems, where the goal is to detect whether there exist correct answers in the set of candidate sentenc"
D16-1029,Q13-1005,0,0.0781941,"ems. We discuss this in detail in Section 4. 1 The new resource and the dataset we used for training is available soon on https://aka.ms/dataimplicit and https://aka.ms/datadraw 298 Solving automatic algebra word problems can be viewed as a semantic parsing task. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (under the name response-driven learning (Clarke et al., 2010)) to knowledge base question answering tasks such as Geoquery (Zelle and Mooney, 1996) and WebQuestions (Berant et al., 2013) or mapping instructions to actions (Artzi and Zettlemoyer, 2013). In these tasks, researchers have shown that it is possible to train a semantic parser only from questionanswer pairs, such as “What is the largest state bordering Texas?” and “New Mexico” (Clarke et al., 2010; Liang et al., 2013; Yih et al., 2015). One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the knowledge base alone, with the help of heuristics developed for the specific domain. For instance, when the question is relatively simple and does not have complex compositional structure,"
D16-1029,D13-1160,0,0.0446335,"ly the solutions with little or no annoated equation systems. We discuss this in detail in Section 4. 1 The new resource and the dataset we used for training is available soon on https://aka.ms/dataimplicit and https://aka.ms/datadraw 298 Solving automatic algebra word problems can be viewed as a semantic parsing task. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (under the name response-driven learning (Clarke et al., 2010)) to knowledge base question answering tasks such as Geoquery (Zelle and Mooney, 1996) and WebQuestions (Berant et al., 2013) or mapping instructions to actions (Artzi and Zettlemoyer, 2013). In these tasks, researchers have shown that it is possible to train a semantic parser only from questionanswer pairs, such as “What is the largest state bordering Texas?” and “New Mexico” (Clarke et al., 2010; Liang et al., 2013; Yih et al., 2015). One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the knowledge base alone, with the help of heuristics developed for the specific domain. For instance, when the question is rela"
D16-1029,D13-1057,1,0.141976,"word problems. Compared to the knowledge base question answering problems, one key difference is that a large number (potentially infinitely many) of different equation systems could end up having the same solutions. Without a database or special rules for combining variables and coefficients, the number of candidate equation systems cannot be trimmed effectively, given only the solutions. From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop latent structured prediction models (Yu and Joachims, 2009; Chang et al., 2013; Zettlemoyer and Collins, 2007) to find latent semantic structures which best explain the answer given the question. Our algorithm is also influenced by the discriminative reranking algorithms (Collins, 2000; Ge and Mooney, 2006; Charniak and Johnson, 2005) and models for learning from intractable supervision (Steinhardt and Liang, 2015). Recently, Huang et al. (2016) collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing s"
D16-1029,P05-1022,0,0.0221719,"les for combining variables and coefficients, the number of candidate equation systems cannot be trimmed effectively, given only the solutions. From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop latent structured prediction models (Yu and Joachims, 2009; Chang et al., 2013; Zettlemoyer and Collins, 2007) to find latent semantic structures which best explain the answer given the question. Our algorithm is also influenced by the discriminative reranking algorithms (Collins, 2000; Ge and Mooney, 2006; Charniak and Johnson, 2005) and models for learning from intractable supervision (Steinhardt and Liang, 2015). Recently, Huang et al. (2016) collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing systems. It will be interesting to see if our proposed algorithm can make further improvements using their newly collected dataset.2 3 Problem Definition Table 1 lists all the symbols representing the components in the process. The input algebra word problem"
D16-1029,W10-2903,1,0.937121,"ushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015), where the authors train the algebra solvers using only the solutions with little or no annoated equation systems. We discuss this in detail in Section 4. 1 The new resource and the dataset we used for training is available soon on https://aka.ms/dataimplicit and https://aka.ms/datadraw 298 Solving automatic algebra word problems can be viewed as a semantic parsing task. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (under the name response-driven learning (Clarke et al., 2010)) to knowledge base question answering tasks such as Geoquery (Zelle and Mooney, 1996) and WebQuestions (Berant et al., 2013) or mapping instructions to actions (Artzi and Zettlemoyer, 2013). In these tasks, researchers have shown that it is possible to train a semantic parser only from questionanswer pairs, such as “What is the largest state bordering Texas?” and “New Mexico” (Clarke et al., 2010; Liang et al., 2013; Yih et al., 2015). One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the"
D16-1029,W02-1001,0,0.118847,"used in this paper to formally describe the problem of mapping algebra word problems to equations. 4 Learning from Mixed Supervision We assume that we have two sets: De = {(xe , ye )} and Dm = {(xm , zm )}. De contains the fully annotated equation system ye for each algebra word problem xe , whereas in Dm , we have access to the numerical solution zm to each problem, but not the equation system (ym = ∅). We refer to De as the explicit set and Dm as the implicit set. For the sake of simplicity, we explain our approach by modifying the training procedure of the structured Perceptron algorithm (Collins, 2002).4 As discussed in Section 3, the key challenge of learning from implicit supervision is that the mapping E(y) is one-directional. Therefore, the correct equation system cannot be easily derived from the numerical solution. Intuitively, for data with only implicit supervision, we can explore the structure ˜∈Y space Y and find the best possible derivation y according to the current model. If E(˜ y) matches z, ˜ . Following then we can update the model based on y this intuition, we propose MixedSP (Algorithm 1). For each example, we use an approximate search algorithm to collect top scoring cand"
D16-1029,P06-2034,0,0.019982,"atabase or special rules for combining variables and coefficients, the number of candidate equation systems cannot be trimmed effectively, given only the solutions. From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop latent structured prediction models (Yu and Joachims, 2009; Chang et al., 2013; Zettlemoyer and Collins, 2007) to find latent semantic structures which best explain the answer given the question. Our algorithm is also influenced by the discriminative reranking algorithms (Collins, 2000; Ge and Mooney, 2006; Charniak and Johnson, 2005) and models for learning from intractable supervision (Steinhardt and Liang, 2015). Recently, Huang et al. (2016) collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing systems. It will be interesting to see if our proposed algorithm can make further improvements using their newly collected dataset.2 3 Problem Definition Table 1 lists all the symbols representing the components in the process. Th"
D16-1029,D14-1058,0,0.44745,"Missing"
D16-1029,P16-1084,0,0.323797,"en only the solutions. From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop latent structured prediction models (Yu and Joachims, 2009; Chang et al., 2013; Zettlemoyer and Collins, 2007) to find latent semantic structures which best explain the answer given the question. Our algorithm is also influenced by the discriminative reranking algorithms (Collins, 2000; Ge and Mooney, 2006; Charniak and Johnson, 2005) and models for learning from intractable supervision (Steinhardt and Liang, 2015). Recently, Huang et al. (2016) collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing systems. It will be interesting to see if our proposed algorithm can make further improvements using their newly collected dataset.2 3 Problem Definition Table 1 lists all the symbols representing the components in the process. The input algebra word problem is denoted by x, and the output y = (T, A) is called a derivation, which consists of an equation system template"
D16-1029,P14-1026,0,0.346527,"a textual number (e.g., four) in a word problem. Let Q(x) be all the textual numbers in the problem x, and C(T ) be the coefficients to be determined in the template T . An alignment is a set of tuples A = {(q, c) |q ∈ Q(x), c ∈ C(T ) ∪ {}}, where the tuple (q, ) indicates that the number q is not relevant to the final equation system. By specifying the value of each coefficient, it identifies an equation system belonging to the family represented by template T . Together, T and A generate a complete equation system, and the solution z can be derived by the mathematical engine E. Following (Kushman et al., 2014; Zhou et al., 2015), our strategy of mapping a word problem to an equation system is to first choose a template that consists of variables and coefficients, and then align each coefficient to a textual number mentioned in the problem. We formulate the mapping between an algebra word problem and an equation system as a structured learning problem. The output space is the set of all possible derivations using templates that are observed in the training data. Our model maps x to y = (T, A) by a linear scoring function wT Φ(x, y), where w is the model parameters and Φ is the feature functions. At"
D16-1029,J13-2005,0,0.0262744,"semantic parsing task. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (under the name response-driven learning (Clarke et al., 2010)) to knowledge base question answering tasks such as Geoquery (Zelle and Mooney, 1996) and WebQuestions (Berant et al., 2013) or mapping instructions to actions (Artzi and Zettlemoyer, 2013). In these tasks, researchers have shown that it is possible to train a semantic parser only from questionanswer pairs, such as “What is the largest state bordering Texas?” and “New Mexico” (Clarke et al., 2010; Liang et al., 2013; Yih et al., 2015). One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the knowledge base alone, with the help of heuristics developed for the specific domain. For instance, when the question is relatively simple and does not have complex compositional structure, paths in the knowledge graph that connect the answers and the entities in the narrative can be interpreted as legitimate semantic parses. However, as we will show in our experiments, learning from implicit supervision alone is not"
D16-1029,P14-5010,0,0.0039774,"Missing"
D16-1029,D15-1202,0,0.350821,"Missing"
D16-1029,D15-1135,0,0.350252,"elated Work Automatically solving mathematical reasoning problems expressed in natural language has been a long-studied problem (Bobrow, 1964; Newell et al., 1959; Mukherjee and Garain, 2008). Recently, Kushman et al. (2014) created a template-base search procedure to map word problems into equations. Then, several following papers studied different aspects of the task: Hosseini et al. (2014) focused on improving the generalization ability of the solvers by leveraging extra annotations; Roy and Roth (2015) focused on how to solve arithmetic problems without using any pre-defined template. In (Shi et al., 2015), the authors focused on number word problems and proposed a system that is created using semi-automatically generated rules. In Zhou et al. (2015), the authors simplified the inference procedure and pushed the state-of-the-art benchmark accuracy. The idea of learning from implicit supervision is discussed in (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015), where the authors train the algebra solvers using only the solutions with little or no annoated equation systems. We discuss this in detail in Section 4. 1 The new resource and the dataset we used for training is a"
D16-1029,P15-1128,1,0.261412,"sk. In the semantic parsing community, the technique of learning from implicit supervision signals has been applied (under the name response-driven learning (Clarke et al., 2010)) to knowledge base question answering tasks such as Geoquery (Zelle and Mooney, 1996) and WebQuestions (Berant et al., 2013) or mapping instructions to actions (Artzi and Zettlemoyer, 2013). In these tasks, researchers have shown that it is possible to train a semantic parser only from questionanswer pairs, such as “What is the largest state bordering Texas?” and “New Mexico” (Clarke et al., 2010; Liang et al., 2013; Yih et al., 2015). One key reason that such implicit supervision is effective is because the correct semantic parses of the questions can often be found using the answers and the knowledge base alone, with the help of heuristics developed for the specific domain. For instance, when the question is relatively simple and does not have complex compositional structure, paths in the knowledge graph that connect the answers and the entities in the narrative can be interpreted as legitimate semantic parses. However, as we will show in our experiments, learning from implicit supervision alone is not a viable strategy"
D16-1029,D07-1071,0,0.220524,"red to the knowledge base question answering problems, one key difference is that a large number (potentially infinitely many) of different equation systems could end up having the same solutions. Without a database or special rules for combining variables and coefficients, the number of candidate equation systems cannot be trimmed effectively, given only the solutions. From the algorithmic point of view, our proposed learning framework is related to several lines of work. Similar efforts have been made to develop latent structured prediction models (Yu and Joachims, 2009; Chang et al., 2013; Zettlemoyer and Collins, 2007) to find latent semantic structures which best explain the answer given the question. Our algorithm is also influenced by the discriminative reranking algorithms (Collins, 2000; Ge and Mooney, 2006; Charniak and Johnson, 2005) and models for learning from intractable supervision (Steinhardt and Liang, 2015). Recently, Huang et al. (2016) collected a large number of noisily annotated word problems from online forums. While they collected a large-scale dataset, unlike our work, they did not demonstrate how to utilize the newly crawled dataset to improve existing systems. It will be interesting t"
D16-1029,D15-1096,0,0.678971,"ell et al., 1959; Mukherjee and Garain, 2008). Recently, Kushman et al. (2014) created a template-base search procedure to map word problems into equations. Then, several following papers studied different aspects of the task: Hosseini et al. (2014) focused on improving the generalization ability of the solvers by leveraging extra annotations; Roy and Roth (2015) focused on how to solve arithmetic problems without using any pre-defined template. In (Shi et al., 2015), the authors focused on number word problems and proposed a system that is created using semi-automatically generated rules. In Zhou et al. (2015), the authors simplified the inference procedure and pushed the state-of-the-art benchmark accuracy. The idea of learning from implicit supervision is discussed in (Kushman et al., 2014; Zhou et al., 2015; Koncel-Kedziorski et al., 2015), where the authors train the algebra solvers using only the solutions with little or no annoated equation systems. We discuss this in detail in Section 4. 1 The new resource and the dataset we used for training is available soon on https://aka.ms/dataimplicit and https://aka.ms/datadraw 298 Solving automatic algebra word problems can be viewed as a semantic pa"
D16-1029,Q15-1042,0,\N,Missing
D17-1252,Q13-1005,0,0.0274295,"parse is fully constructed. Early model update before the search of a full semantic parse is complete is generally infeasible.1 It is also not clear how to leverage implicit and explicit signals integrally during learning when both kinds of labels are present. In this work, we propose Maximum Margin Reward Networks (MMRN), which is a general neural network-based framework that is able to learn from both implicit and explicit supervision signals. By casting structured-output learning as a search problem, the key insight in MMRN is the 1 Existing weakly supervised methods (Clarke et al., 2010; Artzi and Zettlemoyer, 2013) often leverage domain-specific heuristics, which are not always available. 2368 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2368–2378 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics special mechanism of rewards. Rewards can be viewed as the training signals that drive the model to explore the search space and to find the correct structure. The explicit supervision signals can be viewed as a source of immediate rewards, as we can often instantly know the correctness of the current action. On the other"
D17-1252,D13-1160,0,0.33632,"on 1” (Y (s)). By comparing Y (s) to the answer set A, the precision is 16 and the recall is 1. Therefore, the F1 score used for the reward is 27 . action a to state s0 . Let Y (s) be the set of predicted answers generated from state s, and Y (s) = {} when s is not a legitimate semantic parse. The reward function R(s0 , a) can be defined by comparing Y (s) and the labeled answers, A, to the input question. While a set similarity function like the Jaccard coefficient can be used as the reward function, we chose the F1 score in this work as it was used as the evaluation metric in previous work (Berant et al., 2013). Figure 3 shows an example of this reward function. 4.2 Max-Margin Loss & Learning Algorithm The MMRN learning algorithm can be viewed as an extension of M3 N (Taskar et al., 2004) and Structured SVM (Joachims et al., 2009; Yu and Joachims, 2009). The learning algorithm takes three steps, where the first two involve two different search procedures. The final step is to update the models with respect to the inference results. Finding the best path The first search step is to find the best path h∗ by solving the following optimization problem: h∗ = arg max R(h; y) + fθ (h). h∈E(x) (1) The firs"
D17-1252,W10-2903,1,0.538838,"ledge base) after the parse is fully constructed. Early model update before the search of a full semantic parse is complete is generally infeasible.1 It is also not clear how to leverage implicit and explicit signals integrally during learning when both kinds of labels are present. In this work, we propose Maximum Margin Reward Networks (MMRN), which is a general neural network-based framework that is able to learn from both implicit and explicit supervision signals. By casting structured-output learning as a search problem, the key insight in MMRN is the 1 Existing weakly supervised methods (Clarke et al., 2010; Artzi and Zettlemoyer, 2013) often leverage domain-specific heuristics, which are not always available. 2368 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2368–2378 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics special mechanism of rewards. Rewards can be viewed as the training signals that drive the model to explore the search space and to find the correct structure. The explicit supervision signals can be viewed as a source of immediate rewards, as we can often instantly know the correctness of th"
D17-1252,W02-1001,0,0.0461079,"start the description of our method from the search formulation and the state–action spaces in our targeted tasks in Sec. 3, followed by the reward and learning algorithm in Sec. 4 and the detailed neural model design in Sec. 5. Sec. 6 reports the experimental results and Sec. 8 concludes the paper. 2 Related Work Structured output prediction tasks have been studied extensively in the field of natural language processing (NLP). Many supervised structured learning algorithms has been proposed for capturing the relationships between output variables. These models include structured perceptron (Collins, 2002; Collins and Roark, 2004), conditional random fields (Lafferty et al., 2001), and structured SVM (Taskar et al., 2004; Joachims et al., 2009). Later, the learning to search framework is proposed (Daum´e and Marcu, 2005; Daum´e et al., 2009), which casts the structured prediction task as a general search problem. Most recently, recurrent neural networks such as LSTM models (Hochreiter and Schmidhuber, 1997) have been used as a general tool for structured output models (Vinyals et al., 2015). Latent structured learning algorithms address the problem of learning from incomplete labeled data (Yu"
D17-1252,P04-1015,0,0.0198179,"ription of our method from the search formulation and the state–action spaces in our targeted tasks in Sec. 3, followed by the reward and learning algorithm in Sec. 4 and the detailed neural model design in Sec. 5. Sec. 6 reports the experimental results and Sec. 8 concludes the paper. 2 Related Work Structured output prediction tasks have been studied extensively in the field of natural language processing (NLP). Many supervised structured learning algorithms has been proposed for capturing the relationships between output variables. These models include structured perceptron (Collins, 2002; Collins and Roark, 2004), conditional random fields (Lafferty et al., 2001), and structured SVM (Taskar et al., 2004; Joachims et al., 2009). Later, the learning to search framework is proposed (Daum´e and Marcu, 2005; Daum´e et al., 2009), which casts the structured prediction task as a general search problem. Most recently, recurrent neural networks such as LSTM models (Hochreiter and Schmidhuber, 1997) have been used as a general tool for structured output models (Vinyals et al., 2015). Latent structured learning algorithms address the problem of learning from incomplete labeled data (Yu and Joachims, 2009; Quatto"
D17-1252,P16-1004,0,0.0327797,"there are no gold parses, a model needs to explore different parses, where their quality can only be indirectly verified by comparing retrieved answers and the labeled answers. Introduction Structured-output prediction problems, where the goal is to determine values of a set of interdependent variables, are ubiquitous in NLP. Structures of such problems can range from simple sequences like part-of-speech tagging (Ling et al., 2015) and named entity recognition (Lample et al., 2016), to complex syntactic or semantic analysis such as dependency parsing (Dyer et al., 2015) and semantic parsing (Dong and Lapata, 2016). Stateof-the-art methods of these tasks are often neural network models trained using fully annotated structures, which can be costly or time-consuming to obtain. Weakly supervised learning settings, where the algorithm assumes only the existence of implicit signals on whether a prediction is correct, are thus more appealing in many scenarios. For example, Figure 1 shows a weakly supervised setting of learning semantic parsers using only question–answer pairs. When the system generates a candidate semantic parse during training, the quality needs to be indirectly measured by comparing the der"
D17-1252,P15-1033,0,0.0085205,"ervision signals (labeled answers). Since there are no gold parses, a model needs to explore different parses, where their quality can only be indirectly verified by comparing retrieved answers and the labeled answers. Introduction Structured-output prediction problems, where the goal is to determine values of a set of interdependent variables, are ubiquitous in NLP. Structures of such problems can range from simple sequences like part-of-speech tagging (Ling et al., 2015) and named entity recognition (Lample et al., 2016), to complex syntactic or semantic analysis such as dependency parsing (Dyer et al., 2015) and semantic parsing (Dong and Lapata, 2016). Stateof-the-art methods of these tasks are often neural network models trained using fully annotated structures, which can be costly or time-consuming to obtain. Weakly supervised learning settings, where the algorithm assumes only the existence of implicit signals on whether a prediction is correct, are thus more appealing in many scenarios. For example, Figure 1 shows a weakly supervised setting of learning semantic parsers using only question–answer pairs. When the system generates a candidate semantic parse during training, the quality needs t"
D17-1252,Q14-1021,1,0.761333,"Missing"
D17-1252,D13-1085,0,0.0200214,"am search, and the reward function is simply the number of correct tag assignments to the words. The results are shown in Table 1, compared with recently proposed systems based on neural models. When the beam size is set to 20, MMRN achieves 91.4, which is the best published result so far (without using any gazetteers). Notice that when beam size is 5, the performance drops to 90.03. This demonstrates the importance of search quality when applying MMRN. 6.2 Entity linking For entity linking, we adopt two publicly available datasets for tweet entity linking: NEEL (Cano et al., 2014)4 and TACL (Guo et al., 2013; Fang 3 Available at http://nlp.stanford.edu/projects/glove/ 4 NEEL dataset was originally created for an entity linking competition: http://microposts2016.seas. upenn.edu/challenge.html and Chang, 2014; Yang and Chang, 2015; Yang et al., 2016). We follow prior works (Guo et al., 2013; Yang and Chang, 2015) and perform the standard evaluation for an end-to-end entity linking system by computing precision, recall, and F1 scores, according to the entity references and the system output. An output entity is considered correct if it matches the gold entity and the mention boundary overlaps with t"
D17-1252,P17-1167,1,0.0701783,"RN can be more efficient than REINFORCE, as MMRN can use the reward signals of multiple paths more effectively. In addition, MMRN is not a probabilistic model, so it does not need to handle normalization issues, which often causes large variance in estimating the gradient direction when optimizing the expected reward. Semantic Parsing MMRN can be applied for many semantic parsing tasks. One key step is to design the right approximated reward for a given task to guide the beam search to nd the reference parses in MMRN, given that the actual reward is often very sparse. In our companion paper, (Iyyer et al., 2017), we used a simple form of approximated reward to get feedback as early as possible during search. In other words, the semantic parse will be executed as soon as the parse is executable (even if the parse is still not completed) during search. The execution results will be used to calculate the Jaccard coefficient with respect to the labeled answers as the approximated rewards. The use of approximated reward has been proven to be effective in (Iyyer et al., 2017). An important research direction for semantic parsing is to reduce the supervision cost. In (Yih et al., 2016), the authors demonstr"
D17-1252,N16-1030,0,0.00488354,"ner, Alec Sulkin KB A: Lacey Chabert Figure 1: Learning a semantic parser using implicit supervision signals (labeled answers). Since there are no gold parses, a model needs to explore different parses, where their quality can only be indirectly verified by comparing retrieved answers and the labeled answers. Introduction Structured-output prediction problems, where the goal is to determine values of a set of interdependent variables, are ubiquitous in NLP. Structures of such problems can range from simple sequences like part-of-speech tagging (Ling et al., 2015) and named entity recognition (Lample et al., 2016), to complex syntactic or semantic analysis such as dependency parsing (Dyer et al., 2015) and semantic parsing (Dong and Lapata, 2016). Stateof-the-art methods of these tasks are often neural network models trained using fully annotated structures, which can be costly or time-consuming to obtain. Weakly supervised learning settings, where the algorithm assumes only the existence of implicit signals on whether a prediction is correct, are thus more appealing in many scenarios. For example, Figure 1 shows a weakly supervised setting of learning semantic parsers using only question–answer pairs."
D17-1252,W16-3920,0,0.0145837,"n scoring model for EL. The model of the action scoring function fθ (s, a) is depicted in Figure 4, which is basically the dot product of the action embedding and state embedding. The action embedding is initialized randomly for each action, but can be fine-tuned during training (i.e. back-propagate the error through the network and update the word/entity type embeddings). The state embedding is the concatenation of bi-LSTM word embeddings of the current word, the character-based word embeddings, and the embedding of the previous action. We also include the orthographic embeddings proposed by Limsopatham and Collier (2016). Entity Linking An action in entity linking is to determine whether a mention should be linked to a particular entity (cf. Sec. 3.2). As shown in Figure 5, we design the scoring function as a feed-forward neural network that takes as input three different input vectors: (1) surface features from hand-crafted mention-entity statistics that are similar to the ones used in (Yang and Chang, 2015); (2) mention context embeddings from a bidirectional LSTM module; (3) entity embeddings constructed from entity type embeddings. All these embeddings, except the feature vectors, are fine-tuned during tr"
D17-1252,P16-1101,0,0.0198869,"Missing"
D17-1252,D16-1261,0,0.00878913,"d function L(h, h∗ ) is our optimization goal, where we want can be thought of as estimating whether there exto update the model by fixing the biggest violation. ists a high-reward state that is reachable from the Note that the associated constraint is only violated ∗ current state. The effectiveness of this strategy has when L(h, h ) is positive. To find the path h in been demonstrated successfully by several recent this step that maximizes the violation is equivalent efforts (Mnih et al., 2013; Krishnamurthy et al., to maximizing fθ (h) − R(h), given that the rest 2015; Silver et al., 2016; Narasimhan et al., 2016). of the terms are constant with respect to h. When there exist only explicit supervision sig5 Neural Architectures nals, our objective function reduces to the one for optimizing structured SVM without regularWhile the learning algorithm of MMRN described in ization. For implicit signals, we find h∗ approxiSec. 4 is general, the exact model design is taskmately before we optimize the margin loss. In this dependent. In this section, we describe in detail case, the search is not exact as the reward signals the neural network architectures of the three tarare delayed. Nevertheless, we found the m"
D17-1252,D14-1162,0,0.103477,".59 90.10 90.77 90.88 90.94 91.21 MMRN-NER Beam = 5 MMRN-NER Beam = 20 90.03 91.39 Table 1: Explicit Supervision: Named Entity Recognition. Our MMRN with beam size 20 outperforms current best systems, which are based on neural networks. NEEL-Test F1 S-MART 77.7 77.9 NTEL MMRN-EL 78.5 MMRN-EL - Entity 77.4 76.6 MMRN-EL - LSTM TACL F1 63.6 68.1 67.5 66.5 66.0 Table 2: Explicit Supervision: Entity Linking. Our system trained with MMRN is comparable to the state-of-art NTEL system. metric is the F1 score. The pre-trained word embeddings are 100-dimension GloVe vectors trained on 6 billion tokens (Pennington et al., 2014)3 . The search procedure is conducted using beam search, and the reward function is simply the number of correct tag assignments to the words. The results are shown in Table 1, compared with recently proposed systems based on neural models. When the beam size is set to 20, MMRN achieves 91.4, which is the best published result so far (without using any gazetteers). Notice that when beam size is 5, the performance drops to 90.03. This demonstrates the importance of search quality when applying MMRN. 6.2 Entity linking For entity linking, we adopt two publicly available datasets for tweet entity"
D17-1252,W09-1119,0,0.010576,"Missing"
D17-1252,D16-1029,1,0.850528,"h framework is proposed (Daum´e and Marcu, 2005; Daum´e et al., 2009), which casts the structured prediction task as a general search problem. Most recently, recurrent neural networks such as LSTM models (Hochreiter and Schmidhuber, 1997) have been used as a general tool for structured output models (Vinyals et al., 2015). Latent structured learning algorithms address the problem of learning from incomplete labeled data (Yu and Joachims, 2009; Quattoni et al., 2007). The main difference compared to our framework is the existence of the external environment when learning from implicit signals. Upadhyay et al. (2016) first proposed the idea of learning from implicit supervision, and is the most related paper to our work. Compared to their linear algorithm, our framework is more principled and general as we integrate the concept of margin in our method. Furthermore, we also extend the framework using neural models. 3 Search-based Inference In our framework, predicting the best structured output, inference, is formulated as a state/action search problem. Our search space can be described as follows. The initial state, s0 , is the starting point of the search process. We define γ(s) as the set of all feasibl"
D17-1252,P15-1049,1,0.265692,"ncatenation of bi-LSTM word embeddings of the current word, the character-based word embeddings, and the embedding of the previous action. We also include the orthographic embeddings proposed by Limsopatham and Collier (2016). Entity Linking An action in entity linking is to determine whether a mention should be linked to a particular entity (cf. Sec. 3.2). As shown in Figure 5, we design the scoring function as a feed-forward neural network that takes as input three different input vectors: (1) surface features from hand-crafted mention-entity statistics that are similar to the ones used in (Yang and Chang, 2015); (2) mention context embeddings from a bidirectional LSTM module; (3) entity embeddings constructed from entity type embeddings. All these embeddings, except the feature vectors, are fine-tuned during training. Some unique properties of our entity linking model are worth noticing. First, we add mention context embeddings from a bidirectional LSTM module as additional input. While using LSTMs is a common practice for sequence labeling, it is not usually used for short-text entity linking. For each mention, we only extract the output from the bi-LSTM module at the start and end tokens of the me"
D17-1252,D16-1152,1,0.83286,"1.4, which is the best published result so far (without using any gazetteers). Notice that when beam size is 5, the performance drops to 90.03. This demonstrates the importance of search quality when applying MMRN. 6.2 Entity linking For entity linking, we adopt two publicly available datasets for tweet entity linking: NEEL (Cano et al., 2014)4 and TACL (Guo et al., 2013; Fang 3 Available at http://nlp.stanford.edu/projects/glove/ 4 NEEL dataset was originally created for an entity linking competition: http://microposts2016.seas. upenn.edu/challenge.html and Chang, 2014; Yang and Chang, 2015; Yang et al., 2016). We follow prior works (Guo et al., 2013; Yang and Chang, 2015) and perform the standard evaluation for an end-to-end entity linking system by computing precision, recall, and F1 scores, according to the entity references and the system output. An output entity is considered correct if it matches the gold entity and the mention boundary overlaps with the gold mention boundary. Interested readers can refer to (Carmel et al., 2014) for more detail. We initialize the word embeddings from pretrained GloVe vectors trained on the twitter corpus, and type embeddings from the pre-trained skip-gram mo"
D17-1252,P15-1128,1,0.669068,"ample question “who played meg in season 1 of family guy”, assuming the knowledge base is Freebase (Bollacker et al., 2008). An entity linking component plays an important role by mapping “meg” to MegGriffin and “season 1 of family guy” to FamilyGuySeason1. Predicates like cast, actor and character are also from the knowledge base that define the relationships between these entities and the answer. Together the semantic parse in λ-calculus is shown in the top of Figure 2. Equivalently, the semantic parse can be represented as a query graph (Figure 2 bottom), which is used in the STAGG system (Yih et al., 2015). The nodes are either grounded entities or variables, where x is the answer entity. The edges denote the relationship between two entities. Regardless of the choice of the formal language, the process of constructing the semantic parse is typically formulated as a search problem. A state is essentially a partial or complete semantic parse, and an action is to extend the current semantic parse by adding a new relation or constraint. Different from previous systems which treat entity linking as a static component, our search space consists of the search space of both entity linking and semantic"
D17-1252,P16-2033,1,0.39892,"thm. Results of entity linking experiments are presented in Table 2, which are compared with those of S-MART (Yang and Chang, 2015)6 and NTEL (Yang et al., 2016)7 , two state-of-the-art entity linking systems for short texts. Our MMRN-EL is comparable to the best system. We also conducted two ablation studies by removing the entity type vectors (MMRN-EL - Entity), and by removing the LSTM vectors (MMRN-EL - LSTM). Both show significant performance drops, which validates the importance of these two additional input vectors. 6.3 Semantic parsing For semantic parsing, we use the dataset WebQSP8 (Yih et al., 2016) in our experiments. This dataset is a clean and enhanced version of the widely used WebQuestions dataset (Berant et al., 2013), which consists of pairs of questions and answers found in Freebase. Compared to WebQuestions, WebQSP excludes questions with ambiguous intent, and provides verified answers and full semantic parses to the remaining 4,737 questions. We follow the implicit supervision setting in (Yih et al., 2016), using 3, 098 question–answer pairs for training, and 1, 639 for testing. A subset of 620 pairs from the training set is used for hyperparameter tuning. Because there can be"
D18-1006,D14-1159,1,0.83622,"entific processes (Dalvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018). Our work extends these methods and addresses their most common issues by using background knowledge about entities to prune the set of state changes they can experience as the model reads new text. Prior to these neural approaches, some earlier systems for process comprehension did make use of world knowledge, and motivated this work. Like us, the system ProRead (Berant et al., 2014; Scaria et al., 2013) also treated process comprehension as structure prediction, using an Integer Linear Programming (ILP) formalism to enforce global constraints (e.g., if the result of event1 is the agent of event2, then event1 must enable event2). Similarly, Kiddon et al. (2015) used corpus-based priors to guide extraction of an “action graph” from recipes. 3 Problem Definition We first define the general task that we are addressing, before presenting our approach. 3.1 General Formulation We define the task as follows. Given: • A paragraph of procedural text S = an ordered set of sentence"
D18-1006,N18-1144,1,0.679082,"locations at each sentence, but the implied movements violate commonsense constraints (e.g., an object cannot move from itself (1)) and corpus-based preferences (e.g., it is rare to see turbines move (2)). 2018), and NPN (Bosselut et al., 2018) – has focused on learning to predict individual entity states at various points in the text, thereby approximating the underlying dynamics of the world. However, while these models can learn to make local predictions with fair accuracy, their results are often globally unlikely or inconsistent. For example, in Figure 1, the neural ProGlobal model from Dalvi et al. (2018) learns to predict the impossible action of an object moving from itself (1), and the unlikely action of a turbine changing location (2). We observe similar mistakes in other neural models, indicating that these models have little notion of global consistency. Unsurprisingly, mistakes in local predictions compound as the process becomes longer, further reducing the plausibility of the overall result. Introduction Procedural text is ubiquitous (e.g., scientific protocols, news articles, how-to guides, recipes), but is challenging to comprehend because of the dynamic nature of the world being de"
D18-1006,T87-1035,0,0.755647,"Missing"
D18-1006,P16-5005,0,0.0195,"te structures using hard constraints and soft KB priors. Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017). In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objective for dialogue (Wen et al., 2015), machine translation (Tu et al., 2016), and recipe generation (Kiddon et al., 2016). Our work instead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function. Related Work Our work builds off a recent body of work that focuses on using neural networks to explicitly track the states of entities while reading long texts. These works have focused on answering simple commonsense questions (Henaff et al., 2017), tracking entity states in scientific processes (Dalvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional"
D18-1006,P17-1141,0,0.0210284,"until an answer can be provided. Although this context is somewhat different (the primary goal being state discovery from weak dialog evidence), state tracking techniques originally designed for procedural text have been successfully applied in this context also (Liu and Perez, 2017). Finally, our model learns to search over the best candidate structures using hard constraints and soft KB priors. Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017). In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objective for dialogue (Wen et al., 2015), machine translation (Tu et al., 2016), and recipe generation (Kiddon et al., 2016). Our work instead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function. Related Work Our work builds off a recent body of work that focuses on using neural networks"
D18-1006,D15-1199,0,0.0191058,"l learns to search over the best candidate structures using hard constraints and soft KB priors. Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017). In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objective for dialogue (Wen et al., 2015), machine translation (Tu et al., 2016), and recipe generation (Kiddon et al., 2016). Our work instead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function. Related Work Our work builds off a recent body of work that focuses on using neural networks to explicitly track the states of entities while reading long texts. These works have focused on answering simple commonsense questions (Henaff et al., 2017), tracking entity states in scientific processes (Dalvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et"
D18-1006,D15-1114,0,0.0926547,"r most common issues by using background knowledge about entities to prune the set of state changes they can experience as the model reads new text. Prior to these neural approaches, some earlier systems for process comprehension did make use of world knowledge, and motivated this work. Like us, the system ProRead (Berant et al., 2014; Scaria et al., 2013) also treated process comprehension as structure prediction, using an Integer Linear Programming (ILP) formalism to enforce global constraints (e.g., if the result of event1 is the agent of event2, then event1 must enable event2). Similarly, Kiddon et al. (2015) used corpus-based priors to guide extraction of an “action graph” from recipes. 3 Problem Definition We first define the general task that we are addressing, before presenting our approach. 3.1 General Formulation We define the task as follows. Given: • A paragraph of procedural text S = an ordered set of sentences {s1 , ..., sT } describing 58 dataset. Figure 2 gives a (simplified) example of the data, visualized as an (entity x sentence) grid, where each column tracks a different entity (time progressing vertically downwards), and each row denotes the entities’ state (existence and location"
D18-1006,D16-1032,0,0.0476308,"soft KB priors. Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017). In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objective for dialogue (Wen et al., 2015), machine translation (Tu et al., 2016), and recipe generation (Kiddon et al., 2016). Our work instead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function. Related Work Our work builds off a recent body of work that focuses on using neural networks to explicitly track the states of entities while reading long texts. These works have focused on answering simple commonsense questions (Henaff et al., 2017), tracking entity states in scientific processes (Dalvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in s"
D18-1006,D16-1137,0,0.027991,"nd nonsensical predictions. Second, we present a novel, end-to-end model that integrates these constraints and achieves state-of-the-art performance on an existing process comprehension dataset (Dalvi et al., 2018). 2 Our work here can viewed as incorporating these approaches within the neural paradigm. Neural methods for structure prediction have been used extensively in other areas of NLP, and we leverage these methods here. In particular we use a neural encoder-decoder architecture with beam search decoding, representative of several current state-of-the-art systems (Bahdanau et al., 2014; Wiseman and Rush, 2016; Vinyals et al., 2015). As our model’s only supervision signal comes from the final prediction (of state changes), our work is similar to previous work in semantic parsing that extracts structured outputs from text with no intermediate supervision (Krishnamurthy et al., 2017). State tracking also appears in other areas of AI, such as dialog. A typical dialog state tracking task (e.g., the DSTC competitions) involves gradually uncovering the user’s state (e.g., their constraints, preferences, and goals for booking a restaurant), until an answer can be provided. Although this context is somewha"
D18-1006,D17-1160,0,0.131181,"ches within the neural paradigm. Neural methods for structure prediction have been used extensively in other areas of NLP, and we leverage these methods here. In particular we use a neural encoder-decoder architecture with beam search decoding, representative of several current state-of-the-art systems (Bahdanau et al., 2014; Wiseman and Rush, 2016; Vinyals et al., 2015). As our model’s only supervision signal comes from the final prediction (of state changes), our work is similar to previous work in semantic parsing that extracts structured outputs from text with no intermediate supervision (Krishnamurthy et al., 2017). State tracking also appears in other areas of AI, such as dialog. A typical dialog state tracking task (e.g., the DSTC competitions) involves gradually uncovering the user’s state (e.g., their constraints, preferences, and goals for booking a restaurant), until an answer can be provided. Although this context is somewhat different (the primary goal being state discovery from weak dialog evidence), state tracking techniques originally designed for procedural text have been successfully applied in this context also (Liu and Perez, 2017). Finally, our model learns to search over the best candid"
D18-1006,P17-1041,0,0.0280129,"te change is predicted, then the model also must predict its parameter values from the paragraph. Previous models for process comprehension make a sequence of local predictions about the entities’ states, one sentence at a time, maintaining a (typically neural) state at each sentence. However, none have the ability to reverse earlier predictions should an inconsistency arise later in the sequence. ProStruct overcomes this limitation by reformulating the task as structured prediction. To do this, it uses a neural encoder-decoder from the semantic parsing literature (Krishnamurthy et al., 2017; Yin and Neubig, 2017) combined with a search procedure that integrates soft and hard constraints for finding the best candidate structure. For each sentence and entity, the encoder first uses a bidirectional LSTM to encode the sentence and indicator variables identifying which entity is currently being considered (Figure 3). It then produces a (distributed) representation of the action that the sentence describes as being applied to that entity. During decoding, the model decodes each action embedding into a distribution over possible state changes that might result, then performs actions1 a sequence of about a gi"
D18-1006,E17-1029,0,0.0265098,"utputs from text with no intermediate supervision (Krishnamurthy et al., 2017). State tracking also appears in other areas of AI, such as dialog. A typical dialog state tracking task (e.g., the DSTC competitions) involves gradually uncovering the user’s state (e.g., their constraints, preferences, and goals for booking a restaurant), until an answer can be provided. Although this context is somewhat different (the primary goal being state discovery from weak dialog evidence), state tracking techniques originally designed for procedural text have been successfully applied in this context also (Liu and Perez, 2017). Finally, our model learns to search over the best candidate structures using hard constraints and soft KB priors. Previous work in Neural Machine Translation (NMT) has used sets of example-specific lexical constraints in beam search decoding to only produce translations that satisfy every constraint in the set (Hokamp and Liu, 2017). In contrast, our work uses a set of global example-free constraints to prune the set of possible paths the search algorithm can explore. Simultaneously, a recent body of work has explored encoding soft constraints as an additional loss term in the training objec"
D18-1006,D14-1162,0,0.0924963,"ly destroy one entity and create another. Figure 3 shows the encoder operating on s4 : “The generator spins, and produces electricity” and e3 : electricity from Figure 1. Without loss of generality, we define an arbitrary sentence in S as st = {w0 , ..., wI }. Each word wi in the input sentence is encoded as a vector xi = [vw : ve : vv ], which is the concatenation of a pre-trained word embedding vw for wi , an indicator variable ve for whether wi is a reference to the specified entity e j , and an indicator variable vv for whether wi is a verb. We use GloVe vectors as pre-trained embeddings (Pennington et al., 2014) and a POS tagger to extract verbs (Spacy, 2018). Then, a BiLSTM is used to encode the word representations extracted above, yielding a contextualized vector hi for each embedded word xi that is the concatenated output of the backward and forward hidden states produced by the BiLSTM for word wi . An attention over the contextualized embeddings hi is performed to predict a distribution of weights over the sentence: Figure 3: The encoder, illustrated for the ProPara domain with the paragraph from Figure 1. During encoding, ProStruct creates an action embedding ct j representing the action at ste"
D18-1006,P18-1213,1,0.813229,"ead uses soft constraints to re-rank candidate structures and is not directly encoded in the loss function. Related Work Our work builds off a recent body of work that focuses on using neural networks to explicitly track the states of entities while reading long texts. These works have focused on answering simple commonsense questions (Henaff et al., 2017), tracking entity states in scientific processes (Dalvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018). Our work extends these methods and addresses their most common issues by using background knowledge about entities to prune the set of state changes they can experience as the model reads new text. Prior to these neural approaches, some earlier systems for process comprehension did make use of world knowledge, and motivated this work. Like us, the system ProRead (Berant et al., 2014; Scaria et al., 2013) also treated process comprehension as structure prediction, using an Integer Linear Programming (ILP) formalism to enforce global constraints (e.g., if the result of event1 is the agent of e"
D18-1006,D13-1177,1,0.77852,"lvi et al., 2018; Clark et al., 2018), tracking ingredients in cooking recipes (Bosselut et al., 2018), and tracking the emotional reactions and motivations of characters in simple stories (Rashkin et al., 2018). Our work extends these methods and addresses their most common issues by using background knowledge about entities to prune the set of state changes they can experience as the model reads new text. Prior to these neural approaches, some earlier systems for process comprehension did make use of world knowledge, and motivated this work. Like us, the system ProRead (Berant et al., 2014; Scaria et al., 2013) also treated process comprehension as structure prediction, using an Integer Linear Programming (ILP) formalism to enforce global constraints (e.g., if the result of event1 is the agent of event2, then event1 must enable event2). Similarly, Kiddon et al. (2015) used corpus-based priors to guide extraction of an “action graph” from recipes. 3 Problem Definition We first define the general task that we are addressing, before presenting our approach. 3.1 General Formulation We define the task as follows. Given: • A paragraph of procedural text S = an ordered set of sentences {s1 , ..., sT } desc"
D18-1179,P17-1080,0,0.0280108,"studies have examined the learned representations in RNNs. Karpathy et al. (2015) trained a character LSTM language model on source code and showed that individual neurons in the hidden state track the beginning and end of code blocks. Linzen et al. (2016) assessed whether RNNs can learn number agreement in subject-verb dependencies. Our analysis in Sec. 5.1 showed that biLMs also learn number agreement for coreference. K´ad´ar et al. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions. Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions, Gaddy et al. (2018) analyzed neural constituency parsers, Blevins et al. (2018) explored whether RNNs trained with several different objectives can learn hierarchical syntax, and Conneau et al. (2018) examined to what extent sentence representations capture linguistic features. Our intrinsic analysis is most similar to Belinkov et al. (2017); however, we probe s"
D18-1179,P17-1152,0,0.0610817,"Missing"
D18-1179,P18-1198,0,0.058197,"l. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions. Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions, Gaddy et al. (2018) analyzed neural constituency parsers, Blevins et al. (2018) explored whether RNNs trained with several different objectives can learn hierarchical syntax, and Conneau et al. (2018) examined to what extent sentence representations capture linguistic features. Our intrinsic analysis is most similar to Belinkov et al. (2017); however, we probe span representations in addition to word representations, evaluate the transferability of the biLM representations to semantic tasks in addition to syntax tasks, and consider a wider variety of neural architectures in addition to RNNs. Other work has focused on attributing network predictions. Li et al. (2016) examined the impact of erasing portions of a network’s representations on the output, Sundararajan et al. (2017) used a gradi"
D18-1179,N18-1091,0,0.018106,"assessed whether RNNs can learn number agreement in subject-verb dependencies. Our analysis in Sec. 5.1 showed that biLMs also learn number agreement for coreference. K´ad´ar et al. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions. Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions, Gaddy et al. (2018) analyzed neural constituency parsers, Blevins et al. (2018) explored whether RNNs trained with several different objectives can learn hierarchical syntax, and Conneau et al. (2018) examined to what extent sentence representations capture linguistic features. Our intrinsic analysis is most similar to Belinkov et al. (2017); however, we probe span representations in addition to word representations, evaluate the transferability of the biLM representations to semantic tasks in addition to syntax tasks, and consider a wider variety of neural architectures in addition to RNNs. Other work has focus"
D18-1179,W18-2501,1,0.827835,"Missing"
D18-1179,P17-1044,1,0.84723,"Missing"
D18-1179,P18-1031,0,0.0261358,"Missing"
D18-1179,P18-1027,0,0.0153787,"al neurons in the hidden state track the beginning and end of code blocks. Linzen et al. (2016) assessed whether RNNs can learn number agreement in subject-verb dependencies. Our analysis in Sec. 5.1 showed that biLMs also learn number agreement for coreference. K´ad´ar et al. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions. Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions, Gaddy et al. (2018) analyzed neural constituency parsers, Blevins et al. (2018) explored whether RNNs trained with several different objectives can learn hierarchical syntax, and Conneau et al. (2018) examined to what extent sentence representations capture linguistic features. Our intrinsic analysis is most similar to Belinkov et al. (2017); however, we probe span representations in addition to word representations, evaluate the transferability of the biLM representations to semantic tasks in addition to syntax tasks, and"
D18-1179,P18-1249,0,0.0163139,"yer biLM from that work,1 we also trained a deeper 4-layer model to examine the impact of depth using the publicly available training code.2 To reduce the training time for this large 4-layer model, we reduced the number of parameters in the character encoder by first projecting the character CNN filters down to the model dimension before the two highway layers. 3.2 Transformer The Transformer, introduced by Vaswani et al. (2017), is a feed forward self-attention based architecture. In addition to machine translation, it has also provided strong results for Penn Treebank constituency parsing (Kitaev and Klein, 2018) and semantic role labeling (Tan et al., 2018). Each identical layer in the encoder first computes a multi-headed attention between a given token and all other tokens in the history, then runs a position wise feed forward network. To adapt the Transformer for bidirectional language modeling, we modified a PyTorch based 1 http://allennlp.org/elmo 2 https://github.com/allenai/bilm-tf re-implementation (Klein et al., 2017)3 to mask out future tokens for the forward language model and previous tokens for the backward language model, in a similar manner to the decoder masking in the original implem"
D18-1179,P17-4012,0,0.0309732,"(2017), is a feed forward self-attention based architecture. In addition to machine translation, it has also provided strong results for Penn Treebank constituency parsing (Kitaev and Klein, 2018) and semantic role labeling (Tan et al., 2018). Each identical layer in the encoder first computes a multi-headed attention between a given token and all other tokens in the history, then runs a position wise feed forward network. To adapt the Transformer for bidirectional language modeling, we modified a PyTorch based 1 http://allennlp.org/elmo 2 https://github.com/allenai/bilm-tf re-implementation (Klein et al., 2017)3 to mask out future tokens for the forward language model and previous tokens for the backward language model, in a similar manner to the decoder masking in the original implementation. We adopted hyper-parameters from the “base” configuration in Vaswani et al. (2017), providing six layers of 512 dimensional representations for each direction. Concurrent with our work, Radford et al. (2018) trained a large forward Transformer LM and fine tuned it for a variety of NLP tasks. 3.3 Gated CNN Convolutional architectures have also been shown to provide competitive results for sequence modeling incl"
D18-1179,E17-1117,0,0.0457171,"Missing"
D18-1179,D17-1018,1,0.885266,"Missing"
D18-1179,Q16-1037,0,0.0706718,"l. (2018) showed the biLM based representations outperformed CoVe in all considered tasks, we focus exclusively on biLMs. Liu et al. (2018) proposed using densely connected RNNs and layer pruning to speed up the use of context vectors for prediction. As their method is applicable to other architectures, it could also be combined with our approach. Several prior studies have examined the learned representations in RNNs. Karpathy et al. (2015) trained a character LSTM language model on source code and showed that individual neurons in the hidden state track the beginning and end of code blocks. Linzen et al. (2016) assessed whether RNNs can learn number agreement in subject-verb dependencies. Our analysis in Sec. 5.1 showed that biLMs also learn number agreement for coreference. K´ad´ar et al. (2017) attributed the activation patters of RNNs to input tokens and showed that a RNN language model is strongly sensitive to tokens with syntactic functions. Belinkov et al. (2017) used linear classifiers to determine whether neural machine translation systems learned morphology and POS tags. Concurrent with our work, Khandelwal et al. (2018) studied the role of context in influencing language model predictions,"
D18-1179,D18-1153,0,0.0169762,"mer focuses heavily on the word embedding layer, xk , and the first contextual layer. In all cases, the maximum layer weights occur below the top layers as the most transferable contextual representations tend to occur in the middle layers, while the top layers specialize for language modeling. 6 Related work In addition to biLM-based representations, McCann et al. (2017) learned contextualized vectors with a neural machine translation system (CoVe). However, as Peters et al. (2018) showed the biLM based representations outperformed CoVe in all considered tasks, we focus exclusively on biLMs. Liu et al. (2018) proposed using densely connected RNNs and layer pruning to speed up the use of context vectors for prediction. As their method is applicable to other architectures, it could also be combined with our approach. Several prior studies have examined the learned representations in RNNs. Karpathy et al. (2015) trained a character LSTM language model on source code and showed that individual neurons in the hidden state track the beginning and end of code blocks. Linzen et al. (2016) assessed whether RNNs can learn number agreement in subject-verb dependencies. Our analysis in Sec. 5.1 showed that bi"
D18-1179,P18-1110,1,0.805008,"Missing"
D18-1179,J93-2004,0,0.0613187,"Missing"
D18-1179,N18-1101,0,0.0283744,"Missing"
D18-1179,D14-1162,0,0.0875153,"Missing"
D18-1179,P17-1161,1,0.856209,"Missing"
D18-1179,N18-1202,1,0.937893,"fowicz et al., 2016). Similar to Kim et al. (2015), our character-toword encoder is a five-layer sub-module that first embeds single characters with an embedding layer then passes them through 2048 character n-gram CNN filters with max pooling, two highway layers (Srivastava et al., 2015), and a linear projection down to the model dimension. 2.3 Deep contextual word representations After pre-training on a large data set, the internal representations from the biLM can be transferred to a downstream model of interest as contextual word representations. To effectively use all of the biLM layers, Peters et al. (2018) introduced ELMo word representations, whereby all of the layers are combined with a weighted average pooling operPL ation, ELMok = j=0 sj hk,j . The parameters s are optimized as part of the task model so that it may preferentially mix different types of contextual information represented in different layers of the biLM. In Sec. 4 we evaluate the relative effectiveness of ELMo representations from three different biLM architectures vs. pre-trained word vectors in four different state-of-the-art models. 3 Architectures for deep biLMs The primary design choice when training deep biLMs for learn"
D18-1179,W18-5448,0,0.0904006,"STM model.5 Speed ups are relatively faster in the single element batch scenario where the sequential LSTM is most disadvantaged, but are still 2.3-3X for a 64 sentence batch. As the inference speed for the character based word embeddings could be mostly eliminated in a production setting, the table lists timings for both the contextual layers and all layers of the biLM necessary to compute context vectors. We also note that the faster architectures will allow training to scale to large unlabeled corpora, which has been shown to improve the quality of biLM representations for syntactic tasks (Zhang and Bowman, 2018). 4 Evaluation as word representations In this section, we evaluate the quality of the pre-trained biLM representations as ELMo-like contextual word vectors in state-of-the-art mod5 While the CNN and Transformer implementations are reasonably well optimized, the LSTM biLM is not as it does not use an optimized CUDA kernel due to the use of the projection cell. els across a suite of four benchmark NLP tasks. To do so, we ran a series of controlled trials by swapping out pre-trained GloVe vectors (Pennington et al., 2014) for contextualized word vectors from each biLM computed by applying the le"
D18-1179,W12-4501,0,0.136745,"Missing"
D18-1179,W00-0726,0,0.339652,"Missing"
D18-1241,P18-1078,0,0.0356121,"r all other dialog acts (neither for affirmation and don’t follow up for continuation). Transition matrix We divide the supporting text into 12 chunks (with a special chunk for no answer) and use the transition matrix (computed from the training set) in Figure 5b to select an answer given the position of the previous answer. This baseline does not output other dialog acts. 5.2 Upper bounds Gold NA + TM This is the same transition matrix (TM) baseline as before, except that for questions whose gold annotations are no answer, we always output no answer. et al., 2016, BiDAF) with self-attention (Clark and Gardner, 2018) and contextualized embeddings.16 A token for no answer is appended to s to enable its prediction following Levy et al. (2017). Additionally, we modify the model for our task to also predict dialog acts, placing a classifier over the same representation used to predict the end position of the predicted span. BiDAF++ w/ k-ctx As BiDAF++ does not model any dialog context, we modify the passage and question embedding processes to consider the dialog history. We consider context from the previous k QA pairs.17 • Passage embedding We explicitly identify the previous k answers within the section tex"
D18-1241,D17-1070,0,0.0153231,"ng Naively prepending the previous k questions to the current question did not show gains in initial experiments. We opt instead to simply encode the dialog turn number within the question embedding. 5.4 Results Table 4 summarizes our results (each cell displays dev/test scores), where dialog acts are Yes/No (affirmation) and Follow up (continuation). For comparison to other datasets, we report F1 without filtering low-agreement QA pairs (F1’). Pretrained InferSent To test the importance of lexical matching in our dataset, we output the sentence in s whose pretrained InferSent representation (Conneau et al., 2017) has the highest cosine similarity to that of the question. Sanity check Overall, the poor sanity check results imply that is very challenging. Of these, following the transition matrix (TM) gives the best performance, reinforcing the observation that the dialog context plays a significant role in the task. Feature-rich logistic regression We train a logistic regression using Vowpal Wabbit (Langford et al., 2007) to select answer sentences. We use simple matching features (e.g., n-gram overlap between questions and candidate answers), bias features (position and length of a candidate), and con"
D18-1241,H94-1010,0,0.675779,"Missing"
D18-1241,K17-1034,1,0.798034,"ext into 12 chunks (with a special chunk for no answer) and use the transition matrix (computed from the training set) in Figure 5b to select an answer given the position of the previous answer. This baseline does not output other dialog acts. 5.2 Upper bounds Gold NA + TM This is the same transition matrix (TM) baseline as before, except that for questions whose gold annotations are no answer, we always output no answer. et al., 2016, BiDAF) with self-attention (Clark and Gardner, 2018) and contextualized embeddings.16 A token for no answer is appended to s to enable its prediction following Levy et al. (2017). Additionally, we modify the model for our task to also predict dialog acts, placing a classifier over the same representation used to predict the end position of the predicted span. BiDAF++ w/ k-ctx As BiDAF++ does not model any dialog context, we modify the passage and question embedding processes to consider the dialog history. We consider context from the previous k QA pairs.17 • Passage embedding We explicitly identify the previous k answers within the section text by concatenating marker embeddings to the existing word embeddings. Gold sentence + NA To see if can be treated as an answer"
D18-1241,D17-1259,0,0.0131978,"of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, which takes the form of a teacher-student interaction between two crowd workers, encourages questions that are highly contextual, openended, and even unanswerable from the text. Our baselines, which include top performers on existing machine co"
D18-1241,D16-1127,0,0.0402491,"xt (such as traffic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data c"
D18-1241,P18-2124,1,0.903645,"the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles of teacher and student. To encourage natural and diverse questions, we do not follow previous dialogst"
D18-1241,P17-1162,1,0.821322,"short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, which takes the form of a teacher-student interaction between two crowd workers, encourages questions that are highly contextual, openended, and even unanswerable from the text. Our baselines, which include top performers on existing machine comprehension datasets, significantly"
D18-1241,D16-1264,1,0.889651,", who does not see the section text, asks questions. The teacher provides a response in the form of a text span (or No answer ), optionally yes or no ( Yes / No ), and encouragement about continuing a ¯ , or should line of questioning (should, ,→ , could ,→ not 6,→ ask a follow-up question). Wikipedia page), which only the teacher can access. Given just the section’s heading, “Origin & History”, the student aims to learn as much as possible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016)"
D18-1241,P17-1167,1,0.880503,"ible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles o"
D18-1241,P17-1147,1,0.902703,"hese questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd workers play the roles of teacher and student. To encourage natural and diverse questions,"
D18-1241,D11-1054,0,0.0542229,"ic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing interest in open domain dialog, mostly studied in the context of social chit-chat (Li et al., 2016; Ritter et al., 2011; Fang et al., 2017; Ghazvininejad et al., 2018). Most related to our effort is visual dialog (Das et al., 2017), which relies on images as evidence instead of text. More explicit goal driven scenarios, such as bargaining (Lewis et al., 2017) and item guessing (He et al., 2017) have also been explored, but the language is more constrained than in . Information-seeking dialog specifically was studied in Stede and Schlangen (2004). 7 Conclusion In this paper, we introduce , a large scale dataset of information-seeking dialogs over sections from Wikipedia articles. Our data collection process, wh"
D18-1241,D18-1233,0,0.203396,"are the first to incorporate these into information-seeking dialog. Sequential QA Our work is similar to sequential question answering against knowledge bases (Iyyer et al., 2017) and the web (Talmor and Berant, 2018), but instead of decomposing a single question into smaller questions, we rely on the curiosity of the student to generate a sequence of questions. Such open information seeking was studied in semantic parsing on knowledge bases (Dahl et al., 1994) and more recently with modern approaches (Saha et al., 2018), but with questions paraphrased from templates. Concurrent to our work, Saeidi et al. (2018) proposed a task of generating and answering yes/no questions for rule focused text (such as traffic laws) by interacting with a user through dialog. Also concurrently, Reddy et al. (2018) propose conversational question answering (CoQA) from text but allow both students and questioners to see the evidence. As a result, a large percentage of CoQA answers are named entities or short noun phrases, much like those in SQuAD. In contrast, the asymmetric nature of forces students to ask more exploratory questions whose answers can be potentially be followed up on.19 Dialog fits into an increasing in"
D18-1241,N18-1059,0,0.1584,"aims to learn as much as possible about its contents by asking questions. The teacher answers these questions with spans from the evidence text, as in existing reading comprehension tasks (Rajpurkar et al., 2016). Additionally, the teacher uses dialog acts to provide the student with feedback (e.g., “ask a follow up ques2174 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 2174–2184 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics Dataset QuAC CoQA (Reddy et al., 2018) CSQA (Saha et al., 2018) CQA (Talmor and Berant, 2018) SQA (Iyyer et al., 2017) NarrativeQA (Kocisk´y et al., 2017) TriviaQA (Joshi et al., 2017) SQuAD 2.0 (Rajpurkar et al., 2018) MS Marco (Nguyen et al., 2016) NewsQA (Trischler et al., 2016) Multi turn Textbased Dialog Acts Simple Evaluation Unanswerable Questions Asker Can’t See Evidence 4 4 4 4 4 4 4 4 4 4 7 7 7 7 7 4 7 4 7 4 4 4 4 4 7 7 7 7 7 7 7 7 7 4 7 4 4 7 4 4 7 4 4 4 7 7 7 7 4 4 4 7 7 4 7 4 4 7 4 4 Table 1: Comparison of the QUAC dataset to other question answering datasets. tion”), which makes the dialogs more productive. We collect the dataset in an interactive setting where two crowd"
D18-1266,Q13-1005,0,0.294332,"er: England Figure 1: An example of semantic parsing from denotations. Given the table environment, map the question to an executable program that evaluates to the answer. Introduction Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations. Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015). Consider the example in Figure 1. Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England. In the SpFD setting, the training data does not contain the correct programs. Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs and finds suitable candidates, and an update step that uses these programs to update the model. Figure 2 shows"
D18-1266,D13-1160,0,0.690977,"s Wales 25 5 5 8.8 Program: Select Nation Where Points is Maximum Answer: England Figure 1: An example of semantic parsing from denotations. Given the table environment, map the question to an executable program that evaluates to the answer. Introduction Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations. Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015). Consider the example in Figure 1. Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England. In the SpFD setting, the training data does not contain the correct programs. Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs and finds suitable candidates, a"
D18-1266,D16-1245,0,0.0689966,"Missing"
D18-1266,W10-2903,1,0.86718,"eralization in model shaping. 6 Related Work Semantic Parsing from Denotation Mapping natural language text to formal meaning representation was first studied by Montague (1970). Early work on learning semantic parsers rely on labeled formal representations as the supervision signals (Zettlemoyer and Collins, 2005, 2007; Zelle and Mooney, 1993). However, because getting access to gold formal representation generally requires expensive annotations by an expert, distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had m"
D18-1266,P17-1097,0,0.502504,"s approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future. Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998). Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017). Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs. However, random noise need not bias the search towards the correct program(s). In this paper, we introduce a simple policy-shaping method to guide the search. This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs. 2444 1 This transformation preserves the answer of the question. Algorithm 1 Learning a semantic parser from denotation using generalized updates. Inp"
D18-1266,N12-1015,0,0.0607653,"Missing"
D18-1266,P17-1167,1,0.824952,"Missing"
D18-1266,P16-1002,0,0.0489753,"policy which is based on the score function, for example b✓ (y|x, t, z) / exp{score✓ (y, t)}. However, this approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future. Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998). Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017). Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs. However, random noise need not bias the search towards the correct program(s). In this paper, we introduce a simple policy-shaping method to guide the search. This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs. 2444 1 This transformation preserves the answ"
D18-1266,D17-1160,0,0.302332,"n frequently co-occurring pairs of tokens in the program and instruction. For example, the token most is highly likely to co-occur with a correct program containing the keyword Max. This happens for the example in Figure 2. Similarly the token not may co-occur with the keyword NotEqual. We assume access to a lexicon ⇤ = {(wj , !j )}kj=1 containing (3) Addressing Update Strategy Selection: Generalized Update Equation Given the set of programs generated by the search step, one can use many objectives to update the parameters. For example, previous work have utilized maximum marginal likelihood (Krishnamurthy et al., 2017; Guu et al., 2017), reinforcement learning (Zhong et al., 2017; Guu et al., 2017) and margin based methods (Iyyer et al., 2017). It could be difficult to choose the suitable algorithm from these options. In this section, we propose a principle and general update equation such that previous update algorithms can be considered as special cases to this equation. Having a general update is important for the following reasons. First, it allows us to understand existing algorithms better by examining their basic properties. Second, the generalized update equation also makes it easy to implement and"
D18-1266,D15-1032,0,0.0173733,"derperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example. Latent Variable Modeling Learning semantic pars"
D18-1266,D16-1262,0,0.0554985,"sed learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example. Latent Variable Modeling Learning semantic parsers from denotation can be viewed as a latent variable modeling problem, where the program is the latent variable. Probabilistic latent variable models have been studied using EM-algorithm and its variant (Dempster et al., 1977). The graphical model literature has studied latent variable learning on margin-based methods (Yu and Joachims, 2009) and probabilistic m"
D18-1266,D16-1127,0,0.0263699,"expensive annotations by an expert, distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had more than 21 losses?&quot; “of the remaining, which earned the most bronze medals?&quot; “of those competitors from germany, which was not paul sievert?&quot; without policy shaping SELECT Club WHERE Losses = ROW 15 SELECT Nation WHERE Rank = ROW 1 SELECT Name WHERE Time (hand) = ROW 3 with policy shaping SELECT Club WHERE Losses > 21 FollowUp WHERE Bronze is Max FollowUp WHERE Name != ROW 5 Table 4: Training examples and the highest ran"
D18-1266,P11-1060,0,0.4172,"Learning Learning a semantic parser is equivalent to learning the parameters ✓ in the scoring function, which is a structured learning problem, due to the large, structured output space Y. Structured learning algorithms generally consist of two major components: search and update. When the gold programs are available during training, the search procedure finds a set of high-scoring incorrect programs. These programs are used by the update step to derive loss for updating parameters. For example, these programs are used for approximating the partition-function in maximum-likelihood objective (Liang et al., 2011) and finding set of programs causing margin violation in margin based methods (Daumé III and Marcu, 2005). Depending on the exact algorithm being used, these two components are not necessarily separated into isolated steps. For instance, parameters can be updated in the middle of search (e.g., Huang et al., 2012). For learning semantic parsers from denotations, where we assume only answers are available in a training set {(xi , ti , zi )}N i=1 of N examples, the basic construction of the learning algorithms remains the same. However, the problems that search needs to handle in SpFD is more cha"
D18-1266,P05-1012,0,0.147574,"s. ning, 2016), semantic parsing (Guu et al., 2017) and instruction following (Misra et al., 2017). Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms"
D18-1266,D17-1106,1,0.8804,"Missing"
D18-1266,D16-1183,1,0.853215,"d on the score function, for example b✓ (y|x, t, z) / exp{score✓ (y, t)}. However, this approach can suffer from a divergence phenomenon whereby the score of spurious programs picked up by the search in the first epoch increases, making it more likely for the search to pick them up in the future. Such divergence issues are common with latent-variable learning and often require careful initialization to overcome (Rose, 1998). Unfortunately such initialization schemes are not applicable for deep neural networks which form the model of most successful semantic parsers today (Jia and Liang, 2016; Misra and Artzi, 2016; Iyyer et al., 2017). Prior work, such as ✏-greedy exploration (Guu et al., 2017), has reduced the severity of this problem by introducing random noise in the search procedure to avoid saturating the search on high-scoring spurious programs. However, random noise need not bias the search towards the correct program(s). In this paper, we introduce a simple policy-shaping method to guide the search. This approach allows incorporating prior knowledge in the exploration policy and can bias the search away from spurious programs. 2444 1 This transformation preserves the answer of the question. Alg"
D18-1266,P15-1096,1,0.853345,"ple of semantic parsing from denotations. Given the table environment, map the question to an executable program that evaluates to the answer. Introduction Semantic parsing from denotations (SpFD) is the problem of mapping text to executable formal representations (or program) in a situated environment and executing them to generate denotations (or answer), in the absence of access to correct representations. Several problems have been handled within this framework, including question answering (Berant et al., 2013; Iyyer et al., 2017) and instructions for robots (Artzi and Zettlemoyer, 2013; Misra et al., 2015). Consider the example in Figure 1. Given the question and a table environment, a semantic parser maps the question to an executable program, in this case a SQL query, and then executes the query on the environment to generate the answer England. In the SpFD setting, the training data does not contain the correct programs. Thus, the existing learning approaches for SpFD perform two steps for every training example, a search step that explores the space of programs and finds suitable candidates, and an update step that uses these programs to update the model. Figure 2 shows the two step trainin"
D18-1266,D15-1001,0,0.0445552,"distant supervision approaches, where semantic parsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had more than 21 losses?&quot; “of the remaining, which earned the most bronze medals?&quot; “of those competitors from germany, which was not paul sievert?&quot; without policy shaping SELECT Club WHERE Losses = ROW 15 SELECT Nation WHERE Rank = ROW 1 SELECT Name WHERE Time (hand) = ROW 3 with policy shaping SELECT Club WHERE Losses > 21 FollowUp WHERE Bronze is Max FollowUp WHERE Name != ROW 5 Table 4: Training examples and the highest ranked program in the beam search, scored accor"
D18-1266,D16-1261,0,0.0160404,"rsers are learned from denotation only, have become the main learning paradigm (e.g., Clarke et al., 2010; Liang et al., 2011; Artzi and Zettlemoyer, 2013; Berant et al., 2013; Iyyer et al., 2017; Krishnamurthy et al., 2017). Guu et al. (2017) studied the problem of spurious programs and considered adding noise to diversify the search procedure and introduced meritocratic updates. Reinforcement Learning Algorithms Reinforcement learning algorithms have been applied to various NLP problems including dialogue (Li et al., 2016), text-based games (Narasimhan et al., 2015), information extraction (Narasimhan et al., 2016), coreference resolution (Clark and Man2449 Question “of these teams, which had more than 21 losses?&quot; “of the remaining, which earned the most bronze medals?&quot; “of those competitors from germany, which was not paul sievert?&quot; without policy shaping SELECT Club WHERE Losses = ROW 15 SELECT Nation WHERE Rank = ROW 1 SELECT Name WHERE Time (hand) = ROW 3 with policy shaping SELECT Club WHERE Losses > 21 FollowUp WHERE Bronze is Max FollowUp WHERE Name != ROW 5 Table 4: Training examples and the highest ranked program in the beam search, scored according to the shaped policy, after training with MAV"
D18-1266,N12-1087,1,0.808266,"of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the most violated example. Latent Variable Modeling Learning semantic parsers from denotation can be viewed as a latent variable modeling problem, where the program is the latent variable. Probabilistic latent variable models have been studied using EM-algorithm and its variant (Dempster et al., 1977). The graphical model literature has studied latent variable learning on margin-based methods (Yu and Joachims, 2009) and probabilistic models (Quattoni et al., 2007). Samdani et al. (2012) studied various variants of EM algorithm and showed that all of them are special cases of a unified framework. Our generalized update framework is similar in spirit. 7 Conclusion In this paper, we propose a general update equation from semantic parsing from denotation and propose a policy shaping method for addressing the spurious program challenge. For the future, we plan to apply the proposed learning framework to more semantic parsing tasks and consider new methods for policy shaping. 8 Acknowledgements We thank Ryan Benmalek, Alane Suhr, Yoav Artzi, Claire Cardie, Chris Quirk, Michel Gall"
D18-1266,W04-3201,0,0.0982967,"tep for these examples. ning, 2016), semantic parsing (Guu et al., 2017) and instruction following (Misra et al., 2017). Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning,"
D18-1266,D07-1080,0,0.0177084,"al., 2017). Guu et al. (2017) show that policy gradient methods underperform maximum marginal likelihood approaches. Our result on the SQA dataset supports their observation. However, we show that using off-policy sampling, policy gradient methods can provide superior performance to maximum marginal likelihood methods. Margin-based Learning Margin-based methods have been considered in the context of SVM learning. In the NLP literature, margin based learning has been applied to parsing (Taskar et al., 2004; McDonald et al., 2005), text classification (Taskar et al., 2003), machine translation (Watanabe et al., 2007) and semantic parsing (Iyyer et al., 2017). Kummerfeld et al. (2015) found that max-margin based methods generally outperform likelihood maximization on a range of tasks. Previous work have studied connections between margin based method and likelihood maximization for supervised learning setting. We show them as special cases of our unified update equation for distant supervision learning. Similar to this work, Lee et al. (2016) also found that in the context of supervised learning, margin-based algorithms which update all violated examples perform better than the one that only updates the mo"
D18-1266,D07-1071,0,0.197773,"Missing"
D19-1457,D14-1159,1,0.876692,"background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to track how entities’ states change with time, e.g., EntNet (Henaff et al., 2017), ProStruct (Tandon et al., 2018), and Neural Process Networks (Bosselut et al., 2018), applied to procedural text such as cooking recipes (Kiddon et al., 2016), science processes (Dalvi et al., 2018), or toy stories (Weston et al., 2015). Using"
D19-1457,P08-1090,0,0.0462856,"ic representations (“scripts”) of event sequences (or partial orders) from text, including representations of the goals, effects, and purpose of actions, e.g., (Schank and Abelson, 1977; DeJong, 1979; Cullingford, 1986; Mooney, 1986). Some of these systems could explain why actions occurred in text, similar to our goals here, but only on a handful of toy examples using hand-coded background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in rea"
D19-1457,P16-1223,0,0.0515435,"Missing"
D19-1457,N18-1144,1,0.916324,"ehension with an additional task of predicting the dependencies between steps, in the form of their effects and which subsequent action(s) become possible. Building upon the state-ofthe-art framework for predicting effects of actions, we present a new model, called XPAD (“eXPlaining Action Dependencies”) that also considers the purpose of those effects. Specifically, XPAD biases those predictions towards those that (1) explain more of the actions in the paragraph and (2) are more plausible with respect to background knowledge. On a benchmark dataset for procedural text comprehension, ProPara (Dalvi et al., 2018), XPAD significantly improves on the prediction and explanation of action dependencies compared to prior systems, while also matching state-of-the-art results on the original tasks. We thus contribute: 1. A new task for procedural text comprehension, namely predicting and explaining the dependencies between actions (“what depends on what, and why”), including an additional dependency graph dataset for ProPara. 2. A model, XPAD, that significantly outperforms prior systems at predicting and explaining action dependencies, while maintaining its performance on the original tasks in ProPara. 2 Rel"
D19-1457,W18-2501,0,0.0327805,"Missing"
D19-1457,W19-1502,0,0.138633,"dependency layer. P R F1 ProLocal QRN EntNet ProStruct ProGlobal 24.7 32.6 32.8 76.3 43.4 18.0 30.3 38.6 21.3 37.0 20.8 31.4 35.5 33.4 39.9 XPAD 62.0 32.9 43.0 Table 1: Results on the dependency task (test set). Table 1 reports results of all models on the new dependency task. XPAD significantly outperforms the strongest baselines, ProGlobal and ProStruct, by more than 3 points F1 . XPAD has much higher precision than ProGlobal with similar recall, sug3 Since XPAD was developed, two higher unpublished results of 57.6 and 62.5 on the state-change task have appeared on arXiv (Das et al., 2019; Gupta and Durrett, 2019), their systems developed contemporaneously with XPAD. In principle XPAD’s approach of jointly learning both state changes and dependencies could be also applied in these new systems. Our main contribution is to show that jointly learning state changes and dependencies can produce more rational (explainable) results, without damaging (here, slightly improving) the state change predictions themselves. 4502 ProStruct XPAD P R F1 74.3 70.5 43.0 45.3 54.5 55.2 Table 3: Results on the state-change task (test set), comparing with the best published prior result. 7 7.1 Analysis and Discussion Depende"
D19-1457,D16-1014,1,0.853147,"sily generates πtrain , and then apply the heuristics from Section 4 to obtain Gtrain . The distribution in Gtrain can be quite different from that in ProPara (the current task), so we append Gtrain with dependency graphs obtained from the training set in ProPara. We then decompose Gtrain into its Ei j edges (negative examples are created by reversing these edges). This leads to 324,462 training examples. We then add 2,201 examples derived from the ProPara train set. This use of hand-written rules to generate a large number of potentially noisy examples follows others in the literature, e.g. (Sharp et al., 2016; Ahn et al., 2016; Bosselut et al., 2018). To accommodate for lexical variations, we embed the database of training data in a neural model. Our model itself takes as input Ei j and outputs the likelihood of Ei j . To do this, an embedding for Ei j is created using a deep network of biLSTMs, producing a contextual embedding based on the token level embeddings of si , s j and the state change vector πik . This contextual embedding is then decoded using a feedforward network to predict a score for whether si enabled s j through πik . The loss function is designed such that errors on the training"
D19-1457,D18-1006,1,0.874235,"Missing"
D19-1457,D16-1032,0,0.0330512,"er action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to track how entities’ states change with time, e.g., EntNet (Henaff et al., 2017), ProStruct (Tandon et al., 2018), and Neural Process Networks (Bosselut et al., 2018), applied to procedural text such as cooking recipes (Kiddon et al., 2016), science processes (Dalvi et al., 2018), or toy stories (Weston et al., 2015). Using annotated data, these systems learn to predict the effects of actions from text, allowing simple simulations of how the world changes throughout the process. We build on this line of work to identify the purpose of actions, by connecting their effects to subsequent actions. In addition to signal from training data, many systems use background knowledge to help bias predictions towards those consistent with that knowledge. For example, (McLauchlan, 2004) predicts prepositional phrase attachments that are more"
D19-1457,W04-2410,0,0.020795,", applied to procedural text such as cooking recipes (Kiddon et al., 2016), science processes (Dalvi et al., 2018), or toy stories (Weston et al., 2015). Using annotated data, these systems learn to predict the effects of actions from text, allowing simple simulations of how the world changes throughout the process. We build on this line of work to identify the purpose of actions, by connecting their effects to subsequent actions. In addition to signal from training data, many systems use background knowledge to help bias predictions towards those consistent with that knowledge. For example, (McLauchlan, 2004) predicts prepositional phrase attachments that are more consistent with unambiguous attachments derived from thesaurii; (Clark and Harrison, 2009) tune a parser to prefer bracketings consistent with frequent bracketings stored a text-derived corpus of bracketings; and (Tandon et al., 2018) predicts state changes consistent with those seen in a large text corpus. For our purposes, while KBs such as (Speer and Havasi, 2013; Chu et al., 2017; Park and Nezhad, 2018) contain useful information about action dependencies (e.g., “smoking can cause cancer”), they lack explanations for those links. Ins"
D19-1457,K16-1008,0,0.0130658,"ncluding representations of the goals, effects, and purpose of actions, e.g., (Schank and Abelson, 1977; DeJong, 1979; Cullingford, 1986; Mooney, 1986). Some of these systems could explain why actions occurred in text, similar to our goals here, but only on a handful of toy examples using hand-coded background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to track how ent"
D19-1457,W14-1606,0,0.0246656,"l orders) from text, including representations of the goals, effects, and purpose of actions, e.g., (Schank and Abelson, 1977; DeJong, 1979; Cullingford, 1986; Mooney, 1986). Some of these systems could explain why actions occurred in text, similar to our goals here, but only on a handful of toy examples using hand-coded background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to t"
D19-1457,D17-1108,0,0.0621705,"Missing"
D19-1457,D14-1162,0,0.0834093,"to a ‘mixture’ (Figure 2). 5.4 Training and Testing XPAD At training time, XPAD follows only the correct (gold) path through the search space, and learns to minimize the joint loss of predicting the correct state changes and dependency explanation graph for the paragraph. At test time, XPAD performs a beam search to predict the most likely state changes and dependency explanation graph. 5.5 Implementation Details for XPAD We implement XPAD in PyTorch using AllenNLP (Gardner et al., 2018). We use the dataset reader published in ProStruct’s publicly available code. We use 100D Glove embeddings (Pennington et al., 2014), trained on Wikipedia 2014 and Gigaword 5 corpora (6B tokens, 400K vocab, uncased). Starting from glove embeddings appended by entity and verb indicators, we use bidirectional LSTM layer to create contextual representation for every word. We use 100D hidden representations for the bidirectional LSTM (Hochreiter and Schmidhuber, 1997) shared between all inputs (each direction uses 50D hidden vectors). The attention layer on top of BiLSTM, uses a bilinear similarity function similar to (Chen et al., 2016) to compute attention weights over the contextual embedding for every word. To compute the"
D19-1457,P16-1027,0,0.01866,"resentations of the goals, effects, and purpose of actions, e.g., (Schank and Abelson, 1977; DeJong, 1979; Cullingford, 1986; Mooney, 1986). Some of these systems could explain why actions occurred in text, similar to our goals here, but only on a handful of toy examples using hand-coded background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to track how entities’ states change with ti"
D19-1457,D13-1177,1,0.740913,"les using hand-coded background knowledge, and proved difficult to scale. More recent work on event extraction and script learning has proved more effective at extracting event/action sequences from text using statistical methods, e.g., (Chambers and Jurafsky, 2008), and neural techniques, e.g., (Modi and Titov, 2014; Modi, 2016; Pichotta and Mooney, 2016), but have largely focused on what happens and in what order, rather than why. For example, such representations cannot answer questions about which events would fail if an earlier action in a sequence did not occur. The 2014 ProRead system (Scaria et al., 2013; Berant et al., 2014) included dependency relationships between events that it extracted, but assessed dependencies based on surface language cues, hence could not explain why those dependencies held. There has also been a line of research in reading procedural text in which the goal has been to track how entities’ states change with time, e.g., EntNet (Henaff et al., 2017), ProStruct (Tandon et al., 2018), and Neural Process Networks (Bosselut et al., 2018), applied to procedural text such as cooking recipes (Kiddon et al., 2016), science processes (Dalvi et al., 2018), or toy stories (Westo"
D19-1547,P18-1033,0,0.116622,"Missing"
D19-1547,Q13-1005,0,0.227633,"Missing"
D19-1547,D13-1160,0,0.0619832,"k, demonstrated on two text-to-SQL datasets (WikiSQL and Spider) with different state-of-the-art base semantic parsers. Compared to an existing interactive semantic parsing approach that treats the base parser as a black box, our approach solicits less user feedback but yields higher run-time accuracy.1 World Model User Error Detector Action Actuator Intent Question Figure 1: Model-based Interactive Semantic Parsing (MISP) framework. Natural language interfaces that allow users to query data and invoke services without programming have been identified as a key application of semantic parsing (Berant et al., 2013; Thomason et al., 2015; Dong and Lapata, 2016; Zhong et al., 2017; Campagna et al., 2017; Su et al., 2017). However, existing semantic parsing technologies often fall short when deployed in practice, facing several challenges: (1) user utterances can be inherently ambiguous or vague, making it difficult to get the correct result in one shot, (2) the accuracy of state-of-the-art semantic parsers are still not high enough for real use, and (3) it is hard for ∗ Percept State Introduction Work started while at AI2 Code available at https://github.com/ sunlab-osu/MISP. 1 Environment users to valid"
D19-1547,P19-1448,0,0.0646429,"Missing"
D19-1547,P19-1444,0,0.228022,"Missing"
D19-1547,P18-1124,1,0.1337,"tion Work started while at AI2 Code available at https://github.com/ sunlab-osu/MISP. 1 Environment users to validate the semantic parsing results, especially with mainstream neural network models that are known for the lack of interpretability. In response to these challenges, interactive semantic parsing has been proposed recently as a practical solution, which includes human users in the loop to resolve utterance ambiguity, boost system accuracy, and improve user confidence via human-machine collaboration (Li and Jagadish, 2014; He et al., 2016; Chaurasia and Mooney, 2017; Su et al., 2018; Gur et al., 2018; Yao et al., 2019). For example, Gur et al. (2018) built the DialSQL system to detect errors in a generated SQL query and request user selection on alternative options via dialogues. Similarly, Chaurasia and Mooney (2017) and Yao et al. (2019) enabled semantic parsers to ask users clarification questions while generating an If-Then program. Su et al. (2018) showed that users overwhelmingly preferred an interactive system over the noninteractive counterpart for natural language interfaces to web APIs. While these recent studies successfully demonstrated the value of interactive semantic parsin"
D19-1547,I17-2030,0,0.0996301,"d (3) it is hard for ∗ Percept State Introduction Work started while at AI2 Code available at https://github.com/ sunlab-osu/MISP. 1 Environment users to validate the semantic parsing results, especially with mainstream neural network models that are known for the lack of interpretability. In response to these challenges, interactive semantic parsing has been proposed recently as a practical solution, which includes human users in the loop to resolve utterance ambiguity, boost system accuracy, and improve user confidence via human-machine collaboration (Li and Jagadish, 2014; He et al., 2016; Chaurasia and Mooney, 2017; Su et al., 2018; Gur et al., 2018; Yao et al., 2019). For example, Gur et al. (2018) built the DialSQL system to detect errors in a generated SQL query and request user selection on alternative options via dialogues. Similarly, Chaurasia and Mooney (2017) and Yao et al. (2019) enabled semantic parsers to ask users clarification questions while generating an If-Then program. Su et al. (2018) showed that users overwhelmingly preferred an interactive system over the noninteractive counterpart for natural language interfaces to web APIs. While these recent studies successfully demonstrated the v"
D19-1547,N19-1423,0,0.048704,"Missing"
D19-1547,P16-1004,0,0.0393952,"(WikiSQL and Spider) with different state-of-the-art base semantic parsers. Compared to an existing interactive semantic parsing approach that treats the base parser as a black box, our approach solicits less user feedback but yields higher run-time accuracy.1 World Model User Error Detector Action Actuator Intent Question Figure 1: Model-based Interactive Semantic Parsing (MISP) framework. Natural language interfaces that allow users to query data and invoke services without programming have been identified as a key application of semantic parsing (Berant et al., 2013; Thomason et al., 2015; Dong and Lapata, 2016; Zhong et al., 2017; Campagna et al., 2017; Su et al., 2017). However, existing semantic parsing technologies often fall short when deployed in practice, facing several challenges: (1) user utterances can be inherently ambiguous or vague, making it difficult to get the correct result in one shot, (2) the accuracy of state-of-the-art semantic parsers are still not high enough for real use, and (3) it is hard for ∗ Percept State Introduction Work started while at AI2 Code available at https://github.com/ sunlab-osu/MISP. 1 Environment users to validate the semantic parsing results, especially w"
D19-1547,P18-1069,0,0.0187687,"based Uncertainty. Intuitively if the base semantic parser gives a low probability to the top prediction at a step, it is likely uncertain about the prediction. Specifically, we say a prediction ot needs user clarification if its probability is lower than a threshold p∗ , i.e., p(ot ) &lt; p∗ . This strategy is shown to be strong in detecting misclassified and out-of-distribution examples (Hendrycks and Gimpel, 2017). Dropout-based Uncertainty. Dropout (Srivastava et al., 2014) has been used as a Bayesian approximation for estimating model uncertainty (Gal and Ghahramani, 2016) in several tasks (Dong et al., 2018; Siddhant and Lipton, 2018; Xiao and Wang, 2019). Different from its standard application to prevent models from overfitting in training time, we use it at test time to measure model uncertainty, similar to (Dong et al., 2018). The intuition is that if the probability on a prediction varies dramatically (as measured by the 2 In practice this assumption may not be necessary as long as there is a reasonable way to chunk the semantic parse to calculate uncertainty and formulate clarification questions. Figure 2: MISP-SQL Agent. The base semantic parser incrementally parses the user question (Ste"
D19-1547,D16-1258,0,0.308613,"for real use, and (3) it is hard for ∗ Percept State Introduction Work started while at AI2 Code available at https://github.com/ sunlab-osu/MISP. 1 Environment users to validate the semantic parsing results, especially with mainstream neural network models that are known for the lack of interpretability. In response to these challenges, interactive semantic parsing has been proposed recently as a practical solution, which includes human users in the loop to resolve utterance ambiguity, boost system accuracy, and improve user confidence via human-machine collaboration (Li and Jagadish, 2014; He et al., 2016; Chaurasia and Mooney, 2017; Su et al., 2018; Gur et al., 2018; Yao et al., 2019). For example, Gur et al. (2018) built the DialSQL system to detect errors in a generated SQL query and request user selection on alternative options via dialogues. Similarly, Chaurasia and Mooney (2017) and Yao et al. (2019) enabled semantic parsers to ask users clarification questions while generating an If-Then program. Su et al. (2018) showed that users overwhelmingly preferred an interactive system over the noninteractive counterpart for natural language interfaces to web APIs. While these recent studies suc"
D19-1547,P17-1089,0,0.103181,"ility of MISP allows improving on each agent component separately. Take the error detector for example. One can augment the probability-based error detector in MISP-SQL with probability calibration, which has been shown useful in aligning model confidence with its reliability (Guo et al., 2017). One can also use learning-based approaches, such as a reinforced decision policy (Yao et al., 2019), to increase the rate of identifying wrong and solvable predictions. Lifelong Learning for Semantic Parsing. Learning from user feedback is a promising solution for lifelong semantic parser improvement (Iyer et al., 2017; Padmakumar et al., 2017; Labutov et al., 2018). However, this may lead to a non-stationary environment (e.g., changing state transition) from the perspective of the agent, making its training (e.g., error detector learning) unstable. In the context of dialog systems, Padmakumar et al. (2017) suggests that this effect can be mitigated by jointly updating the dialog policy and the semantic parser batchwisely. We leave exploring this aspect in our task to future work. Scaling Up. It is important for MISP agents to scale to larger backend data sources (e.g., knowledge bases like Freebase or Wiki"
D19-1547,P16-1195,0,0.0130121,"is the standard deviation of the prediction probabilities over N random passes. We say ot needs user clarification if its uncertainty score is greater than a threshold s∗ . Terminal State. The only terminal state is when the base semantic parser indicates end of parsing. 4.3 Actuator: An NL Generator The MISP-SQL agent performs its action (e.g., validating the column “place”) via asking users binary questions, hence the actuator is a natural language generator (NLG). Although there has been work on describing a SQL query with an NL statement (Koutrika et al., 2010; Ngonga Ngomo et al., 2013; Iyer et al., 2016; Xu et al., 2018), few work studies generating questions about a certain SQL component in a systematic way. Inspired by (Koutrika et al., 2010; Wang et al., 2015), we define a rule-based NLG, which consists of a seed lexicon and a grammar for deriving questions. Table 1 shows rules covering 5450 [Lexicon] is greater than|equals to|is less than → O P[&gt;|=|&lt;] sum of values in|average value in|number of|minimum value in|maximum value in → AGG[sum|avg|count|min|max] [Grammar] “col” → C OL[col] Does the system need to return information about C OL[col] ? → Q[colkSELECT agg? col] Does the system nee"
D19-1547,D18-1195,0,0.162746,"Missing"
D19-1547,E17-1052,0,0.0203286,"s improving on each agent component separately. Take the error detector for example. One can augment the probability-based error detector in MISP-SQL with probability calibration, which has been shown useful in aligning model confidence with its reliability (Guo et al., 2017). One can also use learning-based approaches, such as a reinforced decision policy (Yao et al., 2019), to increase the rate of identifying wrong and solvable predictions. Lifelong Learning for Semantic Parsing. Learning from user feedback is a promising solution for lifelong semantic parser improvement (Iyer et al., 2017; Padmakumar et al., 2017; Labutov et al., 2018). However, this may lead to a non-stationary environment (e.g., changing state transition) from the perspective of the agent, making its training (e.g., error detector learning) unstable. In the context of dialog systems, Padmakumar et al. (2017) suggests that this effect can be mitigated by jointly updating the dialog policy and the semantic parser batchwisely. We leave exploring this aspect in our task to future work. Scaling Up. It is important for MISP agents to scale to larger backend data sources (e.g., knowledge bases like Freebase or Wikidata). To this end, one c"
D19-1547,P15-1085,0,0.0683174,"Missing"
D19-1547,D18-1318,0,0.0342104,"Intuitively if the base semantic parser gives a low probability to the top prediction at a step, it is likely uncertain about the prediction. Specifically, we say a prediction ot needs user clarification if its probability is lower than a threshold p∗ , i.e., p(ot ) &lt; p∗ . This strategy is shown to be strong in detecting misclassified and out-of-distribution examples (Hendrycks and Gimpel, 2017). Dropout-based Uncertainty. Dropout (Srivastava et al., 2014) has been used as a Bayesian approximation for estimating model uncertainty (Gal and Ghahramani, 2016) in several tasks (Dong et al., 2018; Siddhant and Lipton, 2018; Xiao and Wang, 2019). Different from its standard application to prevent models from overfitting in training time, we use it at test time to measure model uncertainty, similar to (Dong et al., 2018). The intuition is that if the probability on a prediction varies dramatically (as measured by the 2 In practice this assumption may not be necessary as long as there is a reasonable way to chunk the semantic parse to calculate uncertainty and formulate clarification questions. Figure 2: MISP-SQL Agent. The base semantic parser incrementally parses the user question (Step 1) into a SQL query by fi"
D19-1547,P15-1129,0,0.141769,"Missing"
D19-1547,D18-1112,0,0.0215426,"viation of the prediction probabilities over N random passes. We say ot needs user clarification if its uncertainty score is greater than a threshold s∗ . Terminal State. The only terminal state is when the base semantic parser indicates end of parsing. 4.3 Actuator: An NL Generator The MISP-SQL agent performs its action (e.g., validating the column “place”) via asking users binary questions, hence the actuator is a natural language generator (NLG). Although there has been work on describing a SQL query with an NL statement (Koutrika et al., 2010; Ngonga Ngomo et al., 2013; Iyer et al., 2016; Xu et al., 2018), few work studies generating questions about a certain SQL component in a systematic way. Inspired by (Koutrika et al., 2010; Wang et al., 2015), we define a rule-based NLG, which consists of a seed lexicon and a grammar for deriving questions. Table 1 shows rules covering 5450 [Lexicon] is greater than|equals to|is less than → O P[&gt;|=|&lt;] sum of values in|average value in|number of|minimum value in|maximum value in → AGG[sum|avg|count|min|max] [Grammar] “col” → C OL[col] Does the system need to return information about C OL[col] ? → Q[colkSELECT agg? col] Does the system need to return AGG[ag"
D19-1547,N18-2093,0,0.121193,"g the probability distribution or removing certain prediction paths). The error detector makes decisions based on the uncertainty of the predictions: if the parser is uncertain about a prediction, it is more likely to be an error. The actuator is a template-based natural language question generator developed for the general SQL language. Figure 2 shows an example of the MISPSQL agent. 4.1 Agent State For ease of discussion, we assume the base parser generates the SQL query by predicting a sequence 5449 of SQL components,2 as in many state-of-theart systems (Xu et al., 2017; Wang et al., 2018; Yu et al., 2018a; Hwang et al., 2019). Agent state st is thus defined as a partial SQL query, i.e., st ={o1 , o2 , ..., ot }, where ot is the predicted SQL component at time step t, such as SELECT place in Figure 2. What constitutes a SQL component is often defined differently in different semantic parsers, but typically dictated by the SQL syntax. To support introspection and error detection, each prediction is associated with its uncertainty, which is discussed next. 4.2 Error Detector The error detector in MISP-SQL is introspective and greedy. It is introspective because it examines the uncertainty of the"
D19-1547,D18-1193,0,0.0995456,"g the probability distribution or removing certain prediction paths). The error detector makes decisions based on the uncertainty of the predictions: if the parser is uncertain about a prediction, it is more likely to be an error. The actuator is a template-based natural language question generator developed for the general SQL language. Figure 2 shows an example of the MISPSQL agent. 4.1 Agent State For ease of discussion, we assume the base parser generates the SQL query by predicting a sequence 5449 of SQL components,2 as in many state-of-theart systems (Xu et al., 2017; Wang et al., 2018; Yu et al., 2018a; Hwang et al., 2019). Agent state st is thus defined as a partial SQL query, i.e., st ={o1 , o2 , ..., ot }, where ot is the predicted SQL component at time step t, such as SELECT place in Figure 2. What constitutes a SQL component is often defined differently in different semantic parsers, but typically dictated by the SQL syntax. To support introspection and error detection, each prediction is associated with its uncertainty, which is discussed next. 4.2 Error Detector The error detector in MISP-SQL is introspective and greedy. It is introspective because it examines the uncertainty of the"
D19-1547,D18-1425,0,0.183762,"Missing"
H05-2004,W05-0620,0,0.0327476,"Missing"
H05-2004,kingsbury-palmer-2002-treebank,0,0.0431051,"ion of lower level processors, while achieving effective real time performance. 1 Introduction Semantic parsing of sentences is believed to be an important subtask toward natural language understanding, and has immediate applications in tasks such information extraction and question answering. We study semantic role labeling (SRL), defined as follows: for each verb in a sentence, the goal is to identify all constituents that fill a semantic role, and to determine their roles (such as Agent, Patient or Instrument) and their adjuncts (such as Locative, Temporal or Manner). The PropBank project (Kingsbury and Palmer, 2002), which provides a large humanannotated corpus of semantic verb-argument relations, has opened doors for researchers to apply machine learning techniques to this task. The focus of the research has been on improving the performance of the SRL system by using, in addition to raw text, various syntactic and semantic information, e.g. Part of Speech (POS) tags, chunks, clauses, syntactic parse tree, and named entities, which is found crucial to the SRL system (Punyakanok et al., 2005). In order to support a real world application such as an interactive question-answering system, the ability of an"
H05-2004,W05-0625,1,0.858867,"Missing"
H05-2004,W05-0639,0,0.0165936,"Agent, Patient or Instrument) and their adjuncts (such as Locative, Temporal or Manner). The PropBank project (Kingsbury and Palmer, 2002), which provides a large humanannotated corpus of semantic verb-argument relations, has opened doors for researchers to apply machine learning techniques to this task. The focus of the research has been on improving the performance of the SRL system by using, in addition to raw text, various syntactic and semantic information, e.g. Part of Speech (POS) tags, chunks, clauses, syntactic parse tree, and named entities, which is found crucial to the SRL system (Punyakanok et al., 2005). In order to support a real world application such as an interactive question-answering system, the ability of an SRL system to analyze text in real time is a necessity. However, in previous research, the overall efficiency of the SRL system has not been considered. At best, the efficiency of an SRL system may be reported in an experiment assuming that all the necessary information has already been provided, which is not realistic. A real world scenario requires the SRL system to perform all necessary preprocessing steps in real time. The overall efficiency of SRL systems that include the pre"
H05-2004,W04-3212,0,0.0313041,"-art syntactic parser (Charniak, 2000) the output of which provides useful information for the main SRL module. The main SRL module consists of four stages: pruning, argument identification, argument classification, and inference. The following is the overview of these four stages. Details of them can be found in (Koomen et al., 2005). Pruning The goal of pruning is to filter out unlikely argument candidates using simple heuristic rules. Only the constituents in the parse tree are considered as argument candidates. In addition, our system exploits a heuristic modified from that introduced by (Xue and Palmer, 2004) to filter out very unlikely constituents. Argument Identification The argument identification stage uses binary classification to identify whether a candidate is an argument or not. We train and apply the binary classifiers on the constituents supplied by the pruning stage. Argument Classification This stage assigns the final argument labels to the argument candidates supplied from the previous stage. A multi-class classifier is trained to classify the types of the arguments supplied by the argument identification stage. Inference The purpose of this stage is to incorporate some prior linguis"
H05-2004,A00-2018,0,\N,Missing
J08-2005,P98-1013,0,0.0806203,"Missing"
J08-2005,J04-4004,0,0.0226339,"Missing"
J08-2005,W04-2412,0,0.0418443,"Missing"
J08-2005,W05-0620,0,0.0974251,"Missing"
J08-2005,P01-1017,0,0.0485429,"ingsbury 2005), which provides a large human-annotated corpus of verb predicates and their arguments, has enabled researchers to apply machine learning techniques to develop SRL systems (Gildea and Palmer 2002; Chen and Rambow 2003; Gildea and Hockenmaier 2003; Pradhan et al. 2003; Surdeanu et al. 2003; Pradhan et al. 2004; Xue and Palmer 2004; Koomen et al. 2005). However, most systems rely heavily on full syntactic parse trees. Therefore, the overall performance of the system is largely determined by the quality of the automatic syntactic parsers of which the state of the art (Collins 1999; Charniak 2001) is still far from perfect. Alternatively, shallow syntactic parsers (i.e., chunkers and clausers), although they do not provide as much information as a full syntactic parser, have been shown to be more robust in their speciﬁc tasks (Li and Roth 2001). This raises the very natural and interesting question of quantifying the importance of full parsing information to semantic parsing and whether it is possible to use only shallow syntactic information to build an outstanding SRL system. Although PropBank is built by adding semantic annotations to the constituents in the Penn Treebank syntactic"
J08-2005,W03-1006,0,0.0393928,"Missing"
J08-2005,W97-0306,1,0.150784,"Missing"
J08-2005,W01-0502,1,0.691822,"Missing"
J08-2005,W03-1008,0,0.026464,"Missing"
J08-2005,J02-3001,0,0.528607,"on features for the SRL task. The creation of PropBank was inspired by the works of Levin (1993) and Levin and Hovav (1996), which discuss the relation between syntactic and semantic information. Following this philosophy, the features aim to indicate the properties of the predicate, the constituent which is an argument candidate, and the relationship between them through the available syntactic information. We explain these features herein. For further discussion of these features, we 262 Punyakanok, Roth, and Yih Importance of Parsing and Inference in SRL refer the readers to the article by Gildea and Jurafsky (2002), which introduced these features. r r r r r r r Predicate and POS tag of predicate: indicate the lemma of the predicate verb and its POS tag. Voice: indicates passive/active voice of the predicate. Phrase type: provides the phrase type of the constituent, which is the tag of the corresponding constituent in the parse tree. Head word and POS tag of the head word: provides the head word of the constituent and its POS tag. We use the rules introduced by Collins (1999) to extract this feature. Position: describes if the constituent is before or after the predicate, relative to the position in the"
J08-2005,P02-1031,0,0.0284703,"Missing"
J08-2005,C04-1186,0,0.0639183,"Missing"
J08-2005,W04-2416,0,0.0274841,"Missing"
J08-2005,W05-0623,0,0.0290998,"Missing"
J08-2005,kingsbury-palmer-2002-treebank,0,0.289024,"Missing"
J08-2005,W05-0625,1,0.415721,"Missing"
J08-2005,W01-0706,1,0.302998,"nmaier 2003; Pradhan et al. 2003; Surdeanu et al. 2003; Pradhan et al. 2004; Xue and Palmer 2004; Koomen et al. 2005). However, most systems rely heavily on full syntactic parse trees. Therefore, the overall performance of the system is largely determined by the quality of the automatic syntactic parsers of which the state of the art (Collins 1999; Charniak 2001) is still far from perfect. Alternatively, shallow syntactic parsers (i.e., chunkers and clausers), although they do not provide as much information as a full syntactic parser, have been shown to be more robust in their speciﬁc tasks (Li and Roth 2001). This raises the very natural and interesting question of quantifying the importance of full parsing information to semantic parsing and whether it is possible to use only shallow syntactic information to build an outstanding SRL system. Although PropBank is built by adding semantic annotations to the constituents in the Penn Treebank syntactic parse trees, it is not clear how important syntactic parsing is for an SRL system. To the best of our knowledge, this problem was ﬁrst addressed by Gildea and Palmer (2002). In their attempt to use limited syntactic information, the parser they used wa"
J08-2005,J93-2004,0,0.0393983,"Missing"
J08-2005,W05-0628,0,0.0153512,"Missing"
J08-2005,J05-1004,0,0.721049,"Missing"
J08-2005,W05-0634,0,0.0443973,"Missing"
J08-2005,N04-1030,0,0.0125181,"Missing"
J08-2005,C04-1197,1,0.675102,"ent in Section 4.2 where the gold-standard boundaries are used with the parse trees generated by an automatic parse. In such cases, if the information on the constituent, such as phrase type, needs to be extracted, the deepest constituent that covers the whole argument will be used. For example, in Figure 1, the phrase type for by John Smith is PP, and its path feature to the predicate assume is PP↑VP↓VBN. We also use the following additional features. These features have been shown to be useful for the systems by exploiting other information in the absence of the full parse tree information (Punyakanok et al. 2004), and, hence, can be helpful in conjunction with the features extracted from a full parse tree. They also aim to encode the properties of the predicate, the constituent to be classiﬁed, and their relationship in the sentence. r r Context words and POS tags of the context words: the feature includes the two words before and after the constituent, and their POS tags. Verb class: the feature is the VerbNet (Kipper, Palmer, and Rambow 2002) class of the predicate as described in PropBank Frames. Note that a 263 Computational Linguistics Volume 34, Number 2 verb may inhabit many classes and we coll"
J08-2005,W04-2401,1,0.799617,"s were always made for each argument independently, ignoring the global information across arguments in the ﬁnal output. The purpose of the inference stage is to incorporate such information, including both linguistic and structural knowledge, such as “arguments do not overlap” or “each verb takes at most one argument of each type.” This knowledge is useful to resolve any inconsistencies of argument classiﬁcation in order to generate ﬁnal legitimate predictions. We design an inference procedure that is formalized as a constrained optimization problem, represented as an integer linear program (Roth and Yih 2004). It takes as input the argument classiﬁers’ conﬁdence scores for each type of argument, along with a list of constraints. The output is the optimal solution that maximizes the linear sum of the conﬁdence scores, subject to the constraints that encode the domain knowledge. The inference stage can be naturally extended to combine the output of several different SRL systems, as we will show in Section 5. In this section we ﬁrst introduce the constraints and formalize the inference problem for the semantic role labeling task. We then demonstrate how we apply integer linear programming (ILP) to ge"
J08-2005,W00-0726,0,0.00973031,"Missing"
J08-2005,W04-3212,0,0.119377,"Missing"
J08-2005,P97-1003,0,\N,Missing
J08-2005,J03-4003,0,\N,Missing
J08-2005,C98-1013,0,\N,Missing
J08-2005,P03-1002,0,\N,Missing
N12-1077,N09-1003,0,0.0354362,"Missing"
N12-1077,J06-1003,0,0.357823,"human judgements better or similarly compared to existing methods on several benchmark datasets, including WordSim353. 1 Introduction Measuring the semantic relatedness of words is a fundamental problem in natural language processing and has many useful applications, including textual entailment, word sense disambiguation, information retrieval and automatic thesaurus discovery. Existing approaches can be roughly categorized into two kinds: knowledge-based and corpusbased, where the former includes graph-based algorithms and similarity measures operating on a lexical database such as WordNet (Budanitsky and Hirst, 2006; Agirre et al., 2009) and the latter consists of various kinds of vector space models (VSMs) constructed with the help of a large collection of text (Reisinger and Mooney, 2010; Radinsky et al., 2011). In this paper, we present a conceptually simple model for solving this problem. Observing that various kinds of information sources, such as ∗ Work conducted while interning at Microsoft Research. general text corpora, Web search results and thesauruses, have different word and sense coverage, we first build individual vector space models from each of them separately. Given two words, each VSM"
N12-1077,D11-1096,0,0.00473207,"s. 5 Conclusion In this paper we investigated the usefulness of heterogeneous information sources in improving measures of semantic word relatedness. Particularly, we created vector space models using 4 data sources 619 from 3 categories (corpus-based, Web-based and thesaurus-based) and found that simply averaging the cosine similarity derived from these models yields a very robust measure. Other than directly applying it to measuring semantic relatedness, our approach is complementary to more sophisticated similarity measures such as developing kernel functions for different structured data (Croce et al., 2011), where the similarity between words serves as a basic component. While this result is interesting and encouraging, it also raises several research questions, such as how to enhance the quality of each vector space model and whether the models can be combined more effectively3 . We also would like to study whether similar techniques can be useful when comparing longer text segments like phrases or sentences, with potential applications in paraphrase detection and recognizing textual entailment. Acknowledgments We thank Joseph Reisinger for providing his prototype vectors for our initial study,"
N12-1077,D07-1061,0,0.0635146,"mance of state-ofthe-art systems and the bottom shows the results of individual vector space models, as well as combining these models using the averaged cosine scores. We make several observations here. First, while none of the four VSMs we tested outperforms the best existing systems on the benchmark datasets, surprisingly, using the averaged cosine scores of these models, the performance is improved substantially. It achieves higher Spearman’s rank coefficient on WS-353 and MTurk-287 than any other systems2 and are close to the state-of-the-art on MC30 and RG-65. Unlike some approach like (Hughes and Ramage, 2007), which performs well on some datasets but poorly on others, combing the VSMs from heterogeneous sources is more robust. Individually, we notice that Wikipedia context VSM provides consistently strong results, while thesaurusbased models work only reasonable on MC-30 and RG-65, potentially because other datasets contain more out-of-vocabulary words or proper nouns. Due to the inherent ambiguity of the task, there is a high variance among judgements from different annotators. Therefore, it is unrealistic to assume any of the methods can correlate perfectly to the mean human judgement scores. In"
N12-1077,N10-1013,0,0.551549,"is a fundamental problem in natural language processing and has many useful applications, including textual entailment, word sense disambiguation, information retrieval and automatic thesaurus discovery. Existing approaches can be roughly categorized into two kinds: knowledge-based and corpusbased, where the former includes graph-based algorithms and similarity measures operating on a lexical database such as WordNet (Budanitsky and Hirst, 2006; Agirre et al., 2009) and the latter consists of various kinds of vector space models (VSMs) constructed with the help of a large collection of text (Reisinger and Mooney, 2010; Radinsky et al., 2011). In this paper, we present a conceptually simple model for solving this problem. Observing that various kinds of information sources, such as ∗ Work conducted while interning at Microsoft Research. general text corpora, Web search results and thesauruses, have different word and sense coverage, we first build individual vector space models from each of them separately. Given two words, each VSM measures the semantic relatedness by the cosine similarity of the corresponding vectors in its space. The final prediction is simply the averaged cosine scores derived from thes"
N12-3006,D09-1111,1,0.814882,"Missing"
N12-3006,J93-2004,0,0.04636,"the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty,"
N12-3006,J05-1004,0,0.0277178,". The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty, and shared task organizers are now"
N12-3006,P06-1055,0,0.0153341,"Missing"
N12-3006,C08-1094,0,0.0225179,"Missing"
N12-3006,J08-2002,1,0.0600596,"abels, like ARG0, ARG1, …, ARG5 for core arguments, and labels like ARGMTMP,ARGM-LOC, etc. for adjunct-like arguments. The meaning of the numbered arguments is verb-specific, with ARG0 typically representing an agent-like role, and ARG1 a patient-like role. This implementation of an SRL system follows the approach described in (Xue and Palmer, 04), and includes two log-linear models for argument identification and classification. A single syntax tree generated by the MSR SPLAT split-merge parser is used as input. Non-overlapping arguments are derived using the dynamic programming algorithm by Toutanova et al. (2008). 3 3.1 Other Language Analysis Functionality Sentence Boundary / Tokenization This analyzer identifies sentence boundaries and breaks the input into tokens. Both are represented as offsets of character ranges. Each token has both a raw form from the string and a normalized form in the PTB specification, e.g., open and close parentheses are replaced by -LRB- and -RRB-, respectively, to remove ambiguity with parentheses indicating syntactic structure. A finite state machine using simple rules and abbreviations detects sentence boundaries with high accuracy, and a set of regular expressions toke"
N12-3006,W04-3212,0,0.0291436,"Missing"
N12-3006,J03-4003,0,\N,Missing
N13-1090,S12-1047,0,0.203523,"bs (VB). We then systematically generated analogy questions by randomly matching each of the 100 words with 5 other words from the same category, and creating variants as indicated in Table 1. The total test set size is 8000. The test set is available online. 1 totypical word pair clothing:shirt. To measure the degree that a target word pair dish:bowl has the same relation, we form the analogy “clothing is to shirt as dish is to bowl,” and ask how valid it is. 5 The Vector Offset Method In addition to syntactic analogy questions, we used the SemEval-2012 Task 2, Measuring Relation Similarity (Jurgens et al., 2012), to estimate the extent to which RNNLM word vectors contain semantic information. The dataset contains 79 fine-grained word relations, where 10 are used for training and 69 testing. Each relation is exemplified by 3 or 4 gold word pairs. Given a group of word pairs that supposedly have the same relation, the task is to order the target pairs according to the degree to which this relation holds. This can be viewed as another analogy problem. For example, take the ClassInclusion:Singular Collective relation with the proAs we have seen, both the syntactic and semantic tasks have been formulated"
N13-1090,J93-2004,0,0.119941,"Person Singular Present Past/3rd Person Singular Present Patterns Tested JJ/JJR, JJR/JJ JJ/JJS, JJS/JJ JJS/JJR, JJR/JJS # Questions 1000 1000 1000 Example good:better rough: good:best rough: better:best rougher: NN/NNS, NNS/NN NN/NN POS, NN POS/NN VB/VBD, VBD/VB VB/VBZ, VBZ/VB 1000 year:years law: 1000 city:city’s bank: 1000 see:saw return: 1000 see:sees return: VBD/VBZ, VBZ/VBD 1000 saw:sees returned: Table 1: Test set patterns. For a given pattern and word-pair, both orderings occur in the test set. For example, if “see:saw return: ” occurs, so will “saw:see returned: ”. Treebank POS tags (Marcus et al., 1993). We then selected 100 of the most frequent comparative adjectives (words labeled JJR); 100 of the most frequent plural nouns (NNS); 100 of the most frequent possessive nouns (NN POS); and 100 of the most frequent base form verbs (VB). We then systematically generated analogy questions by randomly matching each of the 100 words with 5 other words from the same category, and creating variants as indicated in Table 1. The total test set size is 8000. The test set is available online. 1 totypical word pair clothing:shirt. To measure the degree that a target word pair dish:bowl has the same relati"
N13-1090,S12-1055,0,0.12833,"Missing"
N13-1090,P10-1040,0,0.350591,"The systems were trained with 320M words of Broadcast News data as described in (Mikolov et al., 2011a), and had an 82k vocabulary. Table 2 shows results for both RNNLM and LSA vectors on the syntactic task. LSA was trained on the same data as the RNN. We see that the RNN vectors capture significantly more syntactic regularity than the LSA vectors, and do remarkably well in an absolute sense, answering more than one in three questions correctly. 2 In Table 3 we compare the RNN vectors with those based on the methods of Collobert and Weston (2008) and Mnih and Hinton (2009), as implemented by (Turian et al., 2010) and available online 3 Since different words are present in these datasets, we computed the intersection of the vocabularies of the RNN vectors and the new vectors, and restricted the test set and word vectors to those. This resulted in a 36k word vocabulary, and a test set with 6632 2 3 Guessing gets a small fraction of a percent. http://metaoptimize.com/projects/wordreprs/ 749 Method LSA-80 LSA-320 LSA-640 RNN-80 RNN-320 RNN-640 RNN-1600 Adjectives 9.2 11.3 9.6 9.3 18.2 21.0 23.9 Nouns 11.1 18.1 10.1 5.2 19.0 25.2 29.2 Verbs 17.4 20.7 13.8 30.4 45.0 54.8 62.2 All 12.8 16.5 11.3 16.2 28.5 34"
N13-1120,N09-1003,0,0.168248,"Missing"
N13-1120,J92-4003,0,0.163203,"ra, and learns a highly regularized log-linear model. Finally, the word relation models incorporate existing, specific word relation measures for general relational similarity. 4.1 Directional Similarity Model Our first model for relational similarity extends previous work on semantic word vector representations to a directional similarity model for pairs of words. There are many different methods for creating real-valued semantic word vectors, such as the distributed representation derived from a word co-occurrence matrix and a low-rank approximation (Landauer et al., 1998), word clustering (Brown et al., 1992) and neural-network language modeling (Bengio et al., 2003; Mikolov et al., 2010). Each element in the vectors conceptually represents some latent topicality information of the word. The goal of these methods is that words with similar meanings will tend to be close to each other in the vector space. Although the vector representation of single words has been successfully applied to problems like semantic word similarity and text classification (Turian et al., 2010), the issue of how to represent and compare pairs of words in a vector space remains unclear (Turney, 2012). In a companion paper"
N13-1120,J06-1003,0,0.17158,"Missing"
N13-1120,S12-1047,0,0.257278,"e the problem of attributional similarity, which measures whether two words share similar attributes and is addressed in extensive research work (Budanitsky and Hirst, 2006; Reisinger and Mooney, 2010; Radinsky et al., 2011; Agirre et al., 2009; Yih and Qazvinian, 2012), measuring relational similarity is a relatively new research direction pioneered by Turney (2006), but with many potential applications. For instance, problems of identifying specific relations between words, such as synonyms, ∗ Work conducted while interning at Microsoft Research. In order to promote this research direction, Jurgens et al. (2012) proposed a new shared task of measuring relational similarity in SemEval-2012 recently. In this task, each submitted system is required to judge the degree of a target word pair having a particular relation, measured by its relational similarity compared to a few prototypical example word pairs. The system performance is evaluated by its correlation with the human judgments using two evaluation metrics, Spearman’s rank correlation and MaxDiff accuracy (more details of the task and evaluation metrics will be given in Sec. 3). Although participating systems incorporated substantial amounts of i"
N13-1120,N13-1090,1,0.50038,"and neural-network language modeling (Bengio et al., 2003; Mikolov et al., 2010). Each element in the vectors conceptually represents some latent topicality information of the word. The goal of these methods is that words with similar meanings will tend to be close to each other in the vector space. Although the vector representation of single words has been successfully applied to problems like semantic word similarity and text classification (Turian et al., 2010), the issue of how to represent and compare pairs of words in a vector space remains unclear (Turney, 2012). In a companion paper (Mikolov et al., 2013), we present a vector offset method which performs consistently well in identifying both syntactic and semantic regularities. This method measures the degree of the analogy “a is to b as c is to d” using the cosine score of (~vb − ~va + ~vc , ~vd ), where a, b, c, d are the four given words and ~va , ~vb , ~vc , ~vd are the corresponding vec1003 q v2' desk furniture v2' Figure 1: Directional vectors υ1 and υ2 capture the relations of clothing:shirt and furniture:desk respectively in this semantic vector space. The relational similarity of these two word pairs is estimated by the cosine of θ. t"
N13-1120,S12-1070,0,0.170569,", which includes 79 relation categories exemplified by three or four prototypical word pairs and a schematic description. For example, for the Class-Inclusion:Taxonomic relation, the schematic description is “Y is a kind/type/instance of X”. Using Amazon Mechanical Turk1 , they collected word pairs for each relation, as well as their degrees of being a good representative of a particular relation when compared with defining examples. Participants of this shared task proposed various kinds of approaches that leverage both lexical resources and general corpora. For instance, the Duluth systems (Pedersen, 2012) created word vectors based on WordNet and estimated the degree of a relation using cosine similarity. The BUAP system (Tovar et al., 2012) represented each word pair as a whole by a vector of 4 different types of features: context, WordNet, POS tags and the average number of words separating the two words in text. The degree of relation was then determined by the cosine distance of the target pair from the prototypical examples of each relation. Although their models incorporated a significant amount of information of words or word pairs, unfortunately, the performance were not much better th"
N13-1120,N10-1013,0,0.17404,"Missing"
N13-1120,S12-1055,0,0.380079,"mitted system is required to judge the degree of a target word pair having a particular relation, measured by its relational similarity compared to a few prototypical example word pairs. The system performance is evaluated by its correlation with the human judgments using two evaluation metrics, Spearman’s rank correlation and MaxDiff accuracy (more details of the task and evaluation metrics will be given in Sec. 3). Although participating systems incorporated substantial amounts of information from lexical resources (e.g., WordNet) and contextual patterns from large corpora, only one system (Rink and Harabagiu, 2012) is able to outperform a simple baseline that uses PMI (pointwise mutual information) scoring, which demonstrates the difficulty of this task. In this paper, we explore the problem of measuring relational similarity in the same task setting. We argue that due to the large number of possible relations, building an ensemble of relational simi1000 Proceedings of NAACL-HLT 2013, pages 1000–1009, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics larity models based on heterogeneous information sources is the key to advance the state-of-the-art on this problem. By co"
N13-1120,W01-0511,0,0.0464238,"paper is organized as follows. We first survey the related work in Sec. 2 and formally define the problem in Sec. 3. We describe the individual models in detail in Sec. 4. The combination approach is depicted in Sec. 5, along with experimental comparisons to individual models and existing systems. Finally, Sec. 6 concludes the paper. 2 Related Work Building a classifier to determine whether a relationship holds between a pair of words is a natural approach to the task of measuring relational similarity. While early work was mostly based on hand-crafted rules (Finin, 1980; Vanderwende, 1994), Rosario and Hearst (2001) introduced a machine learning approach to classify word pairs. They targeted classifying noun modifier pairs from the medical domain into 13 classes of semantic relations. Features for each noun modifier pair were constructed 1001 using large medical lexical resources and a multiclass classifier was trained using a feed-forward neural network with one hidden layer. This work was later extended by Nastase and Szpakowicz (2003) to classify general domain noun-modifier pairs into 30 semantic relations. In addition to extracting features using WordNet and Roget’s Thesaurus, they also experimented"
N13-1120,S12-1071,0,0.367112,"f attributional similarity, which measures whether two words share similar attributes and is addressed in extensive research work (Budanitsky and Hirst, 2006; Reisinger and Mooney, 2010; Radinsky et al., 2011; Agirre et al., 2009; Yih and Qazvinian, 2012), measuring relational similarity is a relatively new research direction pioneered by Turney (2006), but with many potential applications. For instance, problems of identifying specific relations between words, such as synonyms, ∗ Work conducted while interning at Microsoft Research. In order to promote this research direction, Jurgens et al. (2012) proposed a new shared task of measuring relational similarity in SemEval-2012 recently. In this task, each submitted system is required to judge the degree of a target word pair having a particular relation, measured by its relational similarity compared to a few prototypical example word pairs. The system performance is evaluated by its correlation with the human judgments using two evaluation metrics, Spearman’s rank correlation and MaxDiff accuracy (more details of the task and evaluation metrics will be given in Sec. 3). Although participating systems incorporated substantial amounts of i"
N13-1120,P10-1040,0,0.0899246,"ted representation derived from a word co-occurrence matrix and a low-rank approximation (Landauer et al., 1998), word clustering (Brown et al., 1992) and neural-network language modeling (Bengio et al., 2003; Mikolov et al., 2010). Each element in the vectors conceptually represents some latent topicality information of the word. The goal of these methods is that words with similar meanings will tend to be close to each other in the vector space. Although the vector representation of single words has been successfully applied to problems like semantic word similarity and text classification (Turian et al., 2010), the issue of how to represent and compare pairs of words in a vector space remains unclear (Turney, 2012). In a companion paper (Mikolov et al., 2013), we present a vector offset method which performs consistently well in identifying both syntactic and semantic regularities. This method measures the degree of the analogy “a is to b as c is to d” using the cosine score of (~vb − ~va + ~vc , ~vd ), where a, b, c, d are the four given words and ~va , ~vb , ~vc , ~vd are the corresponding vec1003 q v2' desk furniture v2' Figure 1: Directional vectors υ1 and υ2 capture the relations of clothing:s"
N13-1120,J06-3003,0,0.885575,"ne the degree of correspondence between two word pairs. For instance, the analogous word pairs silverware:fork and clothing:shirt both exemplify well a Class-Inclusion:Singular Collective relation and thus have high relational similarity. Unlike the problem of attributional similarity, which measures whether two words share similar attributes and is addressed in extensive research work (Budanitsky and Hirst, 2006; Reisinger and Mooney, 2010; Radinsky et al., 2011; Agirre et al., 2009; Yih and Qazvinian, 2012), measuring relational similarity is a relatively new research direction pioneered by Turney (2006), but with many potential applications. For instance, problems of identifying specific relations between words, such as synonyms, ∗ Work conducted while interning at Microsoft Research. In order to promote this research direction, Jurgens et al. (2012) proposed a new shared task of measuring relational similarity in SemEval-2012 recently. In this task, each submitted system is required to judge the degree of a target word pair having a particular relation, measured by its relational similarity compared to a few prototypical example word pairs. The system performance is evaluated by its correla"
N13-1120,C08-1114,0,0.0650569,"pair based on statistics of their co-occurrence patterns in Web documents and applied the k-NN method (k = 1 in their work). Measuring relational similarity, which determines whether two word pairs share the same relation, can be viewed as an extension of classifying relations between two words. Treating a relational similarity measure as a distance metric, a testing pair of words can be judged by whether they have a relation that is similar to some prototypical word pairs having a particular relation. A multi-relation classifier can thus be built easily in this framework as demonstrated in (Turney, 2008), where the problems of identifying synonyms, antonyms and associated words are all reduced to finding good analogous word pairs. Measuring relational similarity has been advocated and pioneered by Turney (2006), who proposed a latent vector space model for answering SAT analogy questions (e.g., mason:stone vs. carpenter:wood). In contrast, we take a slightly different view when building a relational similarity measure. Existing classifiers for specific word relations (e.g., synonyms or Is-A) are combined with general relational similarity measures. Empirically, mixing heterogeneous models ten"
N13-1120,C94-2125,0,0.0367627,"ed. The rest of this paper is organized as follows. We first survey the related work in Sec. 2 and formally define the problem in Sec. 3. We describe the individual models in detail in Sec. 4. The combination approach is depicted in Sec. 5, along with experimental comparisons to individual models and existing systems. Finally, Sec. 6 concludes the paper. 2 Related Work Building a classifier to determine whether a relationship holds between a pair of words is a natural approach to the task of measuring relational similarity. While early work was mostly based on hand-crafted rules (Finin, 1980; Vanderwende, 1994), Rosario and Hearst (2001) introduced a machine learning approach to classify word pairs. They targeted classifying noun modifier pairs from the medical domain into 13 classes of semantic relations. Features for each noun modifier pair were constructed 1001 using large medical lexical resources and a multiclass classifier was trained using a feed-forward neural network with one hidden layer. This work was later extended by Nastase and Szpakowicz (2003) to classify general domain noun-modifier pairs into 30 semantic relations. In addition to extracting features using WordNet and Roget’s Thesau"
N13-1120,N12-1077,1,0.705437,"se in Spearman’s rank correlation. 1 Introduction The problem of measuring relational similarity is to determine the degree of correspondence between two word pairs. For instance, the analogous word pairs silverware:fork and clothing:shirt both exemplify well a Class-Inclusion:Singular Collective relation and thus have high relational similarity. Unlike the problem of attributional similarity, which measures whether two words share similar attributes and is addressed in extensive research work (Budanitsky and Hirst, 2006; Reisinger and Mooney, 2010; Radinsky et al., 2011; Agirre et al., 2009; Yih and Qazvinian, 2012), measuring relational similarity is a relatively new research direction pioneered by Turney (2006), but with many potential applications. For instance, problems of identifying specific relations between words, such as synonyms, ∗ Work conducted while interning at Microsoft Research. In order to promote this research direction, Jurgens et al. (2012) proposed a new shared task of measuring relational similarity in SemEval-2012 recently. In this task, each submitted system is required to judge the degree of a target word pair having a particular relation, measured by its relational similarity co"
N13-1120,D12-1111,1,0.845237,"Missing"
N16-4003,C02-1169,0,\N,Missing
N16-4003,P01-1037,0,\N,Missing
N16-4003,W02-1033,0,\N,Missing
N16-4003,D13-1020,0,\N,Missing
N16-4003,D14-1070,0,\N,Missing
N16-4003,P13-1158,0,\N,Missing
N16-4003,P14-1090,0,\N,Missing
N16-4003,D12-1035,0,\N,Missing
N16-4003,P14-1133,0,\N,Missing
N16-4003,P15-1128,1,\N,Missing
N16-4003,D13-1160,0,\N,Missing
N16-4003,N12-4004,0,\N,Missing
N18-1144,C16-1107,0,0.0227546,"t real-world processes, and (2) we propose two new models that learn to infer and propagate entity states in novel ways, and outperform existing methods on this dataset. Related Work Datasets: Large-scale reading comprehension datasets, e.g., SQuAD (Rajpurkar et al., 2016), TriviaQA (Joshi et al., 2017), have successfully driven progress in question answering, but largely targeting explicitly stated facts. Often, the resulting systems can be fooled (Jia and Liang, 2017), prompting efforts to create harder datasets where a deeper understanding of the text appears necessary (Welbl et al., 2017; Araki et al., 2016). Procedural text is a genre that is particularly challenging, because the worlds they describe are largely implicit and changing. While there are few large datasets in this genre, two exceptions are bAbI (Weston et al., 2015) and SCoNE (Long et al., 2016), described earlier2 . bAbI has helped advance methods for reasoning over text, such as memory network architectures (Weston et al., 2014), but has also been criticized for using machine-generated text over a simulated domain. SCoNE is closer to our goal, but has a different task (given a perfect world model of the initial state, predict the"
N18-1144,D13-1160,0,0.0477917,"oftmax. The detailed comparisons in their design are shown in Table 2. We use the released implementations10 (with default hyper-parameter values), and retrained them on our dataset, adapted to the standard bAbI QA format. Specifically, we create three separate variations of data by adding three bAbI-style questions after each step in a paragraph, respectively: Q1. “Does e exist?” (yes/no) Q2. “Is the location of e known?” (yes/no) Q3. “Where is e?” (span) The template Q1 is applied to all participants. Q2 9 This approach has been adopted previously for questions with multiple answers (e.g., (Berant et al., 2013)). For questions with only one answer, F1 is equivalent to accuracy. 10 https://github.com/siddk/entity-network and https://github.com/uwnlp/qrn will only be present in the training data if Q1 is “yes”, and similarly Q3 is only present if Q2 is “yes”. These three variations of data are used to train three different models from the same method. At test time, we cascade the questions (e.g., ask Q2 only if the answer to the Q1 model is “yes”), and combine the model outputs accordingly to answer the questions in our target tasks (Section 5.1). We also compare against a rule-based baseline and a fe"
N18-1144,D14-1159,1,0.800052,"over text, such as memory network architectures (Weston et al., 2014), but has also been criticized for using machine-generated text over a simulated domain. SCoNE is closer to our goal, but has a different task (given a perfect world model of the initial state, predict the end state) and different motivation (handling ellipsis and coreference in context). It also used a deterministic, simulated world to generate data. Models: For answering questions about procedural text, early systems attempted to extract a process structure (events, arguments, relations) from the paragraph, e.g., ProRead (Berant et al., 2014) and for newswire (Caselli et al., 2017). This allowed questions about event ordering to be answered, but not about state changes, unmodelled by these approaches. More recently, several neural systems have been developed to answer questions about the world state after a process, inspired in part by the bAbI dataset. Building on the general Memory Network architecture (Weston et al., 2014) and gated recurrent models such as GRU (Cho et al., 2014), Recurrent Entity Networks (EntNet) (Henaff et al., 2016) is a state-of-the-art method for bAbI. EntNet uses a dynamic memory of hidden states (memory"
N18-1144,P16-1223,0,0.10149,"Missing"
N18-1144,W14-4012,0,0.0586297,"Missing"
N18-1144,D17-1215,0,0.124468,"re challenging because questions (e.g., the one shown here) often require inference about the process states. http://data.allenai.org/propara. 1 Introduction Building a reading comprehension (RC) system that is able to read a text document and to answer questions accordingly has been a long-standing goal in NLP and AI research. Impressive progress has been made in factoid-style reading comprehension, e.g., (Seo et al., 2017a; Clark and Gardner, 2017), enabled by well-designed datasets and modern neural network models. However, these models still struggle with questions that require inference (Jia and Liang, 2017). Consider the paragraph in Figure 1 about photosynthesis. While top systems on SQuAD (Rajpurkar et al., 2016) can reliably answer lookup questions such as: Q1: What do the roots absorb? (A: water, minerals) they struggle when answers are not explicit, e.g., Q2: Where is sugar produced? (A: in the leaf)1 ∗* Bhavana Dalvi Mishra and Lifu Huang contributed equally to this work. 1 For example, the RC system BiDAF (Seo et al., 2017a) answers “glucose” to this question. To answer Q2, it appears that a system needs knowledge of the world and the ability to reason with state transitions in multiple s"
N18-1144,P17-1147,0,0.0624669,"Missing"
N18-1144,P16-1138,0,0.219513,"in Figure 1. Understanding what is happening in such texts is important for many tasks, e.g., procedure execution and validation, effect prediction. However, it is also difficult because the world state is changing, and the causal effects of actions on that state are often implicit. To address this challenging style of reading comprehension problem, researchers have created several datasets. The bAbI dataset (Weston et al., 2015) includes questions about objects moved throughout a paragraph, using machine-generated language over a deterministic domain with a small lexicon. The SCoNE dataset (Long et al., 2016) contains paragraphs describing a changing world state in three synthetic, deterministic domains, and 1595 Proceedings of NAACL-HLT 2018, pages 1595–1604 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 Figure 2: A (simplified) annotated paragraph from ProPara. Each filled row shows the existence and location of participants between each step (“?” denotes “unknown”, “-” denotes “does not exist”). For example in state0, water is located at the soil. assumes that a complete and correct model of the initial state is given for each task. However, approac"
N18-1144,D14-1162,0,0.0951594,"eural network architecture (biLSTM) with attention for input encoding. The prediction tasks are handled by two different output layers. We give the detail of these layers below. The design of ProLocal consists of two main components: local prediction and commonsense persistence. The former infers all direct effects of individual sentences and the latter algorithmically propagates known values forwards and backwards to fill in any remaining unknown states. Input Encoding: Each word wi in the input sentence is encoded with a vector xi = [vw : ve : vv ], the concatenation of a pre-trained GloVe (Pennington et al., 2014) word embedding vw , indicator variables ve on whether wi is the specified participant and vv on whether wi is a verb (via a POS tagger). Table 1: ProPara vs. other procedural datasets. was then split 80/10/10 into train/dev/test by process prompt, ensuring that the test paragraphs were all about processes unseen in train and dev. Table 1 compares our dataset with bAbI and SCoNE. 4 Models We present two new models for this task. The first, ProLocal, makes local state predictions and then algorithmically propagates them through the process. The second, ProGlobal, is an end-to-end neural model t"
N18-1144,D16-1264,0,0.0641623,"s. http://data.allenai.org/propara. 1 Introduction Building a reading comprehension (RC) system that is able to read a text document and to answer questions accordingly has been a long-standing goal in NLP and AI research. Impressive progress has been made in factoid-style reading comprehension, e.g., (Seo et al., 2017a; Clark and Gardner, 2017), enabled by well-designed datasets and modern neural network models. However, these models still struggle with questions that require inference (Jia and Liang, 2017). Consider the paragraph in Figure 1 about photosynthesis. While top systems on SQuAD (Rajpurkar et al., 2016) can reliably answer lookup questions such as: Q1: What do the roots absorb? (A: water, minerals) they struggle when answers are not explicit, e.g., Q2: Where is sugar produced? (A: in the leaf)1 ∗* Bhavana Dalvi Mishra and Lifu Huang contributed equally to this work. 1 For example, the RC system BiDAF (Seo et al., 2017a) answers “glucose” to this question. To answer Q2, it appears that a system needs knowledge of the world and the ability to reason with state transitions in multiple sentences: If carbon dioxide enters the leaf (stated), then it will be at the leaf (unstated), and as it is the"
N18-1144,H89-1033,0,0.45403,"Missing"
N18-2115,D11-1039,0,0.0113816,"dings by Hashimoto et al. (2017) (100 dimension) and the GloVe word embedding (100 dimension) by Pennington et al. (2014); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer la"
N18-2115,D17-1206,0,0.0607491,"Missing"
N18-2115,P17-1089,0,0.0991901,"Missing"
N18-2115,D13-1160,0,0.0744986,"17) (100 dimension) and the GloVe word embedding (100 dimension) by Pennington et al. (2014); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given i"
N18-2115,D17-1160,0,0.0288339,"dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given input/output examples, while program synthesis models (Zhong et al., 2017; Parisotto et al., 2017) aim to generate explicit programs and"
N18-2115,J84-3009,0,0.113713,"Missing"
N18-2115,P14-5010,0,0.00279152,"arameters with gradient descent: θi0 = θ − α∇θ LTi (fθ ) 9: end for P 10: Update θ ← θ − β∇θ Ti ∼p(T ) LTi (fθi0 ) using each Di0 from Ti and LTi for the metaupdate 11: end while In this section, we introduce the WikiSQL dataset and preprocessing steps, the learner model in our meta-learning setup, and the experimental results. {x(j) , y(j) } 4.1 Dataset We evaluate our model on the WikiSQL dataset (Zhong et al., 2017). We follow the data preprocessing in (Wang et al., 2017). Specifically, we first preprocess the dataset by running both tables and question-query pairs through Stanford Stanza (Manning et al., 2014) using the script included with the WikiSQL dataset, which normalizes punctuations and cases of the dataset. We further normalize each question based on its corresponding table: for table entries and columns occurring in questions or queries, we normalize their format to be consistent with the table. After preprocessing, we filter the training set by removing pairs whose ground truth solution contains constants not mentioned in the question, as our model requires the constants to be copied from the question. We train and tune our model only on the filtered training and filtered development set"
N18-2115,D16-1166,1,0.852119,"14); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given input/output examples, while program synthesis models (Zhong et al., 2017; Parisotto et a"
N18-2115,P15-1129,0,0.0351406,"nnington et al. (2014); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given input/output examples, while program synthesis models (Zhong et al.,"
N18-2115,P15-1128,1,0.891606,"Missing"
N18-2115,P14-2105,1,0.83,"ng (100 dimension) by Pennington et al. (2014); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given input/output examples, while program synthes"
N18-2115,Q14-1042,0,0.0446038,"nd the GloVe word embedding (100 dimension) by Pennington et al. (2014); each token is embedded into a 200 dimensional vector. The encoder is a 3layer bidirectional LSTM with hidden states of size 100, and the decoder is a 3-layer unidirectional LSTM with hidden states of size 100. The model is trained with question-query pairs with a 735 800 relevance function. Semantic Parsing Mapping natural language to logic forms has been actively studied in natural language processing research (Zettlemoyer and Collins, 2005; Giordani and Moschitti, 2010; Artzi and Zettlemoyer, 2011; Berant et al., 2013; Vlachos and Clark, 2014; Yih et al., 2014, 2015; Wang et al., 2015; Golub and He, 2016; Iyer et al., 2017; Krishnamurthy et al., 2017). However, unlike conventional approaches, which fit one model for all training examples, the proposed approach learns to adapt to new tasks. By using the support set based on the relevance function, the proposed model can adapt to a unique model for each example. Program Induction / Synthesis Program induction (Reed and De Freitas, 2016; Neelakantan et al., 2015; Graves et al., 2014; Yin et al., 2015; Devlin et al., 2017) aims to infer latent programs given input/output examples, whi"
N19-1244,D14-1159,1,0.853396,"inimization for data graph. Talukdar et al. (2008) propose a graphbased semi-supervised label propagation algorithm for acquiring open-domain labeled classes and their instances from a combination of unstructured and structured text sources. Our framework extends these ideas by introducing the notion of groups (examples that are expected to be similar) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event ordering and event arguments for biology processes. bAbI (Weston et al., 2015) includes questions about movement of entities, however it’s synthetically generated and with a small lexicon. Kiddon et al. (2015)’s RECIPES dataset introduces the task of predicting the locations of cooking ingredients, and Kiddon et al. (2016) for recipe generation. In this paper, we continue this line of exploration using ProPara, and illustrate how the previous two lines of work (label consistency and semi-supervised learning) can be integrated. 3 3.1 Problem Definition ferent news st"
N19-1244,P16-1223,0,0.0514622,"Missing"
N19-1244,N18-1144,1,0.91825,"agraph about photosynthesis, a recipe). This task is challenging as the world is changing throughout the text, and despite recent advances, current systems still struggle with this task. Our approach is to leverage the fact that, for many procedural texts, multiple independent descriptions are readily available, and that predictions from them should be consistent (label consistency). We present a new learning framework that leverages label consistency during training, allowing consistency bias to be built into the model. Evaluation on a standard benchmark dataset for procedural text, ProPara (Dalvi et al., 2018), shows that our approach significantly improves prediction performance (F1) over prior state-of-the-art systems. 1 Figure 1: Fragments from three independent texts about photosynthesis. Although (1) is ambiguous as to whether oxygen is being created or merely moved, evidence from (2) and (3) suggests it is being created, helping to correctly interpret (1). More generally, encouraging consistency between predictions from different paragraphs about the same process/procedure can improve performance. Introduction We address the task of procedural text comprehension, namely tracking how the prope"
N19-1244,W18-2501,0,0.0285905,"Missing"
N19-1244,P18-1075,0,0.0217954,"ork is related to several important branches of work in both NLP and ML, as we now summarize. Leveraging Label Consistency Leveraging information about label consistency (i.e., similar instances should have consistent labels at a certain granularity) is an effective idea. It has been studied in computer vision (Haeusser et al., 2017; Chen et al., 2018) and IR (Clarke et al., 2001; Dumais et al., 2002). Learning by association (Haeusser et al., 2017) establishes implicit cross-modal links between similar descriptions and leverage more unlabeled data during training. Schütze et al. (2018); 2348 Hangya et al. (2018) adapt the similar idea to exploit unlabeled data for the cross-lingual classification. We extend this line of research in two ways: by developing a framework allowing it to be applied to the task of structure prediction; and by incorporating label consistency into the model itself via end-to-end training, rather than enforcing consistency as a post-processing step. Semi-supervised Learning Approaches Besides utilizing the label consistency knowledge, our learning framework is also able to use unlabeled paragraphs, which fits in the literature of semisupervised learning approaches (for NLP). Z"
N19-1244,D15-1114,0,0.0604539,"ces. Our framework extends these ideas by introducing the notion of groups (examples that are expected to be similar) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event ordering and event arguments for biology processes. bAbI (Weston et al., 2015) includes questions about movement of entities, however it’s synthetically generated and with a small lexicon. Kiddon et al. (2015)’s RECIPES dataset introduces the task of predicting the locations of cooking ingredients, and Kiddon et al. (2016) for recipe generation. In this paper, we continue this line of exploration using ProPara, and illustrate how the previous two lines of work (label consistency and semi-supervised learning) can be integrated. 3 3.1 Problem Definition ferent news stories about a political meeting, we expect top-level features (e.g., where the meeting took place, who attended) to be similar; for different recipes for the same item, we expect loosely similar ingredients and steps; and for different i"
N19-1244,D16-1032,0,0.0264777,"r) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event ordering and event arguments for biology processes. bAbI (Weston et al., 2015) includes questions about movement of entities, however it’s synthetically generated and with a small lexicon. Kiddon et al. (2015)’s RECIPES dataset introduces the task of predicting the locations of cooking ingredients, and Kiddon et al. (2016) for recipe generation. In this paper, we continue this line of exploration using ProPara, and illustrate how the previous two lines of work (label consistency and semi-supervised learning) can be integrated. 3 3.1 Problem Definition ferent news stories about a political meeting, we expect top-level features (e.g., where the meeting took place, who attended) to be similar; for different recipes for the same item, we expect loosely similar ingredients and steps; and for different images of the same person, we expect some high-level characteristics (e.g., height, face shape) to be similar. Note"
N19-1244,D14-1162,0,0.0816999,"ncoder ProStruct uses an encoder-decoder architecture that takes procedural text as input and predicts the state changes of entities E in the text as output. During encoding, each step st is encoded using |E |embeddings, one for each entity e j ∈ E. Each embedding represents the action that st describes, applied to ek . The model thus allows the same action to have different effects on different entities (e.g., a transformation destroys one entity, and creates another). For each (st , e j ) ∈ S × E pair, the step is fed into a BiLSTM (Hochreiter and Schmidhuber, 1997), using pretrained GloVe (Pennington et al., 2014) vectors vw for each word wi concatenated with two indicator variables, one indicating whether wi is a word referring to e j , and one indicating whether wi is a verb. A bilinear attention layer then computes attention over the contextualized vectors hi output by the BiLSTM: ai = hi ∗ B∗hev +b , where B and b are learned parameters, and hev is the concatenation of he (the averaged contextualized embedding for the entity words we ) and hv (the averaged contextualized embedding for the verb words wv ). Finally, the output vector ct j is the attentionPI weighted sum of the hi : ct j = i=1 ai ∗ hi"
N19-1244,D08-1061,0,0.0331192,"rediction; and by incorporating label consistency into the model itself via end-to-end training, rather than enforcing consistency as a post-processing step. Semi-supervised Learning Approaches Besides utilizing the label consistency knowledge, our learning framework is also able to use unlabeled paragraphs, which fits in the literature of semisupervised learning approaches (for NLP). Zhou et al. (2003) propose an iterative label propagation algorithm similar to spectral clustering. Zhu et al. (2003) propose a semi-supervised learning framework via harmonic energy minimization for data graph. Talukdar et al. (2008) propose a graphbased semi-supervised label propagation algorithm for acquiring open-domain labeled classes and their instances from a combination of unstructured and structured text sources. Our framework extends these ideas by introducing the notion of groups (examples that are expected to be similar) and summaries (what similarities are expected), applied in an end-to-end-framework. Procedural Text Understanding and Reading Comprehension There has been a growing interest in procedural text understanding/QA recently. The ProcessBank dataset (Berant et al., 2014) asks questions about event or"
N19-1244,D18-1006,1,0.889929,"Missing"
P06-1065,N06-1015,0,0.255337,"Missing"
P06-1065,N06-1014,0,0.18572,"Missing"
P06-1065,H05-1009,0,0.0719034,"tences and a proposed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a generative model that incorporates a ˆ = argmaxa n X λi fi (a, e, f ) i=1 where the fi are features and the"
P06-1065,P05-1057,0,0.417305,"eature values extracted from a pair of sentences and a proposed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a generative model that incorporates a ˆ = argmaxa n X λi fi (a, e"
P06-1065,J93-2003,0,0.032918,"Missing"
P06-1065,W03-0301,0,0.08921,"Missing"
P06-1065,P03-1012,0,0.250546,"Missing"
P06-1065,H05-1011,1,0.716698,"or many years, statistical machine translation relied on generative models to provide bilingual word alignments. In 2005, several independent efforts showed that discriminative models could be used to enhance or replace the standard generative approach. Building on this work, we demonstrate substantial improvement in word-alignment accuracy, partly though improved training methods, but predominantly through selection of more and better features. Our best model produces the lowest alignment error rate yet reported on Canadian Hansards bilingual data. 2 Overall Approach As in our previous work (Moore, 2005), we train two models we call stage 1 and stage 2, both in the form of a weighted linear combination of feature values extracted from a pair of sentences and a proposed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMba"
P06-1065,W02-1001,0,0.301818,"e the possiblity that more than one alignment may have the same score, which we previously did not take into account. First, we modified the beam search so that the beam size dynamically expands if needed to accomodate all the possible alignments that have the same score. Second we implemented a structural tie breaker, so that the same alignment will always be chosen as the one-best from a set of alignments having the same score. Neither of these changes significantly affected the alignment results. The principal training method is an adaptation of averaged perceptron learning as described by Collins (2002). The differences between our current and earlier training methods mainly address the observation that perceptron training is very sensitive to the order in which data is presented to the learner. We also investigated the large-margin training technique described by Tsochantaridis et al. (2004). The training procedures are described in Sections 5 and 6. two hard constraints. One constraint was that the only alignment patterns allowed were 1–1, 1–2, 1– 3, 2–1, and 3–1. Thus, many-to-many link patterns were disallowed, and a single word could be linked to at most three other words. The second co"
P06-1065,J03-1002,0,0.113683,"and stage 2, both in the form of a weighted linear combination of feature values extracted from a pair of sentences and a proposed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a"
P06-1065,W05-0814,0,0.0230991,"acted from a pair of sentences and a proposed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a generative model that incorporates a ˆ = argmaxa n X λi fi (a, e, f ) i=1 where the fi a"
P06-1065,H05-1010,0,0.813027,"ed word alignment of them. The possible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a generative model that incorporates a ˆ = argmaxa n X λi fi (a, e, f ) i=1 where the fi are features and the λi are weights. The"
P06-1065,H05-1012,0,0.164313,"sible alignment having the highest overall score is selected for each sentence pair. Thus, for a sentence pair (e, f ) we seek the alignment a ˆ such that 1 Introduction Until recently, almost all work in statistical machine translation was based on word alignments obtained from combinations of generative probabalistic models developed at IBM by Brown et al. (1993), sometimes augmented by an HMMbased model or Och and Ney’s “Model 6” (Och and Ney, 2003). In 2005, however, several independent efforts (Liu et al., 2005; Fraser and Marcu, 2005; Ayan et al., 2005; Taskar et al., 2005; Moore, 2005; Ittycheriah and Roukos, 2005) demonstrated that discriminatively trained models can equal or surpass the alignment accuracy of the standard models, if the usual unlabeled bilingual training corpus is supplemented with human-annotated word alignments for only a small subset of the training data. The work cited above makes use of various training procedures and a wide variety of features. Indeed, whereas it can be difficult to design a factorization of a generative model that incorporates a ˆ = argmaxa n X λi fi (a, e, f ) i=1 where the fi are features and the λi are weights. The models are trained on a large number of bili"
P13-1171,N09-1003,0,0.0569711,"Missing"
P13-1171,P11-1059,0,0.00851492,"ge of the Is-A relation (Song et al., 2011). For instance, when a word refers to a named entity, the particular sense and meaning is often not encoded. As a result, relations such as “Apple” is-a “company” and “Jaguar” is-a “car” cannot be found in WordNet. Similar to the case in synonymy, the Is-A relation defined in WordNet does not provide a native, real-valued degree of the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3 Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences"
P13-1171,J06-1003,0,0.00993252,"2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4 Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 1747 network language model (RNNLM) and a concept vector space model learned from click-through data. Semantic word similarity is estimated using the cosine score of the corresponding word vectors in these VSMs. Contextual term-vectors created using the Wikipedia corpus have shown to perform well on measuring word similarity (Reisinger and Mooney, 2010). Following the setting suggested by Yih and Qazvinian (2012), we create termvectors representing about 1 million words by aggregating terms within a window of [−10, 10] of each occurrence of the target word. The vectors are further refined by"
P13-1171,N10-1066,1,0.703494,"question/sentence pairs, such as the one below. Q: Which was the first movie that James Dean was in? A: James Dean, who began as an actor on TV dramas, didn’t make his screen debut until 1951’s “Fixed Bayonet.” While this sentence correctly answers the question, the fact that James Dean began as a TV actor is unrelated to the question. As a result, an “ideal” word alignment structure should not link words in this clause to those in the question. In order to leverage the latent structured information, we adapt a recently proposed framework of learning constrained latent representations (LCLR) (Chang et al., 2010). LCLR can be viewed as a variant of Latent-SVM (Felzenszwalb et al., 2009) with different learning formulations and a general inference framework. The idea of LCLR is to replace the decision function of a standard linear model θT φ(x) with arg max θT φ(x, h), (3) h where θ represents the weight vector and h represents the latent variables. In this answer selection task, x = (q, s) represents a pair of question q and candidate sentence s. As described in Sec. 3, h refers to the latent alignment between q and s. The intuition behinds Eq. (3) is: candidate sentence s correctly answers question q"
P13-1171,C04-1051,0,0.0404812,"ons. First, although we focus on improving TREC-style open-domain question answering in this work, we would like to apply the proposed technology to other QA scenarios, such as community-based QA (CQA). For instance, the sentence matching technique can help map a given question to some questions in an existing CQA database (e.g., Yahoo! Answers). Moreover, the answer sentence selection scheme could also be useful in extracting the most related sentences from the answer text to form a summary answer. Second, because the task of answer sentence selection is very similar to paraphrase detection (Dolan et al., 2004) and recognizing textual entailment (Dagan et al., 2006), we would like to investigate whether systems for these tasks can be improved by incorporating enhanced lexical semantic knowledge as well. Finally, we would like to improve our system for the answer sentence selection task and for question answering in general. In addition to following the directions suggested by the error analysis presented in Sec. 6.4, we plan to use logic-like semantic representations of questions and sentences, and explore the role of lexical semantics for handling questions that require inference. Acknowledgments W"
P13-1171,P03-1003,0,0.0407351,"e exact answer, along with the document that supports the answer. In contrast, the Jeopardy! TV quiz show provides another open-domain question answering setting, in which IBM’s Watson system famously beat the two highest ranked players (Ferrucci, 2012). Questions in this game are presented in a statement form and the system needs to identify the true question and to give the exact answer. A short sentence or paragraph to justify the answer is not required in either TREC-QA or Jeopardy! As any QA system can virtually be decomposed into two major high-level components, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improve"
P13-1171,C92-2082,0,0.0790644,"the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3 Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having"
P13-1171,N10-1145,0,0.836138,"measure how close the question and sentence can be matched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2 L4 ), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead of focusing on the high-level semantic representation, we turn our attention in this work to improvi"
P13-1171,S12-1047,0,0.00933608,"edge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coefficient score, ρ = 0.619. In comparison, the previous best system (Rink and Harabagiu, 2012) in the task only reaches ρ = 0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similari"
P13-1171,S12-1035,0,0.00801338,"and the lack of the coverage of the Is-A relation (Song et al., 2011). For instance, when a word refers to a named entity, the particular sense and meaning is often not encoded. As a result, relations such as “Apple” is-a “company” and “Jaguar” is-a “car” cannot be found in WordNet. Similar to the case in synonymy, the Is-A relation defined in WordNet does not provide a native, real-valued degree of the relation, which can only be roughly approximated using the number of links on the taxonomy path connecting two 3 Mapping two antonyms may be desired if one of them is in the scope of negation (Morante and Blanco, 2012; Blanco and Moldovan, 2011). However, we do not attempt to resolve the negation scope in this work. concepts (Resnik, 1995). In order to remedy these issues, we augment WordNet with the Is-A relations found in Probase (Wu et al., 2012). Probase is a knowledge base that establishes connections between 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the freq"
P13-1171,N10-1013,0,0.015552,"0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity models (Agirre et al., 2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4 Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 174"
P13-1171,S12-1055,0,0.0136018,"y of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coefficient score, ρ = 0.619. In comparison, the previous best system (Rink and Harabagiu, 2012) in the task only reaches ρ = 0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity"
P13-1171,D07-1002,0,0.0629302,"ction can be naturally reduced to a semantic text matching problem. Conceptually, we would like to measure how close the question and sentence can be matched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2 L4 ), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead"
P13-1171,W06-3104,0,0.00568154,"ents, retrieval and selection (Echihabi and Marcu, 2003), the answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, general tree matching methods based on tree-edit distance have been first proposed by Punyakanok et al. (2004) for a similar answer selection task. Heilman and Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. Although lexical semantic information derived from WordNet has"
P13-1171,C10-1131,0,0.537165,"answer selection problem is clearly critical. Limiting the scope of an answer to a sentence is first highlighted by Wang et al. (2007), who argued that it was more informative to present the whole sentence instead of a short answer to users. Observing the limitations of the bag-of-words models, Wang et al. (2007) proposed a syntaxdriven approach, where each pair of question and sentence are matched by their dependency trees. The mapping is learned by a generative probabilistic model based on a Quasi-synchronous Grammar formulation (Smith and Eisner, 2006). This approach was later improved by Wang and Manning (2010) with a tree-edit CRF model that learns the latent alignment structure. In contrast, general tree matching methods based on tree-edit distance have been first proposed by Punyakanok et al. (2004) for a similar answer selection task. Heilman and Smith (2010) proposed a discriminative approach that first computes a tree kernel function between the dependency trees of the question and candidate sentence, and then learns a classifier based on the tree-edit features extracted. Although lexical semantic information derived from WordNet has been used in some of these approaches, the research has main"
P13-1171,D07-1003,0,0.732536,"ched semantically. Due to the variety of word choices and inherent ambiguities in natural languages, bag-ofwords approaches with simple surface-form word matching tend to produce brittle results with poor prediction accuracy (Bilotti et al., 2007). As a result, researchers put more emphasis on exploiting both the syntactic and semantic structure in questions/sentences. Representative examples include methods based on deeper semantic analysis (Shen and Lapata, 2007; Moldovan et al., 2007) and on tree edit-distance (Punyakanok et al., 2004; Heilman and Smith, 2010) and quasisynchronous grammar (Wang et al., 2007) that match the dependency parse trees of questions and sentences. However, such approaches often require more computational resources. In addition to applying a syntactic or semantic parser during run-time, finding the best matching between structured representations of sentences is not trivial. For example, the computational complexity of tree matching is O(V 2 L4 ), where V is the number of nodes and L is the maximum depth (Tai, 1979). Instead of focusing on the high-level semantic representation, we turn our attention in this work to improving the shallow semantic compo1744 Proceedings of"
P13-1171,N12-1077,1,0.578782,"hyponymy, word similarity is only loosely defined when two words can be associated by some implicit relation.4 The general word similarity model can be viewed as a “back-off” solution when the exact lexical relation (e.g., partwhole and attribute) is not available or cannot be accurately detected. Among various word similarity models (Agirre et al., 2009; Reisinger and Mooney, 2010; Gabrilovich and Markovitch, 2007; Radinsky et al., 2011), the vector space models (VSMs) based on the idea of distributional similarity (Turney and Pantel, 2010) are often used as the core component. Inspired by (Yih and Qazvinian, 2012), which argues the importance of incorporating heterogeneous vector space models for measuring word similarity, we leverage three different VSMs in this work: Wiki term-vectors, recurrent neural 4 Instead of making the distinction, word similarity here refers to the larger set of relations commonly covered by word relatedness (Budanitsky and Hirst, 2006). 1747 network language model (RNNLM) and a concept vector space model learned from click-through data. Semantic word similarity is estimated using the cosine score of the corresponding word vectors in these VSMs. Contextual term-vectors create"
P13-1171,W11-0329,1,0.440917,"ke a 640-dimensional version of RNNLM vectors, which is trained using the Broadcast News corpus of 320M words.5 The final word relatedness model is a projection model learned from the click-through data of a commercial search engine (Gao et al., 2011). Unlike the previous two models, which are created or trained using a text corpus, the input for this model is pairs of aggregated queries and titles of pages users click. This parallel data is used to train a projection matrix for creating the mapping between words in queries and documents based on user feedback, using a Siamese neural network (Yih et al., 2011). Each row vector of this matrix is the dense vector representation of the corresponding word in the vocabulary. Perhaps due to its unique information source, we found this particular word embedding seems to complement the other two VSMs and tends to improve the word similarity measure in general. 5 Learning QA Matching Models In this section, we investigate the effectiveness of various learning models for matching questions and sentences, including the bag-of-words setting 5 http://www.fit.vutbr.cz/˜imikolov/ rnnlm/ and the framework of learning latent structures. 5.1 Bag-of-Words Model The b"
P13-1171,D12-1111,1,0.864508,"nd needs to be handled reliably. Although sets of synonyms can be easily found in thesauri or WordNet synsets, such resources typically cover only strict synonyms. When comparing two words, it is more useful to estimate the degree of synonymy as well. For instance, ship and boat are not strict synonyms because a ship is usually viewed as a large boat. Knowing that two words are somewhat synonymous could be valuable in determining whether they should be mapped. In order to estimate the degree of synonymy, we leverage a recently proposed polarity-inducing latent semantic analysis (PILSA) model (Yih et al., 2012). Given a thesaurus, the model first constructs a signed d-by-n co-occurrence matrix W , where d is the number of word groups and n is the size of the vocabulary. Each row consists of a 2 Proposed by an anonymous reviewer, one justification of this word-alignment approach, where syntactic analysis plays a less important role, is that there are often few sensible combinations of words. For instance, knowing only the set of words {”car”, ”fastest”, ”world”}, one may still guess correctly the question “What is the fastest car in the world?” 1746 group of synonyms and antonyms of a particular sens"
P13-1171,N13-1120,1,0.636053,"en 2.7 million concepts, discovered automatically by applying Hearst patterns (Hearst, 1992) to 1.68 billion Web pages. Its abundant concept coverage distinguishes it from other knowledge bases, such as Freebase (Bollacker et al., 2008) and WikiTaxonomy (Ponzetto and Strube, 2007). Based on the frequency of term co-occurrences, each Is-A relation from Probase is associated with a probability value, indicating the degree of the relation. We verified the quality of Probase Is-A relations using a recently proposed SemEval task of relational similarity (Jurgens et al., 2012) in a companion paper (Zhila et al., 2013), where a subset of the data is to measure the degree of two words having a class-inclusion relation. Probase’s prediction correlates well with the human annotations and achieves a high Spearman’s rank correlation coefficient score, ρ = 0.619. In comparison, the previous best system (Rink and Harabagiu, 2012) in the task only reaches ρ = 0.233. These appealing qualities make Probase a robust lexical semantic model for hypernymy/hyponymy. 4.3 Semantic Word Similarity The third lexical semantic model we introduce targets a general notion of word similarity. Unlike synonymy and hyponymy, word sim"
P13-1171,S12-1032,0,\N,Missing
P13-1171,N13-1106,0,\N,Missing
P14-1066,D13-1176,0,0.177152,"lation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 20"
P14-1066,N03-1017,0,0.177482,"continuous representation of a phrase. Finally, the translation score of a 1 Introduction source-target phrase pair is computed by the disThe phrase translation model, also known as the tance between their feature vectors. The main motivation behind the CPTM is to phrase table, is one of the core components of alleviate the data sparseness problem associated phrase-based statistical machine translation (SMT) with the traditional counting-based methods by systems. The most common method of constructing the phrase table takes a two-phase approach grouping phrases with a similar meaning across (Koehn et al. 2003). First, the bilingual phrase different languages. This style of grouping is pairs are extracted heuristically from an automat- made possible because of the distributed nature of ically word-aligned training data. The second the continuous-space representations for phrases. phase, which is the focus of this paper, is parame- No such sharing was possible in the original symter estimation where each phrase pair is assigned bolic space for representing words or phrases. In with some scores that are estimated based on this model, semantically or grammatically related counting these phrases or thei"
P14-1066,2005.mtsummit-posters.11,0,0.0297066,"Missing"
P14-1066,P06-1096,0,0.0100054,"ns a real-valued weight to each feature. The components GEN(. ), ? and ? define a loglinear model that maps ?? to an output sentence as follows: ? ∗ = argmax ?T ?(?? , ?, ?) (1) (?,?)∈GEN(?? ) which states that given ? and ?, argmax returns the highest scoring translation ? ∗ , maximizing over correspondences ?. In phrase-based SMT, ? consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since computing the argmax exactly is intractable, it is commonly performed approximatedly by beam search (Och and Ney 2004). Following Liang et al. (2006), we assume that every translation candidate is always coupled with a corresponding ?, called the Viterbi derivation, generated by (1). 4 A Continuous-Space Phrase Translation Model (CPTM) The architecture of the CPTM is shown in Figures 1 and 2, where for each pair of source and target phrases (?? , ?? ) in a source-target sentence pair, we first project them into feature vectors ??? and ??? in a latent, continuous space via a neural network with one hidden layer (as shown in Figure 2), and then compute the translation score, score(?? , ?? ), by the distance of their feature vectors in that s"
P14-1066,N13-1048,1,0.931372,"proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input ? ∈ ℱ to output ? ∈ ℰ. We are given  Training samples (?? , ?? ) for ? = 1 … ?, where eac"
P14-1066,P12-1031,1,0.819542,"ured in selecting translations. However, longer phrases occur less often in trainThis paper tackles the sparsity problem in ing data, leading to a severe data sparseness probestimating phrase translation probabilities lem in parameter estimation. There has been a by learning continuous phrase representaplethora of research reported in the literature on tions, whose distributed nature enables the improving parameter estimation for the phrase sharing of related phrases in their representranslation model (e.g., DeNero et al. 2006; tations. A pair of source and target phrases Wuebker et al. 2010; He and Deng 2012; Gao and are projected into continuous-valued vecHe 2013). tor representations in a low-dimensional This paper revisits the problem of scoring a latent space, where their translation score phrase translation pair by developing a Continuis computed by the distance between the ous-space Phrase Translation Model (CPTM). pair in this new space. The projection is The translation score of a phrase pair in this model performed by a neural network whose is computed as follows. First, we represent each weights are learned on parallel training phrase as a bag-of-words vector, called word vecdata. Exper"
P14-1066,P07-2045,0,0.0641989,"us representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input ? ∈ ℱ to output ? ∈ ℰ. We are given  Training samples (?? , ?? ) for ? = 1 … ?, where each source sentence ?? is paired with a reference translation in target language ?? ;  A procedure GEN to generate a list of N-best candidates GEN(?? ) for an input ?? , where GEN in this study is the baseline phrasebased SMT system, i.e., an in-house implementation of the Moses system (Koehn et al. 2007) that does not use the CPTM, and each ? ∈ GEN(?? ) is labeled by the sentence-level BLEU score (He and Deng 2012), denoted by sBleu(?? , ?) , which measures the quality of ? with respect to its reference translation ?? ;  A vector of features ? ∈ ℝ? that maps each (?? , ?) to a vector of feature values2; and  A parameter vector ? ∈ ℝ? , which assigns a real-valued weight to each feature. The components GEN(. ), ? and ? define a loglinear model that maps ?? to an output sentence as follows: ? ∗ = argmax ?T ?(?? , ?, ?) (1) (?,?)∈GEN(?? ) which states that given ? and ?, argmax returns the hig"
P14-1066,W06-3114,0,0.0110009,"es of source given target phrase mappings ???? (?|?) and vice versa ???? (?|?), as well as lexical weighting estimates ??? (?|?) and ??? (?|?), word and phrase penalties, a linear distortion feature, and a lexicalized reordering feature. The baseline includes a standard 5-gram modified Kneser-Ney language model trained on the target side of the parallel corpora described below. Log-linear weights are estimated with the MERT algorithm (Och 2003). 704 Evaluation. We test our models on two different data sets. First, we train an English to French system based on the data of WMT 2006 shared task (Koehn and Monz 2006). The parallel corpus includes 688K sentence pairs of parliamentary proceedings for training. The development set contains 2000 sentences, and the test set contains other 2000 sentences, all from the official WMT 2006 shared task. Second, we experiment with a French to English system developed using 2.1M sentence pairs of training data, which amounts to 102M words, from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sentences, as the development set. We evaluate"
P14-1066,D13-1054,0,0.0498113,"00 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier"
P14-1066,W02-1018,0,0.0501549,"phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires le"
P14-1066,N13-1090,1,0.0323834,"e translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the st"
P14-1066,D09-1092,0,0.0576492,"Missing"
P14-1066,W11-2124,0,0.0188708,"similarity between a source phrase and its paired target phrase by projecting them into a common, continuous space that is language independent. The rest of the paper is organized as follows. Section 2 reviews previous work. Section 3 reviews the log-linear model for phrase-based SMT and Sections 4 presents the CPTM. Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper. 1 version of such a model can be trained efficiently because the factor models used by Son et al. cannot be applied directly. Niehues et al. (2011) use different translation units in order to integrate the n-gram translation model into the phrasebased approach. However, it is not clear how a continuous 2 Related Work Representations of words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have"
P14-1066,P03-1021,0,0.658932,"CPTM is incorporated, is parameterized by (?, ?), where ? is a vector of a handful of parameters used in the log-linear model of (1), with one weight for each feature; and ? is the projection matrices used in the CPTM defined by (2) and (3). In our experiments we take three steps to learn (?, ?): 1. We use a baseline phrase-based SMT system to generate for each source sentence in training data an N-best list of translation hypotheses4. 2. We set ? to that of the baseline system and let ??+1 = 1, and optimize ? w.r.t. a loss function on training data5. 3. We fix ? , and optimize ? using MERT (Och 2003) to maximize BLEU on dev data. The translation score of a source phrase f and a target phrase e can be measured as the similarity (or distance) between their feature vectors. We choose the dot product as the similarity function3: score(?, ?) ≡ sim? (?? , ?? ) = ??T ?? (3) According to (2), we see that the value of the scoring function is determined by the projection matrices ? = {?1 , ?2 }. The CPTM of (2) and (3) can be incorporated into the log-linear model for SMT (1) by 3 In our experiments, we compare dot product and the cosine similarity functions and find that the former works better fo"
P14-1066,J04-4002,0,0.135511,"r vector ? ∈ ℝ? , which assigns a real-valued weight to each feature. The components GEN(. ), ? and ? define a loglinear model that maps ?? to an output sentence as follows: ? ∗ = argmax ?T ?(?? , ?, ?) (1) (?,?)∈GEN(?? ) which states that given ? and ?, argmax returns the highest scoring translation ? ∗ , maximizing over correspondences ?. In phrase-based SMT, ? consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since computing the argmax exactly is intractable, it is commonly performed approximatedly by beam search (Och and Ney 2004). Following Liang et al. (2006), we assume that every translation candidate is always coupled with a corresponding ?, called the Viterbi derivation, generated by (1). 4 A Continuous-Space Phrase Translation Model (CPTM) The architecture of the CPTM is shown in Figures 1 and 2, where for each pair of source and target phrases (?? , ?? ) in a source-target sentence pair, we first project them into feature vectors ??? and ??? in a latent, continuous space via a neural network with one hidden layer (as shown in Figure 2), and then compute the translation score, score(?? , ?? ), by the distance of"
P14-1066,P02-1040,0,0.0988536,"from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sentences, as the development set. We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 system combination test set, containing 2034 to 3003 sentences. In this study we perform a detailed empirical comparison using the WMT 2006 data set, and verify our best models and results using the larger WMT 2012 data set. The metric used for evaluation is case insensitive BLEU score (Papineni et al. 2002). We also perform a significance test using the Wilcoxon signed rank test. Differences are considered statistically significant when the p-value is less than 0.05. 6.2 Results of the CPTM Table 1 shows the results measured in BLEU evaluated on the WMT 2006 data set, where Row 1 is the baseline system. Rows 2 to 4 are the systems enhanced by integrating different versions of the CPTM. Rows 5 to 7 present the results of previous models. Row 8 is our best system. Table 2 shows the main results on the WMT 2012 data set. CPTM is the model described in Sections 4. As illustrated in Figure 2, the num"
P14-1066,D10-1025,1,0.710259,"(Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural net"
P14-1066,P10-1049,0,0.0665678,"formation can be captured in selecting translations. However, longer phrases occur less often in trainThis paper tackles the sparsity problem in ing data, leading to a severe data sparseness probestimating phrase translation probabilities lem in parameter estimation. There has been a by learning continuous phrase representaplethora of research reported in the literature on tions, whose distributed nature enables the improving parameter estimation for the phrase sharing of related phrases in their representranslation model (e.g., DeNero et al. 2006; tations. A pair of source and target phrases Wuebker et al. 2010; He and Deng 2012; Gao and are projected into continuous-valued vecHe 2013). tor representations in a low-dimensional This paper revisits the problem of scoring a latent space, where their translation score phrase translation pair by developing a Continuis computed by the distance between the ous-space Phrase Translation Model (CPTM). pair in this new space. The projection is The translation score of a phrase pair in this model performed by a neural network whose is computed as follows. First, we represent each weights are learned on parallel training phrase as a bag-of-words vector, called w"
P14-1066,W11-0329,1,0.895525,"have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to repres"
P14-1066,W11-2119,0,0.0732387,"Missing"
P14-1066,D07-1045,0,0.0166222,"e projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source"
P14-1066,C12-2104,0,0.199051,"shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as w"
P14-1066,W12-2702,0,0.016591,"gle words, are smooth function of these feature vectors, a small Abstract 699 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 699–709, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics change in the features should only lead to a small change in the translation score. The primary research task in developing the CPTM is learning the continuous representation of a phrase that is effective for SMT. Motivated by recent studies on continuous-space language models (e.g., Bengio et al. 2003; Mikolov et al. 2011; Schwenk et al., 2012), we use a neural network to project a word vector to a feature vector. Ideally, the projection would discover those latent features that are useful to differentiate good translations from bad ones, for a given source phrase. However, there is no training data with explicit annotation on the quality of phrase translations. The phrase translation pairs are hidden in the parallel source-target sentence pairs, which are used to train the traditional translation models. The quality of a phrase translation can only be judged implicitly through the translation quality of the sentences, as measured b"
P14-1066,N13-1120,1,0.781116,"Missing"
P14-1066,D13-1141,0,0.0690135,"ing, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier"
P14-1066,N13-1034,0,0.0171881,"rity of the phrases in the project space. As we see from Table 1, both latent semantic models, although leading to some slight improvement over Baseline, are much less effective than CPTM. Finally, we compare the CPTM with the Markov Random Field model using phrase features (MRFP in Tables 1 and 2), proposed by Gao and He (2013)7, on both the WMT 2006 and WMT 2012 datasets. MRFp is a state-of-the-art large scale discriminative training model that uses the same expected BLEU training criterion, which has proven to give superior performance across a range of MT tasks recently (He and Deng 2012, Setiawan and Zhou 2013, Gao and He 2013). Unlike CPTM, MRFp is a linear model that simply treats each phrase pair as a single feature. Therefore, although both are trained using the 7 Gao and He (2013) reported results of MRF models with different feature sets. We picked the MRF using phrase features only (MRFP) for comparison since we are mainly interested in phrase representation. 706 same expected BLEU based objective function, CPTM and MRFp model the translation relationship between two phrases from different angles. MRFp estimates one translation score for each phrase pair explicitly without parameter sharing,"
P14-1066,D12-1110,0,0.0585792,"Missing"
P14-1066,N12-1005,0,0.34284,"vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its transla"
P14-1066,W06-3105,0,\N,Missing
P14-1066,D13-1106,0,\N,Missing
P14-2105,D13-1160,0,0.0869,"graphy-related questions, learned using inductive logic programming (Zelle and Mooney, 1996). Research in this line originally used small, domain-specific databases, such as GeoQuery (Tang and Mooney, 2001; Liang et 643 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 643–648, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics al., 2013). Very recently, researchers have started developing semantic parsers for large, generaldomain knowledge bases like Freebase and DBpedia (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). Despite significant progress, the problem remains challenging. Most methods have not yet been scaled to large KBs that can support general open-domain QA. In contrast, Fader et al. (2013) proposed the PARALEX system, which targets answering single-relation questions using an automatically created knowledge base, ReVerb (Fader et al., 2011). By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of patternmatching rules can be learned for this r"
P14-2105,P13-1042,0,0.0635643,"ser for answering geography-related questions, learned using inductive logic programming (Zelle and Mooney, 1996). Research in this line originally used small, domain-specific databases, such as GeoQuery (Tang and Mooney, 2001; Liang et 643 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 643–648, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics al., 2013). Very recently, researchers have started developing semantic parsers for large, generaldomain knowledge bases like Freebase and DBpedia (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). Despite significant progress, the problem remains challenging. Most methods have not yet been scaled to large KBs that can support general open-domain QA. In contrast, Fader et al. (2013) proposed the PARALEX system, which targets answering single-relation questions using an automatically created knowledge base, ReVerb (Fader et al., 2011). By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of patternmatching rules can"
P14-2105,D11-1142,0,0.0744022,"e 23-25 2014. 2014 Association for Computational Linguistics al., 2013). Very recently, researchers have started developing semantic parsers for large, generaldomain knowledge bases like Freebase and DBpedia (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). Despite significant progress, the problem remains challenging. Most methods have not yet been scaled to large KBs that can support general open-domain QA. In contrast, Fader et al. (2013) proposed the PARALEX system, which targets answering single-relation questions using an automatically created knowledge base, ReVerb (Fader et al., 2011). By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of patternmatching rules can be learned for this restricted form of semantic parsing. The other line of work related to our approach is continuous representations for semantic similarity, which has a long history and is still an active research topic. In information retrieval, TF-IDF vectors (Salton and McGill, 1983), latent semantic analysis (Deerwester et al., 1990) and topic models (Blei et al., 2003) take the bag"
P14-2105,P13-1158,0,0.789814,"rks. Leveraging the question paraphrase data mined from the WikiAnswers corpus by Fader et al. (2013), we train two semantic similarity models: one links a mention from the question to an entity in the KB and the other maps a relation pattern to a relation. The answer to the question can thus be derived by finding the relation–entity triple r(e1 , e2 ) in the KB and returning the entity not mentioned in the question. By using a general semantic similarity model to match patterns and relations, as well as mentions and entities, our system outperforms the existing rule learning system, PARALEX (Fader et al., 2013), with higher precision at all the recall points when answering the questions in the same test set. The highest achievable F1 score of our system is 0.61, versus 0.54 of PARALEX. The rest of the paper is structured as follows. We first survey related work in Sec. 2, followed by the problem definition and the high-level description of our approach in Sec. 3. Sec. 4 details our semantic models and Sec. 5 shows the experimental results. Finally, Sec. 6 concludes the paper. We develop a semantic parsing framework based on semantic similarity for open domain question answering (QA). We focus on sin"
P14-2105,D13-1161,0,0.0181989,"ons, learned using inductive logic programming (Zelle and Mooney, 1996). Research in this line originally used small, domain-specific databases, such as GeoQuery (Tang and Mooney, 2001; Liang et 643 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 643–648, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics al., 2013). Very recently, researchers have started developing semantic parsers for large, generaldomain knowledge bases like Freebase and DBpedia (Cai and Yates, 2013; Berant et al., 2013; Kwiatkowski et al., 2013). Despite significant progress, the problem remains challenging. Most methods have not yet been scaled to large KBs that can support general open-domain QA. In contrast, Fader et al. (2013) proposed the PARALEX system, which targets answering single-relation questions using an automatically created knowledge base, ReVerb (Fader et al., 2011). By applying simple seed templates to the KB and by leveraging community-authored paraphrases of questions from WikiAnswers, they successfully demonstrated a high-quality lexicon of patternmatching rules can be learned for this restricted form of semantic"
P14-2105,J13-2005,0,0.0208691,"Missing"
P14-2105,P11-1060,0,\N,Missing
P15-1128,P14-1091,0,0.396713,"gely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The representation power of the semantic parse is thus controlled by t"
P15-1128,D14-1002,1,0.473482,"etworks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolution matrix: Wc Word hashing layer: ft x y x Figure 7: Extending an inferential chain with constraints and aggregation functions. word boundary symbol #. Then, it uses a convolutional layer to project the letter-trigram vectors of words within a con"
P15-1128,P14-1133,0,0.16292,"wever, most traditional approaches for semantic parsing are largely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The represent"
P15-1128,D13-1160,0,0.888293,"et al., 2014), but with some key differences. The nodes and edges in our query graph closely resemble the exact entities and predicates from the knowledge base. As a result, the graph can be straightforwardly translated to a logical form query that is directly executable. In contrast, the query graph in (Reddy et al., 2014) is mapped from the CCG parse of the question, and needs further transformations before mapping to subgraphs 2 y should be grounded to a CVT entity in this case. of the target knowledge base to retrieve answers. Semantically, our query graph is more related to simple λ-DCS (Berant et al., 2013; Liang, 2013), which is a syntactic simplification of λ-calculus when applied to graph databases. A query graph can be viewed as the tree-like graph pattern of a logical form in λ-DCS. For instance, the path from the answer node to an entity node can be described using a series of join operations in λ-DCS. Different paths of the tree graph are combined via the intersection operators. 3 Staged Query Graph Generation We focus on generating query graphs with the following properties. First, the tree graph consists of one entity node as the root, referred as the topic entity. Second, there exists"
P15-1128,D13-1161,0,0.0148159,"eved simply by executing the query. The semantic parse also provides a deeper understanding of the question, which can be used to justify the answer to users, as well as to provide easily interpretable information to developers for error analysis. However, most traditional approaches for semantic parsing are largely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Sem"
P15-1128,J13-2005,0,0.417175,"ski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The representation power of the semantic parse is thus controlled by the set of legitimate actions applicable to each state. In particular, we stage the actions into three main steps: locating the topic entity in the question, finding the main relationship between the answer and the topic entity, and expanding the query graph with additional constraints that"
P15-1128,D14-1067,0,0.894208,"ce, one of our constructions maps the question to a pattern by replacing the entity mention with a generic symbol <e&gt; and then compares it with a candidate chain, such as “who first voiced meg on <e&gt;” vs. cast-actor. The model consists of two neural networks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolu"
P15-1128,P13-1042,0,0.321803,"Missing"
P15-1128,Q14-1030,0,0.0642215,"to map entities retrieved by the query. The diamond node arg min constrains that the answer needs to be the earliest actor for this role. Equivalently, the logical form query in λ-calculus without the aggregation function is: λx.∃y.cast(FamilyGuy, y) ∧ actor(y, x) ∧ character(y, MegGriffin) Running this query graph against K as in Fig. 1 will match both LaceyChabert and MilaKunis before applying the aggregation function, but only LaceyChabert is the correct answer as she started this role earlier (by checking the from property of the grounded CVT node). Our query graph design is inspired by (Reddy et al., 2014), but with some key differences. The nodes and edges in our query graph closely resemble the exact entities and predicates from the knowledge base. As a result, the graph can be straightforwardly translated to a logical form query that is directly executable. In contrast, the query graph in (Reddy et al., 2014) is mapped from the CCG parse of the question, and needs further transformations before mapping to subgraphs 2 y should be grounded to a CVT entity in this case. of the target knowledge base to retrieve answers. Semantically, our query graph is more related to simple λ-DCS (Berant et al."
P15-1128,P15-1049,1,0.237944,"iority queue, which is formally defined in Appendix A. In the following subsections, we use a running example of finding the semantic parse of question qex = “Who first voiced Meg of Family Guy?” to describe the sequence of actions. 3.1 Family Guy Linking Topic Entity Starting from the initial state s0 , the valid actions are to create a single-node graph that corresponds to the topic entity found in the given question. For instance, possible topic entities in qex can either be FamilyGuy or MegGriffin, shown in Fig. 4. We use an entity linking system that is designed for short and noisy text (Yang and Chang, 2015). For each entity e in the knowledge base, the system first prepares a surface-form lexicon that lists all possible ways that e can be mentioned in text. This lexicon is created using various data sources, such as names and aliases of the entities, the anchor text in Web documents and the Wikipedia redirect table. Given a question, it considers all the y Family Guy cast Family Guy writer y Family Guy genre x Family Guy actor start x x Figure 5: Candidate core inferential chains start from the entity FamilyGuy. consecutive word sequences that have occurred in the lexicon as possible mentions, p"
P15-1128,D14-1071,0,0.379928,"t nodes indicating that certain conditions cannot be satisfied. Our graph generation method is inspired by (Yao and Van Durme, 2014; Bao et al., 2014). Unlike traditional semantic parsing approaches, it uses the knowledge base to help prune the search space when forming the parse. Similar ideas have also been explored in (Poon, 2013). Empirically, our results suggest that it is crucial to identify the core inferential chain, which matches the relationship between the topic entity in the question and the answer. Our CNN models can be analogous to the embedding approaches (Bordes et al., 2014a; Yang et al., 2014), but are more sophisticated. By allowing parameter sharing among different question-pattern and KB predicate pairs, the matching score of a rare or even unseen pair in the training data can still be predicted precisely. This is due to the fact that the prediction is based on the shared model parameters (i.e., projection matrices) that are estimated using all training pairs. 6 Conclusion In this paper, we present a semantic parsing framework for question answering using a knowledge base. We define a query graph as the meaning representation that can be directly mapped to a logical form. Semant"
P15-1128,P14-1090,0,0.722517,"Missing"
P15-1128,P14-2105,1,0.725471,"ctions maps the question to a pattern by replacing the entity mention with a generic symbol <e&gt; and then compares it with a candidate chain, such as “who first voiced meg on <e&gt;” vs. cast-actor. The model consists of two neural networks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolution matrix: Wc Word"
P15-1128,P13-1092,0,\N,Missing
P16-1136,D13-1160,0,0.0328171,"ons. We conduct a theoretical analysis of the efficiency gain from the approach. Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion. 1 Wen-tau Yih Microsoft Research Redmond, WA, USA Introduction Intelligent applications benefit from structured knowledge about the entities and relations in their domains. For example, large-scale knowledge bases (KB), such as Freebase (Bollacker et al., 2008) or DBPedia (Auer et al., 2007), have proven to be important resources for supporting open-domain question answering (Berant et al., 2013; Sun et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 200"
P16-1136,D14-1165,1,0.764736,"d at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al., 2015) or inferring facts from the relationships among known entities (Lao and Cohen, 2010) are thus important approaches for populating existing knowledge bases. Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014). In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors and matrices. Whether two entities have a previously unknown relationship can be predicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In c"
P16-1136,D15-1034,0,0.442388,"known relationship can be predicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB embeddings has been proposed very recently (Guu et al., 2015; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015). While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened w"
P16-1136,D13-1080,0,0.0302143,"oses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propo"
P16-1136,D14-1044,0,0.0461743,"number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic programming method that"
P16-1136,D15-1038,0,0.26163,"Missing"
P16-1136,D12-1093,0,0.0097446,"ormance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact"
P16-1136,D15-1082,0,0.816198,"redicted by simple functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB embeddings has been proposed very recently (Guu et al., 2015; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015). While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is a"
P16-1136,P09-1113,0,0.0591613,"rant et al., 2013; Sun et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al., 2015) or inferring facts from the relationships among known entities (Lao and Cohen, 2010) are thus important approaches for populating existing knowledge bases. Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014). In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors"
P16-1136,P15-1016,0,0.505284,"functions of their corresponding vectors or matrices. Early work in this direction focuses on exploring various kinds of learning objectives and frameworks, but the model is learned solely from known direct relationships between two entities (e.g., father(barack, sasha)) (Nickel et al., 2011; Socher et al., 2013; Bordes et al., 2013; Chang et al., 2014; Yang et al., 2015). In contrast, using multi-step relation paths (e.g., husband(barack, michelle) ∧ mother(michelle, sasha) to train KB embeddings has been proposed very recently (Guu et al., 2015; Garcia-Duran et al., 2015; Lin et al., 2015; Neelakantan et al., 2015). While using relation paths improves model performance, it also poses a critical technical challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled tex"
P16-1136,N13-1008,0,0.861813,"cal challenge. As the number of possible relation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic pr"
P16-1136,D12-1042,0,0.0364585,"un et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al., 2015) or inferring facts from the relationships among known entities (Lao and Cohen, 2010) are thus important approaches for populating existing knowledge bases. Originally proposed as an alternative statistical relational learning method, the knowledge base embedding approach has gained a significant amount of attention, due to its simple prediction time computation and strong empirical performance (Nickel et al., 2011; Chang et al., 2014). In this framework, entities and relations in a knowledge base are represented in a continuous space, such as vectors and matrices. Whether"
P16-1136,W15-4007,1,0.757471,"lation paths between pairs of entities grows exponentially with path length, the training complexity increases sharply. Consequently, existing methods need 1434 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1434–1444, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to make approximations by sampling or pruning. The problem is worsened when the input is augmented with unlabeled text, which has been shown to improve performance (Lao et al., 2012; Gardner et al., 2013; Riedel et al., 2013; Gardner et al., 2014; Toutanova and Chen, 2015). Moreover, none of the prior methods distinguish relation paths that differ in the intermediate nodes they pass through (e.g., michelle in our example); all represent paths as a sequence of relation types. In this work, we aim to develop a KB completion model that can incorporate relation paths efficiently. We start from analyzing the procedures in existing approaches, focusing on their time and space complexity. Based on the observation that compositional representations of relation paths are in fact decomposable, we propose a novel dynamic programming method that enables efficient modeling"
P16-1136,D15-1174,1,0.720524,"uded textual relations that occur at least 5 times between mentions of two genes that have a KB relation. This resulted in 3,827 distinct textual relations and 1,244,186 mentions.6 The number of textual relations is much larger than that of KB relations, and it helped induce much larger connectivity among genes (390,338 pairs of genes are directly connected in text versus 12,100 pairs in KB). Systems A LL - PATHS denotes our compositional learning approach that sums over all paths using 6 Modeling such a large number of textual relations introduces sparsity, which necessitates models such as (Toutanova et al., 2015; Verga et al., 2015) to derive composed representations of text. We leave integration with such methods for future work. 1440 dynamic programming; A LL - PATHS + NODES additionally models nodes in the paths. P RUNED PATHS denotes the traditional approach that learns from sampled paths detailed in §3.1.2; paths with occurrence less than a cutoff are pruned (c = 1 in Table 1 means that all sampled paths are used). The most relevant prior approach is Guu et al. (2015). We ran experiments using both their publicly available code and our re-implementation. We also included the B ILINEAR - DIAG bas"
P16-1136,P15-1128,1,0.122455,"of the efficiency gain from the approach. Experiments on two datasets show that it addresses representational limitations in prior approaches and improves accuracy in KB completion. 1 Wen-tau Yih Microsoft Research Redmond, WA, USA Introduction Intelligent applications benefit from structured knowledge about the entities and relations in their domains. For example, large-scale knowledge bases (KB), such as Freebase (Bollacker et al., 2008) or DBPedia (Auer et al., 2007), have proven to be important resources for supporting open-domain question answering (Berant et al., 2013; Sun et al., 2015; Yih et al., 2015). In biomedicine, KBs such as the Pathway Interaction Database (NCI-PID) (Schaefer et al., 2009) are crucial for understanding complex diseases such as cancer and for advancing precision medicine. ∗ This research was conducted during the author’s internship at Microsoft Research. While these knowledge bases are often carefully curated, they are far from complete. In non-static domains, new facts become true or are discovered at a fast pace, making the manual expansion of knowledge bases impractical. Extracting relations from a text corpus (Mintz et al., 2009; Surdeanu et al., 2012; Poon et al."
P16-1136,N16-1103,0,\N,Missing
P16-2033,P14-1133,0,0.0172645,"cted entity will be shown, along with the objects (either properties or entities). Suppose the labeler chooses Meg Griffin as the topic entity. He should then pick actor as the main relationship, meaning the answer should be the persons who have played this role. To accurately describe the question, the labeler should add additional filters like the TV series is Family Guy and the performance type is voice in the final stage2 . The design of our UI is inspired by recent work on semantic parsing that has been applied to the W EB Q UESTIONS dataset (Bast and Haussmann, 2015; Reddy et al., 2014; Berant and Liang, 2014; Yih et al., 2015), as these approaches use a simpler and yet more restricted semantic representation than first-order logic expressions. Following the notion of query graph in (Yih et al., 2015), the semantic parse is anchored to one of the entities in the question as the topic entity and the core component is to represent the relation between the entity and the answer, referred as the inferential chain. Constraints, such as properties of the answer or additional conditions the relation needs to hold, are captured as well. Figure 1 shows an example of these annotated semantic parse component"
P16-2033,D13-1160,0,0.713298,"o an absolute 5-point increase in average F1 . Our work demonstrates that semantic parse labels can provide additional value over answer labels while, with the right labeling tools, being comparable in cost to collect. Besides accuracy gains, semantic parses also have further benefits in yielding answers that are more accurate and consistent, as well as being updatable if the knowledge base changes (for example, as facts are added or revised). 2 Collecting Semantic Parses In order to verify the benefits of having labeled semantic parses, we completely re-annotated the W EB Q UESTIONS dataset (Berant et al., 2013) such that it contains both semantic parses and the derived answers. We chose to annotate the questions with the full semantic parses in SPARQL, based on the schema and data of the latest and last version of Freebase (2015-08-09). Labeling interface Writing SPARQL queries for natural language questions using a text editor is obviously not an efficient way to provide semantic parses even for experts. Therefore, we designed a staged, dialog-like user interface (UI) to improve the labeling efficiency. Our UI breaks the potentially complicated structured-labeling task into separate, but inter-depe"
P16-2033,P13-1042,0,0.0413412,"Missing"
P16-2033,W10-2903,1,0.309285,"ins, 2005; Wong and Mooney, 2007). Recent work on semantic parsing for knowledge base questionanswering (KBQA) has called into question the value of collecting such semantic parse labels, with most recent KBQA semantic parsing systems being trained using only question-answer pairs instead of question-parse pairs. In fact, there is evidence that using only question-answer pairs can yield improved performance as compared with approaches based on semantic parse labels (Liang et al., 2013). It is also widely believed that collecting semantic parse labels can be a “difficult, time consuming task” (Clarke et al., 2010) even for domain experts. Furthermore, recent focus has been more on the final task-specific performance of a 1 Available at http://aka.ms/WebQSP. 201 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 201–206, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics further constrain the answers. One key advantage of our UI design is that the annotator only needs to focus on one particular sub-task during each stage. All of the choices made by the labeler are used to automatically construct a coherent semantic parse. Note"
P16-2033,D15-1076,0,0.0113682,"when we limit to semantically valid paths. However, in domains where the number of potential paths between candidate entities and answers is small, the value of collecting semantic parse labels might also be small. complements existing datasets in these individual tasks, as they tend to target at normal corpora of regular sentences. While our labeling interface design was aimed at supporting labeling experts, it would be valuable to enable crowdsourcing workers to provide semantic parse labels. One promising approach is to use a more dialog-driven interface using natural language (similar to (He et al., 2015)). Such UI design is also crucial for extending our work to handling more complicated questions. For instance, allowing users to traverse longer paths in a sequential manner will increase the expressiveness of the output parses, both in the core relation and constraints. Displaying a small knowledge graph centered at the selected entities and relations may help users explore alternative relations more effectively as well. Semantic parsing labels provide additional benefits. For example, collecting semantic parse labels relative to a knowledge base can ensure that the answers are more faithful"
P16-2033,D13-1161,0,0.260095,"Missing"
P16-2033,J13-2005,0,0.0486608,"ntic parsers made use of datasets of questions and their associated semantic parses (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Recent work on semantic parsing for knowledge base questionanswering (KBQA) has called into question the value of collecting such semantic parse labels, with most recent KBQA semantic parsing systems being trained using only question-answer pairs instead of question-parse pairs. In fact, there is evidence that using only question-answer pairs can yield improved performance as compared with approaches based on semantic parse labels (Liang et al., 2013). It is also widely believed that collecting semantic parse labels can be a “difficult, time consuming task” (Clarke et al., 2010) even for domain experts. Furthermore, recent focus has been more on the final task-specific performance of a 1 Available at http://aka.ms/WebQSP. 201 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 201–206, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics further constrain the answers. One key advantage of our UI design is that the annotator only needs to focus on one particular sub-"
P16-2033,Q14-1030,0,0.0423834,"edicates of the selected entity will be shown, along with the objects (either properties or entities). Suppose the labeler chooses Meg Griffin as the topic entity. He should then pick actor as the main relationship, meaning the answer should be the persons who have played this role. To accurately describe the question, the labeler should add additional filters like the TV series is Family Guy and the performance type is voice in the final stage2 . The design of our UI is inspired by recent work on semantic parsing that has been applied to the W EB Q UESTIONS dataset (Bast and Haussmann, 2015; Reddy et al., 2014; Berant and Liang, 2014; Yih et al., 2015), as these approaches use a simpler and yet more restricted semantic representation than first-order logic expressions. Following the notion of query graph in (Yih et al., 2015), the semantic parse is anchored to one of the entities in the question as the topic entity and the core component is to represent the relation between the entity and the answer, referred as the inferential chain. Constraints, such as properties of the answer or additional conditions the relation needs to hold, are captured as well. Figure 1 shows an example of these annotated"
P16-2033,P07-1121,0,0.012919,"compared to learning from answers, (2) we show that with an appropriate user interface, one can obtain semantic parses with high accuracy and at a cost comparable or lower than obtaining just answers, and (3) we have created and shared the largest semantic-parse labeled dataset to date in order to advance research in question answering. 1 Introduction Semantic parsing is the mapping of text to a meaning representation. Early work on learning to build semantic parsers made use of datasets of questions and their associated semantic parses (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007). Recent work on semantic parsing for knowledge base questionanswering (KBQA) has called into question the value of collecting such semantic parse labels, with most recent KBQA semantic parsing systems being trained using only question-answer pairs instead of question-parse pairs. In fact, there is evidence that using only question-answer pairs can yield improved performance as compared with approaches based on semantic parse labels (Liang et al., 2013). It is also widely believed that collecting semantic parse labels can be a “difficult, time consuming task” (Clarke et al., 2010) even for dom"
P16-2033,P15-1049,1,0.352876,"ic parses in SPARQL, based on the schema and data of the latest and last version of Freebase (2015-08-09). Labeling interface Writing SPARQL queries for natural language questions using a text editor is obviously not an efficient way to provide semantic parses even for experts. Therefore, we designed a staged, dialog-like user interface (UI) to improve the labeling efficiency. Our UI breaks the potentially complicated structured-labeling task into separate, but inter-dependent sub-tasks. Given a question, the UI first presents entities detected in the questions using an entity linking system (Yang and Chang, 2015), and asks the user to pick an entity in the question as the topic entity that could lead to the answers. The user can also suggest a new entity if none of the candidates returned by the entity linking system is correct. Once the entity is selected, the system then requests the user to pick the Freebase predicate that represents the relationship between the answers and this topic entity. Finally, additional filters can be added to Labeling process In order to ensure the data quality, we recruit five annotators who are familiar with design of Freebase. Our goal is to provide 2 Screenshots are i"
P16-2033,P15-1128,1,0.82746,"n, along with the objects (either properties or entities). Suppose the labeler chooses Meg Griffin as the topic entity. He should then pick actor as the main relationship, meaning the answer should be the persons who have played this role. To accurately describe the question, the labeler should add additional filters like the TV series is Family Guy and the performance type is voice in the final stage2 . The design of our UI is inspired by recent work on semantic parsing that has been applied to the W EB Q UESTIONS dataset (Bast and Haussmann, 2015; Reddy et al., 2014; Berant and Liang, 2014; Yih et al., 2015), as these approaches use a simpler and yet more restricted semantic representation than first-order logic expressions. Following the notion of query graph in (Yih et al., 2015), the semantic parse is anchored to one of the entities in the question as the topic entity and the core component is to represent the relation between the entity and the answer, referred as the inferential chain. Constraints, such as properties of the answer or additional conditions the relation needs to hold, are captured as well. Figure 1 shows an example of these annotated semantic parse components and the correspon"
P16-2033,P11-1060,0,\N,Missing
P17-1167,N16-1181,0,0.0369064,"ring function. In our design, the score of a state is determined by the scores of actions taken from the initial state to the target state, which are predicted by different neural network modules based on action type. By leveraging a margin-based objective function, the model learning procedure resembles several structured-output learning algorithms such as structured SVMs (Tsochantaridis et al., 2005), but can take either strong or weak supervision seamlessly. DynSP is inspired by STAGG, a search-based semantic parser (Yih et al., 2015), as well as the dynamic neural module network (DNMN) of Andreas et al. (2016). Much like STAGG, DynSP chains together different modules as search progresses; however, these modules are implemented as neural networks, which enables end-to-end training as in DNMN. The key difference between DynSP and DNMN is that in DynSP the network structure of an example is not predetermined. Instead, different network structures are constructed dynamically as our learning procedure explores the state space. It is straightforward to answer sequential questions using our framework: we allow the model to take the previous question and its answers as input, with a slightly modified actio"
P17-1167,D11-1039,0,0.0126661,"y implemented in our language due to concerns with the search space size. Increasing the number of complex actions requires designing smarter optimization procedures, which we leave to future work. 5 Related Work Previous work on conversational QA has focused on small, single-domain datasets. Perhaps most related to our task is the context-dependent sentence analysis described in (Zettlemoyer and Collins, 2009), where conversations between customers and travel agents are mapped to logical forms after resolving referential expressions. Another dataset of travel booking conversations is used by Artzi and Zettlemoyer (2011) to learn a semantic parser for complicated queries given user clarifications. More recently, Long et al. (2016) collect three contextual semantic parsing datasets (from synthetic domains) that contain coreferences to entities and MAX SUBSEQUENT 1 SUBSEQUENT 2 Figure 3: Parses computed by DynSP for three test sequences (actions in blue boxes, values from table in white boxes). Top: all three questions are parsed correctly. Middle: semantic matching errors cause the model to select incorrect columns and conditions. Bottom: The final question is unanswerable due to limitations of our parse langu"
P17-1167,D14-1179,0,0.00414407,"Missing"
P17-1167,P05-1026,0,0.0994602,"rse language. actions. We differentiate ourselves from these prior works in two significant ways: first, our dataset is not restricted to a particular domain, and second, a major goal of our work is to analyze the different types of sequence progressions people create when they are trying to express a complicated intent. Complex, interactive QA tasks have also been proposed in the information retrieval community, where the data source is a corpus of newswire text (Kelly and Lin, 2007). We also build on aspects of some existing interactive question-answering systems. For example, the system of Harabagiu et al. (2005) includes a module that predicts what a user will ask next given their current question. Other than FP and NP, the work of Neural Symbolic Machines (NSM) (Liang et al., 2017) is perhaps the closest to ours. NSM aims to generate formal semantic parses of questions that can be executed on Freebase to retrieve answers, and is trained using the REINFORCE algorithm (Williams, 1992) augmented with approximate gold parses found in a separate curriculum learning stage. In comparison, finding reference parses is an integral part of our algorithm. Our non1828 probabilistic, margin-based objective functi"
P17-1167,P17-1003,0,0.117723,"goal of our work is to analyze the different types of sequence progressions people create when they are trying to express a complicated intent. Complex, interactive QA tasks have also been proposed in the information retrieval community, where the data source is a corpus of newswire text (Kelly and Lin, 2007). We also build on aspects of some existing interactive question-answering systems. For example, the system of Harabagiu et al. (2005) includes a module that predicts what a user will ask next given their current question. Other than FP and NP, the work of Neural Symbolic Machines (NSM) (Liang et al., 2017) is perhaps the closest to ours. NSM aims to generate formal semantic parses of questions that can be executed on Freebase to retrieve answers, and is trained using the REINFORCE algorithm (Williams, 1992) augmented with approximate gold parses found in a separate curriculum learning stage. In comparison, finding reference parses is an integral part of our algorithm. Our non1828 probabilistic, margin-based objective function also helps avoid the need for empirical tricks to handle normalization and proper sampling, which are crucial when applying REINFORCE in practice. 6 Conclusion & Future Wo"
P17-1167,P16-1138,0,0.279417,"s designing smarter optimization procedures, which we leave to future work. 5 Related Work Previous work on conversational QA has focused on small, single-domain datasets. Perhaps most related to our task is the context-dependent sentence analysis described in (Zettlemoyer and Collins, 2009), where conversations between customers and travel agents are mapped to logical forms after resolving referential expressions. Another dataset of travel booking conversations is used by Artzi and Zettlemoyer (2011) to learn a semantic parser for complicated queries given user clarifications. More recently, Long et al. (2016) collect three contextual semantic parsing datasets (from synthetic domains) that contain coreferences to entities and MAX SUBSEQUENT 1 SUBSEQUENT 2 Figure 3: Parses computed by DynSP for three test sequences (actions in blue boxes, values from table in white boxes). Top: all three questions are parsed correctly. Middle: semantic matching errors cause the model to select incorrect columns and conditions. Bottom: The final question is unanswerable due to limitations of our parse language. actions. We differentiate ourselves from these prior works in two significant ways: first, our dataset is n"
P17-1167,P09-1110,0,0.0506913,"e surface, the final question in the bottom sequence of Figure 3 is one such example; the correct semantic parse requires access to the answers of both the first and second question, actions that we have not currently implemented in our language due to concerns with the search space size. Increasing the number of complex actions requires designing smarter optimization procedures, which we leave to future work. 5 Related Work Previous work on conversational QA has focused on small, single-domain datasets. Perhaps most related to our task is the context-dependent sentence analysis described in (Zettlemoyer and Collins, 2009), where conversations between customers and travel agents are mapped to logical forms after resolving referential expressions. Another dataset of travel booking conversations is used by Artzi and Zettlemoyer (2011) to learn a semantic parser for complicated queries given user clarifications. More recently, Long et al. (2016) collect three contextual semantic parsing datasets (from synthetic domains) that contain coreferences to entities and MAX SUBSEQUENT 1 SUBSEQUENT 2 Figure 3: Parses computed by DynSP for three test sequences (actions in blue boxes, values from table in white boxes). Top: a"
P17-1167,D17-1252,1,0.0819152,"time to converge. In this work, we propose a conceptually simple learning algorithm for weakly supervised training that sidesteps the inefficient learning problem. Our key insight is to conduct inference using a beam search procedure guided by an approximate reward function. The search procedure is executed twice for each training example, one for finding the best possible reference semantic parse and the other for finding the predicted semantic parse to update the model. Our framework is suitable for learning from either implicit or explicit supervision, and is detailed in a companion paper (Peng et al., 2017). Below we describe how we adapt it to the semantic parsing problem in this work. Approximate reward Let A(s) be the answers retrieved by executing the semantic parse represented by state s, and let A∗ be the set of gold answers of a given question. We define the reward R(s; A∗ ) = 1[A(s) = A∗ ], or the accuracy of the retrieved answers. We use R(s) as the abbreviation for R(s; A∗ ). A state s with R(s) = 1 is called a goal state. Directly using this reward function in search of goal states can be difficult, as rewards of most states are 0. However, even when the answers from a semantic parse"
P17-1167,D14-1162,0,0.11017,"nal constraints. 4.1 DynSP implementation details Unlike previous dynamic neural network frameworks (Andreas et al., 2016; Looks et al., 2017), where each example can have different but predetermined structure, DynSP needs to dynamically explores and constructs different neural network structures for each question. Therefore, we choose DyNet (Neubig et al., 2017) as our implementation platform for its flexibility in composing computation graphs. We optimize our model parameters using standard stochastic gradient descent. The word embeddings are initialized with 100-d pretrained GloVe vectors (Pennington et al., 2014) and fine-tuned during training with dropout rate 0.5. For follow-up questions, we choose uniformly at random to use either gold answers to the previous question or the model’s previous predictions.8 We constrain the maximum length of actions to 3 for computational efficiency and set the beam size to 15 in our reported models, as accuracy gains are negligible with larger beam sizes. We train our model for 30 epochs, although the best model on the validation set is usually found within the first 20 epochs. Only CPU is used in model training, and each epoch in the beam size 15 setting takes abou"
P17-1167,P15-1128,1,0.52009,"se. The quality of the induced semantic parse obviously depends on the scoring function. In our design, the score of a state is determined by the scores of actions taken from the initial state to the target state, which are predicted by different neural network modules based on action type. By leveraging a margin-based objective function, the model learning procedure resembles several structured-output learning algorithms such as structured SVMs (Tsochantaridis et al., 2005), but can take either strong or weak supervision seamlessly. DynSP is inspired by STAGG, a search-based semantic parser (Yih et al., 2015), as well as the dynamic neural module network (DNMN) of Andreas et al. (2016). Much like STAGG, DynSP chains together different modules as search progresses; however, these modules are implemented as neural networks, which enables end-to-end training as in DNMN. The key difference between DynSP and DNMN is that in DynSP the network structure of an example is not predetermined. Instead, different network structures are constructed dynamically as our learning procedure explores the state space. It is straightforward to answer sequential questions using our framework: we allow the model to take"
P17-1167,P16-2033,1,0.0431227,"cores of actions in the sequence, the goal of model optimization is to learn the parameters in the neural networks behind the policy function. Let θ be the collection of all the model parameters. Then thePstate value function can be written as: Vθ (st ) = ti=1 πθ (si−1 , ai ). In a fully supervised setting where the correct semantic parse of each question is available, learning the policy function can be reduced to a sequence prediction problem. However, while having full supervision leads to a better semantic parser, collecting the correct parses requires a much more sophisticated UI design (Yih et al., 2016). In many scenarios, such as the one in the SQA dataset, it is often the case that only the answers to the questions are available. Adapting a learning algorithm to this weakly supervised setting is thus critical. Generally speaking, weakly supervised semantic parsers operate on one assumption — a candidate semantic parse is treated as a correct one if it results in answers that are identical to the gold answers. Therefore, a straightforward modification of existing structured learning algorithms in our setting is to use any semantic parse found to evaluate to the correct answers during beam s"
P17-1167,P15-1142,0,\N,Missing
Q13-1017,W02-1001,0,0.938558,"ed on four benchmark NLP datasets for part-ofspeech tagging, named-entity recognition and dependency parsing. 1 Introduction Complex natural language processing tasks are inherently structured. From sequence labeling problems like part-of-speech tagging and named entity recognition to tree construction tasks like syntactic parsing, strong dependencies exist among the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the struct"
Q13-1017,D07-1015,0,0.0488286,"Missing"
Q13-1017,P09-1058,0,0.0222853,"echnique has been shown to improve the generalization ability of the model. MIRA The Margin Infused Relaxed Algorithm (MIRA), which was introduced by Crammer et al. (2005), explicitly uses the notion of margin to update the weight vector. The MIRA updates the weight vector by calculating the step size using 1 min kw − w0 k2 w 2 T S . T. w Φyi ,y (xi ) ≥ ∆(y, yi ), ∀y ∈ Hk , where Hk is a set containing the best-k structures according to the weight vector w0 . MIRA is a very popular method in the NLP community and has been applied to NLP tasks like word segmentation and part-of-speech tagging (Kruengkrai et al., 2009), NER and chunking (Mejer and Crammer, 2010) and dependency parsing (McDonald et al., 2005). 2.2 Structural SVM Structural SVM (SSVM) is a maximum margin model for the structured output prediction setting. Training SSVM is equivalent to solving the following global optimization problem: l arg max wT Φ(xi , y). (1) y∈Y(xi ) 208 min w X kwk2 +C L(xi , yi , w), 2 i=1 (2) where l is the number of labeled examples and L1-Loss SSVM (Ratliff et al., 2007). Taskar et   al. (2004a) proposed a structured SMO algorithm.   L(xi , yi , w) = ` max ∆(yi , y) − wT Φyx i,y (xi ) Because the algorithm solve"
Q13-1017,J93-2004,0,0.0478868,"f entities. We followed the settings in (Ratinov and Roth, 2009) and consider three main entities categories: PER, LOC and ORG. We evaluated the results using phrase-level F1 . retic analysis significantly. Also note that Theorem 2 shows that if we put all possible structures in the working sets (i.e., F (α) = FS (α)), then the DCD algorithms can obtain -optimal solution in O(log( 1 )) iterations. 213 POS-WSJ The standard set for evaluating the performance of a part-of-speech tagger. The training, development and test sets consist of sections 0-18, 19-21 and 22-24 of the Penn Treebank data (Marcus et al., 1993), respectively. We evaluated the results by token-level accuracy. DP-WSJ We took sections 02-21 of Penn Treebank as the training set, section 00 as the development set and section 23 as the test set. We implement a simple version of hash kernel to speed up of training procedure for this task (Bohnet, 2010). We reported the unlabeled attachment accuracy for this task (McDonald et al., 2005). 80 86 79 84 78 Test Acc Test F1 Test F1 97 82 96 DCD-SSVM FW-Struct SVM-Struct 77 80 76 0 10 20 30 0 20 Training Time (seconds) 40 60 80 100 120 Training Time (seconds) 95 0 200 400 600 Training Time (secon"
Q13-1017,P05-1012,0,0.615635,"hmark NLP datasets for part-ofspeech tagging, named-entity recognition and dependency parsing. 1 Introduction Complex natural language processing tasks are inherently structured. From sequence labeling problems like part-of-speech tagging and named entity recognition to tree construction tasks like syntactic parsing, strong dependencies exist among the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins"
Q13-1017,D10-1095,0,0.344884,"alization ability of the model. MIRA The Margin Infused Relaxed Algorithm (MIRA), which was introduced by Crammer et al. (2005), explicitly uses the notion of margin to update the weight vector. The MIRA updates the weight vector by calculating the step size using 1 min kw − w0 k2 w 2 T S . T. w Φyi ,y (xi ) ≥ ∆(y, yi ), ∀y ∈ Hk , where Hk is a set containing the best-k structures according to the weight vector w0 . MIRA is a very popular method in the NLP community and has been applied to NLP tasks like word segmentation and part-of-speech tagging (Kruengkrai et al., 2009), NER and chunking (Mejer and Crammer, 2010) and dependency parsing (McDonald et al., 2005). 2.2 Structural SVM Structural SVM (SSVM) is a maximum margin model for the structured output prediction setting. Training SSVM is equivalent to solving the following global optimization problem: l arg max wT Φ(xi , y). (1) y∈Y(xi ) 208 min w X kwk2 +C L(xi , yi , w), 2 i=1 (2) where l is the number of labeled examples and L1-Loss SSVM (Ratliff et al., 2007). Taskar et   al. (2004a) proposed a structured SMO algorithm.   L(xi , yi , w) = ` max ∆(yi , y) − wT Φyx i,y (xi ) Because the algorithm solves the dual formulation y of the L1-Loss SSVM"
Q13-1017,W09-1119,0,0.420529,"2003 shared task (T. K. Sang and De Meulder, 2003). The data set labels sentences from the Reuters Corpus, Volume 1 (Lewis et al., 2004) with four different entity types: PER, LOC, ORG and MISC. We evaluated the results using phrase-level F1 . 4.1 Tasks and Data We evaluated our method and existing structured output learning approaches on named entity recognition (NER), part-of-speech tagging (POS) and dependency parsing (DP) on four benchmark datasets. NER-MUC7 MUC-7 data contains a subset of North American News Text Corpora annotated with many types of entities. We followed the settings in (Ratinov and Roth, 2009) and consider three main entities categories: PER, LOC and ORG. We evaluated the results using phrase-level F1 . retic analysis significantly. Also note that Theorem 2 shows that if we put all possible structures in the working sets (i.e., F (α) = FS (α)), then the DCD algorithms can obtain -optimal solution in O(log( 1 )) iterations. 213 POS-WSJ The standard set for evaluating the performance of a part-of-speech tagger. The training, development and test sets consist of sections 0-18, 19-21 and 22-24 of the Penn Treebank data (Marcus et al., 1993), respectively. We evaluated the results by"
Q13-1017,W03-0419,0,0.23167,"Missing"
Q13-1017,W04-3201,0,0.179695,"ong the labels of individual components. By modeling such relations in the output space, structured output prediction algorithms have been shown to outperform significantly simple binary or multi-class classifiers (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2005). Among the existing structured output prediction algorithms, the linear Structural Support Vector Machine (SSVM) algorithm (Tsochantaridis et al., 2004; Joachims et al., 2009) has shown outstanding performance in several NLP tasks, such as bilingual word alignment (Moore et al., 2007), constituency and dependency parsing (Taskar et al., 2004b; Koo et al., 2007), sentence compression (Cohn and Lapata, 2009) and document summarization (Li et al., 2009). Nevertheless, as a learning method for NLP, the SSVM algorithm has been less than popular algorithms such as the structured Perceptron (Collins, 2002). This may be due to the fact that current SSVM implementations often suffer from several practical issues. First, state-of-the-art implementations of SSVM such as cutting plane methods (Joachims et al., 2009) are typically complicated.1 Second, while methods like stochastic gradient descent are simple to implement, tuning learning rat"
Q13-1017,P10-1040,0,0.0111844,"= 1]  X    [yi = 1][yi−1 = 2]  , Φ(x, y) =    i=l  ... [yi = k][yi−1 = k] where Φemi is the feature vector dedicated to the i-th token (or, the emission features), N represents the number of tokens in this sequence, yi represents the i-th token in the sequence y, [yi = 1] is the indictor variable and k is the number of tags. The inference problems are solved by the Viterbi algorithm. The emission features used in both POS and NER are the standard ones, including word features, word-shape features, etc. For NER, we used additional simple gazetteer features7 and word cluster features (Turian et al., 2010) For dependency parsing, we followed the setting described in (McDonald et al., 2005) and used similar features. The decoding algorithm is the firstorder Eisner’s algorithm (Eisner, 1997). 4.3 Algorithms and Implementation Detail algorithms. For NER-MUC7, NER-CoNLL and POS-WSJ, we ran the online algorithms and DCDSSVM for 25 iterations. For DP-WSJ, we only let the algorithms run for 10 iterations as the inference procedure is very expensive computationally. The algorithms in the experiments are: DCD Our dual coordinate descent method on the L2-Loss SSVM. For DCD-SSVM, r is set to be 5. For bot"
Q13-1017,C96-1058,0,\N,Missing
Q13-1017,P06-1065,1,\N,Missing
Q17-1008,P98-1013,0,0.0391779,"2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) style are also instances of n-ary relation extraction, with extraction of events expressed in a single sentence. McDonald et al. (2005) extract n-ary relations in a biomedical domain, by first factoring the n-ary relation into pair-wise relations between all entity pairs, and then constructing maximal cliques of related entities. Recently, neural models have been applied to semantic role labeling (FitzGerald et al., 2015; Roth 111 and Lapata, 2016). These works learned neural representations by effectively decomposing the n-ary relation into binary relations between the predicate and each arg"
Q17-1008,H05-1091,0,0.217196,"rst review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of"
Q17-1008,P16-1072,0,0.0800421,"g powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker et al., 1998) styl"
Q17-1008,C10-1018,0,0.0365625,"cularly beneficial. 7 Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity"
Q17-1008,M98-1001,0,0.238877,"-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory netwo"
Q17-1008,de-marneffe-etal-2006-generating,0,0.120922,"Missing"
Q17-1008,D15-1112,0,0.0433277,"Missing"
Q17-1008,P10-1160,0,0.211478,"AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs). By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguis"
Q17-1008,P05-1053,0,0.36783,"tic parses, suggesting that encoding high-quality analysis is particularly beneficial. 7 Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue)"
Q17-1008,W09-1401,0,0.0236307,"to the n-ary setting is challenging, as there are n2 paths. One apparent solution is inspired by Davidsonian semantics: first, identify a single trigger phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument. However, challenges remain. It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous. Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009). The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available. Additionally, lexical and syntactic patterns signifying the relation will be sparse. To handle such sparsity, traditional feature-based approaches require extensive engineering and large data. Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences. To overcome these challenges, we explore a general relation extraction framework based on gra"
Q17-1008,J13-4004,0,0.0121401,"ected acyclic graphs (bottom); the graph LSTMs is constructed by a forward pass (Left to Right) followed by a backward pass (Right to Left). Note that information goes from dependency child to parent. 3.1 Document Graph To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies. A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015). Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib. This document graph acts as the backbone upon which a graph LSTM is constructed. If it con104 3.2 Backpropagation in Graph LSTMs Conventional LSTMs are essentially very deep feedforward neural networks. For example, a left-to-right linear LSTM has one hidden vector for each word. This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of"
Q17-1008,P14-5010,0,0.00700447,"ody of biomedical papers is exactly the challenge. As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts. In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles. We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP (Manning et al., 2014). We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions. We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision. The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper. 5 6 Distant Supervision After identifying drug, gene and mutation mentions in the text, co-occurring triples with known interactions were chosen as positive examples. However, unlike the single-sentence setting in"
Q17-1008,P05-1061,0,0.489844,"tation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework"
Q17-1008,P09-1113,0,0.740799,"phrase that signifies the whole relation, then reduce the n-ary relation to n binary relations between the trigger and an argument. However, challenges remain. It is often hard to specify a single trigger, as the relation is manifested by several words, often not contiguous. Moreover, it is expensive and time-consuming to annotate training examples, especially if triggers are required, as is evident in prior annotation efforts such as GENIA (Kim et al., 2009). The realistic and widely adopted paradigm is to leverage indirect supervision, such as distant supervision (Craven and Kumlien, 1999; Mintz et al., 2009), where triggers are not available. Additionally, lexical and syntactic patterns signifying the relation will be sparse. To handle such sparsity, traditional feature-based approaches require extensive engineering and large data. Unfortunately, this challenge becomes much more severe in crosssentence extraction when the text spans multiple sentences. To overcome these challenges, we explore a general relation extraction framework based on graph LSTMs. By learning a continuous representation for words and entities, LSTMs can handle sparsity effectively without requiring intense feature engineeri"
Q17-1008,P16-1105,0,0.26527,"sing a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation. Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks. However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies. In this section, we propose a general framework that generalizes LSTMs to graphs. While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet. Figure 2 shows the architecture of this approach. The input layer is the word embedding of input text. Next is the graph LSTM which learns a contextual representation for each word. For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers. For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work. The layers are trained jointly wit"
Q17-1008,P14-2012,0,0.0332938,"Related Work Most work on relation extraction has been applied to binary relations of entities in a single sentence. We first review relevant work on the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such repre"
Q17-1008,J05-1004,0,0.184752,"P DEP The deletion mutation on exon-19 of EGFR gene was present in 16 patients, while the L858E point mutation on exon-21 was noted in 10. NEXTSENT All patients were treated with gefitinib and showed a partial response. DET AUXPASS NSUBJPASS ROOT AMOD DET DOBJ PREP WITH CONJ AND Figure 1: An example document graph for a pair of sentences expressing a ternary interaction (tumors with L858E mutation in EGFR gene respond to gefitinib treatment). For simplicity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we expl"
Q17-1008,P16-2025,1,0.847562,"dge-type embedding. Edge-Type Embedding To reduce the number of parameters and leverage potential correlation among fine-grained edge types, we learned a lowdimensional embedding of the edge types, and conducted an outer product of the predecessor’s hidden vector and the edge-type embedding to generate a “typed hidden representation”, which is a matrix. The new computation is as follows: 3.5 X Multi-task Learning with Sub-relations Multi-task learning has been shown to be beneficial in training neural networks (Caruana, 1998; Collobert ftj = σ(Wf xt + Uf ×T (hj ⊗ ej ) + bf ) and Weston, 2008; Peng and Dredze, 2016). By X learning contextual entity representations, our frameUo ×T (hj ⊗ ej ) + bo ) ot = σ(Wo xt + j∈P (t) work makes it straightforward to conduct multi-task X Uc ×T (hj ⊗ ej ) + bc ) learning. The only change is to add a separate classic˜t = tanh(Wc xt + j∈P (t) X fier for each related auxiliary relation. All classifiers ftj cj ct = it c˜t + share the same graph LSTMs representation learner j∈P (t) and word embeddings, and can potentially help each ht = ot tanh(ct ) other by pooling their supervision signals. U ’s are now l × l × d tensors (l is the dimension of In the molecular tumor board"
Q17-1008,D14-1162,0,0.0882041,"Missing"
Q17-1008,C08-1088,0,0.0226594,"n the single-sentence binary relation extraction task, and then review related work on n-ary and cross-sentence relation extraction. Binary relation extraction The traditional featurebased methods rely on carefully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolu"
Q17-1008,E17-1110,1,0.202831,"icity, we omit edges between adjacent words or representing discourse relations. work on n-ary relation extraction focused on single sentences (Palmer et al., 2005; McDonald et al., 2005) or entity-centric attributes that can be extracted largely independently (Chinchor, 1998; Surdeanu and Heng, 2014). Prior work on cross-sentence extraction often used coreference to gain access to arguments in a different sentence (Gerber and Chai, 2010; Yoshikawa et al., 2011), without truly modeling inter-sentential relational patterns. (See Section 7 for a more detailed discussion.) A notable exception is Quirk and Poon (2017), which applied distant supervision to general cross-sentence relation extraction, but was limited to binary relations. In this paper, we explore a general framework for cross-sentence n-ary relation extraction, based on graph long short-term memory networks (graph LSTMs). By adopting the graph formulation, our framework subsumes prior approaches based on chain or tree LSTMs, and can incorporate a rich set of linguistic analyses to aid relation extraction. Relation classification takes as input the entity representations learned from the entire text, and can be easily extended for arbitrary re"
Q17-1008,N12-3006,1,0.827666,"f papers contain knowledge about drug-gene-mutation interactions. Extracting such knowledge from the vast body of biomedical papers is exactly the challenge. As we will see in later subsections, distant supervision enables us to generate a sizable training set from a small number of manually curated facts, and the learned model was able to extract orders of magnitude more facts. In future work, we will explore incorporating more known facts for distant supervision and extracting from more full-text articles. We conducted tokenization, part-of-speech tagging, and syntactic parsing using SPLAT (Quirk et al., 2012), and obtained Stanford dependencies (de Marneffe et al., 2006) using Stanford CoreNLP (Manning et al., 2014). We used the entity taggers from Literome (Poon et al., 2014) to identify drug, gene and mutation mentions. We used the Gene Drug Knowledge Database (GDKD) (Dienstmann et al., 2015) and the Clinical Interpretations of Variants In Cancer (CIVIC) knowledge base6 for distant supervision. The knowledge bases distinguish fine-grained interaction types, which we do not use in this paper. 5 6 Distant Supervision After identifying drug, gene and mutation mentions in the text, co-occurring trip"
Q17-1008,reschke-etal-2014-event,0,0.0164171,"ent, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features. Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014; Li et al., 2015) relations, traditionally using hand-engineered features. Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015). Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context. To utilize training data more effectively, we show how multitask learning for component binary su"
Q17-1008,P16-1113,0,0.055469,"Missing"
Q17-1008,P15-1061,0,0.43501,"iLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM). Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5. Quirk and Poon (2017) only extracted binary relations. We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features 108 from all pairs. For binary relation extraction, prior syntax-aware approaches are directly applicable. So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on the shortest dependency"
Q17-1008,D12-1110,0,0.158707,"automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchical syntactic structure. Miwa and Bansal (2016) proposed an architecture that benefits from both types of information, using a surface sequence layer, followed by a dependency-tree sequence layer. N -ary relation extraction Early work on extracting relations between more than two arguments has been done in MUC-7, with a focus on fact/event extraction from news articles (Chinchor, 1998). Semantic role labeling in the Propbank (Palmer et al., 2005) or FrameNet (Baker"
Q17-1008,R11-1004,0,0.156856,"gument, by embedding the dependency path between each pair, or by combining features of the two using a feed-forward network. Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other. In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information. Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010). These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant super"
Q17-1008,P15-1150,0,0.811543,"hese problems by using a series of gates (input, forget and output) to avoid amplifying or suppressing gradients during backpropagation. Consequently, LSTMs are much more effective in capturing long-distance dependencies, and have been applied to a variety of NLP tasks. However, most approaches are based on linear chains and only explicitly model the linear context, which ignores a variety of linguistic analyses, such as syntactic and discourse dependencies. In this section, we propose a general framework that generalizes LSTMs to graphs. While there is some prior work on learning tree LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), to the best of our knowledge, graph LSTMs have not been applied to any NLP task yet. Figure 2 shows the architecture of this approach. The input layer is the word embedding of input text. Next is the graph LSTM which learns a contextual representation for each word. For the entities in question, their contextual representations are concatenated and become the input to the relation classifiers. For a multi-word entity, we simply used the average of its word representations and leave the exploration of more sophisticated aggregation approaches to future work. The layers"
Q17-1008,P16-1123,0,0.436558,"Missing"
Q17-1008,W06-1671,0,0.152345,"ombining features of the two using a feed-forward network. Although some re-ranking or joint inference models have been employed, the representations of the individual arguments do not influence each other. In contrast, we propose a neural architecture that jointly represents n entity mentions, taking into account long-distance dependencies and inter-sentential information. Cross-sentence relation extraction Several relation extraction tasks have benefited from crosssentence extraction, including MUC fact and event extraction (Swampillai and Stevenson, 2011), record extraction from web pages (Wick et al., 2006), extraction of facts for biomedical domains (Yoshikawa et al., 2011), and extensions of semantic role labeling to cover implicit inter-sentential arguments (Gerber and Chai, 2010). These prior works have either relied on explicit co-reference annotation, or on the assumption that the whole document refers to a single coherent event, to simplify the problem and reduce the need for powerful representations of multi-sentential contexts of entity mentions. Recently, cross-sentence relation extraction models have been learned with distant supervision, and used integrated contextual evidence of div"
Q17-1008,D15-1062,0,0.540872,"able 1: Average test accuracy in five-fold crossvalidation for drug-gene-mutation ternary interactions. Feature-Based used the best performing model in (Quirk and Poon, 2017) with features derived from shortest paths between all entity pairs. Model Single-Sent. Cross-Sent. Feature-Based 73.9 75.2 CNN BiLSTM BiLSTM-Shortest-Path Tree LSTM Graph LSTM-EMBED Graph LSTM-FULL 73.0 73.9 70.2 75.9 74.3 75.6 74.9 76.0 71.7 75.9 76.5 76.7 Table 2: Average test accuracy in five-fold crossvalidation for drug-mutation binary relations, with an extra baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-dire"
Q17-1008,D15-1206,0,0.709967,"able 1: Average test accuracy in five-fold crossvalidation for drug-gene-mutation ternary interactions. Feature-Based used the best performing model in (Quirk and Poon, 2017) with features derived from shortest paths between all entity pairs. Model Single-Sent. Cross-Sent. Feature-Based 73.9 75.2 CNN BiLSTM BiLSTM-Shortest-Path Tree LSTM Graph LSTM-EMBED Graph LSTM-FULL 73.0 73.9 70.2 75.9 74.3 75.6 74.9 76.0 71.7 75.9 76.5 76.7 Table 2: Average test accuracy in five-fold crossvalidation for drug-mutation binary relations, with an extra baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-dire"
Q17-1008,C16-1138,0,0.0397931,"egrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on modeling either the surface word sequences or the hierarchi"
Q17-1008,K15-2001,0,0.021367,"hs (bottom); the graph LSTMs is constructed by a forward pass (Left to Right) followed by a backward pass (Right to Left). Note that information goes from dependency child to parent. 3.1 Document Graph To model various dependencies from linguistic analysis at our disposal, we follow Quirk and Poon (2017) and introduce a document graph to capture intra- and inter-sentential dependencies. A document graph consists of nodes that represent words and edges that represent various dependencies such as linear context (adjacent words), syntactic dependencies, and discourse relations (Lee et al., 2013; Xue et al., 2015). Figure 1 shows the document graph for our running example; this instance suggests that tumors with L858E mutation in EGFR gene responds to the drug gefitinib. This document graph acts as the backbone upon which a graph LSTM is constructed. If it con104 3.2 Backpropagation in Graph LSTMs Conventional LSTMs are essentially very deep feedforward neural networks. For example, a left-to-right linear LSTM has one hidden vector for each word. This vector is generated by a neural network (recurrent unit) that takes as input the embedding of the given word and the hidden vector of the previous word."
Q17-1008,C14-1220,0,0.860338,"baseline using a BiLSTM on the shortest dependency path (Xu et al., 2015b; Miwa and Bansal, 2016). results could be noisy (e.g., entity triples not known to have an interaction might actually have one), but this evaluation is automatic and can quickly evaluate the impact of various design choices. We evaluated two variants of graph LSTMs: “Graph LSTM-FULL” with full parametrization and “Graph LSTM-EMBED” with edge-type embedding. We compared graph LSTMs with three strong baseline systems: a well-engineered feature-based classifier (Quirk and Poon, 2017), a convolutional neural network (CNN) (Zeng et al., 2014; Santos et al., 2015; Wang et al., 2016), and a bi-directional LSTM (BiLSTM). Following Wang et al. (2016), we used input attention for the CNN and a input window size of 5. Quirk and Poon (2017) only extracted binary relations. We extended it to ternary relations by deriving features for each entity pair (with added annotation to signify the two entity types), and pooling the features 108 from all pairs. For binary relation extraction, prior syntax-aware approaches are directly applicable. So we also compared with a state-of-the-art tree LSTM system (Miwa and Bansal, 2016) and a BiLSTM on th"
Q17-1008,D15-1203,0,0.237949,"h distant supervision, and used integrated contextual evidence of diverse types without reliance on these assumptions (Quirk and Poon, 2017), but that work focused on binary relations only and explicitly engineered sparse indicator features. Relation extraction using distant supervision Distant supervision has been applied to extraction of binary (Mintz et al., 2009; Poon et al., 2015) and n-ary (Reschke et al., 2014; Li et al., 2015) relations, traditionally using hand-engineered features. Neural architectures have recently been applied to distantly supervised extraction of binary relations (Zeng et al., 2015). Our work is the first to propose a neural architecture for n-ary relation extraction, where the representation of a tuple of entities is not decomposable into independent representations of the individual entities or entity pairs, and which integrates diverse information from multi-sentential context. To utilize training data more effectively, we show how multitask learning for component binary sub-relations can improve performance. Our learned representation combines information sources within a single sentence in a more integrated and generalizable fashion than prior approaches, and can al"
Q17-1008,Y15-1009,0,0.0475333,"fully designed features to learn good models, and often integrate diverse sources of evidence such as word sequences and syntax context (Kambhatla, 2004; GuoDong et al., 2005; Boschee et al., 2005; Suchanek et al., 2006; Chan and Roth, 2010; Nguyen and Grishman, 2014). The kernel-based methods design various subsequence or tree kernels (Mooney and Bunescu, 2005; Bunescu and Mooney, 2005; Qian et al., 2008) to capture structured information. Recently, models based on neural networks have advanced the state of the art by automatically learning powerful feature representations (Xu et al., 2015a; Zhang et al., 2015; Santos et al., 2015; Xu et al., 2015b; Xu et al., 2016). Most neural architectures resemble Figure 2, where there is a core representation learner (blue) that takes word embeddings as input and produces contextual entity representations. Such representations are then taken by relation classifiers to produce the final predictions. Effectively representing sequences of words, both convolutional (Zeng et al., 2014; Wang et al., 2016; Santos et al., 2015) and RNN-based architectures (Zhang et al., 2015; Socher et al., 2012; Cai et al., 2016) have been successful. Most of these have focused on mo"
Q17-1008,C98-1013,0,\N,Missing
W04-2401,C02-1151,1,0.239201,"ion; (2) conjunctions of the features from the two arguments; (3) some patterns extracted from the sentence or between the two arguments. Some features in category (3) are “the number of words between arg1 and arg2 ”, “whether arg1 and arg2 are the same word”, or “arg1 is the beginning of the sentence and has words that consist of all capitalized characters”, where arg1 and arg2 represent the first and second argument entities respectively. In addition, Table 1 presents some patterns we use. The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is specifically tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (entity classes or relation classes, in this case) are represented as linear functions over a common feature space. While SNoW can be used as a classifier and predicts using a winner-take-all mechanism over the activation value of the target classes, we can also rely directly on the raw activation value it outputs, which is the weighted linear sum of the active features, to estimate the posteriors. It can be verified that the resulting"
W04-2401,W03-0419,0,0.102246,"Missing"
W04-2401,W03-0428,0,\N,Missing
W04-2421,C02-1151,1,0.815577,"designed to determine argument type, given a candidate phrase. Because phrases are considered as a whole, global properties of the candidates can be used to discover how likely it is that a phrase is of a given argument type. However, the set of possible role-labelings is restricted by structural and linguistic constraints. We encode these constraints using linear functions and use integer programming to ensure the final prediction is consistent (see Section 4). 2 SNoW Learning Architecture The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is specifically tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (phrase border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It incorporates several improvements over the basic Winnow update rule. In particular, a regularization term is added, which has the affect of trying to separate the data with a think separator (Grove and Roth, 2001; Hang et al., 2002). In the work presented here we use this regularization with a fixed"
W04-2421,W04-2401,1,0.738798,"actually constraints that can be evaluated on a per-phrase basis and thus can be applied to the individual phrases at any time. For efficiency sake, we eliminate these even before the second phase scoring is begun. Constraints 5, 8, and 9 are valid for only a subset of the arguments. These constraints are easy to transform into linear constraintsP (for example, for each class c, constraint 5 bei 2 comes M i=1 [S = c] ≤ 1) . Then the optimum solution of the cost function given in Equation 2 can be found by integer linear programming3. A similar method was used for entity/relation recognition (Roth and Yih, 2004). Almost all previous work on shallow parsing and phrase classification has used Constraint 4 to ensure that there are no overlapping phrases. By considering additional constraints, we show improved performance (see Table 1). 5 Results In this section, we present results. For the second phase, we evaluate the quality of the phrase predictor. The result first evaluates the phrase classifier, given the perfect phrase locations without using inference (i.e. F (P M ) = P M ). The second, adds inference to the phrase classification over the perfect classifiers (see Table 2). We evaluate the overall"
W05-0625,W04-2412,0,0.189783,"Missing"
W05-0625,C04-1197,1,0.590283,"ed to classify the types of the arguments supplied by the argument identification stage. To reduce the excessive candidates mistakenly output by the previous stage, the classifier can also classify the argument as NULL (“not an argument”) to discard the argument. The features used here are the same as those used in the argument identification stage with the following additional features. most one argument of each type.” This knowledge is used to resolve any inconsistencies of argument classification in order to generate final legitimate predictions. We use the inference process introduced by (Punyakanok et al., 2004). The process is formulated as an integer linear programming (ILP) problem that takes as inputs the confidences over each type of the arguments supplied by the argument classifier. The output is the optimal solution that maximizes the linear sum of the confidence scores (e.g., the conditional probabilities estimated by the argument classifier), subject to the constraints that encode the domain knowledge. Formally speaking, the argument classifier attempts to assign labels to a set of arguments, S 1:M , indexed from 1 to M . Each argument S i can take any label from a set of argument labels, P,"
W05-0625,C02-1151,1,0.737508,"ectively. In fact, these two parsers have noticeably different output. In evaluation, we run the system that was trained with Charniak’s parser 5 times with the top-5 parse trees output by Charniak’s parser1 . Together we have six different outputs per predicate. Per each parse tree output, we ran the first three stages, namely pruning, argument 1 The top parse tree were from the official output by CoNLL. The 2nd-5th parse trees were output by Charniak’s parser. 183 3 Learning and Evaluation The learning algorithm used is a variation of the Winnow update rule incorporated in SNoW (Roth, 1998; Roth and Yih, 2002), a multi-class classifier that is tailored for large scale learning tasks. SNoW learns a sparse network of linear functions, in which the targets (argument border predictions or argument type predictions, in this case) are represented as linear functions over a common feature space. It improves the basic Winnow multiplicative update rule with a regularization term, which has the effect of trying to separate the data with a large margin separator (Grove and Roth, 2001; Hang et al., 2002) and voted (averaged) weight vector (Freund and Schapire, 1999). Softmax function (Bishop, 1995) is used to"
W05-0625,W04-3212,0,0.407737,"conditional probability distribution, P rob(S i = ci ), then, given a sentence, the inference procedure seeks an global assignment that maximizes the following objective function, 1:M cˆ = argmax M X P rob(S i = ci ), c1:M ∈P M i=1 subject to linguistic and structural constraints. In other words, this objective function reflects the expected number of correct argument predictions, subject to the constraints. The constraints are encoded as the followings. • Syntactic frame describes the sequential pattern of the noun phrases and the predicate in the sentence. This is the feature introduced by (Xue and Palmer, 2004). • No overlapping or embedding arguments. • Propositional phrase head is the head of the first phrase after the preposition inside PP. • Exactly one V argument per predicate considered. • NEG and MOD feature indicate if the argument is a baseline for AM-NEG or AM-MOD. The rules of the NEG and MOD features are used in a baseline SRL system developed by Erik Tjong Kim Sang (Carreras and M`arquez, 2004). • NE indicates if the target argument is, embeds, overlaps, or is embedded in a named-entity along with its type. 1.4 Inference The purpose of this stage is to incorporate some prior linguistic"
W05-0625,J03-4003,0,\N,Missing
W05-0625,W05-0620,0,\N,Missing
W11-0329,N09-1003,0,0.0187707,"able to efficiently handle a large number of training examples in the highdimensional space. Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient. 1 Introduction Measures of text similarity have many applications and have been studied extensively in both the NLP and IR communities. For example, a combination of corpus and knowledge based methods have been invented for judging word similarity (Lin, 1998; Agirre et al., 2009). Similarity derived from a largescale Web corpus has been used for automatically extending lists of typed entities (Vyas and Pantel, 2009). Judging the degree of similarity between documents is also fundamental to classical IR problems such as document retrieval (Manning et al., 2008). In all these applications, the vector-based similarity method is the most widely used. Term vectors are first constructed to represent the original text objects, where each term is associated with However, the main weakness of this term-vector representation is that different but semantically related terms are"
W11-0329,W05-0601,0,0.0226363,"eval recall by leveraging Wikipedia. In a companion paper, we also demonstrated that various topic mod254 els including S2Net can enhance the ranking function (Gao et al., 2011). For text categorization, similarity between terms is often encoded as kernel functions embedded in the learning algorithms, and thus increase the classification accuracy. Representative approaches include latent semantic kernels (Cristianini et al., 2002), which learns an LSA-based kernel function from a document collection, and work that computes term-similarity based on the linguistic knowledge provided by WordNet (Basili et al., 2005; Bloehdorn and Moschitti, 2007). 6 Conclusions In this paper, we presented S2Net, a discriminative approach for learning a projection matrix that maps raw term-vectors to a low-dimensional space. Our learning method directly optimizes the model so that the cosine score of the projected vectors can become a reliable similarity measure. The strength of this model design has been shown empirically in two very different tasks. For cross-lingual document retrieval, S2Net significantly outperforms OPCA, which is the best prior approach. For ad selection and filtering, S2Net also outperforms all met"
W11-0329,P98-1069,0,0.0171388,"sed in the experiments. 4 Experiments We compare S2Net experimentally with existing approaches on two very different tasks: cross-lingual document retrieval and ad relevance measures. 4.1 Comparable Document Retrieval With the growth of multiple languages on the Web, there is an increasing demand of processing crosslingual documents. For instance, machine translation (MT) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the Web (Munteanu and Marcu, 2005). Word-level translation lexicons can also be learned from comparable documents (Fung and Yee, 1998; Rapp, 1999). In this cross-lingual document retrieval task, given a query document in one language, the goal is to find the most similar document from the corpus in another language. 4.1.1 Data & Setting We followed the comparable document retrieval setting described in (Platt et al., 2010) and evaluated S2Net on the Wikipedia dataset used in that paper. This data set consists of Wikipedia documents 1 Without the γ parameter, the model still outperforms other baselines in our experiments, but with a much smaller gain. in two languages, English and Spanish. An article in English is paired wit"
W11-0329,P98-2127,0,0.025292,"ors, and is able to efficiently handle a large number of training examples in the highdimensional space. Evaluated on two very different tasks, cross-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient. 1 Introduction Measures of text similarity have many applications and have been studied extensively in both the NLP and IR communities. For example, a combination of corpus and knowledge based methods have been invented for judging word similarity (Lin, 1998; Agirre et al., 2009). Similarity derived from a largescale Web corpus has been used for automatically extending lists of typed entities (Vyas and Pantel, 2009). Judging the degree of similarity between documents is also fundamental to classical IR problems such as document retrieval (Manning et al., 2008). In all these applications, the vector-based similarity method is the most widely used. Term vectors are first constructed to represent the original text objects, where each term is associated with However, the main weakness of this term-vector representation is that different but semantica"
W11-0329,D09-1092,0,0.12033,"PLSA to a proper generative model for documents and places Dirichlet priors over the parameters θ and φ. In the experiments in this paper, our implementation of PLSA is LDA with maximum a posteriori (MAP) inference, which was shown to be comparable to the current best Bayesian inference methods for LDA (Asuncion et al., 2009). Recently, these topic models have been generalized to handle pairs or tuples of corresponding documents, which could be translations in multiple languages, or documents in the same language that are considered similar. For instance, the Poly-lingual Topic Model (PLTM) (Mimno et al., 2009) is an extension to LDA that views documents in a tuple as having a shared topic vector θ. Each of the documents in the tuple uses θ to select the topics z of tokens, but could use a different (languagespecific) word-topic-distribution M ULTI(φL z ). Two additional models, Joint PLSA (JPLSA) and Coupled PLSA (CPLSA) were introduced in (Platt et al., 2010). JPLSA is a close variant of PLTM when documents of all languages share the same word-topic distribution parameters, and MAP inference is performed instead of Bayesian. CPLSA extends JPLSA by constraining paired documents to not only share th"
W11-0329,J05-4003,0,0.0121379,"int (A0 ), or simply by early stopping. Empirically we found that the latter is more effective and it is used in the experiments. 4 Experiments We compare S2Net experimentally with existing approaches on two very different tasks: cross-lingual document retrieval and ad relevance measures. 4.1 Comparable Document Retrieval With the growth of multiple languages on the Web, there is an increasing demand of processing crosslingual documents. For instance, machine translation (MT) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the Web (Munteanu and Marcu, 2005). Word-level translation lexicons can also be learned from comparable documents (Fung and Yee, 1998; Rapp, 1999). In this cross-lingual document retrieval task, given a query document in one language, the goal is to find the most similar document from the corpus in another language. 4.1.1 Data & Setting We followed the comparable document retrieval setting described in (Platt et al., 2010) and evaluated S2Net on the Wikipedia dataset used in that paper. This data set consists of Wikipedia documents 1 Without the γ parameter, the model still outperforms other baselines in our experiments, but w"
W11-0329,D10-1025,1,0.901798,"s have been generalized to handle pairs or tuples of corresponding documents, which could be translations in multiple languages, or documents in the same language that are considered similar. For instance, the Poly-lingual Topic Model (PLTM) (Mimno et al., 2009) is an extension to LDA that views documents in a tuple as having a shared topic vector θ. Each of the documents in the tuple uses θ to select the topics z of tokens, but could use a different (languagespecific) word-topic-distribution M ULTI(φL z ). Two additional models, Joint PLSA (JPLSA) and Coupled PLSA (CPLSA) were introduced in (Platt et al., 2010). JPLSA is a close variant of PLTM when documents of all languages share the same word-topic distribution parameters, and MAP inference is performed instead of Bayesian. CPLSA extends JPLSA by constraining paired documents to not only share the same prior topic distribution θ, but to also have similar fractions of tokens assigned to each topic. This constraint is enforced on expectation using posterior regularization (Ganchev et al., 2009). In the rest of the paper, we first survey some existing work in Sec. 2, with an emphasis on approaches included in our experimental comparison. We present"
W11-0329,P99-1067,0,0.0156585,"ts. 4 Experiments We compare S2Net experimentally with existing approaches on two very different tasks: cross-lingual document retrieval and ad relevance measures. 4.1 Comparable Document Retrieval With the growth of multiple languages on the Web, there is an increasing demand of processing crosslingual documents. For instance, machine translation (MT) systems can benefit from training on sentences extracted from parallel or comparable documents retrieved from the Web (Munteanu and Marcu, 2005). Word-level translation lexicons can also be learned from comparable documents (Fung and Yee, 1998; Rapp, 1999). In this cross-lingual document retrieval task, given a query document in one language, the goal is to find the most similar document from the corpus in another language. 4.1.1 Data & Setting We followed the comparable document retrieval setting described in (Platt et al., 2010) and evaluated S2Net on the Wikipedia dataset used in that paper. This data set consists of Wikipedia documents 1 Without the γ parameter, the model still outperforms other baselines in our experiments, but with a much smaller gain. in two languages, English and Spanish. An article in English is paired with a Spanish a"
W11-0329,P10-1040,0,0.0085695,"of S2Net are all different compared to previous work. For example, targeting the application of face verification, Chopra et al. (2005) used a convolutional network and designed a contrastive loss function for optimizing a Eucliden distance metric. In contrast, the network of S2Net is equivalent to a linear projection matrix and has a pairwise loss function. In terms of the learning framework, S2Net is closely related to several neural network based approaches, including autoencoders (Hinton and Salakhutdinov, 2006) and finding low-dimensional word representations (Collobert and Weston, 2008; Turian et al., 2010). Architecturally, S2Net is also similar to RankNet (Burges et al., 2005), which can be viewed as a Siamese neural network that learns a ranking function. The strategy that S2Net takes to learn from labeled pairs of documents can be analogous to the work of distance metric learning. Although high dimensionality is not a problem to algorithms like HDLR, it suffers from a different scalability issue. As we have observed in our experiments, the algorithm can only handle a small number of similarity/dissimilarity constraints (i.e., the labeled examples), and is not able to use a large number of ex"
W11-0329,N09-1033,0,0.00773383,"s-lingual document retrieval and ad relevance measure, our method not only outperforms existing state-of-the-art approaches, but also achieves high accuracy at low dimensions and is thus more efficient. 1 Introduction Measures of text similarity have many applications and have been studied extensively in both the NLP and IR communities. For example, a combination of corpus and knowledge based methods have been invented for judging word similarity (Lin, 1998; Agirre et al., 2009). Similarity derived from a largescale Web corpus has been used for automatically extending lists of typed entities (Vyas and Pantel, 2009). Judging the degree of similarity between documents is also fundamental to classical IR problems such as document retrieval (Manning et al., 2008). In all these applications, the vector-based similarity method is the most widely used. Term vectors are first constructed to represent the original text objects, where each term is associated with However, the main weakness of this term-vector representation is that different but semantically related terms are not matched and cannot influence the final similarity score. As an illustrative example, suppose the two compared term-vectors are: {purcha"
W11-0329,C98-1066,0,\N,Missing
W11-0329,C98-2122,0,\N,Missing
W16-2505,marelli-etal-2014-sick,0,0.0191622,"Missing"
W16-2505,N16-1098,1,0.795855,"Missing"
W16-2505,D15-1036,0,0.0222761,"search in the past few years. While one could evaluate a given vector representation (embedding) on various down-stream applications, it is time-consuming at both implementation and runtime, which gives rise to focusing on an intrinsic evaluation. The intrinsic evaluation has been mostly focused on textual similarity where the task is to predict how semantically similar two words/sentences are, which is evaluated against the gold human similarity scores. It has been shown that semantic similarity tasks do not accurately measure the effectiveness of an embedding in the other down-stream tasks (Schnabel et al., 2015; Tsvetkov et al., 2015). Furthermore, human annotation of similarity at sentencelevel without any underlying context can be subjective, resulting in lower inter-annotator agreement and hence a less reliable evaluation method. 1 Examples of this include the semantic relatedness (SICK) dataset (Marelli et al., 2014), where given two sentences, the task is to produce a score of how semantically related these sentences are 24 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 24–29, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Lingu"
W16-2505,D15-1243,0,0.0454412,"years. While one could evaluate a given vector representation (embedding) on various down-stream applications, it is time-consuming at both implementation and runtime, which gives rise to focusing on an intrinsic evaluation. The intrinsic evaluation has been mostly focused on textual similarity where the task is to predict how semantically similar two words/sentences are, which is evaluated against the gold human similarity scores. It has been shown that semantic similarity tasks do not accurately measure the effectiveness of an embedding in the other down-stream tasks (Schnabel et al., 2015; Tsvetkov et al., 2015). Furthermore, human annotation of similarity at sentencelevel without any underlying context can be subjective, resulting in lower inter-annotator agreement and hence a less reliable evaluation method. 1 Examples of this include the semantic relatedness (SICK) dataset (Marelli et al., 2014), where given two sentences, the task is to produce a score of how semantically related these sentences are 24 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 24–29, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics cs.rochester.edu/"
W16-2505,D13-1020,0,\N,Missing
