2001.mtsummit-papers.34,P89-1010,0,0.179801,"Missing"
2001.mtsummit-papers.34,J98-1001,0,0.0733178,"Missing"
2001.mtsummit-papers.34,P98-2127,0,0.0186808,"ber of patterns shown is set by the user, but will typically be over 200. These are listed for each relation in order of salience, with the count of corpus instances. The instances can be instantly retrieved and shown in a concordance window. Producing a word sketch for a medium-to-high frequency word takes around ten seconds.3 Calculating Salience is Information frequency. I Salience estimated as the product of Mutual I (Church & Hanks, 1989) and log for a (W1,Rel,W2) triple4 is calculated as I(W1,Rel,W2) = log *,Rel,* x W1,Rel,W2 W1,Rel,* x *,Rel,W2 The notation here is adopted from Lin (1998). W1, Rel, W2 denotes the frequency count of the triple (W1, Rel, W2)5 in the grammatical relations database. Where W1, Rel or W2 is the wild card (*), the frequency is of all the dependency triples that match the remainder of the pattern. Our experience of working lexicographers&apos; use of Mutual Information or log-likelihood lists shows that, for lexicographic purposes, these over-emphasise low frequency items, and that multiplying by log frequency is an appropriate adjustment. Matching patterns with target words The next task is to enter a preliminary list of possible target words (or for th"
2001.mtsummit-papers.34,H93-1052,0,0.0438536,"that, for lexicographic purposes, these over-emphasise low frequency items, and that multiplying by log frequency is an appropriate adjustment. Matching patterns with target words The next task is to enter a preliminary list of possible target words (or for the monolingual lexicographer, arbitrary mnemonics to represent the different possible senses). So for the word bank and a taget language of Russian, we might propose the target translations as bank (financial institution), bereg (river bank), gryada (bank of clouds) etc. As Table 2 shows, and in keeping with ``one sense per collocation&apos;&apos; (Yarowsky, 1993), in most cases highsalience patterns or clues indicate just one of the word&apos;s senses. The user then has the task of associating, by selecting from a pop-up menu, the required target for unambiguous clues. Reference can be made at any time to the actual corpus instances, which demonstrate the contexts in which the triple occurs.6 3 A set of pre-compiled word sketches can be seen at http://www.itri.bton.ac.uk/~adam.kilgarriff/wordsketches.html 4 Grammatical-relation, preposition pairs are currently treated as atomic relations for purposes of calculating MI. 5 Strictly, the quintuple (Rel-part-1"
2001.mtsummit-papers.34,P95-1026,0,0.0536755,"u need a lot of them. In this paper we present a corpus-based lexicographers&apos; workstation, in which this process is automatically supported. The software finds, from a large source-language corpus, a candidate set of contexts C which appear salient for a source word, so are good candidates for the left hand side of context is c ⇒ translate as t rules. The lexicographer provides the appropriate value of t for a few of these, in a process that takes just a few minutes per source word. The software then applies the highest-performing Word Sense Disambiguation (WSD) algorithm currently available (Yarowsky (1995)) to bootstrap a long list of rules from the ‘seeds’ the lexicographer has provided. The list is ordered by confidence: the rules at the top of the list are the ones we can be surest of. Thus to disambiguate, and thereby select the appropriate target word t, for an instance of s in a text to be translated, all an MT system need do is work through the list until it finds a rule where the left hand side matches. It can then select the targetlanguage word that the rule specifies. The WASP-Bench is designed to fit readily with MT companies&apos; working practices. It can be used for as many or as few s"
2001.mtsummit-papers.34,J90-1003,0,\N,Missing
2001.mtsummit-papers.34,C98-2122,0,\N,Missing
2006.eamt-1.31,baroni-bernardini-2004-bootcat,1,0.938019,"corpus query tool, for further exploration. Reference corpora are used to identify the key terms in the specialist domain. 1 Introduction Where should a translator look if they want to find the terminology of a specialist area? Regular dictionaries will not cover it, specialist dictionaries, if they exist, will be hard to find and expensive and are likely to be out of date. The obvious answer is the web. In 2006, this is probably what every working translator and terminologist does as a matter of course. The question, then, is how to do it effectively and efficiently. 1 Baroni and Bernardini ([2004]) responded to the challenge with the BootCaT tools. The basic method is • Select a few “seed terms”. • Send queries with the seed terms to Google. • Collect the pages that the Google hits page points to. This is then a first-pass specialist corpus. The vocabulary in this corpus can be compared with a reference corpus and terms can be automatically extracted. The process can 1 For early accounts of using the web in this way see Varantola ([2000]) and Jones and Ghani ([2000]). For an overview of the use of the web as a source of linguistic data see Kilgarriff and Grefenstette ([2003]). also be"
2006.eamt-1.31,E06-2001,1,0.622881,"ecialist corpus. The vocabulary in this corpus can be compared with a reference corpus and terms can be automatically extracted. The process can 1 For early accounts of using the web in this way see Varantola ([2000]) and Jones and Ghani ([2000]). For an overview of the use of the web as a source of linguistic data see Kilgarriff and Grefenstette ([2003]). also be iterated with the new terms as seeds to give a “purer” specialist corpus. The software is freely available for download and has been widely used, both to produce specialist corpus for technical term extraction (see, e.g., Fantinuoli [2006]) and to produce large general-language corpora (Sharoff [2005], Baroni and Kilgarriff [2006]). However, the software must be downloaded and installed, and this presents a barrier for people without computer systems skills. 2 WebBootCaT In this paper we present a web-service version of the BootCaT tools, WebBootCaT. The user no longer needs to download or install software, as they use a copy of the software which is already installed on our webserver. Our webserver also holds the corpus and loads it into a corpus query tool, the Sketch Engine (Kilgarriff et al [2004]) for further investigation"
2006.eamt-1.31,J03-3001,1,0.723111,"ernardini ([2004]) responded to the challenge with the BootCaT tools. The basic method is • Select a few “seed terms”. • Send queries with the seed terms to Google. • Collect the pages that the Google hits page points to. This is then a first-pass specialist corpus. The vocabulary in this corpus can be compared with a reference corpus and terms can be automatically extracted. The process can 1 For early accounts of using the web in this way see Varantola ([2000]) and Jones and Ghani ([2000]). For an overview of the use of the web as a source of linguistic data see Kilgarriff and Grefenstette ([2003]). also be iterated with the new terms as seeds to give a “purer” specialist corpus. The software is freely available for download and has been widely used, both to produce specialist corpus for technical term extraction (see, e.g., Fantinuoli [2006]) and to produce large general-language corpora (Sharoff [2005], Baroni and Kilgarriff [2006]). However, the software must be downloaded and installed, and this presents a barrier for people without computer systems skills. 2 WebBootCaT In this paper we present a web-service version of the BootCaT tools, WebBootCaT. The user no longer needs to down"
2014.tc-1.16,baroni-bernardini-2004-bootcat,0,0.0397266,"e working on. 1. The term-finding functionality The term-finder starts from a domain corpus, and a reference corpus. First it finds all the noun phrases, and their frequencies, in both corpora. It then takes the ratio, and the items with highest ratios will be terms, as in Figure 1 (data supplied by lead users, the World Intellectual Property Organisation). 129 Translating and The Computer 36 14 Figure 1: French terms in the mobile communications domain In some cases, as with WIPO, the user will have domain corpora, but in others they will not. In that case they may use the BootCaT procedure (Baroni and Bernardini 2004). The user, typically a translator working in a domain where they are not an expert, inputs a few domainspecific ‘seed words’; these are sent to a search engine, and the hits identified by the search engine are gathered, cleaned, de-duplicated and processed to give a domain-specific corpus. This functionality has been found to support translators well (Bernardini et al 2013). For some time, the Sketch Engine has incorporated a BootCaT tool, allowing users to create an instant corpus for a domain, which means they can then compare this corpus with a reference corpus to find the keywords of the"
ambati-etal-2012-word,I11-1079,1,\N,Missing
ambati-etal-2012-word,N06-1042,0,\N,Missing
ambati-etal-2012-word,P07-2011,1,\N,Missing
ambati-etal-2012-word,J08-4010,0,\N,Missing
ambati-etal-2012-word,J08-3003,0,\N,Missing
ambati-etal-2012-word,N10-1012,0,\N,Missing
ambati-etal-2012-word,P98-2127,0,\N,Missing
ambati-etal-2012-word,C98-2122,0,\N,Missing
ambati-etal-2012-word,kilgarriff-etal-2010-corpus,1,\N,Missing
baroni-etal-2008-cleaneval,W06-1421,1,\N,Missing
baroni-etal-2008-cleaneval,J03-3001,1,\N,Missing
E03-2007,W03-2202,1,0.803246,"ng up different evaluation themes for different perspectives. We pursued five approaches: SENSEVAL — seen purely as a WSD system, WASPBENCH performed on a par with the best in the world (Tugwell and Kilgarriff, 2001). Expert review — three experienced lexicographers reviewed WASPBENCH very favourably, also providing detailed feedback for future development. Comparison with MT — students at Leeds University 6 were able to produce (with minimal training) word experts for medium-complexity words in 30 minutes which outperformed translation of ambiguous words by commercially-available MT systems (Koeling et al., 2003). Consistency of results — subjects at IIIT, Hyderabad, India 7 confirmed the Leeds result and established that different subjects produced consistent results from the same data (Koeling and Kilgarriff, 2002). Word sketches — lexicographers preparing the new Macmillan English Dictionary for Advanced Leaners (Runde11, 2002) successfully used word sketches as the primary source of evidence for the behaviour of all medium and high frequency nouns, verbs and adjectives (Kilgarriff and Rundell, 2002). These evaluations demonstrate that WASPBENCH does support accurate, efficient, semiautomatic, inte"
E03-2007,H93-1052,0,0.0558536,"l Infor212 count of corpus instances. The instances can be instantly retrieved and shown in a concordance window. Producing a word sketch for a medium-tohigh frequency word takes around ten seconds. 4 2.3 Matching patterns with senses The next task is to enter a preliminary list of senses for the word, in the form of some arbitrary mnemonics, perhaps MONEY, CLOUD and RIVER for three senses of bank. This inventory may be drawn from the user&apos;s knowledge, from a perusal of the word sketch, or from a pre-existing dictionary entry. As Table 2 shows, and in keeping with &quot;one sense per collocation&quot; (Yarowsky, 1993) in most cases, high-salience patterns or clues indicate just one of the word&apos;s senses. The user then has the task of associating, by selecting from a pop-up menu, the required sense for unambiguous clues. Reference can be made at any time to the actual corpus instances, which demonstrate the contexts in which the triple occurs. The number of relations marked will depend on the time available to the lexicographer, as well as the complexity of the sense division to be made. The act of assigning senses to patterns may very well lead the lexicographer to discover fresh, unconsidered senses or sub"
E03-2007,P95-1026,0,0.027152,"the triple occurs. The number of relations marked will depend on the time available to the lexicographer, as well as the complexity of the sense division to be made. The act of assigning senses to patterns may very well lead the lexicographer to discover fresh, unconsidered senses or subsenses of the word. If so, extra sense mnemonics can be added. When the user deems that sufficient patterns have been marked with senses, the pattern-sense pairs are submitted to the next stage: automatic disambiguation. 2.4 The Disambiguation Algorithm WASPBENCH uses Yarowsky&apos;s decision list approach to WSD (Yarowsky, 1995). This is a bootstrapping algorithm that, given some initial seeding, iteratively divides the corpus examples into the different senses. Given a set of classified collocations, or clues, and a set of corpus instances for the word, the algorithm is as follows: mation and log frequency. Our experience of working lexicographers&apos; use of Mutual Information or log-likelihood lists shows that, for lexicographic purposes, these over-emphasise low frequency items, and that multiplying by log frequency is an appropriate adjustment. A set of pre-compiled word sketches can be seen at http://www.itri.brigh"
E03-2007,S01-1037,1,\N,Missing
E06-2001,J03-3001,1,0.681055,"lternative is to crawl the Web ourselves, which also allows us to remove duplicates and nearduplicates, navigational material, and a range of other kinds of non-linguistic matter. We can also tokenize, lemmatise and part-of-speech tag the corpus, and load the data into a corpus query tool which supports sophisticated linguistic queries. We have now done this for German and Italian, with corpus sizes of over 1 billion words in each case. We provide Web access to the corpora in our query tool, the Sketch Engine. 1 Introduction The Web contains vast amounts of linguistic data for many languages (Kilgarriff and Grefenstette, 2003). One key issue for linguists and language technologists is how to access it. The drawbacks of using commercial search engines are presented in Kilgarriff (2003). An alternative is to crawl the Web ourselves.1 We have done this for two languages, German and Italian, and here we report on the pipeline of processes which give us reasonably well-behaved, ‘clean’ corpora for each language. 1 Another Web access option is Alexa (http://pages. alexa.com/company/index.html), who allow the user (for a modest fee) to access their cached Web directly. Using Alexa would mean one did not need to crawl; how"
E14-2014,P13-1040,0,0.18016,"Missing"
E14-2014,baroni-bernardini-2004-bootcat,0,0.133377,"Missing"
E14-2014,2006.eamt-1.31,1,0.846924,"Missing"
E14-2014,gojun-etal-2012-adapting,0,0.065049,"Missing"
E14-2014,zhang-etal-2008-comparative,0,0.0779131,"Missing"
E93-1026,E93-1012,0,0.093384,"Missing"
E93-1026,W91-0209,0,0.135311,"Missing"
E93-1026,A92-1012,0,0.0276184,"Missing"
E93-1026,E89-1009,0,0.236344,"Missing"
E93-1026,P89-1005,0,0.061789,"Missing"
E93-1026,J91-4003,0,0.0459165,"Missing"
E93-1026,A92-1011,0,0.0265073,"Missing"
E93-1026,P91-1028,0,\N,Missing
E93-1026,J92-3003,0,\N,Missing
E99-1046,P92-1032,0,0.0929294,"Missing"
E99-1046,P96-1006,0,0.101021,"Missing"
erjavec-etal-2000-concede,P98-1050,1,\N,Missing
erjavec-etal-2000-concede,C98-1049,1,\N,Missing
I05-3007,O98-3004,1,0.821376,"inese VerbNet Project: that ren.wei ᇡࣁ‘to quotation. Yi.wei and jue.de, on the other hand, can only be used in reportage and cannot think’ behaves most like biao.shi ߄ Ң ‘to introduce direct quotation. express, to state’ (salience 0.451), while yi.wei а Distinction between near synonymous pairs ࣁ ‘to take somebody/something as’ is more like can be obtained from Sketch Difference. This jue.de ள ‘to feel, think’ (salience 0.488). The function is verified with results from Tsai et al.’s study on gao.xing ଯᑫ ‘glad’ and kuai.le ז! synonymous relation can be illustrated by (4) and (5). ‘happy’ (Tsai et al., 1998). Gao.xing ‘glad’ 4a. дᇡࣁੇډѦၗԖঁᢀࡐۺख़ाǴ൩ा specific patterns include the negative imperative bie ձ ‘don’t’. It also has a dominant collocation ޕၰӦޑၯᔍೕ߾Ǵௗڙ೭٤చҹǶ ta ren.wei dao hai.wai tou.zi you yi ge guan.nian with the potentiality complement marker de ள hen zhong.yao, jiu shi yao zhi.dao dang.di de (e.g. ta gao.xing de you jiao you tiao Ӵଯᑫள you.xi gui.ze ΞћΞၢ ‘she was so happy that she cried and ‘He believes that for those investing overseas, danced’). In contrast, kuai.le ‘happy’ has the there is a very important principle-one must know specific collocation with holiday nouns s"
I05-3007,xia-etal-2000-developing,0,0.0096387,"n a list of types and distribution of the keyword’s syntactic category. In addition, users can find possible collocations of the keyword from the output of Mutual Information (MI). The most salient grammatical information, such as grammatical functions (subject, object, adjunct etc.) is beyond the scope of the traditional corpus interface tools. Traditional corpora rely on the human users to arrive at these kinds of generalizations. 3. Sketch Engine: A New Corpus-based approach to Grammatical Information Several existing linguistically annotated corpus of Chinese, e.g. Penn Chinese Tree Bank (Xia et al., 2000), Sinica Treebank (Chen et al., 2003), Proposition Bank (Xue and Palmer, 2003, 2005) and Mandarin VerbNet (Wu and Liu, 50 Linguistic Knowledge Net anchored by a lexicon (Huang et al., 2001). A Word Sketch is a one-page list of a Information (MI) to measure the salience of a keyword’s functional distribution and collocation Engine output. MI provides a measure of the in the corpus. The functional distribution degree of association of a given segment with includes: subject, object, prepositional object, others. Pointwise MI, calculated by Equation 3, and modifier. Its collocations are described"
I05-3007,C90-2010,1,\N,Missing
I05-3007,O97-4003,1,\N,Missing
ivanova-etal-2008-evaluating,schulte-im-walde-2002-subcategorisation,1,\N,Missing
ivanova-etal-2008-evaluating,heid-weller-2008-tools,1,\N,Missing
ivanova-etal-2008-evaluating,C00-2105,1,\N,Missing
ivanova-etal-2008-evaluating,E99-1016,0,\N,Missing
ivanova-etal-2008-evaluating,E06-2001,1,\N,Missing
ivanova-etal-2008-evaluating,evert-2004-statistical,0,\N,Missing
J03-3001,P98-1013,0,0.101583,"Missing"
J03-3001,J93-2001,0,0.00867849,"ge general-language corpus or a relatively tiny corpus of the right kind of text? Nobody knows. There is currently no theory, no mathematical models, and almost no discussion. A related issue is that of porting an application from the sublanguage for which it was developed to another. It should be possible to use corpora for the two sublanguages to estimate how large a task this will be, but again, our understanding is in its infancy. 4.3 Language Modeling Much work in recent years has gone into developing language models. Clearly, the statistics for different types of text will be different (Biber 1993). This imposes a limitation on the applicability of any language model: We can be confident only that it predicts the behavior of language samples of the same text type as the trainingdata text type (and we can be entirely confident only if training and test samples are random samples from the same source). When a language technology application is put to use, it will be applied to new text for which we cannot guarantee the text type characteristics. There is little work on assessing how well one language model fares when applied to a text type that is different from that of the training corpu"
J03-3001,A97-1052,0,0.00575224,"ow translators can use “just-in-time” sublanguage corpora to choose correct target language terms for areas in which they are not expert. Fletcher (2002) demonstrates methods for gathering and using Web corpora in a language-teaching context. 2.2 The 100M Words of the BNC One hundred million words is a large enough corpus for many empirical strategies for learning about language, either for linguists and lexicographers (Baker, Fillmore, and Lowe 1998; Kilgarriff and Rundell 2002) or for technologies that need quantitative information about the behavior of words as input (most notably parsers [Briscoe and Carroll 1997; Korhonen 2000]). However, for some purposes, it is not large enough. This is an outcome of the Zipfian nature of word frequencies. Although 100 million is a huge number, and the BNC contains ample information on the dominant meanings and usage patterns for the 10,000 words that make up the core of English, the bulk of the lexical stock occurs less than 50 times in the BNC, which is not enough to draw statistically stable conclusions about the word. For rarer words, rare meanings of common words, and combinations of words, we frequently find no evidence at all. Researchers are obliged to look"
J03-3001,cavaglia-2002-measuring,0,0.0111588,"be replaced by the meeker balanced. 342 Kilgarriff and Grefenstette Web as Corpus: Introduction The recent history of mathematically sophisticated modeling of language variation begins with Biber (1988), who identifies and quantifies the linguistic features associated with different spoken and written text types. Habert and colleagues (Folch et al. 2000; Beaudouin et al. 2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others. In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work. As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al. (2000). Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation. A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain. Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures. Hi"
J03-3001,J93-1001,0,0.0626153,"r, but they were large, messy, ugly objects clearly lacking in theoretical integrity in all sorts of ways, and many people were skeptical regarding their role in the discipline. Arguments raged, and it was not clear whether corpus work was an acceptable 1 Across the Atlantic, a resurgence in empiricism was led by the success of the noisy-channel model in speech recognition (see Church and Mercer [1993] for references). 334 Kilgarriff and Grefenstette Web as Corpus: Introduction part of the field. It was only with the highly successful 1993 special issue of this journal, “Using Large Corpora” (Church and Mercer 1993), that the relation between computational linguistics and corpora was consummated. There are parallels with Web corpus work. The Web is anarchic, and its use is not in the familiar territory of computational linguistics. However, as students with no budget or contacts realize, it is the obvious place to obtain a corpus meeting their specifications, as companies want the research they sanction to be directly related to the language types they need to handle (almost always available on the Web), as copyright continues to constrain “traditional” corpus development,2 as people want to explore usin"
J03-3001,folch-etal-2000-typtex,0,0.092991,"—until then a promising research avenue but largely constrained to the English-French Canadian Hansard—could be found on the Web: We can grow our own parallel corpus using the many Web pages that exist in parallel in local and in major languages. We are glad to have the further development of this work (co-authored by Noah Smith) presented in this special issue. In the student session of ACL 2000, Rosie Jones and Rayid Ghani (2001) showed how, using the Web, one can build a language-specific corpus from a single document in that language. In the main session Atsushi Fujii and Tetsuya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be acquired from the Web. 2.1 Some Current Themes Since then there have been many papers, at ACL and elsewhere, and we can mention only a few. The EU MEANING project (Rigau et al. 2002) takes forward the exploration of the Web as a data source for word sense disambiguation, working from the premise that within a domain, words often have just one meaning, and that domains can be identified on the Web. Mihalcea and Tchklovski complement this use of Web as corpus with Web technology to gather manual word sense annotations on the Word"
J03-3001,P00-1062,0,0.0170144,"Missing"
J03-3001,W01-0521,0,0.458268,"f any language model: We can be confident only that it predicts the behavior of language samples of the same text type as the trainingdata text type (and we can be entirely confident only if training and test samples are random samples from the same source). When a language technology application is put to use, it will be applied to new text for which we cannot guarantee the text type characteristics. There is little work on assessing how well one language model fares when applied to a text type that is different from that of the training corpus. Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance 341 Computational Linguistics Volume 29, Number 3 Table 5 Hits for Spanish pensar que with and without possible “dequeismos errors” (spurious de between the verb and the relative), from Alltheweb.com (March 2003). Not all items are errors (e.g., “. . .pienso de que manera. . .” . . . think how. . .). The correct form is always at least 500 times more common than any potentially incorrect form. pienso de que pienso que piensas de que piensas que piense de que piense que pensar de que pensar que 388 356,874 173 84,896 92 67,243 1,64"
J03-3001,C94-2174,0,0.244456,"Missing"
J03-3001,P97-1005,0,0.435901,"icability of any language model: We can be confident only that it predicts the behavior of language samples of the same text type as the trainingdata text type (and we can be entirely confident only if training and test samples are random samples from the same source). When a language technology application is put to use, it will be applied to new text for which we cannot guarantee the text type characteristics. There is little work on assessing how well one language model fares when applied to a text type that is different from that of the training corpus. Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance 341 Computational Linguistics Volume 29, Number 3 Table 5 Hits for Spanish pensar que with and without possible “dequeismos errors” (spurious de between the verb and the relative), from Alltheweb.com (March 2003). Not all items are errors (e.g., “. . .pienso de que manera. . .” . . . think how. . .). The correct form is always at least 500 times more common than any potentially incorrect form. pienso de que pienso que piensas de que piensas que piense de que piense que pensar de que pensar que 388 356,874 173 84,"
J03-3001,W00-1327,0,0.00896643,"st-in-time” sublanguage corpora to choose correct target language terms for areas in which they are not expert. Fletcher (2002) demonstrates methods for gathering and using Web corpora in a language-teaching context. 2.2 The 100M Words of the BNC One hundred million words is a large enough corpus for many empirical strategies for learning about language, either for linguists and lexicographers (Baker, Fillmore, and Lowe 1998; Kilgarriff and Rundell 2002) or for technologies that need quantitative information about the behavior of words as input (most notably parsers [Briscoe and Carroll 1997; Korhonen 2000]). However, for some purposes, it is not large enough. This is an outcome of the Zipfian nature of word frequencies. Although 100 million is a huge number, and the BNC contains ample information on the dominant meanings and usage patterns for the 10,000 words that make up the core of English, the bulk of the lexical stock occurs less than 50 times in the BNC, which is not enough to draw statistically stable conclusions about the word. For rarer words, rare meanings of common words, and combinations of words, we frequently find no evidence at all. Researchers are obliged to look to larger data"
J03-3001,P99-1020,0,0.00521051,"Missing"
J03-3001,P99-1068,0,0.0212045,"Missing"
J03-3001,W02-1304,0,0.00864365,"anguages. We are glad to have the further development of this work (co-authored by Noah Smith) presented in this special issue. In the student session of ACL 2000, Rosie Jones and Rayid Ghani (2001) showed how, using the Web, one can build a language-specific corpus from a single document in that language. In the main session Atsushi Fujii and Tetsuya Ishikawa (2000) demonstrated that descriptive, definition-like collections can be acquired from the Web. 2.1 Some Current Themes Since then there have been many papers, at ACL and elsewhere, and we can mention only a few. The EU MEANING project (Rigau et al. 2002) takes forward the exploration of the Web as a data source for word sense disambiguation, working from the premise that within a domain, words often have just one meaning, and that domains can be identified on the Web. Mihalcea and Tchklovski complement this use of Web as corpus with Web technology to gather manual word sense annotations on the Word Expert Web site.3 Santamar´ia et al., in this issue, discuss how to link word senses to Web directory nodes, and thence to Web pages. The Web is being used to address data sparseness for language modeling. In addition to Keller and Lapata (this iss"
J03-3001,W00-0905,0,0.00876436,"ith Biber (1988), who identifies and quantifies the linguistic features associated with different spoken and written text types. Habert and colleagues (Folch et al. 2000; Beaudouin et al. 2001) have been developing a workstation for specifying subcorpora according to text type, using Biber-style analyses, among others. In Kilgarriff (2001) we present a first pass at quantifying similarity between corpora, and Cavaglia (2002) continues this line of work. As mentioned above, Sekine (1997) and Gildea (2001) directly address the relation between NLP systems and text type; one further such item is Roland et al. (2000). Buitelaar and Sacaleanu (2001) explores the relation between domain and sense disambiguation. A practical discussion of a central technical concern is Vossen (2001), which tailors a general-language resource for a domain. Baayen (2001) presents sophisticated mathematical models for word frequency distributions, and it is likely that his mixture models have potential for modeling sublanguage mixtures. His models have been developed with a specific, descriptive goal in mind and using a small number of short texts: It is unclear whether they can be usefully applied in NLP. Although the extensiv"
J03-3001,A97-1015,0,0.23865,"he applicability of any language model: We can be confident only that it predicts the behavior of language samples of the same text type as the trainingdata text type (and we can be entirely confident only if training and test samples are random samples from the same source). When a language technology application is put to use, it will be applied to new text for which we cannot guarantee the text type characteristics. There is little work on assessing how well one language model fares when applied to a text type that is different from that of the training corpus. Two studies in this area are Sekine (1997) and Gildea (2001), both of which show substantial variation in model performance 341 Computational Linguistics Volume 29, Number 3 Table 5 Hits for Spanish pensar que with and without possible “dequeismos errors” (spurious de between the verb and the relative), from Alltheweb.com (March 2003). Not all items are errors (e.g., “. . .pienso de que manera. . .” . . . think how. . .). The correct form is always at least 500 times more common than any potentially incorrect form. pienso de que pienso que piensas de que piensas que piense de que piense que pensar de que pensar que 388 356,874 173 84,"
J03-3001,C98-1013,0,\N,Missing
J03-3001,P01-1005,0,\N,Missing
J07-1010,E06-2001,1,0.826922,"at search engines provide in their hits pages face another issue: The hits are sorted according to a complex and unknown algorithm (with full listings of all results usually not permitted) so we do not know what biases are being introduced. If we wish to investigate the biases, the area we become expert in is googleology, not linguistics. An Academic-Community Alternative An alternative is to work like the search engines, downloading and indexing substantial proportions of the World Wide Web, but to do so transparently, giving reliable figures, and supporting language researchers’ queries. In Baroni and Kilgarriff (2006) we report on a feasibility study: We prepared Web corpora for German (‘DeWaC’) and Italian (‘ItWaC’) with around 1.5 billion words each, now loaded into a sophisticated corpus query tool and available for research use.2 (Of course there are various other large Web datasets that research groups have downloaded and are using for NLP.) By sharing good practice and resources and developing expertise, the prospects of the academic research community having resources to compare with Google, Microsoft, and so forth, improves. Data Cleaning The process involves crawling, downloading, ’cleaning’, and"
J07-1010,J03-3005,0,0.0193578,"generation benefit from big data, so it becomes appealing to use the Web as a data source. The question, then, is how. The low-entry-cost way to use the Web is via a commercial search engine. If the goal is to find frequencies or probabilities for some phenomenon of interest, we can use the hit count given in the search engine’s hits page to make an estimate. People have been doing this for some time now. Early work using hit counts include Grefenstette (1999), who identified likely translations for compositional phrases, and Turney (2001), who found synonyms; perhaps the most cited study is Keller and Lapata (2003), who established the validity of frequencies gathered in this way using experiments with human subjects. Leading recent work includes Nakov and Hearst (2005), who build models of noun compound bracketing. The initial-entry cost for this kind of research is zero. Given a computer and an Internet connection, you input the query and get a hit count. But if the work is to proceed beyond the anecdotal, a range of issues must be addressed. First, the commercial search engines do not lemmatize or part-of-speech tag. To take a simple case: To estimate frequencies for the verb-object pair fulfil oblig"
J07-1010,W05-0603,0,0.0282043,"Missing"
J07-1010,P05-1077,0,0.0360624,"Missing"
J07-1010,P06-1101,0,0.033849,"Missing"
kilgarriff-etal-2010-corpus,E06-2001,1,\N,Missing
kilgarriff-etal-2010-corpus,J03-3005,0,\N,Missing
kilgarriff-etal-2010-corpus,P99-1068,0,\N,Missing
kilgarriff-etal-2010-corpus,ide-etal-2002-american,0,\N,Missing
kilgarriff-etal-2010-corpus,baroni-bernardini-2004-bootcat,0,\N,Missing
kilgarriff-etal-2010-corpus,pomikalek-rychly-2008-detecting,1,\N,Missing
kilgarriff-etal-2014-extrinsic,nivre-etal-2006-maltparser,0,\N,Missing
kilgarriff-etal-2014-extrinsic,W07-1709,0,\N,Missing
kilgarriff-etal-2014-extrinsic,H05-1066,0,\N,Missing
kilgarriff-etal-2014-extrinsic,ambati-etal-2012-word,1,\N,Missing
kilgarriff-yallop-2000-whats,J96-3009,0,\N,Missing
kilgarriff-yallop-2000-whats,C92-4196,0,\N,Missing
kilgarriff-yallop-2000-whats,P98-1046,0,\N,Missing
kilgarriff-yallop-2000-whats,C98-1046,0,\N,Missing
kilgarriff-yallop-2000-whats,P90-1034,0,\N,Missing
kilgarriff-yallop-2000-whats,J98-1004,0,\N,Missing
kilgarriff-yallop-2000-whats,P98-2127,0,\N,Missing
kilgarriff-yallop-2000-whats,C98-2122,0,\N,Missing
N10-2006,C94-2149,0,0.137796,"urate description of the language; and it has been led by a team with singular experience in delivering high-quality and innovative resources. The lexicon will be complete in Summer 2010 and will be available for NLP groups, on terms designed to encourage its research use. • quality and consistency • level of detail • number of examples 1 Introduction • accountability to the corpus Most NLP applications need lexicons. NLP researchers have used databases from dictionary publishers (Boguraev and Briscoe, 1989; Wilks et al., 1996), or developed NLP resources (COMLEX (Macleod et al., 1994), XTAG (Doran et al., 1994)) or used WordNet,(Fellbaum, 1998) or have switched to fully corpus-based strategies which need no lexicons. However the publishers’ dictionaries were pre-corpus, often inconsistent, and licencing constraints were in the end fatal. COMLEX and XTAG address only syntax; WordNet, only semantics. Also these resources were not produced by experienced lexicographers, nor according to a detailed, stringent ‘style guide’ specifying how to handle all the phenomena (in orthography, morphology, syntax, semantics and pragmatics, from spelling variation to • purity: it has been created only as an analysis"
N10-2006,H94-1003,0,0.122335,"t any goals other than an accurate description of the language; and it has been led by a team with singular experience in delivering high-quality and innovative resources. The lexicon will be complete in Summer 2010 and will be available for NLP groups, on terms designed to encourage its research use. • quality and consistency • level of detail • number of examples 1 Introduction • accountability to the corpus Most NLP applications need lexicons. NLP researchers have used databases from dictionary publishers (Boguraev and Briscoe, 1989; Wilks et al., 1996), or developed NLP resources (COMLEX (Macleod et al., 1994), XTAG (Doran et al., 1994)) or used WordNet,(Fellbaum, 1998) or have switched to fully corpus-based strategies which need no lexicons. However the publishers’ dictionaries were pre-corpus, often inconsistent, and licencing constraints were in the end fatal. COMLEX and XTAG address only syntax; WordNet, only semantics. Also these resources were not produced by experienced lexicographers, nor according to a detailed, stringent ‘style guide’ specifying how to handle all the phenomena (in orthography, morphology, syntax, semantics and pragmatics, from spelling variation to • purity: it has been c"
N10-2006,J96-3009,0,0.0264287,"ry-editing technology; it has not been constrained to meet any goals other than an accurate description of the language; and it has been led by a team with singular experience in delivering high-quality and innovative resources. The lexicon will be complete in Summer 2010 and will be available for NLP groups, on terms designed to encourage its research use. • quality and consistency • level of detail • number of examples 1 Introduction • accountability to the corpus Most NLP applications need lexicons. NLP researchers have used databases from dictionary publishers (Boguraev and Briscoe, 1989; Wilks et al., 1996), or developed NLP resources (COMLEX (Macleod et al., 1994), XTAG (Doran et al., 1994)) or used WordNet,(Fellbaum, 1998) or have switched to fully corpus-based strategies which need no lexicons. However the publishers’ dictionaries were pre-corpus, often inconsistent, and licencing constraints were in the end fatal. COMLEX and XTAG address only syntax; WordNet, only semantics. Also these resources were not produced by experienced lexicographers, nor according to a detailed, stringent ‘style guide’ specifying how to handle all the phenomena (in orthography, morphology, syntax, semantics and pra"
P07-2011,P06-1046,0,0.0697381,"oise (of words that just happen to share a few contexts). Lin’s was based on around 300M words and (Curran, 2004) used 2B (billion). A direct approach to thesaurus computation looks at each word and compares it with each other word, checking all contexts to see if they are shared. Thus, complexity is O(n2 m) where n in the number of types and m is the size of the context vector. The number of types increases with the corpus size, and (Ravichandran et al., 2005) propose heuristics for thesaurus building without undertaking the complete calculation. The line of reasoning is explored further by (Gorman and Curran, 2006), who argue that the complete calculation is not realistic given large corpora. They estimate that, given a 2B corpus and its 184,494-word vocabulary comprising all words occurring over five times, the full calculation will take nearly 300 days. With the vocabulary limited to the 75,800 words occuring over 100 times, the calculation took 18 days. The naive algorithm has complexity O(n2 m) but this is not the complexity of the problem. Most of Proceedings of the ACL 2007 Demo and Poster Sessions, pages 41–44, c Prague, June 2007. 2007 Association for Computational Linguistics the n2 word pairs"
P07-2011,P98-2127,0,0.184199,"Missing"
P07-2011,P05-1077,0,0.0172286,"Missing"
P07-2011,J05-4002,0,0.105921,"plural. Is this a salient lexical fact? To form a judgement, we need to know the distribution for all nouns. We use histograms to present the distribution in a way that is easy to grasp. 1 Thesaurus creation Over the last ten years, interest has been growing in distributional thesauruses (hereafter simply ’thesauruses’). Following initial work by (Sp¨arck Jones, 1964) and (Grefenstette, 1994), an early, online distributional thesaurus presented in (Lin, 1998) has been widely used and cited, and numerous authors since have explored thesaurus properties and parameters: see survey component of (Weeds and Weir, 2005). 41 Adam Kilgarriff Lexical Computing Ltd Brighton, UK adam@lexmasterclass.com A thesaurus is created by • taking a corpus • identifying contexts for each word • identifying which words share contexts. For each word, the words that share most contexts (according to some statistic which also takes account of their frequency) are its nearest neighbours. Thesauruses generally improve in accuracy with corpus size. The larger the corpus, the more clearly the signal (of similar words) will be distinguished from the noise (of words that just happen to share a few contexts). Lin’s was based on around"
P07-2011,C00-1027,0,\N,Missing
P07-2011,W00-0902,0,\N,Missing
P07-2011,E06-2001,1,\N,Missing
P07-2011,C98-2122,0,\N,Missing
S01-1004,E99-1046,1,\N,Missing
S01-1037,P89-1010,0,0.0761223,"a page of data such as Table 2, which shows, for the word in question (W1), ordered lists of high-salience grammatical relations, relationW2 pairs, and relation-W2-Prep triples for the word. The number of patterns shown is set by the user, but will typically be over 200. These are listed for each relation in order of salience, with the count of corpus instances. The instances can be instantly retrieved and shown in a concordance window. Producing a word sketch for a medium-to-high frequency word takes in the order of ten seconds. Salience is calculated as the product of Mutual Information I (Church and Hanks, 1989) and log frequency. I for a word W1 in a grammatical relation Rel 5 with a second word W2 is calculated as: 5 {Grammatical-relation, preposition} pairs treated as atomic relations in calculating MI. IWl,Re!,*llxii*,Re!,W21 The notation here is adopted from (Lin, 1998) (who also spells out the derivation from the definition of I). IIW1, Rel, W2ll denotes the frequency count of the triple {W1, Rel, W2} 6 in the grammatical relations database. Where W1, Rel or W2 is the wild card (*), the frequency is of all the dependency triples that match the remainder of the pattern. The word sketches are pr"
S01-1037,P98-2127,0,0.0092859,"listed for each relation in order of salience, with the count of corpus instances. The instances can be instantly retrieved and shown in a concordance window. Producing a word sketch for a medium-to-high frequency word takes in the order of ten seconds. Salience is calculated as the product of Mutual Information I (Church and Hanks, 1989) and log frequency. I for a word W1 in a grammatical relation Rel 5 with a second word W2 is calculated as: 5 {Grammatical-relation, preposition} pairs treated as atomic relations in calculating MI. IWl,Re!,*llxii*,Re!,W21 The notation here is adopted from (Lin, 1998) (who also spells out the derivation from the definition of I). IIW1, Rel, W2ll denotes the frequency count of the triple {W1, Rel, W2} 6 in the grammatical relations database. Where W1, Rel or W2 is the wild card (*), the frequency is of all the dependency triples that match the remainder of the pattern. The word sketches are presented to the user as a list of relations, with items in each list ordered according to salience. Our experience of working lexicographers' use of Mutual Information or log-likelihood lists shows that, for lexicographic purposes, these over-emphasise low frequency ite"
S01-1037,H93-1052,0,0.159653,"that, for lexicographic purposes, these over-emphasise low frequency items, and that multiplying by log frequency is an appropriate adjustment. Table 1: Grammatical Relations 2.2 ' are 152 Matching patterns with senses The next task is to enter a preliminary list of senses for the word, possibly in the form of some arbitrary mnemonics: for example, MONEY, CLOUD and RIVER for three senses of bank. 7 This inventory may be drawn from the user's knowledge, from a perusal of the word sketch, or from a pre-existing dictionary entry. As Table 2 shows, and in keeping with &quot;one sense per collocation&quot; (Yarowsky, 1993) in most cases, high-salience patterns or clues indicate just one of the word's senses. The user then has the task of associating, by selecting from a pop-up menu, the required sense for unambiguous clues. The number of relations marked will depend on the time available, as well as the complexity of the sense division to be made. The act of assigning senses to patterns may very well lead the user to discover fresh, unconsidered senses usages of the word. The pattern-sense associations are then submitted to the next stage: automatic disambiguation. 2.4 The Disambiguation Algorithm The workbench"
S01-1037,P95-1026,0,0.266974,"ust one of the word's senses. The user then has the task of associating, by selecting from a pop-up menu, the required sense for unambiguous clues. The number of relations marked will depend on the time available, as well as the complexity of the sense division to be made. The act of assigning senses to patterns may very well lead the user to discover fresh, unconsidered senses usages of the word. The pattern-sense associations are then submitted to the next stage: automatic disambiguation. 2.4 The Disambiguation Algorithm The workbench currently uses Yarowsky's decision list approach to WSD (Yarowsky, 1995). This is a bootstrapping algorithm that, given strictly, of the quintuple {Wl, Rel - part 1, W2, Rel- part- 2, ANY}. • 7 W:ASP-Bench can also be used for Machine Translation lexicography, where arbitrary mnemonics would be replaced by target language translations. 6 0r, I subj-of lend issue charge operate modifies holiday account loan lending num 95 60 29 45 sal 21.2 11.8 9.5 8.9 404 503 108 68 32.6 32.0 27.5 26.1 I obj-of burst rob overflow line pp of England of Scotland of river of Thames num 27 31 7 13 sal 16.4 15.3 10.2 8.4 988 242 111 41 37.5 26.9 22.1 20.1 I modifier central Swiss comme"
S01-1037,C98-2122,0,\N,Missing
S15-2053,W99-0201,0,0.0608554,"across subtasks is the way Precision and Recall are defined. F1verb = 2 × Precisionverb × Recallverb Precisionverb + Recallverb Pnverb F1verbi ScoreTask = i=1 nverb (1) Subtask 1. Equation 2 illustrates that Precision and Recall are computed on all tags, both syntactic and semantic. To count as correct, tags had to be set on the same token as in the gold standard. Correct tags Retrieved tags Correct tags Recall = Reference tags Precision = (2) Subtask 2. Clustering is known to be difficult to evaluate. Subtask 2 used the B-cubed definition of Precision and Recall, first used for coreference (Bagga and Baldwin, 1999) and later extended to cluster evaluation (Amig´o et al., 2009). Both measures are averages of the precision and recall over all instances. To calculate the precision of each instance we count all correct pairs associated with this instance and divide by the number of actual pairs in the candidate cluster that the instance belongs to. Recall is computed by interchanging Gold and Candidate clusterings (Eq. 3). Pairsi in Candidate found in Gold Pairsi in Candidate Pairsi in Gold found in Candidate Recalli = Pairsi in Gold (3) Precisioni = Subtask 3. This task was evaluated as a slot-filling exer"
S15-2053,S07-1018,0,0.0786767,"Missing"
S15-2053,W13-3821,1,0.796383,"Missing"
S15-2053,W06-2920,0,0.0206406,"er resources. This also implied that the dataset would be constructed so as to make it possible for systems to generalize from the behaviour and description of one set of verbs to a set of unseen verbs used in similar structures, as human language learners do. Although this obviously makes the task harder, it was hoped that this would put us in a better position to evaluate current limits of automatic semantic analysis. 3.1 arguments of the verb. The subtask is similar to Semantic Role Labelling (Carreras and Marquez, 2004) that arguments will be identified in the dependency parsing paradigm (Buchholz and Marsi, 2006), using head words instead of phrases. The syntactic tagset was designed specially for this subtask and kept to a minimum, and the semantic tagset was based on the CPA Semantic Ontology. In Example (1), this would mean identifying government as subject of abolish, from the [[Institution]] type, and tax as object belonging to [[Rule]]. The expected output is represented in XML format in Example (2). (1) In 1981 the Conservative government abolished capital transfer tax capital transfer tax and replaced it with inheritance tax. (2) In 1981 the Conservative &lt;entity syn=‘subj’ sem=‘Institution’&gt; g"
S15-2053,W04-2412,0,0.023445,"approaches, maybe using patterns learnt in an unsupervised manner from very large corpora and other resources. This also implied that the dataset would be constructed so as to make it possible for systems to generalize from the behaviour and description of one set of verbs to a set of unseen verbs used in similar structures, as human language learners do. Although this obviously makes the task harder, it was hoped that this would put us in a better position to evaluate current limits of automatic semantic analysis. 3.1 arguments of the verb. The subtask is similar to Semantic Role Labelling (Carreras and Marquez, 2004) that arguments will be identified in the dependency parsing paradigm (Buchholz and Marsi, 2006), using head words instead of phrases. The syntactic tagset was designed specially for this subtask and kept to a minimum, and the semantic tagset was based on the CPA Semantic Ontology. In Example (1), this would mean identifying government as subject of abolish, from the [[Institution]] type, and tax as object belonging to [[Rule]]. The expected output is represented in XML format in Example (2). (1) In 1981 the Conservative government abolished capital transfer tax capital transfer tax and replac"
S15-2053,P05-1022,0,0.0882373,"old 1,008 777 580 438 308 303 289 192 182 115 CMILLS 0.564 0.659 0.593 0.450 0.545 0.668 0.621 0.410 0.441 0.421 FANTASY 0.694 0.792 0.770 0.479 0.418 0.830 0.517 0.276 0.531 0.594 BLCUNLP 0.739 0.777 0.691 0.393 0.702 0.771 0.736 0.373 0.483 0.526 baseline 0.815 0.783 0.724 0.408 0.729 0.811 0.845 0.211 0.461 0.506 Table 6: Detailed scores for subtask 1 (10 most frequent categories). Team baseline FANTASY BLCUNLP CMILLS Score 0.624 0.589 0.530 0.516 representations to predict the output of each layer. The baseline system was a rule-based system taking as input the output of the BLLIP parser (Charniak and Johnson, 2005), and mapping heads of relevant dependency relations to the most probable tags from subtask 1 tagset. The semantic tags were only then added to those headwords based on the most frequent semantic category found in the training set. Table 5: Official scores for subtask 1. subtask allowed it, some systems used external resources such as Wordnet or larger corpora. BLCUNLP (Feng et al., 2015) used the Stanford CoreNLP package10 to get POS, NE and basic dependency features. These features were used to predict both syntax and semantic information. The method did not involve the use of a statistical"
S15-2053,E12-1085,1,0.868894,"Missing"
S15-2053,cinkova-etal-2012-database,1,0.891442,"Missing"
S15-2053,W13-3826,1,0.504171,"Missing"
S15-2053,el-maarouf-etal-2014-disambiguating,1,0.46533,"Missing"
S15-2053,S15-2054,0,0.493221,"e FANTASY BLCUNLP CMILLS Score 0.624 0.589 0.530 0.516 representations to predict the output of each layer. The baseline system was a rule-based system taking as input the output of the BLLIP parser (Charniak and Johnson, 2005), and mapping heads of relevant dependency relations to the most probable tags from subtask 1 tagset. The semantic tags were only then added to those headwords based on the most frequent semantic category found in the training set. Table 5: Official scores for subtask 1. subtask allowed it, some systems used external resources such as Wordnet or larger corpora. BLCUNLP (Feng et al., 2015) used the Stanford CoreNLP package10 to get POS, NE and basic dependency features. These features were used to predict both syntax and semantic information. The method did not involve the use of a statistical classifier. CMILLS (Mills and Levow, 2015) used three models to solve the task: one for argument detection, and the other two for each layer. Argument detection and syntactic tagging were performed using a MaxEnt supervised classifier, while the last was based on heuristics. CMILLS also reported the use of an external resource, the enTentTen12 (Jakub´ıcˇ ek et al., 2013) corpus available"
S15-2053,J02-3001,0,0.262472,"to automated reasoning. Since its birth, SEMEVAL (or Most lexical resources explored to date have had only limited success, on either front. The most obvious candidates—published dictionaries and WordNets—look like they might support the first task, but are very limited in what they offer to the second. FrameNet moved the game forward a stage. Here was a framework with a convincing account of how the lexical entry might contribute to building the meaning of the sentence, and with enough meat in the lexical entries (e.g. the verb frames) so that it might support disambiguation. Papers such as (Gildea and Jurafsky, 2002) looked promising, and in 2007 there was a SEMEVAL task on Frame Semantic Structure Extraction (Baker et al., 2007) and in 2010, one on Linking Events and Their Participants (Ruppenhofer et al., 2010). While there has been a substantial amount of follow-up work, there are some aspects of FrameNet that make it a hard target. • It is organised around frames, rather than words, so inevitably its priority is to give a co315 Proceedings of the 9th International Workshop on Semantic Evaluation (SemEval 2015), pages 315–324, c Denver, Colorado, June 4-5, 2015. 2015 Association for Computational Lingu"
S15-2053,C12-1073,1,0.909639,"Missing"
S15-2053,S15-2076,0,0.0972218,"in a supervised setting to predict first the syntactic tags, and then the semantic tags. The team used features from the MST parser11 , as well as Stanford CoreNLP for NE, Wordnet12 , they also applied word embedding 10 http://nlp.stanford.edu/software/ corenlp.shtml 11 http://www.seas.upenn.edu/˜strctlrn/ MSTParser/MSTParser.html 12 http://wordnet.princeton.edu/ 5.2 Subtask 2 As opposed to subtask 1, systems in subtask 2 used very few semantic and syntactic resources. BOB90 used a supervised approach to tackle the clustering problem. The main features used were preposition analyses. DULUTH (Pedersen, 2015) used an unsupervised approach and focused on lexical similarity (both first and second order representations) based on unigrams and bigrams (see SenseClusters13 ). The number of clusters was predicted on the basis of the best value for the clustering criterion function. The team also performed some corpus pre-processing, like conversion to lower case and conversion of all numeric values to a string. The baseline system clusters everything together, so its score depends on the distribution of patterns: the more a pattern covers all instances of the data (majority class), the higher the baselin"
S15-2053,W13-0117,1,0.550483,"Missing"
S15-2053,W04-1908,0,0.0809889,"Missing"
S15-2053,C04-1133,0,\N,Missing
temnikova-etal-2014-sublanguage,R13-1086,1,\N,Missing
temnikova-etal-2014-sublanguage,W13-1909,1,\N,Missing
temnikova-etal-2014-sublanguage,R13-1087,1,\N,Missing
temnikova-etal-2014-sublanguage,J07-1004,0,\N,Missing
W03-2202,H93-1052,0,\N,Missing
W03-2202,P95-1026,0,\N,Missing
W03-2202,P98-2127,0,\N,Missing
W03-2202,C98-2122,0,\N,Missing
W03-2808,J96-2004,0,0.0443743,"Missing"
W03-2808,W03-2202,1,0.812728,"uld introduce further biases). So it is a more complex and costly form of evaluation. However it is also far more closely related to a real task. It is a direction that Senseval needs to take.2 The MUCstyle xed-sense-inventory should be seen as what was necessary to open the chapter on WSD evaluation: a graspable, manageable task when we had no experience of the dif culties we might encounter, which also provided researchers with some objective datasets for their development work. For the future the 2 It is also the route we have taken in the WASPS project, which is geared towards WSD for MT (Koeling et al., 2003). emphasis needs to be on assessments such as the Japanese one, related to real tasks. 5 Metric re-use: kappa Consider the ( ctional) game show Couples&quot;. The idea is to establish which couples share the same world view to the greatest extent. Each member of the couple is put in a space where they cannot hear what the other is saying, and is then asked twenty multiple-choice questions like What is the greatest UK pop group of the 1960s? The Beatles/The Rolling Stones or Which month is your oldest nephew/niece&apos;s birthday? Jan/Feb/Mar/Apr/May/Jun/Jul /Aug/Sep/Oct/Nov/Dec /Nonephew-or-niece The c"
W03-2808,S01-1009,0,0.0143676,"Translation evaluation As noted above, overall Senseval design is taken from MUC. We have also followed MUC and TREC discussions of the hub-and-spokes model and the need to forever look towards updating the task, to guard against participants becoming expert at the task as de ned but not at anything else. WSD is not a task of interest in itself. One does WSD in order to improve performance on some other task. The critical end-to-end task, for WSD, is Machine Translation (Kilgarri , 1997). In Senseval-2, for Japanese there was a translation memory task, which took the form of an MT evaluation (Kurohashi, 2001). In that experimental design, each system response potentially requires individual attention from a human assessor. As in assessing human or computer translation, one cannot specify a complete set of correct answers ahead of time, so one must be open to the possibility that the system response is correct but di erent from all the responses seen to date. Thus the exercise is potentially far more expensive than the MUC model. In the MUC model, human attention is required for each data instance. In this model, human attention is potentially required for each datainstance/system combination. Anot"
W04-0807,J96-2004,0,0.0387814,"Missing"
W04-0807,W02-0817,1,0.693609,"Sense Disambiguation. This task is a follow-up to similar tasks organized during the S ENSEVAL -1 (Kilgarriff and Palmer, 2000) and S ENSEVAL -2 (Preiss and Yarowsky, 2001) evaluations. The main changes in this year’s evaluation consist of a new methodology for collecting annotated data (with contributions from Web users, as opposed to trained lexicographers), and a new sense inventory used for verb entries (Wordsmyth). 2 Building a Sense Tagged Corpus with Volunteer Contributions over the Web The sense annotated corpus required for this task was built using the Open Mind Word Expert system (Chklovski and Mihalcea, 2002) 1 . To overcome the current lack of sense tagged data and the limitations imposed by the creation of such data using trained lexicographers, the OMWE system enables the collection of semantically annotated corpora over the Web. Sense tagged examples are collected using 1 Open Mind Word Expert can be accessed at http://teachcomputers.org/ a Web-based application that allows contributors to annotate words with their meanings. The tagging exercise proceeds as follows. For each target word the system extracts a set of sentences from a large textual corpus. These examples are presented to the cont"
W06-1421,E06-1040,1,0.880254,"on of results, and by isolation from the rest of NLP where STE is now standard. It is, moreover, a shrinking field (state-of-the-art MT and summarisation no longer perform generation as a subtask) which lacks the kind of funding and participation that natural language understanding (NLU) has attracted. Evidence from other NLP fields shows that STE campaigns (STECs) can lead to rapid technological progress and substantially increased participation. The past year has seen a groundswell of interest in comparative evaluation among NLG researchers, the first comparative results are being reported (Belz and Reiter, 2006), and the move towards some form of comparative evaluation seems inevitable. In this paper we look at how two decades of NLP STECs might help us decide how best to make this move. Evaluation: NLP STECs have tended to use automatic evaluations because of their speed and reproducibility, but some have used human evaluators, in particular in fields where language is generated (MT, summarisation, speech synthesis). Evaluation scores are not independent of the task and context for which they are calculated. This is clearly true of human-based evaluation, but even scores by a simple metric like word"
W06-1421,W03-2808,1,0.836059,"to consolidate and progress collectively. Conforming to the evaluation paradigm now common to the rest of NLP will also help re-integration, and open up the field to new researchers. Sharing: As PARSEVAL shows, measures and resources alone are not enough. Also required are (i) an event (or better, cycle of events) so people can attend and feel part of a community; (ii) a forum for reviewing task definitions and evaluation methods; (iii) a committee which ‘owns’ the STEC, and organises the next campaign. Funding is usually needed for gold-standard corpus creation but rarely for anything else (Kilgarriff, 2003). Participants can be expected to cover the cost of system development and workshop attendance. A funded project is best seen as supporting and enabling the STEC (especially during the early stages) rather than being it. In sum, STECs are good for community building. They produce energy (as we saw when the possibility was raised for NLG at UCNLG’05 and ENLG ’05) which can lead to rapid scientific and technological progress. They make the field look like a game and draw people in. Tasks: In defining sharable tasks with associated data resources for NLG, the core problem is deciding what inputs"
W06-1421,W06-1422,1,0.715035,"evaluation (as in Morpholympics). An alternative is to approach the issue through tasks with inputs and outputs that ‘occur naturally’, so that participants can use their own NLG-specific representations. Examples include data-to-text mappings where e.g. time-series data or a data repository are mapped to fault reports, forecasts, etc. Both data-independent task definitions and tasks with naturally occurring data have promise, but we propose the second as the simpler, easier to organise solution, at least initially. A specific proposal of a set of tasks can be found elsewhere in this volume (Reiter and Belz, 2006). An interesting idea (recommended by ELRA / ELDA) is to break down the input-output mapping into stages (as in the TC - STAR workshops, see table) and then, in a second round of evaluations, to make available intermediate representations from the most successful systems from the first round. In this way, standardised representations might develop almost as a side-effect of STECs. (Belz and Reiter, 2006), and there is a lot of scepticism among NLG researchers regarding automatic evaluation. We believe that NLG should develop its own automatic metrics (development of such metrics is part of the"
W06-1705,W03-0806,0,0.0175518,"Rundell). They have also served as the starting point for high-accuracy Word Sense Disambiguation. More recently, the Sketch Engine was used to develop the new edition of the Oxford Thesaurus of English (2004, edited by Maurice Waite). Parallelising or distributing processing has been suggested before. Clark and Curran’s (2004) work is in parallelising an implementation of log-linear parsing on the Wall Street Journal Corpus, whereas we focus on part-of-speech tagging of a far larger and more varied web corpus, a technique more widely considered a prerequisite for corpus linguistics research. Curran (2003) 9 http://pie.usna.edu/ 29 suggested distributed processing in terms of web services but only to “allow components developed by different researchers in different locations to be composed to build larger systems” and not for parallel processing. Most significantly, previous investigations have not examined three essential questions: how to apply distributed techniques to vast quantities of corpus data derived from the web, how to ensure that web-derived corpora are representative, and how to provide verifiability and replicability. These core foci of our work represent crucial innovations lack"
W06-1705,baroni-bernardini-2004-bootcat,0,0.0396504,"vided by existing single-server systems. This corpus annotation bottleneck becomes even more problematic for voluminous data sets drawn from the web. The use of the web as a corpus for teaching and research on language has been proposed a number of times (Kilgarriff, 2001; Robb, 2003; Rundell, 2000; Fletcher, 2001, 2004b) and received a special issue of the journal Computational Linguistics (Kilgarriff and Grefenstette, 2003). Studies have used several different methods to mine web data. Turney (2001) extracts word co-occurrence probabilities from unlabelled text collected from a web crawler. Baroni and Bernardini (2004) built a corpus by iteratively searching Google for a small set of seed terms. Prototypes of Internet search engines for linguists, corpus linguists and lexicographers have been proposed: WebCorp (Kehoe and Renouf, 2002), KWiCFinder (Fletcher, 2004a) and the Linguist’s Search Engine (Kilgarriff, 2003; Resnik and Elkiss, 2003). A key concern in corpus linguistics and related disciplines is verifiability and replicability of the results of studies. Word frequency counts in internet search engines are inconsistent and unreliable (Veronis, 2005). Tools based on static corpora do not suffer from th"
W06-1705,W02-1030,0,0.0230867,"rds) to annotate • Crawler based corpus annotation - more general web based corpus annotation in which crawlers are used to locate web pages From a computational linguistic view, the framework will also need to take into account the granularity of the unit (for example, POS tagging requires sentence-units, but anaphoric annotation needs paragraphs or larger). Secondly, we need to investigate techniques for identifying identical documents, virtually identical documents and highly repetitive documents, such as those pioneered by Fletcher (2004b) and shingling techniques described by Chakrabarti (2002). The second stage of our work will involve implementing the framework within a P2P environment. We have already developed a prototype of an object-oriented application environment to support P2P system development using JXTA (Sun's P2P API). We have designed this environment so that specific application functionality Research hypothesis and aims Our research hypothesis is that distributed computational techniques can alleviate the annotation bottleneck for processing corpus data from the web. This leads us to a number of research questions: • How can corpus data from the web be divided into u"
W06-1705,J03-3001,1,0.845748,"ural language processing where models of sparse data are built. The motivation for increasingly large data sets remains the same. Due to the Zipfian nature of word frequencies, around half the word types in a corpus occur only once, so tremendous increases in corpus size are required both to ensure inclusion of essential word and phrase types and to increase the chances of multiple occurrences of a given type. In corpus linguistics building such megacorpora is beyond the scope of individual researchers, and they are not easily accessible (Kennedy, 1998: 56) unless the web is used as a corpus (Kilgarriff and Grefenstette, 2003). Increasingly, corpus researchers are tapping the Web to overcome the sparse data problem (Keller et al., 2002). This topic generated intense interest at workshops held at the University of Heidelberg (October 2004), University of Bologna (January 2005), University of Birmingham (July 2005) and now in Trento in April 2006. In addition, the advantages of using linguistically annotated data over raw data are well documented (Mair, 2005; Granger and Rayson, 1998). As the size of a corpus increases, a near linear increase in computing power is required to annotate the text. Although processing po"
W06-1705,W04-0858,0,\N,Missing
W06-1705,P04-1014,0,\N,Missing
W10-4236,bird-etal-2008-acl,1,0.781642,"aring their conference and journal submissions. They will have the skills and motivation to integrate the use of prototypes into their paper-writing. 2 See the Microsoft ESL Assistant at http://www.eslassistant.com as an embodiment of a similar idea. 2.2 The ACL Anthology Over a number of years, the ACL has sponsored the ongoing development of the ACL Anthology, a large collection of papers in the domain of computational linguistics. This provides an excellent source for the construction of language models for the task described here. The more recently-prepared ACL Anthology Reference Corpus (Bird et al., 2008), in which 10,921 of the Anthology texts (around 40 million words) have been made available in plain text form, has also been made accessible via the Sketch Engine, a leading corpus query tool.3 The corpus is not perfect, of course: not everything in the ACL Anthology is written in flawless English; the ARC was prepared in 2007, so new topics, vocabulary and ideas in CL will not be represented; and the fact that the texts have been automatically extracted from PDF files means that there are errors from the conversion process. 3 The Task in More Detail 3.1 How Do We Measure Quality? To be able"
W10-4236,C08-1109,0,0.0478084,"hould be judged on its research content, not on the author’s skills in English. This problem will surface in any discipline where authors are required to provide material in a language other than their mother tongue. However, as a discipline, computational linguistics holds a privileged position: as scientists, language (of different varieties) is our object of study, and as technologists, language tasks form our agenda. Many of the research problems we focus on could assist with writing problems. There is already existing work that addresses specific problems in this area (see, for example, (Tetreault and Chodorow, 2008)), but to be genuinely useful, we require a solution to the writing problem as a whole, integrating existing solutions to sub-problems with new solutions for problems as yet unexplored. Our proposal, then, is to initiate a shared task that attempts to tackle the problem head-on; we want to ‘help our own’ by developing tools which can help non-native speakers of English (NNSs) (and maybe some native ones) write academic English prose of the kind that helps a paper get accepted. The kinds of assistance we are concerned with here go beyond that which is provided by commonly-available spelling che"
W11-2838,W10-4236,1,\N,Missing
W11-2838,bird-etal-2008-acl,1,\N,Missing
W14-5146,I08-2099,1,0.802491,"Missing"
W97-0122,W95-0110,0,0.129584,"hod is compared empirically with the X2 method in some detail in section 6 below. There is a large body of work aiming to find words which are particularly characteristic of one text, or corpus, in contrast t o another. 4 This includes work on linguistic variation, author identification (Mosteller and Wallace, 1964) and information retrieval (Salton, 1989). (Dunning, 1993) and (Pedersen, 1996) shows how some of the methods which have been used in the past (particularly mutual information scores) are invalid for rare events, and introduce accurate measures of how &apos;surprising&apos; rare events are. (Church and Gale, 1995a) show how Inverse Document Frequency, a measure based on the proportion of documents a word occurs in, can be used alongside word frequency to estimate how distinctive a word is of the texts it occurs in. (Church and Gale, 1995b) extend this work, showing how to model word ~equency distributions in a manner consistent with the fact that some words are evenly spread, while others tend to occur often in documents where they occur at all. As most of this work ~im~ to find good indexing terms for information retrieval, it is mostly concerned with middle-to-low frequency items, and differences in"
W97-0122,J93-1001,0,0.0239434,"n statistical language mode]llng, there is much discussion of related questions. From an information-theoretic point of view, the theoretical answer to the problem is simple: entropy is a measure of a corpus&apos;s homogeneity, and the cross-entropy between two corpora quantifies their similarity. Entropy is not a quantity that can be directly measured. The standard problem for statistical language modelling is to aim to find the model for which the &apos;cross-entropy&apos; of the model for the corpus is as low as possible. For a perfect language model, the cross-entropy would be the entropy of the corpus (Church and Mercer, 1993; Charniak, 1993). The potential for using information-theoretic constructs to measure corpus similarity is a topic for current research. The Known Similarity Corpora evaluation methodology presented in Section 6 will be applicable to the issue of assessing how well cross-entropy captures pre-theoretical notions of corpus similarity and homogeneity. 4For a fuller review, see (Kilgarriff, 1996) 234 I I I I I I I II I I I I I I I I I I I I I I i I 4 Corpus homogeneity A corpus is a collection of texts. The definitiononly serves to Show how heterogeneous a collection of objects the word denotes."
W97-0122,J93-1003,0,0.0746638,"Missing"
W97-0122,A97-1015,0,\N,Missing
W97-0122,J93-2001,0,\N,Missing
W98-1506,J93-2001,0,0.0322113,"h&apos; in the Distano~ column readt&gt; that the distance between the corpora is substantially higher than these within-corpus distances. on the question, it is a good starting point. may be interpreted as a measure of the distance between the two varieties. Similarity and homogeneity Related Work How homogeneous is a corpus? The question is both of interest in its own right, and is a preliminary to any quantitative approach to corpus similarity. In its own right) because a sublanguage corpus) or one containing only a specific language variety, has very different characteristics to a general corpus (Biber, 1993) yet it is not obvious how a corpus&apos;s position on this scale can be assessed. As a preliminary to measuring corpf.ls similarity, because it is not clear what a measure of similarity would mean if a homogeneous corpus (of, ,eg, software manuals) was being compared with a heterogeneous one (eg. Brown). Ideally, the same measure can be used for similarity and homogeneity, as then, Corpus !/Corpus 2 distances will be directly comparable with heterogeneity (or &quot;within-corpus distances&quot;) for Corpusl and Corpus2. This is the approach adopted here. Not all combinations of homogeneity and similarity sc"
W98-1506,A97-1015,0,0.0114378,"entify same-text-type corpora. Spearman is evaluated below. There is a large body of work aiming to find words which are particularly characteristic of one text, or corpus) in contrast to another, in various fields including linguistic variation studies (Rayson, Leech, and Hodges, 1997), author identification (Mosteller and Wallace, 1964) and information retrieval (Salton, 1989; Dunning, 1993). Biber (1988, 1995) explores and quantifies the differences between corpora from a sociolinguistic perspective. While all of this work touches on corpussimilarity, none looks at is as a topic of itself. Sekine (1997) explores the domain dependence of parsing. He parses corpora of various text genres and counts the number of occurrences of each subtree of 47 depth one. This gives him a subtree frequency Jist for each corpus, and he is then able to investigate whieh subtrees arc markedly different in frequency between corpora. Such work is highly salient for customising parsers for particular domains. Subtree frequencies could readily replace word frequencies for the frequency-based measures below. In information-theoretic approaches, perplexity is a widely-used measure. Given a language model and a corpus,"
W98-1506,W97-0122,1,\N,Missing
W98-1506,W97-0118,1,\N,Missing
Y10-1086,C00-2157,0,0.30041,"s these processes are fast and available for many languages, and have been applied to most of our corpora) but not parsed. 2 Related Work There are numerous other corpus query tools available. Here we give brief references to several that are either widely used on large corpora or which concentrate on syntactic search. • the IMS Stuttgart CorpusWorkBench (Christ and Schulze, 1994), limited to 2 billion-words corpora, • a system presented in (Davies, 2009) which uses standard relational database technology, not closely related to syntactic search, • tools being part of the Tigersearch project (König and Lezius, 2000), currently unmaintained, • the Gsearch corpus query system (Steffan et al., 2001), 741 742 Workshop on Advanced Corpus Solutions 3 Core CQL A CQL query is a pattern which may match a token or series of tokens in the corpus. Each token is assigned a set of attributes (word form, lemma, part-of-speeech tag etc.) and each corpus might be assigned a set of structures. Structures may identify any sequence of tokens and are typically used to mark up documents, paragraphs, sentences, utterances, syntactic phrases of various kinds and named entities. Zero-length structures can also be used. Structure"
Y10-1086,kilgarriff-etal-2010-corpus,1,0.793715,"t for much linguistic research to find datasets relating to hunches and hypotheses. Linguists and system developers would like to be able to find large numbers of examples quickly and easily. Our tool computes the Corpus Query Language (CQL) (Christ and Schulze, 1994) queries rapidly on large corpora using the Manatee corpus manager system as its backend which is based on stream processing techniques and has been described in (Rychlý, 2000). We have large (more than 100 million-word) corpora loaded into the tool and accessible over the web for 25 languages and the number rises month by month (Reddy et al., 2010). For English, Italian and German we have very large (more than 1.5 billion-word) corpora, and for English, also a corpus of 5.5 billion words (Pomikálek et al., 2009). We have recently augmented the CQL formalism with extensions to the ‘within’ operator and a new ‘containing’ operator, to better support queries relating to phrase structure. In this paper we first summarise standard CQL, then describe the extensions to the ‘within’ operator, then introduce the ‘containing’ operator, and finally indicate computation times for sample queries. For the same purpose, the ‘meet’ and ‘union’ operator"
