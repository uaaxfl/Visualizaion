2020.bucc-1.4,P17-1042,0,0.0190009,". 6 https://www.gsk.or.jp/catalog/gsk2010-a/ We eliminated the compound words from the dictionary. 23 Figure 1: Tree Structure of Iwanami Kokugo Jiten Table 2: Concept-tags and Their Corresponding Class, Division, Section of “子供 (child or children) ”from WLSP Concept number Class Division Section Article 1.2050 Nominal words Agent Human Young or old 1.2130 Nominal words Agent Family Child or descendant linear projection matrix W was learned when we used a linear transformation matrix. VecMap is an implementation of a framework of Artetxe et al. to learn cross-lingual word embedding mappings (Artetxe et al., 2017)(Artetxe et al., 2018a)(Artetxe et al., 2018b). 1. Generate a word-sense-tag and concept-tag corpora respectively, and learn word-sense or concept embeddings for each corpus from them using word2vec 7 (Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013d) (cf. Figure 3). 4. Experiment 2. Learn a linear projection matrix W from the vector space of the word-senses to that of the concepts using pairs of the embeddings for monosemous common nouns, which are generated in the last step. 4.1. Experimental Setting We utilized BCCWJ tagged with word senses of Iwanami Kokugo Jiten and BCC"
2020.bucc-1.4,P18-1073,0,0.0207769,"r.jp/catalog/gsk2010-a/ We eliminated the compound words from the dictionary. 23 Figure 1: Tree Structure of Iwanami Kokugo Jiten Table 2: Concept-tags and Their Corresponding Class, Division, Section of “子供 (child or children) ”from WLSP Concept number Class Division Section Article 1.2050 Nominal words Agent Human Young or old 1.2130 Nominal words Agent Family Child or descendant linear projection matrix W was learned when we used a linear transformation matrix. VecMap is an implementation of a framework of Artetxe et al. to learn cross-lingual word embedding mappings (Artetxe et al., 2017)(Artetxe et al., 2018a)(Artetxe et al., 2018b). 1. Generate a word-sense-tag and concept-tag corpora respectively, and learn word-sense or concept embeddings for each corpus from them using word2vec 7 (Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013d) (cf. Figure 3). 4. Experiment 2. Learn a linear projection matrix W from the vector space of the word-senses to that of the concepts using pairs of the embeddings for monosemous common nouns, which are generated in the last step. 4.1. Experimental Setting We utilized BCCWJ tagged with word senses of Iwanami Kokugo Jiten and BCCWJ tagged with concep"
2020.bucc-1.4,P14-1006,0,0.0122899,"ed in the source language corpus into the target language using Wiktionary. Then they filtered out the noises of these pairs and trained the model with this corpus in which these pairs are replaced with placeholders to ensure that translations of the same word have the same vector representation. Third approach is cross-lingual training. These approaches train their embeddings on a parallel corpus and optimize a cross-lingual constraint between embeddings of different languages that encourages embeddings of similar words to be close to each other in a shared vector space. Hermann and Blunsom (Hermann and Blunsom, 2014) trained two models to output sentence embeddings for input sentences in two different languages. They retrained these models with sentence embeddings using a least-squares method. Final approach is joint optimization. They not only consider a cross-lingual constraint, but also jointly optimize mono-lingual and cross-lingual objectives. Klementiev et al. (Klementiev et al., 2012) was the first research using joint optimization. Zou (Zou et al., 2013) used a matrix factorization approach to learn cross-lingual word representations for English and Chinese and utilized the representaRecently, cor"
2020.bucc-1.4,C12-1089,0,0.0364516,"parallel corpus and optimize a cross-lingual constraint between embeddings of different languages that encourages embeddings of similar words to be close to each other in a shared vector space. Hermann and Blunsom (Hermann and Blunsom, 2014) trained two models to output sentence embeddings for input sentences in two different languages. They retrained these models with sentence embeddings using a least-squares method. Final approach is joint optimization. They not only consider a cross-lingual constraint, but also jointly optimize mono-lingual and cross-lingual objectives. Klementiev et al. (Klementiev et al., 2012) was the first research using joint optimization. Zou (Zou et al., 2013) used a matrix factorization approach to learn cross-lingual word representations for English and Chinese and utilized the representaRecently, corpora that have tags from more than one tag set are increasing. For example, “ The Balanced Corpus of Contemporary Written Japanese ”(BCCWJ) (Maekawa et al., 2014) is tagged with concept tags from “ Word List by Semantic Principles ” (WLSP) (National Institute for Japanese Language and Linguistics, 1964) after tagged with sense tags from“ Iwanami Kokugo Jiten (Nishio et al., 1994)"
2020.bucc-1.4,N13-1090,0,0.0170422,"of “子供 (child or children) ”from WLSP Concept number Class Division Section Article 1.2050 Nominal words Agent Human Young or old 1.2130 Nominal words Agent Family Child or descendant linear projection matrix W was learned when we used a linear transformation matrix. VecMap is an implementation of a framework of Artetxe et al. to learn cross-lingual word embedding mappings (Artetxe et al., 2017)(Artetxe et al., 2018a)(Artetxe et al., 2018b). 1. Generate a word-sense-tag and concept-tag corpora respectively, and learn word-sense or concept embeddings for each corpus from them using word2vec 7 (Mikolov et al., 2013a; Mikolov et al., 2013c; Mikolov et al., 2013d) (cf. Figure 3). 4. Experiment 2. Learn a linear projection matrix W from the vector space of the word-senses to that of the concepts using pairs of the embeddings for monosemous common nouns, which are generated in the last step. 4.1. Experimental Setting We utilized BCCWJ tagged with word senses of Iwanami Kokugo Jiten and BCCWJ tagged with concepts of WLSP. Table 3 shows the number of word tokens, unique words, unique word senses, and unique concepts. 3. Apply the matrix W to the word-sense embeddings and obtain the projected concept embedding"
2020.bucc-1.4,W14-1613,0,0.01155,"ation method needs two tagged corpora. Keywords: Bilingual Word Embedding, Concept Embeddings, Word Embeddings, Dictionary 1. Introduction 2013b) have shown that vector spaces can encode meaningful relations between words and that the geometric relations that hold between words are similar across languages. Because they do not assume the use of specific language, their method can be used to extend and refine dictionaries for any language pairs. Second approach is pseudo-cross-lingual. These approaches create a pseudocross-lingual corpus by mixing contexts of different languages. Xiao and Guo (Xiao and Guo, 2014) proposed the first pseudo-cross-lingual method that utilized translation pairs. They first translated all words that appeared in the source language corpus into the target language using Wiktionary. Then they filtered out the noises of these pairs and trained the model with this corpus in which these pairs are replaced with placeholders to ensure that translations of the same word have the same vector representation. Third approach is cross-lingual training. These approaches train their embeddings on a parallel corpus and optimize a cross-lingual constraint between embeddings of different lan"
2020.bucc-1.4,D13-1141,0,0.0411079,"different languages that encourages embeddings of similar words to be close to each other in a shared vector space. Hermann and Blunsom (Hermann and Blunsom, 2014) trained two models to output sentence embeddings for input sentences in two different languages. They retrained these models with sentence embeddings using a least-squares method. Final approach is joint optimization. They not only consider a cross-lingual constraint, but also jointly optimize mono-lingual and cross-lingual objectives. Klementiev et al. (Klementiev et al., 2012) was the first research using joint optimization. Zou (Zou et al., 2013) used a matrix factorization approach to learn cross-lingual word representations for English and Chinese and utilized the representaRecently, corpora that have tags from more than one tag set are increasing. For example, “ The Balanced Corpus of Contemporary Written Japanese ”(BCCWJ) (Maekawa et al., 2014) is tagged with concept tags from “ Word List by Semantic Principles ” (WLSP) (National Institute for Japanese Language and Linguistics, 1964) after tagged with sense tags from“ Iwanami Kokugo Jiten (Nishio et al., 1994). ” Because these tags are tagged referring to different dictionaries, t"
2020.paclic-1.15,S01-1001,0,0.215622,"a number of tasks, such as machine translation and text summarization. Word embeddings are usually generated using text corpora. It is possible to generate concept embeddings by the same method used to generate word embeddings if the word sequence (i.e., text corpus) is replaced with a concept sequence constructed from a concept-tagged corpus. However, it is difficult to obtain a sufficiently large concept-tagged corpus because the annotation of concept tags is time-consuming. There have been several studies that assigned word senses using the all-words word sense disambiguation (WSD) method (Edmonds and Cotton, 2001), (Snyder and Palmer, 2004), (Navigli et al., 2007), (Iacobacci et al., 2016), (Raganato et al., 2017a), (Raganato et al., 2017b), (Suzuki et al., 2018), (Shinnou et al., 2018). As a result, it is possible to create a concept-tagged corpus using the methods proposed in these studies. However, the results of all-words WSD systems are not always correct; therefore, an automatically tagged corpus created via all-words WSD may not be suitable for generating concept embeddings. In this paper, we generate concept embeddings of Word List by Semantic Principles (WLSP) (National Institute for Japanese"
2020.paclic-1.15,P16-1085,0,0.0226331,"eddings are usually generated using text corpora. It is possible to generate concept embeddings by the same method used to generate word embeddings if the word sequence (i.e., text corpus) is replaced with a concept sequence constructed from a concept-tagged corpus. However, it is difficult to obtain a sufficiently large concept-tagged corpus because the annotation of concept tags is time-consuming. There have been several studies that assigned word senses using the all-words word sense disambiguation (WSD) method (Edmonds and Cotton, 2001), (Snyder and Palmer, 2004), (Navigli et al., 2007), (Iacobacci et al., 2016), (Raganato et al., 2017a), (Raganato et al., 2017b), (Suzuki et al., 2018), (Shinnou et al., 2018). As a result, it is possible to create a concept-tagged corpus using the methods proposed in these studies. However, the results of all-words WSD systems are not always correct; therefore, an automatically tagged corpus created via all-words WSD may not be suitable for generating concept embeddings. In this paper, we generate concept embeddings of Word List by Semantic Principles (WLSP) (National Institute for Japanese Language and Linguistics, 1964), a Japanese thesaurus, from manually and auto"
2020.paclic-1.15,Y18-1029,1,0.877379,"Missing"
2020.paclic-1.15,N13-1090,0,0.00598347,", with the concept tag annotation via the all-words WSD system. The manual corpus is the part of the core data with manual annotation of the concept tags, which includes approximately 340,000 words. Examples of the text corpus and a generated concept sequence are presented in Table 2. In the table, an original Japanese text, its English translation and concept sequence are shown. The concepts of “な く” and “ない” are both 3.1200 because they are the same words after lemmatization. Table 3 presents the number of words, vocabulary, and concepts in each corpus. 3.3 Vectors In this study, word2vec2 (Mikolov et al., 2013a; Mikolov et al., 2013b; Mikolov et al., 2013c) was used to generate concept embeddings. Then, finetuning was performed. Fine-tuning is a method in which generated distributed representations are 2 https://code.google.com/archive/p/ word2vec/ given as initial values and retrained with a new corpus. The following four types of concept embeddings were created: • All-words WSD vector: concept embeddings were trained with the all-words WSD corpus. • All-words WSD-fine vector: concept embeddings were trained with the all-words WSD corpus and retrained with a manual corpus. • Manual vector: concept"
2020.paclic-1.15,D17-1120,0,0.0678417,"ted using text corpora. It is possible to generate concept embeddings by the same method used to generate word embeddings if the word sequence (i.e., text corpus) is replaced with a concept sequence constructed from a concept-tagged corpus. However, it is difficult to obtain a sufficiently large concept-tagged corpus because the annotation of concept tags is time-consuming. There have been several studies that assigned word senses using the all-words word sense disambiguation (WSD) method (Edmonds and Cotton, 2001), (Snyder and Palmer, 2004), (Navigli et al., 2007), (Iacobacci et al., 2016), (Raganato et al., 2017a), (Raganato et al., 2017b), (Suzuki et al., 2018), (Shinnou et al., 2018). As a result, it is possible to create a concept-tagged corpus using the methods proposed in these studies. However, the results of all-words WSD systems are not always correct; therefore, an automatically tagged corpus created via all-words WSD may not be suitable for generating concept embeddings. In this paper, we generate concept embeddings of Word List by Semantic Principles (WLSP) (National Institute for Japanese Language and Linguistics, 1964), a Japanese thesaurus, from manually and automatically tagged corpora"
2020.paclic-1.15,W04-0811,0,0.0762677,"achine translation and text summarization. Word embeddings are usually generated using text corpora. It is possible to generate concept embeddings by the same method used to generate word embeddings if the word sequence (i.e., text corpus) is replaced with a concept sequence constructed from a concept-tagged corpus. However, it is difficult to obtain a sufficiently large concept-tagged corpus because the annotation of concept tags is time-consuming. There have been several studies that assigned word senses using the all-words word sense disambiguation (WSD) method (Edmonds and Cotton, 2001), (Snyder and Palmer, 2004), (Navigli et al., 2007), (Iacobacci et al., 2016), (Raganato et al., 2017a), (Raganato et al., 2017b), (Suzuki et al., 2018), (Shinnou et al., 2018). As a result, it is possible to create a concept-tagged corpus using the methods proposed in these studies. However, the results of all-words WSD systems are not always correct; therefore, an automatically tagged corpus created via all-words WSD may not be suitable for generating concept embeddings. In this paper, we generate concept embeddings of Word List by Semantic Principles (WLSP) (National Institute for Japanese Language and Linguistics, 1"
2020.paclic-1.15,L18-1162,1,0.693266,"oncept embeddings by the same method used to generate word embeddings if the word sequence (i.e., text corpus) is replaced with a concept sequence constructed from a concept-tagged corpus. However, it is difficult to obtain a sufficiently large concept-tagged corpus because the annotation of concept tags is time-consuming. There have been several studies that assigned word senses using the all-words word sense disambiguation (WSD) method (Edmonds and Cotton, 2001), (Snyder and Palmer, 2004), (Navigli et al., 2007), (Iacobacci et al., 2016), (Raganato et al., 2017a), (Raganato et al., 2017b), (Suzuki et al., 2018), (Shinnou et al., 2018). As a result, it is possible to create a concept-tagged corpus using the methods proposed in these studies. However, the results of all-words WSD systems are not always correct; therefore, an automatically tagged corpus created via all-words WSD may not be suitable for generating concept embeddings. In this paper, we generate concept embeddings of Word List by Semantic Principles (WLSP) (National Institute for Japanese Language and Linguistics, 1964), a Japanese thesaurus, from manually and automatically tagged corpora. First, concept embeddings are generated from a co"
2020.paclic-1.32,N19-1423,0,0.172577,"els for these languages with task-based approaches is challenging. In this work, we evaluate Japanese pre-trained BERT models with CLS token. We input labeled sentences to models, get CLS token embeddings, and calculate scores from in-class and outof-class dispersions, which can be calculated from embeddings and labels of sentences. Experiment results show that a model released by Laboro.AI Inc. is the best Japanese pretrained BERT model. Meanwhile, the results of evaluation with sentence clustering are different from those of evaluations that are based on fill mask task. 1 Introduction BERT (Devlin et al., 2019) is a high-performance pre-training model. It helped in the improvement of the performance of natural language processing tasks. Generally, task-based approaches were adopted for evaluating pre-training models like BERT. In English language, a dataset for task-based evaluation, such as the general language understanding evaluation (GLUE) (Wang et al., 2018), can be used, and it is easy to compare models. However, when a pre-trained model is fine-tuned for taskbased evaluation, meta parameters for fine-tuning may influence scores of the model. Hence, taskbased evaluation with fine-tuning has a"
2020.paclic-1.32,P10-1114,0,0.00925871,"e calculated scores with the method in Sechttps://github.com/yoheikikuta/ bert-japanese tion 3.1 and compared these scores. 6 https://github.com/cl-tohoku/ In the evaluation with fill mask task, we made a bert-japanese This model can be used easily as “clfill mask dataset from Japanese domain of Webistohoku/bert-base-japanese” from HuggingFace’s transformers (Wolf et al., 2019), and we used it. 7 https://alaginrc.nict.go.jp/nict-bert/ index.html We used the byte pair encoding (BPE) version. 8 https://laboro.ai/column/laboro-bert/ http://www.rondhuit.com/download.html# ldcc 9 4 Results CLS-10 (Prettenhofer and Stein, 2010) and used it. The following two steps show how to make a fill mask dataset from Webis. 1. Pick twenty nouns that have the highest frequencies of occurence from the test data of each domain: books, DVDs, and music. 2. Pick five sentences that contain matching selected words from test data of the matching domain randomly to each selected word. 3. Use nouns which were selected in step 1 as labels for matching sentences which were selected in step 2. In this section, we show the resluts of the evaluations. First, we show the result of the evaluation with sentence clustering, and then the result of"
2020.paclic-1.46,P17-1042,0,0.0224069,"ctor space such that the word embeddings of the words whose meanings were similar to each other in two languages can be brought closer. The geometrical relations that hold between words are similar across languages; thus a vector space of a language can be transformed into that of another language using a linear projection. We adapted hereikn two methods of the BWE, namely, linear transformation matrix and VecMap. A linear projection matrix W was 4 https://github.com/artetxem/vecmap#publications learned when we used a linear transformation matrix. VecMap is an implementation of a framework of Artetxe et al. (2017) to learn cross-lingual word embedding mappings (Artetxe et al., 2018a)(Artetxe et al., 2018b). 3.1.1 Linear Transformation Matrix We conducted the following experiments when a linear transformation matrix was learned: 1. Generate short and long unit corpora and learn short or long unit embeddings for each corpus from them using word2vec (cf. Figure 1). 2. Learn a linear projection matrix W from the vector space of the short units to that of the long units using pairs of embeddings for common words generated in the last step. 3. Apply matrix W to the short unit embeddings and obtain the projec"
2020.paclic-1.46,P18-1073,0,0.0178917,"ere similar to each other in two languages can be brought closer. The geometrical relations that hold between words are similar across languages; thus a vector space of a language can be transformed into that of another language using a linear projection. We adapted hereikn two methods of the BWE, namely, linear transformation matrix and VecMap. A linear projection matrix W was 4 https://github.com/artetxem/vecmap#publications learned when we used a linear transformation matrix. VecMap is an implementation of a framework of Artetxe et al. (2017) to learn cross-lingual word embedding mappings (Artetxe et al., 2018a)(Artetxe et al., 2018b). 3.1.1 Linear Transformation Matrix We conducted the following experiments when a linear transformation matrix was learned: 1. Generate short and long unit corpora and learn short or long unit embeddings for each corpus from them using word2vec (cf. Figure 1). 2. Learn a linear projection matrix W from the vector space of the short units to that of the long units using pairs of embeddings for common words generated in the last step. 3. Apply matrix W to the short unit embeddings and obtain the projected long unit embeddings for them. 3.1.2 VecMap VecMap was used as an"
2020.paclic-1.46,P14-1006,0,0.0177335,"that appeared in the source language corpus into the target language using Wiktionary. They then ﬁltered out the noises of these pairs and trained the model with this corpus, in which the pairs were replaced with placeholders to ensure that the translations of the same word have the same vector representation. The third approach is cross-lingual training. This approach trains their embeddings on a parallel corpus and optimizes a cross-lingual constraint between the embeddings of different languages that encourages embeddings of similar words to be close to each other in a shared vector space. Hermann and Blunsom (2014) trained two models to output sentence embeddings for input sentences in two different languages. They retrained these models with sentence embeddings using a least squares method. The ﬁnal approach is joint optimization, which not only considers a cross-lingual constraint but also jointly optimizes monolingual and cross-lingual objectives. Klementiev et al. (2012) performed the ﬁrst research using joint optimization. Zou et al. (2013) used a matrix factorization approach to learn crosslingual word representations for English and Chinese and utilized the representations for a machine 3 https:/"
2020.paclic-1.46,C12-1089,0,0.0254638,"ch trains their embeddings on a parallel corpus and optimizes a cross-lingual constraint between the embeddings of different languages that encourages embeddings of similar words to be close to each other in a shared vector space. Hermann and Blunsom (2014) trained two models to output sentence embeddings for input sentences in two different languages. They retrained these models with sentence embeddings using a least squares method. The ﬁnal approach is joint optimization, which not only considers a cross-lingual constraint but also jointly optimizes monolingual and cross-lingual objectives. Klementiev et al. (2012) performed the ﬁrst research using joint optimization. Zou et al. (2013) used a matrix factorization approach to learn crosslingual word representations for English and Chinese and utilized the representations for a machine 3 https://ruder.io/cross-lingual-embeddings/ translation task. In this study, we used the ﬁrst approach, monolingual mapping. The nearest works to this research are those of Komiya et al. (2019) and Kouno and Komiya (2020). Komiya et al. (2019) composed word embeddings for long units from the two word embeddings of short units using a feed-forward neural network system. The"
2020.paclic-1.46,W14-1613,0,0.0211906,"rains monolingual word embeddings and learns a transformation matrix that maps representations in one language to those of the other language. Mikolov et al. (2013) showed that vector spaces can encode meaningful relations between words and that the geometric relations that hold between words are similar across languages. They did not assume the use of speciﬁc language; thus their method can be used to extend and reﬁne dictionaries for any language pairs. The second approach is pseudo-cross-lingual. This approach creates a pseudo-cross-lingual corpus by mixing contexts of different languages. Xiao and Guo (2014) proposed the ﬁrst pseudo-cross-lingual method that utilized translation pairs. They ﬁrst translated all words that appeared in the source language corpus into the target language using Wiktionary. They then ﬁltered out the noises of these pairs and trained the model with this corpus, in which the pairs were replaced with placeholders to ensure that the translations of the same word have the same vector representation. The third approach is cross-lingual training. This approach trains their embeddings on a parallel corpus and optimizes a cross-lingual constraint between the embeddings of diffe"
2020.paclic-1.46,D13-1141,0,0.0140009,"onstraint between the embeddings of different languages that encourages embeddings of similar words to be close to each other in a shared vector space. Hermann and Blunsom (2014) trained two models to output sentence embeddings for input sentences in two different languages. They retrained these models with sentence embeddings using a least squares method. The ﬁnal approach is joint optimization, which not only considers a cross-lingual constraint but also jointly optimizes monolingual and cross-lingual objectives. Klementiev et al. (2012) performed the ﬁrst research using joint optimization. Zou et al. (2013) used a matrix factorization approach to learn crosslingual word representations for English and Chinese and utilized the representations for a machine 3 https://ruder.io/cross-lingual-embeddings/ translation task. In this study, we used the ﬁrst approach, monolingual mapping. The nearest works to this research are those of Komiya et al. (2019) and Kouno and Komiya (2020). Komiya et al. (2019) composed word embeddings for long units from the two word embeddings of short units using a feed-forward neural network system. They classiﬁed the dependency relations of two short units into 13 groups a"
C96-2205,P93-1022,0,0.250866,"om previous words in a text (Brown et al., 1992). They tackled data sparseness by generalizing the word to the class which contains the word. Pereira ct al. also basically used the above method, but they proposed a soft clustering scheme, in which membership of a word in a class is probabilistic (Pereira et al., 1993). Brown and Pereira provide the clustering algorithm assigning words to proper classes, based on their own models. I)agan eL al. proposed a similarity-based model in which each word is generalized, not to its own specific class, but to a set of words which are most similar to it (Dagan et al., 1993). Using this model, they successfully l)redieted which unobserved cooccurrenccs were more likely than others, and estimated the probability of the cooecurrences (Dagan et al., 1994). However, because these schemes look for similar words in the corpus, the number of similarities which we can define is rather small in comparison with the nunlber of similarities for pairs of the whole. The scheme to look for similar words in the corpus has already taken the influence of data sparseness. In this paper, we propose a method distinct from the above methods, which use a handmade thesaurus to find simi"
C96-2205,P90-1034,0,0.689712,"an shinnou@lily, dse. ibaraki, ac. j p 1 Introduction The aim of this paper is to automatically define the similarity I)etween two nouns which are generally used in various domains. By these similarities, we can construct a large and general thesaurus. In applications of natural language processing, it is necessary to appropriately measure the similarity between two nouns. The similarity is usually calculated from a thesaurus. Since a handmade thesaurus is not slfitahle for machine use, and expensive to compile, automatical construction of~a thesaurus has been a t t e m p t e d using corpora (Hindle, 1990). llowever, the thesaurus constructed by such ways does not contain so m a n y nouns, and these nouns are specified by the used corpus. In other words, we cannot construct the general thesaurus from only a corpus. This can be regarded as data sparseness problem that few nouns appear in the corpus. 9b overcome data sparseness, methods to estim a t e the distribution of unseen eooecurrence frorn the distribution of similar words in the seen cooccurrence has been proposed. Brown et al. proposed a class-based n-gram model, which generalizes the n-gram model, to predict a word from previous words i"
C96-2205,P94-1038,0,\N,Missing
C96-2205,J92-4003,0,\N,Missing
C96-2205,P93-1024,0,\N,Missing
E99-1024,P96-1010,0,0.0167946,"eaning of &apos;l-ff__,~.i is an intuition through a feeling, and the meaning of &apos; ~ &apos; is an intuition through a latent knowledge. ZWe ignore the difference of accents, stresses and parts of speech. That is, the homophone set is the set of words having the same expression in hiragana characters. Proceedings of EACL &apos;99 statistical methods proposed for the word sense disambiguation problem(Fujii, 1998). Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Golding and Schabes, 1996). Hence, statistical methods axe certainly valid for the homophone problem. In particular, the decision list is valid for the homophone problem(Shinnou, 1998). The decision list arranges evidences to identify the word sense in the order of strength of identifying the sense. The word sense is judged by the evidence, with the highest identifying strength, in the context. Although the homophone problem is equivalent to the word sense disambiguation problem, the former has a distinct difference from the latter. In the homophone problem, almost all of the answers axe given correctly, because almost"
E99-1024,W95-0104,0,0.040592,"ho-kkan&apos;. The meaning of &apos;l-ff__,~.i is an intuition through a feeling, and the meaning of &apos; ~ &apos; is an intuition through a latent knowledge. ZWe ignore the difference of accents, stresses and parts of speech. That is, the homophone set is the set of words having the same expression in hiragana characters. Proceedings of EACL &apos;99 statistical methods proposed for the word sense disambiguation problem(Fujii, 1998). Take the case of context-sensitive spelling error detection 3, which is equivalent to the homophone problem. For that problem, some statistical methods have been applied and succeeded(Golding, 1995; Golding and Schabes, 1996). Hence, statistical methods axe certainly valid for the homophone problem. In particular, the decision list is valid for the homophone problem(Shinnou, 1998). The decision list arranges evidences to identify the word sense in the order of strength of identifying the sense. The word sense is judged by the evidence, with the highest identifying strength, in the context. Although the homophone problem is equivalent to the word sense disambiguation problem, the former has a distinct difference from the latter. In the homophone problem, almost all of the answers axe giv"
E99-1024,A94-1026,0,0.0310172,"ent)} is a homophone set because words in the set axe composed of kanji characters that have the same phone &apos;ka-ku-ri-tu&apos;. Thus, q / ~ &apos; and &apos; ~ f _ &apos; are homophone words. In this paper, we name the problem of choosing the correct word from the homophone set the homophone p r o b l e m . In order to detect homophone errors, we make a list of homophone sets in advance, find a homophone word in the text, and then solve the homophone problem for the homophone word. Many methods of solving the homophone problem have been proposed (Tochinai et al., 1986; Ibuki et al., 1997; Oku and Matsuoka, 1997; Oku, 1994; Wakita and Kaneko, 1996). However, they are restricted to the homophone problem, that is, they are heuristic methods. On the other hand, the homophone problem is equivalent to the word sense disambiguation problem if the phone of the homophone word is regarded as the word, and the homophone word as the sense. Therefore, we can solve the homophone problem by using various 1&apos;~&apos;.-~.,~. and &apos;~.~..m~,&apos;have a same phone &apos;i-sift&apos;. The meaning of &apos; ~ , &apos; is a general will, and the meaning of &apos;~:~&apos;.~.,,... is a strong positive will. &apos;~.~.&apos; and &apos; ~ &apos; have a same phone &apos;cho-kkan&apos;. The meaning of &apos;l-ff_"
E99-1024,P94-1013,0,0.249137,"one problem is equivalent to the word sense disambiguation problem. However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper, we incorporate the written word into the original decision list by obtaining the identifying strength of the written word. The improved decision list can raise the F-measure of error detection. 1 Introduction In this paper, we propose a method of detecting Japanese homophone errors in Japanese texts. Our method is based on a decision list proposed by Yarowsky (Yarowsky, 1994; Yarowsky, 1995). We improve the original decision list by using written words in the default evidence. The improved decision list can raise the F-measure of error detection. Most Japanese texts are written using Japanese word processors. To input a word composed of kanji characters, we first input the phonetic hiragana sequence for the word, and then convert it to the desired kanji sequence. However, multiple converted kanji sequences are generally produced, and we must then choose the correct kanji sequence. Therefore, Japanese texts suffer from ho180 ac. jp mophone errors caused by incorre"
E99-1024,P95-1026,0,0.079182,"quivalent to the word sense disambiguation problem. However, the homophone problem is different from the word sense disambiguation problem because the former can use the written word but the latter cannot. In this paper, we incorporate the written word into the original decision list by obtaining the identifying strength of the written word. The improved decision list can raise the F-measure of error detection. 1 Introduction In this paper, we propose a method of detecting Japanese homophone errors in Japanese texts. Our method is based on a decision list proposed by Yarowsky (Yarowsky, 1994; Yarowsky, 1995). We improve the original decision list by using written words in the default evidence. The improved decision list can raise the F-measure of error detection. Most Japanese texts are written using Japanese word processors. To input a word composed of kanji characters, we first input the phonetic hiragana sequence for the word, and then convert it to the desired kanji sequence. However, multiple converted kanji sequences are generally produced, and we must then choose the correct kanji sequence. Therefore, Japanese texts suffer from ho180 ac. jp mophone errors caused by incorrect choices. Carel"
L18-1162,Y15-1005,1,0.771566,". Generally, WSD using supervised learning can achieve high accuracy rates, but requires substantial manual effort due to the need for a sufficient amount of manually-annotated training data. On the other hand, unsupervised learning does not need such manual input, but it is difficult to obtain as high an accuracy rate as with the supervised learning. Many WSD methods have been proposed. WLSP article numbers or hypernyms of target words obtained from the WLSP are often used as supervised learning features. Vu and Parker (2016) proposed the idea of K-embeddings for learning concept embeddings. Komiya et al. (2015) proposed a surrounding word sense model for Japanese allwords WSD using unsupervised learning which assumes that the sense distribution of surrounding words changes depending on the sense in which a polysemous word is used. Shinnou et al. (2017b) proposed a WSD system capable of performing Japanese WSD easily using a supervised approach. 3. WSD Using Synonym Information from the WLSP We propose three WSD methods that use synonym information from the WLSP: 1) a method using only the word embeddings of synonyms, 2) a method using both the word and concept embeddings of synonyms, and 3) a method"
L18-1162,N13-1090,0,0.0942523,"r of iterations -cbow -size -window -negative -hs -sample -iter 1 200 8 25 0 1e-4 15 Table 3: Parameters Used to Generate NWJC2vec Note that we cannot know what is the most frequent sense using an unsupervised approach. We used NWJC2vec2 (Shinnou et al., 2017a) for the Japanese word embeddings. This is a set of word embeddings generated from the NWJC-2014-4Q dataset, which is an enormous Japanese corpus developed using the word2vec3 tool. Tables 2 and 3 present summary statistics for the NWJC-2014-4Q data and the parameters used to generate the word embeddings, respectively. We used word2vec (Mikolov et al., 2013c; Mikolov et al., 2013a; Mikolov et al., 2013b) to generate the concept embeddings, and the parameters used are summarized in Table 4. The window size for the surrounding word vectors was set to two, meaning four words in total. When the number of surrounding words was smaller than the window size, we used a zero vector. Therefore, the dimensionality of the surrounding word vectors was 800 when they were created using only the word embeddings, and 1,000 when both the word and concept embeddings were used. It was 200 when only the concept embeddings are used. We used KNeighborsClassifier from"
L18-1162,Y17-1052,1,0.930799,"d such manual input, but it is difficult to obtain as high an accuracy rate as with the supervised learning. Many WSD methods have been proposed. WLSP article numbers or hypernyms of target words obtained from the WLSP are often used as supervised learning features. Vu and Parker (2016) proposed the idea of K-embeddings for learning concept embeddings. Komiya et al. (2015) proposed a surrounding word sense model for Japanese allwords WSD using unsupervised learning which assumes that the sense distribution of surrounding words changes depending on the sense in which a polysemous word is used. Shinnou et al. (2017b) proposed a WSD system capable of performing Japanese WSD easily using a supervised approach. 3. WSD Using Synonym Information from the WLSP We propose three WSD methods that use synonym information from the WLSP: 1) a method using only the word embeddings of synonyms, 2) a method using both the word and concept embeddings of synonyms, and 3) a method using only the concept embeddings of synonyms. 3.1. WSD Using the Word Embeddings of Surrounding Words Since the senses of words are determined by context, such as the surrounding words, similar words are believed to 1006 have similar sets of s"
L18-1162,N16-1151,0,0.0197738,"thods can broadly be divided into two categories: supervised and unsupervised approaches. Generally, WSD using supervised learning can achieve high accuracy rates, but requires substantial manual effort due to the need for a sufficient amount of manually-annotated training data. On the other hand, unsupervised learning does not need such manual input, but it is difficult to obtain as high an accuracy rate as with the supervised learning. Many WSD methods have been proposed. WLSP article numbers or hypernyms of target words obtained from the WLSP are often used as supervised learning features. Vu and Parker (2016) proposed the idea of K-embeddings for learning concept embeddings. Komiya et al. (2015) proposed a surrounding word sense model for Japanese allwords WSD using unsupervised learning which assumes that the sense distribution of surrounding words changes depending on the sense in which a polysemous word is used. Shinnou et al. (2017b) proposed a WSD system capable of performing Japanese WSD easily using a supervised approach. 3. WSD Using Synonym Information from the WLSP We propose three WSD methods that use synonym information from the WLSP: 1) a method using only the word embeddings of synon"
R15-1039,W03-1307,0,0.0565865,"the production companies and broadcasting companies, the names of related products, and the names of relevant sites, related events, and so on. Table 1 lists some examples of the anime-related words. pervised machine learning in Japanese originally cannot extract NEs that are smaller than morphemes because cascading morphological analysis and chunking is usually used for any NE extraction in Japanese. (An et al., 2003) have automatically collected NE tagged corpora from World Wide Web to alleviate the problem: building corpus is time-consuming. There are some works on the adaptation of NER. (Shen et al., 2003) have investigated the effective features of a Hidden Markov model-based NE recognizer for the biomedical domain. (Chiticariu et al., 2010) have improved NER rule language (NERL) for the pattern-based domain adaptation of NER. (Guo et al., 2009) have proposed a domain adaptation method using latent semantic association. We developed a system to extract Japanese anime-related words using machine learning method, i.e., CRF, for this paper. Since our purpose is to use the anime-related words for the product search, identification, or recommendation, we only extracted them and did not automaticall"
R15-1039,P03-2031,0,0.0527614,"ng the source data not only reduced the time 291 Proceedings of Recent Advances in Natural Language Processing, pages 291–297, Hissar, Bulgaria, Sep 7–9 2015. the names of organizations such as the production companies and broadcasting companies, the names of related products, and the names of relevant sites, related events, and so on. Table 1 lists some examples of the anime-related words. pervised machine learning in Japanese originally cannot extract NEs that are smaller than morphemes because cascading morphological analysis and chunking is usually used for any NE extraction in Japanese. (An et al., 2003) have automatically collected NE tagged corpora from World Wide Web to alleviate the problem: building corpus is time-consuming. There are some works on the adaptation of NER. (Shen et al., 2003) have investigated the effective features of a Hidden Markov model-based NE recognizer for the biomedical domain. (Chiticariu et al., 2010) have improved NER rule language (NERL) for the pattern-based domain adaptation of NER. (Guo et al., 2009) have proposed a domain adaptation method using latent semantic association. We developed a system to extract Japanese anime-related words using machine learnin"
R15-1039,D10-1098,0,0.0217599,"and so on. Table 1 lists some examples of the anime-related words. pervised machine learning in Japanese originally cannot extract NEs that are smaller than morphemes because cascading morphological analysis and chunking is usually used for any NE extraction in Japanese. (An et al., 2003) have automatically collected NE tagged corpora from World Wide Web to alleviate the problem: building corpus is time-consuming. There are some works on the adaptation of NER. (Shen et al., 2003) have investigated the effective features of a Hidden Markov model-based NE recognizer for the biomedical domain. (Chiticariu et al., 2010) have improved NER rule language (NERL) for the pattern-based domain adaptation of NER. (Guo et al., 2009) have proposed a domain adaptation method using latent semantic association. We developed a system to extract Japanese anime-related words using machine learning method, i.e., CRF, for this paper. Since our purpose is to use the anime-related words for the product search, identification, or recommendation, we only extracted them and did not automatically classify them into sub-classes. We examined to see if the existing corpora were useful with the corpus that we built using domain adaptat"
R15-1039,P07-1033,0,0.168056,"Missing"
R15-1039,N09-1032,0,0.0259573,"lly cannot extract NEs that are smaller than morphemes because cascading morphological analysis and chunking is usually used for any NE extraction in Japanese. (An et al., 2003) have automatically collected NE tagged corpora from World Wide Web to alleviate the problem: building corpus is time-consuming. There are some works on the adaptation of NER. (Shen et al., 2003) have investigated the effective features of a Hidden Markov model-based NE recognizer for the biomedical domain. (Chiticariu et al., 2010) have improved NER rule language (NERL) for the pattern-based domain adaptation of NER. (Guo et al., 2009) have proposed a domain adaptation method using latent semantic association. We developed a system to extract Japanese anime-related words using machine learning method, i.e., CRF, for this paper. Since our purpose is to use the anime-related words for the product search, identification, or recommendation, we only extracted them and did not automatically classify them into sub-classes. We examined to see if the existing corpora were useful with the corpus that we built using domain adaptation and showed that the filtering of the source data assisted in the domain adaptation work. 3 4 System to"
R15-1039,W11-3209,0,0.0236949,"tity recognition (NER), which involves seeking to locate and classify elements in text into predefined categories, such as the names of people, organizations, and locations, has recently been intensively studied. There are two types of NER methods, i.e., NER using pattern matching and NER using supervised machine learning. NER using pattern matching finds elements in text that match the manually predefined patterns, i.e., the character strings that tend to co-occur with the NEs, e.g., “Mr.” or “University” (Takemoto et al., 2001). There are some works on analyzing or creating these patterns; (Lertcheva and Aroonmanakun, 2011) have analyzed the patterns of the product names used in Thai economic news. NER using pattern matching can extract the NEs that precisely match the patterns, but cannot extract the NEs that do not match the patterns. Therefore, it is difficult to use these types of methods in our system because the anime-related NEs such as the titles of an anime often do not match the patterns. On the other hand, NER using supervised machine learning trains the patterns to extract the NEs using a tagged corpus. (Yamada et al., 2002) have carried out NER using a support vector machine (SVM). (Nakano and Hirai"
R15-1039,I08-7018,0,0.0328795,"d words include the names of people such as the authors of the original story, animators, and game creators, 5 Data We used an anime corpus that we built for the experiments. The texts consisted of 50 anime articles from Wikipedia. The morphological analysis was automatically carried out but the errors in the word segmentations and the POS tags of personal names were manually corrected. After that, all the NEs that we defined above were manually annotated. We used the extended NE tagged corpora (Hashimoto et al., 2008), which were based on the Balanced Corpus of Contemporary Japanese (BCCWJ) (Maekawa, 2008), for a reference when we built the corpus. 292 Detailed conception Name of animal character Name of special weapon Name of character Nickname of character Example ポチ かめはめ波 宿海仁太 じんたん Translation or Explanation Pochi(Dog’s name) Kame Hame Ha Yadomi Zhinta Zintan Table 1: Examples of anime-related words 6 Experiment The anime-related NE tagged BCCWJ were created based on the extended NE tagged corpora on BCCWJ and they were also used for the training data. We investigated to see if they could be used for the training data on their own and could be used with the corpus that we built with and with"
R15-1039,sekine-2008-extended,0,0.0336657,"the CRF. They were extracted from the surrounding words of the target morpheme. We used the character type as a feature because the Japanese language has many types of characters and it seemed to be related to the ability of the morphemes to be NEs, especially for anime-related words. The values of the types are hiragana, katakana, alphabetical letters, Chinese characters, and others including punctuation marks. We used the type of the initial character of the morpheme for this feature. Definition of Anime-related Words We defined the anime-related NEs based on Sekine’s extended NE hierarchy (Sekine, 2008). The time and numerical representations were removed because they usually do not appear only in anime but also in real life. Place names were also removed because it is difficult to distinguish place names that appear only in anime from those that appear in real life. We had two kinds of anime-related words: interior and exterior. The former contains the titles of anime and the anime-related NEs that appear in the anime, and the latter is those that do not, such as the animators. Our system covered both of these. The interior anime-related words include the titles of anime, the names of chara"
sasaki-shinnou-2004-information,H91-1061,0,\N,Missing
sasaki-shinnou-2012-detection,S10-1012,0,\N,Missing
sasaki-shinnou-2012-detection,shinnou-sasaki-2008-division,1,\N,Missing
shinnou-1998-revision,W98-1120,1,\N,Missing
shinnou-1998-revision,C96-2157,0,\N,Missing
shinnou-1998-revision,C96-1071,0,\N,Missing
shinnou-1998-revision,C96-1039,0,\N,Missing
shinnou-1998-revision,C92-4199,0,\N,Missing
shinnou-1998-revision,C92-1019,0,\N,Missing
shinnou-1998-revision,C96-1079,0,\N,Missing
shinnou-1998-revision,A97-1029,0,\N,Missing
shinnou-2002-learning,P00-1069,0,\N,Missing
shinnou-2002-learning,P95-1026,0,\N,Missing
shinnou-2002-learning,P94-1013,0,\N,Missing
shinnou-2002-learning,W99-0613,0,\N,Missing
shinnou-sasaki-2004-semi,P95-1026,0,\N,Missing
shinnou-sasaki-2004-semi,W03-0406,1,\N,Missing
shinnou-sasaki-2008-division,S01-1033,0,\N,Missing
shinnou-sasaki-2008-division,P90-1034,0,\N,Missing
shinnou-sasaki-2008-division,W03-0406,1,\N,Missing
shinnou-sasaki-2010-detection,S01-1008,0,\N,Missing
W03-0406,P02-1044,0,0.0187737,"ptimum iteration number in the EM algorithm. Many problems in natural language processing can be converted into classification problems, and be solved by an inductive learning method. This strategy has been very successful, but it has a serious problem in that an inductive learning method requires labeled data, which is expensive because it must be made manually. To overcome this problem, unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently(Blum and Mitchell, 1998)(Yarowsky, 1995)(Park et al., 2000)(Li and Li, 2002). Among these methods, Minoru Sasaki Department of Computer and Information Sciences, Ibaraki University 4-12-1 Nakanarusawa, Hitachi, Ibaraki 316-8511 JAPAN sasaki@cis.ibaraki.ac.jp the method using the EM algorithm proposed by the paper(Nigam et al., 2000), which is referred to as the EM method in this paper, is the state of the art. However, the target of the EM method is text classification. It is hoped that this method can be applied to WSD, because WSD is the most important problem in natural language processing. The EM method works well in text classification, but often causes worse cla"
W03-0406,P00-1069,0,0.017702,"s to estimate the optimum iteration number in the EM algorithm. Many problems in natural language processing can be converted into classification problems, and be solved by an inductive learning method. This strategy has been very successful, but it has a serious problem in that an inductive learning method requires labeled data, which is expensive because it must be made manually. To overcome this problem, unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently(Blum and Mitchell, 1998)(Yarowsky, 1995)(Park et al., 2000)(Li and Li, 2002). Among these methods, Minoru Sasaki Department of Computer and Information Sciences, Ibaraki University 4-12-1 Nakanarusawa, Hitachi, Ibaraki 316-8511 JAPAN sasaki@cis.ibaraki.ac.jp the method using the EM algorithm proposed by the paper(Nigam et al., 2000), which is referred to as the EM method in this paper, is the state of the art. However, the target of the EM method is text classification. It is hoped that this method can be applied to WSD, because WSD is the most important problem in natural language processing. The EM method works well in text classification, but often"
W03-0406,shinnou-2002-learning,1,0.821078,"Missing"
W03-0406,P95-1026,0,0.680427,"opose two methods to estimate the optimum iteration number in the EM algorithm. Many problems in natural language processing can be converted into classification problems, and be solved by an inductive learning method. This strategy has been very successful, but it has a serious problem in that an inductive learning method requires labeled data, which is expensive because it must be made manually. To overcome this problem, unsupervised learning methods using huge unlabeled data to boost the performance of rules learned by small labeled data have been proposed recently(Blum and Mitchell, 1998)(Yarowsky, 1995)(Park et al., 2000)(Li and Li, 2002). Among these methods, Minoru Sasaki Department of Computer and Information Sciences, Ibaraki University 4-12-1 Nakanarusawa, Hitachi, Ibaraki 316-8511 JAPAN sasaki@cis.ibaraki.ac.jp the method using the EM algorithm proposed by the paper(Nigam et al., 2000), which is referred to as the EM method in this paper, is the state of the art. However, the target of the EM method is text classification. It is hoped that this method can be applied to WSD, because WSD is the most important problem in natural language processing. The EM method works well in text classi"
W16-1708,I08-7018,0,0.0341758,"seeking to locate and classify elements in text into predefined categories, such as the names of people, organizations, and locations, and has been studied for a long time. Information Retrieval and Extraction Exercise (IREX)1 defined the nine tags including eight types of NEs, i.e., organization, person, artifact, date, time, money, and percent as well as the option tag for shared task of Japanese NER. However, only newswires were used for this task. For the researches of NER, Hashimoto et al. (2008) generated extended NE corpus based on the Balanced Corpus of Contemporary Japanese (BCCWJ) (Maekawa, 2008)2 . Tokunaga et al. (2015) analyzed the eye-tracking data of annotators of NER task. Sasada et al. (2015) proposed the NE recognizer which is trainable from partially annotated data. In 2014, researchers analyzed the errors of Japanese NER using the newly tagged NE corpus of BCCWJ, which consists of six genres as Japanese NLP Project Next 3 (Iwakura, 2015; Hirata and Komachi, 2015; Ichihara et al., 2015). Ichihara et al. (2015) investigated the performance of the existing NE recognizer and showed that the errors increased in the genres far from the training data of the NE recognizer. This pape"
W16-1708,I08-2080,0,0.152098,"greement and Kappa coefficient are calculated as equ. (1) and equ. (2) respectively when the numbers of tag matching between two annotaters are as shown in Table 1. n ∑ d = (1) a00 a00 n ∑ aii − i=1 κ = 3 Comparison of Annotating Method aii i=1 2 (a00 ) − This paper compared the following two methods to annotate a corpus. n ∑ i=1 n ∑ ai0 a0i (2) ai0 a0i i=1 The precisions, the recalls, and the F-measures are calculated as equ. (3), equ. (4), and equ. (5) when we have the set of tags as Figure 1. KNP+M Semi-automatic annotation, which is revising the results of the existing NE recognizer: KNP (Sasano and Kurohashi, 2008) 4 p = Manual Fully manual annotation, whichi is annotating NEs only by hand r = 1 http://nlp.cs.nyu.edu/irex/index-j.html http://pj.ninjal.ac.jp/corpus center/bccwj/ 3 https://sites.google.com/site/projectnextnlp/ 4 http://nlp.ist.i.kyoto-u.ac.jp/index.php?KNP 2 f 60 = n(x) n(c) n(x) n(a) 2pr p+r (3) (4) (5) 4 Experiment Method KNP+M Manual Both We used 136 texts extracted from BCCWJ, which are available as ClassA5 . BCCWJ consists of six genres, “Q & A sites” (OC), “white papers” (OW), “blogs” (OY), “books” (PB), “magazines” (PM), and “newswires” (PN). Table 2 shows the summary of the number"
W16-1708,W10-1804,0,0.0233392,"erage. However they also indicated that sometimes fully manual annotation should be used for some texts whose genres are far from its training data. In addition, the experiments using the annotated corpora via semi-automatic and fully manual annotation as training data for machine learning indicated that the F-measures sometimes could be better for some texts when we used manual annotation than when we used semi-automatic annotation. 1 2 Related Work Snow et al. (2008) evaluated non-expert annotations through comparing with expert annotations from the point of view of time, quality, and cost. Alex et al. (2010) proposed agile data annotation, which is iterative, and compared it with the traditional linear annotation method. van der Plas et al. (2010) described the method to annotate semantic roles to the French corpus using English template to investigate the cross-lingual validity. Marcus et al. (1993) compared the semi-automatic and fully manual annotations to develop the Penn Treebank on the POS tagging task and the bracketing task. However, as far as we know, there is no paper which compared the semi-automatic and Introduction The crowdsourcing made annotation of the training data cheaper and fa"
W16-1708,W10-1814,0,0.046708,"Missing"
W16-1708,W16-2706,1,\N,Missing
W18-3408,W17-4419,0,0.0328533,"s required for fine-tuning. Some works improved the word embeddings using external knowledges such as dictionaries. (Yu and Dredze, 2014) changed the loss function to use pre-knowledges and improved the word embeddings. (Faruqui et al., 2015) proposed to use retrofitting, which is an approach where the word embeddings obtained from a huge corpus are re-learned using external knowledges. Fine-tuning is one of the methods for transfer learning (Pan and Yang, 2009). There are also much work about multi-task learning, which is another approach often used for transfer learning for neural networks (Aguilar et al., 2017) (von D¨aniken and Cieliebak, 2017). 3 4 Effect of Domain Shift for Nwjc2vec We demonstrate that even nwjc2vec, which is a word embeddings obtained from a huge corpus, NWJC, has a problem posed by domain shift in this section. 4.1 Mai2Vec To show this problem, we firstly created word embeddings from newspapers collected for seven years: Mainichi Shimbun newspaper articles from 1993 to 1999. We removed headlines and tables and extracted only sentences. The sentences were divided into words and the words were used for inputs into word2vec. The corpus had 6,791,403 sentences. We used MeCab-0.996"
W18-3408,D15-1036,0,0.0373161,"guage model is higher when the quality of the word embeddings used in the LSTM is higher. Usually, word embeddings are learned from the same corpus as the training corpus for a language model. However, we used the word embeddings to be evaluated instead of the word embeddings learned together with the language model (cf. Figure1). We believe that we can evaluate the quality of the word embeddings by evaluating the perplexity of the language model when they are used in a LSTM. Related Work Generally, effectiveness of word embeddings depends on tasks and target domains of the tasks. Therefore, (Schnabel et al., 2015) proposed tuning of word embeddings according to tasks and their target domains. The simplest tuning is fine-tuning, which is an approach where learned word embeddings are used for the initial values and tuned using an additional corpus. Its effectiveness has been shown for object recognition (Agrawal et al., 2014), named entity recognition (Lee et al., 2017), and many other tasks. Usually, a large target corpus is required for fine-tuning. Some works improved the word embeddings using external knowledges such as dictionaries. (Yu and Dredze, 2014) changed the loss function to use pre-knowledg"
W18-3408,P14-2089,0,0.0345485,"nd target domains of the tasks. Therefore, (Schnabel et al., 2015) proposed tuning of word embeddings according to tasks and their target domains. The simplest tuning is fine-tuning, which is an approach where learned word embeddings are used for the initial values and tuned using an additional corpus. Its effectiveness has been shown for object recognition (Agrawal et al., 2014), named entity recognition (Lee et al., 2017), and many other tasks. Usually, a large target corpus is required for fine-tuning. Some works improved the word embeddings using external knowledges such as dictionaries. (Yu and Dredze, 2014) changed the loss function to use pre-knowledges and improved the word embeddings. (Faruqui et al., 2015) proposed to use retrofitting, which is an approach where the word embeddings obtained from a huge corpus are re-learned using external knowledges. Fine-tuning is one of the methods for transfer learning (Pan and Yang, 2009). There are also much work about multi-task learning, which is another approach often used for transfer learning for neural networks (Aguilar et al., 2017) (von D¨aniken and Cieliebak, 2017). 3 4 Effect of Domain Shift for Nwjc2vec We demonstrate that even nwjc2vec, whic"
Y13-1043,Y12-1008,0,0.0184405,"2010). Plank measured the similarity among each the domain in parsing, and chose the most suitable source domain in oder to analyze the target domain (Plank and van Noord, 2011). Ponomareva (Ponomareva and Thelwall, 2012) and Remus (Remus, 2012) used the similarity among the domains for parameter of learning in sentiment classification. Those studies measured the similarity for every task. It is thought that the similarity among the domains depend on the target words in WSD. Komiya changed the learning methods for each target word by using the property1 including the distance between domains (Komiya and Okumura, 2012) (Komiya and Okumura, 2011). The Weight in the Source Domain In this paper, the topic features are used as follows: B + tp(T) + tp(S+T) + r * tp(S) A problem occurs a apposite setting of the weight r. It is considered that the weight r is the degree of the general knowledge which the source domain has. 1 All those property can be called the similarity among the domains 417 PACLIC-27 4.2 Setting of the weight r 5 Experiments Measuring between the source and the target domains is mean that separating the common knowledge of the both domains and the specific knowledge because the similarity is in"
Y13-1043,P10-1116,0,0.0397743,"Missing"
Y13-1043,P11-1157,0,0.0509181,"Missing"
Y13-1043,S07-1060,0,0.025432,"Missing"
Y13-1043,D07-1109,0,0.0913581,"of weight r is approximately 1. In contrast, when it can be determined that the topic features made from the source domain are not effective for WSD, the value of the weight r is approximately 0. The weight r is set by following equation: r= p(zi )p(d|zi ) i=1 p(w|zi ) for each word can be obtained by using Latent Dirichlet Allocation (LDA) (Blei et al., 2003)that is one of the topic models. Soft clustering can be done by using LDA and regarding the topic zi as a cluster. Suitable p(w|zi ) in each domain is obtained by using each domain corpus and LDA. There are several studies (Li et al., )(Boyd-Graber et al., 2007)(Boyd-Graber and Blei, ) that use information of p(w|zi ) for WSD, and a hard tagging approach (Cai et al., 2007) is used in this paper . The hard tagging approach is a method that give the word w the topic of the highest relevance zˆi . KL(T, S + T ) KL(T, S + T ) + KL(S, S + T ) where S is the source domain corpus, T is the target domain corpus, and S+T is the combined domain corpus; further, KL(A,B) is the Kullback Leibler (KL) divergence of A on criterion B. In our experiments, we chose three domains, PB (books), OC (Yahoo! Chie Bukuro), and PN (news) in the BCCWJ corpus, and selected 17 a"
Y13-1043,D07-1108,0,0.0182143,"Missing"
Y13-1043,P07-1033,0,0.0235998,"eflecting the knowledge of the target domain. (3) , which has the weight of the knowledge of the target domain, is also a promising method. A problem occurs that how tp(S) is used. Currently, the key to a solution is how the knowledge of the source domain is used in domain adaptation. When the knowledge of the source domain is used, it does not necessarily improve the accuracy of WSD, and sometimes actually reduce the accuracy. Because of this, there is no guarantee that (4) is better than (1), (2) and (3). (5) that uses tp(S) is a promising method. This idea is similar to Daum´e (Daum´e III, Hal, 2007). In study of Daum´e, vector xs of training data in the source domain is mapped to augmented input space (xs , xs , 0), and vector xt of test data in the target domain is mapped to augmented input space (0, xt , xt ). Classification problems are solved by using the augmented vector. This is known as the very simply and the high effectiveness method. This method is thought that an effect shows up in domain adaptation because the weight is learned by overlapping the characteristics common to the source and the target domain. It can be considered that (5) is added the knowledge tp(S+T) common to"
Y13-1043,I11-1124,0,0.0178125,"imilarity among each the domain in parsing, and chose the most suitable source domain in oder to analyze the target domain (Plank and van Noord, 2011). Ponomareva (Ponomareva and Thelwall, 2012) and Remus (Remus, 2012) used the similarity among the domains for parameter of learning in sentiment classification. Those studies measured the similarity for every task. It is thought that the similarity among the domains depend on the target words in WSD. Komiya changed the learning methods for each target word by using the property1 including the distance between domains (Komiya and Okumura, 2012) (Komiya and Okumura, 2011). The Weight in the Source Domain In this paper, the topic features are used as follows: B + tp(T) + tp(S+T) + r * tp(S) A problem occurs a apposite setting of the weight r. It is considered that the weight r is the degree of the general knowledge which the source domain has. 1 All those property can be called the similarity among the domains 417 PACLIC-27 4.2 Setting of the weight r 5 Experiments Measuring between the source and the target domains is mean that separating the common knowledge of the both domains and the specific knowledge because the similarity is intrinsically measured by com"
Y13-1043,W10-2605,0,0.0396785,"Missing"
Y15-1005,I08-2108,0,0.0606387,"Missing"
Y15-1005,D07-1109,0,0.478033,"ation (WSD). SWSM assumes that the sense distribution of surrounding words varies according to the sense of a polysemous word. For instance, a word “可能性” (possibility) has three senses according to the Electronic Dictionary Research (EDR) electronic dictionary (Miyoshi et al., 1996): (1) The ability to do something well There are many methods of all-words WSD. Pedersen et al. (2005) proposed calculation of the semantic relatedness of the word senses of ambiguous words and their surrounding words. Some papers have reported that methods using topic models (Blei et al., 2003) are most effective. Boyd-Graber et al. (2007) proposed a model, called Latent Dirichlet Allocation with WORDNET (LDAWN), which was a model where the probability distributions of words that the topics had were replaced with a word generation process on WordNet: WORDNET-WALK. They ap35 29th Pacific Asia Conference on Language, Information and Computation pages 35 - 43 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Kanako Komiya, Yuto Sasaki, Hajime Morita, Minoru Sasaki, Hiroyuki Shinnou and Yoshiyuki Kotani PACLIC 29 plied the topic model to unsupervised English allwords WSD. Although Guo and Diab (2011) also used the to"
Y15-1005,D11-1051,0,0.0184708,"effective. Boyd-Graber et al. (2007) proposed a model, called Latent Dirichlet Allocation with WORDNET (LDAWN), which was a model where the probability distributions of words that the topics had were replaced with a word generation process on WordNet: WORDNET-WALK. They ap35 29th Pacific Asia Conference on Language, Information and Computation pages 35 - 43 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Kanako Komiya, Yuto Sasaki, Hajime Morita, Minoru Sasaki, Hiroyuki Shinnou and Yoshiyuki Kotani PACLIC 29 plied the topic model to unsupervised English allwords WSD. Although Guo and Diab (2011) also used the topic model and WordNet, they also used WordNet as a lexical resource for sense definitions and they did not use its conceptual structure. They reported that the performance of their system was comparable with that reported by Boyd-Graber et al. There has been little work, on the other hand, on unsupervised Japanese all-words WSD. As far as we know, there has only been one paper (Baldwin et al., 2008) and there have been no reported methods that have used the topic model. We think this is because ambiguous words in Japanese tend to have similar topics since coarse semantic polys"
Y15-1005,O97-1002,0,0.103016,"all the word senses of w. probability P (si ) was calculated as: P (si ) = f req(si ) , N (4) where N denotes the number of word tokens. Figure 3 demonstrates the example of the conceptual structure1 . The nodes A∼F represent the 1 The leaf concepts below C, D, E, and F are omitted. PACLIC 29 SWSM uses these counted frequencies to calculate the transition parameter α so that the transition probabilities to each concept are proportional to the word sense frequencies of the surrounding words. We calculate αsi ,sj , i.e., the transition probability from hypernym si to hyponym sj , like that in (Jiang and Conrath, 1997) as: αsi ,sj = P (sj |si ) = P (si , sj ) P (sj ) = . P (si ) P (si ) (5) In addition, probability P (si ) is calculated as: Figure 3: Example of Concept Structure P (si ) = concepts and (a)∼(c) represent the words, which indicates that word (a) is a polyseme that have two word senses, i.e., C and D. When word (a) appeared twice and word (b) appeared once, the probabilities are as illustrated in Figure 3. Note that C and D share the frequencies of word (a). A Turing estimator (Gale and Sampson, 1995) was used for smoothing with rounding of the weighted frequencies. Concept abstraction sometime"
Y15-1005,C96-2195,0,0.036333,"periments with an EDR Japanese corpus and a Concept Dictionary (Section 7) indicated that SWSM was effective for Japanese all-words WSD (Section 8) . We discuss the results (Section 9) and concludes this paper (Section 10) . This paper proposes a surrounding word sense model (SWSM) for unsupervised Japanese allwords Word Sense Disambiguation (WSD). SWSM assumes that the sense distribution of surrounding words varies according to the sense of a polysemous word. For instance, a word “可能性” (possibility) has three senses according to the Electronic Dictionary Research (EDR) electronic dictionary (Miyoshi et al., 1996): (1) The ability to do something well There are many methods of all-words WSD. Pedersen et al. (2005) proposed calculation of the semantic relatedness of the word senses of ambiguous words and their surrounding words. Some papers have reported that methods using topic models (Blei et al., 2003) are most effective. Boyd-Graber et al. (2007) proposed a model, called Latent Dirichlet Allocation with WORDNET (LDAWN), which was a model where the probability distributions of words that the topics had were replaced with a word generation process on WordNet: WORDNET-WALK. They ap35 29th Pacific Asia"
Y15-1005,E95-1016,0,0.10087,"that they compared three methods for concept abstraction, i.e, flat depth, flat size, and flat probability methods, by using the EDR concept dictionary, and the flat probability method was the best. Therefore, we used the flat probability method for concept abstraction. The flat probability method consists of two steps. First, there is a search for nodes from the root node in depth first order. Second, if the concept probability calculated based on the corpus is less than a threshold value, the concept and its hyponym concepts are mapped onto its hypernym concept. We employed the methods of (Ribas, 1995) and (McCarthy, 1997) to calculate the concept probability. Ribas (1995) calculated the frequency of sense s as: ∑ |senses(w) ∈ U (s)| count(w), f req(s) = |senses(w)| w (3) where senses(w) denotes the possible senses of a word w, U (s) denotes concept s and its hyponym concepts, and count(w) denotes the frequency of word w. This equation weights count(w) by the ratio of concept s and its hyponym concepts in all the word senses of w. probability P (si ) was calculated as: P (si ) = f req(si ) , N (4) where N denotes the number of word tokens. Figure 3 demonstrates the example of the conceptual"
Y15-1005,shirai-2002-construction,0,0.0635721,"of one sentence. Type of Text The Nikkei The Asahi Shimbun AERA Heibonsha World Encyclopedia Encyclopedic Dictionary of Computer Science Magazines Collections Docs 5,018 91,400 49,589 Word tokens 121,301 2,272,555 1,183,897 10,072 284,059 13,578 357,607 21,199 16,946 528,452 368,285 Table 1: Summary of Sub-corpora. We used the Nikkei for evaluation. The other six sub-corpora were used for pre-processing in an unsupervised manner. The EDR Japanese corpus did not include the basic forms of words. Thus we used a morphological analyzer, Mecab4 , to identify the basic forms of words in the corpus. Shirai (2002) set up the three difficulty classes listed in Table 2. Tables 7 and 3 indicate the number of word types, noun tokens, and verb tokens according to difficulty and the average polysemy 4 https://github.com/jordwest/mecab-docs-en PACLIC 29 of target words according to difficulty. Only words that appeared more than four times in the corpus were classified based on difficulty. Difficulty Easy Normal Hard Entoropy E(w) < 0.5 0.5 ≤ E(w) < 1 1 ≤ E(w) Table 2: Difficulty of disambiguation Difficulty All Easy Normal Hard Word types 4,822 399 337 105 Tokens(N) 12,149 3,630 2,929 1,028 Tokens(V) 6,199 1,"
Y15-1005,H05-1052,0,\N,Missing
Y15-1057,W06-1615,0,0.245609,") = PT (y|x). Learning under covariate shift is regarded as weighted learning, where the weight is set to the probability density ratio PT (x)/PS (x). The feature-based method is a method that maps the source and target features spaces to a common features space to maintain important characteristics of both domains by reducing the difference between 496 29th Pacific Asia Conference on Language, Information and Computation pages 496 - 503 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Hiroyuki Shinnou, Liying Xiao, Minoru Sasaki and Kanako Komiya PACLIC 29 domains. The paper (Blitzer et al., 2006) proposed the dimension reduction method called structural correspondence learning (SCL). The paper (Daum´e III, Hal, 2007) offered a weighting system for features . In this study, vector xs of the training data in the source domain is mapped to an augmented input space (xs , xs , 0), and vector xt of the training data in the target domain is mapped to an augmented input space (0, xt , xt ). The classifier learned from the augmented vectors solves the classification problem. Daum´e’s method assumes that an effect can be determined by overlapping the characteristics that are common to the sourc"
Y15-1057,P07-1056,0,0.109728,"ning for real problems, there is often problems in domain adaptation. In general, the supervised learning is used to create a classifier which is usually using a learning algorithm such as support vector machine (SVM) by labeled training data, then it is possible to identify the label of the test data using this classifier. In this case, the problem is that the domain of training data and test data is different, so it is a problem of domain adaptation (Søgaard, 2013). As a typical example, there is a sentiment analysis task to judge whether a review article for a commodity is positive or not (Blitzer et al., 2007). For example, if we use review articles for ”book” as the training data to make a classifier, the classifier can not correctly identify the review articles for ”movie” which is in another domain. In addition to the emotion analysis, supervised learning such as morphological analysis (Mori, 2012), parsing (Sagae and Tsujii, 2007), word sense disambiguation (Shinnou et al., 2015) (Komiya and Okumura, 2012) (Komiya and Okumura, 2011) is utilized in all tasks, it is possible that the domain adaptation problems come into being. In general, the method of the domain adaptation can be divided into in"
Y15-1057,P07-1033,0,0.028271,"o PT (x)/PS (x). The feature-based method is a method that maps the source and target features spaces to a common features space to maintain important characteristics of both domains by reducing the difference between 496 29th Pacific Asia Conference on Language, Information and Computation pages 496 - 503 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Hiroyuki Shinnou, Liying Xiao, Minoru Sasaki and Kanako Komiya PACLIC 29 domains. The paper (Blitzer et al., 2006) proposed the dimension reduction method called structural correspondence learning (SCL). The paper (Daum´e III, Hal, 2007) offered a weighting system for features . In this study, vector xs of the training data in the source domain is mapped to an augmented input space (xs , xs , 0), and vector xt of the training data in the target domain is mapped to an augmented input space (0, xt , xt ). The classifier learned from the augmented vectors solves the classification problem. Daum´e’s method assumes that an effect can be determined by overlapping the characteristics that are common to the source and target domains. Although these methods for domain adaption often work well, while the differences between the domains"
Y15-1057,I11-1124,1,0.816923,"lem of domain adaptation (Søgaard, 2013). As a typical example, there is a sentiment analysis task to judge whether a review article for a commodity is positive or not (Blitzer et al., 2007). For example, if we use review articles for ”book” as the training data to make a classifier, the classifier can not correctly identify the review articles for ”movie” which is in another domain. In addition to the emotion analysis, supervised learning such as morphological analysis (Mori, 2012), parsing (Sagae and Tsujii, 2007), word sense disambiguation (Shinnou et al., 2015) (Komiya and Okumura, 2012) (Komiya and Okumura, 2011) is utilized in all tasks, it is possible that the domain adaptation problems come into being. In general, the method of the domain adaptation can be divided into instance-based method and feature-based method (Pan and Yang, 2010). Instance-based method is a method of learning using weighted training data. Learning under covariate shift (Sugiyama and Kawanabe, 2011) is typical in this method. The covariate shift means the assumption that PS (x) 6= PT (x), PS (y|x) = PT (y|x). Learning under covariate shift is regarded as weighted learning, where the weight is set to the probability density rat"
Y15-1057,Y12-1008,1,0.842426,"different, so it is a problem of domain adaptation (Søgaard, 2013). As a typical example, there is a sentiment analysis task to judge whether a review article for a commodity is positive or not (Blitzer et al., 2007). For example, if we use review articles for ”book” as the training data to make a classifier, the classifier can not correctly identify the review articles for ”movie” which is in another domain. In addition to the emotion analysis, supervised learning such as morphological analysis (Mori, 2012), parsing (Sagae and Tsujii, 2007), word sense disambiguation (Shinnou et al., 2015) (Komiya and Okumura, 2012) (Komiya and Okumura, 2011) is utilized in all tasks, it is possible that the domain adaptation problems come into being. In general, the method of the domain adaptation can be divided into instance-based method and feature-based method (Pan and Yang, 2010). Instance-based method is a method of learning using weighted training data. Learning under covariate shift (Sugiyama and Kawanabe, 2011) is typical in this method. The covariate shift means the assumption that PS (x) 6= PT (x), PS (y|x) = PT (y|x). Learning under covariate shift is regarded as weighted learning, where the weight is set to"
Y15-1057,W10-0104,0,0.0744209,"Missing"
Y15-1057,D07-1111,0,0.0352628,"this case, the problem is that the domain of training data and test data is different, so it is a problem of domain adaptation (Søgaard, 2013). As a typical example, there is a sentiment analysis task to judge whether a review article for a commodity is positive or not (Blitzer et al., 2007). For example, if we use review articles for ”book” as the training data to make a classifier, the classifier can not correctly identify the review articles for ”movie” which is in another domain. In addition to the emotion analysis, supervised learning such as morphological analysis (Mori, 2012), parsing (Sagae and Tsujii, 2007), word sense disambiguation (Shinnou et al., 2015) (Komiya and Okumura, 2012) (Komiya and Okumura, 2011) is utilized in all tasks, it is possible that the domain adaptation problems come into being. In general, the method of the domain adaptation can be divided into instance-based method and feature-based method (Pan and Yang, 2010). Instance-based method is a method of learning using weighted training data. Learning under covariate shift (Sugiyama and Kawanabe, 2011) is typical in this method. The covariate shift means the assumption that PS (x) 6= PT (x), PS (y|x) = PT (y|x). Learning under"
Y15-2025,W06-1615,0,0.0506123,"s. Consequently, the effects of the proposed method are confirmed. 2 Related Work Generally, methods for domain adaptation can be divided into instances-based method and featuresbased method (Pan and Yang, 2010). The instancesbased method is a learning method that gives weight to an instance of training data. Learning under covariate shift is typical method for this type. The features-based method is a method that maps the source and target features spaces to a common features space to maintain the important characteristics in each domain by reducing the difference between domains. The paper (Blitzer et al., 2006) proposed the dimension reduction method called structural correspondence learning (SCL). The paper (Pan et al., 2008) evaluated the distance between the spaces mapped in the source domain and the spaces mapped in the target domain by maximum mean discrepancy (MMD). They proposed a conversion method to minimize the distance called MMD embedding (MMDE). Moreover, the paper (Pan et al., 2011) improved MMDE and proposed a novel method called transfer component analysis (TCA). Adding weight to features is a features-based method. The paper (Daum´e III, Hal, 2007) offered a weighting system for fea"
Y15-2025,P06-1012,0,0.0118604,"ct can be determined by overlapping the characteristics that are common to the source and target domains. The domain adaptation problem is considered a data-sparse problem. Self-training and semisupervised learning (Chapelle et al., 2006) and active learning (Settles, 2010) (Rai et al., 2010) are useful for domain adaptation. At last, we introduce researches on domain adaptation for WSD. We assume that Pt (c|x) = Ps (c|x), but the assumption Pt (x|c) = Ps (x|c) is also possible. Under this assumption, we can solve domain adaptation for WSD by estimating Pt (c). AcPACLIC 29 tually, the papers (Chan and Ng, 2006) and (Chan and Ng, 2005) estimated Pt (c) by using EM algorithm to do it．The papers (Komiya and Okumura, 2012a), (Komiya and Okumura, 2011) and (Komiya and Okumura, 2012b) changed the learning method by the combination of source domain, target domain and target word. These studies are a kind of ensemble learning. In those learning methods, only the weight that is applied to data in source and target domain is different. 3 Domain Adaptation under Covariate Shift In this section, we show that weighted learning can solve a domain adaptation problem under assumption of covariate shift. We define t"
Y15-2025,P07-1033,0,0.0814138,"een domains. The paper (Blitzer et al., 2006) proposed the dimension reduction method called structural correspondence learning (SCL). The paper (Pan et al., 2008) evaluated the distance between the spaces mapped in the source domain and the spaces mapped in the target domain by maximum mean discrepancy (MMD). They proposed a conversion method to minimize the distance called MMD embedding (MMDE). Moreover, the paper (Pan et al., 2011) improved MMDE and proposed a novel method called transfer component analysis (TCA). Adding weight to features is a features-based method. The paper (Daum´e III, Hal, 2007) offered a weighting system for features. In this study, vector xs of the training data in the source domain is mapped to an augmented input space (xs , xs , 0), and xt is mapped to an augmented input space (0, xt , xt ). The classifier that learned from the augmented vectors solves the classification problem by the usual method. Daum´e’s method assumes that an effect can be determined by overlapping the characteristics that are common to the source and target domains. The domain adaptation problem is considered a data-sparse problem. Self-training and semisupervised learning (Chapelle et al.,"
Y15-2025,P07-1034,0,0.168452,"ers, pages 215 - 223 Shanghai, China, October 30 - November 1, 2015 Copyright 2015 by Hiroyuki Shinnou, Minoru Sasaki and Kanako Komiya PACLIC 29 two key points: (1) calculation of the weight of an instance and (2) weighted learning. For the first point, the probability density ratio w(x) = Pt (x)/Ps (x) is used theoretically as the weight of the instance x. There are two techniques for calculating the probability density ratio. The first is modeling PS (x) and PT (x) and then taking the ratio between them. The second is modeling w(x) directly. Several studies have examined the former method (Jiang and Zhai, 2007)(Saiki et al., 2008). However, to the best of our knowledge, the latter approach has not been attempted in NLP research. In this paper, we adopt unconstrained least squares importance fitting (uLSIF) as the second calculation (Kanamori et al., 2009). Actually, there are many methods to calculate probability density ratio (Sugiyama and Kawanabe, 2011). In this paper, we use uLSIF because it shows good performance and quick calculation time. uLSIF models w(x) with the sum of Nt pieces of basis functions ψl (x), where Nt is the number of target data. w(x) = Nt X αl ψl (x). l=1 Generally, a Gaussi"
Y15-2025,I11-1124,1,0.805756,"lem is considered a data-sparse problem. Self-training and semisupervised learning (Chapelle et al., 2006) and active learning (Settles, 2010) (Rai et al., 2010) are useful for domain adaptation. At last, we introduce researches on domain adaptation for WSD. We assume that Pt (c|x) = Ps (c|x), but the assumption Pt (x|c) = Ps (x|c) is also possible. Under this assumption, we can solve domain adaptation for WSD by estimating Pt (c). AcPACLIC 29 tually, the papers (Chan and Ng, 2006) and (Chan and Ng, 2005) estimated Pt (c) by using EM algorithm to do it．The papers (Komiya and Okumura, 2012a), (Komiya and Okumura, 2011) and (Komiya and Okumura, 2012b) changed the learning method by the combination of source domain, target domain and target word. These studies are a kind of ensemble learning. In those learning methods, only the weight that is applied to data in source and target domain is different. 3 Domain Adaptation under Covariate Shift In this section, we show that weighted learning can solve a domain adaptation problem under assumption of covariate shift. We define the loss function as l(x, c, d) where x, c and d denote an instance, the class of x and a classifier respectively. Thus, expected loss funct"
Y15-2025,Y12-1008,1,0.733684,"s. The domain adaptation problem is considered a data-sparse problem. Self-training and semisupervised learning (Chapelle et al., 2006) and active learning (Settles, 2010) (Rai et al., 2010) are useful for domain adaptation. At last, we introduce researches on domain adaptation for WSD. We assume that Pt (c|x) = Ps (c|x), but the assumption Pt (x|c) = Ps (x|c) is also possible. Under this assumption, we can solve domain adaptation for WSD by estimating Pt (c). AcPACLIC 29 tually, the papers (Chan and Ng, 2006) and (Chan and Ng, 2005) estimated Pt (c) by using EM algorithm to do it．The papers (Komiya and Okumura, 2012a), (Komiya and Okumura, 2011) and (Komiya and Okumura, 2012b) changed the learning method by the combination of source domain, target domain and target word. These studies are a kind of ensemble learning. In those learning methods, only the weight that is applied to data in source and target domain is different. 3 Domain Adaptation under Covariate Shift In this section, we show that weighted learning can solve a domain adaptation problem under assumption of covariate shift. We define the loss function as l(x, c, d) where x, c and d denote an instance, the class of x and a classifier respectiv"
Y15-2025,W10-0104,0,0.0417265,"Missing"
Y15-2026,I11-1124,1,0.694851,"abstract feature of the input data (basic feature) using Neural Network (Vincent et al., 2010). Recently it has been shown that a higher accuracy in voice and character recognition has been obtained using SdA (Le et al., 2012). We have applied this method to a domain adaptation for WSD and have shown that the abstract feature obtained through SdA can avoid the problem of domain adaptation. It is well-known from previous works that the most efficient methods for domain adaptation for WSD depend on the combination of training data (from the source domain) and test data (from the target domain) (Komiya and Okumura, 2011) (Komiya and Okumura, 2012). Furthermore, in an unsupervised domain adaptation method, even if the accuracy is improved in the combination of the source and target domains, the accuracy rate hardly improve. As a result, the accuracy rate on average of the method decreases, or remains the same. In other words, there are accuracy limitations with each method. In our method, we choose whether or not to apply SdA based on the similarity of features. Our method cannot be applied in the case for pair of do224 29th Pacific Asia Conference on Language, Information and Computation: Posters, pages 224 -"
Y15-2026,Y12-1008,1,0.824214,"ut data (basic feature) using Neural Network (Vincent et al., 2010). Recently it has been shown that a higher accuracy in voice and character recognition has been obtained using SdA (Le et al., 2012). We have applied this method to a domain adaptation for WSD and have shown that the abstract feature obtained through SdA can avoid the problem of domain adaptation. It is well-known from previous works that the most efficient methods for domain adaptation for WSD depend on the combination of training data (from the source domain) and test data (from the target domain) (Komiya and Okumura, 2011) (Komiya and Okumura, 2012). Furthermore, in an unsupervised domain adaptation method, even if the accuracy is improved in the combination of the source and target domains, the accuracy rate hardly improve. As a result, the accuracy rate on average of the method decreases, or remains the same. In other words, there are accuracy limitations with each method. In our method, we choose whether or not to apply SdA based on the similarity of features. Our method cannot be applied in the case for pair of do224 29th Pacific Asia Conference on Language, Information and Computation: Posters, pages 224 - 231 Shanghai, China, Octob"
Y15-2026,W06-1615,0,\N,Missing
Y15-2026,S10-1012,1,\N,Missing
Y16-2009,D14-1113,0,\N,Missing
Y17-1052,N15-1132,0,0.0265948,"Missing"
Y17-1052,D07-1109,0,0.0463327,"which each sense of every word is provided with definition sentences. Lesk’s method counts the overlapping words that are between the words used in the definition sentence and words that are surrounding the target word in the test sentence. Finally, the sense with the largest overlapping is selected. However, a knowledge based method generally cannot make use of the distribution of senses, resulting in low precision. There are various unsupervised learning methods (Yarowsky, 1995; Izquierdo-Bevi´a et al., 2006; Zhong and Ng, 2009). Recently, methods using a generative model have been studied (Boyd-Graber et al., 2007; Tanigaki et al., 2013; Tanigaki et al., 2015; Komiya et al., 2015). These methods have higher precision than knowledge based methods, in general, and can be expected to improve in the future. However, current unsupervised learning methods have the problem that the sense assigned to a word is a concept, because such a method essentially uses the following heuristic: “If the context surrounding the sense a is similar to the context surrounding the sense b, then a is similar to b.” In general, a and b are ambiguous, so we must measure the distance between a and b to use this heuristic. In the c"
Y17-1052,D14-1110,0,0.0650827,"Missing"
Y17-1052,C08-2011,0,0.0322881,"Missing"
Y17-1052,Y15-1005,1,0.416586,"sk’s method counts the overlapping words that are between the words used in the definition sentence and words that are surrounding the target word in the test sentence. Finally, the sense with the largest overlapping is selected. However, a knowledge based method generally cannot make use of the distribution of senses, resulting in low precision. There are various unsupervised learning methods (Yarowsky, 1995; Izquierdo-Bevi´a et al., 2006; Zhong and Ng, 2009). Recently, methods using a generative model have been studied (Boyd-Graber et al., 2007; Tanigaki et al., 2013; Tanigaki et al., 2015; Komiya et al., 2015). These methods have higher precision than knowledge based methods, in general, and can be expected to improve in the future. However, current unsupervised learning methods have the problem that the sense assigned to a word is a concept, because such a method essentially uses the following heuristic: “If the context surrounding the sense a is similar to the context surrounding the sense b, then a is similar to b.” In general, a and b are ambiguous, so we must measure the distance between a and b to use this heuristic. In the case which a and b are concepts, we can measure that distance. Howeve"
Y17-1052,S10-1094,0,0.0314686,"utation (PACLIC 31), pages 392–399 Cebu City, Philippines, November 16-18, 2017 c Copyright 2017 Hiroyuki Shinnou, Kanako Komiya, Minoru Sasaki and Shinsuke Mori 2 Related Work The availability of a supervised learning method for all-words WSD typically requires specifying the domain. Some systems using a supervised learning method have used all-words WSD tasks of SemEval07 (Navigli et al., 2007), but these systems have a problem with scalability. All-words WSD methods not using a supervised learning method are divided into two types: knowledge based methods and unsupervised learning methods (Kulkarni et al., 2010). Lesk’s method (Lesk, 1986), a well known classical knowledge based method uses a dictionary in which each sense of every word is provided with definition sentences. Lesk’s method counts the overlapping words that are between the words used in the definition sentence and words that are surrounding the target word in the test sentence. Finally, the sense with the largest overlapping is selected. However, a knowledge based method generally cannot make use of the distribution of senses, resulting in low precision. There are various unsupervised learning methods (Yarowsky, 1995; Izquierdo-Bevi´a"
Y17-1052,D14-1113,0,0.0194449,"Missing"
Y17-1052,neubig-mori-2010-word,1,0.789731,"Missing"
Y17-1052,P11-2093,1,0.770145,"g time (Navigli, 2009). However, a sense in many allwords WSD systems is defined as a concept, resulting in coarse granularity. Furthermore, the target language is generally English. Japanese all-words WSD has not been achieved, preventing easy access to it. Given this background, we created a Japanese all-words WSD system called KyWSD1 . KyWSD is 1 The substance of KyWSD is a model built using the Kyoto Text Analysis ToolKit (KyTea)2 , a learning system. By executing KyTea using this model, KyWSD accepts plain Japanese text, segments it into words, and assigns a sense to each segmented word (Neubig et al., 2011). Briefly KyTea is a system learning a morphological analysis model. We build KyWSD using KyTea because all-words WSD can be regarded as a kind of morphological analysis. Therefore, KyTea contains a mechanism for learning a model to adapt to a target domain. The ability to use this mechanism provides KyWSD with high adaptability. For example, adding training data to KyWSD, senses to all words, but a target sense. Thus, KyWSD is an appropriate system for domain adaptation. As seen above, KyWSD provides great value as new use of KyTea. We evaluated KyWSD using a Japanese dictionary task in Sense"
Y17-1052,P13-1087,0,0.0243832,"word is provided with definition sentences. Lesk’s method counts the overlapping words that are between the words used in the definition sentence and words that are surrounding the target word in the test sentence. Finally, the sense with the largest overlapping is selected. However, a knowledge based method generally cannot make use of the distribution of senses, resulting in low precision. There are various unsupervised learning methods (Yarowsky, 1995; Izquierdo-Bevi´a et al., 2006; Zhong and Ng, 2009). Recently, methods using a generative model have been studied (Boyd-Graber et al., 2007; Tanigaki et al., 2013; Tanigaki et al., 2015; Komiya et al., 2015). These methods have higher precision than knowledge based methods, in general, and can be expected to improve in the future. However, current unsupervised learning methods have the problem that the sense assigned to a word is a concept, because such a method essentially uses the following heuristic: “If the context surrounding the sense a is similar to the context surrounding the sense b, then a is similar to b.” In general, a and b are ambiguous, so we must measure the distance between a and b to use this heuristic. In the case which a and b are c"
Y17-1052,P95-1026,0,0.588205,"g methods (Kulkarni et al., 2010). Lesk’s method (Lesk, 1986), a well known classical knowledge based method uses a dictionary in which each sense of every word is provided with definition sentences. Lesk’s method counts the overlapping words that are between the words used in the definition sentence and words that are surrounding the target word in the test sentence. Finally, the sense with the largest overlapping is selected. However, a knowledge based method generally cannot make use of the distribution of senses, resulting in low precision. There are various unsupervised learning methods (Yarowsky, 1995; Izquierdo-Bevi´a et al., 2006; Zhong and Ng, 2009). Recently, methods using a generative model have been studied (Boyd-Graber et al., 2007; Tanigaki et al., 2013; Tanigaki et al., 2015; Komiya et al., 2015). These methods have higher precision than knowledge based methods, in general, and can be expected to improve in the future. However, current unsupervised learning methods have the problem that the sense assigned to a word is a concept, because such a method essentially uses the following heuristic: “If the context surrounding the sense a is similar to the context surrounding the sense b,"
Y17-1052,S01-1008,0,\N,Missing
Y18-1005,W06-1615,0,0.0965625,"e on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 standard method because of its simplicity and high ability. The method in the current research is an unsupervised approach. Unsupervised approaches can further be divided into two types: feature-based and instance-based (Pan and Yang, 2010). They are both weighted learning methods; feature-based methods give weights to features and instance-based methods give weights to instances. Among featurebased methods, the most representative method is structural correspondence learning (SCL) (Blitzer et al., 2006). In addition, CORAL (Sun et al., 2016) has attracted much attention for its simplicity and high ability in recent years. Moreover, the feature-based methods with deep learning (Glorot et al., 2011), the expanded CORAL (Sun and Saenko, 2016) and adversarial networks (Ganin and Lempitsky, 2015)(Tzeng et al., 2017) are also considered as the state of the art. On the other hand, instance-based methods have not been studied as much as feature-based methods. The instance-based method assumes a covariate shift. A covariate shift assumes PS (c|x) = PT (c|x) and PS (x) = PT (x). Under a covariate shif"
Y18-1005,P07-1056,0,0.398736,"Missing"
Y18-1005,P07-1033,0,0.247115,"Missing"
Y18-1005,Y18-1000,0,0.147615,"Missing"
Y18-1005,C16-1038,0,0.149869,"Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 standard method because of its simplicity and high ability. The method in the current research is an unsupervised approach. Unsupervised approaches can further be divided into two types: feature-based and instance-based (Pan and Yang, 2010). They are both weighted learning methods; feature-based methods give weights to features and instance-based methods give weights to instances. Among featurebased methods, the most representative method is structural correspondence learning (SCL) (Blitzer et al., 2006). In addition, CORAL (Sun et al., 2016) has attracted much attention for its simplicity and high ability in recent years. Moreover, the feature-based methods with deep learning (Glorot et al., 2011), the expanded CORAL (Sun and Saenko, 2016) and adversarial networks (Ganin and Lempitsky, 2015)(Tzeng et al., 2017) are also considered as the state of the art. On the other hand, instance-based methods have not been studied as much as feature-based methods. The instance-based method assumes a covariate shift. A covariate shift assumes PS (c|x) = PT (c|x) and PS (x) = PT (x). Under a covariate shift, PT (c|x) can be obtained by the weig"
Y18-1068,W06-1615,0,0.0682926,"T (c|x) and PS (x) = PT (x). Under covariate shift, the probability density ratio r is used as the weight of the instance data x in the source domain. The definition of r is as follows: r = PT (x)/PS (x). On the basis of weight learning, we obtain PT (c|x)(Sugiyama and Kawanabe, 2011). The feature-based method gives a weight to the feature of the data. After giving weights, any learning method becomes available, but SVM is typically used. The problem is how to give a weight to the feature. Among feature-based methods, the most representative method is structural correspondence learning (SCL) (Blitzer et al., 2006). The featurebased method can also be used to map the data from the source domain and target domain to the feature subspace W . Among these studies, MMD is a representative method (Borgwardt et al., 2006). Furthermore, CORAL (Sun et al., 2016) is simple and effective; therefore, it has attracted considerable attention in recent years. Furthermore, the domain adaptation using deep learning is regarded a feature-based method (Glorot et al., 2011). Ref. (Sun and Saenko, 2016) shows the expanded CORAL method, and Refs. (Ganin and Lempitsky, 2015) and (Tzeng et al., 2017) show the deep learning met"
Y18-1068,P07-1033,0,0.167968,"accuracy of each domain adaptation. By comparing the use of v,[v : e] or [v : g] 599 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 as feature vectors, the effectiveness of our proposed method is demonstrated. 2 Related Work Domain adaptation can be divided into two types: a supervised learning type that uses labeled data in the target domain and an unsupervised learning type that does not use labeled data in the target domain. In the supervised learning type, the method of Daum´e’s method (Daum´e III, Hal, 2007) is used as the standard method because simplicity and effectiveness. In this study, we deal with the unsupervised learning type. The unsupervised learning can further be divided into two types: the instance-based method and feature-based method (Pan and Yang, 2010). The instance-based method gives a weight to the instance, and weight learning is conducted. Many methods of this type assume the covariate shift which indicates that PS (c|x) = PT (c|x) and PS (x) = PT (x). Under covariate shift, the probability density ratio r is used as the weight of the instance data x in the source domain. The"
Y18-1068,W16-1609,0,0.106201,"es (MMD) (Borgwardt et al., 2006). In recent years, many methods using deep learning have been proposed (Patel et al., 2015). This study focuses on the sentiment analysis task, wherein the data is the document and W is the space of embeddings for the document. Furthermore, there are several methods to embed documents in the lowdimensional space, but there is no single embedding algorithm that can be used to respond to various domain adaptations from the diversity of domain differences. Therefore, we combine the embeddings of the document in this study, specifically the combination of doc2vec (Lau and Baldwin, 2016) and singular value decomposition (SVD). The document d is converted to the embedding e and g by using doc2vec and SVD respectively. Moreover we use the vector v obtained from the bag-of-words (BOW) model of the document d. Generally, v is used as the feature vector to learn the classifier. In this study, we use three connected vectors, namely [v : e : g] as feature vectors instead of v. In the experiment, we used an Amazon dataset that has three domains (“books”, “DVD” and “music”); thus we designed six types of domain adaptations. Thereafter, the method was evaluated on the basis of the aver"
Y18-1068,C16-1038,0,0.016238,"PT (c|x)(Sugiyama and Kawanabe, 2011). The feature-based method gives a weight to the feature of the data. After giving weights, any learning method becomes available, but SVM is typically used. The problem is how to give a weight to the feature. Among feature-based methods, the most representative method is structural correspondence learning (SCL) (Blitzer et al., 2006). The featurebased method can also be used to map the data from the source domain and target domain to the feature subspace W . Among these studies, MMD is a representative method (Borgwardt et al., 2006). Furthermore, CORAL (Sun et al., 2016) is simple and effective; therefore, it has attracted considerable attention in recent years. Furthermore, the domain adaptation using deep learning is regarded a feature-based method (Glorot et al., 2011). Ref. (Sun and Saenko, 2016) shows the expanded CORAL method, and Refs. (Ganin and Lempitsky, 2015) and (Tzeng et al., 2017) show the deep learning methods for domain adaptation. 3 Domain Adaptation Using Embeddings In this study, the target task is sentiment analysis: therefore, the data is a document. As a result, we can construct a shared feature subspace by using the embedding for the do"
Y18-1072,W17-4419,0,0.0147831,"e, 2015) by multitask learning of Chinese word segmentation. Peng and Dredze (2017b) improved the performance of the task using a modified dataset created with (He and Sun, 2017)4 . Peng and Dredze (2017a) proposed a multitask domain adaptation considering Chinese word segmentation and NER. Transfer learning methods were also used for a shared task on emerging and rare entity recognition (Derczynski et al., 2017)5 in the 3rd workshop on noisy user-generated text (WNUT-2017). The shared task defined emerging and rare entities and provided datasets of social media for detecting these entities. (Aguilar et al., 2017), which is a paper that got the first position in this shared task, used multi1 https://nlp.cs.nyu.edu/irex/index-e.html 2 https://sites.google.com/site/ projectnextnlpne/en 3 http://pj.ninjal.ac.jp/corpus_center/bccwj/ en/ 4 https://github.com/hltcoe/golden-horse 5 http://noisy-text.github.io/2017/ emerging-rare-entities.html lw(1) lw(2) vw(1) vw(2) ··· lw(n) Tag (Output) ··· Linear-chain CRF ··· Linear ··· Linear ··· Backward LSTM ··· Foward LSTM ··· vw(n) Word Embedding (Input) Figure 1: The neural network architecture of the proposed method. The dashed arrows indicate the dropout procedure"
Y18-1072,W17-4418,0,0.0256435,"Missing"
Y18-1072,E17-2113,0,0.0157372,"ns a model from wordlevel and character-level representations and obtained state-of-the-art performance in both tasks. There have been researches on NER using transfer learning. The followings are some examples. Qu et al. (2016) proposed a transfer learning method for NER in the case where not only domains but also labels of NER do not match. Peng and Dredze (2016) improved the performance of NER on Chinese social media (Peng and Dredze, 2015) by multitask learning of Chinese word segmentation. Peng and Dredze (2017b) improved the performance of the task using a modified dataset created with (He and Sun, 2017)4 . Peng and Dredze (2017a) proposed a multitask domain adaptation considering Chinese word segmentation and NER. Transfer learning methods were also used for a shared task on emerging and rare entity recognition (Derczynski et al., 2017)5 in the 3rd workshop on noisy user-generated text (WNUT-2017). The shared task defined emerging and rare entities and provided datasets of social media for detecting these entities. (Aguilar et al., 2017), which is a paper that got the first position in this shared task, used multi1 https://nlp.cs.nyu.edu/irex/index-e.html 2 https://sites.google.com/site/ pro"
Y18-1072,W18-3408,1,0.891607,"Missing"
Y18-1072,P16-1101,0,0.197824,"ER model with high performance when we have a small NE corpus and a large POS corpus at the same time. This paper proposes fine-tuning NER using POS tagging. We targeted at Japanese NER and POS tagging in the current study. There are many researches on Japanese NER like (Komiya et al., 2018) and (Iwakura et al., 2016). In addition, many researchers investigated NER using multitasking learning or joint learning like (Qu et al., 2016) and (Peng and Dredze, 2015). We investigated Japanese NER using fine-tuning (See Section 2). The proposed model is developed on the basis of a model described in (Ma and Hovy, 2016). We simplified the model by excluding a CNN for the character-level representations (See Section 3). We evaluated precision, recall, and F-measure based on the gold standard with experiments using the models (described in Section 4). We discuss the results by comparing them to those of the existing method without fine-tuning (See Section 5) and conclude our work (See Section 6). 2 Related Work NER has been studied for a long time. When we focus on Japanese NER, the Information Retrieval and Extraction Exercise (IREX) (Sekine and Isa632 32nd Pacific Asia Conference on Language, Information and"
Y18-1072,D15-1064,0,0.120991,"ng is also a sequential labeling task and POS are popular features for NER. Therefore, fine-tuning for NER using POS tagging would be a good way to learn an NER model with high performance when we have a small NE corpus and a large POS corpus at the same time. This paper proposes fine-tuning NER using POS tagging. We targeted at Japanese NER and POS tagging in the current study. There are many researches on Japanese NER like (Komiya et al., 2018) and (Iwakura et al., 2016). In addition, many researchers investigated NER using multitasking learning or joint learning like (Qu et al., 2016) and (Peng and Dredze, 2015). We investigated Japanese NER using fine-tuning (See Section 2). The proposed model is developed on the basis of a model described in (Ma and Hovy, 2016). We simplified the model by excluding a CNN for the character-level representations (See Section 3). We evaluated precision, recall, and F-measure based on the gold standard with experiments using the models (described in Section 4). We discuss the results by comparing them to those of the existing method without fine-tuning (See Section 5) and conclude our work (See Section 6). 2 Related Work NER has been studied for a long time. When we fo"
Y18-1072,P16-2025,0,0.0171412,"pus using non-expert annotators. In the research on the sequence labeling, NER and POS tagging are often selected as the target tasks at the same time. For example, Ma and Hovy (2016) proposed an end-to-end sequence labeling system that automatically learns a model from wordlevel and character-level representations and obtained state-of-the-art performance in both tasks. There have been researches on NER using transfer learning. The followings are some examples. Qu et al. (2016) proposed a transfer learning method for NER in the case where not only domains but also labels of NER do not match. Peng and Dredze (2016) improved the performance of NER on Chinese social media (Peng and Dredze, 2015) by multitask learning of Chinese word segmentation. Peng and Dredze (2017b) improved the performance of the task using a modified dataset created with (He and Sun, 2017)4 . Peng and Dredze (2017a) proposed a multitask domain adaptation considering Chinese word segmentation and NER. Transfer learning methods were also used for a shared task on emerging and rare entity recognition (Derczynski et al., 2017)5 in the 3rd workshop on noisy user-generated text (WNUT-2017). The shared task defined emerging and rare entiti"
Y18-1072,W17-2612,0,0.0176283,"example, Ma and Hovy (2016) proposed an end-to-end sequence labeling system that automatically learns a model from wordlevel and character-level representations and obtained state-of-the-art performance in both tasks. There have been researches on NER using transfer learning. The followings are some examples. Qu et al. (2016) proposed a transfer learning method for NER in the case where not only domains but also labels of NER do not match. Peng and Dredze (2016) improved the performance of NER on Chinese social media (Peng and Dredze, 2015) by multitask learning of Chinese word segmentation. Peng and Dredze (2017b) improved the performance of the task using a modified dataset created with (He and Sun, 2017)4 . Peng and Dredze (2017a) proposed a multitask domain adaptation considering Chinese word segmentation and NER. Transfer learning methods were also used for a shared task on emerging and rare entity recognition (Derczynski et al., 2017)5 in the 3rd workshop on noisy user-generated text (WNUT-2017). The shared task defined emerging and rare entities and provided datasets of social media for detecting these entities. (Aguilar et al., 2017), which is a paper that got the first position in this shared"
Y18-1072,D16-1087,0,0.165572,"ging because POS tagging is also a sequential labeling task and POS are popular features for NER. Therefore, fine-tuning for NER using POS tagging would be a good way to learn an NER model with high performance when we have a small NE corpus and a large POS corpus at the same time. This paper proposes fine-tuning NER using POS tagging. We targeted at Japanese NER and POS tagging in the current study. There are many researches on Japanese NER like (Komiya et al., 2018) and (Iwakura et al., 2016). In addition, many researchers investigated NER using multitasking learning or joint learning like (Qu et al., 2016) and (Peng and Dredze, 2015). We investigated Japanese NER using fine-tuning (See Section 2). The proposed model is developed on the basis of a model described in (Ma and Hovy, 2016). We simplified the model by excluding a CNN for the character-level representations (See Section 3). We evaluated precision, recall, and F-measure based on the gold standard with experiments using the models (described in Section 4). We discuss the results by comparing them to those of the existing method without fine-tuning (See Section 5) and conclude our work (See Section 6). 2 Related Work NER has been studied"
Y18-1072,sekine-isahara-2000-irex,0,0.191211,"e used nine kinds of tags defined by IREX, i.e., eight NE tag types (See Figure 2) and OPTIONAL tag7 for ambiguous NEs. Table 1 7 OPTIONAL tags do not have to be predicted. 634 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 Table 1: Summary of number of documents and NE tags Tags Documents ARTIFACT DATE LOCATION MONEY ORGANIZATION PERCENT PERSON TIME OPTIONAL All 1,174 747 3,567 5,463 390 3,676 492 3,840 502 585 19,262 shows a summary of the number of documents and NE tags. We used IREX CRL data (CRL) (Sekine and Isahara, 2000)8 , which is an annotated corpus that consists of 1,174 articles of the Japanese newspaper “The Mainichi Shimbun” collected from January 1st to 10th. In POS tagging, we used 21 POS tag types (See Figures 3 and 4) extracted from POS tag types used in UniDic9 . POS tags are annotated for the same articles as the NER task. In other words, we used the Mainichi Shimbun annotated with POS and NER tags. We used the IOBES format as a tagging scheme. We used NWJC2vec (Masayuki Asahara, 2018)10 , which is a 200 dimensional word2vec (Mikolov et al., 2013) model. This model trained from National Institute"
Y18-1072,W16-3919,0,0.0313169,"Missing"
Y18-1072,W17-4422,0,0.0252978,"Missing"
Y96-1019,C90-2070,0,0.0322264,"g is. This meaning is manually decided by observing nouns shown in (2). For example, the Japanese noun ""J*"" has meanings ""voice"" and ""opinion "", but ""r. "" in Bunrui-goi-hyou has only ""voice"" and does not have ""opinion 177 // . Our method points out that ""r4 "" in Bunrui-goi-hyou has a deficiency of a meaning, and shows some nouns such as ""SR (opinion)"" ,""NS (view)"" ,""1 (insistence)"" etc. which have the similar meaning to ""opinion"". By observing these nouns, we can manually decide that the lacking meaning of ""P"" is ""opinion"". The traditional research to acquire the unknown meaning was done by (Wilensky 1990). In his research, if a sense of a word is unknown, the sense is estimated from similar uses of other words to the use of the word. This approach, which estimates a feature of an unknown word from features of similar words, is taken to cope with the data sparseness problem of a corpus (Dagan 1993). In short, this approach is base on the idea that the unseen part can be estimated from seen similar parts. However, the simple use of this idea alone cannot find an unknown meaning of a word, because even similar words hardly have same polysemy to the word. For example, nouns (cheer)"", ""E PA (scream"
Y96-1019,C96-2205,1,\N,Missing
Y96-1019,P93-1022,0,\N,Missing
Y96-1019,P90-1034,0,\N,Missing
Y96-1019,H90-1071,0,\N,Missing
