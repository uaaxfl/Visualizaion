2020.acl-main.305,N12-1074,0,0.0278822,"t we consistently perform better both in conversation cold start and with varying degrees of sparsity of user history and conversation contexts. Lastly, to provide more insights into user interest dynamics, we inspect our model outputs and find that users indeed tend to engage in different types of conversations at different times, confirming the usefulness of tracking user preferences in real-time for conversation recommendation. 2 Related Work User Response Prediction. This work is in line with user response prediction, such as message popularity forecast with handcrafted response features (Artzi et al., 2012; Backstrom et al., 2013) and conversation trajectory with user interaction structures (Cheng et al., 2017b; Jiao et al., 2018; Zeng et al., 2019a). These works predict responses from general public, while we work on personalized recommendation and focus on user interest modeling. For recommendation, there are extensive efforts on post-level recommendation (Chen et al., 2012; Yan et al., 2012) and conversation-level (Chen et al., 2011; Zeng et al., 2018, 2019b). In contrast with them which assume static user interests, we capture how user interests change over time and take advantage of the re"
2020.acl-main.305,D17-1243,0,0.0974531,"ser history and conversation contexts. Lastly, to provide more insights into user interest dynamics, we inspect our model outputs and find that users indeed tend to engage in different types of conversations at different times, confirming the usefulness of tracking user preferences in real-time for conversation recommendation. 2 Related Work User Response Prediction. This work is in line with user response prediction, such as message popularity forecast with handcrafted response features (Artzi et al., 2012; Backstrom et al., 2013) and conversation trajectory with user interaction structures (Cheng et al., 2017b; Jiao et al., 2018; Zeng et al., 2019a). These works predict responses from general public, while we work on personalized recommendation and focus on user interest modeling. For recommendation, there are extensive efforts on post-level recommendation (Chen et al., 2012; Yan et al., 2012) and conversation-level (Chen et al., 2011; Zeng et al., 2018, 2019b). In contrast with them which assume static user interests, we capture how user interests change over time and take advantage of the recent advancement of dynamic product recommendation (Wu et al., 2017; Beutel et al., 2018). To recommend co"
2020.acl-main.305,D14-1179,0,0.0100025,"Missing"
2020.acl-main.305,D14-1181,0,0.00373426,"duce how we model the user interest dynamics with their chatting history in Section 3.1, followed by the description of conversation modeling in Section 3.2. Afterwards, Section 3.3 will present how we produce final recommendation outputs. Objective function and learning procedures will be finally presented in Section 3.4. 3.1 Message-level Modeling. We model messagelevel representation from its word sequence. Specifically, given u’s historical message m, we first use a pre-trained word embedding layer to map each word into a vector space, and then employ a Convolutional Neural Network (CNN) (Kim, 2014) encoder to model word occurrence with their neighbors. Afterwards, we output representation zm to reflect m’s content. User Interest Dynamic Modeling Given a sequence of chronologically ordered historical messages hm1 , m2 , · · · , m|u |i of a user u (|u |is the message number of u), a message therein corresponds to a word sequence wm . Our goal is to capture the temporal patterns from the sequence of user chatting messages and then produce the user interest representation. We employ two-level modeling — message level and user level. User-level Modeling. As shown in Wu et al. (2017), some us"
2020.acl-main.305,W02-0109,0,0.0915391,"th More-path Tech Learn Dataset Fun Figure 4: Distributions of conversation structure. “Onepath”, “Two-path”, and “More-path” indicate the conversation has 1, 2, and more root-to-leaf paths. versations contain two or more paths, illustrating complex who-replies-to-whom interactions in the tree structure (with the original post as the root node and in-reply-to relations as edges). Therefore, graph-structured encoder may be a suitable alternative for capturing rich turn interactions in Reddit conversations. Preprocessing. For all datasets, we applied open source natural language toolkit (NLTK) (Loper and Bird, 2002) for tokenization. Further, links were replaced by a generic tag “hURLi” and all number tokens were removed. In the experiments, we maintained a vocabulary with all the remaining tokens (including punctuation and emoticons). Model Settings. In training, we adopt negative sampling with sampling ratio of 5 (see Section 3.4). We also randomly sample 100 negative instances for each positive one during validation and test, to avoid unbalanced labels. For parameters, we initialize the word embedding layer with 300-dim Common Crawl version of Glove embedding (Pennington et al., 2014), and the dimensi"
2020.acl-main.305,D15-1178,0,0.0278975,"how user interests change over time and take advantage of the recent advancement of dynamic product recommendation (Wu et al., 2017; Beutel et al., 2018). To recommend conversations, we aim to learn user interest dynamics from chatting content and interaction behavior, which have never been explored in previous research. Conversation Structure Modeling. Our work is also related to previous work to understand how participants interact with each other in conversation structure. Earlier efforts focus on discovering word statistic patterns via probabilistic graphical models (Ritter et al., 2010; Louis and Cohen, 2015), which are unable to capture deep semantics embedded in complex interactions. Recent research points out the effectiveness to understand conversation structure from temporal dynamics (Cheng et al., 2017a; Jiao et al., 2018) and replying struc3332 - ?,? (Predicted Score) ? MLP Mechanism Attention Msg Encoder GCN GRU ?|?| … … … Bi-GRU ?? ?? ??? Msg Encoder Text Initialize User User Factor Embedding ?? ?? ?? ?? Figure 2: Overall structure of our model. The left module is to model user interest dynamics, whose results together with conversation representations derived from the right part are used"
2020.acl-main.305,D17-1159,0,0.05026,"4 . We design the model to capture user interests from both what they said in the past, and how they interacted with each other in the conversation structure. We first capture time-variant representations from user chatting history, where we assume user interests may change over time and therefore apply a gated recurrent unit (GRU) (Cho et al., 2014) to model time dependency. User interactions in the conversation context are then explored with both bidirectional gated recurrent unit (Bi-GRU) (Cho et al., 2014) for conversation turns’ chronological order and graph convolutional networks (GCN) (Marcheggiani and Titov, 2017) for in-reply-to relations. Both representations are learned to encode how participants formed the conversation structure, including what they said and whom they replied to. Next, we propose a user-aware attention to convey the user interest dynamics, which is further put over an interactionencoded conversation to measure whether its ongoing contexts fit a user’s current interests. Finally, we predict how likely a user will engage in a conversation, as a result of recommendation. To the best of our knowledge, we are the first to study dynamic online conversation recommendation and to explore t"
2020.acl-main.305,C18-1322,0,0.0175857,"g et al., 2017a; Jiao et al., 2018) and replying struc3332 - ?,? (Predicted Score) ? MLP Mechanism Attention Msg Encoder GCN GRU ?|?| … … … Bi-GRU ?? ?? ??? Msg Encoder Text Initialize User User Factor Embedding ?? ?? ?? ?? Figure 2: Overall structure of our model. The left module is to model user interest dynamics, whose results together with conversation representations derived from the right part are used for producing final prediction. Predicted score yˆu,c indicates how likely u will engage in c. “Msg Encoder” mainly contains two layers: word embedding layer and CNN modeling layer. ture (Miura et al., 2018; Zayats and Ostendorf, 2018; Zeng et al., 2019b). The two factors are coupled in our interaction modeling and their joint effects for dynamic conversation recommendation, ignored by prior work, will be extensively studied here. 3 Our Dynamic Conversation Recommendation Model This section describes our dynamic conversation recommendation model, whose overall structure is shown in Figure 2. In the following, we will first introduce how we model the user interest dynamics with their chatting history in Section 3.1, followed by the description of conversation modeling in Section 3.2. Afterwards,"
2020.acl-main.305,D14-1162,0,0.082686,"Missing"
2020.acl-main.305,N10-1020,0,0.0591909,"interests, we capture how user interests change over time and take advantage of the recent advancement of dynamic product recommendation (Wu et al., 2017; Beutel et al., 2018). To recommend conversations, we aim to learn user interest dynamics from chatting content and interaction behavior, which have never been explored in previous research. Conversation Structure Modeling. Our work is also related to previous work to understand how participants interact with each other in conversation structure. Earlier efforts focus on discovering word statistic patterns via probabilistic graphical models (Ritter et al., 2010; Louis and Cohen, 2015), which are unable to capture deep semantics embedded in complex interactions. Recent research points out the effectiveness to understand conversation structure from temporal dynamics (Cheng et al., 2017a; Jiao et al., 2018) and replying struc3332 - ?,? (Predicted Score) ? MLP Mechanism Attention Msg Encoder GCN GRU ?|?| … … … Bi-GRU ?? ?? ??? Msg Encoder Text Initialize User User Factor Embedding ?? ?? ?? ?? Figure 2: Overall structure of our model. The left module is to model user interest dynamics, whose results together with conversation representations derived from"
2020.acl-main.305,P12-1054,0,0.0286043,"time for conversation recommendation. 2 Related Work User Response Prediction. This work is in line with user response prediction, such as message popularity forecast with handcrafted response features (Artzi et al., 2012; Backstrom et al., 2013) and conversation trajectory with user interaction structures (Cheng et al., 2017b; Jiao et al., 2018; Zeng et al., 2019a). These works predict responses from general public, while we work on personalized recommendation and focus on user interest modeling. For recommendation, there are extensive efforts on post-level recommendation (Chen et al., 2012; Yan et al., 2012) and conversation-level (Chen et al., 2011; Zeng et al., 2018, 2019b). In contrast with them which assume static user interests, we capture how user interests change over time and take advantage of the recent advancement of dynamic product recommendation (Wu et al., 2017; Beutel et al., 2018). To recommend conversations, we aim to learn user interest dynamics from chatting content and interaction behavior, which have never been explored in previous research. Conversation Structure Modeling. Our work is also related to previous work to understand how participants interact with each other in con"
2020.acl-main.305,Q18-1009,0,0.0285754,"o et al., 2018) and replying struc3332 - ?,? (Predicted Score) ? MLP Mechanism Attention Msg Encoder GCN GRU ?|?| … … … Bi-GRU ?? ?? ??? Msg Encoder Text Initialize User User Factor Embedding ?? ?? ?? ?? Figure 2: Overall structure of our model. The left module is to model user interest dynamics, whose results together with conversation representations derived from the right part are used for producing final prediction. Predicted score yˆu,c indicates how likely u will engage in c. “Msg Encoder” mainly contains two layers: word embedding layer and CNN modeling layer. ture (Miura et al., 2018; Zayats and Ostendorf, 2018; Zeng et al., 2019b). The two factors are coupled in our interaction modeling and their joint effects for dynamic conversation recommendation, ignored by prior work, will be extensively studied here. 3 Our Dynamic Conversation Recommendation Model This section describes our dynamic conversation recommendation model, whose overall structure is shown in Figure 2. In the following, we will first introduce how we model the user interest dynamics with their chatting history in Section 3.1, followed by the description of conversation modeling in Section 3.2. Afterwards, Section 3.3 will present how"
2020.acl-main.305,N18-1035,1,0.873366,"s and discuss topics they are interested in. However, the huge volume of online conversations produced daily hinders people’s capability of finding the information they are interested in. As a result, there is pressing demand for developing a conversation recommendation engine that tracks ongoing conversations and recommends suitable ones to users. Viewing the deluge of information streaming through social media, it is not hard to envision that users’ tastes, stances, and behaviors evolve over time (Wu et al., 2017). Nonetheless, existing work on recommending conversations (Chen et al., 2011; Zeng et al., 2018, 2019b) assume users’ discussion preferences do not change over time. Moreover, the common practice of recommendation is via collaborative filtering (CF), which relies on rich user interaction history for model training (Zeng et al., 2018, 2019b). When a conversation is entirely absent from training data, the model performance is inevitably compromised. This phenomenon is referred to as conversation cold start. As a result, existing methods which ignore the time-evolving user interests is insurmountable to tackle a common problem in practice, i.e., to predict future conversations created afte"
2020.acl-main.305,P19-1270,1,0.829133,"previous studies (Zeng et al., 2018, 2019b), which let training data contain partial context for any conversations to allow the possibility of predicting users’ future engagement 1 The datasets and codes are available at: https:// github.com/zxshamson/dy-conv-rec for recommendation. Experimental results in main comparisons show that our model significantly outperforms all previous methods that ignore the change of user interests or interactions within contexts. For example, we achieve 0.375 MAP in discussions of “technology”, compared with 0.222 yielded by our previous stateof-the-art model (Zeng et al., 2019b). Further study shows that we consistently perform better both in conversation cold start and with varying degrees of sparsity of user history and conversation contexts. Lastly, to provide more insights into user interest dynamics, we inspect our model outputs and find that users indeed tend to engage in different types of conversations at different times, confirming the usefulness of tracking user preferences in real-time for conversation recommendation. 2 Related Work User Response Prediction. This work is in line with user response prediction, such as message popularity forecast with hand"
2020.acl-main.305,D19-1470,1,0.79988,"previous studies (Zeng et al., 2018, 2019b), which let training data contain partial context for any conversations to allow the possibility of predicting users’ future engagement 1 The datasets and codes are available at: https:// github.com/zxshamson/dy-conv-rec for recommendation. Experimental results in main comparisons show that our model significantly outperforms all previous methods that ignore the change of user interests or interactions within contexts. For example, we achieve 0.375 MAP in discussions of “technology”, compared with 0.222 yielded by our previous stateof-the-art model (Zeng et al., 2019b). Further study shows that we consistently perform better both in conversation cold start and with varying degrees of sparsity of user history and conversation contexts. Lastly, to provide more insights into user interest dynamics, we inspect our model outputs and find that users indeed tend to engage in different types of conversations at different times, confirming the usefulness of tracking user preferences in real-time for conversation recommendation. 2 Related Work User Response Prediction. This work is in line with user response prediction, such as message popularity forecast with hand"
2020.acl-main.457,P15-1034,0,0.0385924,"credits only when the answers exactly match the ground-truths, thus causing inaccuracies for rephrased answers and discouraging abstract content generation. In contrast, we design a semantic-driven cloze reward by measuring how well a QA system can address multiple choice cloze questions which better encode entity interactions and handle paraphrased answers. 3 Knowledge Graph Construction To construct a knowledge graph from an input document, we utilize Stanford CoreNLP (Manning et al., 2014) to first obtain outputs from coreference resolution and open information extraction (OpenIE) models (Angeli et al., 2015). Note that we do not conduct global entity linking across documents. Next, we take the hsubject, predicate, objecti triples extracted by OpenIE and remove any triple whose argument (subject or object) has more than 10 words. If two triples differ only by one argument, and the arguments overlap, we keep the longer triple. OpenIE Input Article Mayor 's Admission of Cocaine Use …. RoBERTa Layers Node Initialization Bi-LSTM Layer GAT Layers Attention Layer Attention Layer Ctv Ct Generated Summary ... <SOS> The Week column Figure 2: Our ASGARD framework with documentlevel graph encoding. Summary i"
2020.acl-main.457,N19-1264,0,0.0151738,"ork utilizes reinforcement learning to directly optimize the model to maximize the informativeness of summaries by using different forms of ROUGE scores (Paulus et al., 2018; Chen and Bansal, 2018; Sharma et al., 2019). However, ROUGE does not always distinguish good summaries from bad ones (Novikova et al., 2017), and ignores entity interactions. Since question answering (QA) has been used for summary evaluation (Narayan et al., 2018), and is shown to correlate with human judgment of summaries qualities (Eyal et al., 2019), QA-based rewards have been studied for summarization model training. Arumae and Liu (2019) demonstrate that using fill-in-the-blank questions by removing entities or root words leads to improved content selection. Scialom et al. (2019) consider a similar setup, but use both F1 score and QA system confidence as rewards in abstractive summarization. Previous work, however, mainly focuses on single entities or words in human-written summaries, thereby losing contexts and relations. Moreover, fill-in-the-blank questions by prior work give credits only when the answers exactly match the ground-truths, thus causing inaccuracies for rephrased answers and discouraging abstract content gene"
2020.acl-main.457,N04-1015,0,0.0303144,"At each decoding step t, we compute a graph context vector cvt with the attention mechanism (Bahdanau et al., 2014): cvt = (3) (4) where u∗ are also trainable parameters. We omit bias terms for simplicity. Attending the Document. Similarly, the document context ct is computed over input tokens by additionally considering the graph context cvt : ct = X ak,t hk (5) k ak,t = softmax( uT1 tanh(W5 st + W6 hk + W7 cvt )) (2) Encoder Extension to Capture Topic Shift (S EG G RAGH). Modeling topic transitions and recurrences enables the identification of notable content, thus benefiting summarization (Barzilay and Lee, 2004). Since paragraphs naturally divide a document into different topic segments, we extend DocGragh by first encoding each paragraph as a subgraph Gp (for the p-th paragraph) using the same graph encoder, and then connecting all subgraphs with a BiLSTM. If two nodes in separate subgraphs refer to the same entity, they are initialˆi avi,t v ˆ i )) avi,t = softmax(uT0 tanh(W3 st + W4 v (1) where kN n=1 denotes the concatenation of N heads, each producing a vector of the same dimension as vi . We use N = 4 in our experiments with two layers of GATs. N (vi ) denotes the neighbors of vi in graph G. W∗"
2020.acl-main.457,P18-1026,0,0.169619,"oder with graph neural networks (GNNs) to consider token-level entity types, however, entity interactions are largely ignored. On multi-document summarization, Fan et al. (2019) demonstrate the usefulness of encoding a linearized knowledge graph from OpenIE outputs. In this work, we design a graph encoder, which improves upon Graph Attention Networks (GATs) (Veliˇckovi´c et al., 2018), to capture the global context in a more effective manner. Also related is the graph-to-sequence framework that has been adopted for text generation (Song et al., 2018). Both Gated Graph Neural Networks (GGNNs) (Beck et al., 2018) and Graph Convolutional Networks (GCNs) (Damonte and Cohen, 2019) are shown to be effective in generating sentences from AMR graphs. Since Graph Attention Networks can better handle sparse graphs, they are used by Koncel-Kedziorski et al. (2019) with a transformer model to create scientific paper ab5095 stracts from knowledge graphs. Here we use graphs in addition to document encoder, both carrying complementary information for summarization. Reinforcement Learning and QA Reward for Abstractive Summarization. As pointed out by Ranzato et al. (2016), word-level maximum likelihood training brin"
2020.acl-main.457,N18-1150,0,0.0252538,"Best results are in boldface. Best of our models are in italics. ASGARD- SEG+Rrouge +Rcloze yields significantly higher scores than our other models with approximate randomization test (p < 0.0005). we include an extractive baseline L EAD -3. We further add the following abstractive models for comparison: (1) a pointer-generator model with coverage (See et al., 2017) (P OINT G EN + COV); (2) a deep reinforcement learning-based model (Paulus et al., 2018) (D EEP R EINFORCE); (3) a bottom-up model (Gehrmann et al., 2018) (B OTTOM U P); (4) a deep communicating agents-based summarization model (Celikyilmaz et al., 2018) (DCA). We also report results by fine-tuning BART model (Lewis et al., 2019). In Lewis et al. (2019), fine-tuning is only performed on CNN/Daily Mail. We apply the same method for NYT. For NYT, we add results by SENECA model (Sharma et al., 2019) from our prior work, which previously achieved the best ROUGE-2. On CNN/Daily Mail, we include comparisons of a two-stage fine-tuned model (first on an extractor, then on an abstractor) with BERT (Liu and Lapata, 2019) (B ERT S UM E XTA BS), and a unified pretrained language model for generation (Dong et al., 2019) (U NI LM). In addition to ASGARD- D"
2020.acl-main.457,P18-1063,0,0.0369562,"oncel-Kedziorski et al. (2019) with a transformer model to create scientific paper ab5095 stracts from knowledge graphs. Here we use graphs in addition to document encoder, both carrying complementary information for summarization. Reinforcement Learning and QA Reward for Abstractive Summarization. As pointed out by Ranzato et al. (2016), word-level maximum likelihood training brings the problem of exposure bias. Recent work utilizes reinforcement learning to directly optimize the model to maximize the informativeness of summaries by using different forms of ROUGE scores (Paulus et al., 2018; Chen and Bansal, 2018; Sharma et al., 2019). However, ROUGE does not always distinguish good summaries from bad ones (Novikova et al., 2017), and ignores entity interactions. Since question answering (QA) has been used for summary evaluation (Narayan et al., 2018), and is shown to correlate with human judgment of summaries qualities (Eyal et al., 2019), QA-based rewards have been studied for summarization model training. Arumae and Liu (2019) demonstrate that using fill-in-the-blank questions by removing entities or root words leads to improved content selection. Scialom et al. (2019) consider a similar setup, but"
2020.acl-main.457,N19-1366,0,0.0181773,"el entity types, however, entity interactions are largely ignored. On multi-document summarization, Fan et al. (2019) demonstrate the usefulness of encoding a linearized knowledge graph from OpenIE outputs. In this work, we design a graph encoder, which improves upon Graph Attention Networks (GATs) (Veliˇckovi´c et al., 2018), to capture the global context in a more effective manner. Also related is the graph-to-sequence framework that has been adopted for text generation (Song et al., 2018). Both Gated Graph Neural Networks (GGNNs) (Beck et al., 2018) and Graph Convolutional Networks (GCNs) (Damonte and Cohen, 2019) are shown to be effective in generating sentences from AMR graphs. Since Graph Attention Networks can better handle sparse graphs, they are used by Koncel-Kedziorski et al. (2019) with a transformer model to create scientific paper ab5095 stracts from knowledge graphs. Here we use graphs in addition to document encoder, both carrying complementary information for summarization. Reinforcement Learning and QA Reward for Abstractive Summarization. As pointed out by Ranzato et al. (2016), word-level maximum likelihood training brings the problem of exposure bias. Recent work utilizes reinforcemen"
2020.acl-main.457,N19-1423,0,0.0313471,"estions, we create one candidate answer by swapping the subject and the object (e.g. candidate B as in Fig. 3) and two candidates by replacing the subject or the object with another argument of the same role extracted from the salient context (e.g. candidates C and D). If not enough answers are created, we further consider randomly selecting sentences from the input. For predicate questions, we use predicates in other triples from the context as candidate answers. Among all candidates, we select the three that are able to construct the most fluent questions using perplexity predicted by BERT (Devlin et al., 2019). 6 Experimental Setups Datasets. We experiment with two popular summarization datasets with summaries containing multiple sentences: the New York Times annotated corpus (NYT) (Sandhaus, 2008) and the CNN/Daily Mail dataset (CNN/DM) (Hermann et al., 2015). We follow the preprocessing steps and experimental setups from prior work (Paulus et al., 2018; See et al., 2017) for both datasets. For NYT, the training, validation, and test sets contain 588, 909, 32, 716, and 32, 703 samples. For CNN/DM, the numbers are 287, 188, 13, 367, and 11, 490. To train our cloze QA model for NYT, we construct 1,"
2020.acl-main.457,N19-1395,0,0.042664,"Missing"
2020.acl-main.457,D19-1428,0,0.0350867,"and § 7. Finally, we conclude in § 8. 2 Related Work Graph-Augmented Summarization and Generation. Graph structures have long been used for extractive summarization, such as in Textrank (Mihalcea and Tarau, 2004) and Lexrank (Erkan and Radev, 2004). For neural models, Tan et al. (2017) design graph-based attention to identify important sentences. For generating abstractive summaries, Fernandes et al. (2019) enhance a sequence-based encoder with graph neural networks (GNNs) to consider token-level entity types, however, entity interactions are largely ignored. On multi-document summarization, Fan et al. (2019) demonstrate the usefulness of encoding a linearized knowledge graph from OpenIE outputs. In this work, we design a graph encoder, which improves upon Graph Attention Networks (GATs) (Veliˇckovi´c et al., 2018), to capture the global context in a more effective manner. Also related is the graph-to-sequence framework that has been adopted for text generation (Song et al., 2018). Both Gated Graph Neural Networks (GGNNs) (Beck et al., 2018) and Graph Convolutional Networks (GCNs) (Damonte and Cohen, 2019) are shown to be effective in generating sentences from AMR graphs. Since Graph Attention Net"
2020.acl-main.457,D18-1443,0,0.225577,"help for his drug problem about 18 months ago. Figure 1: Sample knowledge graph constructed from an article snippet. The graph localizes relevant information for entities (color coded, e.g. “John M. Fabrizi”) or events (underlined) and provides global context. Introduction Abstractive summarization aims to produce concise and informative summaries with the goal of promoting efficient information consumption and knowledge acquisition (Luhn, 1958). Significant progress has been made in this area by designing sequence-to-sequence-based neural models for single-document abstractive summarization (Gehrmann et al., 2018; Liu et al., 2018; Liu and Lapata, 2019). However, due to the limitations of model structure and word prediction-based learning objectives, these models frequently produce unfaithful content (Cao et al., 2018) and nearextractive summaries (See et al., 2017; Kry´sci´nski et al., 2018). These observations suggest that existing models lack semantic interpretation over the input, which is critical for summarization. We argue that the generation of informative and succinct abstracts requires structured representation to facilitate the connection of relevant subjects, and the preservation of global"
2020.acl-main.457,P18-1013,0,0.0335634,"wledge as proposed by Paulus et al. (2018). We further add a copy mechanism similar to See et al. (2017), with copy probability as: Pcopy = σ(Wcopy [st |ct |cvt |yt−1 ]) (8) where yt−1 denotes the embedding for the token predicted at step t − 1. Modified Hierarchical Attention for SegGraph. As mentioned in § 4.1, SegGraph captures content salience by modeling topic shift across paragraphs. We thus seek to leverage paragraph-level importance to redistribute the node attentions, e.g., giving 5097 more attentions to nodes in important paragraphs. In particular, we utilize hierarchical attention (Hsu et al., 2018), where we first calculate attention agt ˆi over subgraphs as done in Eq. 3 by replacing v with subgraph representation hgp . We then combine subgraph attentions agt with the previously calculated attentions avt for nodes in the subgraph using scalar multiplication and renormalization over all nodes in input. This results in ˆvt , which are used to the new attention weights a obtain graph context vector cvt as done in Eq. 3 for SegGraph. For RL, we use a self-critical policy gradient algorithm (Rennie et al., 2017). During training, two summaries are generated: first, a summary ys , sampling t"
2020.acl-main.457,N19-1238,0,0.0613613,"Missing"
2020.acl-main.457,D18-1207,0,0.0598593,"Missing"
2020.acl-main.457,2020.acl-main.703,0,0.0930129,"Missing"
2020.acl-main.457,N03-1020,0,0.524822,"Missing"
2020.acl-main.457,D19-1387,0,0.40273,"ago. Figure 1: Sample knowledge graph constructed from an article snippet. The graph localizes relevant information for entities (color coded, e.g. “John M. Fabrizi”) or events (underlined) and provides global context. Introduction Abstractive summarization aims to produce concise and informative summaries with the goal of promoting efficient information consumption and knowledge acquisition (Luhn, 1958). Significant progress has been made in this area by designing sequence-to-sequence-based neural models for single-document abstractive summarization (Gehrmann et al., 2018; Liu et al., 2018; Liu and Lapata, 2019). However, due to the limitations of model structure and word prediction-based learning objectives, these models frequently produce unfaithful content (Cao et al., 2018) and nearextractive summaries (See et al., 2017; Kry´sci´nski et al., 2018). These observations suggest that existing models lack semantic interpretation over the input, which is critical for summarization. We argue that the generation of informative and succinct abstracts requires structured representation to facilitate the connection of relevant subjects, and the preservation of global context, e.g. entity interactions and to"
2020.acl-main.457,P14-5010,0,0.00529218,"in human-written summaries, thereby losing contexts and relations. Moreover, fill-in-the-blank questions by prior work give credits only when the answers exactly match the ground-truths, thus causing inaccuracies for rephrased answers and discouraging abstract content generation. In contrast, we design a semantic-driven cloze reward by measuring how well a QA system can address multiple choice cloze questions which better encode entity interactions and handle paraphrased answers. 3 Knowledge Graph Construction To construct a knowledge graph from an input document, we utilize Stanford CoreNLP (Manning et al., 2014) to first obtain outputs from coreference resolution and open information extraction (OpenIE) models (Angeli et al., 2015). Note that we do not conduct global entity linking across documents. Next, we take the hsubject, predicate, objecti triples extracted by OpenIE and remove any triple whose argument (subject or object) has more than 10 words. If two triples differ only by one argument, and the arguments overlap, we keep the longer triple. OpenIE Input Article Mayor 's Admission of Cocaine Use …. RoBERTa Layers Node Initialization Bi-LSTM Layer GAT Layers Attention Layer Attention Layer Ctv"
2020.acl-main.457,W04-3252,0,0.0141803,"s, implying that new evaluation methods are needed to better gauge summary quality. The rest of the paper is organized as follows. We describe related work in the next section (§ 2). We then discuss the knowledge graph construction in § 3 and formulate our graph-augmented summarization framework in § 4. In § 5, we introduce reinforcement learning with cloze reward. Experiments and results are presented in § 6 and § 7. Finally, we conclude in § 8. 2 Related Work Graph-Augmented Summarization and Generation. Graph structures have long been used for extractive summarization, such as in Textrank (Mihalcea and Tarau, 2004) and Lexrank (Erkan and Radev, 2004). For neural models, Tan et al. (2017) design graph-based attention to identify important sentences. For generating abstractive summaries, Fernandes et al. (2019) enhance a sequence-based encoder with graph neural networks (GNNs) to consider token-level entity types, however, entity interactions are largely ignored. On multi-document summarization, Fan et al. (2019) demonstrate the usefulness of encoding a linearized knowledge graph from OpenIE outputs. In this work, we design a graph encoder, which improves upon Graph Attention Networks (GATs) (Veliˇckovi´c"
2020.acl-main.457,D18-1206,0,0.0232165,"nt Learning and QA Reward for Abstractive Summarization. As pointed out by Ranzato et al. (2016), word-level maximum likelihood training brings the problem of exposure bias. Recent work utilizes reinforcement learning to directly optimize the model to maximize the informativeness of summaries by using different forms of ROUGE scores (Paulus et al., 2018; Chen and Bansal, 2018; Sharma et al., 2019). However, ROUGE does not always distinguish good summaries from bad ones (Novikova et al., 2017), and ignores entity interactions. Since question answering (QA) has been used for summary evaluation (Narayan et al., 2018), and is shown to correlate with human judgment of summaries qualities (Eyal et al., 2019), QA-based rewards have been studied for summarization model training. Arumae and Liu (2019) demonstrate that using fill-in-the-blank questions by removing entities or root words leads to improved content selection. Scialom et al. (2019) consider a similar setup, but use both F1 score and QA system confidence as rewards in abstractive summarization. Previous work, however, mainly focuses on single entities or words in human-written summaries, thereby losing contexts and relations. Moreover, fill-in-the-bl"
2020.acl-main.457,D17-1238,0,0.0411714,"s. Here we use graphs in addition to document encoder, both carrying complementary information for summarization. Reinforcement Learning and QA Reward for Abstractive Summarization. As pointed out by Ranzato et al. (2016), word-level maximum likelihood training brings the problem of exposure bias. Recent work utilizes reinforcement learning to directly optimize the model to maximize the informativeness of summaries by using different forms of ROUGE scores (Paulus et al., 2018; Chen and Bansal, 2018; Sharma et al., 2019). However, ROUGE does not always distinguish good summaries from bad ones (Novikova et al., 2017), and ignores entity interactions. Since question answering (QA) has been used for summary evaluation (Narayan et al., 2018), and is shown to correlate with human judgment of summaries qualities (Eyal et al., 2019), QA-based rewards have been studied for summarization model training. Arumae and Liu (2019) demonstrate that using fill-in-the-blank questions by removing entities or root words leads to improved content selection. Scialom et al. (2019) consider a similar setup, but use both F1 score and QA system confidence as rewards in abstractive summarization. Previous work, however, mainly foc"
2020.acl-main.457,D19-1320,0,0.014626,"scores (Paulus et al., 2018; Chen and Bansal, 2018; Sharma et al., 2019). However, ROUGE does not always distinguish good summaries from bad ones (Novikova et al., 2017), and ignores entity interactions. Since question answering (QA) has been used for summary evaluation (Narayan et al., 2018), and is shown to correlate with human judgment of summaries qualities (Eyal et al., 2019), QA-based rewards have been studied for summarization model training. Arumae and Liu (2019) demonstrate that using fill-in-the-blank questions by removing entities or root words leads to improved content selection. Scialom et al. (2019) consider a similar setup, but use both F1 score and QA system confidence as rewards in abstractive summarization. Previous work, however, mainly focuses on single entities or words in human-written summaries, thereby losing contexts and relations. Moreover, fill-in-the-blank questions by prior work give credits only when the answers exactly match the ground-truths, thus causing inaccuracies for rephrased answers and discouraging abstract content generation. In contrast, we design a semantic-driven cloze reward by measuring how well a QA system can address multiple choice cloze questions which"
2020.acl-main.457,P17-1099,0,0.820231,"ntroduction Abstractive summarization aims to produce concise and informative summaries with the goal of promoting efficient information consumption and knowledge acquisition (Luhn, 1958). Significant progress has been made in this area by designing sequence-to-sequence-based neural models for single-document abstractive summarization (Gehrmann et al., 2018; Liu et al., 2018; Liu and Lapata, 2019). However, due to the limitations of model structure and word prediction-based learning objectives, these models frequently produce unfaithful content (Cao et al., 2018) and nearextractive summaries (See et al., 2017; Kry´sci´nski et al., 2018). These observations suggest that existing models lack semantic interpretation over the input, which is critical for summarization. We argue that the generation of informative and succinct abstracts requires structured representation to facilitate the connection of relevant subjects, and the preservation of global context, e.g. entity interactions and topic flows. Take Fig. 1 as an ex5094 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 5094–5107 c July 5 - 10, 2020. 2020 Association for Computational Linguistics ample."
2020.acl-main.457,D19-1323,1,0.923041,"(2019) with a transformer model to create scientific paper ab5095 stracts from knowledge graphs. Here we use graphs in addition to document encoder, both carrying complementary information for summarization. Reinforcement Learning and QA Reward for Abstractive Summarization. As pointed out by Ranzato et al. (2016), word-level maximum likelihood training brings the problem of exposure bias. Recent work utilizes reinforcement learning to directly optimize the model to maximize the informativeness of summaries by using different forms of ROUGE scores (Paulus et al., 2018; Chen and Bansal, 2018; Sharma et al., 2019). However, ROUGE does not always distinguish good summaries from bad ones (Novikova et al., 2017), and ignores entity interactions. Since question answering (QA) has been used for summary evaluation (Narayan et al., 2018), and is shown to correlate with human judgment of summaries qualities (Eyal et al., 2019), QA-based rewards have been studied for summarization model training. Arumae and Liu (2019) demonstrate that using fill-in-the-blank questions by removing entities or root words leads to improved content selection. Scialom et al. (2019) consider a similar setup, but use both F1 score and"
2020.acl-main.457,P18-1150,0,0.0316412,"ummaries, Fernandes et al. (2019) enhance a sequence-based encoder with graph neural networks (GNNs) to consider token-level entity types, however, entity interactions are largely ignored. On multi-document summarization, Fan et al. (2019) demonstrate the usefulness of encoding a linearized knowledge graph from OpenIE outputs. In this work, we design a graph encoder, which improves upon Graph Attention Networks (GATs) (Veliˇckovi´c et al., 2018), to capture the global context in a more effective manner. Also related is the graph-to-sequence framework that has been adopted for text generation (Song et al., 2018). Both Gated Graph Neural Networks (GGNNs) (Beck et al., 2018) and Graph Convolutional Networks (GCNs) (Damonte and Cohen, 2019) are shown to be effective in generating sentences from AMR graphs. Since Graph Attention Networks can better handle sparse graphs, they are used by Koncel-Kedziorski et al. (2019) with a transformer model to create scientific paper ab5095 stracts from knowledge graphs. Here we use graphs in addition to document encoder, both carrying complementary information for summarization. Reinforcement Learning and QA Reward for Abstractive Summarization. As pointed out by Ranz"
2020.acl-main.457,P17-1108,0,0.0930529,"Missing"
2020.acl-main.478,S17-1027,0,0.110158,") 316(4.3) 47(2.1) D3 1876(32.1) 953(19.5) 1867(25.2) 598(26.9) D4 532(9.1) 525(10.7) 924(12.5) 148(6.7) N/A 197(3.3) 736(15.0) 686(9.3) 141(6.3) Table 3: Distribution of Content type labels across media sources, with percentages shown within parentheses. vs. not speech). In the created corpus, 5535 out of 18,155 sentences are labeled as speech. 5 Document-level Neural Network Model for Discourse Profiling A wide range of computational models has been applied for extracting different forms of discourse structures. However, across several tasks, neural network methods (Ji and Eisenstein, 2015; Becker et al., 2017) are found the most effective, with relatively superior performance obtained by modeling discourse-level context (Dai and Huang, 2018a,b). As an initial attempt, we use a hierarchical neural network to derive sentence representations and a document encoding, and model associations between each sentence and the main topic of the document when determining content types for sentences. Shown in Figure 1, it first uses a wordlevel bi-LSTM layer (Hochreiter and Schmidhuber, 1997) with soft-attention over word representations to generate intermediate sentence representations which are further enriche"
2020.acl-main.478,doddington-etal-2004-automatic,0,0.210598,"Missing"
2020.acl-main.478,D15-1263,0,0.0510906,"Missing"
2020.acl-main.478,P18-1045,1,0.727743,"types, seven types specifically, therefore, if main events do not belong to those seven types, they are not annotated as events, which also contributes to the imperfect percentage of main event sentences containing a headline event. 6 All the KBP corpora include documents from both discussion forum and news articles. But as the goal of this study is to leverage discourse structures specific to news articles for improving event coreference resolution performance, we only evaluate the ILP system using news articles in the KBP corpora. This evaluation setting is consistent with our previous work Choubey and Huang (2018). For direct comparisons, the results reported for all the systems and baselines are based on news articles in the test datasets as well 7 The classifier can be obtained from https://git. io/JeDw3 5381 Model Local classifier +Content Structure -Singletons -Main Events -Intra-type Events Lu and Ng (2017) Choubey and Huang (2018) B3 51.47 52.78 51.47 52.65 52.62 50.16 51.67 CEAFe 47.96 49.7 47.96 49.35 49.63 48.59 49.1 KBP 2016 MUC 26.29 34.62 31.42 32.56 32.97 32.41 34.08 BLANC 30.82 34.49 32.89 33.69 34.07 32.72 34.08 AV G 39.13 42.9 40.94 42.06 42.32 40.97 42.23 B3 50.24 51.68 51.17 51.4 51.6"
2020.acl-main.478,N18-2055,1,0.78336,"l network classifier to make content type predictions. 6.2 Proposed Document-level Models Document LSTM adds the sentence-level BiLSTM over sentence representations obtained from the word-level BiLSTM to enrich sentence representations with local contextual information. +Document Encoding uses document encoding for modeling associations with the main topic and obtains the final sentence representations as described previously. +Headline replaces document encoding with headline sentence encoding generated from the wordlevel biLSTM. Headline is known to be a strong predictor for the main event (Choubey et al., 2018). CRF Fine-grained and CRF Coarse-grained adds a CRF layer to make content type predictions for sentences which models dependencies among fine-grained (eight content types) and coarse-grained (main vs. context-informing vs. supportive contents) content types respectively. 6.3 Implementation Details We set hidden states dimension to 512 for both word-level and sentence-level biLSTMs in all our models. Similarly, we use two-layered feed forward networks with 1024-512-1 units to calculate attention weights for both the BiLSTMs. The final classifier uses two-layer feed forward networks with 3072-1"
2020.acl-main.478,D18-1368,1,0.852282,"3) 141(6.3) Table 3: Distribution of Content type labels across media sources, with percentages shown within parentheses. vs. not speech). In the created corpus, 5535 out of 18,155 sentences are labeled as speech. 5 Document-level Neural Network Model for Discourse Profiling A wide range of computational models has been applied for extracting different forms of discourse structures. However, across several tasks, neural network methods (Ji and Eisenstein, 2015; Becker et al., 2017) are found the most effective, with relatively superior performance obtained by modeling discourse-level context (Dai and Huang, 2018a,b). As an initial attempt, we use a hierarchical neural network to derive sentence representations and a document encoding, and model associations between each sentence and the main topic of the document when determining content types for sentences. Shown in Figure 1, it first uses a wordlevel bi-LSTM layer (Hochreiter and Schmidhuber, 1997) with soft-attention over word representations to generate intermediate sentence representations which are further enriched with the context information using another sentence-level bi-LSTM. Enriched sentence representations are then averaged with their s"
2020.acl-main.478,N18-1013,1,0.883146,"3) 141(6.3) Table 3: Distribution of Content type labels across media sources, with percentages shown within parentheses. vs. not speech). In the created corpus, 5535 out of 18,155 sentences are labeled as speech. 5 Document-level Neural Network Model for Discourse Profiling A wide range of computational models has been applied for extracting different forms of discourse structures. However, across several tasks, neural network methods (Ji and Eisenstein, 2015; Becker et al., 2017) are found the most effective, with relatively superior performance obtained by modeling discourse-level context (Dai and Huang, 2018a,b). As an initial attempt, we use a hierarchical neural network to derive sentence representations and a document encoding, and model associations between each sentence and the main topic of the document when determining content types for sentences. Shown in Figure 1, it first uses a wordlevel bi-LSTM layer (Hochreiter and Schmidhuber, 1997) with soft-attention over word representations to generate intermediate sentence representations which are further enriched with the context information using another sentence-level bi-LSTM. Enriched sentence representations are then averaged with their s"
2020.acl-main.478,D08-1035,0,0.496944,"Missing"
2020.acl-main.478,P12-1007,0,0.0610047,"Missing"
2020.acl-main.478,W14-4921,0,0.0633311,"Missing"
2020.acl-main.478,P94-1002,0,0.886448,"atically construct news content structures. Finally, we demonstrate that incorporating system predicted news structures yields new state-of-theart performance for event coreference resolution. The news documents we annotated are openly available and the annotations are publicly released for future research1 . 1 Introduction Detecting and incorporating discourse structures is important for achieving text-level language understanding. Several well-studied discourse analysis tasks, such as RST (Mann and Thompson, 1988) and PDTB style (Prasad et al., 2008) discourse parsing and text segmentation (Hearst, 1994), generate rhetorical and content structures that have been shown useful for many NLP applications. But these widely applicable discourse structures overlook genre specialties. In this paper, we focus on studying content structures specific to news articles, a broadly studied text genre for many NLP tasks and applications. We believe that genre-specific discourse structures can effectively complement genre independent discourse structures and are essential for achieving deep story-level text understanding. What is in a news article? Normally, we expect a news article to describe well verified"
2020.acl-main.478,P14-1002,0,0.112372,"Missing"
2020.acl-main.478,Q15-1024,0,0.0297046,"(17.3) D2 197(3.4) 96(2.0) 316(4.3) 47(2.1) D3 1876(32.1) 953(19.5) 1867(25.2) 598(26.9) D4 532(9.1) 525(10.7) 924(12.5) 148(6.7) N/A 197(3.3) 736(15.0) 686(9.3) 141(6.3) Table 3: Distribution of Content type labels across media sources, with percentages shown within parentheses. vs. not speech). In the created corpus, 5535 out of 18,155 sentences are labeled as speech. 5 Document-level Neural Network Model for Discourse Profiling A wide range of computational models has been applied for extracting different forms of discourse structures. However, across several tasks, neural network methods (Ji and Eisenstein, 2015; Becker et al., 2017) are found the most effective, with relatively superior performance obtained by modeling discourse-level context (Dai and Huang, 2018a,b). As an initial attempt, we use a hierarchical neural network to derive sentence representations and a document encoding, and model associations between each sentence and the main topic of the document when determining content types for sentences. Shown in Figure 1, it first uses a wordlevel bi-LSTM layer (Hochreiter and Schmidhuber, 1997) with soft-attention over word representations to generate intermediate sentence representations whi"
2020.acl-main.478,P17-1092,0,0.0329297,"Missing"
2020.acl-main.478,N18-2075,0,0.0839722,"Missing"
2020.acl-main.478,D14-1220,0,0.0414483,"Missing"
2020.acl-main.478,P14-2047,0,0.0604871,"Missing"
2020.acl-main.478,P11-1100,0,0.0877525,"Missing"
2020.acl-main.478,W10-4327,0,0.0877911,"Missing"
2020.acl-main.478,P17-1009,0,0.0829367,"Missing"
2020.acl-main.478,N18-1202,0,0.0294844,"r enriched with the context information using another sentence-level bi-LSTM. Enriched sentence representations are then averaged with their soft-attention weights to generate document encoding. The final prediction layers model associations between the document encoding and each sentence encoding to predict sentence types. Context-aware sentence encoding: Let a document be a sequence of sentences {s1 , s2 ..sn }, which in turn are sequences of words {(w11 , w12 ..) .. (wn1 , wn2 , ..)}. We first transform a sequence of words in each sentence to contextualized word representations using ELMo (Peters et al., 2018) followed by a word-level biLSTM layer to obtain their hidden state representations Hs . Then, we take weighted sums of hidden representations using soft-attention scores to obtain intermediate senFigure 1: Neural-Network Architecture Incorporating Document Encoding for Content Type Classification tence encodings (Si ) that are uninformed of the contextual information. Therefore, we apply another sentence-level biLSTM over the sequence of sentence encodings to model interactions among sentences and smoothen context flow from the headline until the last sentence in a document. The hidden states"
2020.acl-main.478,P09-2004,0,0.0719949,"Missing"
2020.acl-main.478,prasad-etal-2008-penn,0,0.495448,"e propose several documentlevel neural-network models to automatically construct news content structures. Finally, we demonstrate that incorporating system predicted news structures yields new state-of-theart performance for event coreference resolution. The news documents we annotated are openly available and the annotations are publicly released for future research1 . 1 Introduction Detecting and incorporating discourse structures is important for achieving text-level language understanding. Several well-studied discourse analysis tasks, such as RST (Mann and Thompson, 1988) and PDTB style (Prasad et al., 2008) discourse parsing and text segmentation (Hearst, 1994), generate rhetorical and content structures that have been shown useful for many NLP applications. But these widely applicable discourse structures overlook genre specialties. In this paper, we focus on studying content structures specific to news articles, a broadly studied text genre for many NLP tasks and applications. We believe that genre-specific discourse structures can effectively complement genre independent discourse structures and are essential for achieving deep story-level text understanding. What is in a news article? Normal"
2020.acl-main.478,D16-1246,0,0.0442148,"Missing"
2020.acl-main.478,K16-2007,0,0.0278748,"Missing"
2020.acl-main.478,D10-1037,0,0.0978756,"Missing"
2020.acl-main.478,N19-1178,0,0.0417004,"Missing"
2020.acl-main.478,N03-1030,0,0.333129,"Missing"
2020.acl-main.478,E99-1015,0,0.404529,"Missing"
2020.acl-main.478,W09-3742,0,0.0837645,"Missing"
2020.acl-main.478,D18-1079,0,0.0206765,"Missing"
2020.emnlp-main.293,N19-1423,0,0.350279,"different kinds of semantic units (n-grams or sentences). Experiments on CNN/Daily Mail and New York Times datasets demonstrate that our method can better model the importance of content than prior work based on F1 and ROUGE scores. 1 Introduction and Related Work Text summarization aims to compress long document(s) into a concise summary while maintaining the salient information. It often consists of two critical subtasks, important information identification and natural language generation (for abstractive summarization). With the advancements of large pre-trained language models (PreTLMs) (Devlin et al., 2019; Yang et al., 2019), state-of-the-art results are achieved on both natural language understanding and generation. However, it is still unclear how well these large models can estimate “content importance” for a given document. Previous studies for modeling importance are either empirical-based, which implicitly encode importance during document summarization, or theory-based, which often lacks support by empirical experiments (Peyrard, 2019). Benefiting from ∗ Corresponding author the large-scale summarization datasets (Nallapati et al., 2016; Narayan et al., 2018), data-driven approaches (Na"
2020.emnlp-main.293,N06-1059,0,0.0534654,"ven approaches (Nallapati et al., 2017; Paulus et al., 2018; Zhang et al., 2019) have made significant progress. Yet most of them conduct the information selection implicitly while generating the summaries. It lacks theory support and is hard to be applied to low-resource domains. In another line of work, structure features (Zheng and Lapata, 2019), such as centrality, position, and title, are employed as proxies for importance. However, the information captured by these features can vary in texts of different genres. To overcome this problem, theory-based methods (Louis, 2014; Peyrard, 2019; Lin et al., 2006) aim to formalize the concept of importance, and develop general-purpose systems by modeling the background knowledge of readers. This is based on the intuition that humans are good at identifying important content by using their own interpretation of the world knowledge. Theoretical models usually rely on information theory (IT) (Shannon, 1948). Louis (2014) uses Dirichlet distribution to represent the background knowledge and employs Bayesian surprise to find novel information. Peyrard (2019) instead models the importance with entropy, assuming the important words should be frequent in the g"
2020.emnlp-main.293,P14-2055,0,0.175905,"ayan et al., 2018), data-driven approaches (Nallapati et al., 2017; Paulus et al., 2018; Zhang et al., 2019) have made significant progress. Yet most of them conduct the information selection implicitly while generating the summaries. It lacks theory support and is hard to be applied to low-resource domains. In another line of work, structure features (Zheng and Lapata, 2019), such as centrality, position, and title, are employed as proxies for importance. However, the information captured by these features can vary in texts of different genres. To overcome this problem, theory-based methods (Louis, 2014; Peyrard, 2019; Lin et al., 2006) aim to formalize the concept of importance, and develop general-purpose systems by modeling the background knowledge of readers. This is based on the intuition that humans are good at identifying important content by using their own interpretation of the world knowledge. Theoretical models usually rely on information theory (IT) (Shannon, 1948). Louis (2014) uses Dirichlet distribution to represent the background knowledge and employs Bayesian surprise to find novel information. Peyrard (2019) instead models the importance with entropy, assuming the important"
2020.emnlp-main.293,W04-3252,0,0.0388455,"e ROUGE-1, ROUGE-2 and ROUGE-L respectively. Best results per metric are in bold. Among our models (bottom), I MP yields significantly higher scores on all metrics except when using unigrams as semantic unit and with sentences (based on R-1) on NYT (Welch’s t-test, p<0.05). for a sentence as: Imp(si ) = I(si |w∈s / i , K) = − log P (si |w∈s / i , K). For evaluation, we select a subset of sentences with Eq. (7) and calculate the ROUGE scores (Lin, 2004) against reference summary. The length constraints for CNN/DM and NYT are set to 105 and 95 tokens respectively. and Radev, 2004), T EXT R ANK (Mihalcea and Tarau, 2004) and T EXT R ANK +BERT (Zheng and Lapata, 2019), a frequency-based model S UM BASIC (Ani Nenkova, 2005), and BAYESIAN SR (Louis, 2014) which scores words or sentences with Bayesian surprise. Datasets. We evaluate our method on the test set of two popular summarization datasets: CNN/Daily Mail (abbreviated as CNN/DM) (Nallapati et al., 2017) and New York Times (Sandhaus, 2008). Following See et al. (2017)3 , we use the nonanonymized version that does not replace the name entities, which is most commonly used in recent work. We preprocess them as described in (Paulus et al., 2018). For unigram e"
2020.emnlp-main.293,K16-1028,0,0.0404866,"ments of large pre-trained language models (PreTLMs) (Devlin et al., 2019; Yang et al., 2019), state-of-the-art results are achieved on both natural language understanding and generation. However, it is still unclear how well these large models can estimate “content importance” for a given document. Previous studies for modeling importance are either empirical-based, which implicitly encode importance during document summarization, or theory-based, which often lacks support by empirical experiments (Peyrard, 2019). Benefiting from ∗ Corresponding author the large-scale summarization datasets (Nallapati et al., 2016; Narayan et al., 2018), data-driven approaches (Nallapati et al., 2017; Paulus et al., 2018; Zhang et al., 2019) have made significant progress. Yet most of them conduct the information selection implicitly while generating the summaries. It lacks theory support and is hard to be applied to low-resource domains. In another line of work, structure features (Zheng and Lapata, 2019), such as centrality, position, and title, are employed as proxies for importance. However, the information captured by these features can vary in texts of different genres. To overcome this problem, theory-based meth"
2020.emnlp-main.293,D18-1206,0,0.0384243,"ed language models (PreTLMs) (Devlin et al., 2019; Yang et al., 2019), state-of-the-art results are achieved on both natural language understanding and generation. However, it is still unclear how well these large models can estimate “content importance” for a given document. Previous studies for modeling importance are either empirical-based, which implicitly encode importance during document summarization, or theory-based, which often lacks support by empirical experiments (Peyrard, 2019). Benefiting from ∗ Corresponding author the large-scale summarization datasets (Nallapati et al., 2016; Narayan et al., 2018), data-driven approaches (Nallapati et al., 2017; Paulus et al., 2018; Zhang et al., 2019) have made significant progress. Yet most of them conduct the information selection implicitly while generating the summaries. It lacks theory support and is hard to be applied to low-resource domains. In another line of work, structure features (Zheng and Lapata, 2019), such as centrality, position, and title, are employed as proxies for importance. However, the information captured by these features can vary in texts of different genres. To overcome this problem, theory-based methods (Louis, 2014; Peyra"
2020.emnlp-main.293,P19-1499,0,0.103299,"erent genres. To overcome this problem, theory-based methods (Louis, 2014; Peyrard, 2019; Lin et al., 2006) aim to formalize the concept of importance, and develop general-purpose systems by modeling the background knowledge of readers. This is based on the intuition that humans are good at identifying important content by using their own interpretation of the world knowledge. Theoretical models usually rely on information theory (IT) (Shannon, 1948). Louis (2014) uses Dirichlet distribution to represent the background knowledge and employs Bayesian surprise to find novel information. Peyrard (2019) instead models the importance with entropy, assuming the important words should be frequent in the given document but rare in the background. However, statistical method is only a rough evaluation for informativity, which largely ignores the effect of semantic and context. In fact, the information amount of units is not only determined by frequency, but also by its semantic meaning, context, as well as reader’s background knowledge. In addition, bag-of-words approaches are difficult to generalize beyond unigrams due to the sparsity of n-grams when n is large. In this paper, we propose a novel"
2020.emnlp-main.293,P19-1628,0,0.0696994,"itly encode importance during document summarization, or theory-based, which often lacks support by empirical experiments (Peyrard, 2019). Benefiting from ∗ Corresponding author the large-scale summarization datasets (Nallapati et al., 2016; Narayan et al., 2018), data-driven approaches (Nallapati et al., 2017; Paulus et al., 2018; Zhang et al., 2019) have made significant progress. Yet most of them conduct the information selection implicitly while generating the summaries. It lacks theory support and is hard to be applied to low-resource domains. In another line of work, structure features (Zheng and Lapata, 2019), such as centrality, position, and title, are employed as proxies for importance. However, the information captured by these features can vary in texts of different genres. To overcome this problem, theory-based methods (Louis, 2014; Peyrard, 2019; Lin et al., 2006) aim to formalize the concept of importance, and develop general-purpose systems by modeling the background knowledge of readers. This is based on the intuition that humans are good at identifying important content by using their own interpretation of the world knowledge. Theoretical models usually rely on information theory (IT) ("
2020.emnlp-main.293,P19-1101,0,0.304115,"dentification and natural language generation (for abstractive summarization). With the advancements of large pre-trained language models (PreTLMs) (Devlin et al., 2019; Yang et al., 2019), state-of-the-art results are achieved on both natural language understanding and generation. However, it is still unclear how well these large models can estimate “content importance” for a given document. Previous studies for modeling importance are either empirical-based, which implicitly encode importance during document summarization, or theory-based, which often lacks support by empirical experiments (Peyrard, 2019). Benefiting from ∗ Corresponding author the large-scale summarization datasets (Nallapati et al., 2016; Narayan et al., 2018), data-driven approaches (Nallapati et al., 2017; Paulus et al., 2018; Zhang et al., 2019) have made significant progress. Yet most of them conduct the information selection implicitly while generating the summaries. It lacks theory support and is hard to be applied to low-resource domains. In another line of work, structure features (Zheng and Lapata, 2019), such as centrality, position, and title, are employed as proxies for importance. However, the information captur"
2020.emnlp-main.293,P17-1099,0,0.0706481,"nd calculate the ROUGE scores (Lin, 2004) against reference summary. The length constraints for CNN/DM and NYT are set to 105 and 95 tokens respectively. and Radev, 2004), T EXT R ANK (Mihalcea and Tarau, 2004) and T EXT R ANK +BERT (Zheng and Lapata, 2019), a frequency-based model S UM BASIC (Ani Nenkova, 2005), and BAYESIAN SR (Louis, 2014) which scores words or sentences with Bayesian surprise. Datasets. We evaluate our method on the test set of two popular summarization datasets: CNN/Daily Mail (abbreviated as CNN/DM) (Nallapati et al., 2017) and New York Times (Sandhaus, 2008). Following See et al. (2017)3 , we use the nonanonymized version that does not replace the name entities, which is most commonly used in recent work. We preprocess them as described in (Paulus et al., 2018). For unigram experiments, we remove all the stop words and punctuation in the reference summaries and treat the notional words as the predicting targets. For bigram, we first collect all the bigrams in source document and then discard the ones containing stop words or punctuation. The rest bigrams are employed as the predicting targets. 4 Results Comparisons. We compare our method with two types of models: (1) the met"
2020.emnlp-main.293,W89-0228,0,0.0728524,"d of all possible permutations of factorization orders. The probability prediction can be formalized as: P (zt |z<t ) = gPLM (z<t ) where z denotes a possible permutation sequence of input. Information of a subsequence is estimated as: I(zm:n |z<m ) = n X I(zt |z<t ) (5) t=m 2.3 Modeling Importance with Pre-trained Language Model We argue that important content should be hard to be predicted based on background knowledge only; it should be also difficult to be inferred from the context. Moreover, detecting important content is to find the most informative part from the input. As described in (Shann, 1989), the information amount is a quantification of the uncertainty we have for the semantic units. But the degree of uncertainty is relative to reader’s background knowledge. The less knowledge the reader has, the more uncertainty the source shows. We thus employ pre-trained language models, which contain a wide range of knowledge, to represent background knowledge. If a semantic unit is frequently mentioned in the training corpus, it will get high probability during inference and thus low information amount. We further propose a notion of importance as the information amount conditional on the b"
2020.emnlp-main.57,W19-5204,0,0.0132971,"adaptability to large pre-trained Transformers. One exception is the Plug and Play model (Dathathri et al., 2020), which directly modifies the key and value states of GPT2 (Radford et al., 2019). However, since the signal is derived from the whole generated text, it is too coarse to provide precise sentence-level content control. Here, we instead gain fine-grained controllability through keyphrase assignment and positioning per sentence, which can be adapted to any off-the-shelf pre-trained Transformer generators. Iterative Refinement has been studied in machine translation (Lee et al., 2018; Freitag et al., 2019; Mansimov et al., 2019; Kasai et al., 2020) to gradually improve translation quality. Refinement is also used with masked language models to improve fluency of non-autoregressive generation outputs (Ghazvininejad et al., 2019; Lawrence et al., 2019). Our work uses BART (Lewis et al., 2020), a state-of-the-art seq2seq model that offers better generalizability and stronger capacity for long text generation. Our proposed strategy substantially differs from prior solutions that rely on in-place word substitutions (Novak et al., 2016; Xia et al., 2017; Weston et al., 2018), as we leverage the seq2"
2020.emnlp-main.57,D19-1633,0,0.0168751,"l is derived from the whole generated text, it is too coarse to provide precise sentence-level content control. Here, we instead gain fine-grained controllability through keyphrase assignment and positioning per sentence, which can be adapted to any off-the-shelf pre-trained Transformer generators. Iterative Refinement has been studied in machine translation (Lee et al., 2018; Freitag et al., 2019; Mansimov et al., 2019; Kasai et al., 2020) to gradually improve translation quality. Refinement is also used with masked language models to improve fluency of non-autoregressive generation outputs (Ghazvininejad et al., 2019; Lawrence et al., 2019). Our work uses BART (Lewis et al., 2020), a state-of-the-art seq2seq model that offers better generalizability and stronger capacity for long text generation. Our proposed strategy substantially differs from prior solutions that rely on in-place word substitutions (Novak et al., 2016; Xia et al., 2017; Weston et al., 2018), as we leverage the seq2seq architecture to offer more flexible edits. 3 3.1 Position prediction layer Segment type: 2 KP-1 Bidirectional self-attention Causal attention Content Planning with BERT Our content planner is trained from BERT to assign ke"
2020.emnlp-main.57,2020.acl-main.22,0,0.0224551,"t relevance (Wiseman et al., 2018; Moryossef et al., 2019; Yao et al., 2019; Hua and Wang, 2019). However, it is still an open question to include content plans in large models, given the additional and expensive model retraining required. This work innovates by adding content plans as masked templates and designing refinement strategy to further boost generation performance, without architectural change. Controlled Text Generation. Our work is also in line with the study of controllability of neural text generation models. This includes manipulating the syntax (Duˇsek and Jurˇc´ıcˇ ek, 2016; Goyal and Durrett, 2020) and semantics (Wen et al., 2015; Chen et al., 2019) of the output. Specific applications encourage the model to cover a given topic (Wang et al., 2017; See et al., 2019), mention specified entities (Fan et al., 2018), or display a certain attribute (Hu et al., 2017; Luo et al., 2019; Balakrishnan et al., 2019). However, most existing work relies on model engineering, limiting the generalizability to new domains and adaptability to large pre-trained Transformers. One exception is the Plug and Play model (Dathathri et al., 2020), which directly modifies the key and value states of GPT2 (Radford"
2020.emnlp-main.57,W98-1414,0,0.481927,"if use iven lest until lest thoughoweverhethedrespitein fact nor yet xamfpulerther dditiionnsteadot onlythat is n beca g h w in a for e Figure 7: Discourse markers that are correctly and incorrectly (shaded) generated by PAIRfull , compared to aligned sentences in human references. Discourse markers are grouped (from left to right) into senses of C ONTINGENCY (higher marker generation accuracy observed), C OMPARISON, and E XPANSION. y-axis: # of generated sentences with the corresponding marker. Can PAIR correctly generate discourse markers? Since discourse markers are crucial for coherence (Grote and Stede, 1998; Callaway, 2003) and have received dedicated research efforts in rulebased systems (Reed et al., 2018; Balakrishnan et al., 2019), we examine if PAIRfull can properly generate them. For each sample, we construct sentence pairs based on content word overlaps between system generation and human reference. We manually select a set of unambiguous discourse markers from Appendix A of the Penn Discourse Treebank manual (Prasad et al., 2008). When a marker is present in the first three words in a reference sentence, we check if the corresponding system output does the same. Figure 7 displays the num"
2020.emnlp-main.57,P17-1141,0,0.0230561,"symbols. Figure 3 illustrates the template construction process and our seq2seq generation model. In Appendix B, we show statistics on the constructed templates. The input prompt x, keyphrase assignments m0 , and template t(0) are concatenated as the input to the encoder. The decoder then generates an output y (1) according to the model’s estimation of p(y (1) |x, m0 , t(0) ). y (1) is treated as a draft, to be further refined as described in the next section. Our method is substantially different from prior work that uses constrained decoding to enforce words to appear at specific positions (Hokamp and Liu, 2017; Post and Vilar, 2018; Hu et al., 2019), which is highly biased by the surrounding few words and suffers from disfluency. Since BART is trained to denoise the masked input with contextual understanding, it naturally benefits our method. Decoding. We employ the nucleus sampling strategy (Holtzman et al., 2019), which is shown to yield superior output quality in long text generation. In addition to the standard top-k sampling from tokens with the highest probabilities, nucleus sampling further limits possible choices based on a cumulative probability threshold (set to 0.9 in all experiments bel"
2020.emnlp-main.57,N19-1090,0,0.0408945,"Missing"
2020.emnlp-main.57,W18-2706,0,0.0230032,"ining required. This work innovates by adding content plans as masked templates and designing refinement strategy to further boost generation performance, without architectural change. Controlled Text Generation. Our work is also in line with the study of controllability of neural text generation models. This includes manipulating the syntax (Duˇsek and Jurˇc´ıcˇ ek, 2016; Goyal and Durrett, 2020) and semantics (Wen et al., 2015; Chen et al., 2019) of the output. Specific applications encourage the model to cover a given topic (Wang et al., 2017; See et al., 2019), mention specified entities (Fan et al., 2018), or display a certain attribute (Hu et al., 2017; Luo et al., 2019; Balakrishnan et al., 2019). However, most existing work relies on model engineering, limiting the generalizability to new domains and adaptability to large pre-trained Transformers. One exception is the Plug and Play model (Dathathri et al., 2020), which directly modifies the key and value states of GPT2 (Radford et al., 2019). However, since the signal is derived from the whole generated text, it is too coarse to provide precise sentence-level content control. Here, we instead gain fine-grained controllability through keyphr"
2020.emnlp-main.57,P19-1255,1,0.845067,"e 1: Statistics of the three datasets. We report average lengths of the prompt and the target generation, number of unique keyphrases (# KP) used in the input, and the percentage of content words in target covered by the keyphrases (KP Cov.). y (r) ← argminyi ∈Y GPT2-PPL(yi ); n ← |y (r) |× (1 − r/R); Mask n tokens with the lowest probabilities to create new template t(r) ; 4 116.6 205.6 282.7 Experiment Setups Tasks and Datasets We evaluate our generation and planning models on datasets from three distinct domains for multiparagraph-level text generation: (1) argument generation (A RG G EN) (Hua et al., 2019), to produce a counter-argument to refute a given proposition; (2) writing opinionated articles (O PINION), e.g., editorials and op-eds, to show idea exchange on a given subject; and (3) composing news reports (N EWS) to describe events. The three domains are selected with diverse levels of subjectivity and various communicative goals (persuading vs. informing), with statistics shown in Table 1. Task 1: Argument Generation. We first evaluate our models on persuasive argument generation, based on a dataset collected from Reddit r/ChangeMyView (CMV) in our prior work (Hua et al., 2019). This dat"
2020.emnlp-main.57,D19-1055,1,0.822656,"whatsoever that Trump has any coherent, commonly held beliefs on any topic. Figure 1: An argument generation example using Reddit ChangeMyView. [Top] Partial output by our planner with keyphrase assignment and positions (in subscripts) for each sentence, segmented by special token [SEN], from which a template is constructed. [Bottom] A draft is first produced and then refined, with updated words highlighted in italics. 2001; Stent et al., 2004). Specially designed control codes and auxiliary planning modules have been integrated into neural models (Keskar et al., 2019; Moryossef et al., 2019; Hua and Wang, 2019), yet those solutions require model architecture modification or retraining, making text generation with large models a very costly endeavor. To this end, this work aims to bring new insights into how to effectively incorporate content plans into large models to generate more rele781 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 781–793, c November 16–20, 2020. 2020 Association for Computational Linguistics vant and coherent text. We first study a planning model trained from BERT (Devlin et al., 2019) to produce the initial content plan, which as"
2020.emnlp-main.57,P14-1002,0,0.0110239,"than half of the cases, for both domains. Enforcing keyphrase generation based on their positions is also more favorable than not enforcing such constraint. ArgGen 15 10 10 5 5 0 KPSeq2seq Opinion News 15 10 5 PAIRlight PAIRfull Human Figure 6: Distributions of RST tree depth. PAIRfull better resembles the patterns in human-written texts. perts.4 Here, we aim to investigate whether contentcontrolled generation with ground-truth content plans resembles human-written text by studying discourse phenomena. Are PAIR generations similar to humanwritten text in discourse structure? We utilize DPLP (Ji and Eisenstein, 2014), an off-theshelf Rhetorical Structure Theory (RST) discourse parser. DPLP converts a given text into a binary tree, with elementary discourse units (EDUs, usually clauses) as nucleus and satellite nodes. For instance, a relation NS-elaboration indicates the second node as a satellite (S) elaborating on the first nucleus (N) node. DPLP achieves F1 scores of 81.6 for EDU detection and 71.0 for relation prediction on news articles from the annotated RST Discourse Treebank (Carlson et al., 2001). We run this trained model on our data for both human references and model generations. First, we anal"
2020.emnlp-main.57,W07-0734,0,0.0711632,") by removing keyphrase position information (s) from the input of our generator and using an initial template with all [MASK] symbols. Our model with full plans is denoted as PAIRfull . We first report generation results using ground-truth content plans constructed from human-written text, and also show the end-to-end results with predicted content plans by our planner. 5 5.1 Results Automatic Evaluation We report scores with BLEU (Papineni et al., 2002), which is based on n-gram precision (up to 4-grams); ROUGE-L (Lin, 2004), measuring recall of the longest common subsequences; and METEOR (Lavie and Agarwal, 2007), which accounts for paraphrase. For our models PAIRfull and PAIRlight , we evaluate both the first draft and the final output after refinement. Table 2 lists the results when ground-truth content plans are applied. First, our content-controlled generation model with planning consistently outperforms comparisons and other model variants on all datasets, with or without iterative refinement. Among our model variants, PAIRfull that has access to full content plans obtains significantly better scores than PAIRlight that only includes keyphrase assignments but not their positions. Lengths of PAIRf"
2020.emnlp-main.57,D19-1001,0,0.027846,"generated text, it is too coarse to provide precise sentence-level content control. Here, we instead gain fine-grained controllability through keyphrase assignment and positioning per sentence, which can be adapted to any off-the-shelf pre-trained Transformer generators. Iterative Refinement has been studied in machine translation (Lee et al., 2018; Freitag et al., 2019; Mansimov et al., 2019; Kasai et al., 2020) to gradually improve translation quality. Refinement is also used with masked language models to improve fluency of non-autoregressive generation outputs (Ghazvininejad et al., 2019; Lawrence et al., 2019). Our work uses BART (Lewis et al., 2020), a state-of-the-art seq2seq model that offers better generalizability and stronger capacity for long text generation. Our proposed strategy substantially differs from prior solutions that rely on in-place word substitutions (Novak et al., 2016; Xia et al., 2017; Weston et al., 2018), as we leverage the seq2seq architecture to offer more flexible edits. 3 3.1 Position prediction layer Segment type: 2 KP-1 Bidirectional self-attention Causal attention Content Planning with BERT Our content planner is trained from BERT to assign keyphrases to different se"
2020.emnlp-main.57,D18-1149,0,0.0472411,"o new domains and adaptability to large pre-trained Transformers. One exception is the Plug and Play model (Dathathri et al., 2020), which directly modifies the key and value states of GPT2 (Radford et al., 2019). However, since the signal is derived from the whole generated text, it is too coarse to provide precise sentence-level content control. Here, we instead gain fine-grained controllability through keyphrase assignment and positioning per sentence, which can be adapted to any off-the-shelf pre-trained Transformer generators. Iterative Refinement has been studied in machine translation (Lee et al., 2018; Freitag et al., 2019; Mansimov et al., 2019; Kasai et al., 2020) to gradually improve translation quality. Refinement is also used with masked language models to improve fluency of non-autoregressive generation outputs (Ghazvininejad et al., 2019; Lawrence et al., 2019). Our work uses BART (Lewis et al., 2020), a state-of-the-art seq2seq model that offers better generalizability and stronger capacity for long text generation. Our proposed strategy substantially differs from prior solutions that rely on in-place word substitutions (Novak et al., 2016; Xia et al., 2017; Weston et al., 2018), a"
2020.emnlp-main.57,2020.acl-main.703,0,0.51322,"ally enhance the generation quality within the sequence-tosequence framework. Evaluation with automatic metrics shows that adding planning consistently improves the generation quality on three distinct domains, with an average of 20 BLEU points and 12 METEOR points improvements. In addition, human judges rate our system outputs to be more relevant and coherent than comparisons without planning. Introduction Large pre-trained language models are the cornerstone of many state-of-the-art models in various natural language understanding and generation tasks (Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020), yet they are far from perfect. In generation tasks, although models like GPT-2 (Radford et al., 2019) are able to produce plausible text, their spontaneous nature limits their utility in actual applications, e.g., users cannot specify what contents to include, and in what order. To make large models more useful in practice, and to improve their generation quality, we believe it is critical to inform them of when to say what, which is addressed as content planning in traditional generation systems (Duboue and McKeown, I: Template construction Template: (1) __0 __1 __2 a communist __5 __6 __7"
2020.emnlp-main.57,N16-1014,0,0.0599516,"20/ 2 https://catalog.ldc.upenn.edu/ LDC2008T19 782 BERT to facilitate long-form text generation. • We present a novel template mask-and-fill method to incorporate content planning into generation models based on BART. • We devise an iterative refinement algorithm that works within the seq2seq framework to flexibly improve the generation quality. 2 Related Work Content Planning as a Generation Component. Despite the impressive progress made in many generation tasks, neural systems are known to produce low-quality content (Wiseman et al., 2017; Rohrbach et al., 2018), often with low relevance (Li et al., 2016) and poor discourse structure (Zhao et al., 2017; Xu et al., 2020). Consequently, planning modules are designed and added into neural systems to enhance content relevance (Wiseman et al., 2018; Moryossef et al., 2019; Yao et al., 2019; Hua and Wang, 2019). However, it is still an open question to include content plans in large models, given the additional and expensive model retraining required. This work innovates by adding content plans as masked templates and designing refinement strategy to further boost generation performance, without architectural change. Controlled Text Generation. Our"
2020.emnlp-main.57,W04-1013,0,0.0520632,"ence-level keyphrase assignments helps, we include a model variant (PAIRlight ) by removing keyphrase position information (s) from the input of our generator and using an initial template with all [MASK] symbols. Our model with full plans is denoted as PAIRfull . We first report generation results using ground-truth content plans constructed from human-written text, and also show the end-to-end results with predicted content plans by our planner. 5 5.1 Results Automatic Evaluation We report scores with BLEU (Papineni et al., 2002), which is based on n-gram precision (up to 4-grams); ROUGE-L (Lin, 2004), measuring recall of the longest common subsequences; and METEOR (Lavie and Agarwal, 2007), which accounts for paraphrase. For our models PAIRfull and PAIRlight , we evaluate both the first draft and the final output after refinement. Table 2 lists the results when ground-truth content plans are applied. First, our content-controlled generation model with planning consistently outperforms comparisons and other model variants on all datasets, with or without iterative refinement. Among our model variants, PAIRfull that has access to full content plans obtains significantly better scores than P"
2020.emnlp-main.57,C00-1072,0,0.118457,"). This dataset contains pairs of original post (OP) statement on a controversial issue about politics and filtered high-quality counter-arguments, covering 14, 833 threads from 2013 to 2018. We use the OP title, which contains a proposition (e.g. the minimum wage should be abolished), to form the input prompt x. In our prior work, only the first paragraphs of high-quality counter-arguments are used for generation. Here we consider generating the full post, which is significantly longer. Keyphrases are identified as noun phrases and verb phrases that contain at least one topic signature word (Lin and Hovy, 2000), which is determined by a log-likelihood ratio test that indicates word salience. Following our prior work, we expand the set of topic signatures with their synonyms, hyponyms, hypernyms, and antonyms according to WordNet (Miller, 1994). The keyphrases longer than 10 tokens are further discarded. Task 2: Opinion Article Generation. We collect opinion articles from the New York Times (NYT) corpus (Sandhaus, 2008). An article is selected if its taxonomies label has a prefix of Top/Opinion. We eliminate articles with an empty headline or less than three sentences. Keyphrases are extracted in a s"
2020.emnlp-main.57,2021.ccl-1.108,0,0.0533191,"Missing"
2020.emnlp-main.57,P19-1603,0,0.0230142,"d templates and designing refinement strategy to further boost generation performance, without architectural change. Controlled Text Generation. Our work is also in line with the study of controllability of neural text generation models. This includes manipulating the syntax (Duˇsek and Jurˇc´ıcˇ ek, 2016; Goyal and Durrett, 2020) and semantics (Wen et al., 2015; Chen et al., 2019) of the output. Specific applications encourage the model to cover a given topic (Wang et al., 2017; See et al., 2019), mention specified entities (Fan et al., 2018), or display a certain attribute (Hu et al., 2017; Luo et al., 2019; Balakrishnan et al., 2019). However, most existing work relies on model engineering, limiting the generalizability to new domains and adaptability to large pre-trained Transformers. One exception is the Plug and Play model (Dathathri et al., 2020), which directly modifies the key and value states of GPT2 (Radford et al., 2019). However, since the signal is derived from the whole generated text, it is too coarse to provide precise sentence-level content control. Here, we instead gain fine-grained controllability through keyphrase assignment and positioning per sentence, which can be adapted t"
2020.emnlp-main.57,N19-1236,0,0.0340599,"Missing"
2020.emnlp-main.57,P02-1040,0,0.107818,"catenation of the prompt and the unordered keyphrase set. To study if using only sentence-level keyphrase assignments helps, we include a model variant (PAIRlight ) by removing keyphrase position information (s) from the input of our generator and using an initial template with all [MASK] symbols. Our model with full plans is denoted as PAIRfull . We first report generation results using ground-truth content plans constructed from human-written text, and also show the end-to-end results with predicted content plans by our planner. 5 5.1 Results Automatic Evaluation We report scores with BLEU (Papineni et al., 2002), which is based on n-gram precision (up to 4-grams); ROUGE-L (Lin, 2004), measuring recall of the longest common subsequences; and METEOR (Lavie and Agarwal, 2007), which accounts for paraphrase. For our models PAIRfull and PAIRlight , we evaluate both the first draft and the final output after refinement. Table 2 lists the results when ground-truth content plans are applied. First, our content-controlled generation model with planning consistently outperforms comparisons and other model variants on all datasets, with or without iterative refinement. Among our model variants, PAIRfull that ha"
2020.emnlp-main.57,N18-1119,0,0.0203061,"strates the template construction process and our seq2seq generation model. In Appendix B, we show statistics on the constructed templates. The input prompt x, keyphrase assignments m0 , and template t(0) are concatenated as the input to the encoder. The decoder then generates an output y (1) according to the model’s estimation of p(y (1) |x, m0 , t(0) ). y (1) is treated as a draft, to be further refined as described in the next section. Our method is substantially different from prior work that uses constrained decoding to enforce words to appear at specific positions (Hokamp and Liu, 2017; Post and Vilar, 2018; Hu et al., 2019), which is highly biased by the surrounding few words and suffers from disfluency. Since BART is trained to denoise the masked input with contextual understanding, it naturally benefits our method. Decoding. We employ the nucleus sampling strategy (Holtzman et al., 2019), which is shown to yield superior output quality in long text generation. In addition to the standard top-k sampling from tokens with the highest probabilities, nucleus sampling further limits possible choices based on a cumulative probability threshold (set to 0.9 in all experiments below). We also require t"
2020.emnlp-main.57,prasad-etal-2008-penn,0,0.0338381,"Missing"
2020.emnlp-main.57,W18-6535,0,0.01962,"hat is n beca g h w in a for e Figure 7: Discourse markers that are correctly and incorrectly (shaded) generated by PAIRfull , compared to aligned sentences in human references. Discourse markers are grouped (from left to right) into senses of C ONTINGENCY (higher marker generation accuracy observed), C OMPARISON, and E XPANSION. y-axis: # of generated sentences with the corresponding marker. Can PAIR correctly generate discourse markers? Since discourse markers are crucial for coherence (Grote and Stede, 1998; Callaway, 2003) and have received dedicated research efforts in rulebased systems (Reed et al., 2018; Balakrishnan et al., 2019), we examine if PAIRfull can properly generate them. For each sample, we construct sentence pairs based on content word overlaps between system generation and human reference. We manually select a set of unambiguous discourse markers from Appendix A of the Penn Discourse Treebank manual (Prasad et al., 2008). When a marker is present in the first three words in a reference sentence, we check if the corresponding system output does the same. Figure 7 displays the numbers of generated sentences with markers produced as the same in human references (correct) or not (wr"
2020.emnlp-main.57,D18-1437,0,0.0128748,"ble at: http://xinyuhua. github.io/Resources/emnlp20/ 2 https://catalog.ldc.upenn.edu/ LDC2008T19 782 BERT to facilitate long-form text generation. • We present a novel template mask-and-fill method to incorporate content planning into generation models based on BART. • We devise an iterative refinement algorithm that works within the seq2seq framework to flexibly improve the generation quality. 2 Related Work Content Planning as a Generation Component. Despite the impressive progress made in many generation tasks, neural systems are known to produce low-quality content (Wiseman et al., 2017; Rohrbach et al., 2018), often with low relevance (Li et al., 2016) and poor discourse structure (Zhao et al., 2017; Xu et al., 2020). Consequently, planning modules are designed and added into neural systems to enhance content relevance (Wiseman et al., 2018; Moryossef et al., 2019; Yao et al., 2019; Hua and Wang, 2019). However, it is still an open question to include content plans in large models, given the additional and expensive model retraining required. This work innovates by adding content plans as masked templates and designing refinement strategy to further boost generation performance, without architectu"
2020.emnlp-main.57,N19-1170,0,0.0238819,"given the additional and expensive model retraining required. This work innovates by adding content plans as masked templates and designing refinement strategy to further boost generation performance, without architectural change. Controlled Text Generation. Our work is also in line with the study of controllability of neural text generation models. This includes manipulating the syntax (Duˇsek and Jurˇc´ıcˇ ek, 2016; Goyal and Durrett, 2020) and semantics (Wen et al., 2015; Chen et al., 2019) of the output. Specific applications encourage the model to cover a given topic (Wang et al., 2017; See et al., 2019), mention specified entities (Fan et al., 2018), or display a certain attribute (Hu et al., 2017; Luo et al., 2019; Balakrishnan et al., 2019). However, most existing work relies on model engineering, limiting the generalizability to new domains and adaptability to large pre-trained Transformers. One exception is the Plug and Play model (Dathathri et al., 2020), which directly modifies the key and value states of GPT2 (Radford et al., 2019). However, since the signal is derived from the whole generated text, it is too coarse to provide precise sentence-level content control. Here, we instead g"
2020.emnlp-main.57,P16-1162,0,0.0653862,"Missing"
2020.emnlp-main.57,P04-1011,0,0.144683,"nal generation): (1) To call him a communist, you must begin with that he has some kind of coherent ideology in the first place. (2) He does not. (3) There is no evidence whatsoever that Trump has any coherent, commonly held beliefs on any topic. Figure 1: An argument generation example using Reddit ChangeMyView. [Top] Partial output by our planner with keyphrase assignment and positions (in subscripts) for each sentence, segmented by special token [SEN], from which a template is constructed. [Bottom] A draft is first produced and then refined, with updated words highlighted in italics. 2001; Stent et al., 2004). Specially designed control codes and auxiliary planning modules have been integrated into neural models (Keskar et al., 2019; Moryossef et al., 2019; Hua and Wang, 2019), yet those solutions require model architecture modification or retraining, making text generation with large models a very costly endeavor. To this end, this work aims to bring new insights into how to effectively incorporate content plans into large models to generate more rele781 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 781–793, c November 16–20, 2020. 2020 Association"
2020.emnlp-main.57,D17-1228,0,0.0595852,"Missing"
2020.emnlp-main.57,D15-1199,0,0.0687308,"Missing"
2020.emnlp-main.57,W18-5713,0,0.0268886,"ation (Lee et al., 2018; Freitag et al., 2019; Mansimov et al., 2019; Kasai et al., 2020) to gradually improve translation quality. Refinement is also used with masked language models to improve fluency of non-autoregressive generation outputs (Ghazvininejad et al., 2019; Lawrence et al., 2019). Our work uses BART (Lewis et al., 2020), a state-of-the-art seq2seq model that offers better generalizability and stronger capacity for long text generation. Our proposed strategy substantially differs from prior solutions that rely on in-place word substitutions (Novak et al., 2016; Xia et al., 2017; Weston et al., 2018), as we leverage the seq2seq architecture to offer more flexible edits. 3 3.1 Position prediction layer Segment type: 2 KP-1 Bidirectional self-attention Causal attention Content Planning with BERT Our content planner is trained from BERT to assign keyphrases to different sentences and predict their corresponding positions. As shown in Figure 2, the concatenation of prompt x and unordered keyphrases m is encoded with bidirectional selfattentions. Keyphrase assignments are produced autoregressively as a sequence of tokens m0 = {wj }, with their positions in the sentence s = {sj } predicted as a"
2020.emnlp-main.57,D18-1356,0,0.0292943,"neration models based on BART. • We devise an iterative refinement algorithm that works within the seq2seq framework to flexibly improve the generation quality. 2 Related Work Content Planning as a Generation Component. Despite the impressive progress made in many generation tasks, neural systems are known to produce low-quality content (Wiseman et al., 2017; Rohrbach et al., 2018), often with low relevance (Li et al., 2016) and poor discourse structure (Zhao et al., 2017; Xu et al., 2020). Consequently, planning modules are designed and added into neural systems to enhance content relevance (Wiseman et al., 2018; Moryossef et al., 2019; Yao et al., 2019; Hua and Wang, 2019). However, it is still an open question to include content plans in large models, given the additional and expensive model retraining required. This work innovates by adding content plans as masked templates and designing refinement strategy to further boost generation performance, without architectural change. Controlled Text Generation. Our work is also in line with the study of controllability of neural text generation models. This includes manipulating the syntax (Duˇsek and Jurˇc´ıcˇ ek, 2016; Goyal and Durrett, 2020) and sema"
2020.emnlp-main.57,2020.acl-main.451,0,0.0414232,"tate long-form text generation. • We present a novel template mask-and-fill method to incorporate content planning into generation models based on BART. • We devise an iterative refinement algorithm that works within the seq2seq framework to flexibly improve the generation quality. 2 Related Work Content Planning as a Generation Component. Despite the impressive progress made in many generation tasks, neural systems are known to produce low-quality content (Wiseman et al., 2017; Rohrbach et al., 2018), often with low relevance (Li et al., 2016) and poor discourse structure (Zhao et al., 2017; Xu et al., 2020). Consequently, planning modules are designed and added into neural systems to enhance content relevance (Wiseman et al., 2018; Moryossef et al., 2019; Yao et al., 2019; Hua and Wang, 2019). However, it is still an open question to include content plans in large models, given the additional and expensive model retraining required. This work innovates by adding content plans as masked templates and designing refinement strategy to further boost generation performance, without architectural change. Controlled Text Generation. Our work is also in line with the study of controllability of neural t"
2020.emnlp-main.57,P17-1061,0,0.026739,"782 BERT to facilitate long-form text generation. • We present a novel template mask-and-fill method to incorporate content planning into generation models based on BART. • We devise an iterative refinement algorithm that works within the seq2seq framework to flexibly improve the generation quality. 2 Related Work Content Planning as a Generation Component. Despite the impressive progress made in many generation tasks, neural systems are known to produce low-quality content (Wiseman et al., 2017; Rohrbach et al., 2018), often with low relevance (Li et al., 2016) and poor discourse structure (Zhao et al., 2017; Xu et al., 2020). Consequently, planning modules are designed and added into neural systems to enhance content relevance (Wiseman et al., 2018; Moryossef et al., 2019; Yao et al., 2019; Hua and Wang, 2019). However, it is still an open question to include content plans in large models, given the additional and expensive model retraining required. This work innovates by adding content plans as masked templates and designing refinement strategy to further boost generation performance, without architectural change. Controlled Text Generation. Our work is also in line with the study of controlla"
2021.acl-long.501,P18-2112,0,0.129376,"items, and the best aligned ones (learned by our model) are highlighted in corresponding colors. We also underline words that reflect the input concepts and entities. Introduction Opinion articles serve as an important media to convey the authors’ values, beliefs, and stances on important societal issues. Automatically generating long-form opinion articles has the potential of facilitating various tasks, such as essay writing and speech drafting, and it is the focus of this work. Though opinion generation has been investigated for constructing arguments (Hua and Wang, 2018), writing reviews (Ni and McAuley, 2018), and producing emotional dialogue responses (Song et al., 2019), those outputs are relatively short. While impressive progress in generation has been achieved by using large pre-trained Transformers (Radford et al., 2019; Lewis et al., 2020a), directly adopting them for long-form opinion text generation poses distinct challenges. First, large models still fall short of producing coherent text due to the lack of efficient content control and planning (Ko and Li, 2020; Wu et al., 2020; Tan et al., 2021). A common solution is to use concatenated phrases or semantic representations to guide the g"
2021.acl-long.501,W17-5525,0,0.0473187,"Missing"
2021.acl-long.501,P02-1040,0,0.109097,"Missing"
2021.acl-long.501,2020.coling-main.1,0,0.0229394,"ning a 6408 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6408–6423 August 1–6, 2021. ©2021 Association for Computational Linguistics separate planning module to produce sorted content, which is then fed into a generator (Fan et al., 2019; Hua and Wang, 2020; Goldfarb-Tarrant et al., 2020). Nonetheless, this strategy results in a disconnection between planning and realization, and the output is not guaranteed to respect the planning results (Castro Ferreira et al., 2019; Prabhumoye et al., 2020). The second challenge for opinion generation resides in the diversity of information that is needed to produce an output with consistent stances and supported by pertinent facts. Though large models memorize significant amounts of knowledge, they cannot retrieve and operate with them precisely (Lewis et al., 2020b). Due to the argumentative nature of opinion text, simply including knowledge bases (Guan et al., 2020; Zhou et al., 2020) is insufficient to uphold the desired quality, as it requires the combination of subjective claims and objective evidence as supports. To this end, we propose a"
2021.acl-long.501,P19-1195,0,0.0476512,"Missing"
2021.acl-long.501,D19-1054,0,0.0218759,"y existing models that use implicit planning with attentions or hard copying. • We propose content plan augmentation by automatically generating relevant concepts and claims. • We construct two opinion text generation datasets with content plans that capture prominent entities and concepts. 2 Related Work Neural Generation with Planning. Text planning is seen as a crucial step to guide the generation of high-quality, well-organized natural language text (McKeown, 1992; Reiter and Dale, 2000). Incorporating planning modules to neural text generator has attracted significant research interests (Shen et al., 2019; Moryossef et al., 2019; Puduppully et al., 2019), which proves to be especially beneficial for long-form output (Fan et al., 2019; Hua and Wang, 2019). More recently, large pre-trained Transformers have established new state-of-the-arts for a wide range of text generation tasks (Lewis et al., 2020a; Roller et al., 2020; Kale and Rastogi, 2020). But it is non-trivial to integrate planning modules into them. Existing approaches resort to decoupling planning and decoding stages (Hua and Wang, 2020; Kedzie and McKeown, 2020), which inevitably increases system complexities and potentially introdu"
2021.acl-long.501,P19-1359,0,0.107421,"ghted in corresponding colors. We also underline words that reflect the input concepts and entities. Introduction Opinion articles serve as an important media to convey the authors’ values, beliefs, and stances on important societal issues. Automatically generating long-form opinion articles has the potential of facilitating various tasks, such as essay writing and speech drafting, and it is the focus of this work. Though opinion generation has been investigated for constructing arguments (Hua and Wang, 2018), writing reviews (Ni and McAuley, 2018), and producing emotional dialogue responses (Song et al., 2019), those outputs are relatively short. While impressive progress in generation has been achieved by using large pre-trained Transformers (Radford et al., 2019; Lewis et al., 2020a), directly adopting them for long-form opinion text generation poses distinct challenges. First, large models still fall short of producing coherent text due to the lack of efficient content control and planning (Ko and Li, 2020; Wu et al., 2020; Tan et al., 2021). A common solution is to use concatenated phrases or semantic representations to guide the generation process (Yao et al., 2019; Harkous et al., 2020; Ribei"
2021.acl-long.501,spitkovsky-chang-2012-cross,0,0.0254233,"Missing"
2021.acl-long.501,2020.acl-main.513,0,0.0282811,"presentations (Wen et al., 2015; Elder et al., 2018). Our work differs from all previous methods as we combine different types of content, covering both objective and subjective information, and attain fine-grained sentence-level control using a novel design of mixed conditional language models. Opinion Text Generation. Our model tackles opinion articles, which differs from traditional text generation systems that mostly concern fact-based generations (Gardent et al., 2017; Novikova et al., 2017; Puduppully et al., 2019). An extensive body of work has studied summarizing (Wang and Ling, 2016; Suhara et al., 2020; Braˇzinskas et al., 2020) or generating (Ni and McAuley, 2018; Li et al., 2019) reviews and building dialogue systems enhanced with emotions (Li et al., 2016; Song et al., 2019). More recently, developments are made in generating argumentative text (El Baff et al., 2019; Hidey and McKeown, 2019), which primarily focus on constructing single sentence claims on a limited number of topics. In comparison, our model can handle substantially longer output with improved quality. 3 Model Task Formulation. Our opinion text generation framework takes as input a set of content items. Each content item"
2021.acl-long.501,N16-1007,1,0.690183,"2020), or semantic representations (Wen et al., 2015; Elder et al., 2018). Our work differs from all previous methods as we combine different types of content, covering both objective and subjective information, and attain fine-grained sentence-level control using a novel design of mixed conditional language models. Opinion Text Generation. Our model tackles opinion articles, which differs from traditional text generation systems that mostly concern fact-based generations (Gardent et al., 2017; Novikova et al., 2017; Puduppully et al., 2019). An extensive body of work has studied summarizing (Wang and Ling, 2016; Suhara et al., 2020; Braˇzinskas et al., 2020) or generating (Ni and McAuley, 2018; Li et al., 2019) reviews and building dialogue systems enhanced with emotions (Li et al., 2016; Song et al., 2019). More recently, developments are made in generating argumentative text (El Baff et al., 2019; Hidey and McKeown, 2019), which primarily focus on constructing single sentence claims on a limited number of topics. In comparison, our model can handle substantially longer output with improved quality. 3 Model Task Formulation. Our opinion text generation framework takes as input a set of content item"
2021.acl-long.501,D15-1199,0,0.0765657,"Missing"
2021.acl-long.501,2020.emnlp-demos.6,0,0.0416807,"Missing"
2021.acl-long.501,2020.emnlp-main.226,0,0.0353234,"for long text generation and offers better interpretability. Concurrent work by Zhang et al. (2021) presents a mixtureof-expert decoder to tackle knowledge-grounded generation. However, their score distribution for language models is fixed across all decoding steps, whereas ours is updated as generation progresses and can better reflect the dynamic nature of content planning. Controllable Text Generation. Another related line of research investigates the controllability of generation models (Wiseman et al., 2017), including conditioning over keywords (Keskar et al., 2019; Hua and Wang, 2020; Xu et al., 2020), syntactic structures (Casas et al., 2020; Goyal and Durrett, 2020), or semantic representations (Wen et al., 2015; Elder et al., 2018). Our work differs from all previous methods as we combine different types of content, covering both objective and subjective information, and attain fine-grained sentence-level control using a novel design of mixed conditional language models. Opinion Text Generation. Our model tackles opinion articles, which differs from traditional text generation systems that mostly concern fact-based generations (Gardent et al., 2017; Novikova et al., 2017; Puduppully et"
2021.acl-long.502,2020.acl-main.703,0,0.112178,"t introduce a new question type ontology, drawn upon researches in cognitive science and psychology (Graesser et al., 1992), to capture deeper levels of cognition, such as causal reasoning and judgments. Based on the new ontology, we collect and annotate a dataset of 4,959 questions to benefit research in both question generation and answering.1 We then design a type-aware framework to jointly predict question focuses (what to ask about) and generate questions (how to ask it). Different from pipeline-based approaches (e.g., Sun et al. (2018)), our framework is built on large pre-trained BART (Lewis et al., 2020), and uses shared representations to jointly conduct question focus prediction and question generation while learning taskspecific knowledge. It is further augmented by a semantic graph that leverages both semantic roles and dependency relations, facilitating long text comprehension to pinpoint salient concepts. Moreover, to achieve the goal of producing various types of questions from the same input, we investigate two model variants that use templates to improve controllability and generation diversity: one using pre-identified exemplars, the other employing generated templates to guide ques"
2021.acl-long.502,D19-1317,0,0.0183134,"quit being in [NP]? ⇒ How do I get my son to quit being in a gang? (P ROCEDURAL) - What are [NP]? ⇒ What are some programs for teenagers involved in gangs? (E XAMPLE) - Why do [NP] [V] [NP]? ⇒ Why do teenagers identify gangs? (C AUSE) Introduction Question-asking has long served as an effective instrument for knowledge learning (Andre, 1979; Tobin, 1990) and assessing learning progress (Holme, 2003; Downing and Yudkowsky, 2009; Livingston, 2009). Compared to the widely studied task of generating factoid questions that inquire about “one bit” of information (Du et al., 2017; Duan et al., 2017; Li et al., 2019), this work is interested in generating open-ended questions that require deep comprehension and long-form answers (Labutov et al., 2015). Such open-ended questions are valuable in education, e.g., to facilitate complex knowledge acquisition (Lai et al., 2017) and nurture reasoning skills (Shapley, 2000), as well as in other applications like improving search engines (Han Figure 1: Open-ended questions generated by different models after reading the same input: (1) BART decoded with nucleus sampling, (2) BART that considers different question words, and (3) our type-aware generator T PL G EN,"
2021.acl-long.502,W04-1013,0,0.0125448,"the answer and is trained on our training sets. We follow Liu et al. (2020) and use 9 categories of question words. For both our models and BART+QT YPE, the most confident type predicted by the classifier γa (described in § 3.2), which reads in the answer, is used as input. To test the efficacy of semantic graphs, we further compare with a variant of J OINT G EN that only uses the flat Transformer for focus prediction and question generation, denoted as J OINT G EN w/o graph. We evaluate the generated questions with BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and ROUGE-L (Lin, 2004).5 Results on both Yahoo and Reddit datasets are reported in Table 2. Our J OINT G EN outperforms all comparisons on both datasets over all automatic evaluation metrics except for METEOR on Reddit. When taking out the semantic graphs, model performance degrades substantially, which suggests that 5 We do not consider using Q-BLEU (Nema and Khapra, 2018) since it weighs question words highly. 6429 B-4 D EEP QG 6.53 BART 21.88 BART+QW ORD 22.02 Type-aware Models BART+QT YPE 22.12 J OINT G EN (ours) 22.56∗ w/o graph 22.21 E XPL G EN (ours) 21.74 T PL G EN (ours) 21.51 Yahoo MTR R-L B-4 Reddit MTR"
2021.acl-long.502,2021.ccl-1.108,0,0.0645983,"Missing"
2021.acl-long.502,N19-4009,0,0.044018,"Missing"
2021.acl-long.502,2020.findings-emnlp.416,0,0.142698,"cations like improving search engines (Han Figure 1: Open-ended questions generated by different models after reading the same input: (1) BART decoded with nucleus sampling, (2) BART that considers different question words, and (3) our type-aware generator T PL G EN, that predicts focuses and operates with generated templates (to the left of the arrows). Questions generated by our model have diverse T YPEs. et al., 2019) and building open-domain dialogue systems (Shum et al., 2018). Significant progress has been made in generating factoid questions (Zhang and Bansal, 2019; Zhou et al., 2019b; Su et al., 2020), yet new challenges need to be addressed for open-ended questions. First, specifying the question type is crucial for constructing meaningful questions (Graesser et al., 1992). Question words such as “why” and “when” are generally seen as being indicative of types (Zhou et al., 2019b), but they underspecify the conceptual content of questions (Olney et al., 2012). Using Figure 1 as an example, different 6424 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6424–6439 August"
2021.acl-long.502,2020.acl-main.500,0,0.196029,"t procedures. It thus calls for a new question type ontology that can precisely capture the conceptual nature of questions. Second, constructing questions from a text with multiple sentences needs to focus on its central concepts or phenomena that necessitate extensive descriptions. New representations are needed to capture such content as question focus(es), to go beyond existing methods that rely on entities and their neighboring words (Du et al., 2017; Sun et al., 2018) even though they are effective for generating factoid questions. Third, encouraging the diversity of generated questions (Sultan et al., 2020; Wang et al., 2020) is less explored but critical for real world applications, e.g., various questions should be proposed to gauge how well students grasp the knowledge of complex subjects. In this work, we aim to address the challenges of generating open-ended questions from input consisting of multiple sentences. We first introduce a new question type ontology, drawn upon researches in cognitive science and psychology (Graesser et al., 1992), to capture deeper levels of cognition, such as causal reasoning and judgments. Based on the new ontology, we collect and annotate a dataset of 4,959 q"
2021.acl-long.502,D18-1427,0,0.146862,"ugust 1–6, 2021. ©2021 Association for Computational Linguistics question words, i.e., both “how” and “what”, can be used for inquiring about procedures. It thus calls for a new question type ontology that can precisely capture the conceptual nature of questions. Second, constructing questions from a text with multiple sentences needs to focus on its central concepts or phenomena that necessitate extensive descriptions. New representations are needed to capture such content as question focus(es), to go beyond existing methods that rely on entities and their neighboring words (Du et al., 2017; Sun et al., 2018) even though they are effective for generating factoid questions. Third, encouraging the diversity of generated questions (Sultan et al., 2020; Wang et al., 2020) is less explored but critical for real world applications, e.g., various questions should be proposed to gauge how well students grasp the knowledge of complex subjects. In this work, we aim to address the challenges of generating open-ended questions from input consisting of multiple sentences. We first introduce a new question type ontology, drawn upon researches in cognitive science and psychology (Graesser et al., 1992), to captu"
2021.acl-long.502,2020.findings-emnlp.194,0,0.143553,"calls for a new question type ontology that can precisely capture the conceptual nature of questions. Second, constructing questions from a text with multiple sentences needs to focus on its central concepts or phenomena that necessitate extensive descriptions. New representations are needed to capture such content as question focus(es), to go beyond existing methods that rely on entities and their neighboring words (Du et al., 2017; Sun et al., 2018) even though they are effective for generating factoid questions. Third, encouraging the diversity of generated questions (Sultan et al., 2020; Wang et al., 2020) is less explored but critical for real world applications, e.g., various questions should be proposed to gauge how well students grasp the knowledge of complex subjects. In this work, we aim to address the challenges of generating open-ended questions from input consisting of multiple sentences. We first introduce a new question type ontology, drawn upon researches in cognitive science and psychology (Graesser et al., 1992), to capture deeper levels of cognition, such as causal reasoning and judgments. Based on the new ontology, we collect and annotate a dataset of 4,959 questions to benefit"
2021.acl-long.502,D18-1259,0,0.030412,"we create a separate layer of cross attentions that is dedicated to the semantic graph, while prior work uses the same set of attentions to attend to the concatenated text and graph representations. 1 Our data and code are available at: https:// shuyangcao.github.io/projects/ontology_ open_ended_question. 6425 2 3 https://answers.yahoo.com/ https://www.reddit.com/ Given the data-driven nature of question generation and answering tasks, recent studies take advantage of the availability of large-scale QA datasets, such as SQuAD (Rajpurkar et al., 2016), MS MARCO (Bajaj et al., 2016), HotpotQA (Yang et al., 2018), DROP (Dua et al., 2019), inter alia. These corpora mainly contain factoid questions, while our newly collected datasets are not only larger in size but also comprise significantly more open-ended questions for querying reasons and procedures. A dataset closer to ours is ELI5 (Fan et al., 2019), which also obtains open-ended questionanswer pairs from Reddit, while one of our datasets includes more Reddit communities and thus covers a wider range of topics. Our work is more inline with generating deeper questions with responses that span over multiple sentences, where manually constructed temp"
2021.acl-long.502,D19-1253,0,0.0868286,"s (Shapley, 2000), as well as in other applications like improving search engines (Han Figure 1: Open-ended questions generated by different models after reading the same input: (1) BART decoded with nucleus sampling, (2) BART that considers different question words, and (3) our type-aware generator T PL G EN, that predicts focuses and operates with generated templates (to the left of the arrows). Questions generated by our model have diverse T YPEs. et al., 2019) and building open-domain dialogue systems (Shum et al., 2018). Significant progress has been made in generating factoid questions (Zhang and Bansal, 2019; Zhou et al., 2019b; Su et al., 2020), yet new challenges need to be addressed for open-ended questions. First, specifying the question type is crucial for constructing meaningful questions (Graesser et al., 1992). Question words such as “why” and “when” are generally seen as being indicative of types (Zhou et al., 2019b), but they underspecify the conceptual content of questions (Olney et al., 2012). Using Figure 1 as an example, different 6424 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Langua"
2021.acl-long.502,D19-1337,0,0.0723476,"ll as in other applications like improving search engines (Han Figure 1: Open-ended questions generated by different models after reading the same input: (1) BART decoded with nucleus sampling, (2) BART that considers different question words, and (3) our type-aware generator T PL G EN, that predicts focuses and operates with generated templates (to the left of the arrows). Questions generated by our model have diverse T YPEs. et al., 2019) and building open-domain dialogue systems (Shum et al., 2018). Significant progress has been made in generating factoid questions (Zhang and Bansal, 2019; Zhou et al., 2019b; Su et al., 2020), yet new challenges need to be addressed for open-ended questions. First, specifying the question type is crucial for constructing meaningful questions (Graesser et al., 1992). Question words such as “why” and “when” are generally seen as being indicative of types (Zhou et al., 2019b), but they underspecify the conceptual content of questions (Olney et al., 2012). Using Figure 1 as an example, different 6424 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, page"
2021.acl-long.502,D19-1622,0,0.0748991,"ll as in other applications like improving search engines (Han Figure 1: Open-ended questions generated by different models after reading the same input: (1) BART decoded with nucleus sampling, (2) BART that considers different question words, and (3) our type-aware generator T PL G EN, that predicts focuses and operates with generated templates (to the left of the arrows). Questions generated by our model have diverse T YPEs. et al., 2019) and building open-domain dialogue systems (Shum et al., 2018). Significant progress has been made in generating factoid questions (Zhang and Bansal, 2019; Zhou et al., 2019b; Su et al., 2020), yet new challenges need to be addressed for open-ended questions. First, specifying the question type is crucial for constructing meaningful questions (Graesser et al., 1992). Question words such as “why” and “when” are generally seen as being indicative of types (Zhou et al., 2019b), but they underspecify the conceptual content of questions (Olney et al., 2012). Using Figure 1 as an example, different 6424 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, page"
2021.emnlp-main.532,2021.acl-long.474,0,0.0339249,"active summaries often contain unfaithful content with regard to the source (Falke et al., 2019). To improve summary factuality, three major types of approaches are proposed. First, a separate correction model is learned to fix errors made by the summarizers (Zhao et al., 2020; Chen et al., 2021), including replacing entities absent from the source (Dong et al., 2020) or revising all possible errors (Cao et al., 2020). The second type targets at modifying the sequence-to-sequence architecture to incorporate relation triplets (Cao et al., 2018), knowledge graphs (Zhu et al., 2021), and topics (Aralikatte et al., 2021) to inform the summarizers of article facts. Yet additional engineering efforts and model retraining are often needed. Finally, discarding noisy samples from model training has also been investigated (Nan et al., 2021; Goyal and Durrett, 2021), however, it often leads to degraded summary informativeness. In comparison, our contrastive learning framework allows the model to be end-to-end trained and does not require model modification, thus providing a general solution for learning summarization systems. Alongside improving factuality, we have also witnessed growing interests in automated factu"
2021.emnlp-main.532,2020.emnlp-main.506,0,0.021661,"valuation further confirms that our models consistently reduce both extrinsic and intrinsic errors over baseline across datasets. 2 Related Work Factuality Improvement and Evaluation. Neural abstractive summaries often contain unfaithful content with regard to the source (Falke et al., 2019). To improve summary factuality, three major types of approaches are proposed. First, a separate correction model is learned to fix errors made by the summarizers (Zhao et al., 2020; Chen et al., 2021), including replacing entities absent from the source (Dong et al., 2020) or revising all possible errors (Cao et al., 2020). The second type targets at modifying the sequence-to-sequence architecture to incorporate relation triplets (Cao et al., 2018), knowledge graphs (Zhu et al., 2021), and topics (Aralikatte et al., 2021) to inform the summarizers of article facts. Yet additional engineering efforts and model retraining are often needed. Finally, discarding noisy samples from model training has also been investigated (Nan et al., 2021; Goyal and Durrett, 2021), however, it often leads to degraded summary informativeness. In comparison, our contrastive learning framework allows the model to be end-to-end trained"
2021.emnlp-main.532,2021.naacl-main.475,0,0.0316123,"zes the same negative samples (Welleck et al., 2020), our summaries also obtain consistently better QuestEval scores. Human evaluation further confirms that our models consistently reduce both extrinsic and intrinsic errors over baseline across datasets. 2 Related Work Factuality Improvement and Evaluation. Neural abstractive summaries often contain unfaithful content with regard to the source (Falke et al., 2019). To improve summary factuality, three major types of approaches are proposed. First, a separate correction model is learned to fix errors made by the summarizers (Zhao et al., 2020; Chen et al., 2021), including replacing entities absent from the source (Dong et al., 2020) or revising all possible errors (Cao et al., 2020). The second type targets at modifying the sequence-to-sequence architecture to incorporate relation triplets (Cao et al., 2018), knowledge graphs (Zhu et al., 2021), and topics (Aralikatte et al., 2021) to inform the summarizers of article facts. Yet additional engineering efforts and model retraining are often needed. Finally, discarding noisy samples from model training has also been investigated (Nan et al., 2021; Goyal and Durrett, 2021), however, it often leads to d"
2021.emnlp-main.532,2020.acl-main.703,0,0.351839,"hods. Our contrastive learning model trained on low confidence system outputs correctly generates the full name. Comparisons using cross-entropy loss, beam reranking by entailment scores (Kryscinski et al., 2020), and unlikelihood objective (Welleck et al., 2020) over negative samples all produce unfaithful content. Introduction Our goal is to train abstractive summarization Large pre-trained Transformers have yielded re- systems that generate both faithful and informative markable performance on abstractive summariza- summaries in an end-to-end fashion. We observe tion (Liu and Lapata, 2019; Lewis et al., 2020; that, while the commonly used maximum likelihood training optimizes over references, there is no Zhang et al., 2020a) with impeccable fluency, yet their summaries often contain factually inconsis- guarantee for the model to distinguish references tent content (Maynez et al., 2020; Zhang et al., from incorrect generations (Holtzman et al., 2020; 2020b; Goyal and Durrett, 2020), even for state- Welleck et al., 2020). Therefore, potential solutions reside in designing new learning objectives of-the-art models. Three types of remedies have that can effectively inform preferences of factual been"
2021.emnlp-main.532,W04-1013,0,0.0516173,"Missing"
2021.emnlp-main.532,2021.acl-short.135,0,0.0199214,"rages more faithful summary generation. Contrastive Learning (CL) for NLP. CL has been a popular method for representation learning, especially for vision understanding (Hjelm et al., 2019; Chen et al., 2020). Only recently has CL been used for training language models with selfsupervision (Fang et al., 2020), learning sentence representations (Gao et al., 2021), and improving document clustering (Zhang et al., 2021a). With a supervised setup, Gunel et al. (2021) adopt the contrastive objective to fine-tune pre-trained models on benchmark language understanding datasets. Using a similar idea, Liu and Liu (2021) enlarge the distances among summaries of different quality as measured by ROUGE scores. 3 CLIFF: Contrastive Learning Framework for Summarization We design a contrastive learning (CL)-based training objective that drives the summarization model to learn a preference of faithful summaries over summaries with factual errors. It is then used for fine-tuning BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020a) for training summarization models. Formally, let an article x have a set of reference summaries P (henceforth positive samples) and another set of erroneous summaries N (negative sam"
2021.emnlp-main.532,E17-1083,0,0.0199354,"tive samples, which are critical for CL efficacy (Chen et al., 2020), and (2) how to represent the summaries (i.e., h∗ ). Below we describe positive sample generation and options for h∗ , leaving the strategies for negative samples to § 5. Positive Sample Construction (P ). Summarization datasets often contain a single reference for each article. To create multiple positive samples, in our pilot study, we experiment with paraphrasing with synonym substitution (Ren et al., 2019), randomly replacing words based on the prediction of masked language models (Kobayashi, 2018), and back-translation (Mallinson et al., 2017). We find back-translation to be best at preserving meaning and offering language variation, and thus use NLPAug2 to translate each reference to German and back to English. Together with the reference, the best translation is kept and added to P , if no new named entity is introduced. Summary Representation (h∗ ). We use the outputs of the decoder’s last layer, and investigate three options that average over all tokens, named entity tokens, and the last token of the decoded summary. Entities and other parsing results are obtained by spaCy (Honnibal et al., 2020). We further consider adding a m"
2021.emnlp-main.532,2020.acl-main.173,0,0.102473,"ples all produce unfaithful content. Introduction Our goal is to train abstractive summarization Large pre-trained Transformers have yielded re- systems that generate both faithful and informative markable performance on abstractive summariza- summaries in an end-to-end fashion. We observe tion (Liu and Lapata, 2019; Lewis et al., 2020; that, while the commonly used maximum likelihood training optimizes over references, there is no Zhang et al., 2020a) with impeccable fluency, yet their summaries often contain factually inconsis- guarantee for the model to distinguish references tent content (Maynez et al., 2020; Zhang et al., from incorrect generations (Holtzman et al., 2020; 2020b; Goyal and Durrett, 2020), even for state- Welleck et al., 2020). Therefore, potential solutions reside in designing new learning objectives of-the-art models. Three types of remedies have that can effectively inform preferences of factual been proposed: running a separately learned error correction component (Dong et al., 2020), remov- summaries over incorrect ones. ing noisy training samples (Nan et al., 2021; Goyal Concretely, we hypothesize that including factuand Durrett, 2021), and modifying the Transformer ally inc"
2021.emnlp-main.532,2021.emnlp-main.529,0,0.0476006,"Missing"
2021.emnlp-main.532,D18-1206,0,0.0225533,"formulation that teaches a summarizer to expand the margin between factually consistent summaries and their incorrect peers. Moreover, we design four types of strategies with different variants to construct negative samples by editing reference summaries via rewriting entity-/relation-anchored text, and using system generated summaries that may contain unfaithful errors. Importantly, these strategies are inspired by our new annotation study on errors made by state-of-the-art summarizers—models fine-tuned from BART (Lewis et al., 2020) and PEGASUS (Zhang et al., 2020a)—on two benchmarks: XSum (Narayan et al., 2018) and CNN/DailyMail (CNN/DM) (Hermann et al., 2015). We fine-tune pre-trained large models with our contrastive learning objective on XSum and CNN/DM. Results based on QuestEval (Scialom et al., 2021), a QA-based factuality metric of high correlation with human judgments, show that our models trained with different types of negative samples uniformly outperform strong comparisons, including using a summarizer with post error correction and reranking beams based on entailment scores to the source. Moreover, compared with unlikelihood training method that penalizes the same negative samples (Well"
2021.emnlp-main.532,N19-4009,0,0.0490089,"Missing"
2021.emnlp-main.532,2021.naacl-main.383,0,0.0559176,"Missing"
2021.emnlp-main.532,2020.acl-demos.14,0,0.022182,"errors are in red. Text before is the prefix for regeneration. ple is constructed for each entity in the reference. Though this idea has been studied by Kryscinski et al. (2020), they allow entities of different types to be used, e.g., a PERSON can be replaced by a LOCATION. Examples are displayed in Table 1. S WAP E NT has the advantage of not depending on any trained model. Yet it only introduces intrinsic errors and lacks the coverage for extrinsic errors, which is addressed by the following generationbased methods. relations (M ASK R EL). We first obtain dependency relations using Stanza (Qi et al., 2020), with each relation denoted as <gov, rel, dep>. To incorporate more context, we consider noun phrase spans enclosing the token of gov or dep if it is a content word and the noun phrase contains a named entity. Similar to M ASK E NT, three negative samples are generated by BART based on the input with both gov and dep spans masked in the reference. Only the samples that introduce any new dependency relation that is not contained in the source nor the reference are kept. Specifically, we consider a match of a dependency relation as the same form or synonyms of its gov and and dep is found in th"
2021.emnlp-main.532,P19-1103,0,0.0183841,"To effectively employ CL in summarization, we need to address two challenges: (1) how to automatically construct both positive and negative samples, which are critical for CL efficacy (Chen et al., 2020), and (2) how to represent the summaries (i.e., h∗ ). Below we describe positive sample generation and options for h∗ , leaving the strategies for negative samples to § 5. Positive Sample Construction (P ). Summarization datasets often contain a single reference for each article. To create multiple positive samples, in our pilot study, we experiment with paraphrasing with synonym substitution (Ren et al., 2019), randomly replacing words based on the prediction of masked language models (Kobayashi, 2018), and back-translation (Mallinson et al., 2017). We find back-translation to be best at preserving meaning and offering language variation, and thus use NLPAug2 to translate each reference to German and back to English. Together with the reference, the best translation is kept and added to P , if no new named entity is introduced. Summary Representation (h∗ ). We use the outputs of the decoder’s last layer, and investigate three options that average over all tokens, named entity tokens, and the last t"
2021.emnlp-main.532,D19-1320,0,0.0240873,"ts in automated factuality evaluation, since popular word-matchingbased metrics, e.g., ROUGE, correlate poorly with human-rated factual consistency levels (Gabriel et al., 2021; Fabbri et al., 2021). Entailment-based scorers are designed at summary level (Kryscinski et al., 2020) and finer-grained dependency relation level (Goyal and Durrett, 2020). QA models are employed to measure content consistency by reading the articles to answer questions generated from the summaries (Wang et al., 2020; Durmus et al., 2020), or considering the summaries for addressing questions derived from the source (Scialom et al., 2019). Though not focusing on evaluation, our work highlights that models can produce a significant amount of world knowledge which should be evaluated differently instead of as extrinsic hallucination (Maynez et al., 2020). We also show that world knowledge can possibly be distinguished from errors via model behavior understanding. Training with negative samples has been investi1 gated in several classic NLP tasks, such as gramOur code and annotated data are available at https:// shuyangcao.github.io/projects/cliff_summ. matical error detection (Foster and Andersen, 2009) 6634 and dialogue systems"
2021.emnlp-main.532,2020.acl-main.450,0,0.0179078,"eral solution for learning summarization systems. Alongside improving factuality, we have also witnessed growing interests in automated factuality evaluation, since popular word-matchingbased metrics, e.g., ROUGE, correlate poorly with human-rated factual consistency levels (Gabriel et al., 2021; Fabbri et al., 2021). Entailment-based scorers are designed at summary level (Kryscinski et al., 2020) and finer-grained dependency relation level (Goyal and Durrett, 2020). QA models are employed to measure content consistency by reading the articles to answer questions generated from the summaries (Wang et al., 2020; Durmus et al., 2020), or considering the summaries for addressing questions derived from the source (Scialom et al., 2019). Though not focusing on evaluation, our work highlights that models can produce a significant amount of world knowledge which should be evaluated differently instead of as extrinsic hallucination (Maynez et al., 2020). We also show that world knowledge can possibly be distinguished from errors via model behavior understanding. Training with negative samples has been investi1 gated in several classic NLP tasks, such as gramOur code and annotated data are available at http"
2021.emnlp-main.532,2020.emnlp-demos.6,0,0.036161,"Missing"
2021.emnlp-main.532,2021.naacl-main.427,0,0.228574,"to avoid performance degradation (Saunshi et al., 2019). Second, it is nontrivial to construct “natural” samples that mimic the diverse errors made by state-of-the-art systems that vary in words and syntax (Goyal and Durrett, 2021). To address both challenges, we first propose a new framework, CLIFF, that uses contrastive learning for improving faithfulness and factuality of the generated summaries.1 Contrastive learning (CL) has obtained impressive results on many visual processing tasks, such as image classification (Khosla et al., 2020; Chen et al., 2020) and synthesis (Park et al., 2020; Zhang et al., 2021b). Intuitively, CL improves representation learning by compacting positive samples while contrasting them with negative samples. Here, we design a task-specific CL formulation that teaches a summarizer to expand the margin between factually consistent summaries and their incorrect peers. Moreover, we design four types of strategies with different variants to construct negative samples by editing reference summaries via rewriting entity-/relation-anchored text, and using system generated summaries that may contain unfaithful errors. Importantly, these strategies are inspired by our new annotat"
2021.emnlp-main.532,2020.acl-main.458,0,0.546608,"arisons using cross-entropy loss, beam reranking by entailment scores (Kryscinski et al., 2020), and unlikelihood objective (Welleck et al., 2020) over negative samples all produce unfaithful content. Introduction Our goal is to train abstractive summarization Large pre-trained Transformers have yielded re- systems that generate both faithful and informative markable performance on abstractive summariza- summaries in an end-to-end fashion. We observe tion (Liu and Lapata, 2019; Lewis et al., 2020; that, while the commonly used maximum likelihood training optimizes over references, there is no Zhang et al., 2020a) with impeccable fluency, yet their summaries often contain factually inconsis- guarantee for the model to distinguish references tent content (Maynez et al., 2020; Zhang et al., from incorrect generations (Holtzman et al., 2020; 2020b; Goyal and Durrett, 2020), even for state- Welleck et al., 2020). Therefore, potential solutions reside in designing new learning objectives of-the-art models. Three types of remedies have that can effectively inform preferences of factual been proposed: running a separately learned error correction component (Dong et al., 2020), remov- summaries over incorrec"
2021.emnlp-main.532,2020.findings-emnlp.203,0,0.0156389,"centre to recover. R EGEN R EL: A “rare” short-eared owl found ⇒ A “rare” short-eared owl found in the grounds of a former coal mine is being cared for by the RSPCA in Somerset. S YS L OW C ON: An injured golden owl found in a former coal mine in Lancashire is being cared for by the RSPCA. Table 1: Negative sample construction strategies (§ 5). For summaries edited from the reference, their differences are bolded. Introduced errors are in red. Text before is the prefix for regeneration. ple is constructed for each entity in the reference. Though this idea has been studied by Kryscinski et al. (2020), they allow entities of different types to be used, e.g., a PERSON can be replaced by a LOCATION. Examples are displayed in Table 1. S WAP E NT has the advantage of not depending on any trained model. Yet it only introduces intrinsic errors and lacks the coverage for extrinsic errors, which is addressed by the following generationbased methods. relations (M ASK R EL). We first obtain dependency relations using Stanza (Qi et al., 2020), with each relation denoted as <gov, rel, dep>. To incorporate more context, we consider noun phrase spans enclosing the token of gov or dep if it is a content"
2021.emnlp-main.532,2021.naacl-main.58,0,0.437639,"and Durrett, 2020), even for state- Welleck et al., 2020). Therefore, potential solutions reside in designing new learning objectives of-the-art models. Three types of remedies have that can effectively inform preferences of factual been proposed: running a separately learned error correction component (Dong et al., 2020), remov- summaries over incorrect ones. ing noisy training samples (Nan et al., 2021; Goyal Concretely, we hypothesize that including factuand Durrett, 2021), and modifying the Transformer ally inconsistent summaries (i.e., negative samples) architecture (Huang et al., 2020; Zhu et al., 2021). for training, in addition to references (i.e., positive Yet they either rely on heuristically created data for samples), let models become better at differentiaterror handling, falling short of generalization, or ing these two types of summaries. Although using require learning a large number of new parameters, negative samples has been effective at text repreand summary informativeness is often sacrificed. sentation learning, e.g., word2vec (Mikolov et al., 6633 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6633–6649 c November 7–11, 2021. 202"
2021.naacl-main.112,N18-1150,0,0.0287353,"need for handling long documents via abstractive summarization. To that end, extract-then-abstract methods are proposed. For example, Pilault et al. (2020) first extract relevant sentences and then rewrite them into paper abstracts. Our work is in line with building end-to-end abstractive summarization models for long input. Cohan et al. (2018) design a hierarchical encoder to read different sections separately, and then use combined attentions over words and sections to generate the summary. Multiple agents are created to read segments separately, and then collaboratively write an abstract (Celikyilmaz et al., 2018). However, both work truncates articles to 2K words. Although efficient encoder attentions have been studied in Zaheer et al. (2020) for abstractive summarization, at most 3K tokens can be consumed by their models. Our H EPOS encoderdecoder attention are able to process more than 10K tokens, significantly improving summary informativeness and faithfulness. 8 Conclusion posed APESsrc being the stronger of the two. After inspection, we find that human-written summaries contain paraphrases or acronyms that APES cannot capture via strict lexical matching. For instance, for the question “Diabetes m"
2021.naacl-main.112,W19-4828,0,0.0312332,""" and “care""). 2019), which share a similar theoretical foundation as global tokens; and kernel methods over attentions require training models from scratch (Choromanski et al., 2020; Katharopoulos et al., 2020). 3 Encoder-decoder Attention with Head-wise Positional Strides (Hepos) The efficient design of encoder-decoder attentions with head-wise positional strides (H EPOS) allows models to consume longer sequences. Concretely, our design is motivated by two observations: (1) Attention heads are redundant (Voita et al., 2019). (2) Any individual head rarely attends to several tokens in a row (Clark et al., 2019). Therefore, as illustrated in Fig. 1, H EPOS uses separate encoderdecoder heads on the same layer to cover different subsets of source tokens at fixed intervals. Each head starts at a different position, and all heads collectively attend to the full sequence. Given a stride size of sh , for the h-th head, its attention value between decoder query qj (at step j) and encoder key vector ki (for the i-th input token) can be formulated as: ( ahji = softmax(qj ki ), 0 if (i − h) mod sh = 0 otherwise (1) In H EPOS attention, each query token attends to n/sh tokens per head, yielding a memory complex"
2021.naacl-main.112,N19-4009,0,0.0438152,"Missing"
2021.naacl-main.112,2020.acl-main.703,0,0.62286,"with fewer unfaithful errors. 1 Introduction Long documents, such as scientific papers and government reports, often discuss substantial issues at length, and thus are time-consuming to read, let alone to comprehend. Generating abstractive summaries can help readers quickly grasp the main topics, yet prior work has mostly focused on short texts (containing hundreds of words), e.g., news articles (Gehrmann et al., 2018; Liu and Lapata, 2019; Zhang et al., 2019). Model training efficiency and summary quality present a pair of challenges for long document summarization. State-of-the-art systems (Lewis et al., 2020; Zhang et al., 2019) are built upon Transformer (Vaswani et al., 2017), which uses attentions to compute pairwise relations between tokens. Such framework has quadratic time and memory complexities, and is too costly for long documents 1 . Solutions have been proposed to reduce 1 For instance, to fine-tune BART on documents of 10K the calculation of encoder self-attentions (Wang et al., 2020c; Zaheer et al., 2020) by selectively attending to neighboring tokens (Beltagy et al., 2020; Child et al., 2019) or relevant words (Kitaev et al., 2020; Tay et al., 2020a). Yet, these methods do not apply"
2021.naacl-main.112,W04-1013,0,0.20411,"new entific publications; B ILL S UM (Kornilova and Eisummary-worthy bigrams appear in the later half delman, 2019), a collection of congressional bills; of the articles, showing a more even distribution. and B IG PATENT (Sharma et al., 2019), a corpus of A similar trend is observed on unigrams. However, 4 B ILL S UM has the shortest documents among the www.gao.gov 5 crsreports.congress.gov five datasets. 1422 5 Summary Evaluation with Cloze QA This work aims to evaluate whether processing more text improves both informativeness and faithfulness of abstractive summaries. In addition to ROUGE (Lin, 2004) and human evaluation, we extend existing QA-based metric (Eyal et al., 2019) and consider an entailment-based scorer. Entailment-based Evaluation. We further consider FactCC (Kryscinski et al., 2020), which evaluates factual consistency of a system summary by predicting an entailment score between the source and the summary. We reproduce their method on our datasets. Additional details for implementing the evaluation models and the entity extraction models are given in Appendix B. QA-based Evaluation. We present a new faithfulness evaluation metric by extending the APES score (Eyal et al., 20"
2021.naacl-main.112,2020.acl-main.713,1,0.773036,"Missing"
2021.naacl-main.112,D19-1387,0,0.0238134,"her ROUGE scores than competitive comparisons, including new state-of-the-art results on PubMed. Human evaluation also shows that our models generate more informative summaries with fewer unfaithful errors. 1 Introduction Long documents, such as scientific papers and government reports, often discuss substantial issues at length, and thus are time-consuming to read, let alone to comprehend. Generating abstractive summaries can help readers quickly grasp the main topics, yet prior work has mostly focused on short texts (containing hundreds of words), e.g., news articles (Gehrmann et al., 2018; Liu and Lapata, 2019; Zhang et al., 2019). Model training efficiency and summary quality present a pair of challenges for long document summarization. State-of-the-art systems (Lewis et al., 2020; Zhang et al., 2019) are built upon Transformer (Vaswani et al., 2017), which uses attentions to compute pairwise relations between tokens. Such framework has quadratic time and memory complexities, and is too costly for long documents 1 . Solutions have been proposed to reduce 1 For instance, to fine-tune BART on documents of 10K the calculation of encoder self-attentions (Wang et al., 2020c; Zaheer et al., 2020) by sel"
2021.naacl-main.112,2020.emnlp-main.748,0,0.348369,"both GovReport and Informativeness PubMed, as shown in Table 3. Within learnable patterns, Sinkhorn attention consistently obtains better We investigate whether processing more words generates more informative summaries. ROUGE scores. Moreover, combining techniques in fixed patterns is more effective than simply us- Comparisons include recent top-performing abing window-based sparse attentions, though with stractive models: PEGASUS (Zhang et al., 2019), an increased memory cost. a large pre-trained summarization model with For encoder-decoder attentions, H EPOS consis- truncated inputs; TLM (Pilault et al., 2020), tently yields higher ROUGE scores than Linformer DANCER (Gidiotis and Tsoumakas, 2020), and on both datasets, using either full or Sinkhorn en- SEAL (Zhao et al., 2020), all of them using hybrid coder. Notably, coupled with a Sinkhorn attention, extract-then-abstract methods; and B IG B IRD (Zaour model’s performance matches the variant using heer et al., 2020), which combines sliding window, 1424 Results. Overall, models that read more text obtain higher ROUGE scores, according to results on GovReport and PubMed in Table 4. First, different encoder variants with full encoder-decoder attenti"
2021.naacl-main.112,C08-1087,0,0.075765,"nd human evaluation. We further show that our new cloze QA metric better correlates with human judgment than prior faithfulness evaluation metrics. 7 Acknowledgements Additional Related Work This research is supported in part by Oracle for Summarizing long inputs has been investigated in Research Cloud Credits, National Science Foundamany domains, including books (Mihalcea and Ceylan, 2007), patents (Trappey et al., 2009), tion through Grant IIS-1813341, and by the Office movie scripts (Gorinski and Lapata, 2015), and sci- of the Director of National Intelligence (ODNI), entific publications (Qazvinian and Radev, 2008). Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116. The However, the datasets are often too small to train views and conclusions contained herein are those neural models. Cohan et al. (2018) publish two of the authors and should not be interpreted as large-scale datasets by collecting articles from necessarily representing the official policies, either AR X IV and P UB M ED . Popular methods rely on extractive summarizers that identify salient sen- expressed or implied, of ODNI, IARPA, or the U.S. tences based on positional information (Dong et al., Gov"
2021.naacl-main.112,P19-1212,1,0.831188,"Missing"
2021.naacl-main.112,P19-1032,0,0.0286764,"exities and numbers of newly learned parameters in Table 1. 2.1 Fixed Patterns Fixed patterns are used to limit the scope of attentions. In our experiments, in addition to windowbased attentions, we also combine them with global tokens, stride patterns, or random attentions. Sliding window attentions (Beltagy et al., 2020) aim to capture the local context, which is critical for language understanding (Liu* et al., 2018; Child et al., 2019). Concretely, each query token attends to w/2 neighboring tokens on both left and right, yielding a memory complexity of O(nw). Adaptive span is proposed by Sukhbaatar et al. (2019) to learn attention windows at different layers. This is implemented by learning a masking function for each head independently. In practice, the adaptive span attention has a complexity of O(nw), ˆ where w ˆ is the maximum values of predicted spans for all heads. Besides, it introduces O(1) new parameters for learning spans. Transformer models are built upon multi-head attentions in multiple layers. The attention is calcuT √ lated as Attention(Q, K, V) = softmax( QK )V, dk where Q, K, and V are query, key, and value matrices, each consisting of n vectors for a document Global tokens (Beltagy"
2021.naacl-main.112,P19-1580,0,0.0307304,"(“Job"" and “home"") in the input, heads 2 and 4 look at the second and fourth words (“in"" and “care""). 2019), which share a similar theoretical foundation as global tokens; and kernel methods over attentions require training models from scratch (Choromanski et al., 2020; Katharopoulos et al., 2020). 3 Encoder-decoder Attention with Head-wise Positional Strides (Hepos) The efficient design of encoder-decoder attentions with head-wise positional strides (H EPOS) allows models to consume longer sequences. Concretely, our design is motivated by two observations: (1) Attention heads are redundant (Voita et al., 2019). (2) Any individual head rarely attends to several tokens in a row (Clark et al., 2019). Therefore, as illustrated in Fig. 1, H EPOS uses separate encoderdecoder heads on the same layer to cover different subsets of source tokens at fixed intervals. Each head starts at a different position, and all heads collectively attend to the full sequence. Given a stride size of sh , for the h-th head, its attention value between decoder query qj (at step j) and encoder key vector ki (for the i-th input token) can be formulated as: ( ahji = softmax(qj ki ), 0 if (i − h) mod sh = 0 otherwise (1) In H EPO"
2021.naacl-main.112,2020.acl-main.450,0,0.354432,"es (Gehrmann et al., 2018; Liu and Lapata, 2019; Zhang et al., 2019). Model training efficiency and summary quality present a pair of challenges for long document summarization. State-of-the-art systems (Lewis et al., 2020; Zhang et al., 2019) are built upon Transformer (Vaswani et al., 2017), which uses attentions to compute pairwise relations between tokens. Such framework has quadratic time and memory complexities, and is too costly for long documents 1 . Solutions have been proposed to reduce 1 For instance, to fine-tune BART on documents of 10K the calculation of encoder self-attentions (Wang et al., 2020c; Zaheer et al., 2020) by selectively attending to neighboring tokens (Beltagy et al., 2020; Child et al., 2019) or relevant words (Kitaev et al., 2020; Tay et al., 2020a). Yet, these methods do not apply to encoder-decoder attentions in summarization models since they collaborate and dynamically pinpoint salient content in the source as the summary is decoded. Truncation is commonly used to circumvent the issue. However, training on curtailed content further aggravates “hallucination” in existing abstractive models (Maynez et al., 2020). We argue that summarizing long documents (e.g., with t"
2021.naacl-main.112,D19-1298,0,0.144343,"Missing"
2021.naacl-main.397,N18-1150,0,0.0372993,"Missing"
2021.naacl-main.397,P18-1063,0,0.025298,"ing. 5008 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5008–5016 June 6–11, 2021. ©2021 Association for Computational Linguistics it is the most effective usage. In contrast, we bring insights into the relation between attentions and content selection via masking operations to further improve summarization performance. Content Selection for Abstractive Summarization. Content selection is a crucial step, where salient information is first detected and then summarized into concise abstracts (Chen and Bansal, 2018; Xu and Durrett, 2019). To minimize the propagation of selection errors, content selection is modeled as an extra component and learned within an end-to-end trained model (Zhou et al., 2017; Li et al., 2018; Gehrmann et al., 2018). To the best of our knowledge, we are the first to apply masks on selected layers and attention heads in Transformers for content selection in summarization. Moreover, our masking mechanism is only activated during inference, without any model modification. Analyzing Multi-head Attentions has attracted growing interests in the NLP community (Clark et al., 2019; Kova"
2021.naacl-main.397,W19-4828,0,0.0258449,"s (Chen and Bansal, 2018; Xu and Durrett, 2019). To minimize the propagation of selection errors, content selection is modeled as an extra component and learned within an end-to-end trained model (Zhou et al., 2017; Li et al., 2018; Gehrmann et al., 2018). To the best of our knowledge, we are the first to apply masks on selected layers and attention heads in Transformers for content selection in summarization. Moreover, our masking mechanism is only activated during inference, without any model modification. Analyzing Multi-head Attentions has attracted growing interests in the NLP community (Clark et al., 2019; Kovaleva et al., 2019). Among the work that is relevant to encoder-decoder attentions, Michel et al. (2019) and Voita et al. (2019) observe that only a small portion of heads is relevant for translation and encoder-decoder attentions tend to be more important than self-attentions. Meanwhile, word alignments for machine translation are induced from encoder-decoder attention weights (Li et al., 2019; Kobayashi et al., 2020). However, none of prior work employs attentions to improve generation quality. As far as we are aware, this is the first work that studies the content selection effects of"
2021.naacl-main.397,D18-1443,0,0.0850912,"tics it is the most effective usage. In contrast, we bring insights into the relation between attentions and content selection via masking operations to further improve summarization performance. Content Selection for Abstractive Summarization. Content selection is a crucial step, where salient information is first detected and then summarized into concise abstracts (Chen and Bansal, 2018; Xu and Durrett, 2019). To minimize the propagation of selection errors, content selection is modeled as an extra component and learned within an end-to-end trained model (Zhou et al., 2017; Li et al., 2018; Gehrmann et al., 2018). To the best of our knowledge, we are the first to apply masks on selected layers and attention heads in Transformers for content selection in summarization. Moreover, our masking mechanism is only activated during inference, without any model modification. Analyzing Multi-head Attentions has attracted growing interests in the NLP community (Clark et al., 2019; Kovaleva et al., 2019). Among the work that is relevant to encoder-decoder attentions, Michel et al. (2019) and Voita et al. (2019) observe that only a small portion of heads is relevant for translation and encoder-decoder attentions t"
2021.naacl-main.397,N18-1065,0,0.0654594,"Missing"
2021.naacl-main.397,2020.acl-main.457,1,0.878155,"Missing"
2021.naacl-main.397,D19-1445,0,0.0221312,"2018; Xu and Durrett, 2019). To minimize the propagation of selection errors, content selection is modeled as an extra component and learned within an end-to-end trained model (Zhou et al., 2017; Li et al., 2018; Gehrmann et al., 2018). To the best of our knowledge, we are the first to apply masks on selected layers and attention heads in Transformers for content selection in summarization. Moreover, our masking mechanism is only activated during inference, without any model modification. Analyzing Multi-head Attentions has attracted growing interests in the NLP community (Clark et al., 2019; Kovaleva et al., 2019). Among the work that is relevant to encoder-decoder attentions, Michel et al. (2019) and Voita et al. (2019) observe that only a small portion of heads is relevant for translation and encoder-decoder attentions tend to be more important than self-attentions. Meanwhile, word alignments for machine translation are induced from encoder-decoder attention weights (Li et al., 2019; Kobayashi et al., 2020). However, none of prior work employs attentions to improve generation quality. As far as we are aware, this is the first work that studies the content selection effects of encoder-decoder attentio"
2021.naacl-main.397,2020.acl-main.703,0,0.0314814,"orresponding element in m ˜ is set to 0 (attendable to the attention heads), and −∞ otherwise (hidden from these heads). The saliency labels can be predicted by an externally trained content selector. 4 Encoder-decoder Attentions and Content Selection In this section, we first probe into the content selection behavior of each single head (§ 4.1), and then study the synergism among heads at the same layer (§ 4.2). In § 4.3, we analyze the attentions’ focus. Our analysis is conducted on CNN/DM (Hermann et al., 2015), NYT (Consortium and Company, 2008), and XSum (Narayan et al., 2018). We follow Lewis et al. (2020) for data preprocessing and train/validation/test splits on CNN/DM and XSum, and adopt the setups in Paulus et al. (2018) for NYT, except that we keep entities and numbers. The number of samples in training, validation, and test set are: 287,188, 13,367 and 11,490 for CNN/DM; 588,909, 32,716 and 32,703 for NYT; 204,045, 11,332 and 11,334 for XSum. For experiments in this section, we create an analysis set of 1,000 random samples from the validation split of each dataset to reduce computational cost. 4.1 Content Selection Effects First, we study the feasibility of using encoderdecoder attention"
2021.naacl-main.397,D18-1205,0,0.0277812,"utational Linguistics it is the most effective usage. In contrast, we bring insights into the relation between attentions and content selection via masking operations to further improve summarization performance. Content Selection for Abstractive Summarization. Content selection is a crucial step, where salient information is first detected and then summarized into concise abstracts (Chen and Bansal, 2018; Xu and Durrett, 2019). To minimize the propagation of selection errors, content selection is modeled as an extra component and learned within an end-to-end trained model (Zhou et al., 2017; Li et al., 2018; Gehrmann et al., 2018). To the best of our knowledge, we are the first to apply masks on selected layers and attention heads in Transformers for content selection in summarization. Moreover, our masking mechanism is only activated during inference, without any model modification. Analyzing Multi-head Attentions has attracted growing interests in the NLP community (Clark et al., 2019; Kovaleva et al., 2019). Among the work that is relevant to encoder-decoder attentions, Michel et al. (2019) and Voita et al. (2019) observe that only a small portion of heads is relevant for translation and enco"
2021.naacl-main.397,P19-1124,0,0.0150914,"ization. Moreover, our masking mechanism is only activated during inference, without any model modification. Analyzing Multi-head Attentions has attracted growing interests in the NLP community (Clark et al., 2019; Kovaleva et al., 2019). Among the work that is relevant to encoder-decoder attentions, Michel et al. (2019) and Voita et al. (2019) observe that only a small portion of heads is relevant for translation and encoder-decoder attentions tend to be more important than self-attentions. Meanwhile, word alignments for machine translation are induced from encoder-decoder attention weights (Li et al., 2019; Kobayashi et al., 2020). However, none of prior work employs attentions to improve generation quality. As far as we are aware, this is the first work that studies the content selection effects of encoder-decoder attentions and uses them to guide better summary generation. 3 Attention Head Masking We adopt large pre-trained sequence-to-sequence Transformer models (BART, specifically) for abstractive summarization. Transformer is built with multi-head attentions. Attentions are computed per step based on a query q along with the key and value matrices, K and V: qKT Attention(q, K, V) = softmax"
2021.naacl-main.397,D19-1387,0,0.0390679,"Missing"
2021.naacl-main.397,2021.ccl-1.108,0,0.0556382,"Missing"
2021.naacl-main.397,N19-4009,0,0.0550291,"Missing"
2021.naacl-main.397,P19-1580,0,0.024027,"extra component and learned within an end-to-end trained model (Zhou et al., 2017; Li et al., 2018; Gehrmann et al., 2018). To the best of our knowledge, we are the first to apply masks on selected layers and attention heads in Transformers for content selection in summarization. Moreover, our masking mechanism is only activated during inference, without any model modification. Analyzing Multi-head Attentions has attracted growing interests in the NLP community (Clark et al., 2019; Kovaleva et al., 2019). Among the work that is relevant to encoder-decoder attentions, Michel et al. (2019) and Voita et al. (2019) observe that only a small portion of heads is relevant for translation and encoder-decoder attentions tend to be more important than self-attentions. Meanwhile, word alignments for machine translation are induced from encoder-decoder attention weights (Li et al., 2019; Kobayashi et al., 2020). However, none of prior work employs attentions to improve generation quality. As far as we are aware, this is the first work that studies the content selection effects of encoder-decoder attentions and uses them to guide better summary generation. 3 Attention Head Masking We adopt large pre-trained sequ"
2021.naacl-main.397,D19-1002,0,0.0290739,"ee greater improvement than bottom layers. Layer 1 is the bottom layer connected with the word embeddings. acle masking). Oracle labels are constructed by aligning a reference summary to the source article, where we iteratively find the longest common subsequences between the two. Taking a fine-tuned BART model, we apply oracle masking on each head at each layer when decoding on the analysis set. The ROUGE score obtained in this setting is denoted as rora . We then apply uniform encoder-decoder attention weights over the source to build a baseline that mimics no content selection, inspired by Wiegreffe and Pinter (2019). This yields a ROUGE score of runi . The content select effect per head can thus be calculated as the ROUGE improvement, i.e., rora − runi . Overall, it is more effective to constrain attentions to salient content at the top layers, according to the results on CNN/DM in Fig. 2. Specifically, with oracle masking, the top layer yields the most ROUGE-1 improvement. We observe similar trends on NYT and XSum (figures are in Appendix C). This indicates the feasibility of leveraging attention head masking to improve summary informativeness. 2 0 0 2 layer 1 layer 2 9 11 13 15 6 4 2 1 0.8 3 7 0.0 12 L"
2021.naacl-main.476,2020.sdp-1.1,0,0.0270216,"Missing"
2021.naacl-main.476,W18-2706,0,0.0226088,"ican Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5942–5953 June 6–11, 2021. ©2021 Association for Computational Linguistics evaluations show that our models produce summaries in simpler languages than competitive baselines, and the informativeness is on par with a vanilla BART. Moreover, headlines generated by our models embody stronger ideological leaning than nontrivial comparisons.2 2 Related Work Summarizing documents into different styles are mainly studied on news articles, where one appends style codes as extra embeddings to the encoder (Fan et al., 2018), or connects separate decoders with a shared encoder (Zhang et al., 2018). Similar to our work, Jin et al. (2020) leverage large pre-trained seq2seq models, but they modify model architecture by adding extra style-specific parameters. Nonetheless, existing work requires training new summarizers for different target styles or modifying the model structure. In contrast, our methods only affect decoder states or lexical choices during inference, allowing on-demand style adjustment for summary generation. Style-controlled text generation has received significant research attentions, especially wh"
2021.naacl-main.476,P15-2011,0,0.0636477,"Missing"
2021.naacl-main.476,N19-1315,0,0.0524822,"Missing"
2021.naacl-main.476,D19-1055,1,0.824887,"Missing"
2021.naacl-main.476,2020.acl-main.456,0,0.0324399,"Missing"
2021.naacl-main.476,2020.acl-main.703,0,0.245447,"tion that directly constrains the output vocabulary. Example system outputs are displayed in Fig. 1. Notably, our techniques are deployed at inference time so that the summary style can be adaptively adjusted during decoding. We experiment with two tasks: (1) simplicity control for document summarization with CNN/Daily Mail, and (2) headline generation with various ideological stances on news articles from the SemEval task (Kiesel et al., 2019) and a newly curated corpus consisting of multi-perspective stories from AllSides1 . In this work, the algorithms are experimented with the BART model (Lewis et al., 2020), though they also work with other Transformer models. Both automatic and human Generating summaries with different language styles can benefit readers of varying literacy levels (Chandrasekaran et al., 2020) or interests (Jin et al., 2020). Significant progress has been made in abstractive summarization with large pre-trained Transformers (Dong et al., 2019; Lewis et al., 2020; Zhang et al., 2019; Raffel et al., 2019; Song et al., 2019). However, style-controlled summarization is much less studied (Chandrasekaran et al., 2020), and two key challenges have been identified: (1) lack of parallel"
2021.naacl-main.476,D15-1044,0,0.100128,"Missing"
2021.naacl-main.476,D19-1499,0,0.0452011,"Missing"
2021.naacl-main.476,2020.findings-emnlp.217,0,0.0355872,"Missing"
2021.naacl-main.476,N18-1138,0,0.0516813,"Missing"
C14-1157,S12-1051,0,0.0331024,"tion. Given a set of sentences S, a complete graph is constructed with each sentence in S as a node. The weight of each edge (u, v) is their dissimilarity d0 (u, v). Then the distance between any pair of u and v, d(u, v), is defined as the total weight of the shortest path connecting P u and v.3 We experiment with two forms of dispersion function (Dasgupta et al., 2013): (1) hsum = u,v∈V,u6=v d(u, v), and (2) hmin = minu,v∈V,u6=v d(u, v). Then we need to define the dissimilarity function d0 (·, ·). There are different ways to measure the dissimilarity between sentences (Mihalcea et al., 2006; Agirre et al., 2012). In this work, we experiment with three types of dissimilarity functions. Lexical Dissimilarity. This function is based on the well-known Cosine similarity score using TFIDF weights. Let simtf idf (u, v) be the Cosine similarity between u and v, then we have d0Lex (u, v) = 1 − simtf idf (u, v). Semantic Dissimilarity. This function is based on the semantic meaning embedded in the dependency relations. d0Sem (u, v) = 1 − simSem (v, u), where simSem (v, u) is the semantic similarity used in content coverage measurement in Section 3.2. Topical Dissimilarity. We propose a novel dissimilarity meas"
C14-1157,W10-1201,0,0.0303379,"nt with three types of dissimilarity functions. Lexical Dissimilarity. This function is based on the well-known Cosine similarity score using TFIDF weights. Let simtf idf (u, v) be the Cosine similarity between u and v, then we have d0Lex (u, v) = 1 − simtf idf (u, v). Semantic Dissimilarity. This function is based on the semantic meaning embedded in the dependency relations. d0Sem (u, v) = 1 − simSem (v, u), where simSem (v, u) is the semantic similarity used in content coverage measurement in Section 3.2. Topical Dissimilarity. We propose a novel dissimilarity measure based on topic models. Celikyilmaz et al. (2010) show that estimating the similarity between query and passages by using topic structures can help improve the retrieval performance. As discussed in the topic coverage in Section 3.2, each sentence is represented by its sentence-topic distributions estimated by LDA. For candidate sentence u and v, let their topic distributions be Pu and Pv . Then the dissimilarity between u and v can be defined P P (i) Pv (i) as: d0T opic (u, v) = JSD(Pu ||Pv ) = 21 ( i Pu (i) log2 PPua (i) + i Pv (i) log2 P ) where Pa (i) = 12 (Pu (i) + Pv (i)). a (i) 3.4 Full Objective Function The objective function takes"
C14-1157,P13-1100,0,0.122259,"investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum´e and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive sta"
C14-1157,P06-1039,0,0.0600458,"Missing"
C14-1157,esuli-sebastiani-2006-sentiwordnet,0,0.0384161,"Missing"
C14-1157,E09-1059,0,0.653922,"model, which is a good thing. In short, I don’t think the music industry in particular will ever enjoy the huge profits of the 90’s. ... Ans6: Please-People in those businesses make millions of dollars as it is!! I don’t think piracy hurts them at all!!! Figure 1: Example discussion on Yahoo! Answers. Besides the best answer, other answers also contain relevant information (in italics). For example, the sentence in blue has a contrasting viewpoint compared to the other answers. Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints. Unlike those works, we address user generated online data: community QA and blogs. These forums use a substantially less formal language than news articles, and at the same time address a much broader spectrum of topics than product reviews. As a result, they present new challenges for automatic summarization. For example, Figure 1 illustrates a sample question from Yahoo! Answers1 along with the answers fr"
C14-1157,D08-1098,0,0.0949002,"th an empty set, for each iteration, we add a new sentence so that the current summary achieves the maximum value of the objective function. In addition to the theoretical guarantee, existing work (McDonald, 2007) has empirically shown that classical greedy algorithms usually works near-optimally. 4 Experimental Setup 4.1 Opinion Question Identification We first build a classifier to automatically detect opinion oriented questions in Community QA; questions in the blog dataset are all opinionated. Our opinion question classifier is trained on two opinion question datasets: (1) the first, from Li et al. (2008a), contains 646 opinionated and 332 objective questions; (2) the second dataset, from Amiri et al. (2013), consists of 317 implicit opinion questions, such as “What can you do to help environment?”, and 317 objective questions. We train a RBF kernel based SVM classifier to identify opinion questions, which achieves F1 scores of 0.79 and 0.80 on the two datasets when evaluated using 10-fold cross-validation (the best F1 scores reported are 0.75 and 0.79). 4.2 Datasets Community QA Summarization: Yahoo! Answers. We use the Yahoo! Answers dataset from Yahoo! WebscopeT M program,5 which contains"
C14-1157,P11-1052,0,0.347179,"ng pertinent and diverse information. A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum´e and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are fo"
C14-1157,C00-1072,0,0.469644,"uniformly outperforms semantic similarity computed with WordNet. Moreover, when measuring the summary diversity, topical representation is marginally better than lexical representation, and both of them beats semantic representation. 2 Related Work Our work falls in the realm of query-focused summarization, where a user asks a question and the system generates a summary of the answers containing pertinent and diverse information. A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum´e and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query a"
C14-1157,N03-1020,0,0.310491,"work their business model, which is a good thing. •By removing the profitability of music & film companies, piracy takes away their motivation to produce new music & movies. Figure 2: Sample summaries from Dasgupta et al. (2013), and our systems (100 words and 200 words). Sentences from separate bullets (•) are partial answers from different users. 7 Note that we aim to compare results with the gold-standard best answers of about 100 words. The evaluation of the 200-word summaries is provided only as an additional data-point. 1666 5.3 Blog Summarization Automatic Evaluation. We use the ROUGE (Lin and Hovy, 2003) software with standard options to automatically evaluate summaries with reference to the human labeled nuggets as those are available for this task. ROUGE-2 measures bigram overlap and ROUGE-SU4 measures the overlap of unigram and skip-bigram separated by up to four words. We use the ranker trained on Yahoo! data to produce relevance ordering, and adopt the system parameters from Section 5.2. Table 5 (left) shows that our system outperforms the best system in TAC’08 with highest ROUGE-2 score (Kim et al., 2008), the two baselines (TFIDF+Lexicon, and our ranker), Lin and Bilmes (2011), and Das"
C14-1157,C08-1063,0,0.183311,"ommunity QA and blog summarization, we show that our system outperforms state-of-the-art approaches in both automatic evaluation and human evaluation. A human evaluation task is conducted on Amazon Mechanical Turk with scale, and shows that our systems are able to generate summaries of high overall quality and information diversity. 1 Introduction Social media forums, such as social networks, blogs, newsgroups, and community question answering (QA), offer avenues for people to express their opinions as well collect other people’s thoughts on topics as diverse as health, politics and software (Liu et al., 2008). However, digesting the large amount of information in long threads on newsgroups, or even knowing which threads to pay attention to, can be overwhelming. A text-based summary that highlights the diversity of opinions on a given topic can lighten this information overload. In this work, we design a submodular function-based framework for opinion summarization on community question answering and blog data. Question: What is the long term effect of piracy on the music and film industry? Best Answer: Rising costs for movies and music. ... If they sell less, they need to raise the price to make u"
C14-1157,J13-2002,0,0.015992,"able 2 manifests that our ranker outperforms all the other methods. Avg Precision MRR Baseline (Random) 0.1305 0.3403 Baseline (Length) 0.2834 0.4889 JSD 0.4000 0.5909 Ranker (ListNet) 0.5336 0.6496 Table 2: Performance for best answer prediction. Our ranker outperforms the three baselines. 5.2 Community QA Summarization Automatic Evaluation. Since human written abstracts are not available for the Yahoo! Answers dataset, we adopt the Jensen-Shannon divergence (JSD) to measure the summary quality. Intuitively, a smaller JSD implies that the summary covers more of the content in the answer set. Louis and Nenkova (2013) report that JSD has a strong negative correlation (Spearman correlation = −0.737) with the overall summary quality for multi-document summarization (MDS) on news articles and blogs. Our task is similar to MDS. Meanwhile, the average JSD of the best answers in our test set is smaller than that of the other answers (0.39 vs. 0.49), with an average length of 103 words compared with 67 words for the other answers. Also, on the blog task (Section 5.3), the top two systems by JSD also have the top two ROUGE scores (a common metric for summarization evaluation when human-constructed summaries are av"
C14-1157,N13-1108,1,0.831475,"programming framework to select sentences. 3 Submodular Opinion Summarization In this section, we describe how query-focused opinion summarization can be addressed by submodular functions combined with dispersion functions. We first define our problem. Then we introduce the 1661 Basic Features - answer position in all answers/sentence position in blog - length of the answer/sentence - length is less than 5 words Query-Sentence Overlap Features - unigram/bigram TF/TFIDF similarity with query - number of key phrases in the query that appear in the sentence. A model similar to that described in (Luo et al., 2013) was applied to detect key phrases. Sentiment Features - number/portion of sentiment words from a lexicon (Section 3.2) - if contains sentiment words with the same polarity as sentiment words in query Query-Independent Features - unigram/bigram TFIDF similarity with cluster centroid - sumBasic score (Nenkova and Vanderwende, 2005) - number of topic signature words (Lin and Hovy, 2000) - JS divergence with cluster Table 1: Features used for candidate ranking. We use them for ranking answers in both community QA and blogs. components of our objective function (Sections 3.1–3.3). The full objecti"
C14-1157,D10-1007,0,0.0540666,"been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive standpoints. Our work is more related to opinion summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct taxonomies for questions in community QA. Summaries are generated by clustering sentences according to their polarity based on a small dictionary. Tomasoni and Huang (2010) introduce coverage and quality constraints on the sentences, and utilize an integer linear programming framework to select sentences. 3 Submodular"
C14-1157,E12-1023,0,0.0494933,"se information. A wide range of methods have been investigated, where relevance is often estimated through TF-IDF similarity (Carbonell and Goldstein, 1998), topic signature words (Lin and Hovy, 2000) or by learning a Bayesian model over queries and documents (Daum´e and Marcu, 2006). Most work only implicitly penalizes summary redundancy, e.g. by downweighting the importance of words that are already selected. Encouraging diversity of a summary has recently been addressed through submodular functions, which have been applied for multi-document summarization in newswire (Lin and Bilmes, 2011; Sipos et al., 2012), and comments summarization (Dasgupta et al., 2013). However, these works either ignore the query information (when available) or else use simple ngram matching between the query and sentences. In contrast, we propose to optimize an objective function that addresses both relevance and diversity. Previous work on generating opinion summaries mainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structu"
C14-1157,W06-1640,1,0.907069,"In short, I don’t think the music industry in particular will ever enjoy the huge profits of the 90’s. ... Ans6: Please-People in those businesses make millions of dollars as it is!! I don’t think piracy hurts them at all!!! Figure 1: Example discussion on Yahoo! Answers. Besides the best answer, other answers also contain relevant information (in italics). For example, the sentence in blue has a contrasting viewpoint compared to the other answers. Opinion summarization has previously been applied to restricted domains, such as product reviews (Hu and Liu, 2004; Lerman et al., 2009) and news (Stoyanov and Cardie, 2006), where the output summary is either presented in a structured way with respect to each aspect of the product or organized along contrastive viewpoints. Unlike those works, we address user generated online data: community QA and blogs. These forums use a substantially less formal language than news articles, and at the same time address a much broader spectrum of topics than product reviews. As a result, they present new challenges for automatic summarization. For example, Figure 1 illustrates a sample question from Yahoo! Answers1 along with the answers from different users. The question rece"
C14-1157,P10-1078,0,0.0733062,"ainly considers product reviews (Hu and Liu, 2004; Lerman et al., 2009), and formal texts such as news articles (Stoyanov and Cardie, 2006) or editorials (Paul et al., 2010). Mostly, there is no query information, and summaries are formulated in a structured way based on product features or contrastive standpoints. Our work is more related to opinion summarization on user-generated content, such as community QA. Liu et al. (2008) manually construct taxonomies for questions in community QA. Summaries are generated by clustering sentences according to their polarity based on a small dictionary. Tomasoni and Huang (2010) introduce coverage and quality constraints on the sentences, and utilize an integer linear programming framework to select sentences. 3 Submodular Opinion Summarization In this section, we describe how query-focused opinion summarization can be addressed by submodular functions combined with dispersion functions. We first define our problem. Then we introduce the 1661 Basic Features - answer position in all answers/sentence position in blog - length of the answer/sentence - length is less than 5 words Query-Sentence Overlap Features - unigram/bigram TF/TFIDF similarity with query - number of"
C14-1157,H05-1044,0,0.0427767,"olarity score encourages the selection of summaries that cover both positive and negative opinions. We categorize each sentence simply by counting the number of polarized words given by our lexicon. A sentence belongs to a positive cluster if it has more positive words than negative ones, and vice versa. If any negator co-occurs with a sentiment word (e.g. within a window of size 5), the sentiment is reversed.2 The polarity clustering P thusP have two p clusters corresponding to positive and negative opinions. The score is defined as p(S) = |S ∩ P |. Our lexicon consists of P ∈P MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006). Words with conflicting sentiments from different lexicons are removed. Content Coverage. Similarly to Lin and Bilmes (2011) and Dasgupta et al. (2013), P we use the following function to measure content coverage of the current summary S: c(S) = v∈V min(cov(v, S), θ · P cov(v, V )), where cov(v, S) = u∈S sim(v, u). We experiment with two types of similarity functions. One is a Cosine TFIDF similarity score. The other is a WordNet-based semantic similarity score between pairwise dependency relations from two"
C14-1157,W03-1017,0,0.0910727,"defined P P (i) Pv (i) as: d0T opic (u, v) = JSD(Pu ||Pv ) = 21 ( i Pu (i) log2 PPua (i) + i Pv (i) log2 P ) where Pa (i) = 12 (Pu (i) + Pv (i)). a (i) 3.4 Full Objective Function The objective function takes the interpolation of the submodular functions and dispersion function: F(S) = r(S) + αt(S) + βa(S) + γp(S) + ηc(S) + δh(S). (1) 2 There exists a large amount of work on determining the polarity of a sentence (Pang and Lee, 2008) which can be employed for polarity clustering in this work. We decide to focus on summarization, and estimate sentence polarity through sentiment word summation (Yu and Hatzivassiloglou, 2003), though we do not distinguish different sentiment words. 3 This definition of distance is used to produce theoretical guarantees for the greedy algorithm described in Section 3.5. 1663 The coefficients α, β, γ, η, δ are non-negative real numbers and can be tuned on a development set.4 Notice that each summand except h(S) is a non-decreasing, non-negative, and submodular function, and summation preserves monotonicity, non-negativity, and submodularity. Dispersion function h(s) is either hsum or hmin as introduced previously. 3.5 Summary Generation via Greedy Algorithm Generating the summary th"
C14-1157,D12-1036,0,\N,Missing
C18-1215,D15-1075,0,0.185054,"c resources (Yih et al., 2013), tree edit distance (Yao and Durme, 2013) and named entities (Severyn and Moschitti, 2013). 1 https://www.taobao.com/ 2541 2 In deep learning methods, some neural network algorithms are employed to train learning models. Briefly, these methods could be categorized into three categories, i.e., siamense networks, attentive networks and compare-aggregate networks. In siamense networks, related studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In attentive networks, instead of using the final time step of LSTM to represent a sentence, related studies use the attention strategy to get the weight of overall time steps and then use the weight to represent the sentence. (Tan et al., 2016; Hermann et al., 2015, Yin et al., 2015). In compare-aggregate networks, related studies use different matching strategy to get relationships within words. (He and Lin, 2016; Wang et al., 2017; Wang and Jiang, 2016; Trischler et al., 2016; Parikn et al., 2016.). However, all above approaches are similar to our One vs. One Matching model which deals wi"
C18-1215,N16-1108,0,0.0177049,"studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In attentive networks, instead of using the final time step of LSTM to represent a sentence, related studies use the attention strategy to get the weight of overall time steps and then use the weight to represent the sentence. (Tan et al., 2016; Hermann et al., 2015, Yin et al., 2015). In compare-aggregate networks, related studies use different matching strategy to get relationships within words. (He and Lin, 2016; Wang et al., 2017; Wang and Jiang, 2016; Trischler et al., 2016; Parikn et al., 2016.). However, all above approaches are similar to our One vs. One Matching model which deals with the matching measurement between one sentence (or one piece of text) and another sentence (or another piece of text). In contrast, our approach is a One vs. Many Matching model which deals with the matching measurement between one sentence (or one piece of text) and multiple sentences (or multiple pieces of text). 3 Data Collection and Annotation We collect 4,060 question-answer pairs from “Asking All” in Taobao,"
C18-1215,D16-1244,0,0.0724571,"Missing"
C18-1215,D13-1044,0,0.0493654,"Missing"
C18-1215,P16-1044,0,0.189583,"mine whether an answer is answering a given question. For instance, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filmed in 2009 in Charl"
C18-1215,P16-1041,0,0.0278233,"ce, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filmed in 2009 in Charleston. A2: The film was released on May 25, 2010 on DVD. A: The respon"
C18-1215,C10-1131,0,0.083737,"Missing"
C18-1215,P16-1122,0,0.0269575,"Missing"
C18-1215,D15-1237,0,0.283339,"is a task to determine whether an answer is answering a given question. For instance, in Figure 1, the question “Where is dear john filmed at?” in E1 has two candidate answers “The movie was filmed in 2009 in Charleston.” and “The file was released on May 25, 2010 on DVD.” The first answer is determined with the “Matching” category since it answers the question while the second answer is “Non-matching” since it could not answer the question. The past five years have witnessed a huge exploding interest in the research on QA matching, due to its widely applications, such as question answering (Yang et al., 2015; Tan et al., 2016; Wang et al., 2017) and reading comprehension (Trischler et al., 2016; Dhingra et al., 2017). However, all existing QA matching studies only focus on formal text. In real applications, there exists many scenarios where the QA text is informal. For instance, E2 is a question-answer pair extE1: Two QA pairs in formal text Q1: Where is dear john filmed at? Label: Matching Q2:Where is dear john filmed at? Label: Non-matching E2: Three QA pairs in informal text Q: Will the response time slow after updating os? What about the battery? What about the screen? A1: The movie was filme"
C18-1215,N13-1106,0,0.0393979,"Missing"
C18-1215,P13-1171,0,0.02644,"erent from the above corpora, the question-answer pairs in our corpus are informal text. 2.2 Matching methods Generally speaking, QA matching methods could be split into two categories: shallow learning methods and deep learning methods. In shallow learning methods, some shallow learning algorithms, such as CRF, SVM and MaxEnt, are employed to train the learning models (Wang et al., 2010). Besides the learning algorithms, the related studies on shallow learning methods mainly focus on feature engineering, using linguistic tools and using external resources, such as lexical semantic resources (Yih et al., 2013), tree edit distance (Yao and Durme, 2013) and named entities (Severyn and Moschitti, 2013). 1 https://www.taobao.com/ 2541 2 In deep learning methods, some neural network algorithms are employed to train learning models. Briefly, these methods could be categorized into three categories, i.e., siamense networks, attentive networks and compare-aggregate networks. In siamense networks, related studies use classic neural networks, such as LSTM and CNN, to get the representations separately and then concatenate them to classify. (Feng et al., 2015; Yang et al., 2015; Bowman et al., 2015). In atten"
D18-1447,I11-1130,0,0.456057,"extraction concerns the task of selecting a set of phrases from a document that can indicate the main ideas expressed in the input (Turney, 2000; Hasan and Ng, 2014). It is an essential task for document understanding because accurate identification of keyphrases can be beneficial for a wide range of downstreaming natural language processing and information retrieval applications. For instance, keyphrases can be leveraged to improve text summarization systems (Zhang et al., 2004; Liu et al., 2009a; Wang and Cardie, 2013), facilitate sentiment analysis and opinion mining (Wilson et al., 2005; Berend, 2011), and help with document clustering (Hammouda et al., 2005). Though relatively ∗ not in document Figure 1: Sample document with labeled keyphrases. Work was done while visiting Northeastern University. While seq2seq model demonstrates good performance on keyphrase generation (Meng et al., 2017), it heavily relies on massive amounts of labeled data for model training, which is often unavailable for new domains. To overcome this drawback, in this work, we investigate semisupervised learning for keyphrase generation, by leveraging abundant unlabeled documents along with limited labeled data. Intu"
D18-1447,P04-3031,0,0.121197,"pairs for the main task training, another 15K document-title pairs for the auxiliary task. We further conduct experiments on a 130K large-scale labeled dataset, which includes the small-scale labeled data. Similar to Meng et al. (2017), we test our model on five widely used public datasets from the scientific domain: I NSPEC (Hulth, 2003), NUS (Nguyen and Kan, 2007), K RAPIVIN (Krapivin et al., 2009), S EM E VAL -2010 (Kim et al., 2010) and KP20 K (Meng et al., 2017). 5.2 Experimental Settings Data preprocessing is implemented as in (Meng et al., 2017). The texts are first tokenized by NLTK (Bird and Loper, 2004) and lowercased, then the numbers are replaced with <digit>. We set maximal length of source text as 200, 40 for target text. Encoder and decoder both have a vocabulary size of 50K. The word embedding size is set to 128. Embeddings are randomly initialized and learned during training. The size of hidden vector is 512. Dropout rate is set as 0.3. Maximal gradient normalization is 2. Adagrad (Duchi et al., 2011) is adopted to train the model with learning rate of 0.15 and the initial accumulator rate is 0.1. For synthetic data construction, we first use batch size of 64 for model pre-training an"
D18-1447,I13-1062,0,0.0970824,"uses on the keyphrase extraction task, and a two-step strategy is typically designed. Specifically, a large pool of candidate phrases are first extracted according to predefined syntactic templates (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2009b, 2011) or their estimated importance scores (Hulth, 2003). In the second step, re-ranking is applied to yield the final keyphrases, based on supervised learning (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Lopez and Romary, 2010; Kim and Kan, 2009), unsupervised graph algorithms (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Bougouin et al., 2013), or topic modelings (Liu et al., 2009c, 2010). Keyphrase generation is stud1 We use “semi-supervised learning” as a broad term to refer to the two methods proposed in this paper. ied in more recent work. For instance, Liu et al. (2011) propose to use statistic machine translation model to learn word-alignments between documents and keyphrases, enabling the model to generate keyphrases which do not appear in the input. Meng et al. (2017) train seq2seq-based generation models (Sutskever et al., 2014) on large-scale labeled corpora, which may not be applicable to a new domain with minimal human"
D18-1447,P16-1185,0,0.0206793,"al., 2018), they often rely on large amounts of labeled data, which are expensive to get. In order to mitigate the problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training (Dai and Le, 2015; Ramachandran et al., 2017). For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to improve translation quality (G¨ulc¸ehre et al., 2015), where generating synthetic data (Sennrich et al., 2016; Zhang and Zong, 2016), multi-task learning (Zhang and Zong, 2016), and autoencoderbased methods (Cheng et al., 2016) are shown to be effective. Multi-task learning is also examined for sequence labeling tasks (Rei, 2017; Liu et al., 2018). In our paper, we study semi-supervised learning for keyphrase generation based on seq2seq models, which has not been explored before. Besides, we focus on leveraging source-side unlabeled articles to enhance performance with synthetic keyphrase construction or multi-task learning. 3 Neural Keyphrase Generation Model In this section, we describe the neural keyphrase generation model built on a sequence-to-sequence model (Sutskever et al., 2014) as illustrated in Figure 2."
D18-1447,N15-1014,0,0.0311277,"Ds = {x0(i) , y(i) i=1 aforementioned methods, we mix labeled data Dp with Ds to train the seq2seq model. As described in Algorithm 2, we combine Dp with Ds into Dp+s and shuffle Dp+s randomly, then we pre-train the model on Dp+s , in which no network parameters are frozen during the training process. The best performing model is selected based on validation set, then fine-tuned on Dp until converge. 4.2 Multi-task Learning with Auxiliary Task similar spirit as keyphrases. We thus choose title generation as auxiliary task, which has been studied as a summarization problem (Rush et al., 2015; Colmenares et al., 2015). Let Du0 = {x0 (i) , q(i) }M i=1 denote the dataset which is assigned with titles for unlabeled data Du , the loss function of multi-task learning is factorized as: L(θe , θ1d , θ2d ) = − − log p(y(i) |x(i) ; θe , θ1d ) i=1 M X log p(q(i) |x0 (i) ; θe , θ2d ) (7) i=1 where θe indicates encoder parameters; θ1d and θ2d are the decoder parameters. Training Procedure. We adopt a simple alternating training strategy to switch training between the main task and the auxiliary task. Specifically, we first estimate parameters on auxiliary task with Du0 for one epoch, then train model on the main task"
D18-1447,P16-1004,0,0.0294046,"ied in more recent work. For instance, Liu et al. (2011) propose to use statistic machine translation model to learn word-alignments between documents and keyphrases, enabling the model to generate keyphrases which do not appear in the input. Meng et al. (2017) train seq2seq-based generation models (Sutskever et al., 2014) on large-scale labeled corpora, which may not be applicable to a new domain with minimal human labels. Neural Semi-supervised Learning. As mentioned above, though significant success has been achieved by seq2seq model in many NLP tasks (Luong et al., 2015; See et al., 2017; Dong and Lapata, 2016; Wang and Ling, 2016; Ye et al., 2018), they often rely on large amounts of labeled data, which are expensive to get. In order to mitigate the problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training (Dai and Le, 2015; Ramachandran et al., 2017). For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to improve translation quality (G¨ulc¸ehre et al., 2015), where generating synthetic data (Sennrich et al., 2016; Zhang and Zong, 2016), multi-task learning (Zhang and Zong, 2016), a"
D18-1447,C10-2042,0,0.0574973,"hods of synthetic data construction with unsupervised learning and multi-task learning. We further carry out experiments with randomly selected 50K(1/8), 100K(1/4), 200K(1/2) and 300K(3/4) unlabeled documents from the pool of 400K unlabeled data. After models being trained, we adopt beam search to generate keyphrase sequences with beam size of 50. We keep top N keyphrase sequences to yield the final keyphrases using Algorithm 1. F1 @10 is adopted to illustrate Table 4: Results of keyphrase generation for news from DUC dataset with F1 . Results of unsupervised learning methods are adopted from Hasan and Ng (2010). the model performances. N is set as 10 or 50. The present keyphrase generation results are shown in Figure 6, from which we can see that when increasing the amount of unlabeled data, model performance is further improved. This is because additional unlabeled data can provide with more evidence on linguistic or context features and thus make the model, especially the encoder, have better generalizability. This finding echoes with the training procedure illustrated in Figure 7, where more unlabeled data uniformly leads to better performance. Therefore, we believe that leveraging more unlabeled"
D18-1447,P14-1119,0,0.144747,"d data only. 1 in document easy to implement, extract-based approaches fail to generate keyphrases that do not appear in the source document, which are frequently produced by human annotators as shown in Figure 1. Recently, Meng et al. (2017) propose to use a sequence-to-sequence model (Sutskever et al., 2014) with copying mechanism for keyphrase generation, which is able to produce phrases that are not in the input documents. Introduction Keyphrase extraction concerns the task of selecting a set of phrases from a document that can indicate the main ideas expressed in the input (Turney, 2000; Hasan and Ng, 2014). It is an essential task for document understanding because accurate identification of keyphrases can be beneficial for a wide range of downstreaming natural language processing and information retrieval applications. For instance, keyphrases can be leveraged to improve text summarization systems (Zhang et al., 2004; Liu et al., 2009a; Wang and Cardie, 2013), facilitate sentiment analysis and opinion mining (Wilson et al., 2005; Berend, 2011), and help with document clustering (Hammouda et al., 2005). Though relatively ∗ not in document Figure 1: Sample document with labeled keyphrases. Work"
D18-1447,P82-1020,0,0.822686,"Missing"
D18-1447,W03-1028,0,0.946007,"r models yield better F1 than a model trained on labeled data only. Finally, we also show that training with unlabeled samples can further produce performance gain even when a large amount of labeled data is available. 2 Related Work Keyphrase Extraction and Generation. Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed. Specifically, a large pool of candidate phrases are first extracted according to predefined syntactic templates (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2009b, 2011) or their estimated importance scores (Hulth, 2003). In the second step, re-ranking is applied to yield the final keyphrases, based on supervised learning (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Lopez and Romary, 2010; Kim and Kan, 2009), unsupervised graph algorithms (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Bougouin et al., 2013), or topic modelings (Liu et al., 2009c, 2010). Keyphrase generation is stud1 We use “semi-supervised learning” as a broad term to refer to the two methods proposed in this paper. ied in more recent work. For instance, Liu et al. (2011) propose to use statistic machine translation model to learn w"
D18-1447,W09-2902,0,0.0630298,"eled data is available. 2 Related Work Keyphrase Extraction and Generation. Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed. Specifically, a large pool of candidate phrases are first extracted according to predefined syntactic templates (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2009b, 2011) or their estimated importance scores (Hulth, 2003). In the second step, re-ranking is applied to yield the final keyphrases, based on supervised learning (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Lopez and Romary, 2010; Kim and Kan, 2009), unsupervised graph algorithms (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Bougouin et al., 2013), or topic modelings (Liu et al., 2009c, 2010). Keyphrase generation is stud1 We use “semi-supervised learning” as a broad term to refer to the two methods proposed in this paper. ied in more recent work. For instance, Liu et al. (2011) propose to use statistic machine translation model to learn word-alignments between documents and keyphrases, enabling the model to generate keyphrases which do not appear in the input. Meng et al. (2017) train seq2seq-based generation models (Sutskever et al.,"
D18-1447,N09-1070,0,0.579484,"keyphrase generation, which is able to produce phrases that are not in the input documents. Introduction Keyphrase extraction concerns the task of selecting a set of phrases from a document that can indicate the main ideas expressed in the input (Turney, 2000; Hasan and Ng, 2014). It is an essential task for document understanding because accurate identification of keyphrases can be beneficial for a wide range of downstreaming natural language processing and information retrieval applications. For instance, keyphrases can be leveraged to improve text summarization systems (Zhang et al., 2004; Liu et al., 2009a; Wang and Cardie, 2013), facilitate sentiment analysis and opinion mining (Wilson et al., 2005; Berend, 2011), and help with document clustering (Hammouda et al., 2005). Though relatively ∗ not in document Figure 1: Sample document with labeled keyphrases. Work was done while visiting Northeastern University. While seq2seq model demonstrates good performance on keyphrase generation (Meng et al., 2017), it heavily relies on massive amounts of labeled data for model training, which is often unavailable for new domains. To overcome this drawback, in this work, we investigate semisupervised lear"
D18-1447,W11-0316,0,0.73593,"08; Liu et al., 2009b, 2011) or their estimated importance scores (Hulth, 2003). In the second step, re-ranking is applied to yield the final keyphrases, based on supervised learning (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Lopez and Romary, 2010; Kim and Kan, 2009), unsupervised graph algorithms (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Bougouin et al., 2013), or topic modelings (Liu et al., 2009c, 2010). Keyphrase generation is stud1 We use “semi-supervised learning” as a broad term to refer to the two methods proposed in this paper. ied in more recent work. For instance, Liu et al. (2011) propose to use statistic machine translation model to learn word-alignments between documents and keyphrases, enabling the model to generate keyphrases which do not appear in the input. Meng et al. (2017) train seq2seq-based generation models (Sutskever et al., 2014) on large-scale labeled corpora, which may not be applicable to a new domain with minimal human labels. Neural Semi-supervised Learning. As mentioned above, though significant success has been achieved by seq2seq model in many NLP tasks (Luong et al., 2015; See et al., 2017; Dong and Lapata, 2016; Wang and Ling, 2016; Ye et al., 2"
D18-1447,D10-1036,0,0.140973,"Missing"
D18-1447,D09-1027,0,0.746672,"keyphrase generation, which is able to produce phrases that are not in the input documents. Introduction Keyphrase extraction concerns the task of selecting a set of phrases from a document that can indicate the main ideas expressed in the input (Turney, 2000; Hasan and Ng, 2014). It is an essential task for document understanding because accurate identification of keyphrases can be beneficial for a wide range of downstreaming natural language processing and information retrieval applications. For instance, keyphrases can be leveraged to improve text summarization systems (Zhang et al., 2004; Liu et al., 2009a; Wang and Cardie, 2013), facilitate sentiment analysis and opinion mining (Wilson et al., 2005; Berend, 2011), and help with document clustering (Hammouda et al., 2005). Though relatively ∗ not in document Figure 1: Sample document with labeled keyphrases. Work was done while visiting Northeastern University. While seq2seq model demonstrates good performance on keyphrase generation (Meng et al., 2017), it heavily relies on massive amounts of labeled data for model training, which is often unavailable for new domains. To overcome this drawback, in this work, we investigate semisupervised lear"
D18-1447,S10-1055,0,0.226063,"en a large amount of labeled data is available. 2 Related Work Keyphrase Extraction and Generation. Early work mostly focuses on the keyphrase extraction task, and a two-step strategy is typically designed. Specifically, a large pool of candidate phrases are first extracted according to predefined syntactic templates (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Liu et al., 2009b, 2011) or their estimated importance scores (Hulth, 2003). In the second step, re-ranking is applied to yield the final keyphrases, based on supervised learning (Frank et al., 1999; Witten et al., 1999; Hulth, 2003; Lopez and Romary, 2010; Kim and Kan, 2009), unsupervised graph algorithms (Mihalcea and Tarau, 2004; Wan and Xiao, 2008; Bougouin et al., 2013), or topic modelings (Liu et al., 2009c, 2010). Keyphrase generation is stud1 We use “semi-supervised learning” as a broad term to refer to the two methods proposed in this paper. ied in more recent work. For instance, Liu et al. (2011) propose to use statistic machine translation model to learn word-alignments between documents and keyphrases, enabling the model to generate keyphrases which do not appear in the input. Meng et al. (2017) train seq2seq-based generation models"
D18-1447,D15-1166,0,0.493053,"e two methods proposed in this paper. ied in more recent work. For instance, Liu et al. (2011) propose to use statistic machine translation model to learn word-alignments between documents and keyphrases, enabling the model to generate keyphrases which do not appear in the input. Meng et al. (2017) train seq2seq-based generation models (Sutskever et al., 2014) on large-scale labeled corpora, which may not be applicable to a new domain with minimal human labels. Neural Semi-supervised Learning. As mentioned above, though significant success has been achieved by seq2seq model in many NLP tasks (Luong et al., 2015; See et al., 2017; Dong and Lapata, 2016; Wang and Ling, 2016; Ye et al., 2018), they often rely on large amounts of labeled data, which are expensive to get. In order to mitigate the problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training (Dai and Le, 2015; Ramachandran et al., 2017). For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to improve translation quality (G¨ulc¸ehre et al., 2015), where generating synthetic data (Sennrich et al., 2016; Zhang and Zong, 2016), mult"
D18-1447,D09-1137,0,0.615437,"., 2011; Meng et al., 2017), we adopt top-N macroaveraged precision, recall and F-measure (F1 ) for model evaluation. Precision means how many topN extracted keywords are correct and recall means how many target keyphrases are extracted in topN candidates. Porter Stemmer is applied before comparisons. Baselines. We train a baseline seq2seq model on the small-scale labeled dataset. We further compare with four unsupervised learning methods: TF-IDF, TextRank (Mihalcea and Tarau, 2004), SingleRank (Wan and Xiao, 2008), ExpandRank (Wan and Xiao, 2008), and two supervised learning methods of Maui (Medelyan et al., 2009) and KEA (Witten et al., 1999). We follow Meng et al. (2017) for baselines setups. Results. Here we show results for present and absent keyphrase generation separately4 . From the results of present keyphrase generation as shown in Table 2, although trained on small-scale 4 Recall that present means the keyphrase appears in the document, otherwise, it is absent. 4147 S EQ . 0.012 0.01 0.002 0.001 0.01 S YN .U N . 0.018 0.008 0.011 0.008 0.016 ppl. on validation set Dataset I NSPEC K RAPIVIN NUS S EM E VAL KP20 K 60 Semi-supervised S YN .S ELF. M ULTI . 0.013 0.022 0.018 0.021 0.003 0.013 0.003"
D18-1447,P17-1054,0,0.128015,"fer in the liquid phase. The resulting equations are discretized using a characteristics method in time and a ﬁnite element method in space, and we propose a numerical algorithm to solve the obtained nonlinear discretized problem. Finally, numerical results are given which are compared with industrial experimental measurements. Keyphrase: casting ; thermal; conduction; convection; ﬁnite element Abstract We study the problem of generating keyphrases that summarize the key points for a given document. While sequence-to-sequence (seq2seq) models have achieved remarkable performance on this task (Meng et al., 2017), model training often relies on large amounts of labeled data, which is only applicable to resource-rich domains. In this paper, we propose semi-supervised keyphrase generation methods by leveraging both labeled data and large-scale unlabeled samples for learning. Two strategies are proposed. First, unlabeled documents are first tagged with synthetic keyphrases obtained from unsupervised keyphrase extraction methods or a selflearning algorithm, and then combined with labeled samples for training. Furthermore, we investigate a multi-task learning framework to jointly learn to generate keyphras"
D18-1447,W04-3252,0,0.941152,"Missing"
D18-1447,D17-1039,0,0.0220963,"models (Sutskever et al., 2014) on large-scale labeled corpora, which may not be applicable to a new domain with minimal human labels. Neural Semi-supervised Learning. As mentioned above, though significant success has been achieved by seq2seq model in many NLP tasks (Luong et al., 2015; See et al., 2017; Dong and Lapata, 2016; Wang and Ling, 2016; Ye et al., 2018), they often rely on large amounts of labeled data, which are expensive to get. In order to mitigate the problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training (Dai and Le, 2015; Ramachandran et al., 2017). For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to improve translation quality (G¨ulc¸ehre et al., 2015), where generating synthetic data (Sennrich et al., 2016; Zhang and Zong, 2016), multi-task learning (Zhang and Zong, 2016), and autoencoderbased methods (Cheng et al., 2016) are shown to be effective. Multi-task learning is also examined for sequence labeling tasks (Rei, 2017; Liu et al., 2018). In our paper, we study semi-supervised learning for keyphrase generation based on seq2seq models, which has not been explored"
D18-1447,P17-1194,0,0.03713,"e problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training (Dai and Le, 2015; Ramachandran et al., 2017). For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to improve translation quality (G¨ulc¸ehre et al., 2015), where generating synthetic data (Sennrich et al., 2016; Zhang and Zong, 2016), multi-task learning (Zhang and Zong, 2016), and autoencoderbased methods (Cheng et al., 2016) are shown to be effective. Multi-task learning is also examined for sequence labeling tasks (Rei, 2017; Liu et al., 2018). In our paper, we study semi-supervised learning for keyphrase generation based on seq2seq models, which has not been explored before. Besides, we focus on leveraging source-side unlabeled articles to enhance performance with synthetic keyphrase construction or multi-task learning. 3 Neural Keyphrase Generation Model In this section, we describe the neural keyphrase generation model built on a sequence-to-sequence model (Sutskever et al., 2014) as illustrated in Figure 2. We denote the input source document as a sequence x = x1 · · · x|x |and its correspond|a| ing keyphrase"
D18-1447,D15-1044,0,0.186671,"or Computational Linguistics unlabeled documents are first tagged with synthetic keyphrases, then mixed with labeled data for model pre-training. Synthetic keyphrases are acquired through existing unsupervised keyphrase extraction methods (e.g., TF-IDF or TextRank (Mihalcea and Tarau, 2004)) or a self-learning algorithm. The pre-trained model will further be fine-tuned on the labeled data only. For the second approach, we propose a multi-task learning (MTL) framework1 by jointly learning the main task of keyphrase generation based on labeled samples, and an auxiliary task of title generation (Rush et al., 2015) on unlabeled documents. Here one encoder is shared among the two tasks. Importantly, we test our proposed methods on a seq2seq framework, however, we believe they can be easily adapted to other encoder-decoder-based systems. Extensive experiments are conducted in scientific paper domain. Results on five different datasets show that all of our semi-supervised learning-based models can uniformly significantly outperform a state-of-the-art model (Meng et al., 2017) as well as several competitive unsupervised and supervised keyphrase extraction algorithms based on F1 and recall scores. We further"
D18-1447,P17-1099,0,0.476081,"ed in this paper. ied in more recent work. For instance, Liu et al. (2011) propose to use statistic machine translation model to learn word-alignments between documents and keyphrases, enabling the model to generate keyphrases which do not appear in the input. Meng et al. (2017) train seq2seq-based generation models (Sutskever et al., 2014) on large-scale labeled corpora, which may not be applicable to a new domain with minimal human labels. Neural Semi-supervised Learning. As mentioned above, though significant success has been achieved by seq2seq model in many NLP tasks (Luong et al., 2015; See et al., 2017; Dong and Lapata, 2016; Wang and Ling, 2016; Ye et al., 2018), they often rely on large amounts of labeled data, which are expensive to get. In order to mitigate the problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training (Dai and Le, 2015; Ramachandran et al., 2017). For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to improve translation quality (G¨ulc¸ehre et al., 2015), where generating synthetic data (Sennrich et al., 2016; Zhang and Zong, 2016), multi-task learning (Z"
D18-1447,P16-1009,0,0.0746154,"eq2seq model in many NLP tasks (Luong et al., 2015; See et al., 2017; Dong and Lapata, 2016; Wang and Ling, 2016; Ye et al., 2018), they often rely on large amounts of labeled data, which are expensive to get. In order to mitigate the problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training (Dai and Le, 2015; Ramachandran et al., 2017). For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to improve translation quality (G¨ulc¸ehre et al., 2015), where generating synthetic data (Sennrich et al., 2016; Zhang and Zong, 2016), multi-task learning (Zhang and Zong, 2016), and autoencoderbased methods (Cheng et al., 2016) are shown to be effective. Multi-task learning is also examined for sequence labeling tasks (Rei, 2017; Liu et al., 2018). In our paper, we study semi-supervised learning for keyphrase generation based on seq2seq models, which has not been explored before. Besides, we focus on leveraging source-side unlabeled articles to enhance performance with synthetic keyphrase construction or multi-task learning. 3 Neural Keyphrase Generation Model In this section, we describe the neural"
D18-1447,P13-1137,1,0.873225,"Missing"
D18-1447,N16-1007,1,0.854512,". For instance, Liu et al. (2011) propose to use statistic machine translation model to learn word-alignments between documents and keyphrases, enabling the model to generate keyphrases which do not appear in the input. Meng et al. (2017) train seq2seq-based generation models (Sutskever et al., 2014) on large-scale labeled corpora, which may not be applicable to a new domain with minimal human labels. Neural Semi-supervised Learning. As mentioned above, though significant success has been achieved by seq2seq model in many NLP tasks (Luong et al., 2015; See et al., 2017; Dong and Lapata, 2016; Wang and Ling, 2016; Ye et al., 2018), they often rely on large amounts of labeled data, which are expensive to get. In order to mitigate the problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training (Dai and Le, 2015; Ramachandran et al., 2017). For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to improve translation quality (G¨ulc¸ehre et al., 2015), where generating synthetic data (Sennrich et al., 2016; Zhang and Zong, 2016), multi-task learning (Zhang and Zong, 2016), and autoencoderbased m"
D18-1447,H05-1044,0,0.0317576,"ntroduction Keyphrase extraction concerns the task of selecting a set of phrases from a document that can indicate the main ideas expressed in the input (Turney, 2000; Hasan and Ng, 2014). It is an essential task for document understanding because accurate identification of keyphrases can be beneficial for a wide range of downstreaming natural language processing and information retrieval applications. For instance, keyphrases can be leveraged to improve text summarization systems (Zhang et al., 2004; Liu et al., 2009a; Wang and Cardie, 2013), facilitate sentiment analysis and opinion mining (Wilson et al., 2005; Berend, 2011), and help with document clustering (Hammouda et al., 2005). Though relatively ∗ not in document Figure 1: Sample document with labeled keyphrases. Work was done while visiting Northeastern University. While seq2seq model demonstrates good performance on keyphrase generation (Meng et al., 2017), it heavily relies on massive amounts of labeled data for model training, which is often unavailable for new domains. To overcome this drawback, in this work, we investigate semisupervised learning for keyphrase generation, by leveraging abundant unlabeled documents along with limited lab"
D18-1447,N18-1168,1,0.714886,"t al. (2011) propose to use statistic machine translation model to learn word-alignments between documents and keyphrases, enabling the model to generate keyphrases which do not appear in the input. Meng et al. (2017) train seq2seq-based generation models (Sutskever et al., 2014) on large-scale labeled corpora, which may not be applicable to a new domain with minimal human labels. Neural Semi-supervised Learning. As mentioned above, though significant success has been achieved by seq2seq model in many NLP tasks (Luong et al., 2015; See et al., 2017; Dong and Lapata, 2016; Wang and Ling, 2016; Ye et al., 2018), they often rely on large amounts of labeled data, which are expensive to get. In order to mitigate the problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training (Dai and Le, 2015; Ramachandran et al., 2017). For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to improve translation quality (G¨ulc¸ehre et al., 2015), where generating synthetic data (Sennrich et al., 2016; Zhang and Zong, 2016), multi-task learning (Zhang and Zong, 2016), and autoencoderbased methods (Cheng et a"
D18-1447,D16-1160,0,0.180453,"P tasks (Luong et al., 2015; See et al., 2017; Dong and Lapata, 2016; Wang and Ling, 2016; Ye et al., 2018), they often rely on large amounts of labeled data, which are expensive to get. In order to mitigate the problem, semi-supervised learning has been investigated to incorporate unlabeled data for modeling training (Dai and Le, 2015; Ramachandran et al., 2017). For example, neural machine translation community has studied the usage of source-side or target-side monolingual data to improve translation quality (G¨ulc¸ehre et al., 2015), where generating synthetic data (Sennrich et al., 2016; Zhang and Zong, 2016), multi-task learning (Zhang and Zong, 2016), and autoencoderbased methods (Cheng et al., 2016) are shown to be effective. Multi-task learning is also examined for sequence labeling tasks (Rei, 2017; Liu et al., 2018). In our paper, we study semi-supervised learning for keyphrase generation based on seq2seq models, which has not been explored before. Besides, we focus on leveraging source-side unlabeled articles to enhance performance with synthetic keyphrase construction or multi-task learning. 3 Neural Keyphrase Generation Model In this section, we describe the neural keyphrase generation mo"
D18-1447,S10-1004,0,\N,Missing
D18-1447,P06-4018,0,\N,Missing
D18-1447,W02-0109,0,\N,Missing
D19-1055,W14-3348,0,0.013237,"ection and generation performance, e.g., for BLEU, Pearson correlations of 0.95 (p < 10−4 ) and 0.85 (p < 10−2 ) are established for Wikipedia and abstract. For ROUGE, the values are 0.99 (p < 10−8 ) and 0.72 (p < 10−1 ). Abstract Generation. Lastly, we compare with the state-of-the-art G RAPH W RITER model on AGENDA dataset in Table 5. Although our model does not make use of the relational graph encodResults and Analysis Automatic Evaluation We report precesion-oriented BLEU (Papineni et al., 2002), recall-oriented ROUGE-L (Lin, 2004) that measures the longest common subsequence, and METEOR (Denkowski and Lavie, 2014), which considers both precision and recall. Argument Generation. For each input OP, there can be multiple possible counter-arguments. We thus consider the best matched (i.e., highest scored) reference when reporting results in Table 3. Our models yield significantly higher BLEU and ROUGE scores than all comparisons while 6 We do not compare with our recent model in Hua et al. (2019) due to the training data difference caused by our new sentence style scheme. However, the newly proposed model generates arguments with lengths closer to human arguments, benefiting from the improved content plann"
D19-1055,N18-3011,0,0.0313446,"Missing"
D19-1055,W03-1016,0,0.333406,"Missing"
D19-1055,W18-6539,0,0.043602,"Missing"
D19-1055,E17-1024,0,0.0194455,"candidates are “selected”. words, together with their related terms from WordNet (Miller, 1994), are used to determine whether a phrase in the passage is a keyphrase. Specifically, a keyphrase is (1) a noun phrase or verb phrase that is shorter than 10 tokens; (2) contains at least one content word; (3) has a topic signature or a Wikipedia title. For each keyphrase candidate, we match them with the sentences in the target counter-argument, and we consider it to be “selected” for the sentence if there is any overlapping content word. During test time, we further adopt a stance classifier from Bar-Haim et al. (2017) to produce a stance score for each passage. We retain passages that have a negative stance towards OP, and a greater than 5 stance score. They are further ordered based on the number of overlapping keyphrases with the OP. Top 10 passages are used to construct the input keyphrase bank, and as optional input to our model. Sentence Style Label Construction. For argument generation, we define three sentence styles based on their argumentative discourse functions (Persing and Ng, 2016; Lippi and Torroni, 2016): C LAIM is a proposition, usually containing one or two talking points, e.g., “I believe"
D19-1055,P18-1082,0,0.0994262,"its chance of successfully achieving its goals. ... Simple Wikipedia: Artificial Intelligence is the ability of a computer program or a machine to think and learn. ... Figure 1: [Upper] Sample counter-argument from Reddit. Argumentative stylistic language for persuasion is in italics. [Bottom] Excerpts from Wikipedia, where sophisticated concepts and language of higher complexity used in the standard version are not present in the corresponding simplified version. Both: key concepts are in bold. has been made by developing end-to-end trained neural models (Rush et al., 2015; Yu et al., 2018; Fan et al., 2018), which naturally excel at producing fluent text. Nonetheless, limitations of model structures and training objectives make them suffer from low interpretability and substandard generations which are often incoherent and unfaithful to the input material (See et al., 2017; Wiseman et al., 2017; Li et al., 2017). To address the problems, we believe it is imperative for neural models to gain adequate control on content planning (i.e., content selection and ordering) to produce coherent output, especially for long text generation. We further argue that, in order to achieve desired discourse goals,"
D19-1055,H05-1042,0,0.180767,"coders: for each sentence, (1) a planning decoder selects relevant keyphrases and a desired style conditional on previous selections, and (2) a realization decoder produces the text in 2 Related Work Content selection and text planning are critical components in traditional text generation systems (Reiter and Dale, 2000). Early approaches separately construct each module and mainly rely on hand-crafted rules based on discourse theory (Scott and de Souza, 1990; Hovy, 1993) and expert knowledge (Reiter et al., 2000), or train statistical classifiers with rich features (Duboue and McKeown, 2003; Barzilay and Lapata, 2005). Advances in neural generation models have alleviated human efforts on system engineering, by combining all components into an end-to-end trained conditional text generation framework (Mei et al., 2016; Wiseman et al., 2017). However, without 1 Data and code are available at xinyuhua.github. io/Resources/emnlp19/. 592 which can be a counter-argument, a paragraph as in Wikipedia articles, or a paper abstract. proper planning and control (Rambow and Korelsky, 1992; Stone and Doran, 1997; Walker et al., 2001), the outputs are often found to be incoherent and hallucinating. Recent work (Moryossef"
D19-1055,P19-1255,1,0.893434,"th phrase is selected for the j-th sentence generation. Starting with a <START> tag as the input for the first step, our planner predicts v1 for the first sentence, and recurrently makes predictions per sentence until <END> is selected, as depicted in Figure 2. Formally, we utilize a sentence-level LSTM f , which consumes the summation embedding of selected keyphrases, mj , to produce a hidden state sj for the j-th sentence step: sj = f (sj−1 , mj ) Model mj = |M| X vj,k hek (1) (2) k=1 where vj,k ∈ {0, 1} is the selection decision for the k-th keyphrase in the j-th sentence. Our recent work (Hua et al., 2019) utilizes a similar formulation for sentence representations. However, the prediction of vj+1 is estimated by a bilinear product between hek and sj , which is agnostic to what have been selected so far. While in reality, content selection for a new sentence should depend on previous selections. For inOur model tackles conditional text generation tasks where the input is comprised of two major parts: (1) a topic statement, x = {xi }, which can be an argument, the title of a Wikipedia article, or a scientific paper title, and (2) a keyphrase memory bank, M, containing a list of talking points, w"
D19-1055,E17-1060,0,0.0194262,"t al., 2019). Shared tasks such as WebNLG (Colin et al., 2016) and E2E NLG challenges (Duˇsek et al., 2019) have been designed to evaluate single sentence planning and realization from the given structured inputs with a small set of fixed attribute types. Planning for multiple sentences in the same paragraph is nevertheless much less studied; it poses extra challenges for generating coherent long text, which is addressed in this work. Moreover, structured inputs are only available in a limited number of domains (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Belz, 2008; Liang et al., 2009; Chisholm et al., 2017). The emerging trend is to explore less structured data (Kiddon et al., 2016; Fan et al., 2018; Martin et al., 2018). In our work, keyphrases are used as input to our generation system, which offer flexibility for concept representation and generalizability to broader domains. 3 3.1 Input Encoding The input text x is encoded via a bidirectional LSTM (biLSTM), with its last hidden state used as the initial states for both content planning decoder and surface realization decoder. To encode keyphrases in the memory bank M, each keyphrase is first converted into a vector ek by summing up all its w"
D19-1055,P18-1021,1,0.911275,"of our framework. The LSTM content planning decoder (§ 3.2) first identifies a set of keyphrases from the memory bank conditional on previous selection history, based on which, a style is specified. During surface realization, the hidden states of the planning decoder and the predicted style encoding are fed into the realizer, which generates the final output (§ 3.3). Best viewed in color. the specified style. We demonstrate the effectiveness of our framework on three challenging datasets with diverse topics and varying linguistic styles: persuasive argument generation on Reddit ChangeMyView (Hua and Wang, 2018); introduction paragraph generation on a newly collected dataset from Wikipedia and its simple version; and scientific paper abstract generation on AGENDA dataset (Koncel-Kedziorski et al., 2019). Experimental results on all three datasets show that our models that consider content planning and style selection achieve significantly better BLEU, ROUGE, and METEOR scores than non-trivial comparisons that do not consider such information. Human judges also rate our model generations as more fluent and correct compared to the outputs produced by its variants without style modeling. a series of tal"
D19-1055,W16-6626,0,0.118241,"Missing"
D19-1055,D16-1032,0,0.02048,"lenges (Duˇsek et al., 2019) have been designed to evaluate single sentence planning and realization from the given structured inputs with a small set of fixed attribute types. Planning for multiple sentences in the same paragraph is nevertheless much less studied; it poses extra challenges for generating coherent long text, which is addressed in this work. Moreover, structured inputs are only available in a limited number of domains (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Belz, 2008; Liang et al., 2009; Chisholm et al., 2017). The emerging trend is to explore less structured data (Kiddon et al., 2016; Fan et al., 2018; Martin et al., 2018). In our work, keyphrases are used as input to our generation system, which offer flexibility for concept representation and generalizability to broader domains. 3 3.1 Input Encoding The input text x is encoded via a bidirectional LSTM (biLSTM), with its last hidden state used as the initial states for both content planning decoder and surface realization decoder. To encode keyphrases in the memory bank M, each keyphrase is first converted into a vector ek by summing up all its words’ embeddings from GloVe (Pennington et al., 2014). A biLSTM-based keyphr"
D19-1055,C00-1072,0,0.212067,"ument by itself. Statistics are shown in Table 1. Input Keyphrases and Label Construction. To obtain the input keyphrase candidates and their sentence-level selection labels, we first construct queries to retrieve passages from Wikipedia and news articles collected from commoncrawl. org.3 For training, we construct a query per target argument sentence using its content words for retrieval, and keep top 5 passages per query. For testing, the queries are constructed from the sentences in OP (input statement). We then extract keyphrases from the retrieved passages based on topic signature words (Lin and Hovy, 2000) calculated over the given OP. These • C LAIM: must be shorter than 20 tokens and matches any of the following patterns: (a) i (don’t)? (believe|agree|...); (b) (anyone|all|everyone|nobody...) (should|could|need|must|might...); 3 The choice of news portals, statistics of the dataset, and preprocerssing steps are described in Hua et al. (2019), §4.1. 595 the input contains a paper title and scientific entities mentioned in the abstract. We use the AGENDA data processed by Koncel-Kedziorski et al. (2019), where entities and their relations in the abstracts are extracted by SciIE (Luan et al., 20"
D19-1055,N19-1238,0,0.0418243,"Missing"
D19-1055,D18-1360,0,0.105065,"ration process and shows improved generation quality. However, their method requires an exhaustive search for content ordering and is therefore hard to generalize and scale. In this work, we improve the content selection by incorporating past selection history and directly feeding the predicted language style into the realization module. Our work is also inline with concept-totext generation, where sentences are produced from structured representations, such as database records (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Moryossef et al., 2019), knowledge base items (Luan et al., 2018; Koncel-Kedziorski et al., 2019), and AMR graphs (Konstas et al., 2017; Song et al., 2018; Koncel-Kedziorski et al., 2019). Shared tasks such as WebNLG (Colin et al., 2016) and E2E NLG challenges (Duˇsek et al., 2019) have been designed to evaluate single sentence planning and realization from the given structured inputs with a small set of fixed attribute types. Planning for multiple sentences in the same paragraph is nevertheless much less studied; it poses extra challenges for generating coherent long text, which is addressed in this work. Moreover, structured inputs are only available in"
D19-1055,P17-1014,0,0.0274917,"method requires an exhaustive search for content ordering and is therefore hard to generalize and scale. In this work, we improve the content selection by incorporating past selection history and directly feeding the predicted language style into the realization module. Our work is also inline with concept-totext generation, where sentences are produced from structured representations, such as database records (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Moryossef et al., 2019), knowledge base items (Luan et al., 2018; Koncel-Kedziorski et al., 2019), and AMR graphs (Konstas et al., 2017; Song et al., 2018; Koncel-Kedziorski et al., 2019). Shared tasks such as WebNLG (Colin et al., 2016) and E2E NLG challenges (Duˇsek et al., 2019) have been designed to evaluate single sentence planning and realization from the given structured inputs with a small set of fixed attribute types. Planning for multiple sentences in the same paragraph is nevertheless much less studied; it poses extra challenges for generating coherent long text, which is addressed in this work. Moreover, structured inputs are only available in a limited number of domains (Tanaka-Ishii et al., 1998; Chen and Mooney"
D19-1055,D13-1157,0,0.0251421,"incoherent and hallucinating. Recent work (Moryossef et al., 2019) separates content selection from the neural generation process and shows improved generation quality. However, their method requires an exhaustive search for content ordering and is therefore hard to generalize and scale. In this work, we improve the content selection by incorporating past selection history and directly feeding the predicted language style into the realization module. Our work is also inline with concept-totext generation, where sentences are produced from structured representations, such as database records (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Moryossef et al., 2019), knowledge base items (Luan et al., 2018; Koncel-Kedziorski et al., 2019), and AMR graphs (Konstas et al., 2017; Song et al., 2018; Koncel-Kedziorski et al., 2019). Shared tasks such as WebNLG (Colin et al., 2016) and E2E NLG challenges (Duˇsek et al., 2019) have been designed to evaluate single sentence planning and realization from the given structured inputs with a small set of fixed attribute types. Planning for multiple sentences in the same paragraph is nevertheless much less studied; it poses extra challenges for gener"
D19-1055,N16-1086,0,0.0304181,"ction and text planning are critical components in traditional text generation systems (Reiter and Dale, 2000). Early approaches separately construct each module and mainly rely on hand-crafted rules based on discourse theory (Scott and de Souza, 1990; Hovy, 1993) and expert knowledge (Reiter et al., 2000), or train statistical classifiers with rich features (Duboue and McKeown, 2003; Barzilay and Lapata, 2005). Advances in neural generation models have alleviated human efforts on system engineering, by combining all components into an end-to-end trained conditional text generation framework (Mei et al., 2016; Wiseman et al., 2017). However, without 1 Data and code are available at xinyuhua.github. io/Resources/emnlp19/. 592 which can be a counter-argument, a paragraph as in Wikipedia articles, or a paper abstract. proper planning and control (Rambow and Korelsky, 1992; Stone and Doran, 1997; Walker et al., 2001), the outputs are often found to be incoherent and hallucinating. Recent work (Moryossef et al., 2019) separates content selection from the neural generation process and shows improved generation quality. However, their method requires an exhaustive search for content ordering and is there"
D19-1055,D16-1128,0,0.0610548,"Missing"
D19-1055,C18-1176,0,0.0173784,"phrase bank, and as optional input to our model. Sentence Style Label Construction. For argument generation, we define three sentence styles based on their argumentative discourse functions (Persing and Ng, 2016; Lippi and Torroni, 2016): C LAIM is a proposition, usually containing one or two talking points, e.g., “I believe foreign aid is a useful bargaining chip”; P REMISE contains supporting arguments with reasoning or examples; F UNCTIONAL is usually a generic statement, e.g., “I understand what you said”. For training, we employ a list of rules extended from the claim detection method by Levy et al. (2018) to automatically construct a style label for each sentence. Statistics are displayed in Table 2, and sample rules are shown below, with the complete list in the Supplementary: We reuse the dataset from our previous work (Hua et al., 2019), but annotate with newly designed style scheme. We first briefly summarize the procedures for data collection, keyphrase extraction and selection, and passage reranking; more details can be found in our prior work. Then we describe how to label argument sentences with style types that capture argumentative structures. The dataset is collected from Reddit /r/"
D19-1055,H94-1111,0,0.816565,"Missing"
D19-1055,N19-1236,0,0.0601847,"Missing"
D19-1055,D17-1230,0,0.0387936,"kipedia, where sophisticated concepts and language of higher complexity used in the standard version are not present in the corresponding simplified version. Both: key concepts are in bold. has been made by developing end-to-end trained neural models (Rush et al., 2015; Yu et al., 2018; Fan et al., 2018), which naturally excel at producing fluent text. Nonetheless, limitations of model structures and training objectives make them suffer from low interpretability and substandard generations which are often incoherent and unfaithful to the input material (See et al., 2017; Wiseman et al., 2017; Li et al., 2017). To address the problems, we believe it is imperative for neural models to gain adequate control on content planning (i.e., content selection and ordering) to produce coherent output, especially for long text generation. We further argue that, in order to achieve desired discourse goals, it is beneficial to enable style-controlled surface realization by explicitly modeling and specifying proper linguistic styles. Consider the task of producing counter-arguments to the topic “US should cut off foreign aid completely”. A sample argument in Figure 1 demonstrates how human selects Introduction Au"
D19-1055,P09-1011,0,0.0334135,"Koncel-Kedziorski et al., 2019). Shared tasks such as WebNLG (Colin et al., 2016) and E2E NLG challenges (Duˇsek et al., 2019) have been designed to evaluate single sentence planning and realization from the given structured inputs with a small set of fixed attribute types. Planning for multiple sentences in the same paragraph is nevertheless much less studied; it poses extra challenges for generating coherent long text, which is addressed in this work. Moreover, structured inputs are only available in a limited number of domains (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Belz, 2008; Liang et al., 2009; Chisholm et al., 2017). The emerging trend is to explore less structured data (Kiddon et al., 2016; Fan et al., 2018; Martin et al., 2018). In our work, keyphrases are used as input to our generation system, which offer flexibility for concept representation and generalizability to broader domains. 3 3.1 Input Encoding The input text x is encoded via a bidirectional LSTM (biLSTM), with its last hidden state used as the initial states for both content planning decoder and surface realization decoder. To encode keyphrases in the memory bank M, each keyphrase is first converted into a vector ek"
D19-1055,P02-1040,0,0.104305,"samples into 10 bins based on F1 scores on keyphrase selection.7 We observe a strong correlation between keyphrase selection and generation performance, e.g., for BLEU, Pearson correlations of 0.95 (p < 10−4 ) and 0.85 (p < 10−2 ) are established for Wikipedia and abstract. For ROUGE, the values are 0.99 (p < 10−8 ) and 0.72 (p < 10−1 ). Abstract Generation. Lastly, we compare with the state-of-the-art G RAPH W RITER model on AGENDA dataset in Table 5. Although our model does not make use of the relational graph encodResults and Analysis Automatic Evaluation We report precesion-oriented BLEU (Papineni et al., 2002), recall-oriented ROUGE-L (Lin, 2004) that measures the longest common subsequence, and METEOR (Denkowski and Lavie, 2014), which considers both precision and recall. Argument Generation. For each input OP, there can be multiple possible counter-arguments. We thus consider the best matched (i.e., highest scored) reference when reporting results in Table 3. Our models yield significantly higher BLEU and ROUGE scores than all comparisons while 6 We do not compare with our recent model in Hua et al. (2019) due to the training data difference caused by our new sentence style scheme. However, the n"
D19-1055,W04-1013,0,0.0124858,"ase selection.7 We observe a strong correlation between keyphrase selection and generation performance, e.g., for BLEU, Pearson correlations of 0.95 (p < 10−4 ) and 0.85 (p < 10−2 ) are established for Wikipedia and abstract. For ROUGE, the values are 0.99 (p < 10−8 ) and 0.72 (p < 10−1 ). Abstract Generation. Lastly, we compare with the state-of-the-art G RAPH W RITER model on AGENDA dataset in Table 5. Although our model does not make use of the relational graph encodResults and Analysis Automatic Evaluation We report precesion-oriented BLEU (Papineni et al., 2002), recall-oriented ROUGE-L (Lin, 2004) that measures the longest common subsequence, and METEOR (Denkowski and Lavie, 2014), which considers both precision and recall. Argument Generation. For each input OP, there can be multiple possible counter-arguments. We thus consider the best matched (i.e., highest scored) reference when reporting results in Table 3. Our models yield significantly higher BLEU and ROUGE scores than all comparisons while 6 We do not compare with our recent model in Hua et al. (2019) due to the training data difference caused by our new sentence style scheme. However, the newly proposed model generates argumen"
D19-1055,D14-1162,0,0.082052,"plore less structured data (Kiddon et al., 2016; Fan et al., 2018; Martin et al., 2018). In our work, keyphrases are used as input to our generation system, which offer flexibility for concept representation and generalizability to broader domains. 3 3.1 Input Encoding The input text x is encoded via a bidirectional LSTM (biLSTM), with its last hidden state used as the initial states for both content planning decoder and surface realization decoder. To encode keyphrases in the memory bank M, each keyphrase is first converted into a vector ek by summing up all its words’ embeddings from GloVe (Pennington et al., 2014). A biLSTM-based keyphrase reader, with hidden states hek , is used to encode all keyphrases in M. We also insert entries of <START> and <END> into M to facilitate learning to start and finish selection. 3.2 Sentence-Level Content Planning and Style Specification Content Planning: Context-Aware Keyphrase Selection. Our content planner selects a set of keyphrases from the memory bank M for each sentence, indexed with j, conditional on keyphrases that have been selected in previous sentences, allowing topical coherence and content repetition avoidance. The decisions are denoted as a selection ve"
D19-1055,P98-2209,0,0.37154,"019), and AMR graphs (Konstas et al., 2017; Song et al., 2018; Koncel-Kedziorski et al., 2019). Shared tasks such as WebNLG (Colin et al., 2016) and E2E NLG challenges (Duˇsek et al., 2019) have been designed to evaluate single sentence planning and realization from the given structured inputs with a small set of fixed attribute types. Planning for multiple sentences in the same paragraph is nevertheless much less studied; it poses extra challenges for generating coherent long text, which is addressed in this work. Moreover, structured inputs are only available in a limited number of domains (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Belz, 2008; Liang et al., 2009; Chisholm et al., 2017). The emerging trend is to explore less structured data (Kiddon et al., 2016; Fan et al., 2018; Martin et al., 2018). In our work, keyphrases are used as input to our generation system, which offer flexibility for concept representation and generalizability to broader domains. 3 3.1 Input Encoding The input text x is encoded via a bidirectional LSTM (biLSTM), with its last hidden state used as the initial states for both content planning decoder and surface realization decoder. To encode keyphrases in the memory ban"
D19-1055,N16-1164,0,0.0275013,"e sentence if there is any overlapping content word. During test time, we further adopt a stance classifier from Bar-Haim et al. (2017) to produce a stance score for each passage. We retain passages that have a negative stance towards OP, and a greater than 5 stance score. They are further ordered based on the number of overlapping keyphrases with the OP. Top 10 passages are used to construct the input keyphrase bank, and as optional input to our model. Sentence Style Label Construction. For argument generation, we define three sentence styles based on their argumentative discourse functions (Persing and Ng, 2016; Lippi and Torroni, 2016): C LAIM is a proposition, usually containing one or two talking points, e.g., “I believe foreign aid is a useful bargaining chip”; P REMISE contains supporting arguments with reasoning or examples; F UNCTIONAL is usually a generic statement, e.g., “I understand what you said”. For training, we employ a list of rules extended from the claim detection method by Levy et al. (2018) to automatically construct a style label for each sentence. Statistics are displayed in Table 2, and sample rules are shown below, with the complete list in the Supplementary: We reuse the dat"
D19-1055,N01-1003,0,0.144058,"), or train statistical classifiers with rich features (Duboue and McKeown, 2003; Barzilay and Lapata, 2005). Advances in neural generation models have alleviated human efforts on system engineering, by combining all components into an end-to-end trained conditional text generation framework (Mei et al., 2016; Wiseman et al., 2017). However, without 1 Data and code are available at xinyuhua.github. io/Resources/emnlp19/. 592 which can be a counter-argument, a paragraph as in Wikipedia articles, or a paper abstract. proper planning and control (Rambow and Korelsky, 1992; Stone and Doran, 1997; Walker et al., 2001), the outputs are often found to be incoherent and hallucinating. Recent work (Moryossef et al., 2019) separates content selection from the neural generation process and shows improved generation quality. However, their method requires an exhaustive search for content ordering and is therefore hard to generalize and scale. In this work, we improve the content selection by incorporating past selection history and directly feeding the predicted language style into the realization module. Our work is also inline with concept-totext generation, where sentences are produced from structured represen"
D19-1055,A92-1006,0,0.684882,"y, 1993) and expert knowledge (Reiter et al., 2000), or train statistical classifiers with rich features (Duboue and McKeown, 2003; Barzilay and Lapata, 2005). Advances in neural generation models have alleviated human efforts on system engineering, by combining all components into an end-to-end trained conditional text generation framework (Mei et al., 2016; Wiseman et al., 2017). However, without 1 Data and code are available at xinyuhua.github. io/Resources/emnlp19/. 592 which can be a counter-argument, a paragraph as in Wikipedia articles, or a paper abstract. proper planning and control (Rambow and Korelsky, 1992; Stone and Doran, 1997; Walker et al., 2001), the outputs are often found to be incoherent and hallucinating. Recent work (Moryossef et al., 2019) separates content selection from the neural generation process and shows improved generation quality. However, their method requires an exhaustive search for content ordering and is therefore hard to generalize and scale. In this work, we improve the content selection by incorporating past selection history and directly feeding the predicted language style into the realization module. Our work is also inline with concept-totext generation, where se"
D19-1055,P18-1153,0,0.021608,"ons that maximize its chance of successfully achieving its goals. ... Simple Wikipedia: Artificial Intelligence is the ability of a computer program or a machine to think and learn. ... Figure 1: [Upper] Sample counter-argument from Reddit. Argumentative stylistic language for persuasion is in italics. [Bottom] Excerpts from Wikipedia, where sophisticated concepts and language of higher complexity used in the standard version are not present in the corresponding simplified version. Both: key concepts are in bold. has been made by developing end-to-end trained neural models (Rush et al., 2015; Yu et al., 2018; Fan et al., 2018), which naturally excel at producing fluent text. Nonetheless, limitations of model structures and training objectives make them suffer from low interpretability and substandard generations which are often incoherent and unfaithful to the input material (See et al., 2017; Wiseman et al., 2017; Li et al., 2017). To address the problems, we believe it is imperative for neural models to gain adequate control on content planning (i.e., content selection and ordering) to produce coherent output, especially for long text generation. We further argue that, in order to achieve desir"
D19-1055,W00-1429,0,0.741123,"Missing"
D19-1055,D15-1044,0,0.0324701,"ment and takes actions that maximize its chance of successfully achieving its goals. ... Simple Wikipedia: Artificial Intelligence is the ability of a computer program or a machine to think and learn. ... Figure 1: [Upper] Sample counter-argument from Reddit. Argumentative stylistic language for persuasion is in italics. [Bottom] Excerpts from Wikipedia, where sophisticated concepts and language of higher complexity used in the standard version are not present in the corresponding simplified version. Both: key concepts are in bold. has been made by developing end-to-end trained neural models (Rush et al., 2015; Yu et al., 2018; Fan et al., 2018), which naturally excel at producing fluent text. Nonetheless, limitations of model structures and training objectives make them suffer from low interpretability and substandard generations which are often incoherent and unfaithful to the input material (See et al., 2017; Wiseman et al., 2017; Li et al., 2017). To address the problems, we believe it is imperative for neural models to gain adequate control on content planning (i.e., content selection and ordering) to produce coherent output, especially for long text generation. We further argue that, in order"
D19-1055,P17-1099,0,0.39775,"is in italics. [Bottom] Excerpts from Wikipedia, where sophisticated concepts and language of higher complexity used in the standard version are not present in the corresponding simplified version. Both: key concepts are in bold. has been made by developing end-to-end trained neural models (Rush et al., 2015; Yu et al., 2018; Fan et al., 2018), which naturally excel at producing fluent text. Nonetheless, limitations of model structures and training objectives make them suffer from low interpretability and substandard generations which are often incoherent and unfaithful to the input material (See et al., 2017; Wiseman et al., 2017; Li et al., 2017). To address the problems, we believe it is imperative for neural models to gain adequate control on content planning (i.e., content selection and ordering) to produce coherent output, especially for long text generation. We further argue that, in order to achieve desired discourse goals, it is beneficial to enable style-controlled surface realization by explicitly modeling and specifying proper linguistic styles. Consider the task of producing counter-arguments to the topic “US should cut off foreign aid completely”. A sample argument in Figure 1 demons"
D19-1055,P18-1150,0,0.0260201,"haustive search for content ordering and is therefore hard to generalize and scale. In this work, we improve the content selection by incorporating past selection history and directly feeding the predicted language style into the realization module. Our work is also inline with concept-totext generation, where sentences are produced from structured representations, such as database records (Konstas and Lapata, 2013; Lebret et al., 2016; Wiseman et al., 2017; Moryossef et al., 2019), knowledge base items (Luan et al., 2018; Koncel-Kedziorski et al., 2019), and AMR graphs (Konstas et al., 2017; Song et al., 2018; Koncel-Kedziorski et al., 2019). Shared tasks such as WebNLG (Colin et al., 2016) and E2E NLG challenges (Duˇsek et al., 2019) have been designed to evaluate single sentence planning and realization from the given structured inputs with a small set of fixed attribute types. Planning for multiple sentences in the same paragraph is nevertheless much less studied; it poses extra challenges for generating coherent long text, which is addressed in this work. Moreover, structured inputs are only available in a limited number of domains (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Belz, 2008;"
D19-1055,P97-1026,0,0.390381,"ge (Reiter et al., 2000), or train statistical classifiers with rich features (Duboue and McKeown, 2003; Barzilay and Lapata, 2005). Advances in neural generation models have alleviated human efforts on system engineering, by combining all components into an end-to-end trained conditional text generation framework (Mei et al., 2016; Wiseman et al., 2017). However, without 1 Data and code are available at xinyuhua.github. io/Resources/emnlp19/. 592 which can be a counter-argument, a paragraph as in Wikipedia articles, or a paper abstract. proper planning and control (Rambow and Korelsky, 1992; Stone and Doran, 1997; Walker et al., 2001), the outputs are often found to be incoherent and hallucinating. Recent work (Moryossef et al., 2019) separates content selection from the neural generation process and shows improved generation quality. However, their method requires an exhaustive search for content ordering and is therefore hard to generalize and scale. In this work, we improve the content selection by incorporating past selection history and directly feeding the predicted language style into the realization module. Our work is also inline with concept-totext generation, where sentences are produced fr"
D19-1323,E03-3002,0,0.205084,"Missing"
D19-1323,N18-1064,0,0.0390899,"ours, blue denotes human). In this work, besides informativeness, we also aim to improve upon these aspects of summaries. Role of Entities and Coherence in Summarization. Entities in a text carry useful contextual information (Nenkova, 2008) and therefore play an important role in multi-document summarization (Li et al., 2006) and event summarization for selecting salient sentences (Li et al., 2015). Moreover, entity mentions connecting sentences have also been used to extract non-adjacent yet coherent sentences (Siddharthan et al., 2011; Parveen et al., 2016). For abstractive summarization, Amplayo et al. (2018) find it beneficial to leverage entities that are linked to existing knowledge bases. Unfortunately, it fails to capture the entities that do not exist in these knowledge bases. Acknowledgements This research is supported in part by National Science Foundation through Grants IIS-1566382 and IIS-1813341, and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116. The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the officia"
D19-1323,J08-1001,0,0.119091,"or produces informative and coherent summaries. Both components are connected using reinforcement learning. enables enhanced input text interpretation, salient content selection, and coherent summary generation, three major challenges that need to be addressed by single-document summarization systems (Jones et al., 1999). We use a sample summary in Fig. 1 to show entity usage in summarization. Firstly, frequently mentioned entities from the input, along with their contextual information, underscores the salient content of the article (Nenkova, 2008). Secondly, as also discussed in prior work (Barzilay and Lapata, 2008; Siddharthan et al., 2011), patterns of entity distributions and how they are referred to contribute to the coherence and conciseness of the text. For instance, a human writer places the underlined sentence in the input article next to the first sentence in the summary to improve topical coherence as they are about the same topic (“elections”). Moreover, the human often optimizes on conciseness by referring to entities with pronouns (e.g., “he”) or last names (e.g., “Ahern”) without losing clarity. We therefore propose a two-step neural abstractive summarization framework to emulate the way h"
D19-1323,N04-1015,0,0.187359,"t al., 2018; Tan et al., 2017b; Chen and Bansal, 2018) or phrase-level (Gehrmann et al., 2018), followed by abstract generation. However, prior work mainly focuses on improving the informativeness of abstractive summaries, e.g. copying and coverage mechanisms (See et al., 2017), and reinforcement learning methods optimizing on ROUGE scores (Paulus et al., 2017). Coherence and other aspects of linguistic quality that capture the overall readability of summaries are largely ignored. 3287 Grammar role-based entity transitions have been widely employed to model coherence in text generation tasks (Barzilay and Lee, 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017), which often requires intensive feature engineering. Neural coherence models (Mesgar and Strube, 2018; Li and Hovy, 2014) have, therefore, gained popularity due to their endto-end nature. However, coherence has mainly been investigated in extractive summarization systems (Alonso i Alemany and Fuentes Fort, 2003; Christensen et al., 2013; Parveen et al., 2015; Wu and Hu, 2018). To the best of our knowledge, we are the first to leverage entity information to improve coherence for neur"
D19-1323,N18-1150,0,0.469856,"red. Underlined sentence in the article occurs relatively at an earlier position in the summary ( 2 ) to improve topical coherence. Mentions in brackets (“[]”,“{}”) show different ways in which the same entity is referred to in the article and the summary. Detailed explanation is given in §1. Introduction Automatic abstractive summarization carries strong promise for producing concise and coherent summaries to facilitate quick information consumption (Luhn, 1958). Recent progress in neural abstractive summarization has shown endto-end trained models (Nallapati et al., 2016; Tan et al., 2017a; Celikyilmaz et al., 2018; Kry´sci´nski et al., 2018) excelling at producing fluent summaries. Though encouraging, their outputs are * These authors contributed equally. Work done while LH was at Northeastern University. frequently found to be unfaithful to the input and lack inter-sentence coherence (Cao et al., 2018; See et al., 2017; Wiseman et al., 2017). These observations suggest that existing methods have difficulty in identifying salient entities and related events in the article (Fan et al., 2018), and that existing model training objectives fail to guide the generation of coherent summaries. In this paper, w"
D19-1323,P18-1063,0,0.217282,". Additional evaluation for this model is reported in §4.1. Training Details and Parameters. We used a vocabulary of 50K most common words in the training set (See et al., 2017), with 128-dimensional word embeddings randomly initialized and updated during training. In the content selection component, for both entity and sentence encoders, we implemented one-layer convolutional network with 100 dimensions and used a shared embedding matrix between the two. We employed LSTM models with 256-dimensional hidden states for the input article encoder (per direction) and the content selection decoder (Chen and Bansal, 2018). We used a similar setup for the abstract generator encoder and decoder. During ML training of both components, Adam (Kingma and Ba, 2015) is applied with a learning rate 0.001 and a gradient clipping 2.0, and the batch size 32. During RL stage, we reduced learning rate to 0.0001 (Paulus et al., 2017) and set batch size to 50. For our abstract generator, to reduce variance during RL training, 3284 Baselines and Comparisons. Besides baseline L EAD -3, we compare with popular and existing state-of-the-art abstractive summarization models on NYT and CNN/DM datasets: (1) pointer-generator model w"
D19-1323,N13-1136,0,0.090926,"ty of summaries are largely ignored. 3287 Grammar role-based entity transitions have been widely employed to model coherence in text generation tasks (Barzilay and Lee, 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017), which often requires intensive feature engineering. Neural coherence models (Mesgar and Strube, 2018; Li and Hovy, 2014) have, therefore, gained popularity due to their endto-end nature. However, coherence has mainly been investigated in extractive summarization systems (Alonso i Alemany and Fuentes Fort, 2003; Christensen et al., 2013; Parveen et al., 2015; Wu and Hu, 2018). To the best of our knowledge, we are the first to leverage entity information to improve coherence for neural abstractive summarization along with other important linguistic qualities. Human: New Jersey Legislature recommends 0 ways to overhaul system that has produced highest property taxes in nation; plan includes 0 percent reduction in property taxes to most homeowners through direct tax credits; will place annual limit on property tax increases; will revise financing of education and end special financing of state’s poor districts PointGen+cov: new"
D19-1323,D18-1443,0,0.433038,"conducting cross-sentence information ordering, compression, and revision. Our abstract generator is trained using reinforcement learning with rewards that promote informativeness and optionally boost coherence, conciseness, and clarity of the summary. To the best of our knowledge, we are the first to study coherent abstractive summarization with the inclusion of linguisticallyinformed rewards. We conduct both automatic and human evaluation on popular news summarization datasets. Experimental results show that our model yields significantly better ROUGE scores than previous state-of-the-art (Gehrmann et al., 2018; Celikyilmaz et al., 2018) as well as higher coherence scores on the New York Times and CNN/Daily Mail datasets. Human subjects also rate our system generated summaries as more informative and coherent than those of other popular summarization models. 2 Summarization Framework In this section, we describe our entity-driven abstractive summarization framework which follows a two-step approach as shown in Fig. 2. It comprises of (1) an entity-aware content selection component, that leverages entity guidance to select salient sentences (§2.1), and (2) an abstract generation component (§2.2), tha"
D19-1323,N19-1348,0,0.0227223,"he new reward is written as R(y) = RRouge (y) + γRef RRef (y). Apposition. Next, we consider a reward to teach the model to use apposition and relative clause minimally, which improves summary conciseness. For this, we focus on the non-restrictive appositives and relative clauses, which often carry noncritical information (Conroy et al., 2006; Wang et al., 2013) and can be automatically detected based on comma usage patterns. Specifically, a sentence contains a non-restrictive appositive if i) it contains two commas, and ii) the word after first comma is a possessive pronoun or a determinant (Geva et al., 2019). We penalize a summary with −1 for using non-restrictive appositives and relative clauses, henceforth referred to as apposition, or give 0 otherwise. Similarly, we have the total reward as R(y) = RRouge (y) + γApp RApp (y). 2.4 Connecting Selection and Abstraction Our entity-aware content selection component extracts salient sentences whereas our abstract generation component compresses and paraphrases them. Until this point, they are trained separately without any form of parameter sharing. We add an additional step to connect these two networks by training them together via the self-critica"
D19-1323,N18-1065,0,0.0537365,"Missing"
D19-1323,P13-1010,0,0.0120813,"nn et al., 2018), followed by abstract generation. However, prior work mainly focuses on improving the informativeness of abstractive summaries, e.g. copying and coverage mechanisms (See et al., 2017), and reinforcement learning methods optimizing on ROUGE scores (Paulus et al., 2017). Coherence and other aspects of linguistic quality that capture the overall readability of summaries are largely ignored. 3287 Grammar role-based entity transitions have been widely employed to model coherence in text generation tasks (Barzilay and Lee, 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017), which often requires intensive feature engineering. Neural coherence models (Mesgar and Strube, 2018; Li and Hovy, 2014) have, therefore, gained popularity due to their endto-end nature. However, coherence has mainly been investigated in extractive summarization systems (Alonso i Alemany and Fuentes Fort, 2003; Christensen et al., 2013; Parveen et al., 2015; Wu and Hu, 2018). To the best of our knowledge, we are the first to leverage entity information to improve coherence for neural abstractive summarization along with other important linguistic qualities. Human"
D19-1323,P18-1013,0,0.0271938,". Though not significant, P OINT G EN + COV ranks higher on grammaticality than SENECA + RCoh . We believe this is due to the fact that SENECA + RCoh learns to merge sentences from the input article while making some grammatical errors. We further show sample summaries in Figure 5. 5 Related Work Neural Abstractive Summarization. Two-step abstractive summarization approaches have become popular in recent years, where the two steps, content selection and abstraction, are conveniently separated from each other. In these approaches, salient content is first identified, usually at sentence-level (Hsu et al., 2018; Li et al., 2018; Tan et al., 2017b; Chen and Bansal, 2018) or phrase-level (Gehrmann et al., 2018), followed by abstract generation. However, prior work mainly focuses on improving the informativeness of abstractive summaries, e.g. copying and coverage mechanisms (See et al., 2017), and reinforcement learning methods optimizing on ROUGE scores (Paulus et al., 2017). Coherence and other aspects of linguistic quality that capture the overall readability of summaries are largely ignored. 3287 Grammar role-based entity transitions have been widely employed to model coherence in text generation t"
D19-1323,D14-1181,0,0.00373697,"rs with entities appearing in the first three sentences of the article, and (2) top k clusters containing most numbers of mentions. We experimented with different values of k and found that k = 6 gives us the best set of salient mention clusters having an optimal overlap with entity mentions in the ground truth summary. For each mention cluster, we concatenate mentions of the same entity as they occur in the input into one sequence, segmented with special tokens (&lt;MENT&gt;). Finally, we get entity representations ei for the i-th entity by encoding each cluster via a temporal convolutional model (Kim, 2014). Input Article Encoder. For article encoding, we first learn sentence representations rj by encoding words in the j-th sentence with another temporal convolutional model. Then, we utilize a bidirectional LSTM (biLSTM) to aggregate sentences into a sequence of hidden states hj . Both the encoders use a shared word embedding matrix to allow better alignment. Sentence Selection Decoder. We employ a single-layer unidirectional LSTM with hidden states st to recurrently extract salient sentences. At each time step t, we first compute an entity context vector cet based on attention mechanism (Bahdan"
D19-1323,D18-1207,0,0.0845525,"Missing"
D19-1323,N18-2009,0,0.0753269,"Missing"
D19-1323,D14-1218,0,0.0390909,"d coverage mechanisms (See et al., 2017), and reinforcement learning methods optimizing on ROUGE scores (Paulus et al., 2017). Coherence and other aspects of linguistic quality that capture the overall readability of summaries are largely ignored. 3287 Grammar role-based entity transitions have been widely employed to model coherence in text generation tasks (Barzilay and Lee, 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017), which often requires intensive feature engineering. Neural coherence models (Mesgar and Strube, 2018; Li and Hovy, 2014) have, therefore, gained popularity due to their endto-end nature. However, coherence has mainly been investigated in extractive summarization systems (Alonso i Alemany and Fuentes Fort, 2003; Christensen et al., 2013; Parveen et al., 2015; Wu and Hu, 2018). To the best of our knowledge, we are the first to leverage entity information to improve coherence for neural abstractive summarization along with other important linguistic qualities. Human: New Jersey Legislature recommends 0 ways to overhaul system that has produced highest property taxes in nation; plan includes 0 percent reduction in"
D19-1323,P06-1047,0,0.0276916,"plan would place limit on annual property tax increases and revise way state pays for public education, ending special financing given to state’s poor districts Figure 5: Sample summaries for an NYT article. Our model with coherence reward overlaps the most with human summary (green is ours, blue denotes human). In this work, besides informativeness, we also aim to improve upon these aspects of summaries. Role of Entities and Coherence in Summarization. Entities in a text carry useful contextual information (Nenkova, 2008) and therefore play an important role in multi-document summarization (Li et al., 2006) and event summarization for selecting salient sentences (Li et al., 2015). Moreover, entity mentions connecting sentences have also been used to extract non-adjacent yet coherent sentences (Siddharthan et al., 2011; Parveen et al., 2016). For abstractive summarization, Amplayo et al. (2018) find it beneficial to leverage entities that are linked to existing knowledge bases. Unfortunately, it fails to capture the entities that do not exist in these knowledge bases. Acknowledgements This research is supported in part by National Science Foundation through Grants IIS-1566382 and IIS-1813341, and"
D19-1323,N03-1020,0,0.656309,"Missing"
D19-1323,D16-1230,0,0.0165924,"age of ROUGE-L F1 and ROUGE-2 F1 of the two summaries against that of the ground-truth summary, and define the following loss function: Lrl = − 1 N0 X (R(ys ) − R(ˆ y)) log p(ys |xext ; θ) (ys ,xext )∈D 0 (7) where D0 represents set of sampled summaries paired with extracted input sentences and N 0 represents the total number of sampled summaries.  R(y) = RRouge (y) = 21 RRouge−L (y) + RRouge−2 (y) , is the overall ROUGE reward for a summary y. 2.3 Rewards with Coherence and Linguistic Quality So far, we have described the two basic components of our SENECA framework. As noted in prior work (Liu et al., 2016), optimizing for an ngram-based metric like ROUGE does not guarantee improvement over readability of the generations. We thus augment our framework with additional rewards based on coherence and linguistic quality as described below. Entity-Based Coherence Reward (RCoh ). We use a separately trained coherence model to score summaries and guide our abstract generator to produce more coherent outputs by adding a reward RCoh in the aforementioned RL training process. The new reward takes the following form: R(y) = RRouge (y) + γCoh RCoh (y) (8) Here we show how to calculate RCoh , to capture both"
D19-1323,P14-5010,0,0.00786813,"re the interaction between entity mentions and the input article. Our model learns to identify salient content by aligning entity mentions 3281 and their contexts with human summaries. Concretely, we employ two encoders: one learns entity representations by encoding their mention clusters and the other learns sentence representations. A pointer-network-based decoder (Vinyals et al., 2015b) selects a sequence of important sentences by jointly attending to the entities and the input, as depicted in Fig. 3. Entity Encoder. We run off-the-shelf coreference resolution system from Stanford CoreNLP (Manning et al., 2014) on the input articles to extract entities, each represented as a cluster of mentions. Specifically, from each input article, we extract the coreferenced entities, and construct the mention clusters for all the mentions of each entity in that article. We also consider non coreferenced entity mentions as singleton entity mention clusters. Among all these mention clusters, for our experiments, we only consider salient entity mention clusters. We label clusters as “salient” based on two rules: (1) mention clusters with entities appearing in the first three sentences of the article, and (2) top k"
D19-1323,D18-1464,0,0.0310696,"ummaries, e.g. copying and coverage mechanisms (See et al., 2017), and reinforcement learning methods optimizing on ROUGE scores (Paulus et al., 2017). Coherence and other aspects of linguistic quality that capture the overall readability of summaries are largely ignored. 3287 Grammar role-based entity transitions have been widely employed to model coherence in text generation tasks (Barzilay and Lee, 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017), which often requires intensive feature engineering. Neural coherence models (Mesgar and Strube, 2018; Li and Hovy, 2014) have, therefore, gained popularity due to their endto-end nature. However, coherence has mainly been investigated in extractive summarization systems (Alonso i Alemany and Fuentes Fort, 2003; Christensen et al., 2013; Parveen et al., 2015; Wu and Hu, 2018). To the best of our knowledge, we are the first to leverage entity information to improve coherence for neural abstractive summarization along with other important linguistic qualities. Human: New Jersey Legislature recommends 0 ways to overhaul system that has produced highest property taxes in nation; plan includes 0 p"
D19-1323,K16-1028,0,0.0618404,"2008). Mentions of the same entity are colored. Underlined sentence in the article occurs relatively at an earlier position in the summary ( 2 ) to improve topical coherence. Mentions in brackets (“[]”,“{}”) show different ways in which the same entity is referred to in the article and the summary. Detailed explanation is given in §1. Introduction Automatic abstractive summarization carries strong promise for producing concise and coherent summaries to facilitate quick information consumption (Luhn, 1958). Recent progress in neural abstractive summarization has shown endto-end trained models (Nallapati et al., 2016; Tan et al., 2017a; Celikyilmaz et al., 2018; Kry´sci´nski et al., 2018) excelling at producing fluent summaries. Though encouraging, their outputs are * These authors contributed equally. Work done while LH was at Northeastern University. frequently found to be unfaithful to the input and lack inter-sentence coherence (Cao et al., 2018; See et al., 2017; Wiseman et al., 2017). These observations suggest that existing methods have difficulty in identifying salient entities and related events in the article (Fan et al., 2018), and that existing model training objectives fail to guide the gener"
D19-1323,I08-1016,0,0.190652,"nt selector extracts salient sentences and abstract generator produces informative and coherent summaries. Both components are connected using reinforcement learning. enables enhanced input text interpretation, salient content selection, and coherent summary generation, three major challenges that need to be addressed by single-document summarization systems (Jones et al., 1999). We use a sample summary in Fig. 1 to show entity usage in summarization. Firstly, frequently mentioned entities from the input, along with their contextual information, underscores the salient content of the article (Nenkova, 2008). Secondly, as also discussed in prior work (Barzilay and Lapata, 2008; Siddharthan et al., 2011), patterns of entity distributions and how they are referred to contribute to the coherence and conciseness of the text. For instance, a human writer places the underlined sentence in the input article next to the first sentence in the summary to improve topical coherence as they are about the same topic (“elections”). Moreover, the human often optimizes on conciseness by referring to entities with pronouns (e.g., “he”) or last names (e.g., “Ahern”) without losing clarity. We therefore propose a tw"
D19-1323,D16-1074,0,0.020489,"reward overlaps the most with human summary (green is ours, blue denotes human). In this work, besides informativeness, we also aim to improve upon these aspects of summaries. Role of Entities and Coherence in Summarization. Entities in a text carry useful contextual information (Nenkova, 2008) and therefore play an important role in multi-document summarization (Li et al., 2006) and event summarization for selecting salient sentences (Li et al., 2015). Moreover, entity mentions connecting sentences have also been used to extract non-adjacent yet coherent sentences (Siddharthan et al., 2011; Parveen et al., 2016). For abstractive summarization, Amplayo et al. (2018) find it beneficial to leverage entities that are linked to existing knowledge bases. Unfortunately, it fails to capture the entities that do not exist in these knowledge bases. Acknowledgements This research is supported in part by National Science Foundation through Grants IIS-1566382 and IIS-1813341, and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA8650-17-C-9116. The views and conclusions contained herein are those of the authors and should not"
D19-1323,D15-1226,0,0.0267982,"y ignored. 3287 Grammar role-based entity transitions have been widely employed to model coherence in text generation tasks (Barzilay and Lee, 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017), which often requires intensive feature engineering. Neural coherence models (Mesgar and Strube, 2018; Li and Hovy, 2014) have, therefore, gained popularity due to their endto-end nature. However, coherence has mainly been investigated in extractive summarization systems (Alonso i Alemany and Fuentes Fort, 2003; Christensen et al., 2013; Parveen et al., 2015; Wu and Hu, 2018). To the best of our knowledge, we are the first to leverage entity information to improve coherence for neural abstractive summarization along with other important linguistic qualities. Human: New Jersey Legislature recommends 0 ways to overhaul system that has produced highest property taxes in nation; plan includes 0 percent reduction in property taxes to most homeowners through direct tax credits; will place annual limit on property tax increases; will revise financing of education and end special financing of state’s poor districts PointGen+cov: new jersey legislature, a"
D19-1323,P17-1099,0,0.788521,"tractive summarization carries strong promise for producing concise and coherent summaries to facilitate quick information consumption (Luhn, 1958). Recent progress in neural abstractive summarization has shown endto-end trained models (Nallapati et al., 2016; Tan et al., 2017a; Celikyilmaz et al., 2018; Kry´sci´nski et al., 2018) excelling at producing fluent summaries. Though encouraging, their outputs are * These authors contributed equally. Work done while LH was at Northeastern University. frequently found to be unfaithful to the input and lack inter-sentence coherence (Cao et al., 2018; See et al., 2017; Wiseman et al., 2017). These observations suggest that existing methods have difficulty in identifying salient entities and related events in the article (Fan et al., 2018), and that existing model training objectives fail to guide the generation of coherent summaries. In this paper, we present SENECA, a System for ENtity-drivEn Coherent Abstractive summarization.1 We argue that entity-based modeling 1 Our code is available at evasharma.github.io/SENECA. 3280 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on N"
D19-1323,J11-4007,0,0.137728,"coherent summaries. Both components are connected using reinforcement learning. enables enhanced input text interpretation, salient content selection, and coherent summary generation, three major challenges that need to be addressed by single-document summarization systems (Jones et al., 1999). We use a sample summary in Fig. 1 to show entity usage in summarization. Firstly, frequently mentioned entities from the input, along with their contextual information, underscores the salient content of the article (Nenkova, 2008). Secondly, as also discussed in prior work (Barzilay and Lapata, 2008; Siddharthan et al., 2011), patterns of entity distributions and how they are referred to contribute to the coherence and conciseness of the text. For instance, a human writer places the underlined sentence in the input article next to the first sentence in the summary to improve topical coherence as they are about the same topic (“elections”). Moreover, the human often optimizes on conciseness by referring to entities with pronouns (e.g., “he”) or last names (e.g., “Ahern”) without losing clarity. We therefore propose a two-step neural abstractive summarization framework to emulate the way humans construct summaries w"
D19-1323,P17-1108,0,0.0568329,"Missing"
D19-1323,P17-1121,0,0.0264793,"ract generation. However, prior work mainly focuses on improving the informativeness of abstractive summaries, e.g. copying and coverage mechanisms (See et al., 2017), and reinforcement learning methods optimizing on ROUGE scores (Paulus et al., 2017). Coherence and other aspects of linguistic quality that capture the overall readability of summaries are largely ignored. 3287 Grammar role-based entity transitions have been widely employed to model coherence in text generation tasks (Barzilay and Lee, 2004; Lapata and Barzilay, 2005; Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017), which often requires intensive feature engineering. Neural coherence models (Mesgar and Strube, 2018; Li and Hovy, 2014) have, therefore, gained popularity due to their endto-end nature. However, coherence has mainly been investigated in extractive summarization systems (Alonso i Alemany and Fuentes Fort, 2003; Christensen et al., 2013; Parveen et al., 2015; Wu and Hu, 2018). To the best of our knowledge, we are the first to leverage entity information to improve coherence for neural abstractive summarization along with other important linguistic qualities. Human: New Jersey Legislature reco"
D19-1323,P13-1136,1,0.814974,"e RL step, we either penalize a summary with a reward of −1 for such improper usage, or give 0 otherwise. In our implementation, we define improper usage as the presence of a third personal pronoun or a possessive pronoun before any noun phrase occurs. The new reward is written as R(y) = RRouge (y) + γRef RRef (y). Apposition. Next, we consider a reward to teach the model to use apposition and relative clause minimally, which improves summary conciseness. For this, we focus on the non-restrictive appositives and relative clauses, which often carry noncritical information (Conroy et al., 2006; Wang et al., 2013) and can be automatically detected based on comma usage patterns. Specifically, a sentence contains a non-restrictive appositive if i) it contains two commas, and ii) the word after first comma is a possessive pronoun or a determinant (Geva et al., 2019). We penalize a summary with −1 for using non-restrictive appositives and relative clauses, henceforth referred to as apposition, or give 0 otherwise. Similarly, we have the total reward as R(y) = RRouge (y) + γApp RApp (y). 2.4 Connecting Selection and Abstraction Our entity-aware content selection component extracts salient sentences whereas"
D19-1323,P18-1061,0,0.0265854,"y is selected. The process stops when the model picks the end-of-selection token. Selection Label Construction. We train our content P selection component with a cross-entropy loss: − (yl ,x)∈D log p(yl |x; θ), here yl are the ground truth sentence selection labels and x is the input article. θ denotes all model parameters. To acquire training labels for sentence selection, we collect positive sentences in the following way. First, we employ greedy search to select the best combination of sentences that maximizes ROUGE-2 F1 (Lin and Hovy, 2003) with reference to human summary, as described by Zhou et al. (2018). We further include sentences whose ROUGE-L recall is above 0.5 when each is compared with its best aligned summary sentence. In cases where no sentence is selected, we label the first two sentences from the article as positive. Our combined construction strategy selects an average of 2.96 and 3.18 sentences from New York Times and CNN/Daily Mail articles respectively. 2.2 Abstract Generation with Reinforcement Learning Our abstract generation component takes the selected sentences as input and produces the final summary. This abstract generator is a sequence-to-sequence network with attentio"
D19-1470,N12-1074,0,0.548573,"k Our work is in line with conversation behavior analysis, where studies explore user interactions in ongoing conversations (Ritter et al., 2010) and how they signal the conversations’ future trajectory, such as continued activity (Backstrom et al., 2013; Jiao et al., 2018; Zeng et al., 2019) and the risk of going awry (Zhang et al., 2018). Different from these proposals which do not model personal interests, we study conversation recommendation for a specific user, where we measure how a user’s preferences match a conversation’s context. This work is also related to user response prediction (Artzi et al., 2012; Zhang et al., 2015) and post recommendation (Duan et al., 2010; Chen et al., 2012; Yan et al., 2012; Hong et al., 2013). While most of these studies focus on post modeling, we examine conversation context to predict user engagements, which goes beyond the postlevel prediction task. Other prior work examining conversation-level recommendation relies on either manual features (Chen et al., 2011) or shallow word occurrence patterns (Zeng et al., 2018), largely ignoring the useful features from historical user interactions. On the contrary, we utilize online user interactions in the conversation"
D19-1470,P18-1026,0,0.236442,"ded in their conversation interactions. To the best of our knowledge, this is the first work to explore neural conversation recommendation with online interactions explicitly encoded for user preference modeling. To evaluate our model, we conduct extensive experiments on two large-scale datasets with online conversations from Twitter and Reddit1 . Experimental results show that our method significantly outperforms state-of-the-art models that do not capture user interactions. For example, our model obtains an MAP (Mean Average Precision) of 0.625 on Twitter, compared with 0.591 by Zeng et al. (2018). We further find that our model still exhibits superior performance when the sparsity levels of user history and conversation context are varied, demonstrating our model’s potential ability to handle sparse conversation records. Additional experiments on an ablation study confirms the effectiveness of different components in our framework. A case study further reveals important interaction features captured by our model, which indicate their conversation entries and hence explain our model’s advanced performance. Finally, we 1 The datasets and codes are available at: https:// github.com/zxsha"
D19-1470,D14-1181,0,0.00330313,"/0 9:; 4/72 4/76 4/78, &lt;=&gt; 6 Conversation Interaction Modeling time (a) Graph-State LSTM &1&apos; /0 &1&apos; /2 LSTM Turn Interaction Modeling. To encode conversation interaction structure, we first organize the turns in a conversation c as a reply tree to formulate who replies to whom. Each node therein 4/2 ReLU &1&apos; /5 LSTM Wsuc LSTM (BiLSTM layer) Turn-level Modeling. Here we describe how we model turn-level representations, which combine what content it conveys and who its author is. Content representation is to reflect how words appear therein, where we employ a Convolutional Neural Network (CNN) (Kim, 2014) encoder to model a turn’s word sequence. Specifically, given a turn t in conversation c, we first map each word in t into a word embedding layer (initialized with pre-trained word vectors) to explore deep word semantics. And then, to capture how a word appears in local context with its neighbors, a CNN encoder is exploited to generate the turn-level content representation zt . Next, we concatenate zt , conveying content features, and ruCI , embedded with the interaction patt terns of t’s author ut , to produce a turn representation rtT R . It couples turn t’s word occurrence patterns and its"
D19-1470,C10-1034,0,0.209152,"studies explore user interactions in ongoing conversations (Ritter et al., 2010) and how they signal the conversations’ future trajectory, such as continued activity (Backstrom et al., 2013; Jiao et al., 2018; Zeng et al., 2019) and the risk of going awry (Zhang et al., 2018). Different from these proposals which do not model personal interests, we study conversation recommendation for a specific user, where we measure how a user’s preferences match a conversation’s context. This work is also related to user response prediction (Artzi et al., 2012; Zhang et al., 2015) and post recommendation (Duan et al., 2010; Chen et al., 2012; Yan et al., 2012; Hong et al., 2013). While most of these studies focus on post modeling, we examine conversation context to predict user engagements, which goes beyond the postlevel prediction task. Other prior work examining conversation-level recommendation relies on either manual features (Chen et al., 2011) or shallow word occurrence patterns (Zeng et al., 2018), largely ignoring the useful features from historical user interactions. On the contrary, we utilize online user interactions in the conversation history, to allow the inclusion of richer information of modeli"
D19-1470,D15-1259,1,0.899584,"Missing"
D19-1470,P16-1199,1,0.909654,"Missing"
D19-1470,J18-4008,1,0.830471,"Missing"
D19-1470,W02-0109,0,0.181076,"ter dataset were mainly posted from Jan 23 to Feb 8, 2011, and discussion threads in Reddit dataset were posted from Jan to Dec, 2008. To discover the whole conversations, we retrieved all messages with replying relations (indicated by “parent id” property in Reddit corpus, for example), and recorded their authors and parent messages. Finally, conversations with only one message were removed. We applied the Glove tweet preprocessing toolkit (Pennington et al., 2014)3 on the Twitter dataset. As for the Reddit dataset, we performed tokenization using open source natural language toolkit (NLTK) (Loper and Bird, 2002), with links replaced to a generic tag “URL” and all number tokens removed. We maintained a vocabulary with all the rest characters appearing in the corpus for both datasets, including punctuation and emoticons. Data Statistics and Analysis. The statistics of two datasets are shown in Table 1, with more information in Figure 4. We can observe that Reddit dataset contains more conversations, with a higher average number of conversations per user. On the other hand, Twitter conversations are longer, with fewer participants. Figure 4(a) shows that most users participate in very few conversations"
D19-1470,D17-1159,0,0.360881,"o, a reply tree is extended to a directed graph (such as the one in Figure 1), with both replying and temporal interactions encoded and therefore named as an interaction graph. For each turn t on the graph, we distinguish its neighbors into predecessors, denoted by E p (t), and successors, E s (t). Then, we employ graph-structured networks to model the interaction structure. There are two modeling methods discussed here: Graph-State LSTM (Long Short-Term Memory) (henceforth GLSTM) (Beck et al., 2018; Song et al., 2018) and Graph Convolutional Networks (henceforth GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017), whose empirical effectiveness will be compared in Section 5.1. Here we present their architecture in Figure 3 and describe how they model conversation interactions below. Graph-State LSTM. We start with GLSTM and show its architecture in Figure 3(a). It is an extension of LSTM from sequence to graph structure, where a turn’s hidden states are updated conditioned on both the turn-level representation rtT R 4636 and the states of all its neighbors on the graph. The update strategy is the same as standard LSTM (Hochreiter and Schmidhuber, 1997), except for the following formula, which can be us"
D19-1470,C18-1322,0,0.0823794,"Missing"
D19-1470,P18-1125,0,0.0219937,"we 1 The datasets and codes are available at: https:// github.com/zxshamson/neural-conv-rec investigate the challenging task of first time replies prediction, where our model again produces significantly better results than existing popular recommendation models. 2 Related Work Our work is in line with conversation behavior analysis, where studies explore user interactions in ongoing conversations (Ritter et al., 2010) and how they signal the conversations’ future trajectory, such as continued activity (Backstrom et al., 2013; Jiao et al., 2018; Zeng et al., 2019) and the risk of going awry (Zhang et al., 2018). Different from these proposals which do not model personal interests, we study conversation recommendation for a specific user, where we measure how a user’s preferences match a conversation’s context. This work is also related to user response prediction (Artzi et al., 2012; Zhang et al., 2015) and post recommendation (Duan et al., 2010; Chen et al., 2012; Yan et al., 2012; Hong et al., 2013). While most of these studies focus on post modeling, we examine conversation context to predict user engagements, which goes beyond the postlevel prediction task. Other prior work examining conversatio"
D19-1470,D14-1162,0,0.0834365,"er is from Zeng et al. (2019), which is comprised of discussion threads about political issues on Reddit, a popular discussion website. The tweets in Twitter dataset were mainly posted from Jan 23 to Feb 8, 2011, and discussion threads in Reddit dataset were posted from Jan to Dec, 2008. To discover the whole conversations, we retrieved all messages with replying relations (indicated by “parent id” property in Reddit corpus, for example), and recorded their authors and parent messages. Finally, conversations with only one message were removed. We applied the Glove tweet preprocessing toolkit (Pennington et al., 2014)3 on the Twitter dataset. As for the Reddit dataset, we performed tokenization using open source natural language toolkit (NLTK) (Loper and Bird, 2002), with links replaced to a generic tag “URL” and all number tokens removed. We maintained a vocabulary with all the rest characters appearing in the corpus for both datasets, including punctuation and emoticons. Data Statistics and Analysis. The statistics of two datasets are shown in Table 1, with more information in Figure 4. We can observe that Reddit dataset contains more conversations, with a higher average number of conversations per user."
D19-1470,N10-1020,0,0.152077,"framework. A case study further reveals important interaction features captured by our model, which indicate their conversation entries and hence explain our model’s advanced performance. Finally, we 1 The datasets and codes are available at: https:// github.com/zxshamson/neural-conv-rec investigate the challenging task of first time replies prediction, where our model again produces significantly better results than existing popular recommendation models. 2 Related Work Our work is in line with conversation behavior analysis, where studies explore user interactions in ongoing conversations (Ritter et al., 2010) and how they signal the conversations’ future trajectory, such as continued activity (Backstrom et al., 2013; Jiao et al., 2018; Zeng et al., 2019) and the risk of going awry (Zhang et al., 2018). Different from these proposals which do not model personal interests, we study conversation recommendation for a specific user, where we measure how a user’s preferences match a conversation’s context. This work is also related to user response prediction (Artzi et al., 2012; Zhang et al., 2015) and post recommendation (Duan et al., 2010; Chen et al., 2012; Yan et al., 2012; Hong et al., 2013). Whil"
D19-1470,P18-1150,0,0.118242,"ind of edges to indicate chronological order (such as the blue arrows in Figure 1). In doing so, a reply tree is extended to a directed graph (such as the one in Figure 1), with both replying and temporal interactions encoded and therefore named as an interaction graph. For each turn t on the graph, we distinguish its neighbors into predecessors, denoted by E p (t), and successors, E s (t). Then, we employ graph-structured networks to model the interaction structure. There are two modeling methods discussed here: Graph-State LSTM (Long Short-Term Memory) (henceforth GLSTM) (Beck et al., 2018; Song et al., 2018) and Graph Convolutional Networks (henceforth GCN) (Kipf and Welling, 2017; Marcheggiani and Titov, 2017), whose empirical effectiveness will be compared in Section 5.1. Here we present their architecture in Figure 3 and describe how they model conversation interactions below. Graph-State LSTM. We start with GLSTM and show its architecture in Figure 3(a). It is an extension of LSTM from sequence to graph structure, where a turn’s hidden states are updated conditioned on both the turn-level representation rtT R 4636 and the states of all its neighbors on the graph. The update strategy is the sa"
D19-1470,P12-1054,0,0.783176,"interest. To address this issue, we study the problem of online conversation recommendation, with the goal of identifying conversations that fit a user’s preferences, hence likely to result in the user’s future engagement. ∗ This work was mainly conducted when Jing Li was affiliated with Tencent AI Lab, Shenzhen, China. In previous studies, it has been shown that effective online conversation recommendation has the potential to produce more positive online social interaction experience (Chen et al., 2011; Zeng et al., 2018). Prior work on this subject has focused on post-level recommendation (Yan et al., 2012; Chen et al., 2012), or conversation-level suggestion with handcrafted features (Chen et al., 2011) and word co-occurrence patterns (Zeng et al., 2018). Nevertheless, they ignore the useful information embedded in replying relations, where the conversation structure is formed via messages sent among users. In this work, we examine conversation context, and model the participants’ interactions therein. This approach enables deep representation learning that reflects personal interests and conversation preferences, together signaling what conversations a user is likely to be involved in. To ill"
D19-1470,Q18-1009,0,0.0993896,"Missing"
D19-1470,N18-1035,1,0.666232,"em. It is hence difficult for one to discover online discussions that are potentially of interest. To address this issue, we study the problem of online conversation recommendation, with the goal of identifying conversations that fit a user’s preferences, hence likely to result in the user’s future engagement. ∗ This work was mainly conducted when Jing Li was affiliated with Tencent AI Lab, Shenzhen, China. In previous studies, it has been shown that effective online conversation recommendation has the potential to produce more positive online social interaction experience (Chen et al., 2011; Zeng et al., 2018). Prior work on this subject has focused on post-level recommendation (Yan et al., 2012; Chen et al., 2012), or conversation-level suggestion with handcrafted features (Chen et al., 2011) and word co-occurrence patterns (Zeng et al., 2018). Nevertheless, they ignore the useful information embedded in replying relations, where the conversation structure is formed via messages sent among users. In this work, we examine conversation context, and model the participants’ interactions therein. This approach enables deep representation learning that reflects personal interests and conversation prefer"
D19-1470,P19-1270,1,0.88667,"lain our model’s advanced performance. Finally, we 1 The datasets and codes are available at: https:// github.com/zxshamson/neural-conv-rec investigate the challenging task of first time replies prediction, where our model again produces significantly better results than existing popular recommendation models. 2 Related Work Our work is in line with conversation behavior analysis, where studies explore user interactions in ongoing conversations (Ritter et al., 2010) and how they signal the conversations’ future trajectory, such as continued activity (Backstrom et al., 2013; Jiao et al., 2018; Zeng et al., 2019) and the risk of going awry (Zhang et al., 2018). Different from these proposals which do not model personal interests, we study conversation recommendation for a specific user, where we measure how a user’s preferences match a conversation’s context. This work is also related to user response prediction (Artzi et al., 2012; Zhang et al., 2015) and post recommendation (Duan et al., 2010; Chen et al., 2012; Yan et al., 2012; Hong et al., 2013). While most of these studies focus on post modeling, we examine conversation context to predict user engagements, which goes beyond the postlevel predict"
D19-1664,P15-2072,0,0.124163,"0), and verb transitivity (Greene and Resnik, 2009). However, such studies fail to take into consideration biases that depend on a larger context, which is what we try to address in this work. Our work is also in line with framing analysis in social science theory, or the concept of selecting and signifying specific aspects of an event to promote a particular interpretation (Entman, 1993). In fact, informational bias can be considered a specific form of framing where the author intends to influence the reader’s opinion of an entity. The relationship between framing and news is investigated by Card et al. (2015), in which news articles are annotated with framing dimensions like “legality” and “public opinion.” BASIL contains richer information that allows us to study the purpose of “frames,” i.e., how biased content is invoked to support or oppose the issue at hand. Research in political science has also studied bias induced by the inclusion or omission of certain facts (Entman, 2007; Gentzkow and Shapiro, 2006, 2010; Prat and Str¨omberg, 2013). However, their definition of bias is typically grounded in how a reader perceives the ideological leaning of the article and news outlet, whereas our informa"
D19-1664,D14-1125,0,0.057938,"Missing"
D19-1664,N19-1423,0,0.136633,"l Level), of 300 news articles with lexical and informational bias spans. To examine how media sources encode bias differently, the dataset uses 100 triplets of articles, each reporting the same event from three outlets of different ideology. Based on our annotations, we find that all three sources use more informational bias than lexical bias, and informational bias is embedded uniformly across the entire article, while lexical bias is frequently observed at the beginning. We further explore the challenges in bias detection and benchmark BASIL using rule-based classifiers and the BERT model (Devlin et al., 2019) fine-tuned on our data. Results show that identifying informational bias poses additional difficulty and suggest future directions of encoding contextual knowledge from the full articles as well as reporting by other media. 2 Related Work Prior work on automatic bias detection based on natural language processing methods primarily deals with finding sentence-level bias and considers linguistic attributes like word polarity (Recasens et al., 2013), partisan phrases (Yano et al., 2010), and verb transitivity (Greene and Resnik, 2009). However, such studies fail to take into consideration biases"
D19-1664,N09-1057,0,0.516464,"term ∗ Equal contribution. Lisa Fan focused on annotation schema design and writing, Marshall White focused on data collection and statistical analysis. 1 Dataset can be found at www.ccs.neu.edu/home/ luwang/data.html. Figure 1: Examples of negative bias from Huffington Post (HPO), Fox News (FOX), and New York Times (NYT) discussing the same event. Informational bias and lexical bias are highlighted. The target of the bias is noted at the end of each span. Intermediary targets of indirect bias spans are underlined. lexical bias: bias stemming from content realization, or how things are said (Greene and Resnik, 2009; Hube and Fetahu, 2019; Iyyer et al., 2014; Recasens et al., 2013; Yano et al., 2010). Such forms of bias typically do not depend on context outside of the sentence and can be alleviated while maintaining its semantics: polarized words can be removed or replaced, and clauses written in active voice can be rewritten in passive voice. However, political science researchers find that news bias can also be characterized by decisions 6343 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing,"
D19-1664,P14-1105,0,0.354244,"annotation schema design and writing, Marshall White focused on data collection and statistical analysis. 1 Dataset can be found at www.ccs.neu.edu/home/ luwang/data.html. Figure 1: Examples of negative bias from Huffington Post (HPO), Fox News (FOX), and New York Times (NYT) discussing the same event. Informational bias and lexical bias are highlighted. The target of the bias is noted at the end of each span. Intermediary targets of indirect bias spans are underlined. lexical bias: bias stemming from content realization, or how things are said (Greene and Resnik, 2009; Hube and Fetahu, 2019; Iyyer et al., 2014; Recasens et al., 2013; Yano et al., 2010). Such forms of bias typically do not depend on context outside of the sentence and can be alleviated while maintaining its semantics: polarized words can be removed or replaced, and clauses written in active voice can be rewritten in passive voice. However, political science researchers find that news bias can also be characterized by decisions 6343 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6343–6349, c Hong Kong, China, Novem"
D19-1664,P13-1162,0,0.281463,"sign and writing, Marshall White focused on data collection and statistical analysis. 1 Dataset can be found at www.ccs.neu.edu/home/ luwang/data.html. Figure 1: Examples of negative bias from Huffington Post (HPO), Fox News (FOX), and New York Times (NYT) discussing the same event. Informational bias and lexical bias are highlighted. The target of the bias is noted at the end of each span. Intermediary targets of indirect bias spans are underlined. lexical bias: bias stemming from content realization, or how things are said (Greene and Resnik, 2009; Hube and Fetahu, 2019; Iyyer et al., 2014; Recasens et al., 2013; Yano et al., 2010). Such forms of bias typically do not depend on context outside of the sentence and can be alleviated while maintaining its semantics: polarized words can be removed or replaced, and clauses written in active voice can be rewritten in passive voice. However, political science researchers find that news bias can also be characterized by decisions 6343 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 6343–6349, c Hong Kong, China, November 3–7, 2019. 2019 Ass"
D19-1664,P10-1059,0,0.0333047,"or indirect), and whether the bias is part of a quote. Bias aim investigates the case where the main entity is indirectly targeted through an intermediary figure (see the HPO example in Figure 1, where the sentiment towards the intermediary entity “Trump Administration” is transferred to the main target, “Donald Trump”). Statistics are presented in Table 1. Inter-annotator Agreement (IAA). Two annotators individually annotate each article triplet before discussing their annotations together to resolve conflicts and agree on “gold-standard” labels. We measure span-level agreement according to Toprak et al. (2010), where we calculate the F1 score of span overlaps between two sets of annotations (details are in the Supplementary). Although the F1 scores of IAA are unsurprisingly low for this highly variable task, the score dramatically in2 The likely effect of annotators’ prior beliefs on their perception of bias will be investigated in future work. creases when agreement is calculated between individual annotations and the gold standard—from 0.34 to 0.70 for informational bias spans and from 0.14 to 0.56 for the sparser lexical spans, demonstrating the effectiveness of resolution discussions. During th"
D19-1664,H05-1044,0,0.0128664,"le with the lowest average TF-IDF token scores as containing 4 BERT’s maximum input length is 512 tokens, which is shorter than most articles in BASIL. We thus treat sentences as passages, rather than using text of fixed length. 5 BASIL averages 4.1 informational bias spans per article. Token-level Classifier. From Table 2, we see that the BERT lexical sequence tagger produces better recall and F1 than the informational tagger, highlighting the additional difficulty of accurately identifying spans of informational bias. We also use the polarity and subjectivity lexicons from the MPQA website (Wilson et al., 2005; Choi and Wiebe, 2014) as a simple baseline for lexical bias tagging and find that these word-level cues, though widely used in prior sentiment analysis studies, are insufficient to fully capture lexical bias. In order to evaluate token-level prediction on the larger original test set, we conduct a pipeline experiment with the fine-tuned BERT models where sentences predicted as containing bias by the best sentence-level classifier from cross validation are tagged by the best token-level model. The results reaffirm our hypothesis that while both tasks are extremely difficult, informational bia"
D19-1664,W10-0723,0,0.028067,"Missing"
N15-1112,chang-manning-2012-sutime,0,0.0218669,"r CNN articles via Disqus API. 1 NYT comments come with information on whether a comment is an editor’s-pick. The statistics on the four datasets are displayed in Table 1.2 MH370 Ukraine Israel-Gaza NSA Time Span # Articles # Comments 03/08 - 06/30 955 406,646 03/08 - 06/30 3,779 646,961 07/20 - 09/30 909 322,244 03/23 - 06/30 145 60,481 Table 1: Statistics on the four event datasets. We extract parse trees, dependency trees, and coreference resolution results of articles and comments with Stanford CoreNLP (Manning et al., 2014). Sentences in articles are labeled with timestamps using SUTime (Chang and Manning, 2012). We also collect all articles with comments from NYT in 2013 (henceforth NYT2013) to form a training set for learning importance scoring functions on articles sentences and comments (see Section 3). NYT2013 contains 3, 863 articles and 833, 032 comments. 3 Joint Learning for Importance Scoring We first introduce a joint learning method that uses graph-based regularization to simultaneously learn two functions — a SENTENCE scorer and a COM MENT scorer — that predict the importance of including an individual news article sentence or a particular user comment in the timeline. We train the model"
N15-1112,D11-1142,0,0.0175838,"es. For instance, the following thread connects events about Obama’s action towards the annexation of Crimea by Russia: Day 1: Day 2: Day 3: Day 4: Obama declared sanctions on Russian officials. President Obama warned Russian. Obama urges Russian to move back its troops. Obama condemns Russian aggression in Ukraine. We first collect relation extractions as (entity, relation, entity) triples from OLLIE (Mausam et al., 2012), a dependency relation based open information extraction system. We retain extractions with confidence scores higher than 0.5. We further design syntactic patterns based on Fader et al. (2011) to identify relations expressed as a combination of a verb and nouns. Each relation contains at least one event-related word (Ritter et al., 2012). The entity-centered event threading algorithm works as follows: on the first day, each sentence in the summary becomes an individual cluster; thereafter, each sentence in the current day’s article summary either gets attached to an existing thread or starts a new thread. The updated threads then become the input to next day’s summary generation process. On day n, we have a set of threads T = {τ : s1 , s2 , · · · , sn−1 } constructed from previous"
N15-1112,P11-1052,0,0.108666,"mi(·, ·) is a word similarity function. We experiment with shortest path based similarity defined on WordNet (Miller, 1995) and Cosine similarity with word vectors trained on Google news (Mikolov et al., 2013). Systems using the three metrics that optimize Z(S, C; T ) are henceforth called T HREAD +O PTTFIDF , T HREAD +O PTWordNet and T HREAD +O PTWordVec . 4.4 An Alternating Optimization Algorithm To maximize the full objective function Z(S, C; T ), we design a novel alternating optimization algorithm (Alg. 1) where we alternately find better S and C . We initialize S0 by a greedy algorithm (Lin and Bilmes, 2011) with respect to Squal (S; T ). Notice that Squal (S; T ) is a submodular function, so that the greedy solution is a 1 − 1/e approximation to the optimal solution of Squal (S; T ). Fixing S0 , we model the problem of finding C0 that maximizes Cqual (C) + δX (S0 , C) as a maximum-weight bipar1060 tite graph matching problem. This problem can be reduced to a maximum network flow problem, and then be solved by Ford-Fulkerson algorithm (details are discussed in (Kleinberg and Tardos, 2005)). Thereafter, for each iteration, we alternately find a better St with regard to Squal (S; T ) + δX (S, Ct−1"
N15-1112,N03-1020,0,0.146307,"mmaries with no comment information by optimizing Squal (S; T ) using a greedy algorithm: BASIC ignores event threading; T HREAD considers the threads. T HREAD +O PTTFIDF , T HREAD +O PTWordNet and T HREAD +O PTWordVec (see Section 4.3) leverage user comments to generate article summaries as well as comment summaries based on alternating optimization of Equation 3. Although comment summaries are generated, they are not used in the evaluation. For all systems, we generate daily article summaries of at most 100 words, and select 5 comments for the corresponding comment summary. We employ ROUGE (Lin and Hovy, 2003) to automatically evaluate the content coverage (in terms of ngrams) of the article-based timelines vs. goldstandard timelines. ROUGE-2 (measures bigram overlap) and ROUGE-SU4 (measures unigram and skip-bigrams separated by up to four words) scores are reported in Table 4. As can be seen, under the alternating optimization framework, our systems, employing both articles and comments, consistently yield better ROUGE scores than the three baseline systems and our systems that do not leverage comments. Though constructed from single-article abstracts, baseline A BSTRACT is found to contain redund"
N15-1112,P14-5010,0,0.0060203,"ticles. We collect comments for NYT articles through NYT community API, and comments for CNN articles via Disqus API. 1 NYT comments come with information on whether a comment is an editor’s-pick. The statistics on the four datasets are displayed in Table 1.2 MH370 Ukraine Israel-Gaza NSA Time Span # Articles # Comments 03/08 - 06/30 955 406,646 03/08 - 06/30 3,779 646,961 07/20 - 09/30 909 322,244 03/23 - 06/30 145 60,481 Table 1: Statistics on the four event datasets. We extract parse trees, dependency trees, and coreference resolution results of articles and comments with Stanford CoreNLP (Manning et al., 2014). Sentences in articles are labeled with timestamps using SUTime (Chang and Manning, 2012). We also collect all articles with comments from NYT in 2013 (henceforth NYT2013) to form a training set for learning importance scoring functions on articles sentences and comments (see Section 3). NYT2013 contains 3, 863 articles and 833, 032 comments. 3 Joint Learning for Importance Scoring We first introduce a joint learning method that uses graph-based regularization to simultaneously learn two functions — a SENTENCE scorer and a COM MENT scorer — that predict the importance of including an individu"
N15-1112,D12-1048,0,0.0132997,", followed by X (S, C) in Section 4.3. 4.1 Entity-Centered Event Threading We present an event threading process where each thread connects sequential events centered on a set of relevant entities. For instance, the following thread connects events about Obama’s action towards the annexation of Crimea by Russia: Day 1: Day 2: Day 3: Day 4: Obama declared sanctions on Russian officials. President Obama warned Russian. Obama urges Russian to move back its troops. Obama condemns Russian aggression in Ukraine. We first collect relation extractions as (entity, relation, entity) triples from OLLIE (Mausam et al., 2012), a dependency relation based open information extraction system. We retain extractions with confidence scores higher than 0.5. We further design syntactic patterns based on Fader et al. (2011) to identify relations expressed as a combination of a verb and nouns. Each relation contains at least one event-related word (Ritter et al., 2012). The entity-centered event threading algorithm works as follows: on the first day, each sentence in the summary becomes an individual cluster; thereafter, each sentence in the current day’s article summary either gets attached to an existing thread or starts"
N15-1112,C14-1157,1,0.861638,"Missing"
N15-1112,H05-1044,0,0.0144777,"or sentence importance scoring. Basic Features Readability Features - num of words - Flesch-Kincaid Readability - num of sentences - Gunning-Fog Readability - avg num of words Discourse Features per sentence - num/proportion of connectives - num of NEs - num/proportion of hedge words - num/proportion of Article Features capitalized words - TF/TF-IDF simi with article - avg/sum TF-IDF - TF/TF-IDF simi with comments - contains URL - JS/KL divergence (div) with article - user rating (pos/neg) - JS/KL div with comments Sentiment Features - num /proportion of positive/negative/neutral words (MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966)) - num /proportion of sentiment words Table 3: Features used for comment importance scoring. Ri,j . The interplay between the two types of data is encoded in the following regularizing constraint: Js,c (ws , wc ) = X X λsc · Basic Features - num of words - absolute/relative position - overlaps with headline - avg/sum TF-IDF scores - num of NEs (4) Furthermore, using the following notation,      0   0 ˜ ˜ ˜ ˜ ˜ = Xs 0 Y ˜ = Ys X ˜ 0 = Xs 0 Y ˜ 0 = Ys X 0 ˜ ˜ ˜ ˜ Yc0 0 Xc Yc 0 Xc     0 ˜ = βs Ik 0 ˜ = λs I|X0s | β λ 0 βc I l 0 λc I|X0c |"
N16-1007,D10-1049,0,0.0361693,"he gold-standard summary. From Figure 2, we can see that our importance estimation model produces uniformly better ranking performance on both datasets. 5.2 Automatic Summary Evaluation For automatic summary evaluation, we consider three popular metrics. ROUGE (Lin and Hovy, 2003) is employed to evaluate n-grams recall of the summaries with gold-standard abstracts as reference. ROUGE-SU4 (measures unigram and skipbigrams separated by up to four words) is reported. We also utilize BLEU, a precision-based metric, which has been used to evaluate various language generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014). We further consider METEOR (Denkowski and Lavie, 2014). As a recall-oriented metric, it calculates similarity between generations and references by considering synonyms and paraphrases. For comparisons, we first compare with an abstractive summarization method presented in Ganesan et al. (2010) on the RottenTomatoes dataset. Ganesan et al. (2010) utilize a graph-based algorithm to remove repetitive information, and merge opinionated expressions based on syntactic struc52 tures of product reviews.2 For both datasets, we consider two extractive summarization approa"
N16-1007,P15-1153,0,0.0735218,"al., 2006; Li et al., 2010). The second example lists a set of arguments on “death penalty”, where each argument supports the central claim “death penalty deters crime”. Arguments, as a special type of opinionated text, contain reasons to persuade or inform people on certain issues. Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim generation. Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010). Those systems are not capable of generating new words, and the output summary may suffer from ungrammatical structure. Another line of work requires a large amount of human input to enforce summary quality. For example, Gerani et al. (2014) utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions. To address the challenges above, we propose to use an attention-based abstract generation model — a data-driven approach trained to generate informative, concise, and flue"
N16-1007,P05-1022,0,0.0191366,". Our System: People have a right to freedom of religion. Figure 3: Sample summaries generated by different systems on movie reviews and arguments. We only show a subset of reviews and arguments due to limited space. 54 Figure 4: Sampling effect on RottenTomatoes. ations produces inferior results than re-ranking them with simple heuristics. This suggests that the current models are oblivious to some task specific issues, such as informativeness. Post-processing is needed to make better use of the summary candidates. For example, future work can study other sophisticated re-ranking algorithms (Charniak and Johnson, 2005; Konstas and Lapata, 2012). Furthermore, we also look at the difficult cases where our summaries are evaluated to have lower informativeness. They are often much shorter than the gold-standard human abstracts, thus the information coverage is limited. In other cases, some generations contain incorrect information on domain-dependent facts, e.g. named entities, numbers, etc. For instance, a summary “a poignant coming-of-age tale marked by a breakout lead performance from Cate Shortland” is generated for movie “Lore”. This summary contains “Cate Shortland” which is the director of the movie ins"
N16-1007,P05-1033,0,0.0421488,"nt word with the gold-standard summary. From Figure 2, we can see that our importance estimation model produces uniformly better ranking performance on both datasets. 5.2 Automatic Summary Evaluation For automatic summary evaluation, we consider three popular metrics. ROUGE (Lin and Hovy, 2003) is employed to evaluate n-grams recall of the summaries with gold-standard abstracts as reference. ROUGE-SU4 (measures unigram and skipbigrams separated by up to four words) is reported. We also utilize BLEU, a precision-based metric, which has been used to evaluate various language generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014). We further consider METEOR (Denkowski and Lavie, 2014). As a recall-oriented metric, it calculates similarity between generations and references by considering synonyms and paraphrases. For comparisons, we first compare with an abstractive summarization method presented in Ganesan et al. (2010) on the RottenTomatoes dataset. Ganesan et al. (2010) utilize a graph-based algorithm to remove repetitive information, and merge opinionated expressions based on syntactic struc52 tures of product reviews.2 For both datasets, we consider two extractive"
N16-1007,W14-3348,0,0.0194684,"tion model produces uniformly better ranking performance on both datasets. 5.2 Automatic Summary Evaluation For automatic summary evaluation, we consider three popular metrics. ROUGE (Lin and Hovy, 2003) is employed to evaluate n-grams recall of the summaries with gold-standard abstracts as reference. ROUGE-SU4 (measures unigram and skipbigrams separated by up to four words) is reported. We also utilize BLEU, a precision-based metric, which has been used to evaluate various language generation systems (Chiang, 2005; Angeli et al., 2010; Karpathy and Fei-Fei, 2014). We further consider METEOR (Denkowski and Lavie, 2014). As a recall-oriented metric, it calculates similarity between generations and references by considering synonyms and paraphrases. For comparisons, we first compare with an abstractive summarization method presented in Ganesan et al. (2010) on the RottenTomatoes dataset. Ganesan et al. (2010) utilize a graph-based algorithm to remove repetitive information, and merge opinionated expressions based on syntactic struc52 tures of product reviews.2 For both datasets, we consider two extractive summarization approaches: (1) L EX R ANK (Erkan and Radev, 2004) is an unsupervised method that computes"
N16-1007,W14-4408,0,0.304532,"Missing"
N16-1007,D15-1042,0,0.0614579,"Missing"
N16-1007,C10-1039,0,0.860735,"l., 2010). The second example lists a set of arguments on “death penalty”, where each argument supports the central claim “death penalty deters crime”. Arguments, as a special type of opinionated text, contain reasons to persuade or inform people on certain issues. Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim generation. Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010). Those systems are not capable of generating new words, and the output summary may suffer from ungrammatical structure. Another line of work requires a large amount of human input to enforce summary quality. For example, Gerani et al. (2014) utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions. To address the challenges above, we propose to use an attention-based abstract generation model — a data-driven approach trained to generate informative, concise, and fluent opinion summaries. O"
N16-1007,D14-1168,0,0.207662,"ple on certain issues. Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim generation. Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan et al., 2010). Those systems are not capable of generating new words, and the output summary may suffer from ungrammatical structure. Another line of work requires a large amount of human input to enforce summary quality. For example, Gerani et al. (2014) utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions. To address the challenges above, we propose to use an attention-based abstract generation model — a data-driven approach trained to generate informative, concise, and fluent opinion summaries. Our method is based on the recently proposed framework of neural encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a), which translates a sentence in a source language into a target language. Different from previous"
N16-1007,D13-1176,0,0.0337756,"er from ungrammatical structure. Another line of work requires a large amount of human input to enforce summary quality. For example, Gerani et al. (2014) utilize a set of templates constructed by human, which are filled by extracted phrases to generate grammatical sentences that serve different discourse functions. To address the challenges above, we propose to use an attention-based abstract generation model — a data-driven approach trained to generate informative, concise, and fluent opinion summaries. Our method is based on the recently proposed framework of neural encoder-decoder models (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a), which translates a sentence in a source language into a target language. Different from previous work, our summarization system is designed to support multiple input text units. An attention-based model (Bahdanau et al., 2014) is deployed to al48 low the encoder to automatically search for salient information within context. Furthermore, we propose an importance-based sampling method so that the encoder can integrate information from an important subset of input text. The importance score of a text unit is estimated from a novel regression model with pairwise prefer"
N16-1007,P12-1039,0,0.0116949,"rtance-based sampling rate (K) is set to 5 for experiments in Sections 5.2 3.6 Post-processing and 5.3. For testing phase, we re-rank the n-best summaries Decoding is performed by beam search with a according to their cosine similarity with the input beam size of 20, i.e. we keep 20 most probable outtext units. The one with the highest similarity is input sequences in stack at each step. Outputs with cluded in the final summary. Uses of more sophisend of sentence token are also considered for ticated re-ranking methods (Charniak and Johnson, re-ranking. Decoding stops when every beam in 2005; Konstas and Lapata, 2012) will be investistack generates the end of sentence token. gated in future work. 4 5 Experimental Setup Data Pre-processing. We pre-process the datasets with Stanford CoreNLP (Manning et al., 2014) for tokenization and extracting POS tags and dependency relations. For RottenTomatoes dataset, we replace movie titles with a generic label in training, and substitute it with the movie name if there is any generic label generated in testing. 51 5.1 Results Importance Estimation Evaluation We first evaluate the importance estimation component described in Section 3.5. We compare with Support Vector"
N16-1007,E09-1059,0,0.116486,"gh different aspects of life, ranging from making decisions on regular tasks to judging fundamental societal issues and forming personal ideology. To efficiently absorb the massive amount of opinionated information, there is a pressing need for automated systems that can generate concise and fluent opinion summary about an entity or a topic. In spite of substantial researches in opinion summarization, the most prominent approaches mainly rely on extractive summarization methods, where phrases or sentences from the original documents are selected for inclusion in the summary (Hu and Liu, 2004; Lerman et al., 2009). One of the problems that extractive methods suffer from Wang Ling Google DeepMind London, N1 0AE lingwang@google.com Movie: The Martian Reviews: - One the smartest, sweetest, and most satisfyingly suspenseful sci-fi films in years. - ...an intimate sci-fi epic that is smart, spectacular and stirring. - The Martian is a thrilling, human and moving sci-fi picture that is easily the most emotionally engaging film Ridley Scott has made... - It’s pretty sunny and often funny, a space oddity for a director not known for pictures with a sense of humor. - The Martian highlights the book’s best quali"
N16-1007,C10-1074,0,0.0138023,"mary that describes the opinion consensus of the input. Specifically, we investigate our abstract generation model on two types of opinionated text: movie reviews and arguments on controversial topics. Examples are displayed in Figure 1. The first example contains a set of professional reviews (or critics) about movie “The Martian” and an opinion consensus written by an editor. It would be more useful to automatically generate fluent opinion consensus rather than simply extracting features (e.g. plot, music, etc) and opinion phrases as done in previous summarization work (Zhuang et al., 2006; Li et al., 2010). The second example lists a set of arguments on “death penalty”, where each argument supports the central claim “death penalty deters crime”. Arguments, as a special type of opinionated text, contain reasons to persuade or inform people on certain issues. Given a set of arguments on the same topic, we aim at investigating the capability of our abstract generation system for the novel task of claim generation. Existing abstract generation systems for opinionated text mostly take an approach that first identifies salient phrases, and then merges them into sentences (Bing et al., 2015; Ganesan e"
N16-1007,N03-1020,0,0.645887,"th importance-based sampling, our model can be trained within manageable time, and is still able to learn from diversified input. We demonstrate the effectiveness of our model on two newly collected datasets for movie reviews and arguments. Automatic evaluation by BLEU (Papineni et al., 2002) indicates that our system outperforms the state-of-the-art extract-based and abstractbased methods on both tasks. For example, we achieved a BLEU score of 24.88 on Rotten Tomatoes movie reviews, compared to 19.72 by an abstractive opinion summarization system from Ganesan et al. (2010). ROUGE evaluation (Lin and Hovy, 2003) also indicates that our system summaries have reasonable information coverage. Human judges further rated our summaries to be more informative and grammatical than compared systems. 2 Data Collection We collected two datasets for movie reviews and arguments on controversial topics with goldstandard abstracts.1 Rotten Tomatoes (www. rottentomatoes.com) is a movie review website that aggregates both professional critics and user-generated reviews (henceforth RottenTomatoes). For each movie, a one-sentence critic consensus is constructed by an editor to summarize the opinions in professional cri"
N16-1007,P14-5010,0,0.00499025,"Missing"
N16-1007,P02-1040,0,0.0965339,"omatically search for salient information within context. Furthermore, we propose an importance-based sampling method so that the encoder can integrate information from an important subset of input text. The importance score of a text unit is estimated from a novel regression model with pairwise preference-based regularizer. With importance-based sampling, our model can be trained within manageable time, and is still able to learn from diversified input. We demonstrate the effectiveness of our model on two newly collected datasets for movie reviews and arguments. Automatic evaluation by BLEU (Papineni et al., 2002) indicates that our system outperforms the state-of-the-art extract-based and abstractbased methods on both tasks. For example, we achieved a BLEU score of 24.88 on Rotten Tomatoes movie reviews, compared to 19.72 by an abstractive opinion summarization system from Ganesan et al. (2010). ROUGE evaluation (Lin and Hovy, 2003) also indicates that our system summaries have reasonable information coverage. Human judges further rated our summaries to be more informative and grammatical than compared systems. 2 Data Collection We collected two datasets for movie reviews and arguments on controversia"
N16-1007,D10-1007,0,0.220939,"etc. For instance, a summary “a poignant coming-of-age tale marked by a breakout lead performance from Cate Shortland” is generated for movie “Lore”. This summary contains “Cate Shortland” which is the director of the movie instead of actor. It would require semantic features to handle this issue, which has yet to be attempted. 6 Related Work Our work belongs to the area of opinion summarization. Constructing fluent natural language opinion summaries has mainly considered product reviews (Hu and Liu, 2004; Lerman et al., 2009), community question answering (Wang et al., 2014), and editorials (Paul et al., 2010). Extractive summarization approaches are employed to identify summaryworthy sentences. For example, Hu and Liu (2004) first identify the frequent product features and then attach extracted opinion sentences to the corresponding feature. Our model instead utilizes abstract generation techniques to construct natural language summaries. As far as we know, we are also the first to study claim generation for arguments. Recently, there has been a growing interest in generating abstractive summaries for news articles (Bing et al., 2015), spoken meetings (Wang and Cardie, 2013), and product reviews ("
N16-1007,D15-1044,0,0.0308694,"e not guaranteed to be grammatical. Gerani et al. (2014) then design a set of manually-constructed realization templates for producing grammatical sentences that serve different discourse functions. Our approach does not require any human-annotated rules, and can be applied in various domains. Our task is closely related to recent advances in neural machine translation (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014a). Based on the sequence-to-sequence paradigm, RNNs-based models have been investigated for compression (Filippova et al., 2015) and summarization (Filippova et al., 2015; Rush et al., 2015; Hermann et al., 2015) at sentence-level. Built on the attention-based translation model in Bahdanau et al. (2014), Rush et al. (2015) study the problem of constructing abstract for a single sentence. Our task differs from the models presented above in that our model carries out abstractive decoding from multiple sentences instead of a single sentence. 7 Conclusion In this work, we presented a neural approach to generate abstractive summaries for opinionated text. We employed an attention-based method that finds salient information from different input text units to generate an informative an"
N16-1007,E12-1023,0,0.0576652,"Missing"
N16-1007,P13-1137,1,0.395604,"l., 2014), and editorials (Paul et al., 2010). Extractive summarization approaches are employed to identify summaryworthy sentences. For example, Hu and Liu (2004) first identify the frequent product features and then attach extracted opinion sentences to the corresponding feature. Our model instead utilizes abstract generation techniques to construct natural language summaries. As far as we know, we are also the first to study claim generation for arguments. Recently, there has been a growing interest in generating abstractive summaries for news articles (Bing et al., 2015), spoken meetings (Wang and Cardie, 2013), and product reviews (Ganesan et al., 2010; Di Fabbrizio et al., 2014; Gerani et al., 2014). Most approaches are based on phrase extraction, from which an algorithm concatenates them into sentences (Bing et al., 2015; Ganesan et al., 2010). Nevertheless, the output summaries are not guaranteed to be grammatical. Gerani et al. (2014) then design a set of manually-constructed realization templates for producing grammatical sentences that serve different discourse functions. Our approach does not require any human-annotated rules, and can be applied in various domains. Our task is closely relate"
N16-1007,C14-1157,1,0.181544,"acts, e.g. named entities, numbers, etc. For instance, a summary “a poignant coming-of-age tale marked by a breakout lead performance from Cate Shortland” is generated for movie “Lore”. This summary contains “Cate Shortland” which is the director of the movie instead of actor. It would require semantic features to handle this issue, which has yet to be attempted. 6 Related Work Our work belongs to the area of opinion summarization. Constructing fluent natural language opinion summaries has mainly considered product reviews (Hu and Liu, 2004; Lerman et al., 2009), community question answering (Wang et al., 2014), and editorials (Paul et al., 2010). Extractive summarization approaches are employed to identify summaryworthy sentences. For example, Hu and Liu (2004) first identify the frequent product features and then attach extracted opinion sentences to the corresponding feature. Our model instead utilizes abstract generation techniques to construct natural language summaries. As far as we know, we are also the first to study claim generation for arguments. Recently, there has been a growing interest in generating abstractive summaries for news articles (Bing et al., 2015), spoken meetings (Wang and"
N16-1007,H05-1044,0,0.054179,"˜ 0T ˜ ˜ 0 ˜ ˜ ˜ ˜ ˆ = (R R + R λR + β) (R L + R λL ) (9) are defined with states and cells of 150 dimensions. w The attention of each input word and state pair is computed by being projected into a vector of 100 dimensions (Equation 6). - num of words - category in General Inquirer Training is performed via Adagrad (Duchi et al., - unigram (Stone et al., 1966) - num of POS tags - num of positive/negative/neutral 2011). It terminates when performance does not im- num of named entities words (General Inquirer, prove on the development set. We use BLEU (up to - centroidness (Radev, 2001) MPQA (Wilson et al., 2005)) 4-grams) (Papineni et al., 2002) as evaluation met- avg/max TF-IDF scores ric, which computes the precision of n-grams in genTable 1: Features used for text unit importance estimation. erated summaries with gold-standard abstracts as the reference. Finally, the importance-based sampling rate (K) is set to 5 for experiments in Sections 5.2 3.6 Post-processing and 5.3. For testing phase, we re-rank the n-best summaries Decoding is performed by beam search with a according to their cosine similarity with the input beam size of 20, i.e. we keep 20 most probable outtext units. The one with the hi"
N18-1035,P11-2008,0,0.20005,"Missing"
N18-1035,N12-1074,0,0.67722,"Missing"
N18-1035,W09-3951,0,0.100687,", 2016). Distinguishing from prior work that focuses on post-level recommendation, we tackle the challenges of predicting user reply behaviors at the conversation-level. In addition, our model not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure. Much of the previous work on discourse structure and dialogue acts has relied on labeled data (Jurafsky et al., 1997; Stolcke et al., 2000), while unsupervised approaches have not been applied to the problem of conversation recommendation (Woszczyna and Waibel, 1994; Crook et al., 2009; Ritter et al., 2010; Joty et al., 2011). Our work is also in line with conversation modeling for social media discussions (Ritter et al., 2010; Budak and Agrawal, 2013; Louis and Cohen, 2015; Cheng et al., 2017). Topic modeling 2 To ensure the general applicability of our approach to domains lacking such information, we do not utilize external features such as network structure, but it may certainly be added in future, more narrowly targeted applications. 376 which weights positive instances higher during training and is thus suited to our data. Formally, for user u and conversation c, we me"
N18-1035,C10-1034,0,0.333494,"sing attention in digital communication research (Agichtein et al., 2008; Kwak et al., 2010; Wu et al., 2011). The problem studied here is closely related to work on recommendation and response prediction in microblogs (Artzi et al., 2012; Hong et al., 2013), where the goal is to predict whether a user will share or reply to a given post. Existing methods focus on measuring features that reflect personalized user interests, including topics (Hong et al., 2013) and network structures (Pan et al., 2013; He and Tan, 2015). These features have been investigated under a learning to rank framework (Duan et al., 2010; Artzi et al., 2012), graph ranking models (Yan et al., 2012; Feng and Wang, 2013; Alawad et al., 2016), and neural network-based representation learning methods (Yu et al., 2016). Distinguishing from prior work that focuses on post-level recommendation, we tackle the challenges of predicting user reply behaviors at the conversation-level. In addition, our model not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure. Much of the previous work on discourse structure and dialogue acts has relied on labeled data ("
N18-1035,N10-1020,0,0.59776,"large quantities of superfluous material select which conversations to engage in, and how might we better algorithmically recommend conversations suited to individual users? We approach this problem from a microblog conversation recommendation framework. Where prior work has focused on the content of individual posts for recommendation (Chen et al., 2012; Yan et al., 2012; Vosecky et al., 2014; He and Tan, 2015), we examine the entire history and context of a conversation, including both topical content and discourse modes such as agreement, question-asking, argument and other dialogue acts (Ritter et al., 2010).1 And where Backstrom et al. (2013) leveraged conversation reply structure (such as previous user engagement), their model is unable to predict first entry into new conversations, while ours is able to predict both new 1 In this paper, discourse mode refers to a certain type of dialogue act, e.g., agreement or argument. The discourse structure of a conversation means some combination (or a probability distribution) of discourse modes. 375 Proceedings of NAACL-HLT 2018, pages 375–385 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics and repeated entry i"
N18-1035,D15-1178,0,0.0588574,"del not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure. Much of the previous work on discourse structure and dialogue acts has relied on labeled data (Jurafsky et al., 1997; Stolcke et al., 2000), while unsupervised approaches have not been applied to the problem of conversation recommendation (Woszczyna and Waibel, 1994; Crook et al., 2009; Ritter et al., 2010; Joty et al., 2011). Our work is also in line with conversation modeling for social media discussions (Ritter et al., 2010; Budak and Agrawal, 2013; Louis and Cohen, 2015; Cheng et al., 2017). Topic modeling 2 To ensure the general applicability of our approach to domains lacking such information, we do not utilize external features such as network structure, but it may certainly be added in future, more narrowly targeted applications. 376 which weights positive instances higher during training and is thus suited to our data. Formally, for user u and conversation c, we measure reply preference based on the MSE between predicted preference score pu,c and reply history ru,c . ru,c equals 1 if u is in the conversation history; otherwise, it is 0. The first term o"
N18-1035,J00-3003,0,0.738079,"Missing"
N18-1035,N13-1039,0,0.146625,"Missing"
N18-1035,P12-1054,0,0.839358,"itical discussion, for instance, might prefer content concerning a specific candidate or issue, and only then if discussed in a positive light without controversy (Adamic and Glance, 2005; Bakshy et al., 2015). How do individuals facing such large quantities of superfluous material select which conversations to engage in, and how might we better algorithmically recommend conversations suited to individual users? We approach this problem from a microblog conversation recommendation framework. Where prior work has focused on the content of individual posts for recommendation (Chen et al., 2012; Yan et al., 2012; Vosecky et al., 2014; He and Tan, 2015), we examine the entire history and context of a conversation, including both topical content and discourse modes such as agreement, question-asking, argument and other dialogue acts (Ritter et al., 2010).1 And where Backstrom et al. (2013) leveraged conversation reply structure (such as previous user engagement), their model is unable to predict first entry into new conversations, while ours is able to predict both new 1 In this paper, discourse mode refers to a certain type of dialogue act, e.g., agreement or argument. The discourse structure of a con"
N18-1035,P16-2073,0,0.0554006,"and response prediction in microblogs (Artzi et al., 2012; Hong et al., 2013), where the goal is to predict whether a user will share or reply to a given post. Existing methods focus on measuring features that reflect personalized user interests, including topics (Hong et al., 2013) and network structures (Pan et al., 2013; He and Tan, 2015). These features have been investigated under a learning to rank framework (Duan et al., 2010; Artzi et al., 2012), graph ranking models (Yan et al., 2012; Feng and Wang, 2013; Alawad et al., 2016), and neural network-based representation learning methods (Yu et al., 2016). Distinguishing from prior work that focuses on post-level recommendation, we tackle the challenges of predicting user reply behaviors at the conversation-level. In addition, our model not only captures latent factors such as the topical interests of users, but also leverages the automatically learned discourse structure. Much of the previous work on discourse structure and dialogue acts has relied on labeled data (Jurafsky et al., 1997; Stolcke et al., 2000), while unsupervised approaches have not been applied to the problem of conversation recommendation (Woszczyna and Waibel, 1994; Crook e"
N19-1219,C16-1324,0,0.139786,"Missing"
N19-1219,P14-1048,0,0.0254429,"the accuracy of tokenization, we manually replace mathematical formulas, variables, URL links, and formatted citation with special tokens such as &lt;EQN>, &lt;VAR>, &lt;URL>, and &lt;CIT>. Parameters, lexicons, and features used for the models are described in the supplementary material. 3.1 Task I: Proposition Segmentation We consider three baselines. FullSent: treating each sentence as a proposition. PDTB-conn: further segmenting sentences when any discourse connective (collected from Penn Discourse Treebank (Prasad et al., 2007)) is observed. RSTparser: segmenting discourse units by the RST parser in Feng and Hirst (2014). For learning-based methods, we start with Conditional Random Field (CRF) (Lafferty et al., 2001) with features proposed by Stab and Gurevych ((2017), Table 7), and BiLSTM-CRF, a bidirectional Long Short-Term Memory network (BiLSTM) connected to a CRF output layer and further enhanced with ELMo representation (Peters et al., 2018). We adopt the BIO scheme for sequential tagging (Ramshaw and Marcus, 1999), with O corresponding to N ON - ARG. Finally, we consider jointly modeling segmentation and classification by appending the proposition types to BI tags, e.g., B-fact, with CRF (CRF-joint) an"
N19-1219,P16-2089,0,0.0207436,"Bornmann et al. (2012) for analyzing languages in peer reviews. To the best of our knowledge, our work is the first to understand the content and structure of peer reviews via argument usage. Our work is also in line with the growing body of research in argument mining (Teufel et al., 1999; Palau and Moens, 2009). Most of the work focuses on arguments in social media posts (Park and Cardie, 2014; Wei et al., 2016; Habernal and Gurevych, 2016), online debate portals or Oxfordstyle debates (Wachsmuth et al., 2017; Hua and Wang, 2017; Wang et al., 2017), and student essays (Persing and Ng, 2015; Ghosh et al., 2016). We study a new domain of peer reviews, and identify new challenges for existing models. 6 Conclusion We study the content and structure of peer reviews under the argument mining framework. AMPERE, a new dataset of peer reviews, is collected and annotated with propositions and their types. We benchmark AMPERE with state-of-the-art argument mining models for proposition segmentation and classification. We leverage the classifiers to analyze the proposition usage in reviews across ML and NLP venues, showing interesting patterns in proposition types and content. Acknowledgements This research is"
N19-1219,P16-1150,0,0.106394,"Constructive reviews, e.g., review #2, often contain in-depth analysis as well as concrete suggestions. As a result, automatically identifying propositions and their types would be useful to understand the composition of peer reviews. Therefore, we propose an argument miningbased approach to understand the content and structure of peer reviews. Argument mining studies the automatic detection of argumentative components and structure within discourse (Peldszus and Stede, 2013). Specifically, argument types (e.g. evidence and reasoning) and their arrangement are indicative of argument quality (Habernal and Gurevych, 2016; Wachsmuth et al., 2017). In this work, we focus on two specific tasks: (1) proposition segmentation—detecting elementary argumentative discourse units that are 2131 Proceedings of NAACL-HLT 2019, pages 2131–2137 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics propositions, and (2) proposition classification— labeling the propositions according to their types (e.g., evaluation vs. request). Since there was no annotated dataset for peer reviews, as part of this study, we first collect 14.2K reviews from major machine learning (ML) and natural lan"
N19-1219,J17-1004,0,0.0663114,"Missing"
N19-1219,P17-2032,1,0.852673,"of the field. Shallow linguistic features, e.g., sentiment words, are studied in Bornmann et al. (2012) for analyzing languages in peer reviews. To the best of our knowledge, our work is the first to understand the content and structure of peer reviews via argument usage. Our work is also in line with the growing body of research in argument mining (Teufel et al., 1999; Palau and Moens, 2009). Most of the work focuses on arguments in social media posts (Park and Cardie, 2014; Wei et al., 2016; Habernal and Gurevych, 2016), online debate portals or Oxfordstyle debates (Wachsmuth et al., 2017; Hua and Wang, 2017; Wang et al., 2017), and student essays (Persing and Ng, 2015; Ghosh et al., 2016). We study a new domain of peer reviews, and identify new challenges for existing models. 6 Conclusion We study the content and structure of peer reviews under the argument mining framework. AMPERE, a new dataset of peer reviews, is collected and annotated with propositions and their types. We benchmark AMPERE with state-of-the-art argument mining models for proposition segmentation and classification. We leverage the classifiers to analyze the proposition usage in reviews across ML and NLP venues, showing inter"
N19-1219,N18-1149,0,0.106611,"Missing"
N19-1219,D14-1181,0,0.00848256,"Missing"
N19-1219,C00-1072,0,0.413941,"Missing"
N19-1219,P17-1091,0,0.0584816,"Missing"
N19-1219,W14-2105,0,0.0643866,"ation by experts or editors proves to be more reliable and informative (van Rooyen et al., 1999), but requires substantial work and knowledge of the field. Shallow linguistic features, e.g., sentiment words, are studied in Bornmann et al. (2012) for analyzing languages in peer reviews. To the best of our knowledge, our work is the first to understand the content and structure of peer reviews via argument usage. Our work is also in line with the growing body of research in argument mining (Teufel et al., 1999; Palau and Moens, 2009). Most of the work focuses on arguments in social media posts (Park and Cardie, 2014; Wei et al., 2016; Habernal and Gurevych, 2016), online debate portals or Oxfordstyle debates (Wachsmuth et al., 2017; Hua and Wang, 2017; Wang et al., 2017), and student essays (Persing and Ng, 2015; Ghosh et al., 2016). We study a new domain of peer reviews, and identify new challenges for existing models. 6 Conclusion We study the content and structure of peer reviews under the argument mining framework. AMPERE, a new dataset of peer reviews, is collected and annotated with propositions and their types. We benchmark AMPERE with state-of-the-art argument mining models for proposition segmen"
N19-1219,L18-1257,0,0.0267776,"Missing"
N19-1219,P15-1053,0,0.0299494,"words, are studied in Bornmann et al. (2012) for analyzing languages in peer reviews. To the best of our knowledge, our work is the first to understand the content and structure of peer reviews via argument usage. Our work is also in line with the growing body of research in argument mining (Teufel et al., 1999; Palau and Moens, 2009). Most of the work focuses on arguments in social media posts (Park and Cardie, 2014; Wei et al., 2016; Habernal and Gurevych, 2016), online debate portals or Oxfordstyle debates (Wachsmuth et al., 2017; Hua and Wang, 2017; Wang et al., 2017), and student essays (Persing and Ng, 2015; Ghosh et al., 2016). We study a new domain of peer reviews, and identify new challenges for existing models. 6 Conclusion We study the content and structure of peer reviews under the argument mining framework. AMPERE, a new dataset of peer reviews, is collected and annotated with propositions and their types. We benchmark AMPERE with state-of-the-art argument mining models for proposition segmentation and classification. We leverage the classifiers to analyze the proposition usage in reviews across ML and NLP venues, showing interesting patterns in proposition types and content. Acknowledgem"
N19-1219,N18-1202,0,0.00917274,"aselines. FullSent: treating each sentence as a proposition. PDTB-conn: further segmenting sentences when any discourse connective (collected from Penn Discourse Treebank (Prasad et al., 2007)) is observed. RSTparser: segmenting discourse units by the RST parser in Feng and Hirst (2014). For learning-based methods, we start with Conditional Random Field (CRF) (Lafferty et al., 2001) with features proposed by Stab and Gurevych ((2017), Table 7), and BiLSTM-CRF, a bidirectional Long Short-Term Memory network (BiLSTM) connected to a CRF output layer and further enhanced with ELMo representation (Peters et al., 2018). We adopt the BIO scheme for sequential tagging (Ramshaw and Marcus, 1999), with O corresponding to N ON - ARG. Finally, we consider jointly modeling segmentation and classification by appending the proposition types to BI tags, e.g., B-fact, with CRF (CRF-joint) and BiLSTM-CRF (BiLSTM-CRF-joint). Table 4 shows that BiLSTM-CRF outperforms other methods in F1. More importantly, the perforPrec. 73.68 51.11 30.28 66.53 82.25 74.99 81.12 Rec. 56.00 49.71 43.00 52.92 79.96 63.33 78.42 F1 63.64 50.40 35.54 58.95 81.09∗ 68.67 79.75 Table 4: Proposition segmentation results. Result that is significan"
N19-1219,P17-2039,0,0.0869207,"review #2, often contain in-depth analysis as well as concrete suggestions. As a result, automatically identifying propositions and their types would be useful to understand the composition of peer reviews. Therefore, we propose an argument miningbased approach to understand the content and structure of peer reviews. Argument mining studies the automatic detection of argumentative components and structure within discourse (Peldszus and Stede, 2013). Specifically, argument types (e.g. evidence and reasoning) and their arrangement are indicative of argument quality (Habernal and Gurevych, 2016; Wachsmuth et al., 2017). In this work, we focus on two specific tasks: (1) proposition segmentation—detecting elementary argumentative discourse units that are 2131 Proceedings of NAACL-HLT 2019, pages 2131–2137 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics propositions, and (2) proposition classification— labeling the propositions according to their types (e.g., evaluation vs. request). Since there was no annotated dataset for peer reviews, as part of this study, we first collect 14.2K reviews from major machine learning (ML) and natural language processing (NLP) ve"
N19-1219,Q17-1016,1,0.853654,"ow linguistic features, e.g., sentiment words, are studied in Bornmann et al. (2012) for analyzing languages in peer reviews. To the best of our knowledge, our work is the first to understand the content and structure of peer reviews via argument usage. Our work is also in line with the growing body of research in argument mining (Teufel et al., 1999; Palau and Moens, 2009). Most of the work focuses on arguments in social media posts (Park and Cardie, 2014; Wei et al., 2016; Habernal and Gurevych, 2016), online debate portals or Oxfordstyle debates (Wachsmuth et al., 2017; Hua and Wang, 2017; Wang et al., 2017), and student essays (Persing and Ng, 2015; Ghosh et al., 2016). We study a new domain of peer reviews, and identify new challenges for existing models. 6 Conclusion We study the content and structure of peer reviews under the argument mining framework. AMPERE, a new dataset of peer reviews, is collected and annotated with propositions and their types. We benchmark AMPERE with state-of-the-art argument mining models for proposition segmentation and classification. We leverage the classifiers to analyze the proposition usage in reviews across ML and NLP venues, showing interesting patterns in p"
N19-1219,P16-2032,0,0.0300398,"tors proves to be more reliable and informative (van Rooyen et al., 1999), but requires substantial work and knowledge of the field. Shallow linguistic features, e.g., sentiment words, are studied in Bornmann et al. (2012) for analyzing languages in peer reviews. To the best of our knowledge, our work is the first to understand the content and structure of peer reviews via argument usage. Our work is also in line with the growing body of research in argument mining (Teufel et al., 1999; Palau and Moens, 2009). Most of the work focuses on arguments in social media posts (Park and Cardie, 2014; Wei et al., 2016; Habernal and Gurevych, 2016), online debate portals or Oxfordstyle debates (Wachsmuth et al., 2017; Hua and Wang, 2017; Wang et al., 2017), and student essays (Persing and Ng, 2015; Ghosh et al., 2016). We study a new domain of peer reviews, and identify new challenges for existing models. 6 Conclusion We study the content and structure of peer reviews under the argument mining framework. AMPERE, a new dataset of peer reviews, is collected and annotated with propositions and their types. We benchmark AMPERE with state-of-the-art argument mining models for proposition segmentation and classif"
N19-1219,P11-2088,0,0.0169638,"ws both the commonly used salient words across venues and the unique words with top frequencies for each venue (α = 0.001, χ2 test). For evaluation, all venues tend to focus on clarity and contribution, with ICLR discussing more about “network” and NeurIPS often mentioning equations. ACL reviews then frequently request for “examples”. 5 Related Work There is a growing interest in understanding the content and assessing the quality of peer reviews. Authors’ feedback such as satisfaction and helpfulness have been adopted as quality indicators (Latu and Everett, 2000; Hart-Davidson et al., 2010; Xiong and Litman, 2011). Nonetheless, they suffer from author subjectivity and are often influenced by acceptance decisions (Weber et al., 2002). Evaluation by experts or editors proves to be more reliable and informative (van Rooyen et al., 1999), but requires substantial work and knowledge of the field. Shallow linguistic features, e.g., sentiment words, are studied in Bornmann et al. (2012) for analyzing languages in peer reviews. To the best of our knowledge, our work is the first to understand the content and structure of peer reviews via argument usage. Our work is also in line with the growing body of researc"
N19-1219,N18-2006,0,0.0316398,"Missing"
N19-1219,J17-3005,0,0.13164,"suggesting a course of action. Ex: “The authors should compare with the following methods.” FACT: Objective information of the paper or commonsense knowledge. Ex: “Existing works on multi-task neural networks typically use hand-tuned weights. . .” R EFERENCE: Citations and URLs. Ex: “see MuseGAN (Dong et al), MidiNet (Yang et al), etc ” Q UOTE: Quotations from the paper. Ex: “The author wrote ‘where r is lower bound of feature norm’.” N ON - ARG: Non-argumentative statements. Ex: “Aha, now I understand.” Table 1: Proposition types and examples. Dataset Comments (Park and Cardie, 2018) Essays (Stab and Gurevych, 2017) News (Al Khatib et al., 2016) Web (Habernal and Gurevych, 2017) AMPERE #Doc 731 402 300 340 400 #Sent 3,994 7,116 11,754 3,899 8,030 #Prop 4,931 6,089 14,313 1,882 10,386 Table 2: Statistics for AMPERE and some argument mining corpora, including # of annotated propositions. In total, 14, 202 reviews are collected (ICLR: 4, 057; UAI: 718; ACL: 275; and NeurIPS: 9, 152). All venues except NeurIPS have paper rating scores attached to the reviews. Annotation Process. For proposition segmentation, we adopt the concepts from Park et al. (2015) and instruct the annotators to identify elementary argu"
P13-1136,P11-1049,0,0.258265,". Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while"
P13-1136,J96-1002,0,0.124803,"Missing"
P13-1136,briscoe-carroll-2002-robust,0,0.0538117,"other systems except the rule-based system). 7 Figure 3: Part of the summary generated by the multiscorer based summarizer for topic D0626H (DUC 2006). Grayed out words are removed. Queryirrelevant phrases, such as temporal information or source of the news, have been removed. dependency-tree based compressor (Martins and Smith, 2009)8 . We adopt the metrics in Martins and Smith (2009) to measure the unigram-level macro precision, recall, and F1-measure with respect to human annotated compression. In addition, we also compute the F1 scores of grammatical relations which are annotated by RASP (Briscoe and Carroll, 2002) according to Clarke and Lapata (2008). In Table 7, our context-aware and head-driven tree-based compression systems show statistically significantly (p < 0.01) higher precisions (Uni8 Thanks to Andr´e F.T. Martins for system outputs. Conclusion We have presented a framework for query-focused multi-document summarization based on sentence compression. We propose three types of compression approaches. Our tree-based compression method can easily incorporate measures of query relevance, content importance, redundancy and language quality into the compression process. By testing on a standard dat"
P13-1136,P11-1050,0,0.017869,"Missing"
P13-1136,P06-1039,0,0.0127762,"Missing"
P13-1136,W03-0501,0,0.238213,"Missing"
P13-1136,N04-1001,0,0.0120169,"and parameter tuning for the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3 . As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3 Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40"
P13-1136,P07-2015,0,0.0521182,"Missing"
P13-1136,N07-1023,0,0.0698946,", 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-pr"
P13-1136,W09-1802,0,0.0755725,"gned submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley"
P13-1136,N09-1041,0,0.0486933,"sis applications including openended question answering, recommender systems, and summarization of search engine results. As further evidence of its importance, the Document Understanding Conference (DUC) has used queryfocused MDS as its main task since 2004 to foster new research on automatic summarization in the context of users’ needs. To date, most top-performing systems for multi-document summarization—whether queryspecific or not—remain largely extractive: their summaries are comprised exclusively of sentences selected directly from the documents to be summarized (Erkan and Radev, 2004; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-T¨ur, 2011). Despite their simplicity, extractive approaches have some disadvantages. First, lengthy sentences that are partly relevant are either excluded from the summary or (if selected) can block the selection of other important sentences, due to summary length constraints. In addition, when people write summaries, they tend to abstract the content and seldom use entire sentences taken verbatim from the original documents. In news articles, for example, most sentences are lengthy and contain both potentially useful information for a summary as well as unnecessary"
P13-1136,P11-1052,0,0.00996893,"pervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has bee"
P13-1136,C00-1072,0,0.0769395,"we believe we are the first to successfully show that sentence compression can provide statistically significant improvements over pure extraction-based approaches for queryfocused MDS. 2 Related Work Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in the final summary. A wide range of methods have been employed for this task. For unsupervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the sum"
P13-1136,N03-1020,0,0.164068,"Missing"
P13-1136,W03-1101,0,0.0260883,"efully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (single) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust proba"
P13-1136,H05-1083,0,0.0143206,". DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3 . As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3 Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt classifiers are used for t"
P13-1136,H05-1115,0,0.0142511,"ments over pure extraction-based approaches for queryfocused MDS. 2 Related Work Existing research on query-focused multidocument summarization (MDS) largely relies on extractive approaches, where systems usually take as input a set of documents and select the top relevant sentences for inclusion in the final summary. A wide range of methods have been employed for this task. For unsupervised methods, sentence importance can be estimated by calculating topic signature words (Lin and Hovy, 2000; Conroy et al., 2006), combining query similarity and document centrality within a graph-based model (Otterbacher et al., 2005), or using a Bayesian model with sophisticated inference (Daum´e and Marcu, 2006). Davis et al. (2012) first learn the term weights by Latent Semantic Analysis, and then greedily select sentences that cover the maximum combined weights. Supervised approaches have mainly focused on applying discriminative learning for ranking sentences (Fuentes et al., 2007). Lin and Bilmes (2011) use a class of carefully designed submodular functions to reward the diversity of the summaries and select sentences greedily. Our work is more related to the less studied area of sentence compression as applied to (s"
P13-1136,P05-1036,0,0.042728,"candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by u"
P13-1136,P04-1018,0,0.0103922,"or the multiscorer. DUC 2006 and DUC 2007 are reserved as held out test sets. Sentence Compression. The dataset from Clarke and Lapata (2008) is used to train the CRF and MaxEnt classifiers (Section 4). It includes 82 newswire articles with one manually produced compression aligned to each sentence. Preprocessing. Documents are processed by a full NLP pipeline, including token and sentence segmentation, parsing, semantic role labeling, and an information extraction pipeline consisting of mention detection, NP coreference, crossdocument resolution, and relation detection (Florian et al., 2004; Luo et al., 2004; Luo and Zitouni, 2005). Learning for Sentence Ranking and Compression. We use Weka (Hall et al., 2009) to train a support vector regressor and experiment with various rankers in RankLib (Dang, 2011)3 . As LambdaMART has an edge over other rankers on the held-out dataset, we selected it to produce ranked sentences for further processing. For sequencebased compression using CRFs, we employ Mallet (McCallum, 2002) and integrate the Table 2 rules during inference. NLTK (Bird et al., 2009) 3 Default parameters are used. If an algorithm needs a validation set, we use 10 out of 40 topics. MaxEnt cl"
P13-1136,W09-1801,0,0.781754,"e) document summarization. Zajic et al. (2006) tackle the query-focused MDS problem using a compress-first strategy: they develop heuristics to generate multiple alternative compressions of all sentences in the original document; these then become the candidates for extraction. This approach, however, does not outperform some extractionbased approaches. A similar idea has been studied for MDS (Lin, 2003; Gillick and Favre, 2009), 1385 but limited improvement is observed over extractive baselines with simple compression rules. Finally, although learning-based compression methods are promising (Martins and Smith, 2009; Berg-Kirkpatrick et al., 2011), it is unclear how well they handle issues of redundancy. Our research is also inspired by probabilistic sentence-compression approaches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tre"
P13-1136,E06-1038,0,0.216493,"ches, such as the noisy-channel model (Knight and Marcu, 2000; Turner and Charniak, 2005), and its extension via synchronous context-free grammars (SCFG) (Aho and Ullman, 1969; Lewis and Stearns, 1968) for robust probability estimation (Galley and McKeown, 2007). Rather than attempt to derive a new parse tree like Knight and Marcu (2000) and Galley and McKeown (2007), we learn to safely remove a set of constituents in our parse tree-based compression model while preserving grammatical structure and essential content. Sentence-level compression has also been examined via a discriminative model McDonald (2006), and Clarke and Lapata (2008) also incorporate discourse information by using integer linear programming. 3 The Framework We now present our query-focused MDS framework consisting of three steps: Sentence Ranking, Sentence Compression and Post-processing. First, sentence ranking determines the importance of each sentence given the query. Then, a sentence compressor iteratively generates the most likely succinct versions of the ranked sentences, which are cumulatively added to the summary, until a length limit is reached. Finally, the postprocessing stage applies coreference resolution and sen"
P13-1136,N04-1019,0,0.0713979,"edundancy within the summary through compression. Furthermore, our H EAD-driven beam search method with M ULTI-scorer beats all systems on DUC 20066 and all systems on DUC 2007 except the best system in terms of R-2 (p < 0.01). Its R-SU4 score is also significantly (p < 0.01) better than extractive methods, rule-based and sequence-based compression methods on both DUC 2006 and 2007. Moreover, our systems with learning-based compression have considerable compression rates, indicating their capability to remove superfluous words as well as improve summary quality. Human Evaluation. The Pyramid (Nenkova and Passonneau, 2004) evaluation was developed to manually assess how many relevant facts or Summarization Content Units (SCUs) are captured by system summaries. We ask a professional annotator (who is not one of the authors, is highly experienced in annotating for various NLP tasks, and is fluent in English) to carry out a Pyramid evaluation on 10 randomly selected topics from 4 We looked at various beam sizes on the heldout data, and observed that the performance peaks around this value. 5 ROUGE-1.5.5.pl -n 4 -w 1.2 -m -2 4 -u -c 95 -r 1000 -f A -p 0.5 -t 0 -a -d 6 The system output from Davis et al. (2012) is n"
P13-1137,D10-1049,0,0.0275358,"the meeting transcripts. On the contrary, the manually composed summaries (abstracts) are more compact and readable, and are written in a distinctly non-conversational style. 1395 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1395–1405, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows u"
P13-1137,N03-1003,0,0.0149501,"us to identify and reformulate the important information for the focused summary. Our contributions are as follows: • To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings. • We present a novel template extraction algorithm, based on Multiple Sequence Alignment (MSA) (Durbin et al., 1998), to induce domain-independent templates that guide abstract generation. MSA is commonly used in bioinformatics to identify equivalent fragments of DNAs (Durbin et al., 1998) and has also been employed for learning paraphrases (Barzilay and Lee, 2003). • Although our framework requires labeled training data for each type of focused summary (decisions, problems, etc.), we also make initial tries for domain adaptation so that our summarization method does not need human-written abstracts for each new meeting domain (e.g. faculty meetings, theater group meetings, project group meetings). We instantiate the abstract generation framework on two corpora from disparate domains — the AMI Meeting Corpus (Mccowan et al., 2005) and ICSI Meeting Corpus (Janin et al., 2003) — and produce systems to generate focused summaries with regard to four types o"
P13-1137,W09-3934,0,0.189992,"ng as a whole, they refer to summaries of a specific aspect of a meeting, such as the DECISIONS reached, PROBLEMS discussed, PROGRESS made or AC TION ITEMS that emerged (Carenini et al., 2011). Our goal is to provide an automatic summarization system that can generate abstract-style focused meeting summaries to help users digest the vast amount of meeting content in an easy manner. Existing meeting summarization systems remain largely extractive: their summaries are comprised exclusively of patchworks of utterances selected directly from the meetings to be summarized (Riedhammer et al., 2010; Bui et al., 2009; Xie et al., 2008). Although relatively easy to construct, extractive approaches fall short of producing concise and readable summaries, largely due Figure 1: Clips from the AMI meeting corpus (Mccowan et al., 2005). A, B, C and D refer to distinct speakers. Also shown is the gold-standard (manual) abstract (summary) for the decision and the problem. to the noisy, fragmented, ungrammatical and unstructured text of meeting transcripts (Murray et al., 2010b; Liu and Liu, 2009). In contrast, human-written meeting summaries are typically in the form of abstracts — distillations of the original co"
P13-1137,P11-1054,0,0.0516102,"er of content words that are also in previous DA indicator/argument only contains stopword? number of new nouns Content Features has capitalized word? has proper noun? TF/IDF/TFIDF min/max/average Discourse Features main speaker or not? is in an adjacency pair (AP)? is in the source/target of the AP? number of source/target DA in the AP is the target of the AP a positive/negative/neutral response? is the source of the AP a question? Syntax Features indicator/argument constituent tag dependency relation of indicator and argument tion instances, then, are represented by indicatorargument pairs (Chen et al., 2011). For example, in the DA cluster of Figure 2, hwant, an LCD display with a spinning wheeli and hpush-buttons, on the outsidei are two relation instances. Relation Instance Extraction We adopt and extend the syntactic constraints from Wang and Cardie (2012) to identify all relation instances in the input utterances; the summary-worthy ones will be selected by a discriminative classifier. Constituent and dependency parses are obtained by the Stanford parser (Klein and Manning, 2003). Both the indicator and argument take the form of constituents in the parse tree. We restrict the eligible indicat"
P13-1137,W06-1643,0,0.108062,"or full dialogues (Carenini et al., 2011). Only recently has the task of focused summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation step according to their type, such as a decision or problem. Then an integer linear programming approach is employed to select the utterances that cover more entities as 1396 Dialogue Acts: C: Looking at what we've got, we we want [an LCD display with a spinning wheel]. B: You have to have some push-buttons, don't you? C: Just spinning and not sc"
P13-1137,N10-1086,0,0.0580777,"2013. 2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows us to identify and reformulate the important information for the focused summary. Our contributions are as follows: • To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings. • We present a novel template extraction algorithm, based on Multipl"
P13-1137,P03-1054,0,0.0576942,"uent tag dependency relation of indicator and argument tion instances, then, are represented by indicatorargument pairs (Chen et al., 2011). For example, in the DA cluster of Figure 2, hwant, an LCD display with a spinning wheeli and hpush-buttons, on the outsidei are two relation instances. Relation Instance Extraction We adopt and extend the syntactic constraints from Wang and Cardie (2012) to identify all relation instances in the input utterances; the summary-worthy ones will be selected by a discriminative classifier. Constituent and dependency parses are obtained by the Stanford parser (Klein and Manning, 2003). Both the indicator and argument take the form of constituents in the parse tree. We restrict the eligible indicator to be a noun or verb; the eligible arguments is a noun phrase (NP), prepositional phrase (PP) or adjectival phrase (ADJP). A valid indicator-argument pair should have at least one content word and satisfy one of the following constraints: • When the indicator is a noun, the argument has to be a modifier or complement of the indicator. • When the indicator is a verb, the argument has to be the subject or the object if it is an NP, or a modifier or complement of the indicator if"
P13-1137,P12-1039,0,0.0497661,"pts. On the contrary, the manually composed summaries (abstracts) are more compact and readable, and are written in a distinctly non-conversational style. 1395 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 1395–1405, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows us to identify and reformula"
P13-1137,N10-1134,0,0.0793183,"Missing"
P13-1137,N03-1020,0,0.106053,"also compare our approach to two supervised extractive summarization methods — Support Vector Machines (Joachims, 1998) trained with the same fea1401 tures as our system (see Table 1) to identify the important DAs (no syntax features) (Xie et al., 2008; Sandu et al., 2010) or tokens (Fern´andez et al., 2008) to include into the summary4 . Oracle. We compute an oracle consisting of the words from the DA cluster that also appear in the associated abstract to reflect the gap between the best possible extracts and the human abstracts. 7 Results Content Selection Evaluation. We first employ ROUGE (Lin and Hovy, 2003) to evaluate the content selection component with respect to the human written abstracts. ROUGE computes the ngram overlapping between the system summaries with the reference summaries, and has been used for both text and speech summarization (Dang, 2005; Xie et al., 2008). We report ROUGE-2 (R2) and ROUGE-SU4 (R-SU4) that are shown to correlate with human evaluation reasonably well. In AMI, four meetings of different functions are carried out in each group5 . 35 meetings for “conceptual design” are randomly selected for testing. For ICSI, we reserve 12 meetings for testing. The R-SU4 scores f"
P13-1137,P09-2066,0,0.0267631,"ed exclusively of patchworks of utterances selected directly from the meetings to be summarized (Riedhammer et al., 2010; Bui et al., 2009; Xie et al., 2008). Although relatively easy to construct, extractive approaches fall short of producing concise and readable summaries, largely due Figure 1: Clips from the AMI meeting corpus (Mccowan et al., 2005). A, B, C and D refer to distinct speakers. Also shown is the gold-standard (manual) abstract (summary) for the decision and the problem. to the noisy, fragmented, ungrammatical and unstructured text of meeting transcripts (Murray et al., 2010b; Liu and Liu, 2009). In contrast, human-written meeting summaries are typically in the form of abstracts — distillations of the original conversation written in new language. A user study from Murray et al. (2010b) showed that people demonstrate a strong preference for abstractive summaries over extracts when the text to be summarized is conversational. Consider, for example, the two types of focused summary along with their associated dialogue snippets in Figure 1. We can see that extracts are likely to include unnecessary and noisy information from the meeting transcripts. On the contrary, the manually compose"
P13-1137,W10-4211,0,0.0247195,"the final summary (see Section 5.3). determined by an external ontology. Liu and Liu (2009) apply sentence compression on extracted summary utterances. Though some of the unnecessary words are dropped, the resulting compressions can still be ungrammatical and unstructured. This work is also broadly related to expert system-based language generation (Reiter and Dale, 2000) and concept-to-text generation tasks (Angeli et al., 2010; Konstas and Lapata, 2012), where the generation process is decomposed into content selection (or text planning) and surface realization. For instance, Angeli et al. (2010) learn from structured database records and parallel textual descriptions. They generate texts based on a series of decisions made to select the records, fields, and proper templates for rendering. Those techniques that are tailored to specific domains (e.g. weather forecasts or sportcastings) cannot be directly applied to the conversational data, as their input is well-structured and the templates learned are domain-specific. 3 Framework Our domain-independent abstract generation framework produces a summarizer that generates a grammatical abstract from a cluster of meeting-element-related di"
P13-1137,N01-1003,0,0.1162,"Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics To address the limitations of extract-based summaries, we propose a complete and fully automatic domain-independent abstract generation framework for focused meeting summarization. Following existing language generation research (Angeli et al., 2010; Konstas and Lapata, 2012), we first perform content selection: given the dialogue acts relevant to one element of the meeting (e.g. a single decision or problem), we train a classifier to identify summary-worthy phrases. Next, we develop an “overgenerate-and-rank” strategy (Walker et al., 2001; Heilman and Smith, 2010) for surface realization, which generates and ranks candidate sentences for the abstract. After redundancy reduction, the full meeting abstract can thus comprise the focused summary for each meeting element. As described in subsequent sections, the generation framework allows us to identify and reformulate the important information for the focused summary. Our contributions are as follows: • To the best of our knowledge, our system is the first fully automatic system to generate natural language abstracts for spoken meetings. • We present a novel template extraction a"
P13-1137,W11-0503,1,0.882524,"usterings setting, we use the annotations to create perfect partitions of the DAs for input to the system; in the System Figure 4: Content selection evaluation by using ROUGE-SU4 (multiplied by 100). SVM-DA and SVM-T OKEN denotes for supervised extract-based methods with SVMs on utterance- and token-level. Summaries for decision, problem, action item, and progress are generated and evaluated for AMI and ICSI (with names in parentheses). X-axis shows the number of meetings used for training. Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in (Wang and Cardie, 2011). DAs are grouped according to a classifier trained beforehand. Baselines and Comparisons. We compare our system with (1) two unsupervised baselines, (2) two supervised extractive approaches, and (3) an oracle derived from the gold standard abstracts. Baselines. As in Riedhammer et al. (2010), the L ONGEST DA in each cluster is selected as the summary. The second baseline picks the cluster prototype (i.e. the DA with the largest TFIDF similarity with the cluster centroid) as the summary according to Wang and Cardie (2011). Although it is possible that important content is spread over multiple"
P13-1137,W12-1642,1,0.897104,"rom the same system trained on in-domain data, and statistically significantly outperform supervised extractive summarization approaches trained on in-domain data. 2 Related Work Most research on spoken dialogue summarization attempts to generate summaries for full dialogues (Carenini et al., 2011). Only recently has the task of focused summarization been studied. Supervised methods are investigated to identify key phrases or utterances for inclusion in the decision summary (Fern´andez et al., 2008; Bui et al., 2009). Based on Fern´andez et al. (2008), a relation representation is proposed by Wang and Cardie (2012) to form structured summaries; we adopt this representation here for content selection. Our research is also in line with generating abstractive summaries for conversations. Extractive approaches (Murray et al., 2005; Xie et al., 2008; Galley, 2006) have been investigated extensively in conversation summarization. Murray et al. (2010a) present an abstraction system consisting of interpretation and transformation steps. Utterances are mapped to a simple conversation ontology in the interpretation step according to their type, such as a decision or problem. Then an integer linear programming app"
P13-1137,P02-1040,0,0.0912979,"baselines and supervised systems. The learning curve of our system is relatively flat, which means not many training meetings are required to reach a usable performance level. Note that the ROUGE scores are relative low when the reference summaries are human abstracts, even for evaluation among abstracts produced by different annotators (Dang, 2005). The intrinsic difference of styles between dialogue and human abstract further lowers the scores. But the trend is still respected among the systems. Abstract Generation Evaluation. To evaluate the full abstract generation system, the BLEU score (Papineni et al., 2002) (the precision of unigrams and bigrams with a brevity penalty) is computed with human abstracts as reference. BLEU has a fairly good agreement with human judgement and has been used to evaluate a variety of language generation systems (Angeli et al., 2010; Konstas and Lapata, 2012). 4 We use SVMlight (Joachims, 1999) with RBF kernel by default parameters for SVM-based classifiers and regressor. 5 The four types of meetings in AMI are: project kick-off (35 meetings), functional design (35 meetings), conceptual design (35 meetings), and detailed design (34 meetings). Figure 5: Full abstract gen"
P13-1137,W10-2603,0,0.0757588,"ter prototype (i.e. the DA with the largest TFIDF similarity with the cluster centroid) as the summary according to Wang and Cardie (2011). Although it is possible that important content is spread over multiple DAs, both baselines allow us to determine summary quality when summaries are restricted to a single utterance. Supervised Learning. We also compare our approach to two supervised extractive summarization methods — Support Vector Machines (Joachims, 1998) trained with the same fea1401 tures as our system (see Table 1) to identify the important DAs (no syntax features) (Xie et al., 2008; Sandu et al., 2010) or tokens (Fern´andez et al., 2008) to include into the summary4 . Oracle. We compute an oracle consisting of the words from the DA cluster that also appear in the associated abstract to reflect the gap between the best possible extracts and the human abstracts. 7 Results Content Selection Evaluation. We first employ ROUGE (Lin and Hovy, 2003) to evaluate the content selection component with respect to the human written abstracts. ROUGE computes the ngram overlapping between the system summaries with the reference summaries, and has been used for both text and speech summarization (Dang, 2005"
P13-1137,N10-1132,0,\N,Missing
P14-2113,W11-0707,0,0.428721,"Sentiment Features. We gather connectives from the Penn Discourse TreeBank (Rashmi Prasad and Webber, 2008) and combine them with any sentiment word that precedes or follows it as new features. Sentiment dependency relations are the dependency relations that include a sentiment word. We replace those words with their polarity equivalents. For example, relation “nsubj(wrong, you)” becomes “nsubj(SentiWordneg , you)”. 4 4.1 Online Dispute Detection Training A Sentiment Classifier Dataset. We train the sentiment classifier using the Authority and Alignment in Wikipedia Discussions (AAWD) corpus (Bender et al., 2011) on a 5point scale (i.e. NN, N, O, P, PP). AAWD consists of 221 English Wikipedia discussions with positive and negative alignment annotations. Annotators either label each sentence as positive, negative or neutral, or label the full turn. For instances that have only a turn-level label, we assume all sentences have the same label as the turn. We further transform the labels into the five sentiment labels. Sentences annotated as being a positive alignment by at least two annotators are treated as very positive (PP). If a sentence is only selected as positive by one annotator or obtains the lab"
P14-2113,esuli-sebastiani-2006-sentiwordnet,0,0.0174906,"con). Concretely, we take a lexicon M = Mp ∪Mn , where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive and negative sentiment. Assume µhσ,wi encodes the weight between label σ and feature w, for each feature w ∈ Mp ; then the isotonic CRF enforces σ ≤ σ 0 ⇒ µhσ,wi ≤ µhσ0 ,wi . For example, when “totally agree” is observed in training, parameter µhPP,totally agreei is likely to increase. Similar constraints are defined on Mn . Our lexicon is built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons. Words with contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. “nsubj(wrong, you)” is generalized to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Sentiment Features. We gather connectives from the Penn Discourse TreeBank (Rashmi Prasad and Webber,"
P14-2113,W10-3001,0,0.0252497,"Missing"
P14-2113,P04-1085,0,0.0539025,"ages (3609 disputes, 3609 nondisputes).3 We find that classifiers that employ the learned sentiment features outperform others that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80 on the Wikipedia dispute corpus. To the best of our knowledge, this represents the first computational approach to automatically identify online disputes on a dataset of scale. Additional Related Work. Sentiment analysis has been utilized as a key enabling technique in a number of conversation-based applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using variants of Conditional Random Fields (Lafferty et al., 2001) and predicts sentiment at the turn-level, while our predictions are made for each sentence. 2 Data Construction: A Dispute Corpus We construct the first dispute detection corpus to date; it consists of dispute and non-dispute discussions from Wikipedia Talk pages. Step 1: Get Talk Pages of Disputed Articles. Wikipedia articles are edited by different editors. If an article is observed to have disputes on its talk page, editors can assign dispute tags to the ar"
P14-2113,N06-2014,0,0.155289,"a”. Omitted sentences are indicated by ellipsis. Names of editors are in bold. The start of each set of related turns is numbered; “&gt;” is an indicator for the reply structure. presumably be tagged as a negative sentence as should the sarcastic sentences “Sounds good?” (in the same turn) and “congrats” and “thank you” (in segment 2). We expect that these, and other, examples will be difficult for the sentence-level classifier unless the discourse context of each sentence is considered. Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).2 As the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) (Mao and Lebanon, 2007) for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism for encoding domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on model parameters. We evaluate our dispute detection approach using a newly created corpus of discussions from Wikipedia Talk pages (3609 dis"
P14-2113,D10-1121,0,0.162,"ies: C ONTROVERSY, R EQUEST FOR C OM MENT (RFC), and R ESOLVED based on the tags found in discussions (see Table 1). The numbers of discussions for the three types are 42, 3484, and 105, respectively. Note that dispute tags only appear in a small number of articles and talk pages. There may exist other discussions with disputes. Dispute Subcategory Controversy Request for Comment Resolved Wikipedia Tags on Talk pages C ONTROVERSIAL, TOTALLYDISPUTED, D ISPUTED, C ALM TALK, POV RFC Any tag from above + R ESOLVED Table 1: Subcategory for disputes with corresponding tags. 2 A notable exception is Hassan et al. (2010), which identifies sentences containing “attitudes” (e.g. opinions), but does not distinguish them w.r.t. sentiment. Context information is also not considered. 3 The talk page associated with each article records conversations among editors about the article content and allows editors to discuss the writing process, e.g. planning and organizing the content. Note that each discussion in the R ESOLVED class has more than one tag. Step 3: Get Discussions without Disputes. Likewise, we collect non-dispute discussions from 4 http://en.wikipedia.org/wiki/Wikipedia: Requests_for_comment 694 Lexical"
P14-2113,J93-3003,0,0.0716698,"built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons. Words with contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. “nsubj(wrong, you)” is generalized to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Sentiment Features. We gather connectives from the Penn Discourse TreeBank (Rashmi Prasad and Webber, 2008) and combine them with any sentiment word that precedes or follows it as new features. Sentiment dependency relations are the dependency relations that include a sentiment word. We replace those words with their polarity equivalents. For example, relation “nsubj(wrong, you)” becomes “nsubj(SentiWordneg , you)”. 4 4.1 Online Dispute Detection Training A Sentiment Classifier Dataset. We train the sentiment classifier using the Authority and Alignment in Wikipedia Discussions (AAWD) corpus"
P14-2113,P11-2065,0,0.0555638,"at employ the learned sentiment features outperform others that do not. The best model achieves a very promising F1 score of 0.78 and an accuracy of 0.80 on the Wikipedia dispute corpus. To the best of our knowledge, this represents the first computational approach to automatically identify online disputes on a dataset of scale. Additional Related Work. Sentiment analysis has been utilized as a key enabling technique in a number of conversation-based applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using variants of Conditional Random Fields (Lafferty et al., 2001) and predicts sentiment at the turn-level, while our predictions are made for each sentence. 2 Data Construction: A Dispute Corpus We construct the first dispute detection corpus to date; it consists of dispute and non-dispute discussions from Wikipedia Talk pages. Step 1: Get Talk Pages of Disputed Articles. Wikipedia articles are edited by different editors. If an article is observed to have disputes on its talk page, editors can assign dispute tags to the article to flag it for attention. In this research, we are interested"
P14-2113,H05-1044,0,0.0116014,"l structure and domain knowledge (e.g. word-level sentiment conveyed via a lexicon). Concretely, we take a lexicon M = Mp ∪Mn , where Mp and Mn are two sets of features (usually words) identified as strongly associated with positive and negative sentiment. Assume µhσ,wi encodes the weight between label σ and feature w, for each feature w ∈ Mp ; then the isotonic CRF enforces σ ≤ σ 0 ⇒ µhσ,wi ≤ µhσ0 ,wi . For example, when “totally agree” is observed in training, parameter µhPP,totally agreei is likely to increase. Similar constraints are defined on Mn . Our lexicon is built by combining MPQA (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006) lexicons. Words with contradictory sentiments are removed. We use the features in Table 2 for sentiment prediction. Syntactic/Semantic Features. We have two versions of dependency relation features, the original form and a form that generalizes a word to its POS tag, e.g. “nsubj(wrong, you)” is generalized to “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. Discourse Features. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Sentiment Featur"
P14-2113,W12-3710,0,0.175666,"es are indicated by ellipsis. Names of editors are in bold. The start of each set of related turns is numbered; “&gt;” is an indicator for the reply structure. presumably be tagged as a negative sentence as should the sarcastic sentences “Sounds good?” (in the same turn) and “congrats” and “thank you” (in segment 2). We expect that these, and other, examples will be difficult for the sentence-level classifier unless the discourse context of each sentence is considered. Previous research on sentiment prediction for online discussions, however, focuses on turn-level predictions (Hahn et al., 2006; Yin et al., 2012).2 As the first work that predicts sentence-level sentiment for online discussions, we investigate isotonic Conditional Random Fields (CRFs) (Mao and Lebanon, 2007) for the sentiment-tagging task as they preserve the advantages of the popular CRF-based sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism for encoding domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on model parameters. We evaluate our dispute detection approach using a newly created corpus of discussions from Wikipedia Talk pages (3609 disputes, 3609 nondisp"
P14-2113,prasad-etal-2008-penn,0,\N,Missing
P17-1090,W09-3934,0,0.0349938,"ative discourse information. As can be seen, meeting participants evaluate different options by showing doubt (UNCERTAIN), bringing up alternative solution (OPTION), or giving feedback. The discourse information helps with the identification of the key discussion point, i.e., “which type of battery to use”, by revealing the discussion flow. To date, most efforts to leverage discourse information to detect salient content from dialogues have focused on encoding gold-standard discourse relations as features for use in classifier training (Murray et al., 2006; Galley, 2006; McKeown et al., 2007; Bui et al., 2009). However, automatic discourse parsing in dialogues is still a challenging problem (Perret et al., 2016). Moreover, acquiring human annotation on discourse relations is a timeconsuming and expensive process, and does not Goal-oriented dialogues, such as meetings, negotiations, or customer service transcripts, play an important role in our daily life. Automatically extracting the critical points and important outcomes from dialogues would facilitate generating summaries for complicated conversations, understanding the decision-making process of meetings, or analyzing the effectiveness of collab"
P17-1090,N16-1037,0,0.28848,"ction task, where summary-worthy content represented as indicator and argument pairs is identified by an unsupervised latent variable model. Our work also targets at detecting salient phrases from meetings, but focuses on the joint modeling of critical discussion points and discourse relations held between them. For the area of discourse analysis in dialogues, a significant amount of work has been done in predicting local discourse structures, such as recognizing dialogue acts or social acts of adjacent utterances from phone conversations (Stolcke et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016), spoken meetings (Dielmann and Renals, 2008), or emails (Cohen et al., 2004). Although discourse information from non-adjacent turns has been studied in the context of online discussion forums (Ghosh et al., 2014) and meetings (HakkaniTur, 2009), none of them models the effect of discourse structure on content selection, which is a gap that this work fills in. 3 The Joint Model of Content and Discourse Relations In this section, we first present our joint model in Section 3.1. The algorithms for learning and inference are described in Sections 3.2 and 3.3, followed by feature description (Sec"
P17-1090,W04-3240,0,0.156749,"Missing"
P17-1090,W13-3214,0,0.0280603,"problem as an information extraction task, where summary-worthy content represented as indicator and argument pairs is identified by an unsupervised latent variable model. Our work also targets at detecting salient phrases from meetings, but focuses on the joint modeling of critical discussion points and discourse relations held between them. For the area of discourse analysis in dialogues, a significant amount of work has been done in predicting local discourse structures, such as recognizing dialogue acts or social acts of adjacent utterances from phone conversations (Stolcke et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016), spoken meetings (Dielmann and Renals, 2008), or emails (Cohen et al., 2004). Although discourse information from non-adjacent turns has been studied in the context of online discussion forums (Ghosh et al., 2014) and meetings (HakkaniTur, 2009), none of them models the effect of discourse structure on content selection, which is a gap that this work fills in. 3 The Joint Model of Content and Discourse Relations In this section, we first present our joint model in Section 3.1. The algorithms for learning and inference are described in Sections 3.2 and 3.3, followed by featur"
P17-1090,P03-1054,0,0.00663464,"ractive summarization baselines and a keyword selection algorithm proposed in Liu et al. (2009). Moreover, since both content and discourse structure are critical for building shared understanding among participants (Mulder et al., 2002; 975 Hakkani-Tur, 2009), we assume that the attachment structure between discourse units are given during both training and testing. A set of candidate phrases are extracted from each discourse unit xi , from which salient phrases that contain gist information will be identified. We obtain constituent and dependency parses for utterances using Stanford parser (Klein and Manning, 2003). We restrict eligible candidate to be a noun phrase (NP), verb phrase (VP), prepositional phrase (PP), or adjective phrase (ADJP) with at most 5 words, and its head word cannot be a stop word.1 If a candidate is a parent of another candidate in the constituent parse tree, we will only keep the parent. We further merge a verb and a candidate noun phrase into one candidate if the later is the direct object or subject of the verb. For example, from utterance “let’s use a rubber case as well as rubber buttons”, we can identify candidates “use a rubber case” and “rubber buttons”. For xi , the set"
P17-1090,W06-1643,0,0.151757,"l. (2005), which focuses on argumentative discourse information. As can be seen, meeting participants evaluate different options by showing doubt (UNCERTAIN), bringing up alternative solution (OPTION), or giving feedback. The discourse information helps with the identification of the key discussion point, i.e., “which type of battery to use”, by revealing the discussion flow. To date, most efforts to leverage discourse information to detect salient content from dialogues have focused on encoding gold-standard discourse relations as features for use in classifier training (Murray et al., 2006; Galley, 2006; McKeown et al., 2007; Bui et al., 2009). However, automatic discourse parsing in dialogues is still a challenging problem (Perret et al., 2016). Moreover, acquiring human annotation on discourse relations is a timeconsuming and expensive process, and does not Goal-oriented dialogues, such as meetings, negotiations, or customer service transcripts, play an important role in our daily life. Automatically extracting the critical points and important outcomes from dialogues would facilitate generating summaries for complicated conversations, understanding the decision-making process of meetings,"
P17-1090,W14-2106,0,0.021882,"focuses on the joint modeling of critical discussion points and discourse relations held between them. For the area of discourse analysis in dialogues, a significant amount of work has been done in predicting local discourse structures, such as recognizing dialogue acts or social acts of adjacent utterances from phone conversations (Stolcke et al., 2000; Kalchbrenner and Blunsom, 2013; Ji et al., 2016), spoken meetings (Dielmann and Renals, 2008), or emails (Cohen et al., 2004). Although discourse information from non-adjacent turns has been studied in the context of online discussion forums (Ghosh et al., 2014) and meetings (HakkaniTur, 2009), none of them models the effect of discourse structure on content selection, which is a gap that this work fills in. 3 The Joint Model of Content and Discourse Relations In this section, we first present our joint model in Section 3.1. The algorithms for learning and inference are described in Sections 3.2 and 3.3, followed by feature description (Section 3.4). 3.1 p(c, d|x, w) ∝ exp[w · Φ(c, d, x)] n X ∝ exp[w · φ(ci , di , di0 , x)] Model Description Our proposed model learns to jointly perform phrase-based content selection and discourse relation prediction"
P17-1090,N03-1020,0,0.392365,"Missing"
P17-1090,N09-1070,0,0.497989,"Machines (SVM). Our discourse prediction component also obtains better accuracy than a state-of-the-art neural networkbased approach (59.2 vs. 54.2). Moreover, our model trained with latent discourse outperforms SVMs on both AMI and ICSI corpora for phrase selection. We further evaluate the usage of selected phrases as extractive meeting summaries. Results evaluated by ROUGE (Lin and Hovy, 2003) demonstrate that our system summaries obtain a ROUGE-SU4 F1 score of 21.3 on AMI corpus, which outperforms non-trivial extractive summarization baselines and a keyword selection algorithm proposed in Liu et al. (2009). Moreover, since both content and discourse structure are critical for building shared understanding among participants (Mulder et al., 2002; 975 Hakkani-Tur, 2009), we assume that the attachment structure between discourse units are given during both training and testing. A set of candidate phrases are extracted from each discourse unit xi , from which salient phrases that contain gist information will be identified. We obtain constituent and dependency parses for utterances using Stanford parser (Klein and Manning, 2003). We restrict eligible candidate to be a noun phrase (NP), verb phrase"
P17-1090,W14-4318,0,0.0617743,"t leverages discourse structure for identifying salient content in conversations, which is still largely reliant on features derived from gold-standard discourse labels (McKeown et al., 2007; Murray et al., 2010; Bokaei et al., 2016). For instance, adjacency pairs, which are paired utterances with question-answer or offer-accept relations, are found to frequently appear in meeting summaries together and thus are utilized to extract summary-worthy utterances by Galley (2006). There is much less work that jointly predicts the importance of content along with the discourse structure in dialogus. Oya and Carenini (2014) employs Dynamic Conditional Random Field to recognize sentences in email threads for use in summary as well as their dialogue acts. Only local discourse structures from adjacent utterances are considered. Our model is built on tree structures, which captures more global information. Our work is also in line with keyphrase identification or phrase-based summarization for conversations. Due to the noisy nature of dialogues, recent work focuses on identifying summary-worthy phrases from meetings (Fern´andez et al., 2008; Riedhammer et al., 2010) or email threads (Loza To the best of our knowledg"
P17-1090,N16-1013,0,0.0592446,"ng doubt (UNCERTAIN), bringing up alternative solution (OPTION), or giving feedback. The discourse information helps with the identification of the key discussion point, i.e., “which type of battery to use”, by revealing the discussion flow. To date, most efforts to leverage discourse information to detect salient content from dialogues have focused on encoding gold-standard discourse relations as features for use in classifier training (Murray et al., 2006; Galley, 2006; McKeown et al., 2007; Bui et al., 2009). However, automatic discourse parsing in dialogues is still a challenging problem (Perret et al., 2016). Moreover, acquiring human annotation on discourse relations is a timeconsuming and expensive process, and does not Goal-oriented dialogues, such as meetings, negotiations, or customer service transcripts, play an important role in our daily life. Automatically extracting the critical points and important outcomes from dialogues would facilitate generating summaries for complicated conversations, understanding the decision-making process of meetings, or analyzing the effectiveness of collaborations. We are interested in a specific type of dialogues — spoken meetings, which is a common way for"
P17-1090,loza-etal-2014-building,0,0.0617538,"Missing"
P17-1090,P14-1115,0,0.0523278,"Missing"
P17-1090,D08-1016,0,0.0293186,"Missing"
P17-1090,J00-3003,0,0.811754,"Missing"
P17-1090,W10-4211,0,0.033794,"or learning. We envision that the extracted salient phrases by our model can be used as input to abstractive meeting summarization systems (Wang and Cardie, 2013; Mehdad et al., 2014). Combined with the predicted discourse structure, a visualization tool can be exploited to display conversation flow to support intelligent meeting assistant systems. 2 Related Work Our model is inspired by research work that leverages discourse structure for identifying salient content in conversations, which is still largely reliant on features derived from gold-standard discourse labels (McKeown et al., 2007; Murray et al., 2010; Bokaei et al., 2016). For instance, adjacency pairs, which are paired utterances with question-answer or offer-accept relations, are found to frequently appear in meeting summaries together and thus are utilized to extract summary-worthy utterances by Galley (2006). There is much less work that jointly predicts the importance of content along with the discourse structure in dialogus. Oya and Carenini (2014) employs Dynamic Conditional Random Field to recognize sentences in email threads for use in summary as well as their dialogue acts. Only local discourse structures from adjacent utterance"
P17-1090,W12-1642,1,0.919262,"mple, from utterance “let’s use a rubber case as well as rubber buttons”, we can identify candidates “use a rubber case” and “rubber buttons”. For xi , the set of candidate phrases are denoted as ci = {ci,1 , ci,2 , · · · , ci,mi }, where mi is the number of candidates. ci,j takes a value of 1 if the corresponding candidate is selected as salient phrase; otherwise, ci,j is equal to 0. All candidate phrases in discussion x are represented as c. We then define a log-linear model with feature parameters w for the candidate phrases c and discourse relations d in x as: et al., 2014). For instance, Wang and Cardie (2012) treat the problem as an information extraction task, where summary-worthy content represented as indicator and argument pairs is identified by an unsupervised latent variable model. Our work also targets at detecting salient phrases from meetings, but focuses on the joint modeling of critical discussion points and discourse relations held between them. For the area of discourse analysis in dialogues, a significant amount of work has been done in predicting local discourse structures, such as recognizing dialogue acts or social acts of adjacent utterances from phone conversations (Stolcke et a"
P17-1090,N06-1047,0,0.653855,"(TAS) by Rienks et al. (2005), which focuses on argumentative discourse information. As can be seen, meeting participants evaluate different options by showing doubt (UNCERTAIN), bringing up alternative solution (OPTION), or giving feedback. The discourse information helps with the identification of the key discussion point, i.e., “which type of battery to use”, by revealing the discussion flow. To date, most efforts to leverage discourse information to detect salient content from dialogues have focused on encoding gold-standard discourse relations as features for use in classifier training (Murray et al., 2006; Galley, 2006; McKeown et al., 2007; Bui et al., 2009). However, automatic discourse parsing in dialogues is still a challenging problem (Perret et al., 2016). Moreover, acquiring human annotation on discourse relations is a timeconsuming and expensive process, and does not Goal-oriented dialogues, such as meetings, negotiations, or customer service transcripts, play an important role in our daily life. Automatically extracting the critical points and important outcomes from dialogues would facilitate generating summaries for complicated conversations, understanding the decision-making proces"
P17-1090,P13-1137,1,0.93697,"Missing"
P17-1090,P08-2043,0,0.0326816,"nt” ones in the training set, with learned parameters wcon and wincon . For a discussion in the test set, these two models output two probabilities pcon = maxc,d P (c, d|x, wcon ) and pincon = maxc,d P (c, d|x, wincon ). We use pcon − pincon as a feature. Furthermore, we consider discourse relations of length one and two from the discourse structure tree. Intuitively, some discourse relations, e.g., ELABORATION followed by multiple POSI TIVE feedback, imply consistent understanding. The third feature is based on word entrainment, which has been shown to correlate with task success for groups (Nenkova et al., 2008). Using the formula in Nenkova et al. (2008), we compute the average word entrainment between the main speaker who utters the most words and all the other participants. The content words in the salient phrases predicted by our model is considered for entrainment computation. Results. Leave-one-out is used for experiments. For training, our features are constructed from gold-standard phrase and discourse labels. Predicted labels by our model is used for constructing features during testing. SVM-based classifier is used for experimenting with different sets of features output by our model. A maj"
P17-2032,C16-1324,0,0.0738353,"Missing"
P17-2032,P02-1040,0,0.121913,"Missing"
P17-2032,W14-2105,0,0.074756,"or argument construction), we aim to automatically pinpoint the sentence(s) (in italics) among all sentences in the cited article that can be used to back up the claim. We define such tasks as supporting argument detection. Furthermore, another goal of Introduction Argumentation plays a crucial role in persuasion and decision-making processes. An argument usually consists of a central claim (or conclusion) and several supporting premises. Constructing arguments of high quality would require the inclusion of diverse information, such as factual evidence and solid reasoning (Rieke et al., 1997; Park and Cardie, 2014). For instance, as shown in Figure 1, the editor on idebate.org – a Wikipedia-style website for gathering pro and con arguments on controversial issues, utilizes arguments based on study, factual evidence, and expert opinion to support the anti-gun claim “legally owned guns are frequently stolen and used by criminals”. However, it would require substantial human effort to collect information from diverse resources to support argument construction. In order to facilitate this process, there is a pressing need for tools that can automatically detect supporting arguments. To date, most of the arg"
P17-2032,D15-1050,0,0.0543643,"MART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information. 1 Figure 1: Three different types of arguments used to support the claim “Legally owned guns are frequently stolen and used by criminals”. and their structures from constructed arguments based on curated corpus (Mochales and Moens, 2011; Stab and Gurevych, 2014; Feng and Hirst, 2011; Habernal and Gurevych, 2015; Nguyen and Litman, 2016). Limited work has been done for retrieving supporting arguments from external resources. Initial effort by Rinott et al. (2015) investigates the detection of relevant factual evidence from Wikipedia articles. However, it is unclear whether their method can perform well on documents of different genres (e.g. news articles vs. blogs) for detecting distinct types of supporting information. In this work, we present a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim. Take Figure 2 as an example: assume we are given a claim on the topic of “banning cosmetic surgery” and a relevant article (cited for argument construction), we aim to automatically pinp"
P17-2032,P11-1099,0,0.0256687,"haracterize arguments of different types, and explore whether leveraging type information can facilitate the supporting arguments detection task. Experimental results show that LambdaMART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information. 1 Figure 1: Three different types of arguments used to support the claim “Legally owned guns are frequently stolen and used by criminals”. and their structures from constructed arguments based on curated corpus (Mochales and Moens, 2011; Stab and Gurevych, 2014; Feng and Hirst, 2011; Habernal and Gurevych, 2015; Nguyen and Litman, 2016). Limited work has been done for retrieving supporting arguments from external resources. Initial effort by Rinott et al. (2015) investigates the detection of relevant factual evidence from Wikipedia articles. However, it is unclear whether their method can perform well on documents of different genres (e.g. news articles vs. blogs) for detecting distinct types of supporting information. In this work, we present a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim. Tak"
P17-2032,D15-1255,0,0.019518,"of different types, and explore whether leveraging type information can facilitate the supporting arguments detection task. Experimental results show that LambdaMART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information. 1 Figure 1: Three different types of arguments used to support the claim “Legally owned guns are frequently stolen and used by criminals”. and their structures from constructed arguments based on curated corpus (Mochales and Moens, 2011; Stab and Gurevych, 2014; Feng and Hirst, 2011; Habernal and Gurevych, 2015; Nguyen and Litman, 2016). Limited work has been done for retrieving supporting arguments from external resources. Initial effort by Rinott et al. (2015) investigates the detection of relevant factual evidence from Wikipedia articles. However, it is unclear whether their method can perform well on documents of different genres (e.g. news articles vs. blogs) for detecting distinct types of supporting information. In this work, we present a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim. Take Figure 2 as an example: ass"
P17-2032,D14-1006,0,0.126567,"REASONING . We further characterize arguments of different types, and explore whether leveraging type information can facilitate the supporting arguments detection task. Experimental results show that LambdaMART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information. 1 Figure 1: Three different types of arguments used to support the claim “Legally owned guns are frequently stolen and used by criminals”. and their structures from constructed arguments based on curated corpus (Mochales and Moens, 2011; Stab and Gurevych, 2014; Feng and Hirst, 2011; Habernal and Gurevych, 2015; Nguyen and Litman, 2016). Limited work has been done for retrieving supporting arguments from external resources. Initial effort by Rinott et al. (2015) investigates the detection of relevant factual evidence from Wikipedia articles. However, it is unclear whether their method can perform well on documents of different genres (e.g. news articles vs. blogs) for detecting distinct types of supporting information. In this work, we present a novel study on the task of sentence-level supporting argument detection from relevant documents for a use"
P17-2032,H05-1044,0,0.0341023,"Missing"
P17-2032,P16-1107,0,0.0304488,"re whether leveraging type information can facilitate the supporting arguments detection task. Experimental results show that LambdaMART (Burges, 2010) ranker that uses features informed by argument types yields better performance than the same ranker trained without type information. 1 Figure 1: Three different types of arguments used to support the claim “Legally owned guns are frequently stolen and used by criminals”. and their structures from constructed arguments based on curated corpus (Mochales and Moens, 2011; Stab and Gurevych, 2014; Feng and Hirst, 2011; Habernal and Gurevych, 2015; Nguyen and Litman, 2016). Limited work has been done for retrieving supporting arguments from external resources. Initial effort by Rinott et al. (2015) investigates the detection of relevant factual evidence from Wikipedia articles. However, it is unclear whether their method can perform well on documents of different genres (e.g. news articles vs. blogs) for detecting distinct types of supporting information. In this work, we present a novel study on the task of sentence-level supporting argument detection from relevant documents for a user-specified claim. Take Figure 2 as an example: assume we are given a claim o"
P17-2032,W04-1013,0,\N,Missing
P18-1021,P17-1002,0,0.0246354,"h there is still a noticeable gap between system arguments and human constructed arguments. As discovered by our prior work (Wang et al., 2017), both topical content and language style are essential elements for high quality arguments. For future work, generation models with a better control on linguistic style need to be designed. As for improving coherence, we believe that discourse-aware generation models (Ji et al., 2016) should also be explored in the future work to enhance text planning. 9 community (Park and Cardie, 2014; Ghosh et al., 2014; Palau and Moens, 2009; Niculae et al., 2017; Eger et al., 2017). While argument understanding has received increasingly more attention, the area of automatic argument generation is much less studied. Early work on argument construction investigates the design of argumentation strategies (Reed et al., 1996; Carenini and Moore, 2000; Zukerman et al., 2000). For instance, Reed (1999) describes the first full natural language argument generation system, called Rhetorica. It however only outputs a text plan, mainly relying on heuristic rules. Due to the difficulty of text generation, none of the previous work represents a fully automated argument generation sy"
P18-1021,D10-1049,0,0.0327833,"nts is a daunting task, for both human and computers. We believe that developing effective argument generation models will enable a broad range of compelling applications, including debate coaching, improving students’ essay writing skills, and pro219 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 219–230 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics plates that are either constructed by rules (Hovy, 1993; Belz, 2008; Bouayad-Agha et al., 2011), or acquired from a domain-specific corpus (Angeli et al., 2010) to enhance grammaticality and coherence. This makes them unwieldy to be adapted for new domains. In this work, we study the following novel problem: given a statement on a controversial issue, generate an argument of an alternative stance. To address the above challenges, we present a neural network-based argument generation framework augmented with externally retrieved evidence. Our model is inspired by the observation that when humans construct arguments, they often collect references from external sources, e.g., Wikipedia or research papers, and then write their own arguments by synthesizi"
P18-1021,W14-2106,0,0.03208,"hile, our model also acquires argumentative style language, though there is still a noticeable gap between system arguments and human constructed arguments. As discovered by our prior work (Wang et al., 2017), both topical content and language style are essential elements for high quality arguments. For future work, generation models with a better control on linguistic style need to be designed. As for improving coherence, we believe that discourse-aware generation models (Ji et al., 2016) should also be explored in the future work to enhance text planning. 9 community (Park and Cardie, 2014; Ghosh et al., 2014; Palau and Moens, 2009; Niculae et al., 2017; Eger et al., 2017). While argument understanding has received increasingly more attention, the area of automatic argument generation is much less studied. Early work on argument construction investigates the design of argumentation strategies (Reed et al., 1996; Carenini and Moore, 2000; Zukerman et al., 2000). For instance, Reed (1999) describes the first full natural language argument generation system, called Rhetorica. It however only outputs a text plan, mainly relying on heuristic rules. Due to the difficulty of text generation, none of the"
P18-1021,P17-2032,1,0.927093,"ommunity that argue against original post’s thesis on “government should be allowed to view private emails”. Both arguments leverage supporting information from Wikipedia articles. viding context of controversial issues from different perspectives. As a consequence, there exists a pressing need for automating the argument construction process. To date, progress made in argument generation has been limited to retrieval-based methods— arguments are ranked based on relevance to a given topic, then the top ones are selected for inclusion in the output (Rinott et al., 2015; Wachsmuth et al., 2017; Hua and Wang, 2017). Although sentence ordering algorithms are developed for information structuring (Sato et al., 2015; Reisert et al., 2015), existing methods lack the ability of synthesizing information from different resources, leading to redundancy and incoherence in the output. In general, the task of argument generation presents numerous challenges, ranging from aggregating supporting evidence to generating text with coherent logical structure. One particular hurdle comes from the underlying natural language generation (NLG) stack, whose success has been limited to a small set of domains. Especially, most"
P18-1021,W11-2810,0,0.0239419,", 2006; Park et al., 2012). Nonetheless, constructing persuasive arguments is a daunting task, for both human and computers. We believe that developing effective argument generation models will enable a broad range of compelling applications, including debate coaching, improving students’ essay writing skills, and pro219 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 219–230 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics plates that are either constructed by rules (Hovy, 1993; Belz, 2008; Bouayad-Agha et al., 2011), or acquired from a domain-specific corpus (Angeli et al., 2010) to enhance grammaticality and coherence. This makes them unwieldy to be adapted for new domains. In this work, we study the following novel problem: given a statement on a controversial issue, generate an argument of an alternative stance. To address the above challenges, we present a neural network-based argument generation framework augmented with externally retrieved evidence. Our model is inspired by the observation that when humans construct arguments, they often collect references from external sources, e.g., Wikipedia or"
P18-1021,W00-1407,0,0.483372,"models with a better control on linguistic style need to be designed. As for improving coherence, we believe that discourse-aware generation models (Ji et al., 2016) should also be explored in the future work to enhance text planning. 9 community (Park and Cardie, 2014; Ghosh et al., 2014; Palau and Moens, 2009; Niculae et al., 2017; Eger et al., 2017). While argument understanding has received increasingly more attention, the area of automatic argument generation is much less studied. Early work on argument construction investigates the design of argumentation strategies (Reed et al., 1996; Carenini and Moore, 2000; Zukerman et al., 2000). For instance, Reed (1999) describes the first full natural language argument generation system, called Rhetorica. It however only outputs a text plan, mainly relying on heuristic rules. Due to the difficulty of text generation, none of the previous work represents a fully automated argument generation system. This work aims to close the gap by proposing an end-to-end trained argument construction framework. Additionally, argument retrieval and extraction are investigated (Rinott et al., 2015; Hua and Wang, 2017) to deliver relevant arguments for user-specified queries"
P18-1021,N16-1037,0,0.0684565,"Missing"
P18-1021,P17-1171,0,0.0205881,"tences: given a statement, (1) constructing one query per sentence and retrieving relevant articles from Wikipedia, and (2) reranking paragraphs and then sentences to create the final set of evidence sentences. Wikipedia is used as our evidence source mainly due to its objective perspective and broad coverage of topics. A dump of December 21, 2016 was downloaded. For training, evidence sentences are retrieved with queries constructed from target user arguments. For test, queries are constructed from OP. Article Retrieval. We first create an inverted index lookup table for Wikipedia as done in Chen et al. (2017). For a given statement, we construct one query per sentence to broaden the diversity of retrieved articles. Therefore, multiple passes of retrieval will be conducted if more than one query is created. Specifically, we first collect topic signature words of the post. Topic signatures (Lin and Hovy, 2000) are terms strongly correlated with a given post, measured by log-likelihood ratio against a background corpus. We treat posts from other discussions in our dataset as background. For each sentence, one query is constructed based on the noun phrases and verbs containing at least one topic signa"
P18-1021,C00-1072,0,0.399185,"verage of topics. A dump of December 21, 2016 was downloaded. For training, evidence sentences are retrieved with queries constructed from target user arguments. For test, queries are constructed from OP. Article Retrieval. We first create an inverted index lookup table for Wikipedia as done in Chen et al. (2017). For a given statement, we construct one query per sentence to broaden the diversity of retrieved articles. Therefore, multiple passes of retrieval will be conducted if more than one query is created. Specifically, we first collect topic signature words of the post. Topic signatures (Lin and Hovy, 2000) are terms strongly correlated with a given post, measured by log-likelihood ratio against a background corpus. We treat posts from other discussions in our dataset as background. For each sentence, one query is constructed based on the noun phrases and verbs containing at least one topic signature word. For instance, a query “the government, my e-mails, 5.2 Gold-Standard Keyphrase Construction To create training data for the keyphrase decoder, we use the following rules to identify keyphrases from evidence sentences that are reused by human writers for argument construction: • Extract noun ph"
P18-1021,W14-3348,0,0.0859325,"ree times per statement, where a statement is an user argument for training data and an OP for test set. In our experiments, we remove duplicates samples and the ones without any retrieved evidence sentence. Finally, we break down the augmented data into a training set of 224,553 examples (9,737 unique OPs), 13,911 for validation (640 OPs), and 30,417 retained for test (1,892 OPs). 6.2 first layer are used to initialize the first layer of all comparison models and our models (except for the keyphrase decoder). Experimental results show that pre-training boosts all methods by roughly 2 METEOR (Denkowski and Lavie, 2014) points. We describe more detailed results in the supplementary material. 6.3 We first consider a R ETRIEVAL-based baseline, which concatenates retrieved evidence sentences to form the argument. We further compare with three seq2seq-based generation models with different training data: (1) SEQ 2 SEQ: training with OP as input and the argument as output; (2) SEQ 2 SEQ + encode evd: augmenting input with evidence sentences as in our model; (3) SEQ 2 SEQ + encode KP: augmenting input with gold-standard keyphrases, which assumes some of the talking points are known. All seq2seq models use a regula"
P18-1021,D14-1162,0,0.0810362,") (4) T e0tj = v 0 tanh(Wp0 spj + Wa0 sat + b0attn ) Notice that two sets of parameters and different state update functions g(·) are learned for separate decoders: {Wha , Wsa , baattn , g a (·)} for the argument decoder; {Whp , Wsp , bpattn , g p (·)} for the keyphrase decoder. Encoder. A two-layer bidirectional LSTM (biLSTM) is used to obtain the encoder hidden states hi for each time step i. For biLSTM, the hidden state is the concatenation of forward and back→ − ← − ward hidden states: hi = [hi ; hi ]. Word representations are initialized with 200-dimensional pre-trained GloVe embeddings (Pennington et al., 2014), and updated during training. The last hidden state of encoder is used to initialize both decoders. In our model the encoder is shared by argument and keyphrase decoders. Decoders. Our model is equipped with two decoders: keyphrase decoder and argument decoder, each is implemented with a separate two-layer unidirectional LSTM, in a similar spirit with one(8) (9) where spj is the hidden state of keyphrase decoder at position j, sat is the hidden state of argument decoder at timestep t, and ct is computed in Eq. 2. Decoder Sharing. We also experiment with a shared decoder between keyphrase gene"
P18-1021,P14-5010,0,0.00540883,"ihood ratio against a background corpus. We treat posts from other discussions in our dataset as background. For each sentence, one query is constructed based on the noun phrases and verbs containing at least one topic signature word. For instance, a query “the government, my e-mails, 5.2 Gold-Standard Keyphrase Construction To create training data for the keyphrase decoder, we use the following rules to identify keyphrases from evidence sentences that are reused by human writers for argument construction: • Extract noun phrases and verb phrases from evidence sentences using Stanford CoreNLP (Manning et al., 2014). • Keep phrases of length between 2 and 10 that overlap with content words in the argument. • If there is span overlap between phrases, the longer one is kept if it has more content word coverage of the argument; otherwise the shorter one is retained. The resultant phrases are then concatenated with a special delimiter &lt;phrase&gt; and used as gold-standard generation for training. 6 6.1 Experimental Setup Final Dataset Statistics Encoding the full set of evidence by our current decoder takes a huge amount of time. We there propose a sampling strategy to allow the encoder to finish encoding withi"
P18-1021,N16-1086,0,0.028735,"lays sample arguments by users from Reddit subcommunity /r/ChangeMyView 1 who argue against the motion that “government should be allowed to view private emails”. Both replies leverage information drawn from Wikipedia, such as “political corruption” and “Fourth Amendment on protections of personal privacy”. Concretely, our neural argument generation model adopts the popular encoder-decoderbased sequence-to-sequence (seq2seq) framework (Sutskever et al., 2014), which has achieved significant success in various text generation tasks (Bahdanau et al., 2015; Wen et al., 2015; Wang and Ling, 2016; Mei et al., 2016; Wiseman et al., 2017). Our encoder takes as input a statement on a disputed issue, and a set of relevant evidence automatically retrieved from English Wikipedia2 . Our decoder consists of two separate parts, one of which first generates keyphrases as intermediate representation of “talking points”, and the other then generates an argument based on both input and keyphrases. Automatic evaluation based on BLEU (Papineni et al., 2002) shows that our framework generates better arguments than directly using retrieved sentences or popular seq2seq-based generation models (Bahdanau et al., 2015) tha"
P18-1021,W15-0507,0,0.176575,"ts leverage supporting information from Wikipedia articles. viding context of controversial issues from different perspectives. As a consequence, there exists a pressing need for automating the argument construction process. To date, progress made in argument generation has been limited to retrieval-based methods— arguments are ranked based on relevance to a given topic, then the top ones are selected for inclusion in the output (Rinott et al., 2015; Wachsmuth et al., 2017; Hua and Wang, 2017). Although sentence ordering algorithms are developed for information structuring (Sato et al., 2015; Reisert et al., 2015), existing methods lack the ability of synthesizing information from different resources, leading to redundancy and incoherence in the output. In general, the task of argument generation presents numerous challenges, ranging from aggregating supporting evidence to generating text with coherent logical structure. One particular hurdle comes from the underlying natural language generation (NLG) stack, whose success has been limited to a small set of domains. Especially, most previous NLG systems rely on temIntroduction Generating high quality arguments plays a crucial role in decision-making and"
P18-1021,D15-1050,0,0.183974,"ser arguments from Reddit Change My View subcommunity that argue against original post’s thesis on “government should be allowed to view private emails”. Both arguments leverage supporting information from Wikipedia articles. viding context of controversial issues from different perspectives. As a consequence, there exists a pressing need for automating the argument construction process. To date, progress made in argument generation has been limited to retrieval-based methods— arguments are ranked based on relevance to a given topic, then the top ones are selected for inclusion in the output (Rinott et al., 2015; Wachsmuth et al., 2017; Hua and Wang, 2017). Although sentence ordering algorithms are developed for information structuring (Sato et al., 2015; Reisert et al., 2015), existing methods lack the ability of synthesizing information from different resources, leading to redundancy and incoherence in the output. In general, the task of argument generation presents numerous challenges, ranging from aggregating supporting evidence to generating text with coherent logical structure. One particular hurdle comes from the underlying natural language generation (NLG) stack, whose success has been limite"
P18-1021,P17-1091,0,0.0348375,"style language, though there is still a noticeable gap between system arguments and human constructed arguments. As discovered by our prior work (Wang et al., 2017), both topical content and language style are essential elements for high quality arguments. For future work, generation models with a better control on linguistic style need to be designed. As for improving coherence, we believe that discourse-aware generation models (Ji et al., 2016) should also be explored in the future work to enhance text planning. 9 community (Park and Cardie, 2014; Ghosh et al., 2014; Palau and Moens, 2009; Niculae et al., 2017; Eger et al., 2017). While argument understanding has received increasingly more attention, the area of automatic argument generation is much less studied. Early work on argument construction investigates the design of argumentation strategies (Reed et al., 1996; Carenini and Moore, 2000; Zukerman et al., 2000). For instance, Reed (1999) describes the first full natural language argument generation system, called Rhetorica. It however only outputs a text plan, mainly relying on heuristic rules. Due to the difficulty of text generation, none of the previous work represents a fully automated ar"
P18-1021,P02-1040,0,0.10238,"amework (Sutskever et al., 2014), which has achieved significant success in various text generation tasks (Bahdanau et al., 2015; Wen et al., 2015; Wang and Ling, 2016; Mei et al., 2016; Wiseman et al., 2017). Our encoder takes as input a statement on a disputed issue, and a set of relevant evidence automatically retrieved from English Wikipedia2 . Our decoder consists of two separate parts, one of which first generates keyphrases as intermediate representation of “talking points”, and the other then generates an argument based on both input and keyphrases. Automatic evaluation based on BLEU (Papineni et al., 2002) shows that our framework generates better arguments than directly using retrieved sentences or popular seq2seq-based generation models (Bahdanau et al., 2015) that are also trained with retrieved evidence. We further design a novel evaluation procedure to measure whether the arguments are on-topic by predicting their relevance to the given statement based on a separately trained 1 2 relevance estimation model. Results suggest that our model generated arguments are more likely to be predicted as on-topic, compared to other seq2seq-based generations models. The rest of this paper is organized a"
P18-1021,W14-2105,0,0.0445785,"he first example. Meanwhile, our model also acquires argumentative style language, though there is still a noticeable gap between system arguments and human constructed arguments. As discovered by our prior work (Wang et al., 2017), both topical content and language style are essential elements for high quality arguments. For future work, generation models with a better control on linguistic style need to be designed. As for improving coherence, we believe that discourse-aware generation models (Ji et al., 2016) should also be explored in the future work to enhance text planning. 9 community (Park and Cardie, 2014; Ghosh et al., 2014; Palau and Moens, 2009; Niculae et al., 2017; Eger et al., 2017). While argument understanding has received increasingly more attention, the area of automatic argument generation is much less studied. Early work on argument construction investigates the design of argumentation strategies (Reed et al., 1996; Carenini and Moore, 2000; Zukerman et al., 2000). For instance, Reed (1999) describes the first full natural language argument generation system, called Rhetorica. It however only outputs a text plan, mainly relying on heuristic rules. Due to the difficulty of text gene"
P18-1021,W17-5106,0,0.288942,"ddit Change My View subcommunity that argue against original post’s thesis on “government should be allowed to view private emails”. Both arguments leverage supporting information from Wikipedia articles. viding context of controversial issues from different perspectives. As a consequence, there exists a pressing need for automating the argument construction process. To date, progress made in argument generation has been limited to retrieval-based methods— arguments are ranked based on relevance to a given topic, then the top ones are selected for inclusion in the output (Rinott et al., 2015; Wachsmuth et al., 2017; Hua and Wang, 2017). Although sentence ordering algorithms are developed for information structuring (Sato et al., 2015; Reisert et al., 2015), existing methods lack the ability of synthesizing information from different resources, leading to redundancy and incoherence in the output. In general, the task of argument generation presents numerous challenges, ranging from aggregating supporting evidence to generating text with coherent logical structure. One particular hurdle comes from the underlying natural language generation (NLG) stack, whose success has been limited to a small set of doma"
P18-1021,Q17-1016,1,0.818587,"with evidence retrieved from Wikipedia. Separate decoders were designed to first produce a set of keyphrases as talking points, and then generate the final argument. Both automatic evaluation against human arguments and human assessment showed that our model produced more informative arguments than popular sequence-to-sequence-based generation models. of the world”, as discussed in the first example. Meanwhile, our model also acquires argumentative style language, though there is still a noticeable gap between system arguments and human constructed arguments. As discovered by our prior work (Wang et al., 2017), both topical content and language style are essential elements for high quality arguments. For future work, generation models with a better control on linguistic style need to be designed. As for improving coherence, we believe that discourse-aware generation models (Ji et al., 2016) should also be explored in the future work to enhance text planning. 9 community (Park and Cardie, 2014; Ghosh et al., 2014; Palau and Moens, 2009; Niculae et al., 2017; Eger et al., 2017). While argument understanding has received increasingly more attention, the area of automatic argument generation is much le"
P18-1021,N16-1007,1,0.678087,"rences. Figure 1 displays sample arguments by users from Reddit subcommunity /r/ChangeMyView 1 who argue against the motion that “government should be allowed to view private emails”. Both replies leverage information drawn from Wikipedia, such as “political corruption” and “Fourth Amendment on protections of personal privacy”. Concretely, our neural argument generation model adopts the popular encoder-decoderbased sequence-to-sequence (seq2seq) framework (Sutskever et al., 2014), which has achieved significant success in various text generation tasks (Bahdanau et al., 2015; Wen et al., 2015; Wang and Ling, 2016; Mei et al., 2016; Wiseman et al., 2017). Our encoder takes as input a statement on a disputed issue, and a set of relevant evidence automatically retrieved from English Wikipedia2 . Our decoder consists of two separate parts, one of which first generates keyphrases as intermediate representation of “talking points”, and the other then generates an argument based on both input and keyphrases. Automatic evaluation based on BLEU (Papineni et al., 2002) shows that our framework generates better arguments than directly using retrieved sentences or popular seq2seq-based generation models (Bahdanau"
P18-1021,D15-1199,0,0.0559501,"Missing"
P18-1021,D16-1137,0,0.0150156,"cial token &lt;arg&gt; is inserted between the two sequences, indicating the start of argument generation. 4.2 Hybrid Beam Search Decoding Here we describe our decoding strategy on the argument decoder. We design a hybrid beam expansion method combined with segment-based reranking to promote diversity of beams and informativeness of the generated arguments. Hybrid Beam Expansion. In the standard beam search, the top k words of highest probability are 222 selected deterministically based on the softmax output to expand each hypothesis. However, this may lead to suboptimal output for text generation (Wiseman and Rush, 2016), e.g., one beam often dominates and thus inhibits hypothesis diversity. Here we only pick the top n words (n &lt; k), and randomly draw another k − n words based on the multinomial distribution after removing the n expanded words from the candidates. This leads to a more diverse set of hypotheses. Segment-based Reranking. We also propose to rerank the beams every p steps based on beam’s coverage of content words from input. Based on our observation that likelihood-based reranking often leads to overly generic arguments (e.g., “I don’t agree with you”), this operation has the potential of encoura"
P18-1021,W00-1408,0,0.830415,"rol on linguistic style need to be designed. As for improving coherence, we believe that discourse-aware generation models (Ji et al., 2016) should also be explored in the future work to enhance text planning. 9 community (Park and Cardie, 2014; Ghosh et al., 2014; Palau and Moens, 2009; Niculae et al., 2017; Eger et al., 2017). While argument understanding has received increasingly more attention, the area of automatic argument generation is much less studied. Early work on argument construction investigates the design of argumentation strategies (Reed et al., 1996; Carenini and Moore, 2000; Zukerman et al., 2000). For instance, Reed (1999) describes the first full natural language argument generation system, called Rhetorica. It however only outputs a text plan, mainly relying on heuristic rules. Due to the difficulty of text generation, none of the previous work represents a fully automated argument generation system. This work aims to close the gap by proposing an end-to-end trained argument construction framework. Additionally, argument retrieval and extraction are investigated (Rinott et al., 2015; Hua and Wang, 2017) to deliver relevant arguments for user-specified queries. Wachsmuth et al. (2017"
P19-1201,W13-2322,0,0.0255861,"om semantic parser) and pd (x, y) (learned from NL generator) to an underlying unknown distribution P(x, y). Introduction Semantic parsing studies the task of translating natural language (NL) utterances into formal meaning representations (MRs) (Zelle and Mooney, 1996; Tang and Mooney, 2000). NL generation models can be designed to learn the reverse: mapping MRs to their NL descriptions (Wong and Mooney, 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers"
P19-1201,P17-1005,0,0.019457,"performance, and a high correlation is observed between them (Fig. 7). 5 Related Work Semantic Parsing and NL Generation. Neural sequence-to-sequence models have achieved promising results on semantic parsing (Dong and Lapata, 2016; Jia and Liang, 2016; Ling et al., 2016; Dong and Lapata, 2018) and natural language generation (Iyer et al., 2016; Konstas et al., 2017; Hu et al., 2018). To better model structured MRs, tree structures and more complicated graphs are explored for both parsing and generation (Dong and Lapata, 2016; Rabinovich et al., 2017; Yin and Neubig, 2017; Song et al., 2018; Cheng et al., 2017; Alon et al., 2018). Semisupervised learning has been widely studied for semantic parsing (Yin et al., 2018b; Kocisk´y et al., 2016; Jia and Liang, 2016). Similar to our work, Chen and Zhou (2018) and Allamanis et al. (2015) study code retrieval and code summarization jointly to enhance both tasks. Here, we focus on the more challenging task of code generation instead of retrieval, and we also aim for generalpurpose MRs. Joint Learning in NLP. There has been growing interests in leveraging related NLP problems to enhance primal tasks (Collobert et al., 2011; Peng et al., 2017; Liu et al., 201"
P19-1201,P16-1004,0,0.594396,"l information learning. With the objective of Eq. 3, we jointly learn a parser and a generator, as well as maximize the dual information between the two. LDIM serves as a regularization term to influence the learning process, whose detailed algorithm is described in §3. Our method of DIM is model-independent. If the learning objectives for semantic parser and NL generator are subject to Eq. 1 and Eq. 2, we can always adopt DIM to conduct joint learning. Out of most commonly used seq2seq models for the parser and generator, more complex tree and graph structures have been adopted to model MRs (Dong and Lapata, 2016; Song et al., 2018). In this paper, without loss of generality, we study our joint-learning method on the widely-used seq2seq LeDIM LdDIM frameworks mentioned above (§2.1). 3 Dual Information Maximization In this section, we first introduce dual information in §3.1, followed by its maximization (§3.2). §3.3 discusses its extension with semi-supervision. 3.1 Dual Information As discussed above, we treat semantic parsing and NL generation as the dual tasks and exploit the duality between the two tasks for our joint learning. With conditional distributions pθ (y|x) for the parser and qφ (x|y) fo"
P19-1201,P17-1097,0,0.0233711,"to obtain the final lower bound. To learn the lower bound of LeDIM , we provide the following method to calculate its gradients: Gradient Estimation. We adopt Monte Carlo samples using the REINFORCE policy (Williams, 1992) to approximate the gradient of LeDIM (θ, φ) with regard to θ: ∇θ LeDIM (θ, φ) = Epθ (y|x) ∇θ log pθ (y|x) · [log qφ (x|y) + log q(y) − b] = Epθ (y|x) ∇θ log pθ (y|x) · l(x, y; φ) (7) X 1 ˆ i ; φ) ∇θ log pθ (ˆ yi |x) · l(x, y ≈ |S| ˆ i ∈S y l(x, y; φ) can be seen as the learning signal from the dual model, which is similar to the reward in reinforcement learning algorithms (Guu et al., 2017; Paulus et al., 2017). To handle the highvariance of learning signals, we adopt the baseline function b by empirically averaging the signals to stabilize the learning process (Williams, 1992). With prior pθ (·|x), we use beam search to generate a pool of MR candidates (y), denoted as S, for the input of x. The gradient with regard to φ is then calculated as: ∇φ LeDIM (θ, φ) = Epθ (y|x) ∇φ log qφ (x|y) 1 X ≈ ∇φ log qφ (x|ˆ yi ) (8) |S| ˆ i ∈S y The above maximization procedure for LeDIM is analogous to the EM algorithm: Step 1: Freeze φ and find the optimal θ∗ = arg maxθ LeDIM (θ, φ) with Eq."
P19-1201,P16-1195,0,0.158342,", 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ (y|x), the conditional distribution between NL input x and the corresponding MR output y, as demonstrated in Fig. 1. Similarly, for NL generation from MRs, the goal is to learn a generator of qφ (x|y). As demonstrated in Fig. 2, there is a clear duality between the two tasks, given that one task’s input is the other task’s output, and vice versa. However, such duality remains l"
P19-1201,P16-1002,0,0.256039,"competitive comparmore, we leverage pointer network (Vinyals et al., ison models trained for each task separately. 2015) to copy tokens from the input to handle outof-vocabulary (OOV) words. The structured MRs Overall, we have the following contributions in are linearized for the sequential encoder and dethis work: 2091 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 coder. More details of the parser and the generator can be found in Appendix A. Briefly speaking, our models differ from existing work as follows: PARSER: Our architecture is similar to the one proposed in Jia and Liang (2016) for semantic parsing; G ENERATOR: Our model improves upon the D EEP C OM coder summarization system (Hu et al., 2018) by: 1) replacing LSTM with biLSTM for the encoder to better model context, and 2) adding copying mechanism. 2.2 Jointly Learning Parser and Generator Our joint learning framework is designed to model the duality between a parser and a generator. To incorporate the duality into our learning process, we design the framework to encourage the expected joint distributions pe (x, y) and pd (x, y) to both approximate the unknown joint distribution of x and y (shown in Fig. 1). To ach"
P19-1201,D16-1116,0,0.0408621,"Missing"
P19-1201,P17-1014,0,0.106667,"(Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ (y|x), the conditional distribution between NL input x and the corresponding MR output y, as demonstrated in Fig. 1. Similarly, for NL generation from MRs, the goal is to learn a generator of qφ (x|y). As demonstrated in Fig. 2, there is a clear duality between the two tasks, given that one task’s input is the other task’s output, and vice versa. However, such duality remains largely unstudied, even though joint modeling has been demonstrated effective in various NLP problems, e.g. question a"
P19-1201,P16-1057,0,0.13213,"Missing"
P19-1201,D15-1166,0,0.0192631,"mized (§3.3). pata, 2016; Hu et al., 2018), and without loss We experiment with three datasets from two difof generality, we adopt it as the basic frameferent domains: ATIS for dialogue management; work for both tasks in this work. Specifically, for D JANGO and C O NA L A for code generation and both pθ (y|x) and qφ (x|y), we use a two-layer bisummarization. Experimental results show that directional LSTM (bi-LSTM) as the encoder and both the semantic parser and generator can be conanother one-layer LSTM as the decoder with atsistently improved with joint learning using DIM tention mechanism (Luong et al., 2015). Furtherand S EMI DIM, compared to competitive comparmore, we leverage pointer network (Vinyals et al., ison models trained for each task separately. 2015) to copy tokens from the input to handle outof-vocabulary (OOV) words. The structured MRs Overall, we have the following contributions in are linearized for the sequential encoder and dethis work: 2091 082 083 084 085 086 087 088 089 090 091 092 093 094 095 096 097 098 099 coder. More details of the parser and the generator can be found in Appendix A. Briefly speaking, our models differ from existing work as follows: PARSER: Our architectur"
P19-1201,P18-1068,0,0.417535,"the dual information and unsupervised objectives equally for simplicity, so the lower bounds over them are combined for joint optimization. We combine the labeled and unlabeled data to calculate the lower bounds to optimize the variational lower bounds of dual information and unsupervised objectives. 4 4.1 Experiments Datasets S EMANTIC PARSING (in Acc.) Pro. S UPER DIM S EMI DIM 1/4 64.7 69.0 71.9 1/2 78.1 78.8 80.8 full 84.6 85.3 – Previous Supervised Methods (Pro. = full) S EQ 2T REE (Dong and Lapata, 2016) ASN (Rabinovich et al., 2017) ASN+S UPATT (Rabinovich et al., 2017) C OARSE 2F INE (Dong and Lapata, 2018) S ELF T RAIN 66.3 79.2 – Acc. 84.6 85.3 85.9 87.7 NL G ENERATION (in BLEU) Pro. S UPER DIM S EMI DIM 1/4 36.9 37.7 39.1 1/2 39.1 40.7 40.9 full 39.3 40.6 – Previous Supervised Methods (Pro. = full) D EEP C OM (Hu et al., 2018) BACK B OOST 40.9 39.3 – BLEU 42.3 Table 2: Semantic parsing and NL generation results on ATIS. Pro.: proportion of the training samples used for training. Best result in each row is highlighted in bold. |full |= 4,434. are lowercased and tokenized and the tokens in code snippets are separated with space. Statistics of the datasets are summarized in Table 1. 4.2 Experime"
P19-1201,N16-1087,0,0.0212966,"ns, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ (y|x), the conditional distribution between NL input x and the corresponding MR output y, as demonstrated in Fig. 1. Similarly, for NL generation from MRs, the goal is to learn a generator of qφ (x|y). As demonstrated in Fig. 2, there is a clear duality between the two tasks, given that one task’s input is the other task’s output, and vice versa. However, such duality remains largely unstudied, even though joint modeling has been demonstrated effective in various NLP problems, e.g. question answering and generation"
P19-1201,D16-1183,0,0.0214721,"pd (x, y) (learned from NL generator) to an underlying unknown distribution P(x, y). Introduction Semantic parsing studies the task of translating natural language (NL) utterances into formal meaning representations (MRs) (Zelle and Mooney, 1996; Tang and Mooney, 2000). NL generation models can be designed to learn the reverse: mapping MRs to their NL descriptions (Wong and Mooney, 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ ("
P19-1201,P17-1186,0,0.0225296,"g et al., 2018; Cheng et al., 2017; Alon et al., 2018). Semisupervised learning has been widely studied for semantic parsing (Yin et al., 2018b; Kocisk´y et al., 2016; Jia and Liang, 2016). Similar to our work, Chen and Zhou (2018) and Allamanis et al. (2015) study code retrieval and code summarization jointly to enhance both tasks. Here, we focus on the more challenging task of code generation instead of retrieval, and we also aim for generalpurpose MRs. Joint Learning in NLP. There has been growing interests in leveraging related NLP problems to enhance primal tasks (Collobert et al., 2011; Peng et al., 2017; Liu et al., 2016), e.g. sequence tagging (Collobert et al., 2011), dependency parsing (Peng et al., 2017), discourse analysis (Liu et al., 2016). Among those, multi-task learning (MTL) (Ando and Zhang, 2005) is a common method for joint learning, especially for neural networks where parameter sharing is utilized for representation learning. We follow the recent work on dual learning (Xia et al., 2017) to train dual tasks, where interactions can be employed to enhance both models. Dual learning has been successfully applied in NLP and computer vision problems, such as neural machine translati"
P19-1201,P17-1105,0,0.104424,"i ∼qφ (·|y) (13) where Dx = Ux ∪ Lx and Dy = Uy ∪ Ly . In this work, we weight the dual information and unsupervised objectives equally for simplicity, so the lower bounds over them are combined for joint optimization. We combine the labeled and unlabeled data to calculate the lower bounds to optimize the variational lower bounds of dual information and unsupervised objectives. 4 4.1 Experiments Datasets S EMANTIC PARSING (in Acc.) Pro. S UPER DIM S EMI DIM 1/4 64.7 69.0 71.9 1/2 78.1 78.8 80.8 full 84.6 85.3 – Previous Supervised Methods (Pro. = full) S EQ 2T REE (Dong and Lapata, 2016) ASN (Rabinovich et al., 2017) ASN+S UPATT (Rabinovich et al., 2017) C OARSE 2F INE (Dong and Lapata, 2018) S ELF T RAIN 66.3 79.2 – Acc. 84.6 85.3 85.9 87.7 NL G ENERATION (in BLEU) Pro. S UPER DIM S EMI DIM 1/4 36.9 37.7 39.1 1/2 39.1 40.7 40.9 full 39.3 40.6 – Previous Supervised Methods (Pro. = full) D EEP C OM (Hu et al., 2018) BACK B OOST 40.9 39.3 – BLEU 42.3 Table 2: Semantic parsing and NL generation results on ATIS. Pro.: proportion of the training samples used for training. Best result in each row is highlighted in bold. |full |= 4,434. are lowercased and tokenized and the tokens in code snippets are separated w"
P19-1201,P16-1009,0,0.0407195,"on task in C O NA L A, we use BLEU-4 following the setup in Yin et al. (2018a). Baselines. We compare our methods of DIM and S EMI DIM with the following baselines: S UPER: Train the parser or generator separately without joint learning. The models for the parser and generator are the same as DIM. S ELF T RAIN (Lee, 2013): We use the pre-trained parser or generator to generate pseudo labels for the unlabeled sources, then the constructed pseudo samples will be mixed with the labeled data to fine-tune the pre-trained parser or generator. BACK B OOST: Adopted from the back translation method in Sennrich et al. (2016), which generates sources from unlabeled targets. The training process for BACK B OOST is the same as in S ELF T RAIN. In addition to the above baselines, we also compare with popular supervised methods for each task, shown in the corresponding result tables. 4.3 BACK B OOST 9.0 – Results and Further Analysis Main Results with Full- and Semi-supervision. Results on the three datasets with supervised and semi-supervised setups are presented in Tables 2, 3, and 4. For semi-supervised experiments on ATIS, we use the NL part as extra unlabeled samTable 4: Code generation and code summarization res"
P19-1201,P18-1150,0,0.182968,"ncluding λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ (y|x), the conditional distribution between NL input x and the corresponding MR output y, as demonstrated in Fig. 1. Similarly, for NL generation from MRs, the goal is to learn a generator of qφ (x|y). As demonstrated in Fig. 2, there is a clear duality between the two tasks, given that one task’s input is the other task’s output, and vice versa. However, such duality remains largely unstudied, even though joint modeling has been demonstrated effective in various NLP pro"
P19-1201,D17-1090,0,0.0912481,"Missing"
P19-1201,W00-1317,0,0.118976,"ed and semi-supervised setups1 . 1 x y qϕ pθ x e → y p (x, y) = p(x)pθ (y|x) y − → x d p (x, y) = q(y)qϕ (x|y) Figure 1: Illustration of our joint learning model. x: NL; y: MRs. pθ (y|x): semantic parser; qφ (x|y): NL generator. We model the duality of the two tasks by matching the joint distributions of pe (x, y) (learned from semantic parser) and pd (x, y) (learned from NL generator) to an underlying unknown distribution P(x, y). Introduction Semantic parsing studies the task of translating natural language (NL) utterances into formal meaning representations (MRs) (Zelle and Mooney, 1996; Tang and Mooney, 2000). NL generation models can be designed to learn the reverse: mapping MRs to their NL descriptions (Wong and Mooney, 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automaticall"
P19-1201,P18-1070,0,0.377635,"x y x y x y E XAMPLE can you list all flights from chicago to milwaukee ( lambda $0 e ( and ( flight $0) ( from $0 chicago: ci) ( to $0 milwaukee: ci) ) ) convert max entries into a string, substitute it for self. max entries. self. max entries = int(max entries) more pythonic alternative for getting a value in range not using min and max a = 1 if x < 1 else 10 if x > 10 else x Ave. Token 10.6 26.5 11.9 8.2 9.7 14.1 Figure 2: Sample natural language utterances and meaning representations from datasets used in this work: ATIS for dialogue management; D JANGO (Oda et al., 2015) and C O NA L A (Yin et al., 2018a) for code generation and summarization. 033 034 035 036 037 038 039 040 041 042 043 044 045 046 047 048 049 073 074 075 076 077 078 079 080 081 031 032 072 • We are the first to jointly study semantic parsation (Xia et al., 2017). ing and natural language generation by exIn this paper, we propose to jointly model seploiting the duality between the two tasks; mantic parsing and NL generation by exploiting • We propose DIM to capture the duality and the interaction between the two tasks. Followadopt variational approximation to maximize ing previous work on dual learning (Xia et al., the dual"
P19-1201,D07-1071,0,0.0807189,"ous Supervised Methods (Pro. = full) D EEP C OM (Hu et al., 2018) BACK B OOST 40.9 39.3 – BLEU 42.3 Table 2: Semantic parsing and NL generation results on ATIS. Pro.: proportion of the training samples used for training. Best result in each row is highlighted in bold. |full |= 4,434. are lowercased and tokenized and the tokens in code snippets are separated with space. Statistics of the datasets are summarized in Table 1. 4.2 Experiments are conducted on three datasets with sample pairs shown in Fig. 2: one for dialogue management which studies semantic parsing and generation from λ-calculus (Zettlemoyer and Collins, 2007) (ATIS) and two for code generation and summarization (D JANGO, C O NA L A). ATIS . This dataset has 5,410 pairs of queries (NL) from a flight booking system and corresponding λcalculus representation (MRs). The anonymized version from Dong and Lapata (2016) is used. D JANGO. It contains 18,805 lines of Python code snippets (Oda et al., 2015). Each snippet is annotated with a piece of human-written pseudo code. Similar to Yin and Neubig (2017), we replace strings separated by quotation marks with indexed place holder in NLs and MRs. C O N A L A. This is another Python-related corpus containing"
P19-1201,N07-1022,0,0.0604602,": Illustration of our joint learning model. x: NL; y: MRs. pθ (y|x): semantic parser; qφ (x|y): NL generator. We model the duality of the two tasks by matching the joint distributions of pe (x, y) (learned from semantic parser) and pd (x, y) (learned from NL generator) to an underlying unknown distribution P(x, y). Introduction Semantic parsing studies the task of translating natural language (NL) utterances into formal meaning representations (MRs) (Zelle and Mooney, 1996; Tang and Mooney, 2000). NL generation models can be designed to learn the reverse: mapping MRs to their NL descriptions (Wong and Mooney, 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et"
P19-1201,P17-1041,0,0.442857,"l language (NL) utterances into formal meaning representations (MRs) (Zelle and Mooney, 1996; Tang and Mooney, 2000). NL generation models can be designed to learn the reverse: mapping MRs to their NL descriptions (Wong and Mooney, 2007). Generally speaking, MR often takes a logical form that captures the semantic meaning, including λcalculus (Zettlemoyer and Collins, 2005, 2007), Abstract Meaning Representation (AMR) (Banarescu et al., 2013; Misra and Artzi, 2016), and general-purpose computer programs, such as 1 Code for this paper is available at: github.com/oceanypt/DIM ⟷ https:// Python (Yin and Neubig, 2017) or SQL (Zhong et al., 2017). Recently, NL generation models have been proposed to automatically construct humanreadable descriptions from MRs, for code summarization (Hu et al., 2018; Allamanis et al., 2016; Iyer et al., 2016) that predicts the function of code snippets, and for AMR-to-text generation (Song et al., 2018; Konstas et al., 2017; Flanigan et al., 2016). Specifically, a common objective that semantic parsers aim to estimate is pθ (y|x), the conditional distribution between NL input x and the corresponding MR output y, as demonstrated in Fig. 1. Similarly, for NL generation from MR"
P19-1208,I11-1130,0,0.357716,". Introduction The task of keyphrase generation aims at predicting a set of keyphrases that convey the core ideas of a document. Figure 1 shows a sample document and its keyphrase labels. The keyphrases in red color are present keyphrases that appear in the document, whereas the blue ones are absent keyphrases that do not appear in the input. By distilling the key information of a document into a set of succinct keyphrases, keyphrase generation facilitates a wide variety of downstream applications, including document clustering (Hammouda et al., 2005; Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and summarization (Zhang et al., 2004; Wang and Cardie, 2013). To produce both present and absent keyphrases, generative methods (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018a,b) are designed to apply the attentional encoder-decoder model (Bahdanau et al., 2014; Luong et al., 2015) with copy mechanism (Gu et al., 2016; See et al., 2017) to approach the keyphrase generation task. However, none of the prior models can determine the appropriate number of keyphrases for a document. In reality, the optimal keyphrase count varies, and is dependent on a given document’s content. To that"
P19-1208,D18-1439,0,0.397375,"els. The keyphrases in red color are present keyphrases that appear in the document, whereas the blue ones are absent keyphrases that do not appear in the input. By distilling the key information of a document into a set of succinct keyphrases, keyphrase generation facilitates a wide variety of downstream applications, including document clustering (Hammouda et al., 2005; Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and summarization (Zhang et al., 2004; Wang and Cardie, 2013). To produce both present and absent keyphrases, generative methods (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018a,b) are designed to apply the attentional encoder-decoder model (Bahdanau et al., 2014; Luong et al., 2015) with copy mechanism (Gu et al., 2016; See et al., 2017) to approach the keyphrase generation task. However, none of the prior models can determine the appropriate number of keyphrases for a document. In reality, the optimal keyphrase count varies, and is dependent on a given document’s content. To that end, Yuan et al. (2018) introduced a training setup in which a generative model can learn to decide the number of keyphrases to predict for a given document and proposed two models. Altho"
P19-1208,N19-1292,1,0.560632,"ng et al., 2016) to identify keyphrases. However, extractive methods cannot produce absent keyphrases. 2164 To predict both present and absent keyphrases for a document, Meng et al. (2017) proposed a generative model, CopyRNN, which is composed of an attentional encoder-decoder model (Bahdanau et al., 2014) and a copy mechanism (Gu et al., 2016). Lately, multiple extensions to CopyRNN were also presented. CorrRNN (Chen et al., 2018a) incorporates the correlation among keyphrases. TG-Net (Chen et al., 2018b) exploits the title information to learn a better representation for an input document. Chen et al. (2019) leveraged keyphrase extraction models and external knowledge to improve the performance of keyphrase generation. Ye and Wang (2018) considered a setting where training data is limited, and proposed different semi-supervised methods to enhance the performance. All of the above generative models use beam search to over-generate a large number of keyphrases and select the topk predicted keyphrases as the final predictions, where k is a fixed number. Recently, Yuan et al. (2018) introduced a setting where a model has to determine the appropriate number of keyphrases for an input document. They pr"
P19-1208,D14-1179,0,0.0375548,"Missing"
P19-1208,W03-1028,0,0.821639,"tate drive”, we will assume that the keyphrase “ssd” refers to this entity. 6 Experiments We first report the performance of different models using the conventional evaluation method. Afterwards, we present the results based on our new evaluation method. All experiments are repeated for three times using different random seeds and the averaged results are reported. The source code and the enriched evaluation set are released to the public2 . Sample output is shown in Figure 1. 6.1 Datasets We conduct experiments on five scientific article datasets, including KP20k (Meng et al., 2017), Inspec (Hulth, 2003), Krapivin (Krapivin et al., 2009), NUS (Nguyen and Kan, 2007b), and SemEval (Kim et al., 2010). Each sample from these datasets consists of the title, abstract, and keyphrases of a scientific article. We concatenate the title and abstract as an input document, and use the assigned keyphrases as keyphrase labels. Following the setup in (Meng et al., 2017; Yuan et al., 2018; Chen et al., 2018b), we use the training set 2 Source code and evaluation set are available at https://github.com/kenchan0226/keyphrase-generation-rl of the largest dataset, KP20k, for model training and the testing sets of"
P19-1208,P06-1068,0,0.832374,"els are based on our new evaluation method. Introduction The task of keyphrase generation aims at predicting a set of keyphrases that convey the core ideas of a document. Figure 1 shows a sample document and its keyphrase labels. The keyphrases in red color are present keyphrases that appear in the document, whereas the blue ones are absent keyphrases that do not appear in the input. By distilling the key information of a document into a set of succinct keyphrases, keyphrase generation facilitates a wide variety of downstream applications, including document clustering (Hammouda et al., 2005; Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and summarization (Zhang et al., 2004; Wang and Cardie, 2013). To produce both present and absent keyphrases, generative methods (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018a,b) are designed to apply the attentional encoder-decoder model (Bahdanau et al., 2014; Luong et al., 2015) with copy mechanism (Gu et al., 2016; See et al., 2017) to approach the keyphrase generation task. However, none of the prior models can determine the appropriate number of keyphrases for a document. In reality, the optimal keyphrase count varies, and is dependent on a giv"
P19-1208,D18-1317,0,0.0251482,"he model to generate both sufficient and accurate keyphrases. Empirical studies on real data demonstrate that our deep reinforced models consistently outperform the current state-of-the-art models. In addition, we propose a novel evaluation method which incorporates name variations of the ground-truth keyphrases. As a result, it can more robustly evaluate the quality of generated keyphrases. One potential future direction is to investigate the performance of other encoder-decoder architectures on keyphrase generation such as Transformer (Vaswani et al., 2017) with multi-head attention module (Li et al., 2018; Zhang et al., 2018a). Another interesting direction is to apply our RL approach on the microblog hashtag annotation problem (Wang et al., 2019; Gong and Zhang, 2016; Zhang et al., 2018b). Acknowledgments The work described in this paper was partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 of the General Research Fund) and Meitu (No. 7010445). Lu Wang is supported in part by National Science Foundation through Grants IIS-1566382 and IIS-1813341, and by the Office of the Director of National Intelligence (ODNI), Intelli"
P19-1208,P16-1154,0,0.510415,"in the input. By distilling the key information of a document into a set of succinct keyphrases, keyphrase generation facilitates a wide variety of downstream applications, including document clustering (Hammouda et al., 2005; Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and summarization (Zhang et al., 2004; Wang and Cardie, 2013). To produce both present and absent keyphrases, generative methods (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018a,b) are designed to apply the attentional encoder-decoder model (Bahdanau et al., 2014; Luong et al., 2015) with copy mechanism (Gu et al., 2016; See et al., 2017) to approach the keyphrase generation task. However, none of the prior models can determine the appropriate number of keyphrases for a document. In reality, the optimal keyphrase count varies, and is dependent on a given document’s content. To that end, Yuan et al. (2018) introduced a training setup in which a generative model can learn to decide the number of keyphrases to predict for a given document and proposed two models. Although they provided a more realistic setup, there still exist two drawbacks. First, models trained under this setup tend 2163 Proceedings of the 57"
P19-1208,D17-1279,0,0.0957608,"problem. 2 2.1 Related Work Keyphrase Extraction and Generation Traditional extractive methods select important phrases from the document as its keyphrase predictions. Most of them adopt a two-step approach. First, they identify keyphrase candidates from the document by heuristic rules (Wang et al., 2016; Le et al., 2016). Afterwards, the candidates are either ranked by unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised learning algorithms (Medelyan et al., 2009; Witten et al., 1999; Nguyen and Kan, 2007a). Other extractive methods apply sequence tagging models (Luan et al., 2017; Gollapalli et al., 2017; Zhang et al., 2016) to identify keyphrases. However, extractive methods cannot produce absent keyphrases. 2164 To predict both present and absent keyphrases for a document, Meng et al. (2017) proposed a generative model, CopyRNN, which is composed of an attentional encoder-decoder model (Bahdanau et al., 2014) and a copy mechanism (Gu et al., 2016). Lately, multiple extensions to CopyRNN were also presented. CorrRNN (Chen et al., 2018a) incorporates the correlation among keyphrases. TG-Net (Chen et al., 2018b) exploits the title information to learn a better represen"
P19-1208,D15-1166,0,0.0339663,"are absent keyphrases that do not appear in the input. By distilling the key information of a document into a set of succinct keyphrases, keyphrase generation facilitates a wide variety of downstream applications, including document clustering (Hammouda et al., 2005; Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and summarization (Zhang et al., 2004; Wang and Cardie, 2013). To produce both present and absent keyphrases, generative methods (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018a,b) are designed to apply the attentional encoder-decoder model (Bahdanau et al., 2014; Luong et al., 2015) with copy mechanism (Gu et al., 2016; See et al., 2017) to approach the keyphrase generation task. However, none of the prior models can determine the appropriate number of keyphrases for a document. In reality, the optimal keyphrase count varies, and is dependent on a given document’s content. To that end, Yuan et al. (2018) introduced a training setup in which a generative model can learn to decide the number of keyphrases to predict for a given document and proposed two models. Although they provided a more realistic setup, there still exist two drawbacks. First, models trained under this"
P19-1208,D09-1137,0,0.711048,"to determine the number of keyphrases to generate. This is the first work to study RL approach on the keyphrase generation problem. 2 2.1 Related Work Keyphrase Extraction and Generation Traditional extractive methods select important phrases from the document as its keyphrase predictions. Most of them adopt a two-step approach. First, they identify keyphrase candidates from the document by heuristic rules (Wang et al., 2016; Le et al., 2016). Afterwards, the candidates are either ranked by unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised learning algorithms (Medelyan et al., 2009; Witten et al., 1999; Nguyen and Kan, 2007a). Other extractive methods apply sequence tagging models (Luan et al., 2017; Gollapalli et al., 2017; Zhang et al., 2016) to identify keyphrases. However, extractive methods cannot produce absent keyphrases. 2164 To predict both present and absent keyphrases for a document, Meng et al. (2017) proposed a generative model, CopyRNN, which is composed of an attentional encoder-decoder model (Bahdanau et al., 2014) and a copy mechanism (Gu et al., 2016). Lately, multiple extensions to CopyRNN were also presented. CorrRNN (Chen et al., 2018a) incorporates"
P19-1208,P17-1099,0,0.766839,"distilling the key information of a document into a set of succinct keyphrases, keyphrase generation facilitates a wide variety of downstream applications, including document clustering (Hammouda et al., 2005; Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and summarization (Zhang et al., 2004; Wang and Cardie, 2013). To produce both present and absent keyphrases, generative methods (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018a,b) are designed to apply the attentional encoder-decoder model (Bahdanau et al., 2014; Luong et al., 2015) with copy mechanism (Gu et al., 2016; See et al., 2017) to approach the keyphrase generation task. However, none of the prior models can determine the appropriate number of keyphrases for a document. In reality, the optimal keyphrase count varies, and is dependent on a given document’s content. To that end, Yuan et al. (2018) introduced a training setup in which a generative model can learn to decide the number of keyphrases to predict for a given document and proposed two models. Although they provided a more realistic setup, there still exist two drawbacks. First, models trained under this setup tend 2163 Proceedings of the 57th Annual Meeting o"
P19-1208,P17-1054,0,0.657613,"sample document and its keyphrase labels. The keyphrases in red color are present keyphrases that appear in the document, whereas the blue ones are absent keyphrases that do not appear in the input. By distilling the key information of a document into a set of succinct keyphrases, keyphrase generation facilitates a wide variety of downstream applications, including document clustering (Hammouda et al., 2005; Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and summarization (Zhang et al., 2004; Wang and Cardie, 2013). To produce both present and absent keyphrases, generative methods (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018a,b) are designed to apply the attentional encoder-decoder model (Bahdanau et al., 2014; Luong et al., 2015) with copy mechanism (Gu et al., 2016; See et al., 2017) to approach the keyphrase generation task. However, none of the prior models can determine the appropriate number of keyphrases for a document. In reality, the optimal keyphrase count varies, and is dependent on a given document’s content. To that end, Yuan et al. (2018) introduced a training setup in which a generative model can learn to decide the number of keyphrases to predict for a given d"
P19-1208,W04-3252,0,0.371681,"he-art performance on five real-world datasets in a setting where a model is able to determine the number of keyphrases to generate. This is the first work to study RL approach on the keyphrase generation problem. 2 2.1 Related Work Keyphrase Extraction and Generation Traditional extractive methods select important phrases from the document as its keyphrase predictions. Most of them adopt a two-step approach. First, they identify keyphrase candidates from the document by heuristic rules (Wang et al., 2016; Le et al., 2016). Afterwards, the candidates are either ranked by unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised learning algorithms (Medelyan et al., 2009; Witten et al., 1999; Nguyen and Kan, 2007a). Other extractive methods apply sequence tagging models (Luan et al., 2017; Gollapalli et al., 2017; Zhang et al., 2016) to identify keyphrases. However, extractive methods cannot produce absent keyphrases. 2164 To predict both present and absent keyphrases for a document, Meng et al. (2017) proposed a generative model, CopyRNN, which is composed of an attentional encoder-decoder model (Bahdanau et al., 2014) and a copy mechanism (Gu et al., 2016). Lately, multiple extens"
P19-1208,D17-1103,0,0.0310196,"nerate sufficient and accurate keyphrases. To our best knowledge, this is the first time RL is used for keyphrase generation. Besides, we propose a new evaluation method that considers name variations of the keyphrase labels, a novel contribution to the state-of-the-art. 2.2 Reinforcement Learning for Text Generation Reinforcement learning has been applied to a wide array of text generation tasks, including machine translation (Wu et al., 2016; Ranzato et al., 2015), text summarization (Paulus et al., 2018; Wang et al., 2018), and image/video captioning (Rennie et al., 2017; Liu et al., 2017; Pasunuru and Bansal, 2017). These RL approaches lean on the REINFORCE algorithm (Williams, 1992), or its variants, to train a generative model towards a non-differentiable reward by minimizing the policy gradient loss. Different from existing work, our RL approach uses a novel adaptive reward function, which combines the recall and F1 score via a hard gate (if-else statement). 3 3.1 Preliminary Problem Definition We formally define the problem of keyphrase generation as follows. Given a document x, output a set of ground-truth keyphrases Y = {y1 , y2 , . . . , y|Y |}. The document x and each ground-truth keyphrase yi a"
P19-1208,P13-1137,1,0.906091,"Missing"
P19-1208,N19-1164,1,0.847179,"stently outperform the current state-of-the-art models. In addition, we propose a novel evaluation method which incorporates name variations of the ground-truth keyphrases. As a result, it can more robustly evaluate the quality of generated keyphrases. One potential future direction is to investigate the performance of other encoder-decoder architectures on keyphrase generation such as Transformer (Vaswani et al., 2017) with multi-head attention module (Li et al., 2018; Zhang et al., 2018a). Another interesting direction is to apply our RL approach on the microblog hashtag annotation problem (Wang et al., 2019; Gong and Zhang, 2016; Zhang et al., 2018b). Acknowledgments The work described in this paper was partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 of the General Research Fund) and Meitu (No. 7010445). Lu Wang is supported in part by National Science Foundation through Grants IIS-1566382 and IIS-1813341, and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Research Projects Activity (IARPA), via contract # FA865017-C-9116. The views and conclusions contained herein are those of the a"
P19-1208,D18-1447,1,0.858821,"d its keyphrase labels. The keyphrases in red color are present keyphrases that appear in the document, whereas the blue ones are absent keyphrases that do not appear in the input. By distilling the key information of a document into a set of succinct keyphrases, keyphrase generation facilitates a wide variety of downstream applications, including document clustering (Hammouda et al., 2005; Hulth and Megyesi, 2006), opinion mining (Berend, 2011), and summarization (Zhang et al., 2004; Wang and Cardie, 2013). To produce both present and absent keyphrases, generative methods (Meng et al., 2017; Ye and Wang, 2018; Chen et al., 2018a,b) are designed to apply the attentional encoder-decoder model (Bahdanau et al., 2014; Luong et al., 2015) with copy mechanism (Gu et al., 2016; See et al., 2017) to approach the keyphrase generation task. However, none of the prior models can determine the appropriate number of keyphrases for a document. In reality, the optimal keyphrase count varies, and is dependent on a given document’s content. To that end, Yuan et al. (2018) introduced a training setup in which a generative model can learn to decide the number of keyphrases to predict for a given document and propose"
P19-1208,D16-1080,0,0.33014,"tion and Generation Traditional extractive methods select important phrases from the document as its keyphrase predictions. Most of them adopt a two-step approach. First, they identify keyphrase candidates from the document by heuristic rules (Wang et al., 2016; Le et al., 2016). Afterwards, the candidates are either ranked by unsupervised methods (Mihalcea and Tarau, 2004; Wan and Xiao, 2008) or supervised learning algorithms (Medelyan et al., 2009; Witten et al., 1999; Nguyen and Kan, 2007a). Other extractive methods apply sequence tagging models (Luan et al., 2017; Gollapalli et al., 2017; Zhang et al., 2016) to identify keyphrases. However, extractive methods cannot produce absent keyphrases. 2164 To predict both present and absent keyphrases for a document, Meng et al. (2017) proposed a generative model, CopyRNN, which is composed of an attentional encoder-decoder model (Bahdanau et al., 2014) and a copy mechanism (Gu et al., 2016). Lately, multiple extensions to CopyRNN were also presented. CorrRNN (Chen et al., 2018a) incorporates the correlation among keyphrases. TG-Net (Chen et al., 2018b) exploits the title information to learn a better representation for an input document. Chen et al. (201"
P19-1208,C10-1145,0,0.0399913,"Present (Absent) ground-truth ˜ i of a groundkeyphrase. If a name variation set y truth keyphrase yi only consists of present (absent) keyphrases, then yi is a present (absent) ground-truth keyphrase. Otherwise, yi is both a present ground-truth keyphrase and an absent ground-truth keyphrase, i.e., yi ∈ Y p and yi ∈ Y a. 5.1 Name Variation Extraction We extract name variations of a ground-truth keyphrase from the following sources: acronyms in the ground-truths, Wikipedia disambiguation pages, and Wikipedia entity titles. The later two sources have also been adopted by entity linking methods (Zhang et al., 2010, 2011) to find name variations. Some examples of extracted name variations are shown in Table 1. Acronyms in the ground-truths. We found that some of the ground-truth keyphrases have included an acronym at the end of the string, e.g.,“principal component analysis (pca)”. Thus, we adopt the following simple rule to extract an 2167 acronym from a ground-truth keyphrase. If a ground-truth keyphrase ends with a pair of parentheses, we will extract the phrase inside the pair, e.g., “pca”, as one of the name variations. Wikipedia entity titles. An entity page in Wikipedia provides the information o"
P19-1208,N18-1151,0,0.0562827,"ate both sufficient and accurate keyphrases. Empirical studies on real data demonstrate that our deep reinforced models consistently outperform the current state-of-the-art models. In addition, we propose a novel evaluation method which incorporates name variations of the ground-truth keyphrases. As a result, it can more robustly evaluate the quality of generated keyphrases. One potential future direction is to investigate the performance of other encoder-decoder architectures on keyphrase generation such as Transformer (Vaswani et al., 2017) with multi-head attention module (Li et al., 2018; Zhang et al., 2018a). Another interesting direction is to apply our RL approach on the microblog hashtag annotation problem (Wang et al., 2019; Gong and Zhang, 2016; Zhang et al., 2018b). Acknowledgments The work described in this paper was partially supported by the Research Grants Council of the Hong Kong Special Administrative Region, China (No. CUHK 14208815 of the General Research Fund) and Meitu (No. 7010445). Lu Wang is supported in part by National Science Foundation through Grants IIS-1566382 and IIS-1813341, and by the Office of the Director of National Intelligence (ODNI), Intelligence Advanced Resea"
P19-1208,S10-1004,0,\N,Missing
P19-1212,K16-1028,0,0.0912215,"Missing"
P19-1212,P17-1099,0,0.401231,"Missing"
P19-1212,J99-3001,0,0.250824,"Missing"
P19-1212,W04-3252,0,\N,Missing
P19-1212,N09-1041,0,\N,Missing
P19-1212,N04-1015,0,\N,Missing
P19-1212,J95-2003,0,\N,Missing
P19-1212,J08-1001,0,\N,Missing
P19-1212,P06-4018,0,\N,Missing
P19-1212,W12-3018,0,\N,Missing
P19-1212,E17-2112,0,\N,Missing
P19-1212,P17-1108,0,\N,Missing
P19-1212,D18-1206,0,\N,Missing
P19-1212,D18-1443,0,\N,Missing
P19-1255,E17-1024,0,0.151226,"Missing"
P19-1255,P18-1021,1,0.849881,"h indexes 12 million articles from Wikipedia and four popular English news media of varying ideological leanings. This ensures access to reliable evidence, high-quality reasoning, and diverse opinions from different sources, as opposed to recent work that mostly considers a single origin, such as Wikipedia (Rinott et al., 2015) or online debate portals (Wachsmuth et al., 2018b). We experiment with argument and counterargument pairs collected from the Reddit /r/ChangeMyView group. Automatic evaluation shows that the proposed model significantly outperforms our prior argument generation system (Hua and Wang, 2018) and other non-trivial comparisons. Human evaluation further suggests that our model produces more appropriate counter-arguments with richer content than other automatic systems, while maintaining a fluency level comparable to human-constructed arguments. 2 Related Work To date, the majority of the work on automatic argument generation leads to rule-based models, e.g., designing operators that reflect strategies from argumentation theory (Reed et al., 1996; Carenini and Moore, 2000). Information retrieval systems are recently developed to extract argu1 Code and data are available at https://xi"
P19-1255,A97-1039,0,0.32049,"rom the previous timestamp j − 1. During this step, an argumentative function label is predicted to indicate a desired language style for each sentence, and a subset of the keyphrases are selected from M (content selection) for the next sentence. In the second step, a content realization decoder (§ 5.3) generates the final counter-argument conditioned on previously generated tokens and the corresponding sentence representation sj . 5.2 Text Planning Decoder Text planning is an important component for natural language generation systems to decide on content structure for the target generation (Lavoie and Rambow, 1997; Reiter and Dale, 2000). We propose a text planner with two objectives: selecting talking points from the keyphrase memory M, and choosing a proper argumentative function per sentence. Concretely, we train a sentence-level LSTM that learns to generate a sequence of sentence representations {sj } given the selected keyphrase set C(j) as input for the j-th sentence: sj = f (sj−1 , X ek ) (1) ek ∈C(j) where f is an LSTM network, ek is the embedding for a selected phrase, represented by summing up all its words’ Glove embeddings (Pennington et al., 2014) in our experiments. Content Selection C(j)"
P19-1255,W00-1407,0,0.187389,"up. Automatic evaluation shows that the proposed model significantly outperforms our prior argument generation system (Hua and Wang, 2018) and other non-trivial comparisons. Human evaluation further suggests that our model produces more appropriate counter-arguments with richer content than other automatic systems, while maintaining a fluency level comparable to human-constructed arguments. 2 Related Work To date, the majority of the work on automatic argument generation leads to rule-based models, e.g., designing operators that reflect strategies from argumentation theory (Reed et al., 1996; Carenini and Moore, 2000). Information retrieval systems are recently developed to extract argu1 Code and data are available at https://xinyuhua. github.io/Resources/acl19/. ments relevant to a given debate motion (Sato et al., 2015). Although content ordering has been investigated (Reisert et al., 2015; Yanase et al., 2015), the output arguments are usually a collection of sentences from heterogeneous information sources, thus lacking coherence and conciseness. Our work aims to close the gap by generating eloquent and coherent arguments, assisted by an argument retrieval system. Recent progress in sequence-to-sequenc"
P19-1255,W18-5215,0,0.048892,"., 2015; Yanase et al., 2015), the output arguments are usually a collection of sentences from heterogeneous information sources, thus lacking coherence and conciseness. Our work aims to close the gap by generating eloquent and coherent arguments, assisted by an argument retrieval system. Recent progress in sequence-to-sequence (seq2seq) text generation models has delivered both fluent and content rich outputs by explicitly conducting content selection and ordering (Gehrmann et al., 2018; Wiseman et al., 2018), which is a promising avenue for enabling end-to-end counter-argument construction (Le et al., 2018). In particular, our prior work (Hua and Wang, 2018) leverages passages retrieved from Wikipedia to improve the quality of generated arguments, yet Wikipedia itself has the limitation of containing mostly facts. By leveraging Wikipedia and popular news media, our proposed pipeline can enrich the factual evidence with high-quality opinions and reasoning. Our work is also in line with argument retrieval research, where prior effort mostly considers single-origin information source (Rinott et al., 2015; Levy et al., 2018; Wachsmuth et al., 2017b, 2018b). Recent work by Stab et al. (2018) indexes"
P19-1255,C18-1176,0,0.0365045,"is a promising avenue for enabling end-to-end counter-argument construction (Le et al., 2018). In particular, our prior work (Hua and Wang, 2018) leverages passages retrieved from Wikipedia to improve the quality of generated arguments, yet Wikipedia itself has the limitation of containing mostly facts. By leveraging Wikipedia and popular news media, our proposed pipeline can enrich the factual evidence with high-quality opinions and reasoning. Our work is also in line with argument retrieval research, where prior effort mostly considers single-origin information source (Rinott et al., 2015; Levy et al., 2018; Wachsmuth et al., 2017b, 2018b). Recent work by Stab et al. (2018) indexes all web documents collected in Common Crawl, which inevitably incorporates noisy, lowquality content. Besides, existing work treats individual sentences as arguments, disregarding their crucial discourse structures and logical relations with adjacent sentences. Instead, we use multiple high-quality information sources, and construct paragraph-level passages to retain the context of arguments. 3 Overview of CANDELA Our counter-argument generation framework, as shown in Figure 2, has two main components: argument retrie"
P19-1255,W14-3348,0,0.0405824,", and an LSTM for content realization decoder based on two sources of data: 353K counter-arguments that are high quality root reply paragraphs extended with posts of non-negative karma, and 2.4 million retrieved passages randomly sampled from the training set. Both are trained as done in Bengio et al. (2003). We then use the first layer’s parameters to initialize all models, including our comparisons. 7 7.1 150 125 100 75 50 25 0 Results and Analysis Automatic Evaluation We employ ROUGE (Lin, 2004), a recall-oriented metric, BLEU (Papineni et al., 2002), based on n-gram precision, and METEOR (Denkowski and Lavie, 2014), measuring unigram precision and recall by considering synonyms, paraphrases, and stemming. BLEU-2, BLEU-4, ROUGE-2 recall, and METEOR are reported in Table 3 for both setups. Under system setup, our model CANDELA statistically significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < #distinct n-grams per argument Human Retrieval unigram Seq2seq Seq2seqAug bigram HW (2018) CANDELA trigram Figure 3: Average number of distinct n-grams per argument. K H UMAN R ETRIEVAL S EQ 2 SEQ S EQ 2 SEQ AUG H&W (2018) CANDELA 100 44.1"
P19-1255,D18-1443,0,0.0269291,"l19/. ments relevant to a given debate motion (Sato et al., 2015). Although content ordering has been investigated (Reisert et al., 2015; Yanase et al., 2015), the output arguments are usually a collection of sentences from heterogeneous information sources, thus lacking coherence and conciseness. Our work aims to close the gap by generating eloquent and coherent arguments, assisted by an argument retrieval system. Recent progress in sequence-to-sequence (seq2seq) text generation models has delivered both fluent and content rich outputs by explicitly conducting content selection and ordering (Gehrmann et al., 2018; Wiseman et al., 2018), which is a promising avenue for enabling end-to-end counter-argument construction (Le et al., 2018). In particular, our prior work (Hua and Wang, 2018) leverages passages retrieved from Wikipedia to improve the quality of generated arguments, yet Wikipedia itself has the limitation of containing mostly facts. By leveraging Wikipedia and popular news media, our proposed pipeline can enrich the factual evidence with high-quality opinions and reasoning. Our work is also in line with argument retrieval research, where prior effort mostly considers single-origin information"
P19-1255,W04-1013,0,0.0248496,"om {5, 10, 15} on validation. We also pre-train a biLSTM for encoder based on all OPs from the training set, and an LSTM for content realization decoder based on two sources of data: 353K counter-arguments that are high quality root reply paragraphs extended with posts of non-negative karma, and 2.4 million retrieved passages randomly sampled from the training set. Both are trained as done in Bengio et al. (2003). We then use the first layer’s parameters to initialize all models, including our comparisons. 7 7.1 150 125 100 75 50 25 0 Results and Analysis Automatic Evaluation We employ ROUGE (Lin, 2004), a recall-oriented metric, BLEU (Papineni et al., 2002), based on n-gram precision, and METEOR (Denkowski and Lavie, 2014), measuring unigram precision and recall by considering synonyms, paraphrases, and stemming. BLEU-2, BLEU-4, ROUGE-2 recall, and METEOR are reported in Table 3 for both setups. Under system setup, our model CANDELA statistically significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < #distinct n-grams per argument Human Retrieval unigram Seq2seq Seq2seqAug bigram HW (2018) CANDELA trigram Figure 3:"
P19-1255,C00-1072,0,0.490436,"passages ranked by BM25 (Robertson et al., 1995) are retained, per medium. All passages retrieved for the input statement are merged and deduplicated, and they will 2663 be ranked as discussed in § 4.3. 5 4.2 5.1 Keyphrase Extraction Here we describe a keyphrase extraction procedure for both input statements and retrieved passages, which will be utilized for passage ranking as detailed in the next section. For input statement, our goal is to identify a set of phrases representing the issues under discussion, such as “death penalty” in Figure 1. We thus first extract the topic signature words (Lin and Hovy, 2000) for input representation, and expand them into phrases that better capture semantic meanings. Concretely, topic signature words of an input statement are calculated against all input statements in our training set with log-likelihood ratio test. In order to cover phrases with related terms, we further expand this set with their synonyms, hyponyms, hypernyms, and antonyms based on WordNet (Miller, 1994). The statements are first parsed with Stanford part-of-speech tagger (Manning et al., 2014). Then regular expressions are applied to extract candidate noun phrases and verb phrases (details in"
P19-1255,P14-5010,0,0.0107435,"under discussion, such as “death penalty” in Figure 1. We thus first extract the topic signature words (Lin and Hovy, 2000) for input representation, and expand them into phrases that better capture semantic meanings. Concretely, topic signature words of an input statement are calculated against all input statements in our training set with log-likelihood ratio test. In order to cover phrases with related terms, we further expand this set with their synonyms, hyponyms, hypernyms, and antonyms based on WordNet (Miller, 1994). The statements are first parsed with Stanford part-of-speech tagger (Manning et al., 2014). Then regular expressions are applied to extract candidate noun phrases and verb phrases (details in Appendix A.1). A keyphrase is selected if it contains: (1) at least one content word, (2) no more than 10 tokens, and (3) at least one topic signature word or a Wikipedia article title. For retrieved passages, their keyphrases are extracted using the same procedure as above, except that the input statement’s topic signature words are used as references again. 4.3 Passage Ranking and Filtering We merge the retrieved passages from all media and rank them based on the number of words in overlappi"
P19-1255,H94-1111,0,0.886395,"r input statement, our goal is to identify a set of phrases representing the issues under discussion, such as “death penalty” in Figure 1. We thus first extract the topic signature words (Lin and Hovy, 2000) for input representation, and expand them into phrases that better capture semantic meanings. Concretely, topic signature words of an input statement are calculated against all input statements in our training set with log-likelihood ratio test. In order to cover phrases with related terms, we further expand this set with their synonyms, hyponyms, hypernyms, and antonyms based on WordNet (Miller, 1994). The statements are first parsed with Stanford part-of-speech tagger (Manning et al., 2014). Then regular expressions are applied to extract candidate noun phrases and verb phrases (details in Appendix A.1). A keyphrase is selected if it contains: (1) at least one content word, (2) no more than 10 tokens, and (3) at least one topic signature word or a Wikipedia article title. For retrieved passages, their keyphrases are extracted using the same procedure as above, except that the input statement’s topic signature words are used as references again. 4.3 Passage Ranking and Filtering We merge t"
P19-1255,P02-1040,0,0.104991,"ain a biLSTM for encoder based on all OPs from the training set, and an LSTM for content realization decoder based on two sources of data: 353K counter-arguments that are high quality root reply paragraphs extended with posts of non-negative karma, and 2.4 million retrieved passages randomly sampled from the training set. Both are trained as done in Bengio et al. (2003). We then use the first layer’s parameters to initialize all models, including our comparisons. 7 7.1 150 125 100 75 50 25 0 Results and Analysis Automatic Evaluation We employ ROUGE (Lin, 2004), a recall-oriented metric, BLEU (Papineni et al., 2002), based on n-gram precision, and METEOR (Denkowski and Lavie, 2014), measuring unigram precision and recall by considering synonyms, paraphrases, and stemming. BLEU-2, BLEU-4, ROUGE-2 recall, and METEOR are reported in Table 3 for both setups. Under system setup, our model CANDELA statistically significantly outperforms all comparisons and the retrieval model in all metrics, based on a randomization test (Noreen, 1989) (p < #distinct n-grams per argument Human Retrieval unigram Seq2seq Seq2seqAug bigram HW (2018) CANDELA trigram Figure 3: Average number of distinct n-grams per argument. K H UM"
P19-1255,D14-1162,0,0.0818315,"ontent structure for the target generation (Lavoie and Rambow, 1997; Reiter and Dale, 2000). We propose a text planner with two objectives: selecting talking points from the keyphrase memory M, and choosing a proper argumentative function per sentence. Concretely, we train a sentence-level LSTM that learns to generate a sequence of sentence representations {sj } given the selected keyphrase set C(j) as input for the j-th sentence: sj = f (sj−1 , X ek ) (1) ek ∈C(j) where f is an LSTM network, ek is the embedding for a selected phrase, represented by summing up all its words’ Glove embeddings (Pennington et al., 2014) in our experiments. Content Selection C(j). We propose an attention mechanism to conduct content selection and yield 2664 C(j) from the representation of the previous sentence sj−1 to encourage topical coherence. To allow the selection of multiple keyphrases, we use the sigmoid function to calculate the score: αjm = sigmoid(em W pa sj−1 ) The argumentative function label yjp for the j-th sentence is calculated as follows: p P (yjp |y<j , X) = softmax(wpT (tanh (W po [cj ; sj ])) + bp ) X cj = αjm em (2) where W pa are trainable parameters, keyphrases with αjm &gt; 0.5 are included in C(j), and t"
P19-1255,prasad-etal-2008-penn,0,0.054661,"Missing"
P19-1255,W15-0507,0,0.283187,"challenging task, as it requires an appropriate combination of credible evidence, rigorous logical reasoning, and sometimes emotional appeal (Walton et al., 2008; Wachsmuth et al., 2017a; Wang et al., 2017). A sample counter-argument for a pro-death penalty post is shown in Figure 1. As can be seen, a sequence of talking points on the “imperfect justice system” are presented: it starts with the fundamental concept, then follows up with more specific evaluative claim and supporting fact. Although retrieval-based methods have been investigated to construct counter-arguments (Sato et al., 2015; Reisert et al., 2015), they typically produce a collection of sentences from disparate sources, thus fall short of coherence and conciseness. Moreover, human always deploy stylistic languages with specific argumentative functions to promote persuasiveness, such as making a concessive move (e.g., “In theory I agree with you""). This further requires the generation system to have better control of the languages style. Our goal is to design a counter-argument generation system to address the above challenges and 2661 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 2661–26"
P19-1255,D15-1050,0,0.134916,"handle content selection and ordering, and select a proper argumentative discourse function of a desired language style for each sentence generation. Lastly, the input to our argument generation model is augmented with keyphrases and passages retrieved from a large-scale search engine, which indexes 12 million articles from Wikipedia and four popular English news media of varying ideological leanings. This ensures access to reliable evidence, high-quality reasoning, and diverse opinions from different sources, as opposed to recent work that mostly considers a single origin, such as Wikipedia (Rinott et al., 2015) or online debate portals (Wachsmuth et al., 2018b). We experiment with argument and counterargument pairs collected from the Reddit /r/ChangeMyView group. Automatic evaluation shows that the proposed model significantly outperforms our prior argument generation system (Hua and Wang, 2018) and other non-trivial comparisons. Human evaluation further suggests that our model produces more appropriate counter-arguments with richer content than other automatic systems, while maintaining a fluency level comparable to human-constructed arguments. 2 Related Work To date, the majority of the work on au"
P19-1255,P15-4019,0,0.26619,"sive arguments is a challenging task, as it requires an appropriate combination of credible evidence, rigorous logical reasoning, and sometimes emotional appeal (Walton et al., 2008; Wachsmuth et al., 2017a; Wang et al., 2017). A sample counter-argument for a pro-death penalty post is shown in Figure 1. As can be seen, a sequence of talking points on the “imperfect justice system” are presented: it starts with the fundamental concept, then follows up with more specific evaluative claim and supporting fact. Although retrieval-based methods have been investigated to construct counter-arguments (Sato et al., 2015; Reisert et al., 2015), they typically produce a collection of sentences from disparate sources, thus fall short of coherence and conciseness. Moreover, human always deploy stylistic languages with specific argumentative functions to promote persuasiveness, such as making a concessive move (e.g., “In theory I agree with you""). This further requires the generation system to have better control of the languages style. Our goal is to design a counter-argument generation system to address the above challenges and 2661 Proceedings of the 57th Annual Meeting of the Association for Computational Lin"
P19-1255,N18-5005,0,0.233601,"struction (Le et al., 2018). In particular, our prior work (Hua and Wang, 2018) leverages passages retrieved from Wikipedia to improve the quality of generated arguments, yet Wikipedia itself has the limitation of containing mostly facts. By leveraging Wikipedia and popular news media, our proposed pipeline can enrich the factual evidence with high-quality opinions and reasoning. Our work is also in line with argument retrieval research, where prior effort mostly considers single-origin information source (Rinott et al., 2015; Levy et al., 2018; Wachsmuth et al., 2017b, 2018b). Recent work by Stab et al. (2018) indexes all web documents collected in Common Crawl, which inevitably incorporates noisy, lowquality content. Besides, existing work treats individual sentences as arguments, disregarding their crucial discourse structures and logical relations with adjacent sentences. Instead, we use multiple high-quality information sources, and construct paragraph-level passages to retain the context of arguments. 3 Overview of CANDELA Our counter-argument generation framework, as shown in Figure 2, has two main components: argument retrieval model (§ 4) that takes the input statement and a search engine,"
P19-1255,P17-2039,0,0.0710858,"rguments of a different stance, in order to refute the given proposition on a controversial issue (Toulmin, 1958; Damer, 2012). A system that automatically constructs counter-arguments can effectively present alternative perspectives along with associated evidence and reasoning, and thus facilitate a more comprehensive understanding of complicated problems when controversy arises. Nevertheless, constructing persuasive arguments is a challenging task, as it requires an appropriate combination of credible evidence, rigorous logical reasoning, and sometimes emotional appeal (Walton et al., 2008; Wachsmuth et al., 2017a; Wang et al., 2017). A sample counter-argument for a pro-death penalty post is shown in Figure 1. As can be seen, a sequence of talking points on the “imperfect justice system” are presented: it starts with the fundamental concept, then follows up with more specific evaluative claim and supporting fact. Although retrieval-based methods have been investigated to construct counter-arguments (Sato et al., 2015; Reisert et al., 2015), they typically produce a collection of sentences from disparate sources, thus fall short of coherence and conciseness. Moreover, human always deploy stylistic lang"
P19-1255,W17-5106,0,0.228872,"rguments of a different stance, in order to refute the given proposition on a controversial issue (Toulmin, 1958; Damer, 2012). A system that automatically constructs counter-arguments can effectively present alternative perspectives along with associated evidence and reasoning, and thus facilitate a more comprehensive understanding of complicated problems when controversy arises. Nevertheless, constructing persuasive arguments is a challenging task, as it requires an appropriate combination of credible evidence, rigorous logical reasoning, and sometimes emotional appeal (Walton et al., 2008; Wachsmuth et al., 2017a; Wang et al., 2017). A sample counter-argument for a pro-death penalty post is shown in Figure 1. As can be seen, a sequence of talking points on the “imperfect justice system” are presented: it starts with the fundamental concept, then follows up with more specific evaluative claim and supporting fact. Although retrieval-based methods have been investigated to construct counter-arguments (Sato et al., 2015; Reisert et al., 2015), they typically produce a collection of sentences from disparate sources, thus fall short of coherence and conciseness. Moreover, human always deploy stylistic lang"
P19-1255,C18-1318,0,0.216544,"Missing"
P19-1255,P18-1023,0,0.0347781,"ct a proper argumentative discourse function of a desired language style for each sentence generation. Lastly, the input to our argument generation model is augmented with keyphrases and passages retrieved from a large-scale search engine, which indexes 12 million articles from Wikipedia and four popular English news media of varying ideological leanings. This ensures access to reliable evidence, high-quality reasoning, and diverse opinions from different sources, as opposed to recent work that mostly considers a single origin, such as Wikipedia (Rinott et al., 2015) or online debate portals (Wachsmuth et al., 2018b). We experiment with argument and counterargument pairs collected from the Reddit /r/ChangeMyView group. Automatic evaluation shows that the proposed model significantly outperforms our prior argument generation system (Hua and Wang, 2018) and other non-trivial comparisons. Human evaluation further suggests that our model produces more appropriate counter-arguments with richer content than other automatic systems, while maintaining a fluency level comparable to human-constructed arguments. 2 Related Work To date, the majority of the work on automatic argument generation leads to rule-based m"
P19-1255,Q17-1016,1,0.843352,"tance, in order to refute the given proposition on a controversial issue (Toulmin, 1958; Damer, 2012). A system that automatically constructs counter-arguments can effectively present alternative perspectives along with associated evidence and reasoning, and thus facilitate a more comprehensive understanding of complicated problems when controversy arises. Nevertheless, constructing persuasive arguments is a challenging task, as it requires an appropriate combination of credible evidence, rigorous logical reasoning, and sometimes emotional appeal (Walton et al., 2008; Wachsmuth et al., 2017a; Wang et al., 2017). A sample counter-argument for a pro-death penalty post is shown in Figure 1. As can be seen, a sequence of talking points on the “imperfect justice system” are presented: it starts with the fundamental concept, then follows up with more specific evaluative claim and supporting fact. Although retrieval-based methods have been investigated to construct counter-arguments (Sato et al., 2015; Reisert et al., 2015), they typically produce a collection of sentences from disparate sources, thus fall short of coherence and conciseness. Moreover, human always deploy stylistic languages with specific a"
P19-1255,D18-1356,0,0.0270514,"a given debate motion (Sato et al., 2015). Although content ordering has been investigated (Reisert et al., 2015; Yanase et al., 2015), the output arguments are usually a collection of sentences from heterogeneous information sources, thus lacking coherence and conciseness. Our work aims to close the gap by generating eloquent and coherent arguments, assisted by an argument retrieval system. Recent progress in sequence-to-sequence (seq2seq) text generation models has delivered both fluent and content rich outputs by explicitly conducting content selection and ordering (Gehrmann et al., 2018; Wiseman et al., 2018), which is a promising avenue for enabling end-to-end counter-argument construction (Le et al., 2018). In particular, our prior work (Hua and Wang, 2018) leverages passages retrieved from Wikipedia to improve the quality of generated arguments, yet Wikipedia itself has the limitation of containing mostly facts. By leveraging Wikipedia and popular news media, our proposed pipeline can enrich the factual evidence with high-quality opinions and reasoning. Our work is also in line with argument retrieval research, where prior effort mostly considers single-origin information source (Rinott et al.,"
P19-1255,W15-0512,0,0.0972424,"c systems, while maintaining a fluency level comparable to human-constructed arguments. 2 Related Work To date, the majority of the work on automatic argument generation leads to rule-based models, e.g., designing operators that reflect strategies from argumentation theory (Reed et al., 1996; Carenini and Moore, 2000). Information retrieval systems are recently developed to extract argu1 Code and data are available at https://xinyuhua. github.io/Resources/acl19/. ments relevant to a given debate motion (Sato et al., 2015). Although content ordering has been investigated (Reisert et al., 2015; Yanase et al., 2015), the output arguments are usually a collection of sentences from heterogeneous information sources, thus lacking coherence and conciseness. Our work aims to close the gap by generating eloquent and coherent arguments, assisted by an argument retrieval system. Recent progress in sequence-to-sequence (seq2seq) text generation models has delivered both fluent and content rich outputs by explicitly conducting content selection and ordering (Gehrmann et al., 2018; Wiseman et al., 2018), which is a promising avenue for enabling end-to-end counter-argument construction (Le et al., 2018). In particul"
P19-1255,miltsakaki-etal-2004-penn,0,\N,Missing
P19-1255,W17-5102,0,\N,Missing
P19-1255,N16-1014,0,\N,Missing
P19-1270,N12-1074,0,0.771335,"Missing"
P19-1270,D17-1243,0,0.0884173,"g Embedding t1 LSTM a1 . . . . . . t|c| Context Modeling Layer ...... Turn Encoder Embedding a|c| m1 H& |&apos;| Embedding ...... m|u| User History Modeling Layer Figure 2: The generic framework for re-entry prediction. We implement it with three encoders (Average Embedding, CNN, and BiLSTM) for turn modeling and four mechanisms (Simple Concatenation, Attention, Memory Networks, and Bi-attention) for modeling interactions between context and user history. better understand the structure of conversations, Recurrent Neural Network (RNN)-based methods have been exploited to capture temporal dynamics (Cheng et al., 2017; Zayats and Ostendorf, 2018; Jiao et al., 2018). Different from the above work, our model not only utilizes the conversations themselves, but also leverages users’ prior posts in other discussions. 3 Neural Re-entry Prediction Combining Context and User History This section describes our neural network-based conversation re-entry prediction framework exploring the joint effects of context and user history. Figure 2 shows the overall architecture of our framework, consisting of three main layers: context modeling layer, user history modeling layer, and interaction modeling layer to learn how i"
P19-1270,N10-1020,0,0.204523,"Missing"
P19-1270,P12-1054,0,0.419325,"bi-attention outperforms both humans, suggesting the difficulty of the task as well as the effectiveness of our proposed framework. 2 Related Work Response Prediction. Previous work on response prediction mainly focuses on predicting whether users will respond to a given social media post or thread. Efforts have been made to measure the popularity of a social media post via modeling the response patterns in replies or retweets (Artzi et al., 2012; Zhang et al., 2015). Some studies investigate post recommendation by predicting whether a response will be made by a given user (Chen et al., 2012; Yan et al., 2012; Hong et al., 2013; Alawad et al., 2016). In addition to post-level prediction, other studies focus on response prediction at the conversation-level. Zeng et al. (2018) investigate microblog conversation recommendation by exploiting latent factors of topics and discourse with a Bayesian model, which often requires domain expertise for customized learning algorithms. Our neural framework can automatically acquire the interactions among important components that contribute to the re-entry prediction problem, and can be easily adapted to new domains. For the prediction of re-entry behavior in on"
P19-1270,Q18-1009,0,0.0829916,"a1 . . . . . . t|c| Context Modeling Layer ...... Turn Encoder Embedding a|c| m1 H& |&apos;| Embedding ...... m|u| User History Modeling Layer Figure 2: The generic framework for re-entry prediction. We implement it with three encoders (Average Embedding, CNN, and BiLSTM) for turn modeling and four mechanisms (Simple Concatenation, Attention, Memory Networks, and Bi-attention) for modeling interactions between context and user history. better understand the structure of conversations, Recurrent Neural Network (RNN)-based methods have been exploited to capture temporal dynamics (Cheng et al., 2017; Zayats and Ostendorf, 2018; Jiao et al., 2018). Different from the above work, our model not only utilizes the conversations themselves, but also leverages users’ prior posts in other discussions. 3 Neural Re-entry Prediction Combining Context and User History This section describes our neural network-based conversation re-entry prediction framework exploring the joint effects of context and user history. Figure 2 shows the overall architecture of our framework, consisting of three main layers: context modeling layer, user history modeling layer, and interaction modeling layer to learn how information captured by the p"
P19-1270,N18-1035,1,0.907686,"t). More importantly, our framework enables the re-entry prediction and corresponding representations to be learned in an end-to-end manner. On the contrary, previous methods for the same task rely on handcrafted features (Backstrom et al., 2013; Budak and Agrawal, 2013), which often require laborintensive and time-consuming feature engineering processes. To the best of our knowledge, we are the first to explore the joint effect of conversation context and user history on predicting re-entry behavior in a neural network framework. We experiment with two large-scale datasets, one from Twitter (Zeng et al., 2018), the other from Reddit which is newly collected1 . Our framework with bi-attention significantly outperforms all the comparing methods including the previous state of the art (Backstrom et al., 2013). For instance, our model achieves an F1 score of 61.1 on Twitter conversations, compared to an F1 score of 57.0 produced by Backstrom et al. (2013), which is based on a rich set of handcrafted features. Further experiments also show that the model with bi-attention can consistently outperform comparisons given varying lengths of conversation context. It shows that bi-attention mechanism can well"
P19-1270,P18-1125,0,0.0787958,"Missing"
P19-1270,W02-0109,0,0.0960075,"itter and Reddit users exhibit different conversation behaviors. Reddit users tend to engage in more conversations, resulting in more messages in user history (as shown in Figure 3(a)). Twitter users are more likely to stay within each conversation, leading to lengthy discussions and larger re-entry frequencies on average, as shown in Figure 3(b) and Table 1. 2813 Data Preprocessing and Model Setting. For preprocessing Twitter data, we applied Glove tweet preprocessing toolkit (Pennington et al., 2014).5 For the Reddit dataset, we first applied the open source natural language toolkit (NLTK) (Loper and Bird, 2002) for word tokenization. Then, we replaced links with the generic tag “URL” and removed all the nonalphabetic tokens. For both datasets, a vocabulary was built and maintained in experiments with all the tokens (including emoticons and punctuation) from training data. For model setups, we initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014), where Twitter version is used for our Twitter dataset and the Common Crawl version applied on Reddit dataset.6 All the hyper-parameters are tuned on the development set by grid search. The batch size is set to 32. Ada"
P19-1270,D15-1166,0,0.0601222,"sigmoid-activated neural perceptron (Glorot et al., 2011), for predicting final output p(u, c). It indicates how likely the target user u will re-engage in the target conversation c. We then describe the four mechanisms to learn rO in turn below. Simple Concatenation. Here we simply put context representation (last state) and user representations (with average pooling) side by side, P|u |U C; yielding rO = [H|c| j Hj /|u|] as the interaction representation for re-entry prediction. Attention. To capture the context information useful for re-entry prediction, we exploit an attention mechanism (Luong et al., 2015) over H C . Attentions are employed to “soft-address” important context turns according to their similarity with user representation (with average pooling). Here we adopt dot attention weights and define the attended interaction representation as: rO = |c| X i αi · HiC , αi = sof tmax(HiC · |u| X HjU /|u|) (1) j Memory Networks. To further recognize indicative chatting messages in user history, we also apply end-to-end memory networks (MemN2N) (Sukhbaatar et al., 2015) for interaction modeling. It can be seen as a recurrent attention mechanism over chatting messages (stored in memory). Hence f"
P19-1270,D14-1162,0,0.0820763,"context. Thus combining them both might help alleviate the sparsity in one information source. We also notice that Twitter and Reddit users exhibit different conversation behaviors. Reddit users tend to engage in more conversations, resulting in more messages in user history (as shown in Figure 3(a)). Twitter users are more likely to stay within each conversation, leading to lengthy discussions and larger re-entry frequencies on average, as shown in Figure 3(b) and Table 1. 2813 Data Preprocessing and Model Setting. For preprocessing Twitter data, we applied Glove tweet preprocessing toolkit (Pennington et al., 2014).5 For the Reddit dataset, we first applied the open source natural language toolkit (NLTK) (Loper and Bird, 2002) for word tokenization. Then, we replaced links with the generic tag “URL” and removed all the nonalphabetic tokens. For both datasets, a vocabulary was built and maintained in experiments with all the tokens (including emoticons and punctuation) from training data. For model setups, we initialize the embedding layer with 200-dimensional Glove embedding (Pennington et al., 2014), where Twitter version is used for our Twitter dataset and the Common Crawl version applied on Reddit da"
P19-1270,P06-4018,0,\N,Missing
Q17-1016,W11-0707,0,0.02547,"usage of personal pronouns may imply communicative goals of the speaker (Brown and Gilman, 1960; Wilson, 1990). We also count the frequency of each POS tag output by Stanford parser (Klein and Manning, 2003). Sentiment and emotional language usage is prevalent in discussions on controversial topics (Wang and Cardie, 2014b). We thus count words of positive and negative sentiment based on MPQA lexicon (Wilson et al., 2005), and words per emotion type according to a lexicon from Mohammad and Turney (2013). Moreover, based on the intuition that agreement carries indications on topical alignment (Bender et al., 2011; Wang and Cardie, 2014a), occurrence of agreement phrases (“I/we agree”, “you’re right”) is calculated. Finally, audience feedback, including applause and laughter, is also considered. Style features. Existing work suggests that formality can reveal speakers’ opinions or intentions (Irvine, 1979). Here we utilize a formality lexicon collected by Brooke et al. (2010), which counts the frequencies of formal or informal words in each argument. According to Durik et al. (2008), hedges are indicators of weak arguments, so we compile a list of hedge words from Hyland (2005), and hedging of verbs an"
Q17-1016,C10-2011,0,0.0138498,"ive sentiment based on MPQA lexicon (Wilson et al., 2005), and words per emotion type according to a lexicon from Mohammad and Turney (2013). Moreover, based on the intuition that agreement carries indications on topical alignment (Bender et al., 2011; Wang and Cardie, 2014a), occurrence of agreement phrases (“I/we agree”, “you’re right”) is calculated. Finally, audience feedback, including applause and laughter, is also considered. Style features. Existing work suggests that formality can reveal speakers’ opinions or intentions (Irvine, 1979). Here we utilize a formality lexicon collected by Brooke et al. (2010), which counts the frequencies of formal or informal words in each argument. According to Durik et al. (2008), hedges are indicators of weak arguments, so we compile a list of hedge words from Hyland (2005), and hedging of verbs and non-verbs are counted separately. Lastly, we measure word attributes for their concreteness (perceptible vs. conceptual), valence (or pleasantness), arousal (or intensity of emotion), and dominance (or degree of control) based on the lexicons collected by Brysbaert et al. (2014) and Warriner et al. (2013), following Tan et al. (2016), who observe correlations betwe"
Q17-1016,N16-1166,0,0.0610706,"s. Speakers generally do not just repeat their best argument ad infinitum, which suggests that arguments may lose power with repetition. For each argument, we add an indicator feature (i.e. each argument takes value of 1) and an additional version with a decay factor of exp(−α · tk ), where tk is the number of preceding arguments by a given side which used topic k, and α is fixed at 1.0. Interruption is also measured, when the last argument (of more than 50 words) in a turn is cut off by at most two sentences from opponent or moderator. Word repetition is often used for emphasis in arguments (Cano-Basave and He, 2016), so we measure bigram repetition more than twice in sequential clauses or sentences. Interaction features. In addition to independent language usage, debate strategies are also shaped by interactions with other debaters. For instance, previous work (Zhang et al., 2016) finds that debate winners frequently pursue talking points brought up by their opponents’. Here we construct different types of features to measure how debaters address opponents’ arguments and shift to their favorable subjects. First, for a given argument, we detect if there is an argument of the same topic from the previous t"
Q17-1016,P15-2072,0,0.0184112,"work complements theirs in examining topic interactions, but brings additional focus on the latent persuasive strength of topics, as well as strength interactions. Tan et al. (2016) examines various structural and linguistic features associated with persuasion on Reddit; they find that some topics correlate more with malleable opinions. Here we develop a more general model of latent topic strength and the linguistic features associated with strength. 229 Additional work has focused on the influence of agenda setting — controlling which topics are discussed (Nguyen et al., 2014), and framing (Card et al., 2015; Tsur et al., 2015) — emphasizing certain aspects or interpretations of an issue. Greene and Resnik (2009) study the syntactic aspects of framing, where syntactic choices are found to correlate with the sentiment perceived by readers. Based on the topic shifting model of Nguyen et al. (2014), Prabhakaran et al. (2014) finds that changing topics in presidential primary debates positively correlates with the candidates’ power, which is measured based on their relative standing in recent public polls. This supports our finding that both sides seek to shift topics, but that winners are more likel"
Q17-1016,N10-1066,0,0.0125898,"n kwk2 + C · w 2 hi i We consider samples based on difference feature ˜ p (xi , hi ) during training, which is reprevectors Φ ˜ i , hi ) in Eq. 1 and the rest of this secsented as Φ(x tion. l(·) is squared-hinge loss function. C controls the trade-off between the two items. This objective function is non-convex due to the maximum operation (Yu and Joachims, 2009). We utilize Alg. 1, which is an iterative optimization algorithm, to search for the solution for w and h. We first initialize latent topic strength variables as h0 (see next paragraph) and learn the weight vector as w∗ . Adopted from Chang et al. (2010), our iterative algorithm consists of two major steps. For each iteration, the algorithm first decides the latent variables for positive examples. In the second step, the solver iteratively searches for latent variable assignments for negative samples and updates the weight vector w with a cutting plane algorithm until convergence. Global variable Hi is maintained for each negative sample to store all the topic strength assignments that give the highest scores during training.6 This strategy facilitates efficient training while a local optimum is guaranteed. Topic strength initialization. We i"
Q17-1016,J14-1002,0,0.0182969,"following Tan et al. (2016), who observe correlations between word attributes and their persuasive effect on online arguments. The average score for each of these features is then computed for each argument. Semantic features. We encode semantic information via semantic frames (Fillmore, 1976), which represent the context of word meanings. CanoBasave and He (2016) show that arguments of different types tend to employ different semantic frames, e.g., frames of “reason” and “evaluative comparison” are frequently used in making claims. We count the frequency of each frame, as labeled by SEMAFOR (Das et al., 2014). Discourse features. The usage of discourse connectors has been shown to be effective for detecting argumentative structure in essays (Stab and Gurevych, 2014). We collect discourse connectives from the Penn Discourse Treebank (Prasad et al., 2007), and count the frequency of phrases for each discourse class. Four classes on level one (temporal, comparison, contingency, and expansion) and sixteen classes on level two are considered. Finally, pleading be223 havior is encoded as counting phrases of “urge”, “please”, “ask you”, and “encourage you”, which may be used by debaters to appeal to the"
Q17-1016,P11-1099,0,0.0287663,"This ideal vision of debate and deliberation has taken an increasingly central role in modern theories of democracy (Habermas, 1984; Cohen, 1989; Rawls, 1997; Mansbridge, 2003). However, empirical evidence has also led to an increasing awareness of the dangers of style and rhetoric in biasing participants towards the most skillful, charismatic, or numerous speakers (NoelleNeumann, 1974; Sunstein, 1999). In light of these concerns, most efforts to predict the persuasive effects of debate have focused on the linguistic features of debate speech (Katzav and Reed, 2008; Mochales and Moens, 2011; Feng and Hirst, 2011) or on simple measures of topic control (Dryzek and List, 2003; Mansbridge, 2015; Zhang et al., 2016). In the ideal setting, however, we would wish for the winning side to win based on the strength and merits of their arguments, not based on their skillful deployment of linguistic style. Our model therefore predicts debate outcomes by modeling not just the persuasive effects of directly observable linguistic features, but also by incorporating the inherent, latent strengths of topics and issues specific to each side of a debate. To illustrate this idea, Figure 1 shows a brief ex219 Transaction"
Q17-1016,E14-1069,0,0.0607353,"Missing"
Q17-1016,N09-1057,0,0.0107425,"persuasive strength of topics, as well as strength interactions. Tan et al. (2016) examines various structural and linguistic features associated with persuasion on Reddit; they find that some topics correlate more with malleable opinions. Here we develop a more general model of latent topic strength and the linguistic features associated with strength. 229 Additional work has focused on the influence of agenda setting — controlling which topics are discussed (Nguyen et al., 2014), and framing (Card et al., 2015; Tsur et al., 2015) — emphasizing certain aspects or interpretations of an issue. Greene and Resnik (2009) study the syntactic aspects of framing, where syntactic choices are found to correlate with the sentiment perceived by readers. Based on the topic shifting model of Nguyen et al. (2014), Prabhakaran et al. (2014) finds that changing topics in presidential primary debates positively correlates with the candidates’ power, which is measured based on their relative standing in recent public polls. This supports our finding that both sides seek to shift topics, but that winners are more likely to shift to topics which are strong for them but weak for their opponents. Our work is in line with argum"
Q17-1016,D14-1083,0,0.0213666,"sed on their relative standing in recent public polls. This supports our finding that both sides seek to shift topics, but that winners are more likely to shift to topics which are strong for them but weak for their opponents. Our work is in line with argumentation mining. Existing work in this area focuses on argument extraction (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011) and argument scheme classification (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014). Though stance prediction has also been studied (Thomas et al., 2006; Hasan and Ng, 2014), we are not aware of any work that extracts arguments according to topics and position. Argument strength prediction is also studied largely in the domain of student essays (Higgins et al., 2004; Stab and Gurevych, 2014; Persing and Ng, 2015). Notably, none of these distinguishes an argument’s strength from its linguistic surface features. This is a gap we aim to fill. 7 Conclusion We present a debate prediction model that learns latent persuasive strengths of topics, linguistic style of arguments, and the interactions between the two. Experiments on debate outcome prediction indicate that ou"
Q17-1016,N04-1024,0,0.0288024,"em but weak for their opponents. Our work is in line with argumentation mining. Existing work in this area focuses on argument extraction (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011) and argument scheme classification (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014). Though stance prediction has also been studied (Thomas et al., 2006; Hasan and Ng, 2014), we are not aware of any work that extracts arguments according to topics and position. Argument strength prediction is also studied largely in the domain of student essays (Higgins et al., 2004; Stab and Gurevych, 2014; Persing and Ng, 2015). Notably, none of these distinguishes an argument’s strength from its linguistic surface features. This is a gap we aim to fill. 7 Conclusion We present a debate prediction model that learns latent persuasive strengths of topics, linguistic style of arguments, and the interactions between the two. Experiments on debate outcome prediction indicate that our model outperforms comparisons using audience responses or linguistic features alone. Our model also shows that winners use stronger arguments and strategically shift topics to stronger ground."
Q17-1016,P03-1054,0,0.0338628,"ers. 3.3 Features We group our directly observed linguistic features, roughly ordered by increasing complexity, into categories that characterize various aspects of arguments. For each linguistic feature, we compute two versions: one for the full debate and one for the discussion phase. Basic features. We consider the frequencies of words, numbers, named entities per type, and each personal pronoun. For instance, usage of personal pronouns may imply communicative goals of the speaker (Brown and Gilman, 1960; Wilson, 1990). We also count the frequency of each POS tag output by Stanford parser (Klein and Manning, 2003). Sentiment and emotional language usage is prevalent in discussions on controversial topics (Wang and Cardie, 2014b). We thus count words of positive and negative sentiment based on MPQA lexicon (Wilson et al., 2005), and words per emotion type according to a lexicon from Mohammad and Turney (2013). Moreover, based on the intuition that agreement carries indications on topical alignment (Bender et al., 2011; Wang and Cardie, 2014a), occurrence of agreement phrases (“I/we agree”, “you’re right”) is calculated. Finally, audience feedback, including applause and laughter, is also considered. Sty"
Q17-1016,P15-1139,0,0.0668699,"Missing"
Q17-1016,P15-1053,0,0.0163508,"line with argumentation mining. Existing work in this area focuses on argument extraction (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011) and argument scheme classification (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014). Though stance prediction has also been studied (Thomas et al., 2006; Hasan and Ng, 2014), we are not aware of any work that extracts arguments according to topics and position. Argument strength prediction is also studied largely in the domain of student essays (Higgins et al., 2004; Stab and Gurevych, 2014; Persing and Ng, 2015). Notably, none of these distinguishes an argument’s strength from its linguistic surface features. This is a gap we aim to fill. 7 Conclusion We present a debate prediction model that learns latent persuasive strengths of topics, linguistic style of arguments, and the interactions between the two. Experiments on debate outcome prediction indicate that our model outperforms comparisons using audience responses or linguistic features alone. Our model also shows that winners use stronger arguments and strategically shift topics to stronger ground. We also find that strong and weak arguments diff"
Q17-1016,D14-1157,0,0.0364185,"re with malleable opinions. Here we develop a more general model of latent topic strength and the linguistic features associated with strength. 229 Additional work has focused on the influence of agenda setting — controlling which topics are discussed (Nguyen et al., 2014), and framing (Card et al., 2015; Tsur et al., 2015) — emphasizing certain aspects or interpretations of an issue. Greene and Resnik (2009) study the syntactic aspects of framing, where syntactic choices are found to correlate with the sentiment perceived by readers. Based on the topic shifting model of Nguyen et al. (2014), Prabhakaran et al. (2014) finds that changing topics in presidential primary debates positively correlates with the candidates’ power, which is measured based on their relative standing in recent public polls. This supports our finding that both sides seek to shift topics, but that winners are more likely to shift to topics which are strong for them but weak for their opponents. Our work is in line with argumentation mining. Existing work in this area focuses on argument extraction (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011) and argument scheme classification (Biran and Rambow, 2011; Feng and"
Q17-1016,D13-1170,0,0.00291099,"ses for each discourse class. Four classes on level one (temporal, comparison, contingency, and expansion) and sixteen classes on level two are considered. Finally, pleading be223 havior is encoded as counting phrases of “urge”, “please”, “ask you”, and “encourage you”, which may be used by debaters to appeal to the audience. Sentence-level features. We first consider the frequency of questioning since rhetorical questions are commonly used for debates and argumentation. To model the sentiment distribution of arguments, sentence-level sentiment is labeled by the Stanford sentiment classifier (Socher et al., 2013) as positive (sentiment score of 4 or 5), negative (score of 1 or 2), and neutral (score of 3). We then count single sentence sentiment as well as transitions between adjacent sentences (e.g. positive → negative) for each type. Since readability level may affect how the audience perceives arguments, we compute readability levels based on Flesch reading ease scores, FleschKincaid grade levels, and the Coleman-Liau index for each sentence. We use the maximum, minimum, and average of scores as the final features. The raw number of sentences is also calculated. Argument-level features. Speakers ge"
Q17-1016,D14-1006,0,0.130955,"of these features is then computed for each argument. Semantic features. We encode semantic information via semantic frames (Fillmore, 1976), which represent the context of word meanings. CanoBasave and He (2016) show that arguments of different types tend to employ different semantic frames, e.g., frames of “reason” and “evaluative comparison” are frequently used in making claims. We count the frequency of each frame, as labeled by SEMAFOR (Das et al., 2014). Discourse features. The usage of discourse connectors has been shown to be effective for detecting argumentative structure in essays (Stab and Gurevych, 2014). We collect discourse connectives from the Penn Discourse Treebank (Prasad et al., 2007), and count the frequency of phrases for each discourse class. Four classes on level one (temporal, comparison, contingency, and expansion) and sixteen classes on level two are considered. Finally, pleading be223 havior is encoded as counting phrases of “urge”, “please”, “ask you”, and “encourage you”, which may be used by debaters to appeal to the audience. Sentence-level features. We first consider the frequency of questioning since rhetorical questions are commonly used for debates and argumentation. To"
Q17-1016,W06-1639,0,0.0134322,"which is measured based on their relative standing in recent public polls. This supports our finding that both sides seek to shift topics, but that winners are more likely to shift to topics which are strong for them but weak for their opponents. Our work is in line with argumentation mining. Existing work in this area focuses on argument extraction (Moens et al., 2007; Palau and Moens, 2009; Mochales and Moens, 2011) and argument scheme classification (Biran and Rambow, 2011; Feng and Hirst, 2011; Rooney et al., 2012; Stab and Gurevych, 2014). Though stance prediction has also been studied (Thomas et al., 2006; Hasan and Ng, 2014), we are not aware of any work that extracts arguments according to topics and position. Argument strength prediction is also studied largely in the domain of student essays (Higgins et al., 2004; Stab and Gurevych, 2014; Persing and Ng, 2015). Notably, none of these distinguishes an argument’s strength from its linguistic surface features. This is a gap we aim to fill. 7 Conclusion We present a debate prediction model that learns latent persuasive strengths of topics, linguistic style of arguments, and the interactions between the two. Experiments on debate outcome predic"
Q17-1016,P15-1157,0,0.0198746,"heirs in examining topic interactions, but brings additional focus on the latent persuasive strength of topics, as well as strength interactions. Tan et al. (2016) examines various structural and linguistic features associated with persuasion on Reddit; they find that some topics correlate more with malleable opinions. Here we develop a more general model of latent topic strength and the linguistic features associated with strength. 229 Additional work has focused on the influence of agenda setting — controlling which topics are discussed (Nguyen et al., 2014), and framing (Card et al., 2015; Tsur et al., 2015) — emphasizing certain aspects or interpretations of an issue. Greene and Resnik (2009) study the syntactic aspects of framing, where syntactic choices are found to correlate with the sentiment perceived by readers. Based on the topic shifting model of Nguyen et al. (2014), Prabhakaran et al. (2014) finds that changing topics in presidential primary debates positively correlates with the candidates’ power, which is measured based on their relative standing in recent public polls. This supports our finding that both sides seek to shift topics, but that winners are more likely to shift to topics"
Q17-1016,W14-2617,1,0.720155,"ategories that characterize various aspects of arguments. For each linguistic feature, we compute two versions: one for the full debate and one for the discussion phase. Basic features. We consider the frequencies of words, numbers, named entities per type, and each personal pronoun. For instance, usage of personal pronouns may imply communicative goals of the speaker (Brown and Gilman, 1960; Wilson, 1990). We also count the frequency of each POS tag output by Stanford parser (Klein and Manning, 2003). Sentiment and emotional language usage is prevalent in discussions on controversial topics (Wang and Cardie, 2014b). We thus count words of positive and negative sentiment based on MPQA lexicon (Wilson et al., 2005), and words per emotion type according to a lexicon from Mohammad and Turney (2013). Moreover, based on the intuition that agreement carries indications on topical alignment (Bender et al., 2011; Wang and Cardie, 2014a), occurrence of agreement phrases (“I/we agree”, “you’re right”) is calculated. Finally, audience feedback, including applause and laughter, is also considered. Style features. Existing work suggests that formality can reveal speakers’ opinions or intentions (Irvine, 1979). Here"
Q17-1016,P14-2113,1,0.452633,"ategories that characterize various aspects of arguments. For each linguistic feature, we compute two versions: one for the full debate and one for the discussion phase. Basic features. We consider the frequencies of words, numbers, named entities per type, and each personal pronoun. For instance, usage of personal pronouns may imply communicative goals of the speaker (Brown and Gilman, 1960; Wilson, 1990). We also count the frequency of each POS tag output by Stanford parser (Klein and Manning, 2003). Sentiment and emotional language usage is prevalent in discussions on controversial topics (Wang and Cardie, 2014b). We thus count words of positive and negative sentiment based on MPQA lexicon (Wilson et al., 2005), and words per emotion type according to a lexicon from Mohammad and Turney (2013). Moreover, based on the intuition that agreement carries indications on topical alignment (Bender et al., 2011; Wang and Cardie, 2014a), occurrence of agreement phrases (“I/we agree”, “you’re right”) is calculated. Finally, audience feedback, including applause and laughter, is also considered. Style features. Existing work suggests that formality can reveal speakers’ opinions or intentions (Irvine, 1979). Here"
Q17-1016,H05-1044,0,0.016059,"ersions: one for the full debate and one for the discussion phase. Basic features. We consider the frequencies of words, numbers, named entities per type, and each personal pronoun. For instance, usage of personal pronouns may imply communicative goals of the speaker (Brown and Gilman, 1960; Wilson, 1990). We also count the frequency of each POS tag output by Stanford parser (Klein and Manning, 2003). Sentiment and emotional language usage is prevalent in discussions on controversial topics (Wang and Cardie, 2014b). We thus count words of positive and negative sentiment based on MPQA lexicon (Wilson et al., 2005), and words per emotion type according to a lexicon from Mohammad and Turney (2013). Moreover, based on the intuition that agreement carries indications on topical alignment (Bender et al., 2011; Wang and Cardie, 2014a), occurrence of agreement phrases (“I/we agree”, “you’re right”) is calculated. Finally, audience feedback, including applause and laughter, is also considered. Style features. Existing work suggests that formality can reveal speakers’ opinions or intentions (Irvine, 1979). Here we utilize a formality lexicon collected by Brooke et al. (2010), which counts the frequencies of for"
Q17-1016,N16-1017,0,0.521283,"of democracy (Habermas, 1984; Cohen, 1989; Rawls, 1997; Mansbridge, 2003). However, empirical evidence has also led to an increasing awareness of the dangers of style and rhetoric in biasing participants towards the most skillful, charismatic, or numerous speakers (NoelleNeumann, 1974; Sunstein, 1999). In light of these concerns, most efforts to predict the persuasive effects of debate have focused on the linguistic features of debate speech (Katzav and Reed, 2008; Mochales and Moens, 2011; Feng and Hirst, 2011) or on simple measures of topic control (Dryzek and List, 2003; Mansbridge, 2015; Zhang et al., 2016). In the ideal setting, however, we would wish for the winning side to win based on the strength and merits of their arguments, not based on their skillful deployment of linguistic style. Our model therefore predicts debate outcomes by modeling not just the persuasive effects of directly observable linguistic features, but also by incorporating the inherent, latent strengths of topics and issues specific to each side of a debate. To illustrate this idea, Figure 1 shows a brief ex219 Transactions of the Association for Computational Linguistics, vol. 5, pp. 219–232, 2017. Action Editor: Noah Sm"
W11-0503,J96-1002,0,0.0152048,"relative position5 and whether they 3 We cannot easily associate each topic with a decision because the number of decisions is not known a priori. 4 Parameter estimation and inference done by GibbsLDA++. 5 Here is the definition for the relative position of pairwise DAs. Suppose there are N DAs in one meeting ordered by time, 19 have the same DA type. We employ Support Vector Machines (SVMs) and Maximum Entropy (MaxEnt) as our learning methods, because SVMs are shown to be effective in text categorization (Joachims, 1998) and MaxEnt has been applied in many natural language processing tasks (Berger et al., 1996). Given an −−→ F Vij , for SVMs, we utilize the decision value of −−→ wT · F Vij + b as the similarity, where w is the weight vector and b is the bias. For MaxEnt, we make use of the probability of P (SameDecision | −−→ F Vij ) as the similarity value. 3.3 Experiments Corpus. We use the AMI meeting Corpus (Carletta et al., 2005), a freely available corpus of multiparty meetings that contains a wide range of annotations. The 129 scenario-driven meetings involve four participants playing different roles on a design team. A short (usually one-sentence) abstract is included that describes each dec"
W11-0503,W09-3934,0,0.433553,"ed summarization setting, and given true clusterings of decisionrelated utterances, we find that token-level summaries that employ discourse context can approach an upper bound for decision abstracts derived directly from dialogue acts. In the unsupervised summarization setting,we find that summaries based on unsupervised partitioning of decision-related utterances perform comparably to those based on partitions generated using supervised techniques (0.22 ROUGE-F1 using LDA-based topic models vs. 0.23 using SVMs). 1 tle work has focused on decision summarization: Fern´andez et al. (2008a) and Bui et al. (2009) investigate the use of a semantic parser and machine learning methods for phrase- and token-level decision summarization. We believe our work is the first to explore and compare token-level and dialogue act-level approaches — using both unsupervised and supervised learning methods — for summarizing decisions in meetings. Introduction Meetings are a common way for people to share information and discuss problems. And an effective meeting always leads to concrete decisions. As a result, it would be useful to develop automatic methods that summarize not the entire meeting dialogue, but just the"
W11-0503,A00-2004,0,0.046773,"gn team. A short (usually one-sentence) abstract is included that describes each decision, action, or problem discussed in the meeting; and each DA is linked to the abstracts it supports. We use the manually constructed decision abstracts as gold-standard summaries and assume that all decision-related DAs have been identified (but not linked to the decision(s) it supports). Baselines. Two clustering baselines are utilized for comparison. One baseline places all decisionrelated DAs for the meeting into a single partition (A LL I N O NE G ROUP). The second uses the text segmentation software of Choi (2000) to partition the decision-related DAs (ordered according to time) into several topic-based groups (C HOI S EGMENT). Experimental Setup and Evaluation. Results for pairwise supervised clustering were obtained using 3-fold cross-validation. In the current work, stopping conditions for hierarchical agglomerative clustering are selected manually: For the TF-IDF and topic model approaches, we stop when the similarity measure reaches 0.035 and 0.015, respectively; For the SVM and MaxEnt versions, we use 0 and 0.45, respectively. We use the Mallet implementation for MaxEnt and the SVMlight implement"
W11-0503,W08-0125,0,0.053685,"Missing"
W11-0503,D09-1118,0,0.287277,"stering of decision-related DAs. Here we aim to partition the decision-related utterances (DRDAs) according to the decisions each supports. This step is similar in spirit to many standard text summarization techniques (Salton et al., 1997) that begin by grouping sentences according to semantic similarity. Summarization at the DA-level. We select just the important DRDAs in each cluster. Our goal is to eliminate redundant and less informative utterances. The 1 These are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern´andez et al. (2008a), Frampton et al. (2009). The latter refer to all DAs that appear in a decision discussion even if they do NOT support any particular decision. 17 selected DRDAs are then concatenated to form the decision summary. Optional token-level summarization of the selected DRDAs. Methods are employed to capture concisely the gist of each decision, discarding any distracting text. Incorporation of the discourse context as needed. We hypothesize that this will produce more interpretable summaries. More specifically, we compare both unsupervised (TFIDF (Salton et al., 1997) and LDA topic modeling (Blei et al., 2003)) and (pairwi"
W11-0503,W06-1643,0,0.439921,"omatic text summarization using corpus-based, knowledgebased or statistical methods (Mani, 1999; Marcu, 2000). Dialogue summarization methods, however, generally try to account for the special characteristics of speech. Among early work in this subarea, Zechner (2002) investigates speech summarization based on maximal marginal relevance (MMR) and cross-speaker linking of information. Popular supervised methods for summarizing speech — including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) — are investigated in Buist et al. (2004), Xie et al. (2008) and Galley (2006). Techniques for determining semantic similarity are used for selecting relevant utterances in Gurevych and Strube (2004). Studies in Banerjee et al. (2005) show that decisions are considered to be one of the most important outputs of meetings. And in recent years, there has been much research on detecting decisionrelated DAs. Hsueh and Moore (2008), for example, propose maximum entropy classification techniques to identify DRDAs in meetings; Fern´andez et al. (2008b) develop a model of decision-making dialogue structure and detect decision DAs based on it; and Frampton et al. (2009) implement"
W11-0503,C04-1110,0,0.162287,"). Dialogue summarization methods, however, generally try to account for the special characteristics of speech. Among early work in this subarea, Zechner (2002) investigates speech summarization based on maximal marginal relevance (MMR) and cross-speaker linking of information. Popular supervised methods for summarizing speech — including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) — are investigated in Buist et al. (2004), Xie et al. (2008) and Galley (2006). Techniques for determining semantic similarity are used for selecting relevant utterances in Gurevych and Strube (2004). Studies in Banerjee et al. (2005) show that decisions are considered to be one of the most important outputs of meetings. And in recent years, there has been much research on detecting decisionrelated DAs. Hsueh and Moore (2008), for example, propose maximum entropy classification techniques to identify DRDAs in meetings; Fern´andez et al. (2008b) develop a model of decision-making dialogue structure and detect decision DAs based on it; and Frampton et al. (2009) implement a real-time decision detection system. Fern´andez et al. (2008a) and Bui et al. (2009), however, might be the most relev"
W11-0503,N03-1020,0,0.887363,"partitioning DRDAs according to the decision each supports. We also investigate unsupervised methods and supervised learning for decision summarization at both the DA and token level, with and without the incorporation of discourse context. During training, the supervised decision summarizers are told which DRDAs for each decision are the most informative for constructing the decision abstract. Our experiments employ the aforementioned AMI meeting corpus: we compare our decision summaries to the manually generated decision abstracts for each meeting and evaluate performance using the ROUGE-1 (Lin and Hovy, 2003) text summarization evaluation metric. In the supervised summarization setting, our experiments demonstrate that with true clusterings of decision-related DAs, token-level summaries that employ limited discourse context can approach an upper bound for summaries extracted directly from DRDAs2 — 0.4387 ROUGE-F1 vs. 0.5333. When using system-generated DRDA clusterings, the DAlevel summaries always dominate token-level methods in terms of performance. For the unsupervised summarization setting, we investigate the use of both unsupervised and supervised methods for the initial DRDA clustering step."
W11-0503,de-marneffe-etal-2006-generating,0,0.00728913,"0.1727 0.3391 0.3760 0.2903 0.4882 0.2097 0.1427 0.1869 0.1486 0.2349 0.1843 0.2068 0.2056 0.3508 0.2807 0.3583 0.4891 0.1884 0.04968 0.1891 0.0822 0.2197 0.0777 0.2221 0.1288 0.3592 0.3607 0.3418 0.4873 0.2026 0.0885 0.1892 0.0914 0.2348 0.1246 0.2213 0.1393 0.08673 0.1906 0.1957 0.0625 0.0993 0.0868 0.0707 0.1890 0.1979 0.3068 0.0916 0.2057 Table 7: Results for ROUGE-1: Summary Generation Using Supervised Learning et al. (2008a), we design features that encode (a) basic predicate-argument structures involving major phrase types (S, VP, NP, and PP) and (b) additional typed dependencies from Marneffe et al. (2006). We use the Stanford Parser. 5 Experiments Experiments based on supervised learning are performed using 3-fold cross-validation. We train two different types of classifiers for identifying informative DAs or tokens: Conditional Random Fields (CRFs) (via Mallet) and Support Vector Machines (SVMs) (via SVMlight ). We remove function words from DAs before using them as the input of our systems. The AMI decision abstracts are the gold-standard summaries. We use the ROUGE (Lin and Hovy, 2003) evaluation measure. ROUGE is a recall-based method that can identify systems producing succinct and descri"
W11-0503,J02-4003,0,0.517629,"the supervised summarization setting, we observe that including additional discourse context boosts performance only for token-level summaries. 2 The upper bound measures the vocabulary overlap of each gold-standard decision summary with the complete text of all of its associated DRDAs. 2 Related Work There exists much previous research on automatic text summarization using corpus-based, knowledgebased or statistical methods (Mani, 1999; Marcu, 2000). Dialogue summarization methods, however, generally try to account for the special characteristics of speech. Among early work in this subarea, Zechner (2002) investigates speech summarization based on maximal marginal relevance (MMR) and cross-speaker linking of information. Popular supervised methods for summarizing speech — including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) — are investigated in Buist et al. (2004), Xie et al. (2008) and Galley (2006). Techniques for determining semantic similarity are used for selecting relevant utterances in Gurevych and Strube (2004). Studies in Banerjee et al. (2005) show that decisions are considered to be one of the most important outputs of meetings. And in rec"
W12-1605,I08-1018,0,0.175291,", 2008), (Bui et al., 2009) and (Wang and Cardie, 2011). (Fern´andez et al., 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases. They also train HMM and SVM directly on a set of decision-related dialogue acts on token level and use the classifiers to identify summary-worthy words. Wang and Cardie (2011) provide an exploration on supervised and unsupervised learning for decision summarization on both 42 utterance- and token- level. Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010). Mostly, the sentences are ranked according to importance based on latent topic structures, and top ones are selected as the summary. There are some works for applying document-level topic models to speech summarization (Kong and shan Leek, 2006; Chen and Chen, 2008; Hazen, 2011). Different from their work, we further investigate the topic models of fine granularity on sentence level and leverage context information for decision summarization task. Most existing approaches for speech summari"
W12-1605,N10-1122,0,0.20975,"ted as: S ∗ = arg min KL(PC ||PS ) = arg min S:|S|&lt;θ 4 X S:|S|&lt;θ T i P (Ti |C)log P (Ti |C) P (Ti |S) Topic Models In this section, we briefly describe the three finegrained topic models employed to compute the latent topic distributions on utterance level in the meetings. According to the input of Algorithm 1, we are interested in estimating the topic distribution for each DA P (T |DA) and the word distribution for each topic P (w|T ). For MG-LDA, P (T |DA) is computed as the expectation of local topic distributions with respect to the window distribution. 4.1 Local LDA Local LDA (LocalLDA) (Brody and Elhadad, 2010) uses almost the same probabilistic generative model as Latent Dirichlet Allocation (LDA) (Blei et al., 2003), except that it treats each sentence as a separate document2 . Each DA d is generated as follows: 1. For each topic k: (a) Choose word distribution: φk ∼ Dir(β) 2. For each DA d: (a) Choose topic distribution: θd ∼ Dir(α) (b) For each word w in DA d: i. Choose topic: zd,w ∼ θd ii. choose word: w ∼ φzd,w 4.2 Multi-grain LDA Multi-grain LDA (MG-LDA) (Titov and McDonald, 2008) can model both the meeting specific topics (e.g. the design of a remote control) and various concrete aspects (e."
W12-1605,W09-3934,0,0.135358,"d be to add complementary knowledge when the DRDAs cannot provide complete information. Therefore, we need a summarization approach that is tolerant of dialogue phenomena, can determine the key semantic content and is easily transferable between domains. Recently, topic modeling approaches have been investigated and achieved state-of-the-art results in multi-document summarization (Haghighi and Vanderwende, 2009; Celiky1 These DRDAs are annotated in the AMI corpus and usually contain the decision content. They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern´andez et al. (2008), Frampton et al. (2009). 41 ilmaz and Hakkani-Tur, 2010). Thus, topic models appear to be a better ref for document similarity w.r.t. semantic concepts than simple literal word matching. However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approaches for focused summarization in meetings. In contrast to previous work, we study the unsupervised token-level decision summarization in meetings by identifying a concise set of key words or phrases, w"
W12-1605,P10-1084,0,0.0180247,". (Fern´andez et al., 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases. They also train HMM and SVM directly on a set of decision-related dialogue acts on token level and use the classifiers to identify summary-worthy words. Wang and Cardie (2011) provide an exploration on supervised and unsupervised learning for decision summarization on both 42 utterance- and token- level. Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009; Celikyilmaz and Hakkani-Tur, 2010; Celikyilmaz and Hakkani-Tur, 2010). Mostly, the sentences are ranked according to importance based on latent topic structures, and top ones are selected as the summary. There are some works for applying document-level topic models to speech summarization (Kong and shan Leek, 2006; Chen and Chen, 2008; Hazen, 2011). Different from their work, we further investigate the topic models of fine granularity on sentence level and leverage context information for decision summarization task. Most existing approaches for speech summarization result in a selection of utterances from the dialogue, which"
W12-1605,D09-1118,0,0.0309555,"DRDAs cannot provide complete information. Therefore, we need a summarization approach that is tolerant of dialogue phenomena, can determine the key semantic content and is easily transferable between domains. Recently, topic modeling approaches have been investigated and achieved state-of-the-art results in multi-document summarization (Haghighi and Vanderwende, 2009; Celiky1 These DRDAs are annotated in the AMI corpus and usually contain the decision content. They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern´andez et al. (2008), Frampton et al. (2009). 41 ilmaz and Hakkani-Tur, 2010). Thus, topic models appear to be a better ref for document similarity w.r.t. semantic concepts than simple literal word matching. However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approaches for focused summarization in meetings. In contrast to previous work, we study the unsupervised token-level decision summarization in meetings by identifying a concise set of key words or phrases, which can either be output as a compact summary or"
W12-1605,W06-1643,0,0.0883874,"oposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary. 1 Introduction Meetings are an important way for information sharing and collaboration, where people can discuss problems and make concrete decisions. Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a). Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.e., summaries of a particular aspect of a meeting rather than of the meeting as a whole. For example, the decisions made, the action items that emerged and the problems arised are all important outcomes of meetings. In particular, decision summaries would allow participants to review decisions from previous meetings and understand the related topics quickly, which facilitates preparation for the upcoming meetings. Decision Abstracts (Summary) D ECISION 1: The targ"
W12-1605,N09-1041,0,0.397634,"n” is not specified in any of the listed DRDAs supporting it. By looking at the transcript, we find “power button” mentioned in one of the preceding, but not decision-related DAs. Consequently another challenge would be to add complementary knowledge when the DRDAs cannot provide complete information. Therefore, we need a summarization approach that is tolerant of dialogue phenomena, can determine the key semantic content and is easily transferable between domains. Recently, topic modeling approaches have been investigated and achieved state-of-the-art results in multi-document summarization (Haghighi and Vanderwende, 2009; Celiky1 These DRDAs are annotated in the AMI corpus and usually contain the decision content. They are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of Bui et al. (2009), Fern´andez et al. (2008), Frampton et al. (2009). 41 ilmaz and Hakkani-Tur, 2010). Thus, topic models appear to be a better ref for document similarity w.r.t. semantic concepts than simple literal word matching. However, very little work has investigated its role in spoken document summarization (Chen and Chen, 2008; Hazen, 2011), and much less conducted comparisons among topic modeling approa"
W12-1605,P10-1009,0,0.017854,"evel summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary. 1 Introduction Meetings are an important way for information sharing and collaboration, where people can discuss problems and make concrete decisions. Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a). Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.e., summaries of a particular aspect of a meeting rather than of the meeting as a whole. For example, the decisions made, the action items that emerged and the problems arised are all important outcomes of meetings. In particular, decision summaries would allow participants to review decisions from previous meetings and understand the related topics quickly, which facilitates preparation for the upcoming meetings. Decision Abstracts (Summary) D ECISION 1: The target group comprises o"
W12-1605,N03-1020,0,0.0186919,"gs. In the True Clusterings setting, we use the AMI annotations to create perfect partitionings of the DRDAs as the input; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in previous work (Wang and Cardie, 2011). The Wang and Cardie (2011) clustering method groups DRDAs according to their LDA topic distribution similarity. As better approaches for DRDA clustering become available, they could be employed instead. Evaluation Metric. To evaluate the performance of various summarization approaches, we use the widely accepted ROUGE (Lin and Hovy, 2003) metrics. We use the stemming option of the ROUGE software at http://berouge.com/ and remove stopwords from both the system and gold-standard summaries, same as Riedhammer et al. (2010) do. Inference and Hyperparameters We use the implementation from (Lu et al., 2011) for the three topic models in Section 4. The collapsed Gibbs Sampling approach (Griffiths and Steyvers, 2004) is exploited for inference. Hyperparameters are chosen according to (Brody and Elhadad, 2010), (Titov and McDonald, 2008) and (Du et al., 2010). In LDA and LocalLDA, α and β are both set to 0.1 . For MG-LDA, αgl , αloc an"
W12-1605,N09-1070,0,0.0814448,"Missing"
W12-1605,W10-4211,0,0.0642725,"pproach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary. 1 Introduction Meetings are an important way for information sharing and collaboration, where people can discuss problems and make concrete decisions. Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a). Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.e., summaries of a particular aspect of a meeting rather than of the meeting as a whole. For example, the decisions made, the action items that emerged and the problems arised are all important outcomes of meetings. In particular, decision summaries would allow participants to review decisions from previous meetings and understand the related topics quickly, which facilitates preparation for the upcoming meetings. Decision Abstracts (Summary) D ECISION 1: The target group comprises of individuals who can"
W12-1605,W11-0503,1,0.848553,"teristics of dialogue. Early work in this area investigated supervised learning methods, including maximum entropy, conditional random fields (CRFs), and support vector machines (SVMs) (Buist et al., 2004; Galley, 2006; Xie et al., 2008). For unsupervised methods, maximal marginal relevance (MMR) is investigated in (Zechner, 2002) and (Xie and Liu, 2010). Gillick et al. (2009) introduce a conceptbased global optimization framework by using integer linear programming (ILP). Only in very recent works has decision summarization been addressed in (Fern´andez et al., 2008), (Bui et al., 2009) and (Wang and Cardie, 2011). (Fern´andez et al., 2008) and (Bui et al., 2009) utilize semantic parser to identify candidate phrases for decision summaries and employ SVM to rank those phrases. They also train HMM and SVM directly on a set of decision-related dialogue acts on token level and use the classifiers to identify summary-worthy words. Wang and Cardie (2011) provide an exploration on supervised and unsupervised learning for decision summarization on both 42 utterance- and token- level. Our work also arises out of applying topic models to text summarization (Bhandari et al., 2008; Haghighi and Vanderwende, 2009;"
W12-1605,J02-4003,0,0.257571,"the decisionmaking process. Moreover, our proposed token-level summarization approach, which is able to remove redundancies within utterances, outperforms existing utterance ranking based summarization methods. Finally, context information is also investigated to add additional relevant information to the summary. 1 Introduction Meetings are an important way for information sharing and collaboration, where people can discuss problems and make concrete decisions. Not surprisingly, there is an increasing interest in developing methods for extractive summarization for meetings and conversations (Zechner, 2002; Maskey and Hirschberg, 2005; Galley, 2006; Lin and Chen, 2010; Murray et al., 2010a). Carenini et al. (2011) describe the specific need for focused summaries of meetings, i.e., summaries of a particular aspect of a meeting rather than of the meeting as a whole. For example, the decisions made, the action items that emerged and the problems arised are all important outcomes of meetings. In particular, decision summaries would allow participants to review decisions from previous meetings and understand the related topics quickly, which facilitates preparation for the upcoming meetings. Decisio"
W12-1605,N10-1132,0,\N,Missing
W12-1605,N10-1006,0,\N,Missing
W12-1642,P08-1004,0,0.0310767,"Missing"
W12-1642,W09-3934,0,0.658826,"gonna have”), but do not themselves describe the decision. We will refer to this portion of a DRDA (underlined in Figure 1) as the Decision Cue. Moreover, the decision cue is generally directly followed by the actual Decision Content (e.g., “be a little apple”, “have rubber cases”). Decision Content phrases are denoted in Figure 1 via italics and square brackets. Importantly, it is just the decision content portion of the utterance that should be considered for incorporation into the focused summary. 1 These are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of (Bui et al., 2009), (Fern´andez et al., 2008), (Frampton et al., 2009). 2 Murray et al. (2010b) show that users much prefer abstractive summaries over extracts when the text to be summarized is a conversation. In particular, extractive summaries drawn from group conversations can be confusing to the reader without additional context; and the noisy, error-prone, disfluent text of speech transcripts is likely to result in extractive summaries with low readability. 304 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 304–313, c Seoul, South Korea, 5-6"
W12-1642,P11-1054,0,0.436064,"8), Bui et al. (2009)), we view the problem as an information extraction task and hypothesize that existing methods for domain-specific relation extraction can be modified to identify salient phrases for use in generating abstractive summaries. Very generally, information extraction methods identify a lexical “trigger” or “indicator” that evokes a relation of interest and then employ syntactic information, often in conjunction with semantic constraints, to find the “target phrase” or “argument constituent” to be extracted. Relation instances, then, are represented by indicator-argument pairs (Chen et al., 2011). Figure 1 shows some possible indicator-argument pairs for identifying the Decision Content phrases in the dialogue sample. Content indicator words 305 are shown in italics; the Decision Content target phrases are the arguments. For example, in the fourth DRDA, “require” is the indicator, and “rubber buttons” and “rubber case” are both arguments. Although not shown in Figure 1, it is also possible to identify relations that correspond to the Decision Cue phrases.3 Specifically, we focus on the task of decision summarization and, as in previous work in meeting summarization (e.g., Fern´andez e"
W12-1642,de-marneffe-etal-2006-generating,0,0.0146691,"Missing"
W12-1642,D09-1118,0,0.0124431,"he decision. We will refer to this portion of a DRDA (underlined in Figure 1) as the Decision Cue. Moreover, the decision cue is generally directly followed by the actual Decision Content (e.g., “be a little apple”, “have rubber cases”). Decision Content phrases are denoted in Figure 1 via italics and square brackets. Importantly, it is just the decision content portion of the utterance that should be considered for incorporation into the focused summary. 1 These are similar, but not completely equivalent, to the decision dialogue acts (DDAs) of (Bui et al., 2009), (Fern´andez et al., 2008), (Frampton et al., 2009). 2 Murray et al. (2010b) show that users much prefer abstractive summaries over extracts when the text to be summarized is a conversation. In particular, extractive summaries drawn from group conversations can be confusing to the reader without additional context; and the noisy, error-prone, disfluent text of speech transcripts is likely to result in extractive summaries with low readability. 304 Proceedings of the 13th Annual Meeting of the Special Interest Group on Discourse and Dialogue (SIGDIAL), pages 304–313, c Seoul, South Korea, 5-6 July 2012. 2012 Association for Computational Lingui"
W12-1642,W06-1643,0,0.588542,"baselines as well as an existing generic relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score. 1 Introduction For better or worse, meetings play an integral role in most of our daily lives — they let us share information and collaborate with others to solve a problem, to generate ideas, and to weigh options. Not surprisingly then, there is growing interest in developing automatic methods for meeting summarization (e.g., Zechner (2002), Maskey and Hirschberg (2005), Galley (2006), Lin and Chen (2010), Murray et al. (2010a)). This paper tackles the task of focused meeting summarization , i.e., generating summaries of a particular aspect of a meeting rather than of the meeting as a whole (Carenini et al., 2011). For example, one might want a summary of just the DECISIONS made during the meeting, the ACTION ITEMS that emerged, the IDEAS discussed, or the HYPOTHESES put forth, etc. Consider, for example, the task of summarizing the decisions in the dialogue snippet in Figure 1. The figure shows only the decision-related dialogue acts (DRDAs) — utterances associated with o"
W12-1642,D09-1044,0,0.232806,"adopt here the two unsupervised baselines (utterance-level summaries) from that work for use in our evaluation. We further employ their supervised summarization methods as comparison points for token-level summarization, adding additional features for consis306 tency with the other approaches in the evaluation. Murray et al. (2010a) develop an integer linear programming approach for focused summarization at the utterance-level, selecting sentences that cover more of the entities mentioned in the meeting as determined through the use of an external ontology. The most relevant previous work is Hachey (2009), which uses relational representations to facilitate sentence-ranking for multi-document summarization. The method utilizes generic relation extraction to represent the concepts in the documents as relation instances; summaries are generated based on a set cover algorithm that selects a subset of the sentences that best cover the weighted concepts. Thus, the goal of Hachey’s approach is sentence extraction rather than phrase extraction. Although his relation extraction method, like ours (see Section 4), is probabilistic and unsupervised (he uses Latent Dirichelt Allocation (Blei et al., 2003)"
W12-1642,P03-1054,0,0.00469157,"opriate for our decision summarization setting. Nevertheless, we will adapt his approach for comparison with our relation-based summarization technique and include it for evaluation. 3 Focused Summarization as Relation Extraction Given the DRDAs for each meeting grouped (not necessarily correctly) according to the decisions they support, we put each cluster of DRDAs (ordered according to time within the cluster) into one “decision document”. The goal will be to produce one decision abstract for each such decision document. We obtain constituent and dependency parses using the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006). With the corpus of constituent-parsed decision documents as the input, we will use and modify Chen et al. (2011)’s system to identify decision cue relations and decision content relations for each cluster.4 (Section 6 will make clear how the learned decision cue relations will be used to identify decision content relations.) The salient decision content relation instances will be returned as decision summary com4 Other unsupervised relation learning methods might also be appropriate (e.g., Open IE (Banko et al., 2007)), but they generally model relations between pa"
W12-1642,P10-1009,0,0.0200589,"ell as an existing generic relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score. 1 Introduction For better or worse, meetings play an integral role in most of our daily lives — they let us share information and collaborate with others to solve a problem, to generate ideas, and to weigh options. Not surprisingly then, there is growing interest in developing automatic methods for meeting summarization (e.g., Zechner (2002), Maskey and Hirschberg (2005), Galley (2006), Lin and Chen (2010), Murray et al. (2010a)). This paper tackles the task of focused meeting summarization , i.e., generating summaries of a particular aspect of a meeting rather than of the meeting as a whole (Carenini et al., 2011). For example, one might want a summary of just the DECISIONS made during the meeting, the ACTION ITEMS that emerged, the IDEAS discussed, or the HYPOTHESES put forth, etc. Consider, for example, the task of summarizing the decisions in the dialogue snippet in Figure 1. The figure shows only the decision-related dialogue acts (DRDAs) — utterances associated with one or more decisions."
W12-1642,N03-1020,0,0.193241,"ts. We consider two system input settings. In the True Clusterings setting, we use the AMI annotations to create perfect partitionings of the DRDAs for input to the summarization system; in the System Clusterings setting, we employ a hierarchical agglomerative clustering algorithm used for this task in previous work (Wang and Cardie, 2011). The Wang and Cardie (2011) clustering method groups DRDAs according to their LDA topic distribution similarity. As better approaches for DRDA clustering become available, they could be employed instead. Evaluation Metrics. We use the widely accepted ROUGE (Lin and Hovy, 2003) evaluation measure. We adopt the ROUGE-1 and ROUGE-SU4 metrics from (Hachey, 2009), and also use ROUGE2. We choose the stemming option of the ROUGE software at http://berouge.com/ and remove stopwords from both the system and gold-standard summaries. Training and Parameters. The Dirichlet hyperparameters are set to 0.1 for the priors. When training the model, ten random restarts are performed and each run stops when reaching a convergence threshold (10−5 ). Then we select the posterior with the lowest final free energy. For the parameters used in posterior constraints, we either adopt them fr"
W12-1642,D08-1081,0,0.0194305,"ecision cue relation if any (before, after) Table 3: Additional features for Decision Content relation extraction, inspired by Decision Cue relations. Both indicator and argument use those features. Table 1: Features for Decision Cue and Decision Content relation extraction. All features, except the last type of features, are used for both the indicator and argument. (An Adjacency Pair (AP) is an important conversational analysis concept (Schegloff and Sacks, 1973). In the AMI corpus, an AP pair consists of a source utterance and a target utterance, produced by different speakers.) ley, 2006; Murray and Carenini, 2008; Fern´andez et al., 2008; Wang and Cardie, 2011). Features employed only for argument’s are listed in the last category in Table 1. After applying the features in Table 1 and the global constraints from Section 5 in preliminary experiments, we found that the extracted relation instances are mostly derived from decision cue relations. Sample decision cue relations and instances are displayed in Table 2 and are not necessarily surprising: previous research (Hsueh and Moore, 2007) has observed the important role of personal pronouns, such as “we” and “I”, in decision-making expressions. Notably,"
W12-1642,W10-4211,0,0.54678,"neric relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score. 1 Introduction For better or worse, meetings play an integral role in most of our daily lives — they let us share information and collaborate with others to solve a problem, to generate ideas, and to weigh options. Not surprisingly then, there is growing interest in developing automatic methods for meeting summarization (e.g., Zechner (2002), Maskey and Hirschberg (2005), Galley (2006), Lin and Chen (2010), Murray et al. (2010a)). This paper tackles the task of focused meeting summarization , i.e., generating summaries of a particular aspect of a meeting rather than of the meeting as a whole (Carenini et al., 2011). For example, one might want a summary of just the DECISIONS made during the meeting, the ACTION ITEMS that emerged, the IDEAS discussed, or the HYPOTHESES put forth, etc. Consider, for example, the task of summarizing the decisions in the dialogue snippet in Figure 1. The figure shows only the decision-related dialogue acts (DRDAs) — utterances associated with one or more decisions.1 Each DRDA is labele"
W12-1642,2007.sigdial-1.4,0,0.174245,"I SION 1, 2 or 3. Also shown is the gold-standard (manual) abstract (summary) for each decision. Colors indicate overlapping vocabulary between utterances and the summary. Underlining, italics, and [bracketing] are decscribed in the running text. This paper presents an unsupervised framework for focused meeting summarization that supports the generation of abstractive summaries. (Note that we do not currently generate actual abstracts, but rather aim to identify those Content phrases that should comprise the abstract.) In contrast to existing approaches to focused meeting summarization (e.g., Purver et al. (2007), Fern´andez et al. (2008), Bui et al. (2009)), we view the problem as an information extraction task and hypothesize that existing methods for domain-specific relation extraction can be modified to identify salient phrases for use in generating abstractive summaries. Very generally, information extraction methods identify a lexical “trigger” or “indicator” that evokes a relation of interest and then employ syntactic information, often in conjunction with semantic constraints, to find the “target phrase” or “argument constituent” to be extracted. Relation instances, then, are represented by in"
W12-1642,W11-0503,1,0.660663,"ws some possible indicator-argument pairs for identifying the Decision Content phrases in the dialogue sample. Content indicator words 305 are shown in italics; the Decision Content target phrases are the arguments. For example, in the fourth DRDA, “require” is the indicator, and “rubber buttons” and “rubber case” are both arguments. Although not shown in Figure 1, it is also possible to identify relations that correspond to the Decision Cue phrases.3 Specifically, we focus on the task of decision summarization and, as in previous work in meeting summarization (e.g., Fern´andez et al. (2008), Wang and Cardie (2011)), assume that all decision-related utterances (DRDAs) have been identified. We adapt the unsupervised relation learning approach of Chen et al. (2011) to separately identify relations associated with decision cues vs. the decision content within DRDAs by defining a new set of task-specific constraints and features to take the place of the domain-specific constraints and features of the original model. Output of the system is a set of extracted indicator-argument decision content relations (see the “O UR M ETHOD” sample summary of Table 6) that can be used as the basis of the decision abstract"
W12-1642,J02-4003,0,0.146497,"vised utterance-level extractive summarization baselines as well as an existing generic relation-extraction-based summarization method. Moreover, our approach produces summaries competitive with those generated by supervised methods in terms of the standard ROUGE score. 1 Introduction For better or worse, meetings play an integral role in most of our daily lives — they let us share information and collaborate with others to solve a problem, to generate ideas, and to weigh options. Not surprisingly then, there is growing interest in developing automatic methods for meeting summarization (e.g., Zechner (2002), Maskey and Hirschberg (2005), Galley (2006), Lin and Chen (2010), Murray et al. (2010a)). This paper tackles the task of focused meeting summarization , i.e., generating summaries of a particular aspect of a meeting rather than of the meeting as a whole (Carenini et al., 2011). For example, one might want a summary of just the DECISIONS made during the meeting, the ACTION ITEMS that emerged, the IDEAS discussed, or the HYPOTHESES put forth, etc. Consider, for example, the task of summarizing the decisions in the dialogue snippet in Figure 1. The figure shows only the decision-related dialogu"
W12-1642,N10-1132,0,\N,Missing
W14-2617,W11-0702,0,0.854029,") of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and dis98 groups based on the intuition that people in the same group should mostly agree with each other. Though those work highly relies on the component of agreement and disagreement detection, the evaluation is always performed on the ultimate application only. for online debate. Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants. Automatically inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a"
W14-2617,P12-1042,0,0.0411988,"Missing"
W14-2617,D10-1121,0,0.119061,"t features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. 3 The Model We first give a brief overview on isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007), which is used as"
W14-2617,W11-0707,0,0.788166,"ructure, where turns starting with &gt; are response for most previous turn that with one less &gt;. We use “NN”, “N”, and “PP” to indicate “strongly disagree”, “disagree”, and “strongly agree”. Sentences in blue are examples whose sentiment is hard to detect by an existing lexicon. or segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis. We employ two existing online discussion data sets: the Authority and Alignment in Wikipedia Discussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and dis98 groups based on the intuition that people in the same group should mostly agree with each other. Though those work highly relies on the component of agreement and disagreement detection, the evaluation is always pe"
W14-2617,D12-1006,0,0.40296,"near chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages. 1 Introduction We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users’ attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process. 1 Our online discussion lexicon (Section 4) will be made publicly available. 97 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97–106, c Baltimore, Maryland, USA. June 27, 2014. 2014 Association for Computational Linguistics Zer0faults: So questions comments feedback welcome"
W14-2617,D09-1062,1,0.31828,"l language used. As an example, consider a snippet of discussion from Wikipedia Talk page for article “Iraq War” where editors argue on the correctness of the information in the opening paragraph (Figure 1). “So what?” should presumably be tagged as a negative sentence as should the sentence “If you’re going to troll, do us all a favor and stick to the guidelines.”. We hypothesize that these, and other, examples will be difficult for the tagger unless the context surrounding each sentence is considered and in the absence of a sentiment lexicon tuned for conversational text (Ding et al., 2008; Choi and Cardie, 2009). As a result, we investigate isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007) for the sentiment tagging task since they preserve the advantages of the popular CRF sequential tagging models (Lafferty et al., 2001) while providing an efficient mechanism to encode domain knowledge — in our case, a sentiment lexicon — through isotonic constraints on the model parameters. In particular, we bootstrap the construction of a sentiment lexicon from Wikipedia talk pages using the lexical items in existing general-purpose sentiment lexicons as seeds and in conjunction with an exi"
W14-2617,J93-3003,0,0.0611429,"e Section 4). The parameters can be found by maximizing the likelihood subject to the monotonicity constraints. We adopt the re-parameterization from Mao and Lebanon (2007) for a simpler optimization problem, and refer the readers to Mao and Lebanon (2007) for more details.2 3.3 Table 1: Features used in sentiment prediction. eralizing a word to its POS tag in turn. For instance, “nsubj(wrong, you)” is generlized as the “nsubj(ADJ, you)” and “nsubj(wrong, PRP)”. We use Stanford parser (de Marneffe et al., 2006) to obtain parse trees and dependency relations. Discourse Features. Previous work (Hirschberg and Litman, 1993; Abbott et al., 2011) suggests that discourse markers, such as what?, actually, may have their use for expressing opinions. We extract the initial unigram, bigram, and trigram of each utterance as discourse features (Hirschberg and Litman, 1993). Hedge words are collected from the CoNLL-2012 shared task (Farkas et al., 2010). Conversation Features. Conversation features encode some useful information regarding the similarity between the current utterance(s) and the sentences uttered by the target participant. TFIDF similarity is computed. We also check if the current utterance(s) quotes targe"
W14-2617,de-marneffe-etal-2006-generating,0,0.0408168,"Missing"
W14-2617,P09-2079,0,0.0426911,"Missing"
W14-2617,esuli-sebastiani-2006-sentiwordnet,0,0.00901566,"N N ] However I agree that the emphasis needs to be on the armaments crisis because it was the reason sold to the public and the major one used to justify the invasion but it needs to acknowledge that there was at least 12 reasons for the war as well.[P P ] ... agreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide"
W14-2617,W10-3001,0,0.02276,"Missing"
W14-2617,P13-1174,0,0.0140273,"e is no lexicon available for online discussions. Thus, we create from a large-scale corpus via label propagation. The label propagation algorithm, proposed by Zhu and Ghahramani (2002), is a semi-supervised learning method. In general, it takes as input a set of seed samples (e.g. sentiment words in our case), and the similarity between pairwise samples, then iteratively assigns values to the unlabeled samples (see Algorithm 1). The construction of graph G is discussed in Section 4.1. Sample sentiment words in the new lexicon are listed in Table 2. Edge Set E. As Velikovich et al. (2010) and Feng et al. (2013) notice, a dense graph with a large number of nodes is susceptible to propagating noise, and will not scale well. We thus adopt the algorithm in Feng et al. (2013) to construct a sparsely connected graph. For each text unit t, we first compute its representation vector ~a using Pairwise Mutual Information scores with respect to the top 50 co-occuring text units. We define “co-occur” as text units appearing in the same sentence. An edge is created between two text units t0 and t1 only if they ever co-occur. The similarity between t0 and t1 is calculated as the Cosine similarity between ~a0 and"
W14-2617,W13-4006,0,0.2293,"n the component of agreement and disagreement detection, the evaluation is always performed on the ultimate application only. for online debate. Abbott et al. (2011) investigate different types of features based on dependency relations as well as manually-labeled features, such as if the participants are nice, nasty, or sarcastic, and respect or insult the target participants. Automatically inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each to"
W14-2617,E12-1079,0,0.0146493,"ant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware of any published work that utilizes these annotations. Dialogue act recognition on talk pages (Ferschke et al., 2012) might be the most related. 3 The Model We first give a brief overview on isotonic Conditional Random Fields (isotonic CRF) (Mao and Lebanon, 2007), which is used as the backbone approach for our sentence- or segment-level agreement and disagreement detection model. We defer the explanation of online discussion lexicon construction in Section 4. 3.1 Problem Description Consider a discussion comprised of sequential turns uttered by the participants; each turn consists of a sequence of text units, where each unit can be a sentence or a segment of several sentences. Our model takes as input the t"
W14-2617,N13-1041,0,0.0179851,"0.67 for agreement and disagreement detection, when a linear chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages. 1 Introduction We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users’ attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process. 1 Our online discussion lexicon (Section 4) will be made publicly available. 97 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97–106, c Baltimore, Maryland, USA. June 27, 2014. 2014 Association for Computational Ling"
W14-2617,P04-1085,0,0.106885,"ns derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001). Galley et al. (2004) employ Conditional Markov models to detect if discussants reach at an agreement in spoken meetings. Each state in their model is an individual turn and prediction is made on the turnlevel. In the same spirit, Wang et al. (2011) also propose a sequential model based on CRF for detecting agreements and disagreements in broadcast conversations, where they primarily show the efficiency of prosodic features. While we also exploit a sequential model"
W14-2617,W12-3710,0,0.418571,"cally inducing those features from human annotation are challenging itself, so it would be difficult to reproduce their work on new datasets. We use only automatically generated features. Using the same dataset, Misra and Walker (2013) study the effectiveness of topicindependent features, e.g. discourse cues indicating agreement or negative opinion. Those cues, which serve a similar purpose as a sentiment lexicon, are also constructed manually. In our work, we create an online discussion lexicon automatically and construct sentiment features based on the lexicon. Also targeting online debate, Yin et al. (2012) train a logistic regression classifier with features aggregating posts from the same participant to predict the sentiment for each individual post. This approach works only when the speaker has enough posts on each topic, which is not applicable to newcomers. Hassan et al. (2010) focus on predicting the attitude of participants towards each other. They relate the sentiment words to the second person pronoun, which produces strong baselines. We also adopt their baselines in our work. Although there are available datasets with (dis)agreement annotated on Wikipedia talk pages, we are not aware o"
W14-2617,P09-1026,0,0.298182,"isotonic CRF model achieves F1 scores of 0.74 and 0.67 for agreement and disagreement detection, when a linear chain CRF obtains 0.58 and 0.56 for the discussions on Wikipedia Talk pages. 1 Introduction We are in an era where people can easily voice and exchange their opinions on the internet through forums or social media. Mining public opinion and the social interactions from online discussions is an important task, which has a wide range of applications. For example, by analyzing the users’ attitude in forum posts on social and political problems, it is able to identify ideological stance (Somasundaran and Wiebe, 2009) and user relations (Qiu et al., 2013), and thus further discover subgroups (Hassan et al., 2012; Abu-Jbara et al., 2012) with similar ideological viewpoint. Meanwhile, catching the sentiment in the conversation can help detect online disputes, reveal popular or controversial topics, and potentially disclose the public opinion formation process. 1 Our online discussion lexicon (Section 4) will be made publicly available. 97 Proceedings of the 5th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 97–106, c Baltimore, Maryland, USA. June 27, 2014. 2"
W14-2617,W10-0214,0,0.0497347,"tify the invasion but it needs to acknowledge that there was at least 12 reasons for the war as well.[P P ] ... agreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applicat"
W14-2617,W06-1639,0,0.113471,"elements in the partially ordered set O possess an ordinal relation ≤. Here, we differentiate agreement and disagreement with different intensity, because the output of our classifier can be used for other applications, such as dispute detection, where “strongly disagree” (e.g. NN) plays an important role. Meanwhile, fine-grained sentiment labels potentially provide richer context information for the sequential model employed for this task. While detecting agreement and disagreement in conversations is useful on its own, it is also a key component for related tasks, such as stance prediction (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Walker et al., 2012b) and subgroup detection (Hassan et al., 2012; Abu-Jbara et al., 2012). For instance, Thomas et al. (2006) train an agreement detection classifier with Support Vector Machines on congressional floor-debate transcripts to determine whether the speeches represent support of or opposition to the proposed legislation. Somasundaran and Wiebe (2009) design various sentiment constraints for inclusion in an integer linear programming framework for stance classification. For subgroup detection, Abu-Jbara et al. (2012) uses the polarity of the expressi"
W14-2617,N10-1119,0,0.0103901,"oise. So far as we know, there is no lexicon available for online discussions. Thus, we create from a large-scale corpus via label propagation. The label propagation algorithm, proposed by Zhu and Ghahramani (2002), is a semi-supervised learning method. In general, it takes as input a set of seed samples (e.g. sentiment words in our case), and the similarity between pairwise samples, then iteratively assigns values to the unlabeled samples (see Algorithm 1). The construction of graph G is discussed in Section 4.1. Sample sentiment words in the new lexicon are listed in Table 2. Edge Set E. As Velikovich et al. (2010) and Feng et al. (2013) notice, a dense graph with a large number of nodes is susceptible to propagating noise, and will not scale well. We thus adopt the algorithm in Feng et al. (2013) to construct a sparsely connected graph. For each text unit t, we first compute its representation vector ~a using Pairwise Mutual Information scores with respect to the top 50 co-occuring text units. We define “co-occur” as text units appearing in the same sentence. An edge is created between two text units t0 and t1 only if they ever co-occur. The similarity between t0 and t1 is calculated as the Cosine simi"
W14-2617,walker-etal-2012-corpus,0,0.048886,"less &gt;. We use “NN”, “N”, and “PP” to indicate “strongly disagree”, “disagree”, and “strongly agree”. Sentences in blue are examples whose sentiment is hard to detect by an existing lexicon. or segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis. We employ two existing online discussion data sets: the Authority and Alignment in Wikipedia Discussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and dis98 groups based on the intuition that people in the same group should mostly agree with each other. Though those work highly relies on the component of agreement and disagreement detection, the evaluation is always performed on the ultimate application only. for online debate. Abbott et al. (2011) inv"
W14-2617,N12-1072,0,0.0354572,"less &gt;. We use “NN”, “N”, and “PP” to indicate “strongly disagree”, “disagree”, and “strongly agree”. Sentences in blue are examples whose sentiment is hard to detect by an existing lexicon. or segment-level, are able to discover fine-grained sentiment flow within each turn, which can be further applied in other applications, such as dispute detection or argumentation structure analysis. We employ two existing online discussion data sets: the Authority and Alignment in Wikipedia Discussions (AAWD) corpus of Bender et al. (2011) (Wikipedia talk pages) and the Internet Argument Corpus (IAC) of Walker et al. (2012a). Experimental results show that our model significantly outperforms state-of-the-art methods on the AAWD data (our F1 scores are 0.74 and 0.67 for agreement and disagreement, vs. 0.58 and 0.56 for the linear chain CRF approach) and IAC data (our F1 scores are 0.61 and 0.78 for agreement and dis98 groups based on the intuition that people in the same group should mostly agree with each other. Though those work highly relies on the component of agreement and disagreement detection, the evaluation is always performed on the ultimate application only. for online debate. Abbott et al. (2011) inv"
W14-2617,P11-2065,0,0.16263,"the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction (Section 4). After explain the experimental setup, we display the results and provide further analysis in Section 6. 2 Related Work Sentiment analysis has been utilized as a key enabling technique in a number of conversationbased applications. Previous work mainly studies the attitudes in spoken meetings (Galley et al., 2004; Hahn et al., 2006) or broadcast conversations (Wang et al., 2011) using Conditional Random Fields (CRF) (Lafferty et al., 2001). Galley et al. (2004) employ Conditional Markov models to detect if discussants reach at an agreement in spoken meetings. Each state in their model is an individual turn and prediction is made on the turnlevel. In the same spirit, Wang et al. (2011) also propose a sequential model based on CRF for detecting agreements and disagreements in broadcast conversations, where they primarily show the efficiency of prosodic features. While we also exploit a sequential model extended from CRFs, our predictions are made for each sentence or s"
W14-2617,H05-1044,0,0.0369926,"that the ONLY reason for the war was WMD’s is wrong - because it simply isn’t.[N N ] However I agree that the emphasis needs to be on the armaments crisis because it was the reason sold to the public and the major one used to justify the invasion but it needs to acknowledge that there was at least 12 reasons for the war as well.[P P ] ... agreement, vs. 0.28 and 0.73 for SVM). (2) Furthermore, we construct a new sentiment lexicon for online discussion. We show that the learned lexicon significantly improves performance over systems that use existing generalpurpose lexicons (i.e. MPQA lexicon (Wilson et al., 2005), General Inquirer (Stone et al., 1966), and SentiWordNet (Esuli and Sebastiani, 2006)). Our lexicon is constructed from a very large-scale discussion corpus based on Wikipedia talk page, where previous work (Somasundaran and Wiebe, 2010) for constructing online discussion lexicon relies on human annotations derived from limited number of conversations. In the remainder of the paper, we describe first the related work (Section 2). Then we introduce the sentence-level agreement and disagreement identification model (Section 3) as well as the label propagation algorithm for lexicon construction"
W14-2617,prasad-etal-2008-penn,0,\N,Missing
W14-2617,N06-2014,0,\N,Missing
W17-4513,P02-1040,0,0.0994259,"Missing"
W17-4513,P07-1056,0,0.246116,"Missing"
W17-4513,P14-1084,0,0.01331,"human summarization process and produces more concise summaries (Nenkova et al., 2011). Built on the success of sequenceto-sequence learning with encoder-decoder neural networks (Bahdanau et al., 2014), there has been growing interest in utilizing this framework for generating abstractive summaries (Rush et al., 2015; Wang and Ling, 2016; Takase et al., 2016; Nallapati et al., 2016; See et al., 2017). The end-to-end learning framework circumvents efforts in feature engineering and template construction as done in previous work (Ganesan et al., 2010; Wang and Cardie, 2013; Gerani et al., 2014; Pighin et al., 2014), by directly learning to detect summary-worthy content as well as generate fluent sentences. Nevertheless, training such systems requires large amounts of labeled data, which creates a big hurdle for new domains where training data is scant and expensive to acquire. Consequently, we raise the following research questions: Figure 1: A snippet of sample news story and opinion article from The New York Times Annotated Corpus (Sandhaus, 2008). • domain adaptation: whether we can leverage available out-of-domain abstracts or extractive summaries to help train a neural summarization system for a ne"
W17-4513,P07-1033,0,0.224762,"Missing"
W17-4513,N04-1001,0,0.12156,"Missing"
W17-4513,D10-1044,0,0.146409,"Missing"
W17-4513,C10-1039,0,0.0270714,"s towards producing abstractive summmaries, which better emulates human summarization process and produces more concise summaries (Nenkova et al., 2011). Built on the success of sequenceto-sequence learning with encoder-decoder neural networks (Bahdanau et al., 2014), there has been growing interest in utilizing this framework for generating abstractive summaries (Rush et al., 2015; Wang and Ling, 2016; Takase et al., 2016; Nallapati et al., 2016; See et al., 2017). The end-to-end learning framework circumvents efforts in feature engineering and template construction as done in previous work (Ganesan et al., 2010; Wang and Cardie, 2013; Gerani et al., 2014; Pighin et al., 2014), by directly learning to detect summary-worthy content as well as generate fluent sentences. Nevertheless, training such systems requires large amounts of labeled data, which creates a big hurdle for new domains where training data is scant and expensive to acquire. Consequently, we raise the following research questions: Figure 1: A snippet of sample news story and opinion article from The New York Times Annotated Corpus (Sandhaus, 2008). • domain adaptation: whether we can leverage available out-of-domain abstracts or extract"
W17-4513,D15-1044,0,0.0511216,"ther analysis shows that, the model is capable to select salient content even trained on out-of-domain data, but requires in-domain data to capture the style for a target domain. 1 Introduction Recent text summarization research moves towards producing abstractive summmaries, which better emulates human summarization process and produces more concise summaries (Nenkova et al., 2011). Built on the success of sequenceto-sequence learning with encoder-decoder neural networks (Bahdanau et al., 2014), there has been growing interest in utilizing this framework for generating abstractive summaries (Rush et al., 2015; Wang and Ling, 2016; Takase et al., 2016; Nallapati et al., 2016; See et al., 2017). The end-to-end learning framework circumvents efforts in feature engineering and template construction as done in previous work (Ganesan et al., 2010; Wang and Cardie, 2013; Gerani et al., 2014; Pighin et al., 2014), by directly learning to detect summary-worthy content as well as generate fluent sentences. Nevertheless, training such systems requires large amounts of labeled data, which creates a big hurdle for new domains where training data is scant and expensive to acquire. Consequently, we raise the fol"
W17-4513,W10-2603,0,0.55723,"Missing"
W17-4513,D14-1168,0,0.0213502,"hich better emulates human summarization process and produces more concise summaries (Nenkova et al., 2011). Built on the success of sequenceto-sequence learning with encoder-decoder neural networks (Bahdanau et al., 2014), there has been growing interest in utilizing this framework for generating abstractive summaries (Rush et al., 2015; Wang and Ling, 2016; Takase et al., 2016; Nallapati et al., 2016; See et al., 2017). The end-to-end learning framework circumvents efforts in feature engineering and template construction as done in previous work (Ganesan et al., 2010; Wang and Cardie, 2013; Gerani et al., 2014; Pighin et al., 2014), by directly learning to detect summary-worthy content as well as generate fluent sentences. Nevertheless, training such systems requires large amounts of labeled data, which creates a big hurdle for new domains where training data is scant and expensive to acquire. Consequently, we raise the following research questions: Figure 1: A snippet of sample news story and opinion article from The New York Times Annotated Corpus (Sandhaus, 2008). • domain adaptation: whether we can leverage available out-of-domain abstracts or extractive summaries to help train a neural summari"
W17-4513,P16-1154,0,0.0408215,"Missing"
W17-4513,P17-1099,0,0.495543,"on out-of-domain data, but requires in-domain data to capture the style for a target domain. 1 Introduction Recent text summarization research moves towards producing abstractive summmaries, which better emulates human summarization process and produces more concise summaries (Nenkova et al., 2011). Built on the success of sequenceto-sequence learning with encoder-decoder neural networks (Bahdanau et al., 2014), there has been growing interest in utilizing this framework for generating abstractive summaries (Rush et al., 2015; Wang and Ling, 2016; Takase et al., 2016; Nallapati et al., 2016; See et al., 2017). The end-to-end learning framework circumvents efforts in feature engineering and template construction as done in previous work (Ganesan et al., 2010; Wang and Cardie, 2013; Gerani et al., 2014; Pighin et al., 2014), by directly learning to detect summary-worthy content as well as generate fluent sentences. Nevertheless, training such systems requires large amounts of labeled data, which creates a big hurdle for new domains where training data is scant and expensive to acquire. Consequently, we raise the following research questions: Figure 1: A snippet of sample news story and opinion artic"
W17-4513,D16-1112,0,0.0295045,"apable to select salient content even trained on out-of-domain data, but requires in-domain data to capture the style for a target domain. 1 Introduction Recent text summarization research moves towards producing abstractive summmaries, which better emulates human summarization process and produces more concise summaries (Nenkova et al., 2011). Built on the success of sequenceto-sequence learning with encoder-decoder neural networks (Bahdanau et al., 2014), there has been growing interest in utilizing this framework for generating abstractive summaries (Rush et al., 2015; Wang and Ling, 2016; Takase et al., 2016; Nallapati et al., 2016; See et al., 2017). The end-to-end learning framework circumvents efforts in feature engineering and template construction as done in previous work (Ganesan et al., 2010; Wang and Cardie, 2013; Gerani et al., 2014; Pighin et al., 2014), by directly learning to detect summary-worthy content as well as generate fluent sentences. Nevertheless, training such systems requires large amounts of labeled data, which creates a big hurdle for new domains where training data is scant and expensive to acquire. Consequently, we raise the following research questions: Figure 1: A sni"
W17-4513,K16-1028,0,0.114894,"Missing"
W17-4513,N16-1007,1,0.848879,"that, the model is capable to select salient content even trained on out-of-domain data, but requires in-domain data to capture the style for a target domain. 1 Introduction Recent text summarization research moves towards producing abstractive summmaries, which better emulates human summarization process and produces more concise summaries (Nenkova et al., 2011). Built on the success of sequenceto-sequence learning with encoder-decoder neural networks (Bahdanau et al., 2014), there has been growing interest in utilizing this framework for generating abstractive summaries (Rush et al., 2015; Wang and Ling, 2016; Takase et al., 2016; Nallapati et al., 2016; See et al., 2017). The end-to-end learning framework circumvents efforts in feature engineering and template construction as done in previous work (Ganesan et al., 2010; Wang and Cardie, 2013; Gerani et al., 2014; Pighin et al., 2014), by directly learning to detect summary-worthy content as well as generate fluent sentences. Nevertheless, training such systems requires large amounts of labeled data, which creates a big hurdle for new domains where training data is scant and expensive to acquire. Consequently, we raise the following research quest"
W17-4513,H05-1044,0,0.0560437,"Missing"
W17-4513,W04-1013,0,\N,Missing
W17-4513,W01-0100,0,\N,Missing
