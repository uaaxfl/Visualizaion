2020.acl-main.206,P16-1068,1,0.850152,"and Paulus, 1968; Attali and Burstein, 2004; Bhat and Yoon, 2015; Sakaguchi et al., 2015), as well as grammatical errors and phrase-structure rules (Yannakoudakis et al., 2011; Andersen et al., 1 https://github.com/hcraighead/ automated-english-transcription-grader; the corpus we work with is not publicly available as it is private exams data, but the code repository allows you to work with any set of English texts and proficiency scores. 2013). More recently, word and character embeddings have served as input to deep neural network models, with a final regression layer predicting the score (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017; Jin et al., 2018). The advantage of the latter approach is the relative ease of data pre-processing since text representations are learned through distributional methods rather than hand-crafted features. The field of NLP has seen advances recently thanks to a shift from fixed word embeddings to contextualized representations such as ELMo (Peters et al., 2018) and those which can be obtained from large transformer models such as BERT (Devlin et al., 2019). Similarly in text scoring, some have incorporated contextualized word embeddings to improve pe"
2020.acl-main.206,W13-1704,1,0.881955,"Missing"
2020.acl-main.206,K17-1017,0,0.011911,"and Yoon, 2015; Sakaguchi et al., 2015), as well as grammatical errors and phrase-structure rules (Yannakoudakis et al., 2011; Andersen et al., 1 https://github.com/hcraighead/ automated-english-transcription-grader; the corpus we work with is not publicly available as it is private exams data, but the code repository allows you to work with any set of English texts and proficiency scores. 2013). More recently, word and character embeddings have served as input to deep neural network models, with a final regression layer predicting the score (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017; Jin et al., 2018). The advantage of the latter approach is the relative ease of data pre-processing since text representations are learned through distributional methods rather than hand-crafted features. The field of NLP has seen advances recently thanks to a shift from fixed word embeddings to contextualized representations such as ELMo (Peters et al., 2018) and those which can be obtained from large transformer models such as BERT (Devlin et al., 2019). Similarly in text scoring, some have incorporated contextualized word embeddings to improve performance (Nadeem et al., 2019). We now app"
2020.acl-main.206,W19-4410,1,0.845405,"g in a 768 dimensional embedding. Auxiliary objectives We further extend the model to incorporate auxiliary objectives, and experiment with four different tasks: language modelling (LM), native language prediction (L1), POS-tagging, and UD prediction where we predict the UD type of a dependent with its head (see Section 3). These auxiliary objectives are based on previous work indicating that learning to make such predictions aids in tasks such as essay scoring and grammatical error detection (Cheng et al., 2015; Rei and Yannakoudakis, 2017; Cummins and Rei, 2018; Johan Berggren et al., 2019; Bell et al., 2019). Specifically, for the last three tasks, we predict a label y per word xt (Figure 3; left). Each task s is assigned an individual prediction head, identical to the scoring head described above, followed by a softmax layer that produces a probability distribution over the set of output labels to replace the bounded scoring activation function. When using BERT, our model only predicts labels for auxiliary objectives on the first token of a word, in an identical fashion to Devlin et al. (2019)’s evaluation of BERT on named entity recognition. The LM objective is implemented differently for each"
2020.acl-main.206,W17-4604,1,0.851812,"porated contextualized word embeddings to improve performance (Nadeem et al., 2019). We now apply such approaches to the grading of spoken transcriptions in a scenario where the audio, or information derived from it, is not available. In other words the task is analogous to essay scoring except for the presence of characteristic speech features such as false starts, repetitions and filled pauses (Moore et al., 2015; Carter and McCarthy, 2017). This poses a particular challenge as most models used in data pre-processing and representation learning have been trained on written not spoken texts (Caines et al., 2017). Furthermore, most existing approaches to speech grading do have access to audio features, and indeed extract a large number of prosodic or duration-based features (Zechner et al., 2009; Higgins et al., 2011; Loukina et al., 2017; Wang et al., 2018a). Prosodic and phonological features extracted from the audio and ASR model are undoubtedly useful for human assessment of speech proficiency and for providing feedback. On the other hand, previous work suggests that models trained solely on ASR text-based features are competitive with those using only acoustic features or a combination of the two"
2020.acl-main.206,D15-1085,0,0.0526647,"Missing"
2020.acl-main.206,N19-1423,0,0.477265,"as input to deep neural network models, with a final regression layer predicting the score (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017; Jin et al., 2018). The advantage of the latter approach is the relative ease of data pre-processing since text representations are learned through distributional methods rather than hand-crafted features. The field of NLP has seen advances recently thanks to a shift from fixed word embeddings to contextualized representations such as ELMo (Peters et al., 2018) and those which can be obtained from large transformer models such as BERT (Devlin et al., 2019). Similarly in text scoring, some have incorporated contextualized word embeddings to improve performance (Nadeem et al., 2019). We now apply such approaches to the grading of spoken transcriptions in a scenario where the audio, or information derived from it, is not available. In other words the task is analogous to essay scoring except for the presence of characteristic speech features such as false starts, repetitions and filled pauses (Moore et al., 2015; Carter and McCarthy, 2017). This poses a particular challenge as most models used in data pre-processing and representation learning hav"
2020.acl-main.206,P18-1100,0,0.0292337,"Missing"
2020.acl-main.206,W19-4409,0,0.0723087,"lti-task learning has been shown to help in automated essay 2259 Candidates Transcriptions Total words Mean response length (words) Train 691 4,589 205,311 44.7 Valid 297 1,982 91,224 46.0 Test 225 1488 67,832 45.6 Total 1213 8,059 343,367 42.6 Table 1: Training, validation and test split statistics. scoring (Cummins and Rei, 2018) and grammatical error detection of learner English essays (Rei and Yannakoudakis, 2017), whilst information about a learner’s native language has been shown to help in error detection for English and the grading of Norwegian essays (Rozovskaya and Roth, 2011; Johan Berggren et al., 2019). Furthermore, multi-task learning objectives can allow the model to learn more general features of language and composition, and a much richer set of representations (Sanh et al., 2019), without relying on the availability of any external linguistic tools or annotations at inference time. 3 Data We train our models using spoken responses collected from candidates taking Cambridge Assessment’s BULATS examination2 . The spoken section of the BULATS exam tests candidates’ proficiency in business English through monologue responses to a series of prompts. The candidate may speak for up to one min"
2020.acl-main.206,W16-0514,0,0.028367,"Furthermore, most existing approaches to speech grading do have access to audio features, and indeed extract a large number of prosodic or duration-based features (Zechner et al., 2009; Higgins et al., 2011; Loukina et al., 2017; Wang et al., 2018a). Prosodic and phonological features extracted from the audio and ASR model are undoubtedly useful for human assessment of speech proficiency and for providing feedback. On the other hand, previous work suggests that models trained solely on ASR text-based features are competitive with those using only acoustic features or a combination of the two (Loukina and Cahill, 2016). Their interpretation of these results was that the transcription offers some proxy information for prosodic and phonological performance – for instance the presence of hesitation and silence markers, the number of word tokens in the transcription, and the transcription errors which might arise from mispronunciations. We instead allow our models to learn from auxiliary (morpho-syntactic and other) tasks: multi-task learning has been shown to help in automated essay 2259 Candidates Transcriptions Total words Mean response length (words) Train 691 4,589 205,311 44.7 Valid 297 1,982 91,224 46.0"
2020.acl-main.206,W17-4609,0,0.0170479,"ble. In other words the task is analogous to essay scoring except for the presence of characteristic speech features such as false starts, repetitions and filled pauses (Moore et al., 2015; Carter and McCarthy, 2017). This poses a particular challenge as most models used in data pre-processing and representation learning have been trained on written not spoken texts (Caines et al., 2017). Furthermore, most existing approaches to speech grading do have access to audio features, and indeed extract a large number of prosodic or duration-based features (Zechner et al., 2009; Higgins et al., 2011; Loukina et al., 2017; Wang et al., 2018a). Prosodic and phonological features extracted from the audio and ASR model are undoubtedly useful for human assessment of speech proficiency and for providing feedback. On the other hand, previous work suggests that models trained solely on ASR text-based features are competitive with those using only acoustic features or a combination of the two (Loukina and Cahill, 2016). Their interpretation of these results was that the transcription offers some proxy information for prosodic and phonological performance – for instance the presence of hesitation and silence markers, t"
2020.acl-main.206,W19-4450,0,0.0127934,"and Ng, 2016; Dong et al., 2017; Jin et al., 2018). The advantage of the latter approach is the relative ease of data pre-processing since text representations are learned through distributional methods rather than hand-crafted features. The field of NLP has seen advances recently thanks to a shift from fixed word embeddings to contextualized representations such as ELMo (Peters et al., 2018) and those which can be obtained from large transformer models such as BERT (Devlin et al., 2019). Similarly in text scoring, some have incorporated contextualized word embeddings to improve performance (Nadeem et al., 2019). We now apply such approaches to the grading of spoken transcriptions in a scenario where the audio, or information derived from it, is not available. In other words the task is analogous to essay scoring except for the presence of characteristic speech features such as false starts, repetitions and filled pauses (Moore et al., 2015; Carter and McCarthy, 2017). This poses a particular challenge as most models used in data pre-processing and representation learning have been trained on written not spoken texts (Caines et al., 2017). Furthermore, most existing approaches to speech grading do ha"
2020.acl-main.206,D14-1162,0,0.0822836,"Missing"
2020.acl-main.206,W17-5004,1,0.921312,"e number of word tokens in the transcription, and the transcription errors which might arise from mispronunciations. We instead allow our models to learn from auxiliary (morpho-syntactic and other) tasks: multi-task learning has been shown to help in automated essay 2259 Candidates Transcriptions Total words Mean response length (words) Train 691 4,589 205,311 44.7 Valid 297 1,982 91,224 46.0 Test 225 1488 67,832 45.6 Total 1213 8,059 343,367 42.6 Table 1: Training, validation and test split statistics. scoring (Cummins and Rei, 2018) and grammatical error detection of learner English essays (Rei and Yannakoudakis, 2017), whilst information about a learner’s native language has been shown to help in error detection for English and the grading of Norwegian essays (Rozovskaya and Roth, 2011; Johan Berggren et al., 2019). Furthermore, multi-task learning objectives can allow the model to learn more general features of language and composition, and a much richer set of representations (Sanh et al., 2019), without relying on the availability of any external linguistic tools or annotations at inference time. 3 Data We train our models using spoken responses collected from candidates taking Cambridge Assessment’s BU"
2020.acl-main.206,P11-1093,0,0.0332018,"ho-syntactic and other) tasks: multi-task learning has been shown to help in automated essay 2259 Candidates Transcriptions Total words Mean response length (words) Train 691 4,589 205,311 44.7 Valid 297 1,982 91,224 46.0 Test 225 1488 67,832 45.6 Total 1213 8,059 343,367 42.6 Table 1: Training, validation and test split statistics. scoring (Cummins and Rei, 2018) and grammatical error detection of learner English essays (Rei and Yannakoudakis, 2017), whilst information about a learner’s native language has been shown to help in error detection for English and the grading of Norwegian essays (Rozovskaya and Roth, 2011; Johan Berggren et al., 2019). Furthermore, multi-task learning objectives can allow the model to learn more general features of language and composition, and a much richer set of representations (Sanh et al., 2019), without relying on the availability of any external linguistic tools or annotations at inference time. 3 Data We train our models using spoken responses collected from candidates taking Cambridge Assessment’s BULATS examination2 . The spoken section of the BULATS exam tests candidates’ proficiency in business English through monologue responses to a series of prompts. The candida"
2020.acl-main.206,N15-1111,0,0.0273419,"on ASR transcriptions which are labeled with proficiency scores assigned by human examiners, and guide the networks with objectives that prioritize language understanding. To the best of our knowledge, there has been no previous work using text-based auxiliary training objectives in automated speech grading systems. 2 Related Work Automated grading of student responses to exam questions until recently tended to adopt featurebased approaches to score prediction, for instance using distinctive word or part-of-speech n-grams (Page and Paulus, 1968; Attali and Burstein, 2004; Bhat and Yoon, 2015; Sakaguchi et al., 2015), as well as grammatical errors and phrase-structure rules (Yannakoudakis et al., 2011; Andersen et al., 1 https://github.com/hcraighead/ automated-english-transcription-grader; the corpus we work with is not publicly available as it is private exams data, but the code repository allows you to work with any set of English texts and proficiency scores. 2013). More recently, word and character embeddings have served as input to deep neural network models, with a final regression layer predicting the score (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017; Jin et al., 2018). Th"
2020.acl-main.206,silveira-etal-2014-gold,0,0.073022,"Missing"
2020.acl-main.206,D16-1193,0,0.0177625,"and Burstein, 2004; Bhat and Yoon, 2015; Sakaguchi et al., 2015), as well as grammatical errors and phrase-structure rules (Yannakoudakis et al., 2011; Andersen et al., 1 https://github.com/hcraighead/ automated-english-transcription-grader; the corpus we work with is not publicly available as it is private exams data, but the code repository allows you to work with any set of English texts and proficiency scores. 2013). More recently, word and character embeddings have served as input to deep neural network models, with a final regression layer predicting the score (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017; Jin et al., 2018). The advantage of the latter approach is the relative ease of data pre-processing since text representations are learned through distributional methods rather than hand-crafted features. The field of NLP has seen advances recently thanks to a shift from fixed word embeddings to contextualized representations such as ELMo (Peters et al., 2018) and those which can be obtained from large transformer models such as BERT (Devlin et al., 2019). Similarly in text scoring, some have incorporated contextualized word embeddings to improve performance (Nadeem et al."
2020.acl-main.206,W18-0515,0,0.0165092,"ompared to the single-task scoring model. a single, or multiple auxiliary objectives respectively). Responses are processed in batches of 8 and are padded/truncated to a length of 128. LSTM token embeddings of size 300 are randomly initialized and fine-tuned during training.4 The LSTM has 3 hidden layers with hidden state sizes of 256 for each direction. Weightings for each of the auxiliary objectives were selected by evaluation on the validation set and are outlined in Table 2. Baseline model Our baseline approach is a feature-based model of the type which has been used in previous research (Vajjala and Rama, 2018; Yannakoudakis et al., 2018). Specifically, we train a linear regression model and use as features tf– idf weighted word and POS n-grams (up to trigrams), grammatical constructions extracted from the phrase-structure trees, the length of the transcript, and the number of errors, estimated by counting the number of trigrams that are absent from a large background corpus of correct English (Ferraresi et al., 2008). Evaluation Our primary metric is root-meansquare error (RMSE), which results in real valued average distances from the gold standard examiner scores on our 0–6 scale. For each model"
2020.acl-main.206,N18-1202,0,0.0128497,"sh texts and proficiency scores. 2013). More recently, word and character embeddings have served as input to deep neural network models, with a final regression layer predicting the score (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017; Jin et al., 2018). The advantage of the latter approach is the relative ease of data pre-processing since text representations are learned through distributional methods rather than hand-crafted features. The field of NLP has seen advances recently thanks to a shift from fixed word embeddings to contextualized representations such as ELMo (Peters et al., 2018) and those which can be obtained from large transformer models such as BERT (Devlin et al., 2019). Similarly in text scoring, some have incorporated contextualized word embeddings to improve performance (Nadeem et al., 2019). We now apply such approaches to the grading of spoken transcriptions in a scenario where the audio, or information derived from it, is not available. In other words the task is analogous to essay scoring except for the presence of characteristic speech features such as false starts, repetitions and filled pauses (Moore et al., 2015; Carter and McCarthy, 2017). This poses"
2020.acl-main.206,P17-1194,0,0.0578176,"Missing"
2020.acl-main.206,P11-1019,1,0.741446,"xaminers, and guide the networks with objectives that prioritize language understanding. To the best of our knowledge, there has been no previous work using text-based auxiliary training objectives in automated speech grading systems. 2 Related Work Automated grading of student responses to exam questions until recently tended to adopt featurebased approaches to score prediction, for instance using distinctive word or part-of-speech n-grams (Page and Paulus, 1968; Attali and Burstein, 2004; Bhat and Yoon, 2015; Sakaguchi et al., 2015), as well as grammatical errors and phrase-structure rules (Yannakoudakis et al., 2011; Andersen et al., 1 https://github.com/hcraighead/ automated-english-transcription-grader; the corpus we work with is not publicly available as it is private exams data, but the code repository allows you to work with any set of English texts and proficiency scores. 2013). More recently, word and character embeddings have served as input to deep neural network models, with a final regression layer predicting the score (Alikaniotis et al., 2016; Taghipour and Ng, 2016; Dong et al., 2017; Jin et al., 2018). The advantage of the latter approach is the relative ease of data pre-processing since t"
2020.acl-main.394,S19-2100,1,0.781489,"comes in the form of expletives, derogatory words or threats, with substantial success (Mishra et al., 2019b). However, abuse can also be expressed in more implicit and subtle ways, for instance, through the use of ambiguous terms and figurative language, which has proved more challenging to identify. The NLP community has experimented with a range of techniques for abuse detection, such as recurrent and convolutional neural networks (Pavlopoulos et al., 2017; Park and Fung, 2017; Wang, 2018), character-based models (Nobata et al., 2016) and graph-based learning methods (Mishra et al., 2018a; Aglionby et al., 2019; Mishra et al., 2019a), obtaining promising results. However, all of the existing approaches have focused on modelling the linguistic properties of the comments or the meta-data about the users. On the other hand, abusive language and behaviour are also inextricably linked to the emotional and psychological state of the speaker (Patrick, 1901), which is reflected in the affective characteristics of their language (Mabry, 1974). In this paper, we propose to model these two phenomena jointly and present the first abusive language detection method that incorporates affective features via a multi"
2020.acl-main.394,C18-1093,1,0.944357,"explicit abuse, that comes in the form of expletives, derogatory words or threats, with substantial success (Mishra et al., 2019b). However, abuse can also be expressed in more implicit and subtle ways, for instance, through the use of ambiguous terms and figurative language, which has proved more challenging to identify. The NLP community has experimented with a range of techniques for abuse detection, such as recurrent and convolutional neural networks (Pavlopoulos et al., 2017; Park and Fung, 2017; Wang, 2018), character-based models (Nobata et al., 2016) and graph-based learning methods (Mishra et al., 2018a; Aglionby et al., 2019; Mishra et al., 2019a), obtaining promising results. However, all of the existing approaches have focused on modelling the linguistic properties of the comments or the meta-data about the users. On the other hand, abusive language and behaviour are also inextricably linked to the emotional and psychological state of the speaker (Patrick, 1901), which is reflected in the affective characteristics of their language (Mabry, 1974). In this paper, we propose to model these two phenomena jointly and present the first abusive language detection method that incorporates affect"
2020.acl-main.394,N19-1221,1,0.869815,"ological consequences for its victims (Munro, 2011). This stresses the need for automated techniques for abusive language detection, a problem that has recently gained a great deal of interest in the natural language processing community. The term abuse refers collectively to all forms of expression that vilify or offend an individual or a group, including racism, sexism, personal attacks, harassment, cyber-bullying, and many others. Much of the recent research has focused on detecting explicit abuse, that comes in the form of expletives, derogatory words or threats, with substantial success (Mishra et al., 2019b). However, abuse can also be expressed in more implicit and subtle ways, for instance, through the use of ambiguous terms and figurative language, which has proved more challenging to identify. The NLP community has experimented with a range of techniques for abuse detection, such as recurrent and convolutional neural networks (Pavlopoulos et al., 2017; Park and Fung, 2017; Wang, 2018), character-based models (Nobata et al., 2016) and graph-based learning methods (Mishra et al., 2018a; Aglionby et al., 2019; Mishra et al., 2019a), obtaining promising results. However, all of the existing app"
2020.acl-main.394,W18-5101,1,0.889863,"explicit abuse, that comes in the form of expletives, derogatory words or threats, with substantial success (Mishra et al., 2019b). However, abuse can also be expressed in more implicit and subtle ways, for instance, through the use of ambiguous terms and figurative language, which has proved more challenging to identify. The NLP community has experimented with a range of techniques for abuse detection, such as recurrent and convolutional neural networks (Pavlopoulos et al., 2017; Park and Fung, 2017; Wang, 2018), character-based models (Nobata et al., 2016) and graph-based learning methods (Mishra et al., 2018a; Aglionby et al., 2019; Mishra et al., 2019a), obtaining promising results. However, all of the existing approaches have focused on modelling the linguistic properties of the comments or the meta-data about the users. On the other hand, abusive language and behaviour are also inextricably linked to the emotional and psychological state of the speaker (Patrick, 1901), which is reflected in the affective characteristics of their language (Mabry, 1974). In this paper, we propose to model these two phenomena jointly and present the first abusive language detection method that incorporates affect"
2020.acl-main.394,S18-1001,0,0.0836111,"Missing"
2020.acl-main.394,W17-3006,0,0.182203,"rsonal attacks, harassment, cyber-bullying, and many others. Much of the recent research has focused on detecting explicit abuse, that comes in the form of expletives, derogatory words or threats, with substantial success (Mishra et al., 2019b). However, abuse can also be expressed in more implicit and subtle ways, for instance, through the use of ambiguous terms and figurative language, which has proved more challenging to identify. The NLP community has experimented with a range of techniques for abuse detection, such as recurrent and convolutional neural networks (Pavlopoulos et al., 2017; Park and Fung, 2017; Wang, 2018), character-based models (Nobata et al., 2016) and graph-based learning methods (Mishra et al., 2018a; Aglionby et al., 2019; Mishra et al., 2019a), obtaining promising results. However, all of the existing approaches have focused on modelling the linguistic properties of the comments or the meta-data about the users. On the other hand, abusive language and behaviour are also inextricably linked to the emotional and psychological state of the speaker (Patrick, 1901), which is reflected in the affective characteristics of their language (Mabry, 1974). In this paper, we propose to m"
2020.acl-main.394,W17-3004,0,0.0163922,"cluding racism, sexism, personal attacks, harassment, cyber-bullying, and many others. Much of the recent research has focused on detecting explicit abuse, that comes in the form of expletives, derogatory words or threats, with substantial success (Mishra et al., 2019b). However, abuse can also be expressed in more implicit and subtle ways, for instance, through the use of ambiguous terms and figurative language, which has proved more challenging to identify. The NLP community has experimented with a range of techniques for abuse detection, such as recurrent and convolutional neural networks (Pavlopoulos et al., 2017; Park and Fung, 2017; Wang, 2018), character-based models (Nobata et al., 2016) and graph-based learning methods (Mishra et al., 2018a; Aglionby et al., 2019; Mishra et al., 2019a), obtaining promising results. However, all of the existing approaches have focused on modelling the linguistic properties of the comments or the meta-data about the users. On the other hand, abusive language and behaviour are also inextricably linked to the emotional and psychological state of the speaker (Patrick, 1901), which is reflected in the affective characteristics of their language (Mabry, 1974). In this p"
2020.acl-main.394,D14-1162,0,0.0852711,"re W l1 and W l2 are the weight matrices of the 2-layer MLP. Dropout is applied to the output m(p) of the MLP, which is then followed by a linear output layer to get the unnormalized output o(p) . For OffensEval, a sigmoid activation σ is then applied in order to make a binary prediction with respect to whether a post is offensive or not, while the network parameters are optimized to minimize the binary cross-entropy (BCE): Across the two STL baselines, we further experiment with two different input representations: 1) GloVe (G), where the input is projected through the GloVe embedding layer (Pennington et al., 2014); 2) GloVe+ELMo (G+E), where the input is first projected through the GloVe embedding layer and the ELMo embedding layer (Peters et al., 2018) separately, and then the final word representation e is obtained by concatenating the output of these two layers. Given these input representations, we have a total of 4 different baseline models for abuse detection. We use grid search to tune the hyperparameters of the baselines on the development sets of the primary task (i.e., abuse detection). 4.2 LBCE = − N 1 X yi · log(p(yi ))+ N i=1 (1 − yi ) · log(1 − p(yi )) (6) where N is the number of trainin"
2020.acl-main.394,N18-1202,0,0.0519517,"output layer to get the unnormalized output o(p) . For OffensEval, a sigmoid activation σ is then applied in order to make a binary prediction with respect to whether a post is offensive or not, while the network parameters are optimized to minimize the binary cross-entropy (BCE): Across the two STL baselines, we further experiment with two different input representations: 1) GloVe (G), where the input is projected through the GloVe embedding layer (Pennington et al., 2014); 2) GloVe+ELMo (G+E), where the input is first projected through the GloVe embedding layer and the ELMo embedding layer (Peters et al., 2018) separately, and then the final word representation e is obtained by concatenating the output of these two layers. Given these input representations, we have a total of 4 different baseline models for abuse detection. We use grid search to tune the hyperparameters of the baselines on the development sets of the primary task (i.e., abuse detection). 4.2 LBCE = − N 1 X yi · log(p(yi ))+ N i=1 (1 − yi ) · log(1 − p(yi )) (6) where N is the number of training examples, and y denotes the true and p(y) the predicted label. For Waseem&Hovy, a log sof tmax activation is applied for multiclass classifi"
2020.acl-main.394,P19-1163,0,0.0777485,"Missing"
2020.acl-main.394,W18-5111,0,0.0773734,"sment, cyber-bullying, and many others. Much of the recent research has focused on detecting explicit abuse, that comes in the form of expletives, derogatory words or threats, with substantial success (Mishra et al., 2019b). However, abuse can also be expressed in more implicit and subtle ways, for instance, through the use of ambiguous terms and figurative language, which has proved more challenging to identify. The NLP community has experimented with a range of techniques for abuse detection, such as recurrent and convolutional neural networks (Pavlopoulos et al., 2017; Park and Fung, 2017; Wang, 2018), character-based models (Nobata et al., 2016) and graph-based learning methods (Mishra et al., 2018a; Aglionby et al., 2019; Mishra et al., 2019a), obtaining promising results. However, all of the existing approaches have focused on modelling the linguistic properties of the comments or the meta-data about the users. On the other hand, abusive language and behaviour are also inextricably linked to the emotional and psychological state of the speaker (Patrick, 1901), which is reflected in the affective characteristics of their language (Mabry, 1974). In this paper, we propose to model these tw"
2020.acl-main.394,N16-2013,0,0.345304,"for abuse detection. Furthermore, we compare the performance of MTL to a transfer learning baseline and demonstrate that MTL provides significant improvements over transfer learning. 2 Related Work Techniques for abuse detection have gone through several stages of development, starting with extensive manual feature engineering and then turning to deep learning. Early approaches experimented with lexicon-based features (Gitari et al., 2015), bagof-words (BOW) or n-gram features (Sood et al., 2012; Dinakar et al., 2011), and user-specific features, such as age (Dadvar et al., 2013) and gender (Waseem and Hovy, 2016). With the advent of deep learning, the trend shifted, with abundant work focusing on neural architectures for abuse detection. In particular, the use of convolutional neural networks (CNNs) for detecting abuse has shown promising results (Park and Fung, 2017; Wang, 2018). This can be attributed to the fact that CNNs are well suited to extract local and position-invariant features (Yin et al., 2017). Character-level features have also been shown to be beneficial in tackling the issue of Out-of-Vocabulary (OOV) words (Mishra et al., 2018b), since abusive comments tend to contain obfuscated word"
2020.acl-main.394,N19-1144,0,0.0665069,"Missing"
2020.acl-main.394,S19-2010,0,0.0455156,"Missing"
2020.codi-1.11,P05-1018,0,0.0977503,"e coherence models. 1 Introduction Coherence refers to the properties of a text that indicate how meaningful (sub-)sentential constituents are connected to convey document-level meaning. Different theories have been proposed to describe the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textu"
2020.codi-1.11,J08-1001,0,0.166317,"Missing"
2020.codi-1.11,N04-1015,0,0.129221,"s (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed"
2020.codi-1.11,N10-1099,0,0.0647927,"Missing"
2020.codi-1.11,D16-1245,0,0.0682306,"Missing"
2020.codi-1.11,P18-1198,0,0.351309,"51.6 61.6 86.6 83.3 76.6 86.6 50.0 53.3 53.3 60.0 64.2 71.4 66.0 69.1 LCDbert 75.4 96.7 71.0 94.8 86.6 78.3 86.6 93.3 80.0 76.6 82.8 72.2 Egridcnn 84.6 88.1 53.4 68.8 83.3 71.6 76.6 80.0 53.3 56.6 70.4 65.8 Table 5: PRA performance on the CLA (bottom) and CC datasets (top; ‘fine-tuned’ shows results for models tuned on the respective cloze training sets). Task SubjNum ObjNum CoordInv CorruptAgr MTL 64.9 64.5 58.5 53.2 MTLbert 75.4 72.1 63.4 69.7 STL 62.2 61.1 53.0 57.7 STLbert 71.5 70.7 63.7 68.6 LC 52.7 54.5 53.0 52.2 Models LCDrnnlm 71.2 65.0 56.6 64.2 LCDbert 88.0 86.5 78.4 94.3 Best from Conneau et al. (2018) 95.1 (Seq2Tree) 95.1 (Seq2Tree) 76.2 (NMT En-De) - Human 88.0 86.5 85.0 - Table 6: Classification accuracy on probing tasks. ‘Human’ shows the human upper bound on the task. However, the exception is LCDbert (with PRA 80 on lexical perturbations and 76.6 on corrupt pronoun) suggesting a better ability at capturing semantics and resolving references. Across all six CLA datasets (‘All data’; Table 5), we find that, overall, LCDbert is the top performing model (average PRA). The ‘All data’ row reports the result of comparing a coherent example against its incoherent counterparts across the diffe"
2020.codi-1.11,D18-1465,0,0.0147804,"vantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 2019) and focusing on open-domain models of coherence (Li and Jurafsky, 2017; Xu et al., 2019). However, less attention has been directed to investigating and analyzing the properties of coherence that current models can capture, nor what knowledge is encoded in their representations and how it might relate to aspects of coherence. In this work, we systematically examine what properties of discourse coherence current coherence models can capture. We devise two datasets that exhibit various kinds of incoherence an"
2020.codi-1.11,N19-1423,0,0.0310514,"word embeddings (Pennington et al., 2014) followed by attention to build sentence representations; then builds a second Bi-LSTM with attention to compose a document vector. A linear operation followed by a sigmoid function is applied to the document representation to predict an overall coherence score as the main objective. Inspired by the Egrid approaches, the model is also optimized to predict the grammatical roles of the input words at the bottom layer of the network as an auxiliary task. MTL with BERT embeddings (MTLbert ): We replicate the previous MTL model but now use BERT embeddings (Devlin et al., 2019) to initialize the input words. Single-task learning (STL, Farag and Yannakoudakis, 2019): This model has the same architecture as MTL but only performs the coherence prediction task, excluding the grammatical role auxiliary objective. STL with BERT (STLbert ): This is the same as STL but uses BERT embeddings. Local Coherence Discriminator with Language modeling (LCDrnnlm , Xu et al., 2019): The model generates sentence representations via an RNN language model, where word embeddings are initialized using GloVe. It then generates a representation for two consecutive sentences via concatenating"
2020.codi-1.11,N07-1055,0,0.054098,"e, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created"
2020.codi-1.11,P11-2022,0,0.0260707,"ve been proposed to describe the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether th"
2020.codi-1.11,P19-1060,1,0.790353,"2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 2019) and focusing on open-domain models of coherence (Li"
2020.codi-1.11,C14-1089,0,0.0172828,"1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis,"
2020.codi-1.11,W07-2321,0,0.0630942,"onvey document-level meaning. Different theories have been proposed to describe the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Othe"
2020.codi-1.11,J95-2003,0,0.907072,"Missing"
2020.codi-1.11,P13-1010,0,0.0192766,"e the properties that contribute to discourse coherence and some have been integrated with computational models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner e"
2020.codi-1.11,N19-1419,0,0.0209616,"elate to aspects of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we release our evaluation datasets as a resource for the community to use to test discourse coherence models.1 2 Neural Coherence Models We experiment with a number of existing and stateof-the-art neural approaches to coherence assessment, that have publicly available implementations, and present details of the models below. Across all the BERT-based models, we use bert-large-uncased and layer 16 following Liu et al. (2019) and Hewitt and Manning (2019). Multi-task learning (MTL, Farag and Yannakoudakis, 2019): The model applies a Bi-LSTM on input GloVe word embeddings (Pennington et al., 2014) followed by attention to build sentence representations; then builds a second Bi-LSTM with attention to compose a document vector. A linear operation followed by a sigmoid function is applied to the document representation to predict an overall coherence score as the main objective. Inspired by the Egrid approaches, the model is also optimized to predict the grammatical roles of the input words at the bottom layer of the network as an auxiliary task."
2020.codi-1.11,P18-1052,0,0.0238123,"l., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering th"
2020.codi-1.11,W18-5023,0,0.024545,"representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 2019) and focusing on open-domain models of coherence (Li and Jurafsky, 2017; Xu et al., 2019). However, less attention has been directed to investigating and analyzing the properties of coherence that current models can capture, nor what knowledge is encoded in their representations and how it might relate to aspects of coherence. In this work, we systematically examine what properties of discourse coherence current coherence models can capture. We devise two datasets that exhibit various kinds of incoherence and analyze model ability to capture syntactic and semant"
2020.codi-1.11,D17-1019,0,0.0762258,"tual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 20"
2020.codi-1.11,P11-1100,0,0.0573953,"Missing"
2020.codi-1.11,Q16-1037,0,0.0828633,"Missing"
2020.codi-1.11,D12-1106,0,0.150333,"al models for empirical evaluation. A popular approach is the entity-based model which hypothesizes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence."
2020.codi-1.11,D19-1231,0,0.0404688,"fts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 2019) and focusing on open-domain models of coherence (Li and Jurafsky, 2017; Xu et al., 2019)."
2020.codi-1.11,N16-1098,0,0.0616619,"Missing"
2020.codi-1.11,D14-1162,0,0.0900615,"Missing"
2020.codi-1.11,C14-1090,0,0.0197233,"e assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et a"
2020.codi-1.11,P06-2103,0,0.0821693,"izes that coherence can be assessed in terms of the distribution of and transitions between entities in a text – by constructing an entity-grid (Egrid) representation (Barzilay and Lapata, 2005, 2008), building on Centering Theory (Grosz et al., 1995). Subsequent work has adapted and further extended Egrid representations (Filippova and Strube, 2007; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other research has focused on syntactic patterns that co-occur in text (Louis and Nenkova, 2012) or semantic relatedness between sentences (Lapata and Barzilay, 2005; Soricut and Marcu, 2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguy"
2020.codi-1.11,N19-1112,0,0.0223964,"ons and how it might relate to aspects of coherence. We hope this study shall provide further insight into how to frame the task and improve models of coherence assessment further. Finally, we release our evaluation datasets as a resource for the community to use to test discourse coherence models.1 2 Neural Coherence Models We experiment with a number of existing and stateof-the-art neural approaches to coherence assessment, that have publicly available implementations, and present details of the models below. Across all the BERT-based models, we use bert-large-uncased and layer 16 following Liu et al. (2019) and Hewitt and Manning (2019). Multi-task learning (MTL, Farag and Yannakoudakis, 2019): The model applies a Bi-LSTM on input GloVe word embeddings (Pennington et al., 2014) followed by attention to build sentence representations; then builds a second Bi-LSTM with attention to compose a document vector. A linear operation followed by a sigmoid function is applied to the document representation to predict an overall coherence score as the main objective. Inspired by the Egrid approaches, the model is also optimized to predict the grammatical roles of the input words at the bottom layer of the"
2020.codi-1.11,P17-1121,0,0.261038,"2006; Somasundaran et al., 2014) as key aspects of coherence modeling. There have also been attempts to model coherence by identifying rhetorical relations that connect textual units (Mann and Thompson, 1988; Lin et al., 2011; Feng et al., 2014) or capturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks s"
2020.codi-1.11,P19-1067,0,0.0694207,"pturing topic shifts via Hidden Markov Models (HMM, Barzilay and Lee, 2004). Other work has combined approaches to study whether they are complementary (Elsner et al., 2007; Feng et al., 2014). More recently, neural networks have been used to model coherence. Some models utilize structured representations of text (e.g. Egrid representations, Tien Nguyen and Joty, 2017; Joty et al., 2018) and others operate on unstructured text, taking advantage of neural models’ ability to learn useful representations for the task (Li and Jurafsky, 2017; Logeswaran et al., 2018; Farag and Yannakoudakis, 2019; Xu et al., 2019; Moon et al., 2019). Coherence has typically been assessed by a model’s ability to rank a well-organized document higher than its noisy counterparts created by corrupting sentence order in the original document (binary discrimination task), and neural models have achieved remarkable accuracy on this task. Recent efforts have targeted additional tasks such as recovering the correct sentence order (Logeswaran et al., 2018; Cui et al., 2018), evaluating on realistic data (Lai and Tetreault, 2018; Farag and Yannakoudakis, 2019) and focusing on open-domain models of coherence (Li and Jurafsky, 201"
2020.emnlp-main.680,W19-4412,0,0.0106932,"te an inability to rely on language modelling in low errordensity domains. 5.2 Perplexity ratio Score differences for the R:SPELL error type seem to be driven by a different propensity of spelling errors being of a typographical vs. phonetical nature in the two datasets. versions of an input sentence and then deciding if any of the alternatives are preferable to the original version, based on language model probabilities. The authors use an n-gram language model, which we replace with GPT-2 (Radford et al., 2019) to see how a strong neural language model performs – this approach is similar to Alikaniotis and Raheja (2019). Hyperparameters are tuned for each dataset (see Appendix C for details). Table 7 displays the results on the different datasets. Recall and, in particular, precision is substantially lower on CWEB and AESW compared to other datasets. In general, scores are higher in domains with a higher proportion of errors and those containing edits which result in high perplexity improvements. In these cases systems can rely on a rough heuristic of replacing low probability sequences with high probability ones. However, in CWEB, where errors are fewer and more subtle, this leads to low precision, as perpl"
2020.emnlp-main.680,D19-1435,0,0.0222983,"Missing"
2020.emnlp-main.680,W18-0529,0,0.014767,"OTHER, R:SPELL and R:VERB. These are open class errors, where the error and correction can be quite different. It is therefore reasonable that differences in edits’ degree of semantic change and perplexity improvement across domains are particularly observed in these cases.22 Language Model Importance We also investigate the degree to which systems can rely on a strong internal language model representation when evaluated against different domains. We examine this by looking at the performance of a purely language model based GEC system over the different datasets. We build on the approach of Bryant and Briscoe (2018), using confusion sets to generate alternative 22 0.34 0.51 0.69 Table 8: Examples of false positives on the CWEB dataset that improve perplexity substantially – even more than the average gold edit in CWEB (0.86 perplexity ratio). Table 7: Scores of a language model based GEC system. The lower scores on CWEB and AESW indicate an inability to rely on language modelling in low errordensity domains. 5.2 Perplexity ratio Score differences for the R:SPELL error type seem to be driven by a different propensity of spelling errors being of a typographical vs. phonetical nature in the two datasets. ve"
2020.emnlp-main.680,W19-4406,0,0.288356,"e. We use the jusText5 tool to retrieve the content from HTML pages (removing boilerplate elements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English6 and incomplete sentences. To ensure diversity of the data, we also remove duplicate sentences. Among the million sentences gathered, we select paragraphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that"
2020.emnlp-main.680,P17-1074,0,0.113216,"ng of error corrections, as as there are often many different ways to correct a sentence (Bryant and Ng, 2015). Kappa is 0.39 and 0.44 for sponsored (CWEB-S) and generic website (CWEB-G) data respectively, and Table 3 presents how our agreement results compare to those of existing GEC datasets. The table also includes a number of other statistics, and the different datasets are further analyzed, compared and contrasted in Section 5. 7 8 The texts are tokenized using SpaCy9 and automatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017). Release For each dataset, we release a development and a test set: we propose a roughly equal division of the data into the two splits, which presents a fair amount of errors to evaluate on (see Table 2). To avoid copyright restrictions, we split the collected paragraphs into sentences and shuffle all sentences in order to break the original and coherent structure that would be needed to reproduce the copyrighted material. This approach has successfully been used in previous work for devising web-based corpora (Sch¨afer, 2015; Biemann et al., 2007). The data is available at https: //github.c"
2020.emnlp-main.680,P15-1068,0,0.133747,"minimum number of edits to make the text grammatical. During error annotation, the annotators have access to the entire paragraph in which a sentence belongs, therefore using the context of a sentence to help them in the correction. Examples of erroneous sentences from our data are shown in Table 1. Annotator agreement is calculated at the sentence level using Cohen’s Kappa, i.e. we calculate whether annotators agree on which sentences are erroneous. This approach is preferable to relying on exact matching of error corrections, as as there are often many different ways to correct a sentence (Bryant and Ng, 2015). Kappa is 0.39 and 0.44 for sponsored (CWEB-S) and generic website (CWEB-G) data respectively, and Table 3 presents how our agreement results compare to those of existing GEC datasets. The table also includes a number of other statistics, and the different datasets are further analyzed, compared and contrasted in Section 5. 7 8 The texts are tokenized using SpaCy9 and automatically labeled for error types (and converted into the M2 format) using the ERRor ANnotation Toolkit (ERRANT) (Bryant et al., 2017). Release For each dataset, we release a development and a test set: we propose a roughly"
2020.emnlp-main.680,D16-1195,0,0.0124626,"s gathered, we select paragraphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that when applied to data outside of the language learning domain, current state-of-the-art systems exhibit low precision due to a tendency to over-predict errors. Recent work tackled the domain adaptation problem, and released GEC benchmarks from Wikipedia data and online comments [GMEG Wiki+Yahoo (Napoles et al., 2019)]. However, these datasets p"
2020.emnlp-main.680,W13-1703,0,0.0688895,".77 41.89 37.57 32.26 36.00 14.05 21.34 13.24 23.00 13.88 21.58 17.27 19.97 15.75 20.28 16.91 19.98 33.08 26.97 31.29 8.78 14.29 9.67 18.91 8.94 14.98 5.73 10.80 8.78 15.11 6.15 11.43 PIE system 32.77 44.71 23.11 19.66 30.24 35.58 Table 5: Scores of two SOTA GEC systems on each domain. For both systems performance is substantially lower on CWEB than ESL domains. Scores are calculated against each individual annotator and averaged pre-trained on synthetic errors and fine-tuned on learner data from the train section of FCE (Yannakoudakis et al., 2011), Lang-8 (Mizumoto et al., 2011), and NUCLE (Dahlmeier et al., 2013) and for GEC-PSEUDODATA additionally on the W&I train split (Bryant et al., 2019). Performance is evaluated using the F0.5 metric calculated by ERRANT (Bryant et al., 2017).18 However, the more annotators a dataset has, the higher score a system will get on this data (Bryant and Ng, 2015). In order to perform a fair comparison of systems across datasets with a different number of annotators, we calculate the ERRANT score against each individual annotator and then take the average to get the final score. Evaluation results are presented in Table 5. Across all datasets, we observe lower scores w"
2020.emnlp-main.680,W16-0506,0,0.109906,"ements and splitting the content into paragraphs). We heavily filter the data by removing paragraphs which contain non-English6 and incomplete sentences. To ensure diversity of the data, we also remove duplicate sentences. Among the million sentences gathered, we select paragraphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that when applied to data outside of the language learning domain, current state-of-the-art systems exhi"
2020.emnlp-main.680,N19-1423,0,0.00755079,"mprovements than datasets from more ad∆S ∆P 0.3 4 3 0.2 2 0.1 1 0 :P R: U N C OT T H E M R :D U :P ET U N R: CT SP EL R: L PR R: EP O RT R: H V ER B 0 ∆ Semantic Similarity (∆S) ∆ Perplexity Ratio (∆P ) ·10−2 M We limit our analysis to sentences containing exactly one edit, as we are interested in how individual edits change a sentence, regardless of how domains differ in amounts of erroneous sentences and in the number of edits per sentence (Table 3). Regarding 1), to measure the semantic change of a sentence after an edit is introduced, we use sentence embeddings generated by Sentence-BERT (Devlin et al., 2019) and calculate the cosine similarity between the original sentence and its corrected counterpart. Regarding 2), the degree of sentence improvement is calculated as the ratio of the perplexity of GPT-2 (Radford et al., 2019) on a sentence after and before it has been edited. Figure 4: Difference in semantic similarity and perplexity ratio between CWEB-S and FCE for the most frequent error types (M: missing; R: replace; U: unnecessary). vanced speakers. CWEB and AESW in particular stand out, with edits that largely retain the semantics of a sentence and that result in more subtle improvements. E"
2020.emnlp-main.680,D19-1119,0,0.063297,"sets). In particular, precision is improved (+20.8/+18.6 on CWEB-G/S) at the expense of recall (−6.4/−2.8 on CWEB-G/S). However, performance is still low compared to the language learning domain (F0.5 of at least 41), further indicating that there is scope for developing more robust and general-purpose, open-domain GEC systems. For the purpose of future benchmarking, Appendix B lists the system's ERRANT scores based on both annotators – as opposed to the average of individual annotator scores reported in Table 6. 19 www.github.com/chrisjbryant/errant 8471 We use the fine-tuning parameters of Kiyono et al. (2019). 5 Analysis FCE (PSEUDO) Wiki (PSEUDO) CWEB-G (PSEUDO) In order to assess the impact our new dataset can have on the GEC field, we carry out analyses to show 1) to what degree the domain of our data is different from existing GEC corpora, and how existing GEC systems are affected by the domain shift; and 2) that a factor behind the performance drop on CWEB data is the inability of systems to rely on a strong internal language model in low error density domains. 5.1 FCE (PIE) Wiki (PIE) CWEB-G (PIE) precision 60 40 20 Domain Shift Moving from error correction in learner texts to error correcti"
2020.emnlp-main.680,I11-1017,0,0.0219533,"13 35.94 47.09 52.81 34.13 23.02 43.77 41.89 37.57 32.26 36.00 14.05 21.34 13.24 23.00 13.88 21.58 17.27 19.97 15.75 20.28 16.91 19.98 33.08 26.97 31.29 8.78 14.29 9.67 18.91 8.94 14.98 5.73 10.80 8.78 15.11 6.15 11.43 PIE system 32.77 44.71 23.11 19.66 30.24 35.58 Table 5: Scores of two SOTA GEC systems on each domain. For both systems performance is substantially lower on CWEB than ESL domains. Scores are calculated against each individual annotator and averaged pre-trained on synthetic errors and fine-tuned on learner data from the train section of FCE (Yannakoudakis et al., 2011), Lang-8 (Mizumoto et al., 2011), and NUCLE (Dahlmeier et al., 2013) and for GEC-PSEUDODATA additionally on the W&I train split (Bryant et al., 2019). Performance is evaluated using the F0.5 metric calculated by ERRANT (Bryant et al., 2017).18 However, the more annotators a dataset has, the higher score a system will get on this data (Bryant and Ng, 2015). In order to perform a fair comparison of systems across datasets with a different number of annotators, we calculate the ERRANT score against each individual annotator and then take the average to get the final score. Evaluation results are presented in Table 5. Across all"
2020.emnlp-main.680,D19-5504,0,0.0150608,"graphs randomly. We split the data with respect to where they 4 https://commoncrawl.org/ https://github.com/miso-belica/ jusText 6 Using the langdetect package. 5 3 Total Test CWEB-S CWEB-G Total 2014)], student essays [W&I+LOCNESS (Bryant et al., 2019; Granger, 1998)] or target a specific domain [scientific writing; AESW (Daudaravicius et al., 2016)]. Supervised systems trained on specific domains are less likely to be as effective at correcting distinctive errors from other domains, as is the case for systems trained on learner data with different native languages (Chollampatt et al., 2016; Nadejde and Tetreault, 2019). The recent BEA 2019 shared task (Bryant et al., 2019) encouraged research in the use of low-resource and unsupervised approaches; however, evaluation primarily targeted the restricted domain of student essays. We show that when applied to data outside of the language learning domain, current state-of-the-art systems exhibit low precision due to a tendency to over-predict errors. Recent work tackled the domain adaptation problem, and released GEC benchmarks from Wikipedia data and online comments [GMEG Wiki+Yahoo (Napoles et al., 2019)]. However, these datasets present a high density of error"
2020.emnlp-main.680,Q19-1032,0,0.0408577,"Missing"
2020.emnlp-main.680,E17-2037,0,0.017618,"into sentences and shuffle all sentences in order to break the original and coherent structure that would be needed to reproduce the copyrighted material. This approach has successfully been used in previous work for devising web-based corpora (Sch¨afer, 2015; Biemann et al., 2007). The data is available at https: //github.com/SimonHFL/CWEB. 3 GEC Corpora We compare our data with existing GEC corpora which cover a range of domains and proficiency levels. Table 3 presents a number of different statistics and Table 4 their error-type frequencies.10 3.1 English as a second language (ESL) JFLEG (Napoles et al., 2017) The JHU FluencyExtended GUG corpus consists of sentences written by English language learners (with different proficiency levels and L1s) for the TOEFL® exam, 9 top-level domains: .gov, .edu, .mil, .int, and .museum. top-level domains: .com, .info, .net, .org. 10 8469 https://spacy.io/ See links to downloadable versions in Appendix A W&I JFLEG FCE 2.1 CoNLL14 A P UNCT V ERB OTHER D ET N OUN P REP S PELL A LL 147.7 233.5 295.6 180.7 167.7 107.1 242.5 112.3 176.7 138.3 149.1 105.4 113.8 107.8 1675.6 1084.9 65.5 200.5 158.1 134.9 116.8 92.7 26.0 244.8 300.0 237.3 159.1 139.8 137.2 79.3 LOCNESS B"
2020.emnlp-main.680,W14-1701,0,0.0302274,"1050.7 504.1 400.6 732.3 635.3 239.2 G S 48.9 23.4 31.6 20.9 19.6 15.6 3.8 48.7 13.1 21.0 19.7 12.8 9.8 2.4 208.9 147.2 Table 4: Number of error occurrences for the most frequent error types (per 10, 000 token). covering a range of topics. Texts have been corrected for grammatical errors and fluency. FCE (Yannakoudakis et al., 2011) consists of 1, 244 error corrected texts produced by learners taking the First Certificate in English exam, which assesses English at an upper-intermediate level. We use the data split made available for the BEA GEC shared task 2019 (Bryant et al., 2019). CoNLL14 (Ng et al., 2014) consists of (mostly argumentative) essays written by ESL learners from the National University of Singapore, which are annotated for grammatical errors by two native speakers of English. Write&Improve (W&I) (Bryant et al., 2019) Cambridge English Write & Improve (Yannakoudakis et al., 2018) is an online web platform that automatically provides diagnostic feedback to non-native English-language learners, including an overall language proficiency score based on the Common European Framework of Reference for Languages (CEFR).11 The W&I corpus contains 3, 600 texts across 3 different CEFR levels"
2020.emnlp-main.680,W17-5019,0,0.0334002,"Missing"
2020.emnlp-main.680,P11-1019,1,0.767761,"rse range of writing and constitute a major part of what people read and write on an everyday basis. This work highlights two major prevailing challenges of current approaches to GEC: domain adaptation and low precision in texts with low error density. Previous work has primarily targeted essaystyle text with high error density (see Figure 1); however, this lack of diversity means that it is not clear how systems perform on other domains and under different error distributions (Sakaguchi et al., 2017).2 Current publicly available datasets are restricted to non-native English essays [e.g. FCE (Yannakoudakis et al., 2011); CoNLL14 (Ng et al., 2 Leacock et al. (2010) highlighted the variations in the distribution of errors in non-native and native English writings. 8467 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 8467–8478, c November 16–20, 2020. 2020 Association for Computational Linguistics Error type Example sentence V ERB :S VA They develop positive relationships with swimmers and members, and promotes promote programs in order to generate more participation. In a small agriculture agricultural town on the east side of Washington state State called Yakima."
2020.findings-emnlp.405,D15-1075,0,\N,Missing
2020.findings-emnlp.405,W02-1006,0,\N,Missing
2020.findings-emnlp.405,H94-1046,0,\N,Missing
2020.findings-emnlp.405,P10-4014,0,\N,Missing
2020.findings-emnlp.405,Q14-1019,0,\N,Missing
2020.findings-emnlp.405,P07-1005,0,\N,Missing
2020.findings-emnlp.405,P12-1029,0,\N,Missing
2020.findings-emnlp.405,J14-1003,0,\N,Missing
2020.findings-emnlp.405,D14-1162,0,\N,Missing
2020.findings-emnlp.405,N15-1035,0,\N,Missing
2020.findings-emnlp.405,K16-1006,0,\N,Missing
2020.findings-emnlp.405,P16-1085,0,\N,Missing
2020.findings-emnlp.405,C16-1130,0,\N,Missing
2020.findings-emnlp.405,E17-1010,0,\N,Missing
2020.findings-emnlp.405,D17-1120,0,\N,Missing
2020.findings-emnlp.405,P19-1568,0,\N,Missing
2020.findings-emnlp.405,P19-1589,0,\N,Missing
2020.findings-emnlp.405,N19-1423,0,\N,Missing
2020.findings-emnlp.405,D19-1533,0,\N,Missing
2020.findings-emnlp.405,D19-1112,0,\N,Missing
2020.findings-emnlp.405,W19-4326,0,\N,Missing
2020.findings-emnlp.405,D19-1045,0,\N,Missing
2020.findings-emnlp.405,D19-1403,0,\N,Missing
2020.findings-emnlp.405,D19-1431,0,\N,Missing
2020.nlp4call-1.2,W19-4406,0,0.0526347,"Missing"
2020.nlp4call-1.2,P17-1074,0,0.0328029,"Missing"
2020.nlp4call-1.2,P15-1068,0,0.028085,"each participant, we also provide grammatically corrected versions of the student turns. The teachers make errors too, which is interesting in itself, but the focus of teaching is on the students and therefore we economise effort by correcting student turns only. The process includes grammatical errors, typos, and improvements to lexical choice. This was done in a minimal fashion to stay as close to the original meaning as possible. In addition, there can often be many possible corrections for any one grammatical error, a known problem in corpus annotation and NLP work on grammatical errors (Bryant and Ng, 2015). The usual solution is to collect multiple annotations, which we have not yet done, but plan to. In the meantime, the error annotation is useful for grammatical error detection even if correction might be improved by more annotation. Sequence type: We indicate major and minor shifts in conversational sequences – sections of interaction with a particular purpose, even if that purpose is from time-to-time more social than it is educational. Borrowing key concepts from the CONVERSATION ANALYSIS (CA) approach (Sacks et al., 1974), we seek out groups of turns which together represent the building"
2020.nlp4call-1.2,W17-5547,0,0.0551408,"Missing"
2020.nlp4call-1.2,D18-1547,0,0.0565982,"Missing"
2020.nlp4call-1.2,N10-1020,0,0.0541221,"nd discourse functions (Csomay, 2012), the interaction of roles and goal-driven behaviour in academic discourse (Evison, 2013), and knowledge development at different stages of higher education learning (Atwood et al., 2010). On a larger scale, corpora such as the MultiDomain Wizard-of-Oz datasets (MultiWOZ) contain thousands of goal-directed dialogue collected through crowdsourcing and intended for the training of automated dialogue systems (Budzianowski et al., 2018; Eric et al., 2020). Other work has involved the collation of pre-existing conversations on the web, for example from Twitter (Ritter et al., 2010), Reddit (Schrading et al., 2015), and movie scripts (Danescu-Niculescu-Mizil and Lee, 2011). Such datasets are useful for training dialogue systems to respond to written inputs – so-called ‘chatbots’ – which in recent years have greatly improved in terms of presenting some kind of personality, empathy and world knowledge (Roller et al., 2020), where previously there had been relatively little of all three. The improvement in chatbots has caught the attention of, and in turn has been driven by, the technology industry, for they have clear commercial applications in customer service scenarios s"
2020.nlp4call-1.2,W03-0205,0,0.0609884,"Missing"
2020.nlp4call-1.2,W11-0609,0,0.0255946,"riven behaviour in academic discourse (Evison, 2013), and knowledge development at different stages of higher education learning (Atwood et al., 2010). On a larger scale, corpora such as the MultiDomain Wizard-of-Oz datasets (MultiWOZ) contain thousands of goal-directed dialogue collected through crowdsourcing and intended for the training of automated dialogue systems (Budzianowski et al., 2018; Eric et al., 2020). Other work has involved the collation of pre-existing conversations on the web, for example from Twitter (Ritter et al., 2010), Reddit (Schrading et al., 2015), and movie scripts (Danescu-Niculescu-Mizil and Lee, 2011). Such datasets are useful for training dialogue systems to respond to written inputs – so-called ‘chatbots’ – which in recent years have greatly improved in terms of presenting some kind of personality, empathy and world knowledge (Roller et al., 2020), where previously there had been relatively little of all three. The improvement in chatbots has caught the attention of, and in turn has been driven by, the technology industry, for they have clear commercial applications in customer service scenarios such as helplines and booking systems. 2 Corpus design We set out a design for the TSCC which"
2020.nlp4call-1.2,2020.bea-1.5,0,0.0361681,"Missing"
2020.nlp4call-1.2,P11-1019,1,0.81033,"Missing"
2021.acl-long.210,N19-1050,1,0.83103,"the first dataset of 6000 English language Reddit comments that has finegrained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive) – normative offensiveness ratings for the comments. For the first time, we use comparative annotations to detect offensive language. In its simplest form, comparative annotations involve giving the annotators two instances at a time, and asking which exhibits the property of interest to a greater extent. This alleviates several annotation biases present in standard rating scales, such as scale-region bias (Presser and Schuman, 1996; Asaadi et al., 2019), and improves annotation consistency (Kiritchenko and Mohammad, 2017). However, instead of needing to annotate N instances, one now needs to annotate N 2 instance pairs—which can be prohibitive. Thus, we annotate our dataset using an efficient form of comparative annotation called Best–Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). Surveys by Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Vidgen and Derczynski (2020) discuss various existing datasets and their compositions in detail. Waseem and Hovy (2016); David"
2021.acl-long.210,S19-2007,0,0.0334653,"s), the reduced-range dataset. We discuss the models’ performance on this dataset in the next section. HateBERT HateBERT (Caselli et al., 2020b) is a version of BERT pretrained for abusive language detection in English. HateBERT was trained on RAL-E, a large dataset of English language Reddit comments from communities banned for being offensive or hateful. HateBERT has been shown to outperform the general purpose BERT model on the offensive language detection task when finetuned on popular datasets such as OffensEval 2019 (Zampieri et al., 2019), AbusEval (Caselli et al., 2020a), and HatEval (Basile et al., 2019). We fine-tuned HateBERT on Ruddit and its variants. The experimental setup for this model is the same as that described for the BERT model. 6.2 6 Computational Modeling In this section, we present benchmark experiments on Ruddit and its variants by implementing some commonly used model architectures. The task of the models was to predict the offensiveness score of a given comment. We performed 5-fold crossvalidation for each of the models.6 6.1 Models Bidirectional LSTM We fed pre-trained 300 dimensional GloVe word embeddings (Pennington et al., 2014) to a 2-layered BiLSTM to obtain a sentenc"
2021.acl-long.210,S12-1047,1,0.755705,"annotation results in inequalities for 5 of the 6 item pairs. For example, a 4-tuple with items A, B, C, and D, where A is the best, and D is the worst, results in inequalities: A&gt;B, A&gt;C, A&gt;D, B&gt;D, and C&gt;D. Real-valued scores of associations are calculated between the items and the property of interest from the best–worst annotations for a set of 4-tuples (Orme, 2009; Flynn and Marley, 2014). The scores can be used to rank items by the degree of association with the property of interest. Within the NLP community, BWS has thus far been used only for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018). Using BWS, we create the first dataset with degree of offensiveness scores for social media comments. 3 Data collection and sampling We extracted Reddit data from the Pushshift repository (Baumgartner et al., 2020) using Google BigQuery. Reddit is a social news aggregation, web content rating, and discussion website. It contains forums calle"
2021.acl-long.210,P17-2074,1,0.933368,"that has finegrained, real-valued scores between -1 (maximally supportive) and 1 (maximally offensive) – normative offensiveness ratings for the comments. For the first time, we use comparative annotations to detect offensive language. In its simplest form, comparative annotations involve giving the annotators two instances at a time, and asking which exhibits the property of interest to a greater extent. This alleviates several annotation biases present in standard rating scales, such as scale-region bias (Presser and Schuman, 1996; Asaadi et al., 2019), and improves annotation consistency (Kiritchenko and Mohammad, 2017). However, instead of needing to annotate N instances, one now needs to annotate N 2 instance pairs—which can be prohibitive. Thus, we annotate our dataset using an efficient form of comparative annotation called Best–Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). Surveys by Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Vidgen and Derczynski (2020) discuss various existing datasets and their compositions in detail. Waseem and Hovy (2016); Davidson et al. (2017); Founta et al. (2018) created datasets based on Twit"
2021.acl-long.210,N16-1095,1,0.888787,"Missing"
2021.acl-long.210,2020.alw-1.17,0,0.0310683,"ffensive nor hate-speech and Founta et al. (2018) as abusive, hateful, normal, spam. Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Kiritchenko and Nejadgholi (2020) summarize the different definitions. However, these categories have significant overlaps with each other, creating ill-defined boundaries, thus introducing ambiguity and annotation inconsistency (Founta et al., 2018). A further challenge is that after encountering several highly offensive comments, an annotator might find subsequent moderately offensive comments to not be offensive (de-sensitization) (Kurrek et al., 2020; Soral et al., 2018). At the same time, existing approaches do not take into account that comments can be offensive to a different degree. Knowing the degree of offensiveness of a comment has practical implications, when taking action against inappropriate behaviour online, as it allows for a more fine-grained analysis and prioritization in moderation. The representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that i"
2021.acl-long.210,2020.alw-1.9,0,0.0473109,"Missing"
2021.acl-long.210,P18-1017,1,0.850033,"Emotions are highly representative of one’s mental state, which in turn are associated with their behaviour (Poria et al., 2019). For example, Jay and Janschewitz (2008) show that people tend to swear when they are angry, frustrated or anxious. Studies have shown that the primary dimensions of emotion are valence, arousal, and dominance (VAD) (Osgood et al., 1957; Russell, 1980, 2702 2003). Valence is the positive–negative or pleasure– displeasure dimension. Arousal is the excited– calm or active–passive dimension. Dominance is powerful–weak or ‘have full control’–‘have no control’ dimension (Mohammad, 2018). To boost the representation of offensive and emotional comments in our dataset, we up-sampled comments that included low-valence (highly negative) words and those that included high-arousal words (as per the NRC VAD lexicon (Mohammad, 2018)). The manually constructed NRC VAD lexicon includes 20,000 English words, each with a real-valued score between 0 and 1 in the V, A, D dimensions. In order to do this upsampling, we first defined the valence score of each comment as the average valence score of the negative words within the comment (A negative word is defined as a word with a valence scor"
2021.acl-long.210,S17-1007,1,0.938177,"negative consequences for the victim’s mental health (Munro, 2011). Automated offensive language detection has thus been gaining interest in the NLP community, as a promising direction to better understand the nature and spread of such content. There are several challenges in the automatic detection of offensive language (Wiedemann et al., 2018). The NLP community has adopted various definitions for offensive language, classifying it into specific categories. For example, Waseem and ∗ Both authors contributed equally. Hovy (2016) classified comments as racist, sexist, neither; Davidson et al. (2017) as hate-speech, offensive but not hate-speech, neither offensive nor hate-speech and Founta et al. (2018) as abusive, hateful, normal, spam. Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Kiritchenko and Nejadgholi (2020) summarize the different definitions. However, these categories have significant overlaps with each other, creating ill-defined boundaries, thus introducing ambiguity and annotation inconsistency (Founta et al., 2018). A further challenge is that after encountering several highly offensive comments, an annotator might find subsequent moderately of"
2021.acl-long.210,L18-1030,1,0.849767,"calculated between the items and the property of interest from the best–worst annotations for a set of 4-tuples (Orme, 2009; Flynn and Marley, 2014). The scores can be used to rank items by the degree of association with the property of interest. Within the NLP community, BWS has thus far been used only for creating datasets for relational similarity (Jurgens et al., 2012), word-sense disambiguation (Jurgens, 2013), word–sentiment intensity (Kiritchenko et al., 2014), phrase sentiment composition (Kiritchenko and Mohammad, 2016), and tweet-emotion intensity (Mohammad and Bravo-Marquez, 2017; Mohammad and Kiritchenko, 2018). Using BWS, we create the first dataset with degree of offensiveness scores for social media comments. 3 Data collection and sampling We extracted Reddit data from the Pushshift repository (Baumgartner et al., 2020) using Google BigQuery. Reddit is a social news aggregation, web content rating, and discussion website. It contains forums called subreddits dedicated to specific topics. Users can make a post on the subreddit to start a discussion. Users can comment on existing posts or comments to participate in the discussion. As users can also reply to a comment, the entire discussion has a hi"
2021.acl-long.210,D18-1302,0,0.0188152,"gen and Derczynski (2020) discuss various existing datasets and their compositions in detail. Waseem and Hovy (2016); Davidson et al. (2017); Founta et al. (2018) created datasets based on Twitter data. Due to prevalence of the non-offensive class in naturallyoccurring data (Waseem, 2016; Founta et al., 2018), the authors devised techniques to boost the presence of the offensive class in the dataset. Waseem and Hovy (2016) used terms frequently occurring in offensive tweets, while Davidson et al. (2017) used a list of hate-related terms to extract offensive tweets from the Twitter search API. Park et al. (2018), Wiegand et al. (2019), and Davidson et al. (2019) show that the Waseem and Hovy (2016) dataset exhibits topic bias and author bias due to the employed sampling strategy. Founta et al. (2018) boosted the representation of offensive class in their dataset by analysing the sentiment of the tweets and checking for the presence of offensive terms. In our work, we employ a hybrid approach, selecting our data in three ways: specific topics, emotion-related key-words, and random sampling. Past work has partitioned offensive comments into explicitly offensive (those that include profanity—swear words"
2021.acl-long.210,D14-1162,0,0.0864822,"busEval (Caselli et al., 2020a), and HatEval (Basile et al., 2019). We fine-tuned HateBERT on Ruddit and its variants. The experimental setup for this model is the same as that described for the BERT model. 6.2 6 Computational Modeling In this section, we present benchmark experiments on Ruddit and its variants by implementing some commonly used model architectures. The task of the models was to predict the offensiveness score of a given comment. We performed 5-fold crossvalidation for each of the models.6 6.1 Models Bidirectional LSTM We fed pre-trained 300 dimensional GloVe word embeddings (Pennington et al., 2014) to a 2-layered BiLSTM to obtain a sentence representation (using a concatenation of the last hidden state from the forward and backward direction). This sentence representation was then passed to a linear layer with a tanh activation to produce a score between −1 and 1. We used Mean Squared Error (MSE) loss as the objective function, Adam with 0.001 learning rate as the optimizer, hidden dimension of 256, batch size of 32, and a dropout of 0.5. The model was trained for 7 epochs. BERT We fine-tuned BERTbase (Devlin et al., 2019). We added a regression head containing a linear layer to the pre"
2021.acl-long.210,P19-1163,0,0.049151,"Missing"
2021.acl-long.210,2020.acl-main.486,0,0.12141,"lysing the sentiment of the tweets and checking for the presence of offensive terms. In our work, we employ a hybrid approach, selecting our data in three ways: specific topics, emotion-related key-words, and random sampling. Past work has partitioned offensive comments into explicitly offensive (those that include profanity—swear words, taboo words, or hate terms) and implicitly offensive (those that do not include profanity) (Waseem et al., 2017; Caselli et al., 2020a; Wiegand et al., 2021). Some other past work has defined explicitly and implicitly offensive instances a little differently: Sap et al. (2020) considered factors such as obviousness, intent to offend and biased implications, Breitfeller et al. (2019) considered factors such as the context and the person annotating the instance, and Razo and K¨ubler (2020) considered the kind of lexicon used. Regardless of the exact definition, implicit offensive language, due to a lack of lexical cues, is harder to classify not only for computational models, but also for humans. In our work, we consider implicitly offensive comments as those offensive comments that do not contain any swear words. By eliminating different offensiveness categories, tr"
2021.acl-long.210,W17-1101,0,0.177044,"community, as a promising direction to better understand the nature and spread of such content. There are several challenges in the automatic detection of offensive language (Wiedemann et al., 2018). The NLP community has adopted various definitions for offensive language, classifying it into specific categories. For example, Waseem and ∗ Both authors contributed equally. Hovy (2016) classified comments as racist, sexist, neither; Davidson et al. (2017) as hate-speech, offensive but not hate-speech, neither offensive nor hate-speech and Founta et al. (2018) as abusive, hateful, normal, spam. Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Kiritchenko and Nejadgholi (2020) summarize the different definitions. However, these categories have significant overlaps with each other, creating ill-defined boundaries, thus introducing ambiguity and annotation inconsistency (Founta et al., 2018). A further challenge is that after encountering several highly offensive comments, an annotator might find subsequent moderately offensive comments to not be offensive (de-sensitization) (Kurrek et al., 2020; Soral et al., 2018). At the same time, existing approaches do not take into account that c"
2021.acl-long.210,W19-3509,0,0.054317,"Missing"
2021.acl-long.210,S19-2010,0,0.0352511,"ents with scores from −0.5 to 0.5. We call this subset (of 5151 comments), the reduced-range dataset. We discuss the models’ performance on this dataset in the next section. HateBERT HateBERT (Caselli et al., 2020b) is a version of BERT pretrained for abusive language detection in English. HateBERT was trained on RAL-E, a large dataset of English language Reddit comments from communities banned for being offensive or hateful. HateBERT has been shown to outperform the general purpose BERT model on the offensive language detection task when finetuned on popular datasets such as OffensEval 2019 (Zampieri et al., 2019), AbusEval (Caselli et al., 2020a), and HatEval (Basile et al., 2019). We fine-tuned HateBERT on Ruddit and its variants. The experimental setup for this model is the same as that described for the BERT model. 6.2 6 Computational Modeling In this section, we present benchmark experiments on Ruddit and its variants by implementing some commonly used model architectures. The task of the models was to predict the offensiveness score of a given comment. We performed 5-fold crossvalidation for each of the models.6 6.1 Models Bidirectional LSTM We fed pre-trained 300 dimensional GloVe word embedding"
2021.acl-long.210,W16-5618,0,0.237183,"deration. The representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that is unambiguous in its potential to be offensive, such as those using slurs or swear words (Waseem et al., 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al., 2019). Further, Wiegand et al. (2019) show that key-word based sampling often results in spurious correlations (e.g., sports-related expressions such as announcer and sport occur very frequently in offensive tweets). 2700 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2700–2717 August 1–6, 2021. ©2021 Association for Computational Linguistics Lastly, existing datasets consider offensive comments in isolation from the wider conversation of which they are a"
2021.acl-long.210,W17-3012,0,0.0837158,"comments can be offensive to a different degree. Knowing the degree of offensiveness of a comment has practical implications, when taking action against inappropriate behaviour online, as it allows for a more fine-grained analysis and prioritization in moderation. The representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that is unambiguous in its potential to be offensive, such as those using slurs or swear words (Waseem et al., 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al., 2019). Further, Wiegand et al. (2019) show that key-word based sampling often results in spurious correlations (e.g., sports-related expressions such as announcer and sport occur very frequently in offensive tweets). 2700 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th Internation"
2021.acl-long.210,N16-2013,0,0.0362638,"n, 1996; Asaadi et al., 2019), and improves annotation consistency (Kiritchenko and Mohammad, 2017). However, instead of needing to annotate N instances, one now needs to annotate N 2 instance pairs—which can be prohibitive. Thus, we annotate our dataset using an efficient form of comparative annotation called Best–Worst Scaling (BWS) (Louviere, 1991; Louviere et al., 2015; Kiritchenko and Mohammad, 2016, 2017). Surveys by Schmidt and Wiegand (2017); Fortuna and Nunes (2018); Mishra et al. (2019); Vidgen and Derczynski (2020) discuss various existing datasets and their compositions in detail. Waseem and Hovy (2016); Davidson et al. (2017); Founta et al. (2018) created datasets based on Twitter data. Due to prevalence of the non-offensive class in naturallyoccurring data (Waseem, 2016; Founta et al., 2018), the authors devised techniques to boost the presence of the offensive class in the dataset. Waseem and Hovy (2016) used terms frequently occurring in offensive tweets, while Davidson et al. (2017) used a list of hate-related terms to extract offensive tweets from the Twitter search API. Park et al. (2018), Wiegand et al. (2019), and Davidson et al. (2019) show that the Waseem and Hovy (2016) dataset e"
2021.acl-long.210,2021.eacl-main.27,0,0.372393,"and prioritization in moderation. The representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that is unambiguous in its potential to be offensive, such as those using slurs or swear words (Waseem et al., 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al., 2019). Further, Wiegand et al. (2019) show that key-word based sampling often results in spurious correlations (e.g., sports-related expressions such as announcer and sport occur very frequently in offensive tweets). 2700 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2700–2717 August 1–6, 2021. ©2021 Association for Computational Linguistics Lastly, existing datasets consider offensive comments in isolation from the wider conversation of w"
2021.acl-long.210,N19-1060,0,0.22765,"representation of the offensive class in a dataset is often boosted using different strategies. The most common strategy used is key-word based sampling. This results in datasets that are rich in explicit offensive language (language that is unambiguous in its potential to be offensive, such as those using slurs or swear words (Waseem et al., 2017)) but lack cases of implicit offensive language (language with its true offensive nature obscured due to lack of unambiguous swear words, usage of sarcasm or offensive analogies, and others (Waseem et al., 2017; Wiegand et al., 2021)) (Waseem, 2016; Wiegand et al., 2019). Further, Wiegand et al. (2019) show that key-word based sampling often results in spurious correlations (e.g., sports-related expressions such as announcer and sport occur very frequently in offensive tweets). 2700 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2700–2717 August 1–6, 2021. ©2021 Association for Computational Linguistics Lastly, existing datasets consider offensive comments in isolation from the wider conversation of which they are a part. Offensive langua"
2021.eacl-main.168,Q19-1038,0,0.13224,"in valid forever. With the aim of extending the global reach of Natural Language Processing (NLP) technology, much recent research has focused on the development of multilingual models and methods to efficiently transfer knowledge across languages. 1 https://www.ethnologue.com/statistics Among these advances are multilingual word vectors which aim to give word-translation pairs a similar encoding in some embedding space (Mikolov et al., 2013a; Lample et al., 2017). There has also been a lot of work on multilingual sentence and word encoders that either explicitly utilizes corpora of bi-texts (Artetxe and Schwenk, 2019; Lample and Conneau, 2019) or jointly trains language models for many languages in one encoder (Devlin et al., 2018; Conneau et al., 2019). Although great progress has been made in cross-lingual transfer learning, these methods either do not close the gap with performance in a single high-resource language (Artetxe and Schwenk, 2019; Conneau et al., 2019), e.g., because of cultural differences in languages which are not accounted for, or are impractically expensive (Lai et al., 2019). Meta-learning, or learning to learn (Schmidhuber, 1987; Bengio et al., 1990; Thrun and Pratt, 1998), is a lea"
2021.eacl-main.168,T75-2034,0,0.578202,"Missing"
2021.eacl-main.168,D19-1431,0,0.0281442,"setting and show superior performance for parameter initialization over selfsupervised pretraining and multi-task learning. Their method is an adaptation of MAML where a combination of a text-encoder, BERT (Devlin et al., 2018), is coupled with a parameter generator that learns to generate task-dependent initializations of the classification head such that metalearning can be performed across tasks with disjoint label spaces. Obamuyide and Vlachos (2019b) apply meta-learning on the task of relation extraction; Obamuyide and Vlachos (2019a) apply lifelong meta-learning for relation extraction; Chen et al. (2019) apply meta-learning for few-shot learning on missing link prediction in knowledge graphs. Multilingual Gu et al. (2018) apply metalearning to Neural Machine Translation (NMT) and show its advantage over strong baselines such as cross-lingual transfer learning. By viewing each language pair as a task, the authors apply MAML to obtain competitive NMT systems with as little as 600 parallel sentences. To our best knowledge, the only application of meta-learning for cross-lingual few-shot learning is the one by Nooralahzadeh et al. (2020). The authors study the application of X-MAML, a MAML-based"
2021.eacl-main.168,P19-4007,0,0.0480021,"Missing"
2021.eacl-main.168,D18-1269,0,0.0217194,"rediction in knowledge graphs. Multilingual Gu et al. (2018) apply metalearning to Neural Machine Translation (NMT) and show its advantage over strong baselines such as cross-lingual transfer learning. By viewing each language pair as a task, the authors apply MAML to obtain competitive NMT systems with as little as 600 parallel sentences. To our best knowledge, the only application of meta-learning for cross-lingual few-shot learning is the one by Nooralahzadeh et al. (2020). The authors study the application of X-MAML, a MAML-based variant, to crosslingual Natural Language Inference (XNLI) (Conneau et al., 2018) and Multilingual Question An1969 swering (MLQA) (Lewis et al., 2019) in both a cross-domain and cross-language setting. XMAML works by pretraining some model M on a high-resource task h to obtain initial model parameters θmono . Consecutively, a set L of one or more auxiliary languages is taken, and MAML is applied to achieve fast adaptation of θmono for l ∈ L. In their experiments, the authors use either one or two auxiliary languages and evaluate their method in both a zero- and few-shot setting. It should be noted that, in the few-shot setting, the full development set (2.5k instances) is"
2021.eacl-main.168,D19-1572,0,0.040088,"Missing"
2021.eacl-main.168,2020.emnlp-main.368,0,0.0612069,"ize to unseen tasks at test time. Meta-learning has recently emerged as a promising technique for few-shot learning for a wide array of tasks (Finn et al., 2017; Koch et al., 2015; Ravi and Larochelle, 2017) including NLP (Dou et al., 2019; Gu et al., 2018). To our best knowledge, no previous work has been done in investigating meta-learning as a framework for multilingual and cross-lingual few-shot learning. We propose such a framework and demonstrate its effectiveness in document classification tasks. The only current study on meta-learning for cross-lingual few-shot learning is the one by (Nooralahzadeh et al., 2020), focusing on natural language inference and multilingual question answering. In their work, the authors focus on applying meta-learning to learn to adapt a monolingually trained classi1966 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 1966–1976 April 19 - 23, 2021. ©2021 Association for Computational Linguistics Algorithm 1 Meta-training procedure. Require: p(D): distribution over tasks. Require: α, β: step size hyper-parameters Initialize θ while not done do Sample batch of tasks {Dl } = {(S l , Ql )} ∼ p(D) for all (S l ,"
2021.eacl-main.168,D18-1398,0,0.118082,"pensive (Lai et al., 2019). Meta-learning, or learning to learn (Schmidhuber, 1987; Bengio et al., 1990; Thrun and Pratt, 1998), is a learning paradigm which focuses on the quick adaption of a learner to new tasks. The idea is that by training a learner to adapt quickly and from a few examples on a diverse set of training tasks, the learner can also generalize to unseen tasks at test time. Meta-learning has recently emerged as a promising technique for few-shot learning for a wide array of tasks (Finn et al., 2017; Koch et al., 2015; Ravi and Larochelle, 2017) including NLP (Dou et al., 2019; Gu et al., 2018). To our best knowledge, no previous work has been done in investigating meta-learning as a framework for multilingual and cross-lingual few-shot learning. We propose such a framework and demonstrate its effectiveness in document classification tasks. The only current study on meta-learning for cross-lingual few-shot learning is the one by (Nooralahzadeh et al., 2020), focusing on natural language inference and multilingual question answering. In their work, the authors focus on applying meta-learning to learn to adapt a monolingually trained classi1966 Proceedings of the 16th Conference of th"
2021.eacl-main.168,W19-4326,0,0.0271246,"rge amounts of unlabeled data in the target lanMeta-learning in NLP Monolingual Bansal et al. (2019) apply metalearning to a wide range of NLP tasks within a monolingual setting and show superior performance for parameter initialization over selfsupervised pretraining and multi-task learning. Their method is an adaptation of MAML where a combination of a text-encoder, BERT (Devlin et al., 2018), is coupled with a parameter generator that learns to generate task-dependent initializations of the classification head such that metalearning can be performed across tasks with disjoint label spaces. Obamuyide and Vlachos (2019b) apply meta-learning on the task of relation extraction; Obamuyide and Vlachos (2019a) apply lifelong meta-learning for relation extraction; Chen et al. (2019) apply meta-learning for few-shot learning on missing link prediction in knowledge graphs. Multilingual Gu et al. (2018) apply metalearning to Neural Machine Translation (NMT) and show its advantage over strong baselines such as cross-lingual transfer learning. By viewing each language pair as a task, the authors apply MAML to obtain competitive NMT systems with as little as 600 parallel sentences. To our best knowledge, the only appli"
2021.eacl-main.168,P19-1589,0,0.025715,"rge amounts of unlabeled data in the target lanMeta-learning in NLP Monolingual Bansal et al. (2019) apply metalearning to a wide range of NLP tasks within a monolingual setting and show superior performance for parameter initialization over selfsupervised pretraining and multi-task learning. Their method is an adaptation of MAML where a combination of a text-encoder, BERT (Devlin et al., 2018), is coupled with a parameter generator that learns to generate task-dependent initializations of the classification head such that metalearning can be performed across tasks with disjoint label spaces. Obamuyide and Vlachos (2019b) apply meta-learning on the task of relation extraction; Obamuyide and Vlachos (2019a) apply lifelong meta-learning for relation extraction; Chen et al. (2019) apply meta-learning for few-shot learning on missing link prediction in knowledge graphs. Multilingual Gu et al. (2018) apply metalearning to Neural Machine Translation (NMT) and show its advantage over strong baselines such as cross-lingual transfer learning. By viewing each language pair as a task, the authors apply MAML to obtain competitive NMT systems with as little as 600 parallel sentences. To our best knowledge, the only appli"
2021.eacl-main.168,P10-1114,0,0.29343,"ll entries of both x ˆ and µc have values between -1 and 1. The pre-softmax activation for class c is computed as x ˆT µc . Due to the size of the vectors and the scale of their respective entries, this in-product can yield a wide range of values, which in turn results in relatively high loss values, making the inner-loop optimization unstable. 3 3.1 Related work Multilingual NLP guage and domain using the MLM objective. With their method, the authors obtain state-of-the-art results on the MLDoc document classification task (Schwenk and Li, 2018) and the Amazon Sentiment Polarity Review task (Prettenhofer and Stein, 2010). A downside, however, is the high computational cost involved. For every language and domain combination: 1) a machine translation system has to be inferred on a large amount of unlabeled samples; 2) the UDA method needs to be applied to obtain a teacher model to generate pseudo-labels on the unlabeled in-domain data; 3) a language model must be finetuned, which involves forwards and backwards computation of a softmax function over a large output space (e.g., 50k tokens for mBERT and 250k tokens for XLM-RoBERTa). The final classifier is then obtained by 4) training the finetuned language mode"
2021.eacl-main.168,L18-1560,0,0.0802293,"space. Since the final activation function is the tanh activation, all entries of both x ˆ and µc have values between -1 and 1. The pre-softmax activation for class c is computed as x ˆT µc . Due to the size of the vectors and the scale of their respective entries, this in-product can yield a wide range of values, which in turn results in relatively high loss values, making the inner-loop optimization unstable. 3 3.1 Related work Multilingual NLP guage and domain using the MLM objective. With their method, the authors obtain state-of-the-art results on the MLDoc document classification task (Schwenk and Li, 2018) and the Amazon Sentiment Polarity Review task (Prettenhofer and Stein, 2010). A downside, however, is the high computational cost involved. For every language and domain combination: 1) a machine translation system has to be inferred on a large amount of unlabeled samples; 2) the UDA method needs to be applied to obtain a teacher model to generate pseudo-labels on the unlabeled in-domain data; 3) a language model must be finetuned, which involves forwards and backwards computation of a softmax function over a large output space (e.g., 50k tokens for mBERT and 250k tokens for XLM-RoBERTa). The"
2021.findings-emnlp.287,C18-1135,0,0.0262775,"online platform who may have posted a com- for some specific demographic (Waseem et al., ment that is to be classified as abusive or not. The 2017). Information about how a term is being community of this user comprises other users and used by other members of a user’s community, contents that they interact with on the online plat- e.g., in abusive contexts or otherwise, can help 3375 decipher linguistic variations that come up from time to time. In fact, it is usually the users with strong ties who are responsible for popularizing language variations as well as for spreading hate speech (Del Tredici and Fernández, 2018; Ribeiro et al., 2018). Therefore, having user and community information alongside linguistic features helps capture linguistic variations and their diffusion. Prevailing stereotypes. Previous research has shown that prevailing stereotypes often form the basis and justification of abuse. For example, many twitter accounts were open about their anger and hatred for Muslims in the wake of the Rochdale scandal that involved several British–Asian men getting convicted for child grooming (Awan, 2014). Stereotypes are not only explicit but implicit too (Hinton, 2017), which often show up as implici"
2021.findings-emnlp.287,D19-1477,0,0.0471978,"Missing"
2021.findings-emnlp.287,P19-1357,0,0.0632795,"unity information to enhance the detection of abusive language in comments should be preferred over those that leverage the information to classify users or communities themselves as abusive. This is because the latter can lead to unwarranted penalties, e.g., a platform may prohibit a user from engaging even in restorative conversations simply because of their past abusive behavior. • Elucidate stereotypes(s) underlying the abuse (or the absence thereof), be they explicit or be they in the form of implicit associations. Explainability is an important concept within abusive language detection. Jurgens et al. (2019) noted in their work that explainable ML techniques can promote restorative and procedural justice by surfacing the norms that have been violated and clarifying how they have been violated. That said, there has been limited discussion of the issue within the domain of abusive language detection. In this section, we first formalize the properties that an explainable detection method should aim to exhibit in order to thoroughly substantiate its decisions. We then describe how user and community information play an important role in the realization of each of the properties. Finally, we discuss w"
2021.findings-emnlp.287,gao-huang-2017-detecting,0,0.0180757,"ly, we address the topic of explainability in abusive language detection, proposing properties that an explainable detection method should aim to exhibit. We describe how user and community information can facilitate the realization of these properties and discuss the effective operationalization of explainability in view of the properties. form. In other words, community refers to the neighborhood of the user in the social graph of the platform. Conversations online are inherently contextual. Consequently, abuse on online platforms can only be effectively interpreted within a larger context (Gao and Huang, 2017) rather than in isolation. This is especially true for implicit or generalized abuse, which are harder to interpret than explicit abuse for humans and machines alike. Information of the user who posted the comment, or of the surrounding community including the targets of the comment, offers insights into several aspects of the context that are otherwise not accessible through the linguistic content of the comment alone. Here, information may refer to demographic traits like age or gender, knowledge about linguistic behavior, location details, etc. Below we categorize and discuss the aspects of"
2021.findings-emnlp.287,W16-3638,0,0.0594089,"Missing"
2021.findings-emnlp.287,C18-1093,1,0.908393,"6) constructed a dataset of 1, 900 tweets from 19 different twitter accounts with time of publication, language, and geo-position for each tweet taken from the profile of the user who created it. Waseem and Hovy (2016) released a list of 16, 907 tweet IDs along with their corresponding annotations, labeling each tweet as racist, sexist or neither. For each tweet, the dataset contains the gender of the user who created it along with their geo-location. Since Twitter APIs allow researchers to access information about a user given a tweet ID, the dataset of Waseem and Hovy (2016) was expanded by Mishra et al. (2018a) to include the follower-following information amongst users who created the tweets contained in the dataset. Ribeiro et al. (2018) collected a dataset of 100, 386 Twitter users along with up to 200 tweets for each of them. They created a graph of the users based on retweet relationship amongst them and annotated 4, 972 users as hateful or benign based on their tweets. Founta el al. (2018a) released a dataset of 80k tweet IDs with labels as normal, spam, hateful, Demographic characteristics. Previous research and abusive. Augmenting this dataset, Tredici et al. has demonstrated that some dem"
2021.findings-emnlp.287,N19-1221,1,0.861533,"Missing"
2021.findings-emnlp.287,W17-3006,0,0.0418918,"Missing"
2021.findings-emnlp.287,D17-1117,0,0.0385288,"Missing"
2021.findings-emnlp.287,W17-4209,0,0.0457469,"Missing"
2021.findings-emnlp.287,N18-2019,0,0.0354432,"Missing"
2021.findings-emnlp.287,2020.acl-main.394,1,0.848277,"Missing"
2021.findings-emnlp.287,N16-3020,0,0.0274,"ividual (i.e., directed tant for multiple reasons. Firstly, if the detection abuse) or a group (i.e., generalized abuse). method exhibits all the four properties of explain3380 ability, then the designers can easily gain insights into the factors that contributed to the decision made by the method given a comment. This can allow the designers to recognize when the method may be overly relying on a specific factor, e.g., the demographic traits. In the case of social feature engineering and user embeddings based methods, operationalization of explainability via feature attribution such as LIME (Ribeiro et al., 2016) and Integrated Gradients (Sundararajan et al., 2017) can be effective in offering such insights. For social graph based methods that employ graph neural networks, attribution techniques like GNNExplainer (Ying et al., 2019) can be used instead. The second reason why explainability is important for the designers is because it can allow them to optimize the method by removing inputs that do not contribute significantly. Here again, explainability via feature attribution can be effective. Lastly, explainability is also important for the designers to understand how their method would perform in c"
2021.findings-emnlp.287,2020.acl-main.486,0,0.13297,", as this can easily lead to scenarios of faulty generalizations where comments from a particular gender or race are always labeled abusive/benign. Moreover, relying solely on personal traits of users also comes with the risk that such information may not always be present or may not be accurate even when present (Drouin et al., 2016). On the other hand, more complex inductive biases learned from data, as in the case of social graph based methods, provide a safer and more reliable generalization from personal behaviors of users or communities to population level trends. specific demographics (Sap et al., 2020), hence diminishing the power of the methods to generalize. In fact, this bias is not only a problem for methods we discussed, but for any NLP method in general. When it comes to methods that incorporate user or community information specifically, there are two other biases that must be kept in mind when constructing datasets; we refer to them as comment distribution bias and label distribution bias. Comment distribution bias occurs when the majority of comments in the dataset come from a small number of unique users. Such datasets allow the methods to simply overfit to the linguistic or socia"
2021.findings-emnlp.287,W17-1101,0,0.0485991,"Missing"
2021.findings-emnlp.287,W18-5110,0,0.0123207,"eated a graph of the users based on retweet relationship amongst them and annotated 4, 972 users as hateful or benign based on their tweets. Founta el al. (2018a) released a dataset of 80k tweet IDs with labels as normal, spam, hateful, Demographic characteristics. Previous research and abusive. Augmenting this dataset, Tredici et al. has demonstrated that some demographic settings (2019) created a graph of users whose tweets are are inherently more abusive than others. For ex- included based on retweet relationships amongst ample, a study by Stephens et al. (2013) mapped the them. Similarly, Unsvåg and Gambäck (2018) the locations of hateful tweets across the United augmented the datasets of Fortuna (2017) and Ross States to uncover the regions where people use hate et al. (2016) which respective contain 5, 668 Porspeech the most. They observed that areas with low tuguese tweets and 13, 766 German tweets by using diversity use more derogatory slurs against racial Twitter APIs to get user information such as gender, and sexual minorities. A separate line of work by number of followers, number of status updates, etc. Savicki et al. (1996) concluded that male-only dis- Deviating from Twitter, Pavlopoulos et"
2021.findings-emnlp.287,W12-2103,0,0.137391,"Missing"
2021.findings-emnlp.287,W17-3012,0,0.0169412,"t to discuss racism and sexism. we review and analyze the state of the art methThe definitions for different types of abuse tend ods that leverage user or community information to enhance the understanding and detecto be overlapping and ambiguous. However, retion of abusive language. We then explore the gardless of the specific type, we define abuse as ethical challenges of incorporating user and any expression that is meant to denigrate or offend community information, laying out consideraa particular person or group. Taking a coursetions to guide future research. Finally, we adgrained view, Waseem et al. (2017) classify abuse dress the topic of explainability in abusive laninto broad categories based on explicitness and diguage detection, proposing properties that an rectness. Explicit abuse comes in the form of expleexplainable method should aim to exhibit. We describe how user and community information tives, derogatory words or threats, while implicit can facilitate the realization of these properties abuse has a more subtle appearance characterized and discuss the effective operationalization of by the presence of ambiguous terms and figures explainability in view of the properties. of speech su"
2021.findings-emnlp.287,N16-2013,0,0.144808,"and community In this section, we first recap the datasets in the domain of abusive language detection that contain user or community information alongside comments. We then go on to discuss the methods that have been applied to them. 3.1 Datasets Twitter has been the most common online platform from which researchers have sourced datasets with user and community information. Galán-García et al. (2016) constructed a dataset of 1, 900 tweets from 19 different twitter accounts with time of publication, language, and geo-position for each tweet taken from the profile of the user who created it. Waseem and Hovy (2016) released a list of 16, 907 tweet IDs along with their corresponding annotations, labeling each tweet as racist, sexist or neither. For each tweet, the dataset contains the gender of the user who created it along with their geo-location. Since Twitter APIs allow researchers to access information about a user given a tweet ID, the dataset of Waseem and Hovy (2016) was expanded by Mishra et al. (2018a) to include the follower-following information amongst users who created the tweets contained in the dataset. Ribeiro et al. (2018) collected a dataset of 100, 386 Twitter users along with up to 20"
2021.findings-emnlp.287,N19-1060,0,0.015137,"eralize. In fact, this bias is not only a problem for methods we discussed, but for any NLP method in general. When it comes to methods that incorporate user or community information specifically, there are two other biases that must be kept in mind when constructing datasets; we refer to them as comment distribution bias and label distribution bias. Comment distribution bias occurs when the majority of comments in the dataset come from a small number of unique users. Such datasets allow the methods to simply overfit to the linguistic or social behaviors and community roles of specific users (Wiegand et al., 2019). Label distribution bias occurs when only the abusive comments of a user are included in the dataset. Abuse is a relatively infrequent phenomenon, even at an individual level (Waseem and Hovy, 2016; Wulczyn et al., 2017). Only getting abusive comments of a user can make the methods simply associate the identity of the user to abusiveness when including user information. Moreover, datasets with this bias can also make phenomena like homophily appear overly effective in the detection of abuse by sampling only abusive comments from users who are close in the social network. Observability. The ob"
2021.findings-emnlp.287,N18-1095,0,0.0327049,"Missing"
2021.repl4nlp-1.20,2020.acl-main.386,0,0.0227411,"Missing"
2021.repl4nlp-1.20,W19-4828,0,0.0183357,"entences separately. The generated scores were used to perform binary classification of tokens, with the threshold based on F1 performance on the development set. The token-level predictions were evaluated against human explanations of the entailment relation using the e-SNLI dataset (Camburu et al., 2018). LIME was found to outperform other methods, however, it was also 1000× slower than attention-based methods at generating these explanations. 2.2 Attention heads The attention heads in a trained transformer model are designed to identify and combine useful information for a particular task. Clark et al. (2019) 1 https://github.com/bujol12/ bert-seq-interpretability 2.3 Soft attention Rei and Søgaard (2018) described a method for predicting token-level labels based on a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) architecture supervised at the sentence-level only. A dedicated attention module was integrated for building sentence representations, with its attention weights also acting as token-level importance scores. The architecture was found to outperform a gradient-based approach on the tasks of zero-shot sequence labeling for error detection, uncertainty detection, and sentiment analys"
2021.repl4nlp-1.20,N19-1357,0,0.0611159,"Missing"
2021.repl4nlp-1.20,C18-1328,0,0.0184561,". We apply LIME to a RoBERTa model supervised as a sentence classifier and investigate whether its scores can be used for sequence labeling. We use RoBERTa’s MASK token to mask out individual words and allow LIME to generate 5000 masked samples per sentence. The resulting explanation weights are then used as classification scores for each word, with the decision threshold fine-tuned based on the development set performance. Thorne et al. (2019) found LIME to outperform attention-based approaches on the task of explaining NLI models. LIME was used to probe a LSTMbased sentence-pair classifier (Lan and Xu, 2018) by removing tokens from the premise and hypothesis sentences separately. The generated scores were used to perform binary classification of tokens, with the threshold based on F1 performance on the development set. The token-level predictions were evaluated against human explanations of the entailment relation using the e-SNLI dataset (Camburu et al., 2018). LIME was found to outperform other methods, however, it was also 1000× slower than attention-based methods at generating these explanations. 2.2 Attention heads The attention heads in a trained transformer model are designed to identify a"
2021.repl4nlp-1.20,2021.ccl-1.108,0,0.0582218,"Missing"
2021.repl4nlp-1.20,N18-1027,1,0.916053,"with the threshold based on F1 performance on the development set. The token-level predictions were evaluated against human explanations of the entailment relation using the e-SNLI dataset (Camburu et al., 2018). LIME was found to outperform other methods, however, it was also 1000× slower than attention-based methods at generating these explanations. 2.2 Attention heads The attention heads in a trained transformer model are designed to identify and combine useful information for a particular task. Clark et al. (2019) 1 https://github.com/bujol12/ bert-seq-interpretability 2.3 Soft attention Rei and Søgaard (2018) described a method for predicting token-level labels based on a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) architecture supervised at the sentence-level only. A dedicated attention module was integrated for building sentence representations, with its attention weights also acting as token-level importance scores. The architecture was found to outperform a gradient-based approach on the tasks of zero-shot sequence labeling for error detection, uncertainty detection, and sentiment analysis. In order to obtain a single raw attention value eei for each token, biLSTM output vectors were"
2021.repl4nlp-1.20,P16-1112,1,0.888283,"Missing"
2021.repl4nlp-1.20,N16-3020,0,0.0554086,"tance for the overall task. Given a particular head, we can obtain an importance score for each token by averaging the attention scores from all the tokens that attend to it. In order to investigate the best possible setting, we report results for the attention head that achieves the highest token-level Mean Average Precision score on the development set. • We make our source code and models publicly available to facilitate further research in the field.1 2 Methods We evaluate four different methods for turning sentence-level transformer models into zero-shot sequence labelers. 2.1 LIME LIME (Ribeiro et al., 2016) generates local wordlevel importance scores through a meta-model that is trained on perturbed data generated by randomly masking out words in the input sentence. It was originally investigated in the context of Support Vector Machine (Hearst et al., 1998) text classifiers with unigram features. We apply LIME to a RoBERTa model supervised as a sentence classifier and investigate whether its scores can be used for sequence labeling. We use RoBERTa’s MASK token to mask out individual words and allow LIME to generate 5000 masked samples per sentence. The resulting explanation weights are then use"
C18-1093,K16-1017,0,0.0337369,"approach has the advantage of overcoming the absence of information that the previous approaches face. Among those that implement this idea are Yang et al. (2016), who used representations derived from a social graph to achieve better performance in entity linking tasks, and Chen and Ku (2016), who used them for stance classification. A considerable amount of literature has also been devoted to sentiment analysis with representations built from demographic factors (Yang and Eisenstein, 2017; Chen et al., 2016). Other tasks that have benefited from social representations are sarcasm detection (Amir et al., 2016) and political opinion prediction (T˘alm˘acel and Leon, 2017). 3 Dataset We experiment with the dataset of Waseem and Hovy (2016), containing tweets manually annotated for abuse. The authors retrieved around 136k tweets over a period of two months. They bootstrapped their collection process with a search for commonly used slurs and expletives related to religious, sexual, gender and ethnic minorities. From the results, they identified terms and references to entities that 1090 frequently showed up in abusive tweets. Based on this sample, they used a public Twitter API to collect the entire cor"
C18-1093,C16-1154,0,0.0307012,"ase, node representations (where nodes represent the authors in the social network) are typically induced using neural architectures. Given the graph representing the social network, such methods create low-dimensional representations for each node, which are optimized to predict the nodes close to it in the network. This approach has the advantage of overcoming the absence of information that the previous approaches face. Among those that implement this idea are Yang et al. (2016), who used representations derived from a social graph to achieve better performance in entity linking tasks, and Chen and Ku (2016), who used them for stance classification. A considerable amount of literature has also been devoted to sentiment analysis with representations built from demographic factors (Yang and Eisenstein, 2017; Chen et al., 2016). Other tasks that have benefited from social representations are sarcasm detection (Amir et al., 2016) and political opinion prediction (T˘alm˘acel and Leon, 2017). 3 Dataset We experiment with the dataset of Waseem and Hovy (2016), containing tweets manually annotated for abuse. The authors retrieved around 136k tweets over a period of two months. They bootstrapped their col"
C18-1093,D16-1171,0,0.030938,"presentations for each node, which are optimized to predict the nodes close to it in the network. This approach has the advantage of overcoming the absence of information that the previous approaches face. Among those that implement this idea are Yang et al. (2016), who used representations derived from a social graph to achieve better performance in entity linking tasks, and Chen and Ku (2016), who used them for stance classification. A considerable amount of literature has also been devoted to sentiment analysis with representations built from demographic factors (Yang and Eisenstein, 2017; Chen et al., 2016). Other tasks that have benefited from social representations are sarcasm detection (Amir et al., 2016) and political opinion prediction (T˘alm˘acel and Leon, 2017). 3 Dataset We experiment with the dataset of Waseem and Hovy (2016), containing tweets manually annotated for abuse. The authors retrieved around 136k tweets over a period of two months. They bootstrapped their collection process with a search for commonly used slurs and expletives related to religious, sexual, gender and ethnic minorities. From the results, they identified terms and references to entities that 1090 frequently show"
C18-1093,P15-1073,0,0.0506886,"s of racist tweets in response to President Obama’s re-election to show that such tweets were not uniformly distributed across the United States but formed clusters instead. In this paper, we present the first approach to abuse detection that leverages author profiling information based on properties of the authors’ social network and investigate its effectiveness. Author profiling has emerged as a powerful tool for NLP applications, leading to substantial performance improvements in several downstream tasks, such as text classification, sentiment analysis and author attribute identification (Hovy, 2015; Eisenstein, 2015; Yang and Eisenstein, 2017). The relevance of information gained from it is best explained by the idea of homophily, i.e., the phenomenon that people, both in real life as well as on the Internet, tend to associate more with those who appear similar. Here, similarity can be defined along various axes, e.g., location, age, language, etc. The strength of author profiling lies in that if we have information about members of a community c defined by some similarity criterion, and we know that the person p belongs to c, we can infer information about p. This concept has a straigh"
C18-1093,K15-1011,0,0.0227947,"r to classify the comments based on those representations. 2.2 Author profiling Author profiling has been leveraged in several ways for a variety of purposes in NLP. For instance, many studies have relied on demographic information of the authors. Amongst these are Hovy et al. (2015) and Ebrahimi et al. (2016) who extracted age and gender-related information to achieve superior performance in a text classification task. Pavalanathan and Eisenstein (2015), in their work, further showed the relevance of the same information to automatic text-based geo-location. Researching along the same lines, Johannsen et al. (2015) and Mirkin et al. (2015) utilized demographic factors to improve syntactic parsing and machine translation respectively. While demographic information has proved to be relevant for a number of tasks, it presents a significant drawback: since this information is not always available for all authors in a social network, it is not particularly reliable. Consequently, of late, a new line of research has focused on creating representations of users in a social network by leveraging the information derived from the connections that they have with other users. In this case, node representations (whe"
C18-1093,D15-1130,0,0.0233444,"ased on those representations. 2.2 Author profiling Author profiling has been leveraged in several ways for a variety of purposes in NLP. For instance, many studies have relied on demographic information of the authors. Amongst these are Hovy et al. (2015) and Ebrahimi et al. (2016) who extracted age and gender-related information to achieve superior performance in a text classification task. Pavalanathan and Eisenstein (2015), in their work, further showed the relevance of the same information to automatic text-based geo-location. Researching along the same lines, Johannsen et al. (2015) and Mirkin et al. (2015) utilized demographic factors to improve syntactic parsing and machine translation respectively. While demographic information has proved to be relevant for a number of tasks, it presents a significant drawback: since this information is not always available for all authors in a social network, it is not particularly reliable. Consequently, of late, a new line of research has focused on creating representations of users in a social network by leveraging the information derived from the connections that they have with other users. In this case, node representations (where nodes represent the au"
C18-1093,W17-3006,0,0.0925651,"GLoVe word embeddings.8 We employ Lightgbm (Ke et al., 2017) as our GDBT classifier and tune its hyper-parameters using 5-fold grid search. For the node2vec framework, we use the same parameters as in the original paper (Grover and Leskovec, 2016) except we set the dimensionality of node embeddings to 200 and increase the number of iterations to 25 for better convergence. 5.2 Results We perform 10-fold stratified cross validation (CV), as suggested by Forman and Scholz (2010), to evaluate all seven methods described in the previous section. Following previous research (Badjatiya et al., 2017; Park and Fung, 2017), we report the average weighted precision, recall, and F1 scores for all the methods. The average weighted precision is calculated as: P10 i=1 (wr · Pir + ws · Pis + wn · Pin ) 10 where Pir , Pis , Pin are precision scores on the racism, sexism, and none classes from the ith fold of the CV . The values wr , ws , and wn are the proportions of the racism, sexism, and none classes in the dataset respectively; since we use stratification, these proportions are constant (wr = 0.12, ws = 0.19, wn = 0.69) across all folds. Average weighted recall and F1 are calculated in the same manner. The results"
C18-1093,D15-1256,0,0.0183922,"017) improved the results of Wulczyn et al. by using a gated recurrent unit (GRU) model to encode the comments into dense low-dimensional representations, followed by a LR layer to classify the comments based on those representations. 2.2 Author profiling Author profiling has been leveraged in several ways for a variety of purposes in NLP. For instance, many studies have relied on demographic information of the authors. Amongst these are Hovy et al. (2015) and Ebrahimi et al. (2016) who extracted age and gender-related information to achieve superior performance in a text classification task. Pavalanathan and Eisenstein (2015), in their work, further showed the relevance of the same information to automatic text-based geo-location. Researching along the same lines, Johannsen et al. (2015) and Mirkin et al. (2015) utilized demographic factors to improve syntactic parsing and machine translation respectively. While demographic information has proved to be relevant for a number of tasks, it presents a significant drawback: since this information is not always available for all authors in a social network, it is not particularly reliable. Consequently, of late, a new line of research has focused on creating representat"
C18-1093,W17-3004,0,0.432619,"ile simultaneously being able to separate the racist and offensive ones. Their best model was a LR classifier trained using TF - IDF and POS n-gram features, as well as the count of hash tags and number of words. Wulczyn et al. (2017) prepared three different datasets of comments collected from the English Wikipedia Talk page; one was annotated for personal attacks, another for toxicity and the third one for aggression. Their best performing model was a multi-layered perceptron (MLP) classifier trained on character n-gram features. Experimenting with the personal attack and toxicity datasets, Pavlopoulos et al. (2017) improved the results of Wulczyn et al. by using a gated recurrent unit (GRU) model to encode the comments into dense low-dimensional representations, followed by a LR layer to classify the comments based on those representations. 2.2 Author profiling Author profiling has been leveraged in several ways for a variety of purposes in NLP. For instance, many studies have relied on demographic information of the authors. Amongst these are Hovy et al. (2015) and Ebrahimi et al. (2016) who extracted age and gender-related information to achieve superior performance in a text classification task. Pava"
C18-1093,D14-1162,0,0.0930946,"sults on the Wikipedia datasets released by Wulczyn et al. (2017). The method comprises a 1-layer gated recurrent unit (GRU) that takes a sequence w1 , . . . , wn of words represented as d-dimensional embeddings and encodes them into hidden states h1 , . . . , hn . This is followed by an LR layer that uses the last hidden state hn to classify the tweet. We make two minor modifications to the authors’ original architecture: we deepen the 1-layer GRU to a 2-layer GRU and use softmax instead of sigmoid in the LR layer.4 Like Pavlopoulos et al., we initialize the word embeddings to GLoVe vectors (Pennington et al., 2014). In all our methods, words not available in the GLoVe set are randomly initialized in the range ±0.05, indicating the lack of semantic information. By not mapping these words to a single random embedding, we mitigate against the errors that may arise due to their conflation (Madhyastha et al., 2015). A special OOV (out of vocabulary) token is also initialized in the same range. All the embeddings are updated during training, allowing some of the randomly-initialized ones to get task-tuned; the ones that do not get tuned lie closely clustered around the OOV token, to which unseen words in the"
C18-1093,N16-2013,0,0.406018,"bag-of-words (BOW) representations in a supervised classification setting for hate speech detection. Nobata et al. (2016) improved upon the results of Djuric et al. by training their classifier on a combination of features drawn from four different categories: linguistic (e.g., count of insult words), syntactic (e.g., POS tags), distributional semantic (e.g., word and comment embeddings) and BOW -based (word and characters n-grams). They reported that while the best results were obtained with all features combined, character n-grams contributed more to performance than all the other features. Waseem and Hovy (2016) created and experimented with a dataset of racist, sexist and clean tweets. Utilizing a logistic regression (LR) classifier to distinguish amongst them, they found that character n-grams coupled with gender information of users formed the optimal feature set; on the other hand, geographic and word-length distribution features provided little to no improvement. Working with the same dataset, Badjatiya et al. (2017) improved on their results by training a gradient-boosted decision 1 https://github.com/pushkarmishra/AuthorProfilingAbuseDetection 1089 tree (GBDT) classifier on averaged word embed"
C18-1093,W16-5618,0,0.164306,"ion (LR) classifier to distinguish amongst them, they found that character n-grams coupled with gender information of users formed the optimal feature set; on the other hand, geographic and word-length distribution features provided little to no improvement. Working with the same dataset, Badjatiya et al. (2017) improved on their results by training a gradient-boosted decision 1 https://github.com/pushkarmishra/AuthorProfilingAbuseDetection 1089 tree (GBDT) classifier on averaged word embeddings learnt using a long short-term memory (LSTM) network that they initialized with random embeddings. Waseem (2016) sampled 7k more tweets in the same manner as Waseem and Hovy (2016). They recruited expert and amateur annotators to annotate the tweets as racism, sexism, both or neither in order to study the influence of annotator knowledge on the task of hate speech detection. Combining this dataset with that of Waseem and Hovy (2016), Park et al. (2017) explored the merits of a two-step classification process. They first used a LR classifier to separate abusive and non-abusive tweets, followed by another LR classifier to distinguish between racist and sexist ones. They showed that this setup had comparab"
C18-1093,D16-1152,0,0.133695,"s of users in a social network by leveraging the information derived from the connections that they have with other users. In this case, node representations (where nodes represent the authors in the social network) are typically induced using neural architectures. Given the graph representing the social network, such methods create low-dimensional representations for each node, which are optimized to predict the nodes close to it in the network. This approach has the advantage of overcoming the absence of information that the previous approaches face. Among those that implement this idea are Yang et al. (2016), who used representations derived from a social graph to achieve better performance in entity linking tasks, and Chen and Ku (2016), who used them for stance classification. A considerable amount of literature has also been devoted to sentiment analysis with representations built from demographic factors (Yang and Eisenstein, 2017; Chen et al., 2016). Other tasks that have benefited from social representations are sarcasm detection (Amir et al., 2016) and political opinion prediction (T˘alm˘acel and Leon, 2017). 3 Dataset We experiment with the dataset of Waseem and Hovy (2016), containing tw"
D17-1297,W07-1604,0,0.0155587,"res that require no linguistic information, in contrast to previous work that has utilised a large set of features in a supervised setting (Hoang et al., 2016; Yuan et al., 2016). 2 Previous work The first approaches to GEC primarily treat the task as a classification problem over vectors of contextual lexical and syntactic features extracted from a fixed window around the target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron cla"
D17-1297,P06-1032,0,0.480342,"L 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise SMT to tackle determiner and preposition errors, while Yuan and Felice (2013) use POS-f"
D17-1297,D16-1195,0,0.0250888,"Missing"
D17-1297,D11-1010,0,0.0316973,"iors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise SMT to tackle determiner and preposition errors, while Yuan and Felice (2013) use POS-factored, phrase-based SMT systems, trained on both learner and artificially generated data to tackle determiner, preposition, noun number"
D17-1297,D12-1052,0,0.302328,"b agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise SMT to tackle determiner and preposition errors, while Yuan and Felice (2013) use POS-factored, phrase-based SMT systems, trained on both learner and artificially generated data to tackle determiner, preposition, noun number, verb form, and subject–verb agreement errors. The SMT approach has better capacity to correct complex errors, and it onl"
D17-1297,N12-1067,0,0.427367,"b agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise SMT to tackle determiner and preposition errors, while Yuan and Felice (2013) use POS-factored, phrase-based SMT systems, trained on both learner and artificially generated data to tackle determiner, preposition, noun number, verb form, and subject–verb agreement errors. The SMT approach has better capacity to correct complex errors, and it onl"
D17-1297,W13-1703,0,0.314071,"eural network framework, we develop a neural sequence-labelling model for error detection to calculate the probability of each token in a sentence as being correct or incorrect; using the error detection model, we propose a small set of features that require no linguistic processing to re-rank the N best hypotheses. We evaluate our approach on three different GEC datasets and achieve stateof-the-art results, outperforming all previous approaches to GEC. 3 Datasets We use the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011), and the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) that was used in the CoNLL GEC shared tasks. Both datasets are annotated with the language errors committed and suggested corrections from expert annotators. The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university 2796 students from an Asian language background. We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has been annotated by two different annotators). We also use the publicly ava"
D17-1297,W11-2838,0,0.0383093,"error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and"
D17-1297,C08-1022,0,0.384912,"Missing"
D17-1297,W14-1702,1,0.236149,"different datasets, and it has the additional advantage of only using a small set of easily computed features that require no linguistic input. 1 Introduction Grammatical Error Correction (GEC) in nonnative text attempts to automatically detect and correct errors that are typical of those found in learner writing. High precision and good coverage of learner errors is important in the development of GEC systems. Phrase-based Statistical Machine Translation (SMT) approaches to GEC have attracted considerable attention in recent years as they have been shown to achieve state-of-the-art results (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016). Given an ungrammatical input sentence, the task is formulated as “translating” it to its grammatical counterpart. Using a parallel dataset of input sentences and their corrected counterparts, SMT systems are typically trained to correct all error types in text without requiring any further linguistic input. To further adapt SMT approaches to the task of GEC and tackle the paucity of error-annotated learner data, previous work has investigated a number of extensions, ranging from the addition of further features into the decoding process (Felice et al."
D17-1297,N10-1019,0,0.0251462,"ow around the target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examp"
D17-1297,I08-1059,0,0.0352658,"ed from a fixed window around the target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to"
D17-1297,D16-1161,0,0.654357,"and it has the additional advantage of only using a small set of easily computed features that require no linguistic input. 1 Introduction Grammatical Error Correction (GEC) in nonnative text attempts to automatically detect and correct errors that are typical of those found in learner writing. High precision and good coverage of learner errors is important in the development of GEC systems. Phrase-based Statistical Machine Translation (SMT) approaches to GEC have attracted considerable attention in recent years as they have been shown to achieve state-of-the-art results (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016). Given an ungrammatical input sentence, the task is formulated as “translating” it to its grammatical counterpart. Using a parallel dataset of input sentences and their corrected counterparts, SMT systems are typically trained to correct all error types in text without requiring any further linguistic input. To further adapt SMT approaches to the task of GEC and tackle the paucity of error-annotated learner data, previous work has investigated a number of extensions, ranging from the addition of further features into the decoding process (Felice et al., 2014) via reranking the SMT decoder’s o"
D17-1297,N03-1017,0,0.0389189,".69 26.53 CoNLL test annotation 2 P R F0.5 23.60 25.10 23.90 27.62 21.18 25.88 69.60 7.91 27.18 Table 1: Token-level error detection performance of our detection models (LSTMFCE and LSTM) on FCE and the two CoNLL 2014 test set annotations. Baseline LSTMFCE and LSTMFCE are trained only on the public FCE training set. A Language Model (LM) is used to estimate the correction hypothesis probability pLM (c) from a corpus of correct English, and a translation model to estimate the conditional p(s|c) from a parallel corpus of corrected learner sentences. Stateof-the-art SMT systems are phrase-based (Koehn et al., 2003) in that they use phrases as “translation” units and therefore allow many-to-many “translation” mappings. The translation model is decomposed into a phrase-translation probability model and a phrase re-ordering probability model, and the 1-best correction hypothesis is of the following log-linear form (Och and Ney, 2002): ∗ c = arg max exp c K ∑ λi hi (c, s) (14) i=1 where h represents a feature function (e.g., phrasetranslation probability) and λ the feature weight. In this work, we employ two SMT systems: Yuan et al. (2016)2 and Junczys-Dowmunt and Grundkiewicz (2016). We apply our re-rankin"
D17-1297,C12-2084,0,0.0335458,"CoNLL GEC shared tasks. Both datasets are annotated with the language errors committed and suggested corrections from expert annotators. The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university 2796 students from an Asian language background. We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has been annotated by two different annotators). We also use the publicly available Lang-8 corpus (Mizumoto et al., 2012; Tajiri et al., 2012) and the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). Lang-8 contains learner English from lang-8.com, a languagelearning social networking service, which has been corrected by native speakers. JFLEG is a newly released corpus for GEC evaluation that contains fluency edits to make the text more native-like in addition to correcting grammatical errors, and contains learner data from a range of proficiency levels. We use Lang-8 and the FCE and CoNLL training sets to train our neural sequence-labelling model, and test correction performance on JFLEG, and t"
D17-1297,N16-1133,0,0.030573,"substantial improvements over the CoNLL state of the art. Chollampatt et al. (2016a) integrate a neural network joint model that has been adapted using native-language-specific learner text as a feature in SMT, while Chollampatt et al. (2016b) integrate a neural network global lexicon model and a neural network joint model to exploit continuous space representations of words rather than discrete ones, and learn non-linear mappings. Yuan and Briscoe (2016) present a Neural Machine Translation (NMT) model and propose an approach that tackles the rare-word problem in NMT. Yuan et al. (2016) and Mizumoto and Matsumoto (2016) employ supervised discriminative methods to re-rank the SMT decoder’s N -best list output based on language model and syntactic features respectively. Hoang et al. (2016) also exploit syntactic features in a supervised framework, but further extend their approach to generate new hypotheses. Our approach is similar in spirit, but differs in the following aspects: inspired by the work of Rei and Yannakoudakis (2016) who tackle error detection rather than correction within a neural network framework, we develop a neural sequence-labelling model for error detection to calculate the probability of"
D17-1297,P15-2097,0,0.0456016,"ate hypothesis c according to feature i; λ is a parameter that controls the effect feature i has on the final ranking; and K = 4 as we have four different features (three features presented in this section, plus the original score output by the SMT system). λs are tuned on the FCE development set and are set to 1, except for the sentence probability feature which has λ = 1.5.3 6 Evaluation We evaluate the effectiveness of our re-ranking approach on three different datasets: FCE, CoNLL 2014 and JFLEG. We report F0.5 using the shared task’s M 2 scorer (Dahlmeier and Ng, 2012b), and GLEU scores (Napoles et al., 2015). The latter is based on a variant of BLEU (Papineni et al., 2002) that is designed to reward correct edits and penalise ungrammatical ones. As mentioned in Section 5, we re-rank the 10-best lists of two SMT systems: Yuan et al. (2016) (CAMB16SMT ) and Junczys-Dowmunt and Grundkiewicz (2016) (AMU16SMT ). The results are presented in Table 2. We replicate the AMU16SMT system to obtain the 10-best output, and report results using this 3 We experimented with a small set of values (from 0 to 2 with increments of .1), though not exhaustively. version (AMU16SMT (replicated) ). Compared to the origin"
D17-1297,E17-2037,0,0.172934,"ggested corrections from expert annotators. The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university 2796 students from an Asian language background. We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has been annotated by two different annotators). We also use the publicly available Lang-8 corpus (Mizumoto et al., 2012; Tajiri et al., 2012) and the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). Lang-8 contains learner English from lang-8.com, a languagelearning social networking service, which has been corrected by native speakers. JFLEG is a newly released corpus for GEC evaluation that contains fluency edits to make the text more native-like in addition to correcting grammatical errors, and contains learner data from a range of proficiency levels. We use Lang-8 and the FCE and CoNLL training sets to train our neural sequence-labelling model, and test correction performance on JFLEG, and the FCE and CoNLL test sets. For JFLEG, we use the 754 sentences on which Napoles et al. (2017"
D17-1297,W14-1701,0,0.362538,"o correct complex errors, and it only requires parallel corrected sentences as input. Two state-of-the-art systems in the 2014 CoNLL shared task on correction of all errors regardless of type use SMT systems: Felice et al. (2014) use a hybrid approach that includes a rule-based and an SMT system augmented by a large web-based language model and combined with correction-type estimation to filter out error types with zero precision. Junczys-Dowmunt and Grundkiewicz (2016) investigate parameter tuning based on the MaxMatch (M 2 ) scorer, the sharedtask evaluation metric (Dahlmeier and Ng, 2012b; Ng et al., 2014), and experiment with different optimisers and interactions of dense and sparse features. Susanto et al. (2014) and Rozovskaya and Roth (2016) explore combinations of SMT systems and classifiers, the latter showing substantial improvements over the CoNLL state of the art. Chollampatt et al. (2016a) integrate a neural network joint model that has been adapted using native-language-specific learner text as a feature in SMT, while Chollampatt et al. (2016b) integrate a neural network global lexicon model and a neural network joint model to exploit continuous space representations of words rather"
D17-1297,W13-3601,0,0.19139,"-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data"
D17-1297,P16-1208,0,0.384563,"lgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise"
D17-1297,W11-2843,0,0.0380695,"Missing"
D17-1297,P02-1038,0,0.0436227,"guage Model (LM) is used to estimate the correction hypothesis probability pLM (c) from a corpus of correct English, and a translation model to estimate the conditional p(s|c) from a parallel corpus of corrected learner sentences. Stateof-the-art SMT systems are phrase-based (Koehn et al., 2003) in that they use phrases as “translation” units and therefore allow many-to-many “translation” mappings. The translation model is decomposed into a phrase-translation probability model and a phrase re-ordering probability model, and the 1-best correction hypothesis is of the following log-linear form (Och and Ney, 2002): ∗ c = arg max exp c K ∑ λi hi (c, s) (14) i=1 where h represents a feature function (e.g., phrasetranslation probability) and λ the feature weight. In this work, we employ two SMT systems: Yuan et al. (2016)2 and Junczys-Dowmunt and Grundkiewicz (2016). We apply our re-ranking approach to each SMT system’s N -best list using features derived from the neural sequencelabelling model for error detection described in the previous section, improve each of the SMT systems, and achieve state-of-the-art results on all three GEC datasets: FCE, CoNLL and JFLEG. 5.1 N -best list re-ranking For each SMT"
D17-1297,W12-2032,0,0.0440396,"of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated l"
D17-1297,P02-1040,0,0.115851,"ntrols the effect feature i has on the final ranking; and K = 4 as we have four different features (three features presented in this section, plus the original score output by the SMT system). λs are tuned on the FCE development set and are set to 1, except for the sentence probability feature which has λ = 1.5.3 6 Evaluation We evaluate the effectiveness of our re-ranking approach on three different datasets: FCE, CoNLL 2014 and JFLEG. We report F0.5 using the shared task’s M 2 scorer (Dahlmeier and Ng, 2012b), and GLEU scores (Napoles et al., 2015). The latter is based on a variant of BLEU (Papineni et al., 2002) that is designed to reward correct edits and penalise ungrammatical ones. As mentioned in Section 5, we re-rank the 10-best lists of two SMT systems: Yuan et al. (2016) (CAMB16SMT ) and Junczys-Dowmunt and Grundkiewicz (2016) (AMU16SMT ). The results are presented in Table 2. We replicate the AMU16SMT system to obtain the 10-best output, and report results using this 3 We experimented with a small set of values (from 0 to 2 with increments of .1), though not exhaustively. version (AMU16SMT (replicated) ). Compared to the original results on CoNLL reported in their paper (AMU16SMT (reported) )"
D17-1297,D14-1102,0,0.0791023,"systems in the 2014 CoNLL shared task on correction of all errors regardless of type use SMT systems: Felice et al. (2014) use a hybrid approach that includes a rule-based and an SMT system augmented by a large web-based language model and combined with correction-type estimation to filter out error types with zero precision. Junczys-Dowmunt and Grundkiewicz (2016) investigate parameter tuning based on the MaxMatch (M 2 ) scorer, the sharedtask evaluation metric (Dahlmeier and Ng, 2012b; Ng et al., 2014), and experiment with different optimisers and interactions of dense and sparse features. Susanto et al. (2014) and Rozovskaya and Roth (2016) explore combinations of SMT systems and classifiers, the latter showing substantial improvements over the CoNLL state of the art. Chollampatt et al. (2016a) integrate a neural network joint model that has been adapted using native-language-specific learner text as a feature in SMT, while Chollampatt et al. (2016b) integrate a neural network global lexicon model and a neural network joint model to exploit continuous space representations of words rather than discrete ones, and learn non-linear mappings. Yuan and Briscoe (2016) present a Neural Machine Translation"
D17-1297,P17-1194,1,0.776657,"m (7) where Wz1 , Wz2 and Wz3 are weight matrices, z is a dynamically calculated gating vector, and x et is the resulting token representation at position t. We optimise the model by minimising crossentropy between the predicted label distributions and the annotated labels. In addition to training the error detection objective, we make use of a multi-task loss function and train specific parts of the architecture as language models. This provides the model with a more informative loss function, while also encouraging it to learn more general compositional features and acting as a regulariser (Rei, 2017). First, two extra hidden layers are constructed: 2797 → → − − → = tanh − m Wm ht t (8) 4.1 Experimental settings Figure 1: Error detection network architecture that is repeated for all the words in a sentence (illustration for the word “cat”). − ← − ← ← m−t = tanh Wm ht (9) − → ← − where Wm and Wm are direction-specific weight matrices, used for connecting a forward or backward LSTM hidden state to a separate layer. The surrounding tokens are then predicted based on each hidden state using a softmax output layer: − → → P (wt+1 |w1 ...wt ) = softmax Wq − mt (10) ← − − P (wt−1 |wt ...wT ) = sof"
D17-1297,P12-2039,0,0.136333,"Both datasets are annotated with the language errors committed and suggested corrections from expert annotators. The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university 2796 students from an Asian language background. We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has been annotated by two different annotators). We also use the publicly available Lang-8 corpus (Mizumoto et al., 2012; Tajiri et al., 2012) and the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). Lang-8 contains learner English from lang-8.com, a languagelearning social networking service, which has been corrected by native speakers. JFLEG is a newly released corpus for GEC evaluation that contains fluency edits to make the text more native-like in addition to correcting grammatical errors, and contains learner data from a range of proficiency levels. We use Lang-8 and the FCE and CoNLL training sets to train our neural sequence-labelling model, and test correction performance on JFLEG, and the FCE and CoNLL test"
D17-1297,C16-1030,1,0.0986294,"STM, ht is the hidden state of the backwardmoving LSTM, and ht is the concatenation of both hidden states. A feedforward hidden layer with tanh activation is then used to map the representations from both directions into a more suitable combined space, and allow the model to learn higher-level features: dt = tanh Wd ht (4) where Wd is a weight matrix. Finally, a softmax output layer predicts the label distribution for each token, given the input sequence: P (yt |w1 ...wT ) = softmax Wo dt (5) where Wo is an output weight matrix. We also make use of the character-level architecture proposed by Rei et al. (2016), allowing the model to learn morphological patterns and capture out-of-vocabulary words. Each individual character is mapped to a character embedding and a bidirectional LSTM is used to combine them together into a character-based token representation. This vector m, constructed only from individual characters, is then combined with the regular token embedding xt using an adaptive gating mechanism: ( ) z = σ Wz1 · tanh(Wz2 xt + Wz3 m) (6) x et = z · xt + (1 − z) · m (7) where Wz1 , Wz2 and Wz3 are weight matrices, z is a dynamically calculated gating vector, and x et is the resulting token re"
D17-1297,P10-2065,0,0.0766109,"roblem over vectors of contextual lexical and syntactic features extracted from a fixed window around the target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the"
D17-1297,P16-1112,1,0.853892,"-linear mappings. Yuan and Briscoe (2016) present a Neural Machine Translation (NMT) model and propose an approach that tackles the rare-word problem in NMT. Yuan et al. (2016) and Mizumoto and Matsumoto (2016) employ supervised discriminative methods to re-rank the SMT decoder’s N -best list output based on language model and syntactic features respectively. Hoang et al. (2016) also exploit syntactic features in a supervised framework, but further extend their approach to generate new hypotheses. Our approach is similar in spirit, but differs in the following aspects: inspired by the work of Rei and Yannakoudakis (2016) who tackle error detection rather than correction within a neural network framework, we develop a neural sequence-labelling model for error detection to calculate the probability of each token in a sentence as being correct or incorrect; using the error detection model, we propose a small set of features that require no linguistic processing to re-rank the N best hypotheses. We evaluate our approach on three different GEC datasets and achieve stateof-the-art results, outperforming all previous approaches to GEC. 3 Datasets We use the First Certificate in English (FCE) dataset (Yannakoudakis e"
D17-1297,C08-1109,0,0.0161468,"and syntactic features extracted from a fixed window around the target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability t"
D17-1297,W14-1704,0,0.0477036,"Missing"
D17-1297,D10-1094,0,0.0212298,"target token. A large body of work has investigated error-typespecific models, and in particular models targeting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pul2795 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 2795–2806 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics man, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily re"
D17-1297,P11-1093,0,0.0170905,"008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Rozovskaya and Roth, 2010; Rozovskaya et al., 2012; Dale and Kilgarriff, 2011; Leacock et al., 2014). Core components of one of the top systems in the CoNLL 2013 and 2014 shared tasks on GEC (Ng et al., 2013, 2014) include Averaged Perceptron classifiers, native-language error correction priors in Naive Bayes models, and joint inference frameworks capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2012, 2014, 2011; Rozovskaya and Roth, 2011). The power of the classification paradigm comes from its ability to generalise well to unseen examples, without necessarily requiring error-annotated learner data (Rozovskaya and Roth, 2016). One of the first approaches to GEC as an SMT task is the one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained bea"
D17-1297,P11-1019,1,0.86773,"koudakis (2016) who tackle error detection rather than correction within a neural network framework, we develop a neural sequence-labelling model for error detection to calculate the probability of each token in a sentence as being correct or incorrect; using the error detection model, we propose a small set of features that require no linguistic processing to re-rank the N best hypotheses. We evaluate our approach on three different GEC datasets and achieve stateof-the-art results, outperforming all previous approaches to GEC. 3 Datasets We use the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011), and the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) that was used in the CoNLL GEC shared tasks. Both datasets are annotated with the language errors committed and suggested corrections from expert annotators. The former consists of upper-intermediate learner texts written by speakers from a number of different native language backgrounds, while the latter consists of essays written by advanced undergraduate university 2796 students from an Asian language background. We use the public FCE train/test split, and the NUCLE train/test set used in CoNLL 2014 (the test set has b"
D17-1297,N16-1042,1,0.785741,"ractions of dense and sparse features. Susanto et al. (2014) and Rozovskaya and Roth (2016) explore combinations of SMT systems and classifiers, the latter showing substantial improvements over the CoNLL state of the art. Chollampatt et al. (2016a) integrate a neural network joint model that has been adapted using native-language-specific learner text as a feature in SMT, while Chollampatt et al. (2016b) integrate a neural network global lexicon model and a neural network joint model to exploit continuous space representations of words rather than discrete ones, and learn non-linear mappings. Yuan and Briscoe (2016) present a Neural Machine Translation (NMT) model and propose an approach that tackles the rare-word problem in NMT. Yuan et al. (2016) and Mizumoto and Matsumoto (2016) employ supervised discriminative methods to re-rank the SMT decoder’s N -best list output based on language model and syntactic features respectively. Hoang et al. (2016) also exploit syntactic features in a supervised framework, but further extend their approach to generate new hypotheses. Our approach is similar in spirit, but differs in the following aspects: inspired by the work of Rei and Yannakoudakis (2016) who tackle e"
D17-1297,W16-0530,1,0.489914,"an ungrammatical input sentence, the task is formulated as “translating” it to its grammatical counterpart. Using a parallel dataset of input sentences and their corrected counterparts, SMT systems are typically trained to correct all error types in text without requiring any further linguistic input. To further adapt SMT approaches to the task of GEC and tackle the paucity of error-annotated learner data, previous work has investigated a number of extensions, ranging from the addition of further features into the decoding process (Felice et al., 2014) via reranking the SMT decoder’s output (Yuan et al., 2016) to neural-network adaptation components to SMT (Chollampatt et al., 2016a). In this paper, we propose an approach to N -best list re-ranking using neural sequence-labelling models. N -best list re-ranking allows for fast experimentation since the decoding process remains unchanged and only needs to be performed once. Crucially, it can be applied to any GEC system that can produce multiple alternative hypotheses. More specifically, we train a neural compositional model for error detection that calculates the probability of each token in a sentence being correct or incorrect, utilising the full"
D17-1297,W13-3607,1,0.685519,"he one by Brockett et al. (2006), who generate artificial data based on hand-crafted rules to train a model that can correct countability errors. Dahlmeier and Ng (2011) focus on correcting collocation errors based on paraphrases extracted from parallel corpora, while Dahlmeier and Ng (2012a) are the first to investigate a discriminatively trained beam-search decoder for fullsentence correction, focusing on five different error types: spelling, articles, prepositions, punctuation insertion, and noun number. Yoshimoto et al. (2013) utilise SMT to tackle determiner and preposition errors, while Yuan and Felice (2013) use POS-factored, phrase-based SMT systems, trained on both learner and artificially generated data to tackle determiner, preposition, noun number, verb form, and subject–verb agreement errors. The SMT approach has better capacity to correct complex errors, and it only requires parallel corrected sentences as input. Two state-of-the-art systems in the 2014 CoNLL shared task on correction of all errors regardless of type use SMT systems: Felice et al. (2014) use a hybrid approach that includes a rule-based and an SMT system augmented by a large web-based language model and combined with correc"
D17-1297,P17-1074,0,\N,Missing
N18-1024,P16-1068,1,0.917429,"oach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens the validity of neural essay scoring models. 1 i. We examine the robustness of state-of-the-art neural AES models to adversarially crafted input,1 and specifically focus on input related to local coherence; that is, grammatical but incoherent sequences of sentences.2 In addition to the superiority in performance of neural approaches against “standard” machine learning models (Alikaniotis et al., 2016; Taghipour and Ng, 2016), such a setup allows us to investigate their potential superiority / capacity in handling adversarial input without being explicitly designed to do so. Introduction Automated Essay Scoring (AES) focuses on automatically analyzing the quality of writing and assigning a score to the text. Typically, AES models exploit a wide range of manually-tuned shallow and deep linguistic features (Shermis and Hammer, 2012; Burstein et al., 2003; Rudner et al., 2006; Williamson et al., 2012; Andersen et al., 2013). Recent advances in deep learning have shown that neural approaches t"
N18-1024,W13-1704,1,0.853287,"ce of neural approaches against “standard” machine learning models (Alikaniotis et al., 2016; Taghipour and Ng, 2016), such a setup allows us to investigate their potential superiority / capacity in handling adversarial input without being explicitly designed to do so. Introduction Automated Essay Scoring (AES) focuses on automatically analyzing the quality of writing and assigning a score to the text. Typically, AES models exploit a wide range of manually-tuned shallow and deep linguistic features (Shermis and Hammer, 2012; Burstein et al., 2003; Rudner et al., 2006; Williamson et al., 2012; Andersen et al., 2013). Recent advances in deep learning have shown that neural approaches to AES achieve state-of-the-art results (Alikaniotis et al., 2016; Taghipour and Ng, 2016) with the additional advantage of utilizing features that are automatically learned from the data. In order to facilitate interpretability of neural models, a number of visualization techniques have been proposed to identify textual (superficial) features that contribute to model performance (Alikaniotis et al., 2016). To the best of our knowledge, however, no prior work has investigated the robustness of neural AES systems to adversaria"
N18-1024,J08-1001,0,0.679771,"connectedness features between sentences. 1 We use the terms ‘adversarially crafted input’ and ‘adversarial input’ to refer to text that is designed with the intention to trick the system. 2 Coherence can be assessed locally in terms of transitions between adjacent sentences. 263 Proceedings of NAACL-HLT 2018, pages 263–271 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics iii. A local coherence model is typically evaluated based on its ability to rank coherently ordered sequences of sentences higher than their incoherent / permuted counterparts (e.g., Barzilay and Lapata (2008)). We focus on a stricter evaluation setting in which the model is tested on its ability to rank coherent sequences of sentences higher than any incoherent / permuted set of sentences, and not just its own permuted counterparts. This supports a more rigorous evaluation that facilitates development of more robust models. 12 texts in total. Higgins and Heilman (2014) proposed a framework for evaluating the susceptibility of AES systems to gaming behavior. Neural AES Models Alikaniotis et al. (2016) developed a deep bidirectional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) ne"
N18-1024,P98-1032,0,0.0781803,"AES approaches with respect to adversarial input related to local aspects of coherence. For our experiments, we use the Automated Student Assessment Prize (ASAP) dataset,3 which contains essays written by students ranging from Grade 7 to Grade 10 in response to a number of different prompts (see Section 4). 2 Related Work AES Evaluation against Adversarial Input One of the earliest attempts at evaluating AES models against adversarial input was by Powers et al. (2002) who asked writing experts – that had been briefed on how the e-Rater scoring system works – to write essays to trick e-Rater (Burstein et al., 1998). The participants managed to fool the system into assigning higher-than-deserved grades, most notably by simply repeating a few wellwritten paragraphs several times. Yannakoudakis et al. (2011) and Yannakoudakis and Briscoe (2012) created and used an adversarial dataset of well-written texts and their random sentence permutations, which they released in the public domain, together with the grades assigned by a human expert to each piece of text. Unfortunately, however, the dataset is quite small, consisting of 3 https://www.kaggle.com/c/asap-aes/ 264 Figure 1: Local Coherence (LC) model archi"
N18-1024,D16-1193,0,0.211424,"aselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens the validity of neural essay scoring models. 1 i. We examine the robustness of state-of-the-art neural AES models to adversarially crafted input,1 and specifically focus on input related to local coherence; that is, grammatical but incoherent sequences of sentences.2 In addition to the superiority in performance of neural approaches against “standard” machine learning models (Alikaniotis et al., 2016; Taghipour and Ng, 2016), such a setup allows us to investigate their potential superiority / capacity in handling adversarial input without being explicitly designed to do so. Introduction Automated Essay Scoring (AES) focuses on automatically analyzing the quality of writing and assigning a score to the text. Typically, AES models exploit a wide range of manually-tuned shallow and deep linguistic features (Shermis and Hammer, 2012; Burstein et al., 2003; Rudner et al., 2006; Williamson et al., 2012; Andersen et al., 2013). Recent advances in deep learning have shown that neural approaches to AES achieve state-of-th"
N18-1024,D16-1115,0,0.164171,"bility of AES systems to gaming behavior. Neural AES Models Alikaniotis et al. (2016) developed a deep bidirectional Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) network, augmented with score-specific word embeddings that capture both contextual and usage information for words. Their approach outperformed traditional feature-engineered AES models on the ASAP dataset. Taghipour and Ng (2016) investigated various recurrent and convolutional architectures on the same dataset and found that an LSTM layer followed by a Mean over Time operation achieves state-of-the-art results. Dong and Zhang (2016) showed that a twolayer Convolutional Neural Network (CNN) outperformed other baselines (e.g., Bayesian Linear Ridge Regression) on both in-domain and domainadaptation experiments on the ASAP dataset. iv. We propose a framework for integrating and jointly training the local coherence model with a state-of-the-art AES model. We evaluate our approach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens AES validity. Neural Coherence Mo"
N18-1024,P17-1121,0,0.020069,"oring sentence representations to extract local coherence features. The sentence representations were constructed with recursive and recurrent neural methods. Their approach outperformed previous methods on the task of selecting maximally coherent sentence orderings from sets of candidate permutations (Barzilay and Lapata, 2008). Lin et al. (2015) developed a hierarchical Recurrent Neural Network (RNN) for document modeling. Among others, they looked at capturing coherence between sentences using a sentence-level language model, and evaluated their approach on the sentence ordering task. Tien Nguyen and Joty (2017) built a CNN over entity grid representations, and trained the network in a pairwise ranking fashion. Their model outperformed other graph-based and distributed sentence models. We note that our goal is not to identify the “best” model of local coherence on randomly permuted grammatical sentences in the domain of AES, but rather to propose a framework that strengthens the validity of AES approaches with respect to adversarial input related to local aspects of coherence. At the outset, our goal is to develop a framework that strengthens the validity of state-of-the-art neural AES approaches wit"
N18-1024,P11-2022,0,0.0212531,"tored on the development sets – we select the model that yields the highest PRA value.12 We use as a baseline the LC model that is based on the multiplication of the clique scores (similarly to Li and Hovy (2014)), and compare the results (LCmul ) to our averaged approach. As another baseline, we use the entity grid (EGrid) (Barzilay and Lapata, 2008) that models transitions between sentences based on sequences of entity mentions labeled with their grammatical role. EGrid has been shown to give competitive results on similar coherence tasks in other domains. Using the Brown Coherence Toolkit (Eisner and Charniak, 2011),13 we construct the entity transition probabilities with length = 3 and salience = 2. The transition probabilities are then used as features that are fed as input to an SVM classifier with an RBF kernel and penalty parameter C = 1.5 to predict a coherence score. where M is the number of synthetic essays in the development set. We furthermore evaluate a baseline where the joint model is trained without sharing the word embedding layer between the two submodels, and report the effect on performance (Joint Learningno layer sharing ). Finally, we evaluate a baseline where for the joint model we s"
N18-1024,W12-2004,1,0.914003,"g from Grade 7 to Grade 10 in response to a number of different prompts (see Section 4). 2 Related Work AES Evaluation against Adversarial Input One of the earliest attempts at evaluating AES models against adversarial input was by Powers et al. (2002) who asked writing experts – that had been briefed on how the e-Rater scoring system works – to write essays to trick e-Rater (Burstein et al., 1998). The participants managed to fool the system into assigning higher-than-deserved grades, most notably by simply repeating a few wellwritten paragraphs several times. Yannakoudakis et al. (2011) and Yannakoudakis and Briscoe (2012) created and used an adversarial dataset of well-written texts and their random sentence permutations, which they released in the public domain, together with the grades assigned by a human expert to each piece of text. Unfortunately, however, the dataset is quite small, consisting of 3 https://www.kaggle.com/c/asap-aes/ 264 Figure 1: Local Coherence (LC) model architecture using a window of size 3. All hsnt representations are computed the same way as hsnt 1 . The figure depicts the process of predicting the first clique score, which is applied to all the cliques in the text. The output coher"
N18-1024,P11-1019,1,0.922093,"ing for Adversarially Crafted Input Youmna Farag Helen Yannakoudakis Ted Briscoe Department of Computer Science and Technology The ALTA Institute University of Cambridge United Kingdom {youmna.farag,helen.yannakoudakis,ted.briscoe}@cl.cam.ac.uk Abstract missclassifications; for instance, a high score to a low quality text. Examining and addressing such validity issues is critical and imperative for AES deployment. Previous work has primarily focused on assessing the robustness of “standard” machine learning approaches that rely on manual feature engineering; for example, Powers et al. (2002); Yannakoudakis et al. (2011) have shown that such AES systems, unless explicitly designed to handle adversarial input, can be susceptible to subversion by writers who understand something of the systems’ workings and can exploit this to maximize their score. In this paper, we make the following contributions: We demonstrate that current state-of-theart approaches to Automated Essay Scoring (AES) are not well-suited to capturing adversarially crafted input of grammatical but incoherent sequences of sentences. We develop a neural model of local coherence that can effectively learn connectedness features between sentences,"
N18-1024,D14-1218,0,0.281075,"(e.g., Bayesian Linear Ridge Regression) on both in-domain and domainadaptation experiments on the ASAP dataset. iv. We propose a framework for integrating and jointly training the local coherence model with a state-of-the-art AES model. We evaluate our approach against a number of baselines and experimentally demonstrate its effectiveness on both the AES task and the task of flagging adversarial input, further contributing to the development of an approach that strengthens AES validity. Neural Coherence Models A number of approaches have investigated neural models of coherence on news data. Li and Hovy (2014) used a window approach where a sliding kernel of weights was applied over neighboring sentence representations to extract local coherence features. The sentence representations were constructed with recursive and recurrent neural methods. Their approach outperformed previous methods on the task of selecting maximally coherent sentence orderings from sets of candidate permutations (Barzilay and Lapata, 2008). Lin et al. (2015) developed a hierarchical Recurrent Neural Network (RNN) for document modeling. Among others, they looked at capturing coherence between sentences using a sentence-level"
N18-1024,D17-1019,0,0.0291831,", j ∈ {1, ..., N − m + 1}, N is the number of sentences in the text, and ∗ is the linear convolutional operation. Scoring The cliques’ predicted scores are calculated via a linear operation followed by a sigmoid function to project the predictions to a [0, 1] probability space: (1) yˆjclq = sigmoid(hclq j .V ) (3) where V ∈ Rdcnn is a learned weight. The network optimizes its parameters to minimize the negative log-likelihood of the cliques’ gold scores y clq , given the network’s predicted scores: Clique Representation Each window of sentences in a text represents a clique q = 4 We note that Li and Jurafsky (2017) also present an extended version of the work by Li and Hovy (2014), evaluated on different domains. 5 LSTMs have been shown to produce state-of-the-art results in AES (Alikaniotis et al., 2016; Taghipour and Ng, 2016). Llocal T 1X = [−yjclq log(ˆ yjclq ) T j=1 −(1 − yjclq )log(1 − yˆjclq )] 265 (4) 3.3 Combined Models We propose a framework for integrating the LSTMT&N model with the Local Coherence (LC) one. Our goal is to have a robust AES system that is able to correctly flag adversarial input while maintaining a high performance on essay scoring. 3.3.1 The baseline model simply concatenate"
N18-1024,D13-1141,0,0.0116969,"e is larger than the threshold. We experimentally demonstrate that this approach enables the model to perform well on both original ASAP and synthetic essays. During model evaluation, the texts flagged as adversarial by the model are assigned a score of zero, while the rest are assigned the predicted essay score (ˆ y esy in Figure 3). 4 5 Model Parameters and Baselines Coherence models We train and test the LC model described in Section 3.1 on the synthetic dataset and evaluate it using PRA and TPRA. During pre-processing, words are lowercased and initialized with pre-trained word embeddings (Zou et al., 2013). Words that occur only once in the training set are mapped to a special UNK embedData and Evaluation We use the ASAP dataset, which contains 12, 976 essays written by students ranging from Grade 7 to 10 We note that this threshold is different than the one mentioned in Section 3.3.2. 11 This is primarily done to keep the data balanced: initial experiments showed that training with all 10 permutations per essay harms AES performance, but has negligible effect on adversarial input detection. 9 We note that, during training, the scores are mapped to a range between 0 and 1 (similarly to Taghipou"
N18-1024,D15-1106,0,0.0214758,"the development of an approach that strengthens AES validity. Neural Coherence Models A number of approaches have investigated neural models of coherence on news data. Li and Hovy (2014) used a window approach where a sliding kernel of weights was applied over neighboring sentence representations to extract local coherence features. The sentence representations were constructed with recursive and recurrent neural methods. Their approach outperformed previous methods on the task of selecting maximally coherent sentence orderings from sets of candidate permutations (Barzilay and Lapata, 2008). Lin et al. (2015) developed a hierarchical Recurrent Neural Network (RNN) for document modeling. Among others, they looked at capturing coherence between sentences using a sentence-level language model, and evaluated their approach on the sentence ordering task. Tien Nguyen and Joty (2017) built a CNN over entity grid representations, and trained the network in a pairwise ranking fashion. Their model outperformed other graph-based and distributed sentence models. We note that our goal is not to identify the “best” model of local coherence on randomly permuted grammatical sentences in the domain of AES, but rat"
N18-1024,C98-1032,0,\N,Missing
N19-1059,S13-2007,0,0.0206857,"often required to correctly interpret metaphors. To the best of our knowledge, this is the first work to investigate the effects of broader discourse on metaphor identification.1 Introduction From bottled up anger to the world is your oyster, metaphor is a defining component of language, adding poetry and humor to communication (Glucksberg and McGlone, 2001) and serving as a tool for reasoning about relations between concepts (Lakoff and Johnson, 1980). Designing metaphor processing systems has thus seen significant interest in the NLP community, with applications from information retrieval (Korkontzelos et al., 2013) to machine translation (Saygin, 2001). An important first step in any metaphor processing pipeline is metaphor identification. To date, most approaches to its identification operate in restricted contexts, for instance, by only considering isolated verb–argument pairs (e.g. deflate economy) (Rei et al., 2017) or the sentence containing an utterance (Gao et al., 2018). However, wider context is crucial for understanding metaphor: for instance, the phrase drowning students can be interpreted as literal (in the context of water) or metaphorical (in the context of homework). Of2 Related work Meta"
N19-1059,P16-2017,1,0.898693,"tion is typically framed as a binary classification task, either with (1) word tu1 Code and data available at https://github.com/ jayelm/broader-metaphor. 596 Proceedings of NAACL-HLT 2019, pages 596–601 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ples such as SVO triples (car drinks gasoline) or (2) whole sentences as input, where the goal is to predict the metaphoricity of a token in the sentence. Recent work has used a variety of features extracted from these two types of contexts, including selectional preferences (Shutova, 2013; Beigman Klebanov et al., 2016), concreteness/imageability (Turney et al., 2011; Tsvetkov et al., 2014), multi-modal (Tekiroglu et al., 2015; Shutova et al., 2016) and neural features (Do Dinh and Gurevych, 2016; Rei et al., 2017). At the recent VU Amsterdam (VUA) metaphor identification shared task (Leong et al., 2018), neural approaches dominated, with most teams using LSTMs trained on word embeddings and additional linguistic features, such as semantic classes and part of speech tags (Wu et al., 2018; Stemle and Onysko, 2018; Mykowiecka et al., 2018; Swarnkar and Singh, 2018). Most recently, Gao et al. (2018) revisited t"
N19-1059,W16-1609,0,0.0491173,"Missing"
N19-1059,W16-1104,0,0.0177879,"-HLT 2019, pages 596–601 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ples such as SVO triples (car drinks gasoline) or (2) whole sentences as input, where the goal is to predict the metaphoricity of a token in the sentence. Recent work has used a variety of features extracted from these two types of contexts, including selectional preferences (Shutova, 2013; Beigman Klebanov et al., 2016), concreteness/imageability (Turney et al., 2011; Tsvetkov et al., 2014), multi-modal (Tekiroglu et al., 2015; Shutova et al., 2016) and neural features (Do Dinh and Gurevych, 2016; Rei et al., 2017). At the recent VU Amsterdam (VUA) metaphor identification shared task (Leong et al., 2018), neural approaches dominated, with most teams using LSTMs trained on word embeddings and additional linguistic features, such as semantic classes and part of speech tags (Wu et al., 2018; Stemle and Onysko, 2018; Mykowiecka et al., 2018; Swarnkar and Singh, 2018). Most recently, Gao et al. (2018) revisited this task, reporting state-of-the-art results with BiLSTMs and contextualized word embeddings (Peters et al., 2018). To the best of our knowledge, none of the existing approaches ha"
N19-1059,W18-0907,1,0.87645,"istics ples such as SVO triples (car drinks gasoline) or (2) whole sentences as input, where the goal is to predict the metaphoricity of a token in the sentence. Recent work has used a variety of features extracted from these two types of contexts, including selectional preferences (Shutova, 2013; Beigman Klebanov et al., 2016), concreteness/imageability (Turney et al., 2011; Tsvetkov et al., 2014), multi-modal (Tekiroglu et al., 2015; Shutova et al., 2016) and neural features (Do Dinh and Gurevych, 2016; Rei et al., 2017). At the recent VU Amsterdam (VUA) metaphor identification shared task (Leong et al., 2018), neural approaches dominated, with most teams using LSTMs trained on word embeddings and additional linguistic features, such as semantic classes and part of speech tags (Wu et al., 2018; Stemle and Onysko, 2018; Mykowiecka et al., 2018; Swarnkar and Singh, 2018). Most recently, Gao et al. (2018) revisited this task, reporting state-of-the-art results with BiLSTMs and contextualized word embeddings (Peters et al., 2018). To the best of our knowledge, none of the existing approaches have utilized information from wider discourse context in metaphor identification, nor investigated its effects."
N19-1059,D18-1060,0,0.632044,"tool for reasoning about relations between concepts (Lakoff and Johnson, 1980). Designing metaphor processing systems has thus seen significant interest in the NLP community, with applications from information retrieval (Korkontzelos et al., 2013) to machine translation (Saygin, 2001). An important first step in any metaphor processing pipeline is metaphor identification. To date, most approaches to its identification operate in restricted contexts, for instance, by only considering isolated verb–argument pairs (e.g. deflate economy) (Rei et al., 2017) or the sentence containing an utterance (Gao et al., 2018). However, wider context is crucial for understanding metaphor: for instance, the phrase drowning students can be interpreted as literal (in the context of water) or metaphorical (in the context of homework). Of2 Related work Metaphor identification is typically framed as a binary classification task, either with (1) word tu1 Code and data available at https://github.com/ jayelm/broader-metaphor. 596 Proceedings of NAACL-HLT 2019, pages 596–601 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ples such as SVO triples (car drinks gasoline) or (2) w"
N19-1059,P14-5010,0,0.00466753,"Missing"
N19-1059,W18-0916,0,0.029288,"Missing"
N19-1059,W15-1404,0,0.0211548,"https://github.com/ jayelm/broader-metaphor. 596 Proceedings of NAACL-HLT 2019, pages 596–601 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ples such as SVO triples (car drinks gasoline) or (2) whole sentences as input, where the goal is to predict the metaphoricity of a token in the sentence. Recent work has used a variety of features extracted from these two types of contexts, including selectional preferences (Shutova, 2013; Beigman Klebanov et al., 2016), concreteness/imageability (Turney et al., 2011; Tsvetkov et al., 2014), multi-modal (Tekiroglu et al., 2015; Shutova et al., 2016) and neural features (Do Dinh and Gurevych, 2016; Rei et al., 2017). At the recent VU Amsterdam (VUA) metaphor identification shared task (Leong et al., 2018), neural approaches dominated, with most teams using LSTMs trained on word embeddings and additional linguistic features, such as semantic classes and part of speech tags (Wu et al., 2018; Stemle and Onysko, 2018; Mykowiecka et al., 2018; Swarnkar and Singh, 2018). Most recently, Gao et al. (2018) revisited this task, reporting state-of-the-art results with BiLSTMs and contextualized word embeddings (Peters et al.,"
N19-1059,D14-1162,0,0.0964298,"Missing"
N19-1059,P14-1024,0,0.0866881,") word tu1 Code and data available at https://github.com/ jayelm/broader-metaphor. 596 Proceedings of NAACL-HLT 2019, pages 596–601 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ples such as SVO triples (car drinks gasoline) or (2) whole sentences as input, where the goal is to predict the metaphoricity of a token in the sentence. Recent work has used a variety of features extracted from these two types of contexts, including selectional preferences (Shutova, 2013; Beigman Klebanov et al., 2016), concreteness/imageability (Turney et al., 2011; Tsvetkov et al., 2014), multi-modal (Tekiroglu et al., 2015; Shutova et al., 2016) and neural features (Do Dinh and Gurevych, 2016; Rei et al., 2017). At the recent VU Amsterdam (VUA) metaphor identification shared task (Leong et al., 2018), neural approaches dominated, with most teams using LSTMs trained on word embeddings and additional linguistic features, such as semantic classes and part of speech tags (Wu et al., 2018; Stemle and Onysko, 2018; Mykowiecka et al., 2018; Swarnkar and Singh, 2018). Most recently, Gao et al. (2018) revisited this task, reporting state-of-the-art results with BiLSTMs and contextual"
N19-1059,N18-1202,0,0.0815765,"ooksCorpus (Zhu et al., 2015). From this model, we extract 4800-dimensional representations for verb lemma, arguments, and contexts. Models For each utterance, our models learn generic representations of a verb lemma,2 its syntactic arguments, and its broader discourse context. We concatenate these features into a single feature vector and feed them into a gradient boosting decision tree classifier (Chen and Guestrin, 2016).3 By observing performance differences when using the lemma only (L), lemma + arguments (LA), or ELMo Finally, we use ELMo, a model of deep contextualized word embeddings (Peters et al., 2018). We extract 1024-dimensional representations from the last layer of a stacked BiLSTM 4 These methods differ significantly in dimensionality and training data. Our intent is not to exhaustively compare these methods, but rather claim generally that many embeddings give good performance on this task. 5 Since some methods provide only document embeddings and not word embeddings, for consistency, in all methods we use the same embedding process even for single-word verbs and arguments. 2 The lemmatized form of the verb has improved generalization in other systems (Beigman Klebanov et al., 2016)."
N19-1059,D11-1063,0,0.0725586,"task, either with (1) word tu1 Code and data available at https://github.com/ jayelm/broader-metaphor. 596 Proceedings of NAACL-HLT 2019, pages 596–601 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ples such as SVO triples (car drinks gasoline) or (2) whole sentences as input, where the goal is to predict the metaphoricity of a token in the sentence. Recent work has used a variety of features extracted from these two types of contexts, including selectional preferences (Shutova, 2013; Beigman Klebanov et al., 2016), concreteness/imageability (Turney et al., 2011; Tsvetkov et al., 2014), multi-modal (Tekiroglu et al., 2015; Shutova et al., 2016) and neural features (Do Dinh and Gurevych, 2016; Rei et al., 2017). At the recent VU Amsterdam (VUA) metaphor identification shared task (Leong et al., 2018), neural approaches dominated, with most teams using LSTMs trained on word embeddings and additional linguistic features, such as semantic classes and part of speech tags (Wu et al., 2018; Stemle and Onysko, 2018; Mykowiecka et al., 2018; Swarnkar and Singh, 2018). Most recently, Gao et al. (2018) revisited this task, reporting state-of-the-art results wit"
N19-1059,D17-1162,1,0.825747,"munication (Glucksberg and McGlone, 2001) and serving as a tool for reasoning about relations between concepts (Lakoff and Johnson, 1980). Designing metaphor processing systems has thus seen significant interest in the NLP community, with applications from information retrieval (Korkontzelos et al., 2013) to machine translation (Saygin, 2001). An important first step in any metaphor processing pipeline is metaphor identification. To date, most approaches to its identification operate in restricted contexts, for instance, by only considering isolated verb–argument pairs (e.g. deflate economy) (Rei et al., 2017) or the sentence containing an utterance (Gao et al., 2018). However, wider context is crucial for understanding metaphor: for instance, the phrase drowning students can be interpreted as literal (in the context of water) or metaphorical (in the context of homework). Of2 Related work Metaphor identification is typically framed as a binary classification task, either with (1) word tu1 Code and data available at https://github.com/ jayelm/broader-metaphor. 596 Proceedings of NAACL-HLT 2019, pages 596–601 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguist"
N19-1059,W18-0913,0,0.479282,"y of features extracted from these two types of contexts, including selectional preferences (Shutova, 2013; Beigman Klebanov et al., 2016), concreteness/imageability (Turney et al., 2011; Tsvetkov et al., 2014), multi-modal (Tekiroglu et al., 2015; Shutova et al., 2016) and neural features (Do Dinh and Gurevych, 2016; Rei et al., 2017). At the recent VU Amsterdam (VUA) metaphor identification shared task (Leong et al., 2018), neural approaches dominated, with most teams using LSTMs trained on word embeddings and additional linguistic features, such as semantic classes and part of speech tags (Wu et al., 2018; Stemle and Onysko, 2018; Mykowiecka et al., 2018; Swarnkar and Singh, 2018). Most recently, Gao et al. (2018) revisited this task, reporting state-of-the-art results with BiLSTMs and contextualized word embeddings (Peters et al., 2018). To the best of our knowledge, none of the existing approaches have utilized information from wider discourse context in metaphor identification, nor investigated its effects. 3 lemma + arguments + context (LAC), we can investigate the effects of including broader context. To obtain arguments for verbs, we extract subjects and direct objects with Stanford Core"
N19-1059,S13-1040,1,0.815273,"ork Metaphor identification is typically framed as a binary classification task, either with (1) word tu1 Code and data available at https://github.com/ jayelm/broader-metaphor. 596 Proceedings of NAACL-HLT 2019, pages 596–601 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ples such as SVO triples (car drinks gasoline) or (2) whole sentences as input, where the goal is to predict the metaphoricity of a token in the sentence. Recent work has used a variety of features extracted from these two types of contexts, including selectional preferences (Shutova, 2013; Beigman Klebanov et al., 2016), concreteness/imageability (Turney et al., 2011; Tsvetkov et al., 2014), multi-modal (Tekiroglu et al., 2015; Shutova et al., 2016) and neural features (Do Dinh and Gurevych, 2016; Rei et al., 2017). At the recent VU Amsterdam (VUA) metaphor identification shared task (Leong et al., 2018), neural approaches dominated, with most teams using LSTMs trained on word embeddings and additional linguistic features, such as semantic classes and part of speech tags (Wu et al., 2018; Stemle and Onysko, 2018; Mykowiecka et al., 2018; Swarnkar and Singh, 2018). Most recentl"
N19-1059,N16-1020,1,0.873034,"elm/broader-metaphor. 596 Proceedings of NAACL-HLT 2019, pages 596–601 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics ples such as SVO triples (car drinks gasoline) or (2) whole sentences as input, where the goal is to predict the metaphoricity of a token in the sentence. Recent work has used a variety of features extracted from these two types of contexts, including selectional preferences (Shutova, 2013; Beigman Klebanov et al., 2016), concreteness/imageability (Turney et al., 2011; Tsvetkov et al., 2014), multi-modal (Tekiroglu et al., 2015; Shutova et al., 2016) and neural features (Do Dinh and Gurevych, 2016; Rei et al., 2017). At the recent VU Amsterdam (VUA) metaphor identification shared task (Leong et al., 2018), neural approaches dominated, with most teams using LSTMs trained on word embeddings and additional linguistic features, such as semantic classes and part of speech tags (Wu et al., 2018; Stemle and Onysko, 2018; Mykowiecka et al., 2018; Swarnkar and Singh, 2018). Most recently, Gao et al. (2018) revisited this task, reporting state-of-the-art results with BiLSTMs and contextualized word embeddings (Peters et al., 2018). To the best of o"
N19-1059,W18-0918,0,0.056736,"racted from these two types of contexts, including selectional preferences (Shutova, 2013; Beigman Klebanov et al., 2016), concreteness/imageability (Turney et al., 2011; Tsvetkov et al., 2014), multi-modal (Tekiroglu et al., 2015; Shutova et al., 2016) and neural features (Do Dinh and Gurevych, 2016; Rei et al., 2017). At the recent VU Amsterdam (VUA) metaphor identification shared task (Leong et al., 2018), neural approaches dominated, with most teams using LSTMs trained on word embeddings and additional linguistic features, such as semantic classes and part of speech tags (Wu et al., 2018; Stemle and Onysko, 2018; Mykowiecka et al., 2018; Swarnkar and Singh, 2018). Most recently, Gao et al. (2018) revisited this task, reporting state-of-the-art results with BiLSTMs and contextualized word embeddings (Peters et al., 2018). To the best of our knowledge, none of the existing approaches have utilized information from wider discourse context in metaphor identification, nor investigated its effects. 3 lemma + arguments + context (LAC), we can investigate the effects of including broader context. To obtain arguments for verbs, we extract subjects and direct objects with Stanford CoreNLP (Manning et al., 2014"
N19-1059,W18-0914,0,0.0224686,"Missing"
N19-1221,C18-1093,1,0.705478,"a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection. 1 2 Introduction Matthew Zook (2012) carried out an interesting study showing that the racist tweets posted in response to President Obama’s re-election were not distributed uniformly across the United States but instead formed clusters. This phenomenon is known as homophily: i.e., people, both in real life and online, tend to cluster with those who appear similar to themselves. To model homophily, recent research in abusive language detection on Twitter (Mishra et al., 2018a) incorporates embeddings for authors (i.e., users who have composed tweets) that encode the structure of their surrounding communities. The embeddings (called author profiles) are generated by applying a node embedding framework to an undirected unlabeled community graph where nodes denote the authors and edges the follower–following relationships amongst them on Twitter. However, these profiles do not capture the linguistic behavior of the authors and their communities and do not convey whether their tweets tend to be abusive or not. Related work Supervised learning for abusive language det"
N19-1221,W18-5101,1,0.755809,"a heterogeneous graph-structured modeling of communities significantly advances the current state of the art in abusive language detection. 1 2 Introduction Matthew Zook (2012) carried out an interesting study showing that the racist tweets posted in response to President Obama’s re-election were not distributed uniformly across the United States but instead formed clusters. This phenomenon is known as homophily: i.e., people, both in real life and online, tend to cluster with those who appear similar to themselves. To model homophily, recent research in abusive language detection on Twitter (Mishra et al., 2018a) incorporates embeddings for authors (i.e., users who have composed tweets) that encode the structure of their surrounding communities. The embeddings (called author profiles) are generated by applying a node embedding framework to an undirected unlabeled community graph where nodes denote the authors and edges the follower–following relationships amongst them on Twitter. However, these profiles do not capture the linguistic behavior of the authors and their communities and do not convey whether their tweets tend to be abusive or not. Related work Supervised learning for abusive language det"
N19-1221,D17-1117,0,0.0515604,"Missing"
N19-1221,W17-4209,0,0.13164,"Missing"
N19-1221,N18-2019,0,0.0582534,"9; Warner and Hirschberg, 2012). Djuric et al. (2015) showed that dense comment representations generated using paragraph2vec outperform bag-of-words features. Several works have since utilized (deep) neural architectures to achieve impressive results on a variety of abuse-annotated datasets (Nobata et al., 2016; Pavlopoulos et al., 2017a). Recently, the research focus has shifted towards extraction of features that capture behavioral and social traits of users. Pavlopoulos et al. (2017b) showed that including randomly-initialized user embeddings improved the performance of their RNN methods. Qian et al. (2018) employed LSTMs to generate inter and intra-user representations based on tweets, but they did not leverage community information. 2145 Proceedings of NAACL-HLT 2019, pages 2145–2150 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 3 Dataset Following previous work (Mishra et al., 2018a), we experiment with a subset of the Twitter dataset compiled by Waseem and Hovy (2016). Waseem and Hovy released a list of 16, 907 tweet IDs along with their corresponding annotations,1 labeling each tweet as racist, sexist or neither (clean). Recently, Mishra et"
N19-1221,W12-2103,0,0.341837,"an undirected unlabeled community graph where nodes denote the authors and edges the follower–following relationships amongst them on Twitter. However, these profiles do not capture the linguistic behavior of the authors and their communities and do not convey whether their tweets tend to be abusive or not. Related work Supervised learning for abusive language detection was first explored by Spertus (1997) who extracted rule-based features to train their classifier. Subsequently, manually-engineered lexical– syntactic features formed the crux of most approaches to the task (Yin et al., 2009; Warner and Hirschberg, 2012). Djuric et al. (2015) showed that dense comment representations generated using paragraph2vec outperform bag-of-words features. Several works have since utilized (deep) neural architectures to achieve impressive results on a variety of abuse-annotated datasets (Nobata et al., 2016; Pavlopoulos et al., 2017a). Recently, the research focus has shifted towards extraction of features that capture behavioral and social traits of users. Pavlopoulos et al. (2017b) showed that including randomly-initialized user embeddings improved the performance of their RNN methods. Qian et al. (2018) employed LST"
N19-1221,N16-2013,0,0.0599349,"is set to 3 for the output O ∈ Rn×3 of the GCN to be a softmax distribution over the 3 classes in the data. The GCN is trained by minimizing the crossentropy loss with respect to the labeled nodes of the graph. Once the model is trained, we extract e F W (1) 200-dimensional embeddings E = A from the first layer (i.e., the layer’s output without activation). This contains embeddings for author nodes as well as tweet nodes. For our experiments on author profiles, we make use of the former. 2 Stacking more layers does not improve results on the validation set further. This method is adopted from Waseem and Hovy (2016) wherein they train a logistic regression classifier on character n-grams (up to 4grams) of the tweets. Character n-grams have been shown to be highly effective for abuse detection due to their robustness to spelling variations. + AUTH. This is the state of the art method (Mishra et al., 2018a) for the dataset we are using. For each tweet, the profile of its author (generated by node2vec from the community graph) is appended onto the tweet’s character n-gram representation for training the LR classifier as above. LR + EXTD. This method is identical to LR + AUTH , except that we now run node2ve"
N19-1251,W13-1704,1,0.827158,"uding GED and the subtask of identifying SVA errors, have, in recent years, been handled with Recurrent Neural Networks (RNNs) trained on large amounts of data (Rei and Yannakoudakis, 2016, 2017). However, most publicly available datasets for GED are relatively small, making it difficult to learn a general grammar representation and potentially leading to over-fitting. Previous work has also shown that neural language models with a similar architecture have difficulty learning subject–verb agreement patterns in the presence of agreement attractors (Linzen et al., 2016). Rule-based approaches (Andersen et al., 2013) are still considered a strong alternative to end-toend neural networks, with many industry solutions still relying on rules defined over syntactic trees. The rule-based approach has the advantage of not requiring manual annotation, while also allowing easy access to adding and removing individual rules. On the other hand, language is continuously evolving, and there are exceptions to most grammar rules we know. Additionally, rule-based matching typically relies on syntactic pre-processing, which is error-prone, leading to compounding errors that hurt the downstream GED performance. Our contri"
N19-1251,P06-1032,0,0.0728054,"er the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we th"
N19-1251,W18-0529,0,0.0173854,"focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correction systems. 3 Subject–verb agreement detection Following recent work on GED (Rei and Yannakoudakis, 2016), we define SVA error detection as a sequence labeling task, where each token is simply labeled as correct or incorrect. For a given SVA error, only the verb is labeled as incorrect. Error types other than SVA are ignored, i.e., we do not corr"
N19-1251,P17-1074,0,0.0690779,"Missing"
N19-1251,Y09-1008,0,0.0357055,") who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun e"
N19-1251,D14-1082,0,0.0353693,"Missing"
N19-1251,W13-1703,0,0.0312982,"5.3 Training data ESL writings. We use the following ESL datasets as training data: • Lang8 is a parallel corpus of sentences with errors and their corrected versions created by scraping the Lang-8 website5 , which is an open platform where language learners can write texts and native speakers of that language can provide feedback via error correction (Mizumoto et al., 2011). It contains 1, 047, 393 sentences. • NUCLE comprises around 1, 400 essays written by students from the National University of Singapore. It is annotated for error tags and corrections by professional English instructors (Dahlmeier et al., 2013). It contains 57, 151 sentences. • FCE train set. We use the publicly available FCE training set, containing 25, 748 sentences. A subset of 5, 000 sentences was separated and used for development experiments. 4 Artificial errors. We generate artificial subject– verb agreement errors from large amounts of data. Specifically, we use the British National Corpus (BNC, BNC-Consortium et al., 2007), a collection of British English sentences that includes samples from different media such as newspapers, journals, letters or essays. Subject–verb agreement in English merely consists of inflecting 3rd p"
N19-1251,W16-0506,0,0.0147561,"there. 2 2420 • FCE. The Cambridge Learner Corpus of First Certificate in English (FCE) exam scripts consists of texts produced by ESL learners taking the FCE exam, which assesses English at the upper-intermediate proficiency level (Yannakoudakis et al., 2011). We use the publicly available test set. • AESW. The dataset from the Automated Evaluation of Scientific Writing Shared Task 3 https://github.com/chrisjbryant/errant 2016 (AESW) is a collection of text extracts from published journal articles (mostly in physics and mathematics) along with their (sentence-aligned) corrected counterparts (Daudaravicius et al., 2016). We test on the combined trained, development and test set.4 • JFLEG. The JHU Fluency-Extended GUG corpus (JFLEG) represents a cross-section of ungrammatical data, consisting of sentences written by ESL learners with different proficiency levels and L1s (Napoles et al., 2017). We evaluate our models on the public test set. • CoNLL14. The test dataset from the CoNLL 2014 shared task consists of (mostly argumentative) essays written by advanced undergraduate students from the National University of Singapore, and are annotated for grammatical errors by two native speakers of English (Ng et al.,"
N19-1251,E14-3013,0,0.0221815,"g et al. (2015) use a similar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical error"
N19-1251,N18-1108,0,0.022276,"s, and on average achieves a new state-of-the-art on detecting SVA errors. 2 Related work Neural approaches. Recent neural approaches to GED include Rei and Yannakoudakis (2016) who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four"
N19-1251,D18-1541,0,0.0142505,"imilar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al"
N19-1251,L16-1498,0,0.0153074,"ers or essays. Subject–verb agreement in English merely consists of inflecting 3rd person singular verbs in the present tense (and be in the past), which makes any text in English fairly easy to corrupt with SVA errors. We assume that the BNC data is written in correct British English. Using predicted PoS tags provided by the Stanford Log-linear PoS Tagger, we identify verbs in present tense, as well as was and were for the past tense, and flip them to their respective opposite version using the list of inflected English words (annotated with morphological features) from the Unimorph project (Kirov et al., 2016). The final artificial training set includes the sentences with injected errors (265, 742 sentences), their original counterpart, and sentences where SVA errors could not be injected due to not containing candidate verbs that could be flipped (241, 295 sentences). Sentences containing special placeholders for mathematical equations, dates, etc. are filtered out. 5 http://lang-8.com/ 6 Experiments The models. We compare our neural model trained on both artificially generated errors and ESL data (LSTMESL+art ) to three baselines: a neural model trained only on ESL data (LSTMESL ) (i.e., reflecti"
N19-1251,P18-1132,0,0.019321,"es a new state-of-the-art on detecting SVA errors. 2 Related work Neural approaches. Recent neural approaches to GED include Rei and Yannakoudakis (2016) who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four different sentence typ"
N19-1251,N16-1030,0,0.0592706,"rd Neural Network Dependency Parser (Chen and Manning, 2014) respectively. 4.2 Neural system We use the state-of-the-art neural sequence labeling architecture for error detection (Rei and Yannakoudakis, 2016). The model receives a sequence of tokens (w1 , ..., wT ) as input and outputs a sequence of labels (l1 , ..., lT ), i.e., one for each token, indicating whether a token is grammatically correct (in agreement) or not, in the given context. All tokens are first mapped to distributed word representations, pre-trained using word2vec (Mikolov et al., 2013) on the Google News corpus. Following Lample et al. (2016), character-based representations are also built for every word using a bi-LSTM (Hochreiter and Schmidhuber, 1997) and then concatenated onto the word embedding. The combined embeddings are then given as input to a word-level bi-LSTM, creating representations that are conditioned on the context from both sides of the target word. These representations are then passed through an additional feedforward layer, in order to combine the extracted features and map them to a more suitable space. A softmax output layer returns the probability distribution over the two possible labels (correct or incorr"
N19-1251,Q16-1037,0,0.479233,"ches. Sequence labeling problems in NLP, including GED and the subtask of identifying SVA errors, have, in recent years, been handled with Recurrent Neural Networks (RNNs) trained on large amounts of data (Rei and Yannakoudakis, 2016, 2017). However, most publicly available datasets for GED are relatively small, making it difficult to learn a general grammar representation and potentially leading to over-fitting. Previous work has also shown that neural language models with a similar architecture have difficulty learning subject–verb agreement patterns in the presence of agreement attractors (Linzen et al., 2016). Rule-based approaches (Andersen et al., 2013) are still considered a strong alternative to end-toend neural networks, with many industry solutions still relying on rules defined over syntactic trees. The rule-based approach has the advantage of not requiring manual annotation, while also allowing easy access to adding and removing individual rules. On the other hand, language is continuously evolving, and there are exceptions to most grammar rules we know. Additionally, rule-based matching typically relies on syntactic pre-processing, which is error-prone, leading to compounding errors that"
N19-1251,J93-2004,0,0.0653251,"cts and verbs are far apart, i.e., when the agreement relation is defined over a long-distance dependency. In order to see how our systems are affected by the distance between the subject and verb, we split the test sets based on different subject–verb distances. Note, however, that our benchmarks are not annotated with PoS tags and dependency relations. If we binned our test data based on predicted dependencies, the inductive bias of our syntactic parser and the errors it made would bias our evaluation. Instead, we perform our analyses on section 22 and 23 of the Penn Treebank (PTB) dataset (Marcus et al., 1993). The PTB however is not annotated with grammatical errors. We therefore corrupt the sentences by injecting SVA errors, in the same 2423 way we corrupted the BNC (§5.3) to create additional training data. For each sentence in the PTB, we identify a subject–verb pair, and group the sentences by the subject–verb distance. We then run our models on two versions of each sentence: an unaltered version and a corrupted one, where we have generated an SVA error by corrupting the verb, using the method described earlier (§5.3). This way we can compute the performance of our models as F0.5 scores over t"
N19-1251,I11-1017,0,0.023707,"CoNLL 2014 shared task consists of (mostly argumentative) essays written by advanced undergraduate students from the National University of Singapore, and are annotated for grammatical errors by two native speakers of English (Ng et al., 2014). 5.3 Training data ESL writings. We use the following ESL datasets as training data: • Lang8 is a parallel corpus of sentences with errors and their corrected versions created by scraping the Lang-8 website5 , which is an open platform where language learners can write texts and native speakers of that language can provide feedback via error correction (Mizumoto et al., 2011). It contains 1, 047, 393 sentences. • NUCLE comprises around 1, 400 essays written by students from the National University of Singapore. It is annotated for error tags and corrections by professional English instructors (Dahlmeier et al., 2013). It contains 57, 151 sentences. • FCE train set. We use the publicly available FCE training set, containing 25, 748 sentences. A subset of 5, 000 sentences was separated and used for development experiments. 4 Artificial errors. We generate artificial subject– verb agreement errors from large amounts of data. Specifically, we use the British National"
N19-1251,C10-2103,0,0.0340272,"00, while character representations have size 100. The word-level LSTM hidden layers have size 300 for each direction, and the character-level LSTM hidden layers have size 100 for each direction. Evaluation. Existing approaches are typically optimised for high precision at the cost of recall, as a system’s utility depends strongly on the ratio of true to false positives, which has been found to be more important in terms of learning effect. A high number of false positives would mean that the system often flags correct language as incorrect, and may therefore end up doing more harm than good (Nagata and Nakatani, 2010). Because of this, F0.5 is preferred to F1 in the GED domain as it puts more weight on precision than recall. For each experiment, we report the token-level precision (P), the recall (R), and the F0.5 scores. 7 Results The main results are summarized in Table 1. Looking at the performance of the LSTMESL+art system, we see that on 3 out of 4 benchmarks, our neural model trained on artificially generated errors outperforms the LSTMESL system with respect to F0.5 . On average, over the four benchmarks, its F0.5 score is 2.43 points higher than the best performing baseline. Both neural models obta"
N19-1251,E17-2037,0,0.0178762,"vailable test set. • AESW. The dataset from the Automated Evaluation of Scientific Writing Shared Task 3 https://github.com/chrisjbryant/errant 2016 (AESW) is a collection of text extracts from published journal articles (mostly in physics and mathematics) along with their (sentence-aligned) corrected counterparts (Daudaravicius et al., 2016). We test on the combined trained, development and test set.4 • JFLEG. The JHU Fluency-Extended GUG corpus (JFLEG) represents a cross-section of ungrammatical data, consisting of sentences written by ESL learners with different proficiency levels and L1s (Napoles et al., 2017). We evaluate our models on the public test set. • CoNLL14. The test dataset from the CoNLL 2014 shared task consists of (mostly argumentative) essays written by advanced undergraduate students from the National University of Singapore, and are annotated for grammatical errors by two native speakers of English (Ng et al., 2014). 5.3 Training data ESL writings. We use the following ESL datasets as training data: • Lang8 is a parallel corpus of sentences with errors and their corrected versions created by scraping the Lang-8 website5 , which is an open platform where language learners can write"
N19-1251,P17-1194,1,0.856235,"(Hochreiter and Schmidhuber, 1997) and then concatenated onto the word embedding. The combined embeddings are then given as input to a word-level bi-LSTM, creating representations that are conditioned on the context from both sides of the target word. These representations are then passed through an additional feedforward layer, in order to combine the extracted features and map them to a more suitable space. A softmax output layer returns the probability distribution over the two possible labels (correct or incorrect) for each word. We also include the language modeling objective proposed by Rei (2017), which encourages the model to learn better representations via multi-tasking and predicting surrounding words in the sentence. Dropout (Srivastava et al., 2014) with probability 0.5 is applied to word representations and to the output from the word-level bi-LSTM. The model is optimised using categorical cross-entropy with AdaDelta (Zeiler, 2012). 5 Data 5.1 Data preprocessing As the public datasets either have their own taxonomy or they are not annotated with error types at all, we apply the error type extraction tool of Bryant, Felice, and Briscoe (2017) to automatically get error types map"
N19-1251,W17-5032,1,0.856999,"tterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correcti"
N19-1251,P16-1112,1,0.831037,"ine translation systems, guiding automatically generated output towards grammatically correct sequences. The problem of detecting subject–verb agreement (SVA) errors is an important subtask of GED. In this work, we focus on detecting subject– verb agreement errors in the English as a Second Language (ESL) domain. Most SVA errors occur at the third-person present tense when determining Approaches. Sequence labeling problems in NLP, including GED and the subtask of identifying SVA errors, have, in recent years, been handled with Recurrent Neural Networks (RNNs) trained on large amounts of data (Rei and Yannakoudakis, 2016, 2017). However, most publicly available datasets for GED are relatively small, making it difficult to learn a general grammar representation and potentially leading to over-fitting. Previous work has also shown that neural language models with a similar architecture have difficulty learning subject–verb agreement patterns in the presence of agreement attractors (Linzen et al., 2016). Rule-based approaches (Andersen et al., 2013) are still considered a strong alternative to end-toend neural networks, with many industry solutions still relying on rules defined over syntactic trees. The rule-ba"
N19-1251,W17-5004,1,0.852217,"ural sequence labeling models. We demonstrate that a system trained on a combination of available labeled data and large volumes of silver standard data outperforms both neural and rule-based baselines by a margin on three out of four standard benchmarks, and on average achieves a new state-of-the-art on detecting SVA errors. 2 Related work Neural approaches. Recent neural approaches to GED include Rei and Yannakoudakis (2016) who argue that bidirectional (bi-) LSTMs, in particular, are superior to other RNNs when evaluated on standard ESL benchmarks for GED and give state-of-the-art results. Rei and Yannakoudakis (2017) show even better performance using a multi-task learning architecture for training bi-LSTMs that additionally predicts linguistic properties of words, such as their part of speech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified,"
N19-1251,P11-1093,0,0.0378914,"ch type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task o"
N19-1251,N03-1033,0,0.0125469,"the system. However, our rulebased system is not limited to the detection of simple cases of SVA errors. It relies on PoS tags and dependency relations to identify all types of SVA errors. Specifically, our rule-based system operates as follows: (i) it identifies the candidate verbs based on PoS tags;1 (ii) for a given verb, it uses the dependency relations to find its subject;2 (iii) the PoS tag of the verb and its subject are used to check whether they agree in number and person. We use predicted Penn Treebank PoS tags and dependency relations provided by the Stanford Loglinear PoS Tagger (Toutanova et al., 2003) and the Stanford Neural Network Dependency Parser (Chen and Manning, 2014) respectively. 4.2 Neural system We use the state-of-the-art neural sequence labeling architecture for error detection (Rei and Yannakoudakis, 2016). The model receives a sequence of tokens (w1 , ..., wT ) as input and outputs a sequence of labels (l1 , ..., lT ), i.e., one for each token, indicating whether a token is grammatically correct (in agreement) or not, in the given context. All tokens are first mapped to distributed word representations, pre-trained using word2vec (Mikolov et al., 2013) on the Google News cor"
N19-1251,Y15-2040,0,0.0259707,"ech (PoS). Recent studies (Linzen et al., 2016; Gulordava et al., 2018; Kuncoro et al., 2018) have specifically analyzed the performance of LSTMs in learning syntax-sensitive dependencies such as SVA. Rule-based approaches. Cai et al. (2009) use a combination of dependency parsing and sentence simplification, as well as special handling of wh-elements, to detect SVA errors. Once the subject–verb relation is identified, after parsing the simplified input sentence, a PoS tagger is used to check agreement. This is similar in spirit to the rule-based baseline system used in our experiments below. Wang et al. (2015) use a similar approach, distinguishing between four different sentence types and using slightly different rules for each type. Their rules are, again, defined over the outputs of a dependency parser and a PoS tagger. Sun et al. (2007) use labeled data to derive rules based on dependency tree patterns. Automatic error generation. Because of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan"
N19-1251,N18-1057,0,0.0158796,"cause of the scarcity of annotated datasets in GED, research has been carried out on creating artificial errors, where errors are injected into otherwise correct text using deterministic rules or probabilistic approaches using linguistic information (Felice and Yuan, 2014; Kasewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correction systems. 3 Subject–verb agreement d"
N19-1251,P11-1019,1,0.706811,"s as our target class. 5.2 Test data We compare the rule-based and neural approaches for the task of SVA error detection on four benchmarks in the ESL domain. 1 Present tense verbs + “was” and “were”. The subject can be direct – attached with a nsubj relation – or indirect, such as when the syntactic subject is a relative pronoun, e.g., who, or an expletive, e.g., there. 2 2420 • FCE. The Cambridge Learner Corpus of First Certificate in English (FCE) exam scripts consists of texts produced by ESL learners taking the FCE exam, which assesses English at the upper-intermediate proficiency level (Yannakoudakis et al., 2011). We use the publicly available test set. • AESW. The dataset from the Automated Evaluation of Scientific Writing Shared Task 3 https://github.com/chrisjbryant/errant 2016 (AESW) is a collection of text extracts from published journal articles (mostly in physics and mathematics) along with their (sentence-aligned) corrected counterparts (Daudaravicius et al., 2016). We test on the combined trained, development and test set.4 • JFLEG. The JHU Fluency-Extended GUG corpus (JFLEG) represents a cross-section of ungrammatical data, consisting of sentences written by ESL learners with different profi"
N19-1251,D17-1297,1,0.826452,"asewa et al., 2018). Studies focusing on detecting specific error types such as determiners and prepositions (Rozovskaya and Roth, 2011) or noun number (Brockett et al., 2006) are mainly developed within the framework of automatic error generation. Recent work, expanding the detection (Rei et al., 2017) and the correction (Xie et al., 2018) tasks to all types of errors, improves the performance of neural models by training on additional artificial error data generated via machine translation methods. Miscellaneous. Recent work has also led to good performance in correcting grammatical errors (Yannakoudakis et al., 2017; Bryant and Briscoe, 2018; Chollampatt and Ng, 2018). However, in this paper, we are interested in the task of grammatical error detection and we therefore compare our work to current state-of-the-art approaches to detecting errors and do not report the performance of correction systems. 3 Subject–verb agreement detection Following recent work on GED (Rei and Yannakoudakis, 2016), we define SVA error detection as a sequence labeling task, where each token is simply labeled as correct or incorrect. For a given SVA error, only the verb is labeled as incorrect. Error types other than SVA are ign"
P11-1019,P06-4020,1,0.551746,"mising the differences between closely-ranked data pairs. The principal advantage of applying rank preference learning to the AA task is that we explicitly model the grade relationships between scripts and do not need to apply a further regression step to fit the classifier output to the scoring scheme. The results reported in this paper are obtained by learning a linear classification function. 3.2 Feature set We parsed the training and test data (see Section 2) using the Robust Accurate Statistical Parsing (RASP) system with the standard tokenisation and sentence boundary detection modules (Briscoe et al., 2006) in order to broaden the space of candidate features suitable for the task. The features used in our experiments are mainly motivated by the fact that lexical and grammatical features should be highly discriminative for the AA task. Our full feature set is as follows: i. Lexical ngrams (a) Word unigrams (b) Word bigrams ii. Part-of-speech (PoS) ngrams (a) PoS unigrams (b) PoS bigrams (c) PoS trigrams iii. Features representing syntax (a) Phrase structure (PS) rules (b) Grammatical relation (GR) distance measures iv. Other features (a) Script length (b) Error-rate Word unigrams and bigrams are"
P11-1019,P98-1032,0,0.0645038,"s of the scripts, such as discourse cohesion or relevance to the prompt eliciting the text, that examiners will take into account. 5 Validity tests The practical utility of an AA system will depend strongly on its robustness to subversion by writers who understand something of its workings and attempt to exploit this to maximise their scores (independently of their underlying ability). Surprisingly, there is very little published data on the robustness of existing systems. However, Powers et al. (2002) invited writing experts to trick the scoring capabilities of an earlier version of e-Rater (Burstein et al., 1998). e-Rater (see Section 6 for more details) assigns a score to a text based on linguistic feature types extracted using relatively domain-specific techniques. Participants were given a description of these techniques as well as of the cue words that the system uses. The results showed that it was easier to fool the system into assigning higher than lower scores. Our goal here is to determine the extent to which knowledge of the feature types deployed poses a threat to the validity of our system, where certain text generation strategies may give rise to large positive discrepancies. As mentioned"
P11-1019,W03-0209,0,0.316874,"re labelled with a grade and unlabelled test texts are fitted to the same grade point scale via a regression step applied to the classifier output (see Section 6 for more details). Different techniques have been used, including cosine similarity of vectors representing text in various ways (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al., 2003), generative machine learning models (Rudner and Liang, 2002), domain-specific feature extraction (Attali and Burstein, 2006), and/or modified syntactic parsers (Lonsdale and Strong-Krause, 2003). A recent review identifies twelve different automated free-text scoring systems (Williamson, 2009). Examples include e-Rater (Attali and Burstein, 2006), Intelligent Essay Assessor (IEA) (Landauer et al., 2003), IntelliMetric (Elliot, 2003; Rudner et al., 2006) and Project Essay Grade (PEG) (Page, 2003). Several of these are now deployed in highstakes assessment of examination scripts. Although there are many published analyses of the perfor180 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 180–189, c Portland, Oregon, June 19-24, 2011. 2011 As"
P11-1019,C98-1032,0,\N,Missing
P16-1068,P06-4020,0,0.0160396,"the LSTM in a uni-directional manner (i.e., from left to right) might leave out important information about the sentence. For example, our 3 The maximum time for jointly training a particular SSWE + LSTM combination took about 55–60 hours on an Amazon EC2 g2.2xlarge instance (average time was 27–30 hours). 719 3.4 hidden layer back to the embedding matrix (i.e., we do not provide any pre-trained word embeddings).4 Other Baselines We train a Support Vector Regression model (see Section 4), which is one of the most widely used approaches in text scoring. We parse the data using the RASP parser (Briscoe et al., 2006) and extract a number of different features for assessing the quality of the essays. More specifically, we use character and part-of-speech unigrams, bigrams and trigrams; word unigrams, bigrams and trigrams where we replace open-class words with their POS; and the distribution of common nouns, prepositions, and coordinators. Additionally, we extract and use as features the rules from the phrase-structure tree based on the top parse for each sentence, as well as an estimate of the error rate based on manually-derived error rules. N grams are weighted using tf–idf, while the rest are count-base"
P16-1068,P13-1113,0,0.193059,"Missing"
P16-1068,W03-0209,0,0.0370904,"depends largely on the ability to examine their characteristics, whether they measure what is intended to be measured, and whether their internal marking criteria can be interpreted in a meaningful and useful way. The deep architecture of neural network models, however, makes it rather difficult to identify and extract those properties of text that the network has identified as discriminative. Therefore, we also describe a preliminary method for visualizing the information the model is exploiting when assigning a specific score to an input text. 2 the training set to which it is most similar. Lonsdale and Strong-Krause (2003) use the Link Grammar parser (Sleator and Templerley, 1995) to analyse and score texts based on the average sentencelevel scores calculated from the parser’s cost vector. The Bayesian Essay Test Scoring sYstem (Rudner and Liang, 2002) investigates multinomial and Bernoulli Naive Bayes models to classify texts based on shallow content and style features. eRater (Attali and Burstein, 2006), developed by the Educational Testing Service, was one of the first systems to be deployed for operational scoring in high-stakes assessments. The model uses a number of different features, including aspects o"
P16-1068,W15-0608,0,0.141078,"ssments. The model uses a number of different features, including aspects of grammar, vocabulary and style (among others), whose weights are fitted to a marking scheme by regression. Chen et al. (2010) use a voting algorithm and address text scoring within a weakly supervised bag-of-words framework. Yannakoudakis et al. (2011) extract deep linguistic features and employ a discriminative learning-to-rank model that outperforms regression. Recently, McNamara et al. (2015) used a hierachical classification approach to scoring, utilizing linguistic, semantic and rhetorical features, among others. Farra et al. (2015) utilize variants of logistic and linear regression and develop models that score persuasive essays based on features extracted from opinion expressions and topical elements. There have also been attempts to incorporate more diverse features to text scoring models. Klebanov and Flor (2013) demonstrate that essay scoring performance is improved by adding to the model information about percentages of highly associated, mildly associated and dis-associated pairs of words that co-exist in a given text. Somasundaran et al. (2014) exploit lexical chains and their interaction with discourse elements"
P16-1068,W15-0625,1,0.834457,"ns about where to next evaluate the function. The hyperparameters for our baselines were also determined using the same methodology. All models are trained on our training set (see Section 4), except the one prefixed ‘word2vecpre-trained ’ which uses pre-trained embeddings on the Google News Corpus. We report the Spearman’s rank correlation coefficient ρ, Pearson’s product-moment correlation coefficient r, and the root mean square error (RMSE) between the predicted scores and the gold standard on our test set, which are considered more appropriate metrics for evaluating essay scoring systems (Yannakoudakis and Cummins, 2015). However, we also report Cohen’s κ with quadratic weights, which was the evaluation metric used in the Kaggle competition. Performance of the models is shown in Table 1. In terms of correlation, SVMs produce competitive results (ρ = 0.78 and r = 0.77), outperforming doc2vec, LSTM and BLSTM, as well as their deep counterparts. As described 721 fair comparison as these are trained on a much larger corpus than our training set (which we use to train our models). Nevertheless, when we use our SSWEs models we are able to outperform ‘word2vecpre-trained + Two-layer BLSTM’, even though our embedding"
P16-1068,P11-1019,1,0.949718,"g, 2002) investigates multinomial and Bernoulli Naive Bayes models to classify texts based on shallow content and style features. eRater (Attali and Burstein, 2006), developed by the Educational Testing Service, was one of the first systems to be deployed for operational scoring in high-stakes assessments. The model uses a number of different features, including aspects of grammar, vocabulary and style (among others), whose weights are fitted to a marking scheme by regression. Chen et al. (2010) use a voting algorithm and address text scoring within a weakly supervised bag-of-words framework. Yannakoudakis et al. (2011) extract deep linguistic features and employ a discriminative learning-to-rank model that outperforms regression. Recently, McNamara et al. (2015) used a hierachical classification approach to scoring, utilizing linguistic, semantic and rhetorical features, among others. Farra et al. (2015) utilize variants of logistic and linear regression and develop models that score persuasive essays based on features extracted from opinion expressions and topical elements. There have also been attempts to incorporate more diverse features to text scoring models. Klebanov and Flor (2013) demonstrate that e"
P16-1068,N15-1111,0,0.0368613,"Missing"
P16-1068,C14-1090,0,0.0332949,"oring, utilizing linguistic, semantic and rhetorical features, among others. Farra et al. (2015) utilize variants of logistic and linear regression and develop models that score persuasive essays based on features extracted from opinion expressions and topical elements. There have also been attempts to incorporate more diverse features to text scoring models. Klebanov and Flor (2013) demonstrate that essay scoring performance is improved by adding to the model information about percentages of highly associated, mildly associated and dis-associated pairs of words that co-exist in a given text. Somasundaran et al. (2014) exploit lexical chains and their interaction with discourse elements for evaluating the quality of persuasive essays with respect to discourse coherence. Crossley et al. (2015) identify student attributes, such as standardized test scores, as predictive of writing success and use them in conjunction with textual features to develop essay scoring models. In 2012, Kaggle,2 sponsored by the Hewlett Foundation, hosted the Automated Student Assessment Prize (ASAP) contest, aiming to demonRelated Work In this section, we describe a number of the more influential and/or recent approaches in automate"
P16-1068,P15-1150,0,0.0120681,"Missing"
P16-1112,W13-1704,1,0.557473,"UI consists of different classifiers for each individual error type; and P1+P2+S1+S2 is a combination of four different error correction systems. In contrast, the Bi-LSTM is a single model for detecting all error types, and therefore represents a more scalable data-driven approach. 8 Essay Scoring In this section, we perform an extrinsic evaluation of the efficacy of the error detection system and examine the extent to which it generalises at higher levels of granularity on the task of automated essay scoring. More specifically, we replicate experiments using the text-level model described by Andersen et al. (2013), which is currently deployed in a self-assessment and tutoring system (SAT), an online automated writing feedback tool actively used by language learners.2 The SAT system predicts an overall score for a given text, which provides a holistic assessment of linguistic competence and language proficiency. The authors trained a supervised ranking perceptron model on the FCE-public dataset, using features such as error-rate estimates from a language model and various lexical and grammatical properties of text (e.g., word n-grams, part-of-speech n-grams and phrase-structure rules). We replicate this"
P16-1112,W14-4012,0,0.00617698,"Missing"
P16-1112,A00-2019,0,0.0998518,"nguistics to performance comparable to human annotators. 2 Background and Related Work The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser. The pilot Helping Our Own shared task (Dale and Kilgarriff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition"
P16-1112,W07-1604,0,0.065746,"ally, we integrate the error detection framework with a publicly deployed self-assessment system, leading 1181 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1181–1191, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to performance comparable to human annotators. 2 Background and Related Work The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outp"
P16-1112,C12-1038,0,0.0324911,"ly after the incorrect gap – this is motivated by the intuition that while this token is correct when considered in isolation, it is incorrect in the current context, as another token should have preceeded it. As the main evaluation measure for error detection we use F0.5 , which was also the measure adopted in the CoNLL-14 shared task on error correction (Ng et al., 2014). It combines both precision and recall, while assigning twice as much weight to precision, since accurate feedback is often more important than coverage in error detection applications (Nagata and Nakatani, 2010). Following Chodorow et al. (2012), we also report raw counts for predicted and correct tokens. Related evaluation measures, such as the M 2 -scorer (Ng et al., 2014) and the I-measure (Felice and where xt is the current input, ht−1 is the previous hidden state, bi and bf are biases, ct−1 is the previous internal state (referred to as the cell), and σ is the logistic function. The new internal state is calculated based on the current input and the previous hidden state, and then interpolated with the previous internal state using ft and it as weights: cet = tanh(Wc xt + Uc ht−1 + bc ) ct = ft ct−1 + it cet (10) (11) where is e"
P16-1112,W13-1703,0,0.186665,"Missing"
P16-1112,W11-2838,0,0.025034,"neff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser. The pilot Helping Our Own shared task (Dale and Kilgarriff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition errors (Rozovskaya et al., 2011). We extend this line of research, working towards general error detection systems, and investigate the use of neural compositional models on this task. The related area of grammatical error correction has also gained considerable momentum in the past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al"
P16-1112,W12-2006,0,0.022702,"iff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition errors (Rozovskaya et al., 2011). We extend this line of research, working towards general error detection systems, and investigate the use of neural compositional models on this task. The related area of grammatical error correction has also gained considerable momentum in the past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). The current state-of-the-art approaches can broadly be separated into two categories: 1. Phrase-based statistical machine translation techniques, essentially translating the incorrect source text into the corrected version (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) 2. Averaged Perceptrons and Naive Bayes classifiers making use of native-language error correction priors (Rozovskaya et al., 2014; Rozovskaya et al., 2013). Error correction systems require very specialised models, as they need to generate an improved version of the input text"
P16-1112,P15-1068,0,0.0366487,", this assumes that systems are able to propose a correction for every detected error, and accurate systems for correction might not be optimal for detection. While closed-class errors such as incorrect prepositions and determiners can be modeled with a supervised classification approach, content-content word errors are the 3rd most frequent error type and pose a serious challenge to error correction frameworks (Leacock et al., 2014; Kochmar and Briscoe, 2014). Evaluation of error correction is also highly subjective and human annotators have rather low agreement on gold-standard corrections (Bryant and Ng, 2015). Therefore, we treat error detection in learner writing as an independent task and propose a system for labeling each token as being correct or incorrect in context. Common approaches to similar sequence labeling tasks involve learning weights or probabilities for context n-grams of varying sizes, or relying on previously extracted high-confidence context patterns. Both of these methods can suffer from data sparsity, as they treat words as independent units and miss out on potentially related patterns. In addition, they need to specify a fixed context size and are therefore often limited to u"
P16-1112,N15-1060,0,0.0556486,"Missing"
P16-1112,W14-1702,1,0.788251,"orking towards general error detection systems, and investigate the use of neural compositional models on this task. The related area of grammatical error correction has also gained considerable momentum in the past years, with four recent shared tasks highlighting several emerging directions (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013; Ng et al., 2014). The current state-of-the-art approaches can broadly be separated into two categories: 1. Phrase-based statistical machine translation techniques, essentially translating the incorrect source text into the corrected version (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014) 2. Averaged Perceptrons and Naive Bayes classifiers making use of native-language error correction priors (Rozovskaya et al., 2014; Rozovskaya et al., 2013). Error correction systems require very specialised models, as they need to generate an improved version of the input text, whereas a wider range of tagging and classification models can be deployed on error detection. In addition, automated writing feedback systems that indicate the presence and location of errors may be better from a pedagogic point of view, rather than providing a panacea and cor"
P16-1112,W11-1422,0,0.0194899,"ecific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser. The pilot Helping Our Own shared task (Dale and Kilgarriff, 2011) also evaluated grammatical error detection of a number of different error types, though most systems were error-type specific and the best approach was heavily skewed towards article and preposition errors (Rozovskaya et al., 2011). We extend this line of research, working towards general error detection systems, and investigate the use of neural compositional models on this task. The related a"
P16-1112,han-etal-2004-detecting,0,0.0633033,"tion framework with a publicly deployed self-assessment system, leading 1181 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1181–1191, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to performance comparable to human annotators. 2 Background and Related Work The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency pars"
P16-1112,D14-1080,0,0.0428244,"guage modeling (Mikolov et al., 2011; Chelba et al., 2013), where they learn an incremental composition function for predicting the next token in the sequence. However, while language models can estimate the probability of each token, they are unable to differentiate between infrequent and incorrect token sequences. For error detection, the composition function needs to learn to identify semantic anomalies or ungrammatical combinations, independent of their frequency. The bidirectional model provides extra information, as it allows the network to use context on both sides of the target token. Irsoy and Cardie (2014) created an extension of this architecture by connecting together multiple layers of bidirectional Elman-type recurrent network modules. This deep bidirectional RNN (Figure 1d) calculates a context-dependent representation for each token using a bidirectional RNN, and then uses this as input to another bidirectional RNN. The multi-layer structure allows the model to learn more complex higher-level features and effectively perform multiple recurrent passes through the sentence. The long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is an advanced alternative to the Elman-type netw"
P16-1112,D13-1176,0,0.0109538,"s can suffer from data sparsity, as they treat words as independent units and miss out on potentially related patterns. In addition, they need to specify a fixed context size and are therefore often limited to using a small window near the target. Neural network models aim to address these weaknesses and have achieved success in various NLP tasks such as language modeling (Bengio et al., 2003) and speech recognition (Dahl et al., 2012). Recent developments in machine translation have also shown that text of varying length can be represented as a fixed-size vector using convolutional networks (Kalchbrenner and Blunsom, 2013; Cho et al., 2014a) or recurrent neural networks (Cho et al., 2014b; Bahdanau et al., 2015). In this paper, we present the first experiments using neural network models for the task of error detection in learner writing. We perform a systematic comparison of alternative compositional structures for constructing informative context representations. Based on the findings, we propose a novel framework for performing error detection in learner writing, which achieves state-of-the-art results on two datasets of errorannotated learner essays. The sequence labeling model creates a single variable-si"
P16-1112,C14-1164,0,0.179908,"rs has focussed on error correction, with error detection performance measured as a byproduct of the correction output (Ng et al., 2013; Ng et al., 2014). However, this assumes that systems are able to propose a correction for every detected error, and accurate systems for correction might not be optimal for detection. While closed-class errors such as incorrect prepositions and determiners can be modeled with a supervised classification approach, content-content word errors are the 3rd most frequent error type and pose a serious challenge to error correction frameworks (Leacock et al., 2014; Kochmar and Briscoe, 2014). Evaluation of error correction is also highly subjective and human annotators have rather low agreement on gold-standard corrections (Bryant and Ng, 2015). Therefore, we treat error detection in learner writing as an independent task and propose a system for labeling each token as being correct or incorrect in context. Common approaches to similar sequence labeling tasks involve learning weights or probabilities for context n-grams of varying sizes, or relying on previously extracted high-confidence context patterns. Both of these methods can suffer from data sparsity, as they treat words as"
P16-1112,P08-1021,0,0.0342,"ssment system, leading 1181 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1181–1191, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to performance comparable to human annotators. 2 Background and Related Work The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, string features, and outputs from a constituency parser. The pilot Helping Our Own shared task (Dale and Ki"
P16-1112,D15-1166,0,0.0241515,"Missing"
P16-1112,D14-1102,0,0.0446707,"Missing"
P16-1112,C08-1109,0,0.022685,"raining data to the model. Finally, we integrate the error detection framework with a publicly deployed self-assessment system, leading 1181 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 1181–1191, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics to performance comparable to human annotators. 2 Background and Related Work The field of automatically detecting errors in learner text has a long and rich history. Most work has focussed on tackling specific types of errors, such as usage of incorrect prepositions (Tetreault and Chodorow, 2008; Chodorow et al., 2007), articles (Han et al., 2004; Han et al., 2006), verb forms (Lee and Seneff, 2008), and adjective-noun pairs (Kochmar and Briscoe, 2014). However, there has been limited work on more general error detection systems that could handle all types of errors in learner text. Chodorow and Leacock (2000) proposed a method based on mutual information and the chi-square statistic to detect sequences of part-of-speech tags and function words that are likely to be ungrammatical in English. Gamon (2011) used Maximum Entropy Markov Models with a range of features, such as POS tags, s"
P16-1112,D15-1199,0,0.00494131,"ex higher-level features and effectively perform multiple recurrent passes through the sentence. The long-short term memory (LSTM) (Hochreiter and Schmidhuber, 1997) is an advanced alternative to the Elman-type networks that has recently become increasingly popular. It uses 1183 two separate hidden vectors to pass information between different time steps, and includes gating mechanisms for modulating its own output. LSTMs have been successfully applied to various tasks, such as speech recognition (Graves et al., 2013), machine translation (Luong et al., 2015), and natural language generation (Wen et al., 2015). Two sets of gating values (referred to as the input and forget gates) are first calculated based on the previous states of the network: more complex features and performing multiple passes through the sentence. For comparison with non-neural models, we also report results using CRFs (Lafferty et al., 2001), which are a popular choice for sequence labeling tasks. We trained the CRF++ 1 implementation on the same dataset, using as features unigrams, bigrams and trigrams in a 7-word window surrouding the target word (3 words before and after). The predicted label is also conditioned on the prev"
P16-1112,P11-1019,1,0.813156,"ral models, we also report results using CRFs (Lafferty et al., 2001), which are a popular choice for sequence labeling tasks. We trained the CRF++ 1 implementation on the same dataset, using as features unigrams, bigrams and trigrams in a 7-word window surrouding the target word (3 words before and after). The predicted label is also conditioned on the previous label in the sequence. it = σ(Wi xt + Ui ht−1 + Vf ct−1 + bi ) (8) 4 ft = σ(Wf xt + Uf ht−1 + Vf ct−1 + bf ) (9) We evaluate the alternative network structures on the publicly released First Certificate in English dataset (FCE-public, Yannakoudakis et al. (2011)). The dataset contains short texts, written by learners of English as an additional language in response to exam prompts eliciting freetext answers and assessing mastery of the upperintermediate proficiency level. The texts have been manually error-annotated using a taxonomy of 77 error types. We use the released test set for evaluation, containing 2,720 sentences, leaving 30,953 sentences for training. We further separate 2,222 sentences from the training set for development and hyper-parameter tuning. The dataset contains manually annotated error spans of various types of errors, together w"
P16-1112,N10-1018,0,\N,Missing
P16-1112,W14-1701,0,\N,Missing
P16-1112,W13-3602,0,\N,Missing
P16-1112,W14-1703,0,\N,Missing
P16-1112,W13-3601,0,\N,Missing
P16-1112,W11-2843,0,\N,Missing
P16-1112,C10-2103,0,\N,Missing
P19-1060,D18-1465,0,0.343665,"and entity transitions across sentences are used as features for coherence assessment. A large body of work has utilized and extended the egrid approach (Elsner and Charniak, 2008; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other features have also been leveraged, such as syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Lin et al., 2011; Feng et al., 2014). Deep learning architectures have also been successfully applied to the task of coherence scoring, achieving state-of-theart results (Li and Jurafsky, 2017; Logeswaran et al., 2018; Cui et al., 2018). Some have exploited To compose a sentence representation s, the hidw den states {hw 1 , ..., hn } of its words are combined with an attention mechanism: w w uw t = tanh(W ht ) exp(v w uw t ) aw t = P w uw ) exp(v t Xt w w s= a t ht t 630 (2) Figure 1: The hierarchical architecture of the STL and MTL models. The dotted red box is specific to the MTL framework. The dotted purple box is applied if the document contains paragraph boundaries (which is the case for the Grammarly Corpus in Section 4.1) in order to create paragraph representations prior to the document one. where W w and v w are lea"
P19-1060,W18-3701,0,0.0297911,"and prediction tasks, and demonstrate its effectiveness not only on standard binary evaluation coherence tasks, but also on real-world tasks involving the prediction of varying degrees of coherence, achieving a new state of the art. 1 Introduction Discourse coherence refers to the way textual units relate to one another and form a coherent whole. Coherence is an important aspect of text quality and therefore its modeling is essential in many NLP applications, including summarization (Barzilay et al., 2002; Parveen et al., 2016), question-answering (Verberne et al., 2007), question generation (Desai et al., 2018), and language assessment (Burstein et al., 2010; Somasundaran et al., 2014; Farag et al., 2018). A large body of work has investigated models for the assessment of inter-sentential coherence, that is, assessment in terms of transitions between adjacent sentences (Barzilay and Lapata, 2008; Yannakoudakis and Briscoe, 2012; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017; Joty et al., 2018). The properties of text that result in inter-sentential connectedness have been translated into a number of computational models – some of the most prominent ones include the entity-based approaches,"
P19-1060,P11-1100,0,0.181281,"Missing"
P19-1060,C14-1089,0,0.0807173,"95). In the egrid model, texts are represented as matrices of entities (columns) and sentences (rows). Entities in the matrix are represented by their grammatical role (i.e., subject, object, neither), and entity transitions across sentences are used as features for coherence assessment. A large body of work has utilized and extended the egrid approach (Elsner and Charniak, 2008; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other features have also been leveraged, such as syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Lin et al., 2011; Feng et al., 2014). Deep learning architectures have also been successfully applied to the task of coherence scoring, achieving state-of-theart results (Li and Jurafsky, 2017; Logeswaran et al., 2018; Cui et al., 2018). Some have exploited To compose a sentence representation s, the hidw den states {hw 1 , ..., hn } of its words are combined with an attention mechanism: w w uw t = tanh(W ht ) exp(v w uw t ) aw t = P w uw ) exp(v t Xt w w s= a t ht t 630 (2) Figure 1: The hierarchical architecture of the STL and MTL models. The dotted red box is specific to the MTL framework. The dotted purple box is applied if"
P19-1060,J95-2003,0,0.408881,"Missing"
P19-1060,D12-1106,0,0.0605553,"pata (2005, 2008) and inspired by Centering Theory (Grosz et al., 1995). In the egrid model, texts are represented as matrices of entities (columns) and sentences (rows). Entities in the matrix are represented by their grammatical role (i.e., subject, object, neither), and entity transitions across sentences are used as features for coherence assessment. A large body of work has utilized and extended the egrid approach (Elsner and Charniak, 2008; Burstein et al., 2010; Elsner and Charniak, 2011; Guinaudeau and Strube, 2013). Other features have also been leveraged, such as syntactic patterns (Louis and Nenkova, 2012) and discourse relations (Lin et al., 2011; Feng et al., 2014). Deep learning architectures have also been successfully applied to the task of coherence scoring, achieving state-of-theart results (Li and Jurafsky, 2017; Logeswaran et al., 2018; Cui et al., 2018). Some have exploited To compose a sentence representation s, the hidw den states {hw 1 , ..., hn } of its words are combined with an attention mechanism: w w uw t = tanh(W ht ) exp(v w uw t ) aw t = P w uw ) exp(v t Xt w w s= a t ht t 630 (2) Figure 1: The hierarchical architecture of the STL and MTL models. The dotted red box is speci"
P19-1060,P13-1010,0,0.590766,"ole. Coherence is an important aspect of text quality and therefore its modeling is essential in many NLP applications, including summarization (Barzilay et al., 2002; Parveen et al., 2016), question-answering (Verberne et al., 2007), question generation (Desai et al., 2018), and language assessment (Burstein et al., 2010; Somasundaran et al., 2014; Farag et al., 2018). A large body of work has investigated models for the assessment of inter-sentential coherence, that is, assessment in terms of transitions between adjacent sentences (Barzilay and Lapata, 2008; Yannakoudakis and Briscoe, 2012; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017; Joty et al., 2018). The properties of text that result in inter-sentential connectedness have been translated into a number of computational models – some of the most prominent ones include the entity-based approaches, inspired by Center629 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 629–639 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics egrid features in a CNN model aimed at capturing long range entity transitions (Tien Nguyen and Joty, 2017; Joty et al., 2018); furthe"
P19-1060,D16-1074,0,0.0221442,"two tasks. We assess the extent to which our framework generalizes to different domains and prediction tasks, and demonstrate its effectiveness not only on standard binary evaluation coherence tasks, but also on real-world tasks involving the prediction of varying degrees of coherence, achieving a new state of the art. 1 Introduction Discourse coherence refers to the way textual units relate to one another and form a coherent whole. Coherence is an important aspect of text quality and therefore its modeling is essential in many NLP applications, including summarization (Barzilay et al., 2002; Parveen et al., 2016), question-answering (Verberne et al., 2007), question generation (Desai et al., 2018), and language assessment (Burstein et al., 2010; Somasundaran et al., 2014; Farag et al., 2018). A large body of work has investigated models for the assessment of inter-sentential coherence, that is, assessment in terms of transitions between adjacent sentences (Barzilay and Lapata, 2008; Yannakoudakis and Briscoe, 2012; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017; Joty et al., 2018). The properties of text that result in inter-sentential connectedness have been translated into a number of compu"
P19-1060,P18-1052,0,0.470528,"ities are represented by their syntactic role in the sentence (e.g., subject, object). Current state-of-the-art deep learning adaptations of the entity-based framework involve the use of Convolutional Neural Networks (CNNs) over an entity-based representation of text to discriminate between a coherent document and its incoherent variants containing a random reordering of the document’s sentences (Tien Nguyen and Joty, 2017); as well as lexicalized counterparts of such models that further incorporate lexical information regarding the entities, thereby distinguishing between different entities (Joty et al., 2018). In contrast to existing approaches, we propose a more generalized framework that allows neural models to encode information about the types of grammatical roles all words in a sentence participate in, rather than focusing only on the roles of entities within a sentence. Inspired by recent advances in Multi-Task Learning (MTL) (Rei and Yannakoudakis, 2017; Sanh et al., 2018), we propose a simple, yet effective hierarchical model trained in a multi-task fashion that learns to perform two tasks: scoring a document’s discourse coherence and predicting the type of grammatical role (GR) of a depen"
P19-1060,D14-1162,0,0.0820947,"oo dataset. WSJ Yahoo Clinton Enron LSTM hidden dim hw hs hp 100 100 100 100 100 100 200 100 100 100 100 α β 0.7 1 1 1 0.3 0.1 0.1 0.2 Table 3: Model hypermarameters: w, s and p refer to word, sentence and paragraph hidden layers respectively; α is the main and β the secondary loss weight. ning, 2014) and obtain a total of 39 different types of Universal Dependencies and their subtypes (see Appendix A for the full list). For the MTLSOX model, we consider direct objects, indirect objects and subjects of passive verbs as objects (O). Our models are initialized with pre-trained GloVe embeddings (Pennington et al., 2014). We use minibatches of size 32, optimize the models using RMSProp (Tieleman and Hinton, 2012), and set the learning rate to 0.001. Dropout (Srivastava et al., 2014) is used for regularization with probability 0.5 and applied to the word embedding layer and the output of the Bi-LSTM sentence layer. Table 3 shows the different hyperparameters used for training.6 Training is done for 30 epochs and performance is monitored over the development set; the model with the highest performance (highest PRA on the synthetic data and highest classification accuracy on GCDC) on the development set is selec"
P19-1060,W18-5023,0,0.71751,"the authors’ source code.5 Local Coherence Model (LC). This model, initially proposed by Li and Hovy (2014), applies a window approach to assess a text’s local coherence. Sentences are encoded with a recurrent or recursive layer and a filter of weights is applied over each window of sentence vectors to extract “clique” scores that are aggregated to calculate the overall document coherence score. We use an improved variant that captures sentence representations via an LSTM and predicts an overall coherence score by averaging the local clique scores (Li and Jurafsky, 2017; Farag et al., 2018). Lai and Tetreault (2018) recently showed that the LC model achieves state-of-the-art results on the Clinton and Enron datasets. Models and Baselines CNN Egrid (Egrid CNNext ). We replicate the model proposed by Tien Nguyen and Joty (2017) using their source code.3 The authors generate entity-grid representations of texts (i.e., matrices 4 https://bitbucket.org/melsner/ browncoherence 5 https://ntunlpsg.github.io/project/ coherence/n-coh-acl18/ 2 https://github.com/aylai/GCDC-corpus 3 https://github.com/datienguyen/cnn_ coherence 633 Paragraph sequence (PARSEQ). Lai and Tetreault (2018) implemented a hierarchical neur"
P19-1060,W17-5004,1,0.813179,"g a random reordering of the document’s sentences (Tien Nguyen and Joty, 2017); as well as lexicalized counterparts of such models that further incorporate lexical information regarding the entities, thereby distinguishing between different entities (Joty et al., 2018). In contrast to existing approaches, we propose a more generalized framework that allows neural models to encode information about the types of grammatical roles all words in a sentence participate in, rather than focusing only on the roles of entities within a sentence. Inspired by recent advances in Multi-Task Learning (MTL) (Rei and Yannakoudakis, 2017; Sanh et al., 2018), we propose a simple, yet effective hierarchical model trained in a multi-task fashion that learns to perform two tasks: scoring a document’s discourse coherence and predicting the type of grammatical role (GR) of a dependent with its head. We take advantage of inductive transfer between these tasks by giving a supervision signal at the bottom layers of a network with respect to the types of GRs, and a supervision signal at the top layers with respect to document-level coherence. Our contributions are four-fold: (1) We propose a MTL approach to coherence assessment and com"
P19-1060,J06-4002,0,0.163071,"Missing"
P19-1060,N16-1082,0,0.0129447,"18) reported 0.885 PRA for their Egrid CNNlex , which we were unable to replicate using their code; however, this is still lower compared to our results. 9 We note that the low correlation is due to the nature of the task: binary evaluation rather than absolute scoring of coherence. 635 Figure 3: Visualization of the model’s gradients with respect to the input word embeddings for MTL and STL on the WSJ dev set. Words that contribute the most to coherence scoring (i.e., those with high gradient norms) are colored: the contribution of words decreases from dark red to lighter tones of orange. to Li et al. (2016)) to determine which words maximize the model’s prediction (more influential words should have higher gradient norms). Figure 3 presents example visualizations obtained with STL and MTL. We observe that for MTL, important words are those that are considered the center of attention: in the first example (top two sentences) where the document is about seats in the stock exchange, “seat” and “Seats” are considered more important than the subject entities. On the other hand, the STL model considers the subject of the first sentence (“The American Stock Exchange”) more important than the object “se"
P19-1060,C14-1090,0,0.0610703,"tandard binary evaluation coherence tasks, but also on real-world tasks involving the prediction of varying degrees of coherence, achieving a new state of the art. 1 Introduction Discourse coherence refers to the way textual units relate to one another and form a coherent whole. Coherence is an important aspect of text quality and therefore its modeling is essential in many NLP applications, including summarization (Barzilay et al., 2002; Parveen et al., 2016), question-answering (Verberne et al., 2007), question generation (Desai et al., 2018), and language assessment (Burstein et al., 2010; Somasundaran et al., 2014; Farag et al., 2018). A large body of work has investigated models for the assessment of inter-sentential coherence, that is, assessment in terms of transitions between adjacent sentences (Barzilay and Lapata, 2008; Yannakoudakis and Briscoe, 2012; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017; Joty et al., 2018). The properties of text that result in inter-sentential connectedness have been translated into a number of computational models – some of the most prominent ones include the entity-based approaches, inspired by Center629 Proceedings of the 57th Annual Meeting of the Associ"
P19-1060,D14-1218,0,0.0453305,"t with lexical information about the entities: they represent each entity with its lexical presentation and attach it to its syntactic role (S, O, X). For instance, if “Obama” appears as a subject and an object, there will be two different representations for it in the input embedding matrix: Obama-S and Obama-O. Joty et al. (2018) achieve state-of-the-art results on the WSJ, outperforming Egrid CNNext without including the three entity-specific features in their model. We also replicate their model using the authors’ source code.5 Local Coherence Model (LC). This model, initially proposed by Li and Hovy (2014), applies a window approach to assess a text’s local coherence. Sentences are encoded with a recurrent or recursive layer and a filter of weights is applied over each window of sentence vectors to extract “clique” scores that are aggregated to calculate the overall document coherence score. We use an improved variant that captures sentence representations via an LSTM and predicts an overall coherence score by averaging the local clique scores (Li and Jurafsky, 2017; Farag et al., 2018). Lai and Tetreault (2018) recently showed that the LC model achieves state-of-the-art results on the Clinton"
P19-1060,D17-1019,0,0.650691,"in a CNN model aimed at capturing long range entity transitions (Tien Nguyen and Joty, 2017; Joty et al., 2018); further details are provided in Section 4.2. Traditionally, coherence evaluation has been treated as a binary task, where a model is trained to distinguish between a coherent document and its incoherent counterparts created by randomly shuffling the sentences it contains. The news domain has been a popular source of well-written, coherent texts. Among the popular datasets are articles about EARTHQUAKES and AIRPLANES accidents (Barzilay and Lapata, 2008; Guinaudeau and Strube, 2013; Li and Jurafsky, 2017) and the Wall Street Journal (WSJ) portion of the Penn Treebank (Elsner and Charniak, 2008; Lin et al., 2011; Tien Nguyen and Joty, 2017). Elsner and Charniak (2008) argue that the WSJ documents are normal informative articles, whereas the AIRPLANES and EARTHQUAKES ones have a more constrained style. dependencies between the two prediction tasks and achieve state-of-the-art results in predicting document-level coherence; (2) We assess the extent to which the information encoded in the network generalizes to different domains and prediction tasks, and demonstrate the effectiveness of our approa"
P19-1060,P17-1121,0,0.334404,"z et al., 1995) and proposed in the pioneering work of Barzilay and Lapata (2005, 2008). Such approaches model local coherence in terms of entity transitions between adjacent sentences, where entities are represented by their syntactic role in the sentence (e.g., subject, object). Current state-of-the-art deep learning adaptations of the entity-based framework involve the use of Convolutional Neural Networks (CNNs) over an entity-based representation of text to discriminate between a coherent document and its incoherent variants containing a random reordering of the document’s sentences (Tien Nguyen and Joty, 2017); as well as lexicalized counterparts of such models that further incorporate lexical information regarding the entities, thereby distinguishing between different entities (Joty et al., 2018). In contrast to existing approaches, we propose a more generalized framework that allows neural models to encode information about the types of grammatical roles all words in a sentence participate in, rather than focusing only on the roles of entities within a sentence. Inspired by recent advances in Multi-Task Learning (MTL) (Rei and Yannakoudakis, 2017; Sanh et al., 2018), we propose a simple, yet effe"
P19-1060,W12-2004,1,0.818944,"ne another and form a coherent whole. Coherence is an important aspect of text quality and therefore its modeling is essential in many NLP applications, including summarization (Barzilay et al., 2002; Parveen et al., 2016), question-answering (Verberne et al., 2007), question generation (Desai et al., 2018), and language assessment (Burstein et al., 2010; Somasundaran et al., 2014; Farag et al., 2018). A large body of work has investigated models for the assessment of inter-sentential coherence, that is, assessment in terms of transitions between adjacent sentences (Barzilay and Lapata, 2008; Yannakoudakis and Briscoe, 2012; Guinaudeau and Strube, 2013; Tien Nguyen and Joty, 2017; Joty et al., 2018). The properties of text that result in inter-sentential connectedness have been translated into a number of computational models – some of the most prominent ones include the entity-based approaches, inspired by Center629 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 629–639 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics egrid features in a CNN model aimed at capturing long range entity transitions (Tien Nguyen and Joty, 201"
P19-1060,C00-2137,0,0.0199563,"ng the subject and object types using MTL or MTLSOX . We can see that learning to predict a larger set of GR types enhances the model’s predictive power for the subject and object types, corroborating the value of entity-based properties for coherence. Three-way Classification. On GCDC (Table 5) we can see that MTL achieves state-of-the-art performance across all three datasets. Although different evaluation metrics are employed, we note that the numbers obtained on this dataset are quite low compared to those on the WSJ. Assessing 7 Significance is calculated based on the randomization test (Yeh, 2000). 8 Joty et al. (2018) reported 0.885 PRA for their Egrid CNNlex , which we were unable to replicate using their code; however, this is still lower compared to our results. 9 We note that the low correlation is due to the nature of the task: binary evaluation rather than absolute scoring of coherence. 635 Figure 3: Visualization of the model’s gradients with respect to the input word embeddings for MTL and STL on the WSJ dev set. Words that contribute the most to coherence scoring (i.e., those with high gradient norms) are colored: the contribution of words decreases from dark red to lighter t"
P19-1060,N10-1099,0,\N,Missing
P19-1060,P11-2022,0,\N,Missing
P19-1060,J08-1001,0,\N,Missing
P19-1060,P08-2011,0,\N,Missing
P19-1060,D14-1082,0,\N,Missing
S17-1018,P14-2135,0,0.0807651,"direct object relations in the BNC. In case of the indirect object relations, the accompanying prepostions were discarded and the noun counts were aggregated. 5 Discussion and Data Analysis Our results show that the vision-based model outperforms the language-only model on our dataset. The difference in performance is particularly pronounced for the concrete verbs. For the abstract verbs in isolation, however, LING attains a higher 151 precision and recall. This is not surprising, as the visual information is better suited to capture the properties of concrete concepts than the abstract ones (Kiela et al., 2014). However, our results indicate that integrating linguistic and visual information provides a better overall model than the linguistic information alone. Our qualitative analysis of the data revealed a number of interesting trends. Some of the errors of both systems can be traced back to the clustering step. Different argument roles according to FrameNet are sometimes found in one cluster. For instance, both the killer and the victim are in the same cluster, as shown in Figure 2. However, it is also the case that one FrameNet role can be split into several clusters, e.g. the Victim role in the"
S17-1018,P15-2020,0,0.087498,"Missing"
S17-1018,D15-1015,0,0.0390626,"Missing"
S17-1018,P06-4020,0,0.00909538,"implemented in a lexicalsemantic resource called FrameNet (Fillmore et al., 2003). Each semantic frame is encoded in FrameNet as a list of lexical units that evoke this frame (typically verbs) and the roles that their semantic arguments may take given the scenario represented by the frame. FrameNet has inspired a direction in NLP research known as semantic role labelling (Gildea and Jurafsky, 2002; M`arquez et al., 2 Experimental Data Textual data. We extracted linguistic features for our model from the British National Corpus (BNC) (Burnard, 2007). We parsed the corpus using the RASP parser (Briscoe et al., 2006) and 149 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 149–154, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics graph as P = D−1 S, where theP degree matrix D is a diagonal matrix with Dii = N j=1 Sij . It then computes the K leading eigenvectors of P , where K is the desired number of clusters. The graph is partitioned by finding approximately equal elements in the eigenvectors using a simpler clustering algorithm, such as k-means. Meila and Shi (2001) have shown that the partition I derived in this way"
S17-1018,P12-1015,0,0.0367722,"tensor factorization model to identify argument fillers based on the role predictions and the predicate. To the best of our knowledge, ours is the first approach to this task exploiting visual data, in the form of image and video descriptions. 6 7 6.1 6.2 Multi-modal Methods in Semantics Visual data has been previously used to learn meaning representations that project multiple modalities into the same vector space. Semantic models integrating linguistic and visual information have been shown successful in tasks such as modeling semantic similarity and relatedness (Silberer and Lapata, 2014; Bruni et al., 2012), lexical entailment (Kiela et al., 2015a), compositionality (Roller and Schulte im Walde, 2013), bilingual lexicon induction (Kiela et al., 2015b) and metaphor identification (Shutova et al., 2016). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) show that large collections of image captions can be exploited for entailment tasks. Shutova et al. (2015) used image and video descriptions to induce verb selectional preferences enhanced with visual information. Relat"
S17-1018,N10-1137,0,0.0605021,"Missing"
S17-1018,J14-3006,0,0.0377643,"Missing"
S17-1018,J08-2001,0,0.0688829,"Missing"
S17-1018,D13-1115,0,0.244547,"Missing"
S17-1018,P12-2029,0,0.0431008,"Missing"
S17-1018,J02-3001,0,0.30793,"which can be mapped to higher-level semantic roles such as agent, patient, instrument etc. The verbs linked to this frame are buy, sell, pay, cost and charge, each evoking different aspects of the frame. This theory has been implemented in a lexicalsemantic resource called FrameNet (Fillmore et al., 2003). Each semantic frame is encoded in FrameNet as a list of lexical units that evoke this frame (typically verbs) and the roles that their semantic arguments may take given the scenario represented by the frame. FrameNet has inspired a direction in NLP research known as semantic role labelling (Gildea and Jurafsky, 2002; M`arquez et al., 2 Experimental Data Textual data. We extracted linguistic features for our model from the British National Corpus (BNC) (Burnard, 2007). We parsed the corpus using the RASP parser (Briscoe et al., 2006) and 149 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 149–154, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics graph as P = D−1 S, where theP degree matrix D is a diagonal matrix with Dii = N j=1 Sij . It then computes the K leading eigenvectors of P , where K is the desired number of cl"
S17-1018,N16-1020,1,0.86271,"ta, in the form of image and video descriptions. 6 7 6.1 6.2 Multi-modal Methods in Semantics Visual data has been previously used to learn meaning representations that project multiple modalities into the same vector space. Semantic models integrating linguistic and visual information have been shown successful in tasks such as modeling semantic similarity and relatedness (Silberer and Lapata, 2014; Bruni et al., 2012), lexical entailment (Kiela et al., 2015a), compositionality (Roller and Schulte im Walde, 2013), bilingual lexicon induction (Kiela et al., 2015b) and metaphor identification (Shutova et al., 2016). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) show that large collections of image captions can be exploited for entailment tasks. Shutova et al. (2015) used image and video descriptions to induce verb selectional preferences enhanced with visual information. Related Work Semantic Role Induction Conclusion We have presented a method for semantic frame induction from text, images and videos and shown that it operates with a high precision and recall. Although"
S17-1018,W06-1601,0,0.107857,"Missing"
S17-1018,P15-1092,1,0.896099,"Missing"
S17-1018,P14-1068,0,0.0591984,"eatures and a probabilistic tensor factorization model to identify argument fillers based on the role predictions and the predicate. To the best of our knowledge, ours is the first approach to this task exploiting visual data, in the form of image and video descriptions. 6 7 6.1 6.2 Multi-modal Methods in Semantics Visual data has been previously used to learn meaning representations that project multiple modalities into the same vector space. Semantic models integrating linguistic and visual information have been shown successful in tasks such as modeling semantic similarity and relatedness (Silberer and Lapata, 2014; Bruni et al., 2012), lexical entailment (Kiela et al., 2015a), compositionality (Roller and Schulte im Walde, 2013), bilingual lexicon induction (Kiela et al., 2015b) and metaphor identification (Shutova et al., 2016). Other applications of multimodal data include language modeling (Kiros et al., 2014) and knowledge mining from images (Chen et al., 2013; Divvala et al., 2014). Young et al. (2014) show that large collections of image captions can be exploited for entailment tasks. Shutova et al. (2015) used image and video descriptions to induce verb selectional preferences enhanced with visu"
S17-1018,D09-1067,0,0.0346383,"eneralise the predicateargument structure in semantic frames. Example clusters produced by our method are shown in Fig. 1. The resulting clusters represent frame elements, i.e. argument roles, in our model. Frame Induction Model Argument Clustering We use a clustering method to obtain semantic classes of arguments of verbs, thus generalising from individual arguments to their semantic types which correspond to frame roles. We obtain argument classes by means of spectral clustering of nouns with lexico-syntactic features, which has been shown effective in previous lexical classification tasks (Sun and Korhonen, 2009). Spectral clustering partitions the data relying on a similarity matrix that records similarities between all pairs of data points. We use JensenShannon divergence to measure similarity between feature vectors for two nouns, wi and wj , defined as follows: 1 1 dJS (wi , wj ) = dKL (wi ||m) + dKL (wj ||m), 2 2 (1) where dKL is the Kullback-Leibler divergence, and m is the average of wi and wj . We construct the similarity matrix S computing similarities Sij as Sij = exp(−dJS (wi , wj )). The matrix S then encodes a similarity graph G (over our nouns), where Sij are the adjacency weights. The c"
S17-1018,D11-1095,0,0.0232301,"ion that minimizes the cluster distortion, i.e. distances to its centroid. We clustered the 2,000 most frequent nouns in the BNC, using their grammatical relations as features. The features consisted of verb lemmas appearing in the subject, direct object and indirect object relations with the given nouns in the RASPparsed BNC, indexed by relation type. The feature vectors were first constructed from the corpus counts, and subsequently normalized by the sum of the feature values. Our use of linguistic dependency features for argument clustering is motivated by the results of previous research (Sun and Korhonen, 2011; Shutova et al., 2015), that has shown that such features lead to clusters of nouns belonging to the same semantic type, as opposed to topic or scene as it is the case with linguistic windowbased features or image-derived features (Shutova et al., 2015). Since the argument roles in semantic frames correspond to semantic types (such as location or instrument), the linguistic dependency features are best suited to generalise the predicateargument structure in semantic frames. Example clusters produced by our method are shown in Fig. 1. The resulting clusters represent frame elements, i.e. argum"
S17-1018,N15-1001,0,0.014369,"ked lower or not appearing at all. In contrast, the output of VIS encompases a range of situational roles, such as Instrument, Location, Time etc. The two models also sometimes differ in the roles that they identify. For instance, for the verb risk the VIS output is dominated by arguments of type Asset and the LING output by the arguments related to the Bad outcome role in FrameNet. Bayesian clustering based on Chinese Restaurant Process (Titov and Klementiev, 2012) and integer linear programming to incorporate semantic and structural constraints during clustering (Woodsend and Lapata, 2015). Titov and Khoddam (2015) proposed a reconstruction-error minimization approach using a log-linear model to predict roles given syntactic and lexical features and a probabilistic tensor factorization model to identify argument fillers based on the role predictions and the predicate. To the best of our knowledge, ours is the first approach to this task exploiting visual data, in the form of image and video descriptions. 6 7 6.1 6.2 Multi-modal Methods in Semantics Visual data has been previously used to learn meaning representations that project multiple modalities into the same vector space. Semantic models integratin"
S17-1018,E12-1003,0,0.0245883,"orical transfer. A common trend in the LING output is that it is dominated by the Agent and Theme roles, with situational roles (e.g. Location) typically ranked lower or not appearing at all. In contrast, the output of VIS encompases a range of situational roles, such as Instrument, Location, Time etc. The two models also sometimes differ in the roles that they identify. For instance, for the verb risk the VIS output is dominated by arguments of type Asset and the LING output by the arguments related to the Bad outcome role in FrameNet. Bayesian clustering based on Chinese Restaurant Process (Titov and Klementiev, 2012) and integer linear programming to incorporate semantic and structural constraints during clustering (Woodsend and Lapata, 2015). Titov and Khoddam (2015) proposed a reconstruction-error minimization approach using a log-linear model to predict roles given syntactic and lexical features and a probabilistic tensor factorization model to identify argument fillers based on the role predictions and the predicate. To the best of our knowledge, ours is the first approach to this task exploiting visual data, in the form of image and video descriptions. 6 7 6.1 6.2 Multi-modal Methods in Semantics Vis"
S17-1018,D15-1295,0,0.0124214,"e.g. Location) typically ranked lower or not appearing at all. In contrast, the output of VIS encompases a range of situational roles, such as Instrument, Location, Time etc. The two models also sometimes differ in the roles that they identify. For instance, for the verb risk the VIS output is dominated by arguments of type Asset and the LING output by the arguments related to the Bad outcome role in FrameNet. Bayesian clustering based on Chinese Restaurant Process (Titov and Klementiev, 2012) and integer linear programming to incorporate semantic and structural constraints during clustering (Woodsend and Lapata, 2015). Titov and Khoddam (2015) proposed a reconstruction-error minimization approach using a log-linear model to predict roles given syntactic and lexical features and a probabilistic tensor factorization model to identify argument fillers based on the role predictions and the predicate. To the best of our knowledge, ours is the first approach to this task exploiting visual data, in the form of image and video descriptions. 6 7 6.1 6.2 Multi-modal Methods in Semantics Visual data has been previously used to learn meaning representations that project multiple modalities into the same vector space."
S17-1018,J14-1002,0,\N,Missing
S17-1018,Q14-1006,0,\N,Missing
S19-2100,W18-5109,1,0.709828,"Related Work There has been much work characterising offensive online discourse including hate speech and cyberbullying (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Xu et al., 2013; Waseem et al., 2017; Ribeiro et al., 2018). This work also includes creating datasets for training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics A OFF OFF OFF OFF NOT All and other languages (e.g. Arabic (Mubarak et al., 2017), Chinese (Su et al., 2017), Slovene (Fiˇser et al., 2017)). Automated detection approaches have drawn on traditional document classification methods for spam detection and sentiment analysis, and tend to use lexical and syntactic features (Nobata et al., 2016; Li et al., 2017; Bourgonje et al., 2018). Machine learning"
S19-2100,D14-1179,0,0.0468349,"Missing"
S19-2100,L18-1008,0,0.0298282,"to the GBDT classifier. We provide details of each extension in the following sections. For each subtask, we experiment with combinations of the above and additionally tune the RNN type (between LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014)), dimension, and batch size, whether to use character ngrams (n ∈ [1, 4]), and, when used, the size of self-attention layers. We also run experiments using the unmodified model to find which pretrained embeddings give the best performance. We compare publicly available embeddings trained using Word2Vec (Mikolov et al., 2013), FastText (Mikolov et al., 2018), and GLoVe (Pennington et al., 2014). 4.1 and a further 1-dimensional dense layer. The final dense layer has either sigmoid or exponential activation, corresponding to soft or sharp attention respectively. The weights are normalised to sum to 1, yielding final attention values aei , which are used P to obtain the final sentential representation s = i aei hi . The RNN is then trained using categorical cross-entropy on s passed through a final tanh layer. 4.3 This modification includes the post-softmax output of the RNN as an additional input feature to the decision tree. 4.4 ELMo Self-attentio"
S19-2100,W17-3007,0,0.0223019,"Missing"
S19-2100,C18-1093,1,0.299803,"nstitute for Logic, Language and Computation, University of Amsterdam, Netherlands e.shutova@uva.nl Abstract one, but one with real world impact: if measures can be taken to identify and curtail trolling, the toxicity of the internet can to some extent be reduced. There is evidence that online harassment is connected with oppression, violence and suicide (Dinakar et al., 2011; Sood et al., 2012; Wulczyn et al., 2017), and there may moreover be reasons for concern about the perpetrator’s wellbeing along with that of the victims (Cheng et al., 2017). Our approach to the task extends the work of Mishra et al. (2018b), who extract features from tweets using an RNN for subsequent use in a gradient-boosted decision tree (GBDT) (Ke et al., 2017). Firstly, we experiment with changes to the RNN, including the use of self-attention (Rei and Søgaard, 2019) and ELMo embeddings (Peters et al., 2018). Secondly, we add additional features to the GBDT, including globally-optimised hashtag embeddings learned from a graph of tweet contents using node2vec (Grover and Leskovec, 2016). We show that this method of learning distributional information about hashtags improves performance over just learning their embeddings w"
S19-2100,W17-3013,0,0.0443658,"Missing"
S19-2100,W18-5101,1,0.388541,"nstitute for Logic, Language and Computation, University of Amsterdam, Netherlands e.shutova@uva.nl Abstract one, but one with real world impact: if measures can be taken to identify and curtail trolling, the toxicity of the internet can to some extent be reduced. There is evidence that online harassment is connected with oppression, violence and suicide (Dinakar et al., 2011; Sood et al., 2012; Wulczyn et al., 2017), and there may moreover be reasons for concern about the perpetrator’s wellbeing along with that of the victims (Cheng et al., 2017). Our approach to the task extends the work of Mishra et al. (2018b), who extract features from tweets using an RNN for subsequent use in a gradient-boosted decision tree (GBDT) (Ke et al., 2017). Firstly, we experiment with changes to the RNN, including the use of self-attention (Rei and Søgaard, 2019) and ELMo embeddings (Peters et al., 2018). Secondly, we add additional features to the GBDT, including globally-optimised hashtag embeddings learned from a graph of tweet contents using node2vec (Grover and Leskovec, 2016). We show that this method of learning distributional information about hashtags improves performance over just learning their embeddings w"
S19-2100,W17-3008,0,0.0259519,"training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics A OFF OFF OFF OFF NOT All and other languages (e.g. Arabic (Mubarak et al., 2017), Chinese (Su et al., 2017), Slovene (Fiˇser et al., 2017)). Automated detection approaches have drawn on traditional document classification methods for spam detection and sentiment analysis, and tend to use lexical and syntactic features (Nobata et al., 2016; Li et al., 2017; Bourgonje et al., 2018). Machine learning techniques range from logistic regression (Cheng et al., 2015) to support vector machines (Yin et al., 2009) to neural networks (Gamb¨ack and Sikdar, 2017). We draw on the work by Mishra and colleagues, who used a character-based recurrent neural network to form contextual word"
S19-2100,N13-1082,0,0.0285617,"nline texts, including those posted in discussion forums, news article comment sections, and social networks. Such detection is not straightforwardly a matter of identifying texts containing obscene words (Malmasi and Zampieri, 2018); offensiveness often arises from the context, current affairs, world knowledge, the use of acronyms and slang, and the identity of the authors and audience. Therefore the task is a challenging 2 Related Work There has been much work characterising offensive online discourse including hate speech and cyberbullying (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Xu et al., 2013; Waseem et al., 2017; Ribeiro et al., 2018). This work also includes creating datasets for training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Associatio"
S19-2100,D14-1162,0,0.0840716,"details of each extension in the following sections. For each subtask, we experiment with combinations of the above and additionally tune the RNN type (between LSTM (Hochreiter and Schmidhuber, 1997) and GRU (Cho et al., 2014)), dimension, and batch size, whether to use character ngrams (n ∈ [1, 4]), and, when used, the size of self-attention layers. We also run experiments using the unmodified model to find which pretrained embeddings give the best performance. We compare publicly available embeddings trained using Word2Vec (Mikolov et al., 2013), FastText (Mikolov et al., 2018), and GLoVe (Pennington et al., 2014). 4.1 and a further 1-dimensional dense layer. The final dense layer has either sigmoid or exponential activation, corresponding to soft or sharp attention respectively. The weights are normalised to sum to 1, yielding final attention values aei , which are used P to obtain the final sentential representation s = i aei hi . The RNN is then trained using categorical cross-entropy on s passed through a final tanh layer. 4.3 This modification includes the post-softmax output of the RNN as an additional input feature to the decision tree. 4.4 ELMo Self-attention The model proposed by Mishra et al."
S19-2100,N18-1202,0,0.144031,"Missing"
S19-2100,N19-1144,0,0.0368619,"gs in instances where words were deliberately obscured to evade detection. Following common practice in named entity recognition (Sang and De Meulder, 2003), where fine-grained labels are used to improve performance on the sequence labeling task, we take advantage of the hierarchical labels available for each tweet. For subtasks A and B we train a model to predict all cascading labels, and sum the probabilities of labels under the relevant class to make a final prediction. For example, for subtask A the Data The OffensEval shared task uses the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a), which hierarchically labels tweets according to whether or not they are offensive, whether any offence is targeted, and if so targeted at whom: an individual, a group or otherwise. The three subtasks in this shared task correspond to predicting labels at each level of granularity. The data is structured to allow this: all tweets presented in subtask B are guaranteed to be offensive, and all of those in subtask C are targeted. Tweets were collected by using the Twitter API to search for terms that are frequently associated with offensive behaviour. These included political keywords, as poli"
S19-2100,S19-2010,0,0.0283946,"gs in instances where words were deliberately obscured to evade detection. Following common practice in named entity recognition (Sang and De Meulder, 2003), where fine-grained labels are used to improve performance on the sequence labeling task, we take advantage of the hierarchical labels available for each tweet. For subtasks A and B we train a model to predict all cascading labels, and sum the probabilities of labels under the relevant class to make a final prediction. For example, for subtask A the Data The OffensEval shared task uses the Offensive Language Identification Dataset (OLID) (Zampieri et al., 2019a), which hierarchically labels tweets according to whether or not they are offensive, whether any offence is targeted, and if so targeted at whom: an individual, a group or otherwise. The three subtasks in this shared task correspond to predicting labels at each level of granularity. The data is structured to allow this: all tweets presented in subtask B are guaranteed to be offensive, and all of those in subtask C are targeted. Tweets were collected by using the Twitter API to search for terms that are frequently associated with offensive behaviour. These included political keywords, as poli"
S19-2100,W03-0419,0,0.229996,"Missing"
S19-2100,W17-3003,0,0.0202596,"n models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics A OFF OFF OFF OFF NOT All and other languages (e.g. Arabic (Mubarak et al., 2017), Chinese (Su et al., 2017), Slovene (Fiˇser et al., 2017)). Automated detection approaches have drawn on traditional document classification methods for spam detection and sentiment analysis, and tend to use lexical and syntactic features (Nobata et al., 2016; Li et al., 2017; Bourgonje et al., 2018). Machine learning techniques range from logistic regression (Cheng et al., 2015) to support vector machines (Yin et al., 2009) to neural networks (Gamb¨ack and Sikdar, 2017). We draw on the work by Mishra and colleagues, who used a character-based recurrent neural network to form contextual word representations of out-of-v"
S19-2100,W12-2103,0,0.0540799,"tic detection of offensive opinions expressed in online texts, including those posted in discussion forums, news article comment sections, and social networks. Such detection is not straightforwardly a matter of identifying texts containing obscene words (Malmasi and Zampieri, 2018); offensiveness often arises from the context, current affairs, world knowledge, the use of acronyms and slang, and the identity of the authors and audience. Therefore the task is a challenging 2 Related Work There has been much work characterising offensive online discourse including hate speech and cyberbullying (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Xu et al., 2013; Waseem et al., 2017; Ribeiro et al., 2018). This work also includes creating datasets for training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota,"
S19-2100,W17-3012,0,0.0306069,"uding those posted in discussion forums, news article comment sections, and social networks. Such detection is not straightforwardly a matter of identifying texts containing obscene words (Malmasi and Zampieri, 2018); offensiveness often arises from the context, current affairs, world knowledge, the use of acronyms and slang, and the identity of the authors and audience. Therefore the task is a challenging 2 Related Work There has been much work characterising offensive online discourse including hate speech and cyberbullying (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Xu et al., 2013; Waseem et al., 2017; Ribeiro et al., 2018). This work also includes creating datasets for training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational L"
S19-2100,N16-2013,0,0.0364426,"018); offensiveness often arises from the context, current affairs, world knowledge, the use of acronyms and slang, and the identity of the authors and audience. Therefore the task is a challenging 2 Related Work There has been much work characterising offensive online discourse including hate speech and cyberbullying (Warner and Hirschberg, 2012; Kwok and Wang, 2013; Xu et al., 2013; Waseem et al., 2017; Ribeiro et al., 2018). This work also includes creating datasets for training and evaluating detection models, for example the Hate Speech Twitter Annotations and Wikipedia Comments Corpora (Waseem and Hovy, 2016; Davidson et al., 2017; Wulczyn et al., 2017). Most work has been conducted on English data – tweets in particular – with some extensions to other domains (e.g. hacking forums (Caines et al., 2018)) 556 Proceedings of the 13th International Workshop on Semantic Evaluation (SemEval-2019), pages 556–563 Minneapolis, Minnesota, USA, June 6–7, 2019. ©2019 Association for Computational Linguistics A OFF OFF OFF OFF NOT All and other languages (e.g. Arabic (Mubarak et al., 2017), Chinese (Su et al., 2017), Slovene (Fiˇser et al., 2017)). Automated detection approaches have drawn on traditional docu"
W12-0206,P06-4020,1,0.882403,"of language attainment at different stages of learning. The English Profile (EP)2 research programme aims to enhance the learning, teaching and assessment of English as an additional language by creating detailed reference level descriptions of the language abilities expected at each level. As part of our research within that framework, we modify and combine techniques developed for information visualisation with methodologies from computational linguistics to support a novel and more empirical perspective on CEFR 3 Briscoe et al. (2010) POS tagged and parsed the data using the RASP toolkit (Briscoe et al., 2006). POS tags are based on the CLAWS tagset. 1 http://www.coe.int/t/dg4/linguistic/cadre en.asp 2 http://www.englishprofile.org/ 35 Proceedings of the EACL 2012 Joint Workshop of LINGVIS & UNCLH, pages 35–43, c Avignon, France, April 23 - 24 2012. 2012 Association for Computational Linguistics Feature Example VM RR (POS bigram: +) could clearly , because (word bigram: −) , because of necessary (word unigram: +) it is necessary that the people (word bigram: −) *the people are clever VV∅ VV∅ (POS bigram: −) *we go see film NN2 VVG (POS bigram: +) children smiling tween features can rapidly grow and"
W12-0206,P11-2053,0,0.0181983,"ary collections of items, such as a linguistic parse tree. VisLink provides a general platform within which multiple visualisations of language (e.g., a force-directed graph and a radial graph) can be connected, cross-queried and compared. Moreover, he explores the space of content analysis. DocuBurst is an interactive visualisation of document content, which spatially organizes words using an expert-created ontology (e.g., WordNet). Parallel Tag Clouds combine keyword extraction and coordinated visualisations to provide comparative overviews across subsets of a faceted text corpus. Recently, Rohrdantz et al. (2011) proposed a new approach to detecting and investigating changes in word senses by visually modelling and plotting aggregated views about the diachronic development in word contexts. Visualisation techniques have been successfully used in other areas including the humanities (e.g., Plaisant et al. (2006) and Don et al. (2007)), as well as genomics (e.g., Meyer et al. (2010a) and Meyer et al. (2010b)). For example, Meyer 41 the development of the tool and was eager to use and test it. There were dozens of meetings over a period of seven months, and the feedback on early interfaces was incorporat"
W12-0206,P11-1019,1,0.849506,"weight; + and − show their association with either passing or failing scripts. tions we describe in detail the visualiser, illustrate how it can support the investigation of individual features, and discuss how such investigations can shed light on the relationships between features and developmental aspects of learner grammars. To the best of our knowledge, this is the first attempt to visually analyse as well as perform a linguistic interpretation of discriminative features that characterise learner English. We also apply our visualiser to a set of 1,244 publicallyavailable FCE ESOL texts (Yannakoudakis et al., 2011) and make it available as a web service to other researchers5 . 2 Dataset We use texts produced by candidates taking the FCE exam, which assesses English at an upperintermediate level. The FCE texts, which are part of the Cambridge Learner Corpus6 , are produced by English language learners from around the world sitting Cambridge Assessment’s ESOL examinations7 . The texts are manually tagged with information about linguistic errors (Nicholls, 2003) and linked to meta-data about the learners (e.g., age and native language) and the exam (e.g., grade). 3 The English Profile visualiser 3.1 Basic"
W12-2004,W07-0607,0,0.0685571,"tom /item3646603/ texts under the framework of AA. Most of the methods we investigate require syntactic analysis. As in Yannakoudakis et al. (2011), we analyze all texts using the RASP toolkit (Briscoe et al., 2006)4 . vocabulary. We thus assess the minimum, maximum and average word length as a superficial proxy for coherence. 4.1 We explore the utility of inter-sentential feature types for assessing discourse coherence. Among the features used in Yannakoudakis et al. (2011), none explicitly captures coherence and none models intersentential relationships. Incremental Semantic analysis (ISA) (Baroni et al., 2007) is a word-level distributional model that induces a semantic space from input texts. ISA is a fully-incremental variation of Random Indexing (RI) (Sahlgren, 2005), which can efficiently capture second-order effects in common with other dimensionality-reduction methods based on singular value decomposition, but does not rely on stoplists or global statistics for weighting purposes. Utilizing the S-Space package (Jurgens and Stevens, 2010), we trained an ISA model5 using a subset of ukWaC (Ferraresi et al., 2008), a large corpus of English containing more than 2 billion tokens. We used the POS"
W12-2004,J08-1001,0,0.0608695,"tein (2007) use RI to determine the semantic similarity between sentences of same/different discourse segments (e.g., from the essay thesis and conclusion, or between sentences and the essay prompt), and assess the percentage of sentences that are correctly classified as related or unrelated. The main differences from our approach are that we assess the utility of semantic space models for predicting the overall grade for a text, in contrast to binary classification at the sentence-level, and we use ISA rather than RI7 . 4.3 Entity-based Coherence The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al., 1995). Local coherence is modeled on the basis of sequences of entity mentions that are labeled with their syntactic roles (e.g., subject, object). We construct the entity grids using the Brown Coherence Toolkit8,9 (Elsner and Charniak, 2011b), and use as features the probabilities of different entity transition types, defined in terms of their role in adjacent sentences10 . Burstein et al. (2010) show how the entity-grid ca"
W12-2004,N04-1015,0,0.0945124,"Missing"
W12-2004,P06-4020,1,0.814619,"us on the development and evaluation of (automated) methods for assessing coherence in learner 1 Powers et al. (2002) report the results of a related experiment with the AA system e-Rater, in which experts tried to subvert the system by submitting essays they believed would be inaccurately scored. 2 http://ilexir.co.uk/applications/clc-fce-dataset/ 3 http://www.cup.cam.ac.uk/gb/elt/catalogue/subject/custom /item3646603/ texts under the framework of AA. Most of the methods we investigate require syntactic analysis. As in Yannakoudakis et al. (2011), we analyze all texts using the RASP toolkit (Briscoe et al., 2006)4 . vocabulary. We thus assess the minimum, maximum and average word length as a superficial proxy for coherence. 4.1 We explore the utility of inter-sentential feature types for assessing discourse coherence. Among the features used in Yannakoudakis et al. (2011), none explicitly captures coherence and none models intersentential relationships. Incremental Semantic analysis (ISA) (Baroni et al., 2007) is a word-level distributional model that induces a semantic space from input texts. ISA is a fully-incremental variation of Random Indexing (RI) (Sahlgren, 2005), which can efficiently capture"
W12-2004,N10-1099,0,0.191056,"ence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al., 1995). Local coherence is modeled on the basis of sequences of entity mentions that are labeled with their syntactic roles (e.g., subject, object). We construct the entity grids using the Brown Coherence Toolkit8,9 (Elsner and Charniak, 2011b), and use as features the probabilities of different entity transition types, defined in terms of their role in adjacent sentences10 . Burstein et al. (2010) show how the entity-grid can be used to discriminate highcoherence from low-coherence learner texts. The main difference with our approach is that we evaluate the entity-grid model in the context of AA text grading, rather than binary classification. 7 We also used RI in addition to ISA, and found that it did not yield significantly different results. In particular, we trained a RI model with 2,000 dimensions and a context window of 3 on the same ukWaC data. Below we only report results for the fully-incremental ISA model. 8 https://bitbucket.org/melsner/browncoherence 9 The tool does not per"
W12-2004,E09-1017,0,0.0262547,"si+1 as the maximum cosine similarity between the history vectors of the words they contain. The overall coherence of a text T is then measured by taking the mean of all sentence-pair scores: Pn−1 maxk,j sim(ski , sji+1 ) coherence(T ) = i=1 (1) n−1 ‘Superficial’ Proxies In this section we introduce diverse classes of ‘superficial’ cohesive features that serve as proxies for coherence. Surface text properties have been assessed in the framework of automatic summary evaluation (Pitler et al., 2010), and have been shown to significantly correlate with the fluency of machinetranslated sentences (Chae and Nenkova, 2009). 4.1.1 Part-of-Speech (POS) Distribution The AA system described in Yannakoudakis et al. (2011) exploited features based on POS tag sequences, but did not consider the distribution of POS types across grades. In coherent texts, constituent clauses and sentences are related and depend on each other for their interpretation. Anaphors such as pronouns link the current sentence to those where the entities were previously mentioned. Pronouns can be directly related to (lack of) coherence and make intuitive sense as cohesive devices. We compute the number of pronouns in a text and use it as a shall"
W12-2004,E09-1018,0,0.0615917,"e trained a RI model with 2,000 dimensions and a context window of 3 on the same ukWaC data. Below we only report results for the fully-incremental ISA model. 8 https://bitbucket.org/melsner/browncoherence 9 The tool does not perform full coreference resolution; instead, coreference is approximated by linking entities that share a head noun. 10 We represent entities with specified roles (Subject, Object, Neither, Absent), use transition probabilities of length 2, 3 and 4, and a salience option of 2. 36 4.4 Pronoun Coreference Model Pronominal anaphora is another important aspect of coherence. Charniak and Elsner (2009) present an unsupervised generative model of pronominal anaphora for coherence modeling. In their implementation, they model each pronoun as generated by an antecedent somewhere in the previous two sentences. If a ‘good’ antecedent is found, the probability of a pronoun will be high; otherwise, the probability will be low. The overall probability of a text is then calculated as the probability of the resulting sequence of pronoun assignments. In our experiments, we use the pre-trained model distributed by Charniak and Elsner (2009) for news text to estimate the probability of a text and includ"
W12-2004,P08-2011,0,0.462645,"ntential discourse relations between textual units (Halliday and Hasan, 1976). Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues. Coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. There is a large body of work that has investigated a number of different coherence models on news texts (e.g., Lin et al. (2011), Elsner and Charniak (2008), and Soricut and Marcu (2006)). Recently, Pitler et al. (2010) presented a detailed survey of current techniques in coherence analysis of extractive summaries. To date, however, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. Coherence quality is typically present in marking criteria for evaluating learner texts, and it is iden33 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pag"
W12-2004,P11-1118,0,0.160474,"l grade for a text, in contrast to binary classification at the sentence-level, and we use ISA rather than RI7 . 4.3 Entity-based Coherence The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al., 1995). Local coherence is modeled on the basis of sequences of entity mentions that are labeled with their syntactic roles (e.g., subject, object). We construct the entity grids using the Brown Coherence Toolkit8,9 (Elsner and Charniak, 2011b), and use as features the probabilities of different entity transition types, defined in terms of their role in adjacent sentences10 . Burstein et al. (2010) show how the entity-grid can be used to discriminate highcoherence from low-coherence learner texts. The main difference with our approach is that we evaluate the entity-grid model in the context of AA text grading, rather than binary classification. 7 We also used RI in addition to ISA, and found that it did not yield significantly different results. In particular, we trained a RI model with 2,000 dimensions and a context window of 3 o"
W12-2004,P11-2022,0,0.339282,"l grade for a text, in contrast to binary classification at the sentence-level, and we use ISA rather than RI7 . 4.3 Entity-based Coherence The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al., 1995). Local coherence is modeled on the basis of sequences of entity mentions that are labeled with their syntactic roles (e.g., subject, object). We construct the entity grids using the Brown Coherence Toolkit8,9 (Elsner and Charniak, 2011b), and use as features the probabilities of different entity transition types, defined in terms of their role in adjacent sentences10 . Burstein et al. (2010) show how the entity-grid can be used to discriminate highcoherence from low-coherence learner texts. The main difference with our approach is that we evaluate the entity-grid model in the context of AA text grading, rather than binary classification. 7 We also used RI in addition to ISA, and found that it did not yield significantly different results. In particular, we trained a RI model with 2,000 dimensions and a context window of 3 o"
W12-2004,P11-1030,0,0.0476069,"Missing"
W12-2004,J95-2003,0,0.398328,"essay prompt), and assess the percentage of sentences that are correctly classified as related or unrelated. The main differences from our approach are that we assess the utility of semantic space models for predicting the overall grade for a text, in contrast to binary classification at the sentence-level, and we use ISA rather than RI7 . 4.3 Entity-based Coherence The entity-based coherence model, proposed by Barzilay and Lapata (2008), is one of the most popular statistical models of inter-sentential coherence, and learns coherence properties similar to those employed by Centering Theory (Grosz et al., 1995). Local coherence is modeled on the basis of sequences of entity mentions that are labeled with their syntactic roles (e.g., subject, object). We construct the entity grids using the Brown Coherence Toolkit8,9 (Elsner and Charniak, 2011b), and use as features the probabilities of different entity transition types, defined in terms of their role in adjacent sentences10 . Burstein et al. (2010) show how the entity-grid can be used to discriminate highcoherence from low-coherence learner texts. The main difference with our approach is that we evaluate the entity-grid model in the context of AA te"
W12-2004,N04-1024,0,0.192101,"ng this coherence score, as well as the maximum 5 The parameters of our ISA model are fairly standard: 1800 dimensions, a context window of 3 words, impact rate i = 0.0003 and decay rate km = 50. 6 We exclude articles, conjunctions, prepositions and auxiliary verbs from the calculation of sentence similarity. sim value found over the entire text, to the vectors of features associated with a text. The hypothesis is that the degree of semantic relatedness between adjoining sentences serves as a proxy for local discourse coherence; that is, coherent text units contain semantically-related words. Higgins et al. (2004) and Higgins and Burstein (2007) use RI to determine the semantic similarity between sentences of same/different discourse segments (e.g., from the essay thesis and conclusion, or between sentences and the essay prompt), and assess the percentage of sentences that are correctly classified as related or unrelated. The main differences from our approach are that we assess the utility of semantic space models for predicting the overall grade for a text, in contrast to binary classification at the sentence-level, and we use ISA rather than RI7 . 4.3 Entity-based Coherence The entity-based coherenc"
W12-2004,P10-4006,0,0.0121557,"eatures used in Yannakoudakis et al. (2011), none explicitly captures coherence and none models intersentential relationships. Incremental Semantic analysis (ISA) (Baroni et al., 2007) is a word-level distributional model that induces a semantic space from input texts. ISA is a fully-incremental variation of Random Indexing (RI) (Sahlgren, 2005), which can efficiently capture second-order effects in common with other dimensionality-reduction methods based on singular value decomposition, but does not rely on stoplists or global statistics for weighting purposes. Utilizing the S-Space package (Jurgens and Stevens, 2010), we trained an ISA model5 using a subset of ukWaC (Ferraresi et al., 2008), a large corpus of English containing more than 2 billion tokens. We used the POS tagger lexicon provided with the RASP system to discard documents whose proportion of valid English words to total words is less than 0.4; 78,000 documents were extracted in total and were then preprocessed replacing URLs, email addresses, IP addresses, numbers and emoticons with special markers. To measure local coherence we define the similarity between two sentences si and si+1 as the maximum cosine similarity between the history vecto"
W12-2004,P11-1100,0,0.0850322,"l primarily suprasentential discourse relations between textual units (Halliday and Hasan, 1976). Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues. Coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. There is a large body of work that has investigated a number of different coherence models on news texts (e.g., Lin et al. (2011), Elsner and Charniak (2008), and Soricut and Marcu (2006)). Recently, Pitler et al. (2010) presented a detailed survey of current techniques in coherence analysis of extractive summaries. To date, however, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. Coherence quality is typically present in marking criteria for evaluating learner texts, and it is iden33 The 7th Workshop on the Innovative Use of NLP for Building E"
W12-2004,P00-1056,0,0.0155456,"idered to be coreferent. semantic space models such as ISA or RI (discussed above), this method models the intuition that local coherence is signaled by the identification of word co-occurrence patterns across adjacent sentences. We compute two features introduced by Soricut and Marcu (2006): the forward likelihood and the backward likelihood. The first refers to the likelihood of observing the words in sentence si+1 conditioned on si , and the latter to the likelihood of observing the words in si conditioned on si+1 . We extract 3 million adjacent sentences from ukWaC12 , and use the GIZA++ (Och and Ney, 2000) implementation of IBM model 1 to obtain the probabilities of recurring patterns. The forward and backward probabilities are calculated over the entire text, and their values are used as features in our feature vectors13 . We further extend the above model and incorporate syntactic aspects of text coherence by training on POS tags instead of lexical items. We try to model the intuition that local coherence is signaled by the identification of POS co-occurrence patterns across adjacent sentences, where the use of certain POS tags in a sentence tends to trigger the use of other POS tags in an ad"
W12-2004,P10-1056,0,0.568577,"n, 1976). Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues. Coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. There is a large body of work that has investigated a number of different coherence models on news texts (e.g., Lin et al. (2011), Elsner and Charniak (2008), and Soricut and Marcu (2006)). Recently, Pitler et al. (2010) presented a detailed survey of current techniques in coherence analysis of extractive summaries. To date, however, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. Coherence quality is typically present in marking criteria for evaluating learner texts, and it is iden33 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 33–43, c Montr´eal, Canada, June 3-8, 2012. 2012 Association"
W12-2004,P06-2103,0,0.359085,"ween textual units (Halliday and Hasan, 1976). Cohesion is not the only mechanism of discourse coherence, which may also be inferred from meaning without presence of explicit linguistic cues. Coherence can be assessed locally in terms of transitions between adjacent clauses, parentheticals, and other textual units capable of standing in discourse relations, or more globally in terms of the overall topical coherence of text passages. There is a large body of work that has investigated a number of different coherence models on news texts (e.g., Lin et al. (2011), Elsner and Charniak (2008), and Soricut and Marcu (2006)). Recently, Pitler et al. (2010) presented a detailed survey of current techniques in coherence analysis of extractive summaries. To date, however, few attempts have been made to develop new methods and validate existing ones for automatic evaluation of discourse coherence and cohesion in the noisy domain of learner texts, where spelling and grammatical errors are common. Coherence quality is typically present in marking criteria for evaluating learner texts, and it is iden33 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 33–43, c Montr´eal, Canada,"
W12-2004,P11-1019,1,0.0513038,"systems of English learner text assign grades based on textual features which attempt to balance evidence of writing competence against evidence of performance errors. Previous work has mostly treated AA as a supervised text classification or regression task. A number of techniques have been investigated, including cosine similarity of feature vectors (Attali and Burstein, 2006), often combined with dimensionality reduction techniques such as Latent Semantic Analysis (LSA) (Landauer et al., 2003), and generative machine learning models (Rudner and Liang, 2002) as well as discriminative ones (Yannakoudakis et al., 2011). As multiple factors influence the linguistic quality of texts, such systems exploit features that correspond to different properties of texts, such as grammar, style, vocabulary usage, topic similarity, and discourse coherence and cohesion. Cohesion refers to the use of explicit linguistic cohesive devices (e.g., anaphora, lexical semantic relatedness, discourse markers, etc.) within a text that can signal primarily suprasentential discourse relations between textual units (Halliday and Hasan, 1976). Cohesion is not the only mechanism of discourse coherence, which may also be inferred from m"
W12-2004,J93-2003,0,\N,Missing
W13-1704,P06-4020,0,0.0184766,"st set (Yannakoudakis et al., 2011). Our best configuration is model b, which achieves the highest results according to most evaluation measures with a feature space consisting of 1) error counts identified through the absence of word trigrams in a large background corpus, 2) phrasestructure rules, 3) presence of frequent errors, as well as the number of words defining an error, as described in Section 2.3, 4) the presence of main verbs, nouns, adjectives, subordinating conjuctions and adverbs, 5) affixes and 6) the presence of clausal subjects and modifiers. The texts were parsed using RASP (Briscoe et al., 2006). Model a, the script-level model, does not work as well at the sentence level. However, it does perform better when evaluated against script-level scores (rs and ρs ), and this is expected given that it is trained directly on gold script-level scores. On the other hand, this evaluation measure is not as indicative of good performance in our application as the others, as it does not take into account the varying quality of individual sentences within a script. Training the script-level model with different feature sets (including those utilised in the sentencelevel model) did not yield an impr"
W13-1704,W12-2011,0,0.0435515,"Missing"
W13-1704,N04-1024,0,0.0507713,"n to the difficulty in acquiring annotated data, since rating a response sentence by sentence is not something examiners typically do and would therefore require an additional and expensive manual annotation effort. Previous work has primarily focused on automatic content scoring of short answers, ranging from a few words to a few sentences (Pulman and Sukkarieh, 2005; Attali et al., 2008; Mohler et al., 2011; Ziai et al., 2012). On the other hand, scoring of individual sentences with respect to their linguistic quality, specifically in learner texts, has received considerably less attention. Higgins et al. (2004) devised guidelines for the manual annotation of sentences in learner texts, and evaluated a rule-based approach that classifies sentences with respect to clarity of expression based on grammar, mechanics and word usage errors; however, their system performs binary classification, whereas we are focusing on scoring sentences. Writing instruction tools, such as Criterion (Burstein et al., 2003), give advice on stylistic and organisational issues and automatically detect a variety of errors in the text, though they do not explicitly allow for an overall evaluation of sentences with respect to va"
W13-1704,P11-1076,0,0.018846,"t. The challenge of assessing intra-sentential quality lies in the limited linguistic evidence that can be extracted automatically from relatively short sentences for them to be assessed reliably, in addition to the difficulty in acquiring annotated data, since rating a response sentence by sentence is not something examiners typically do and would therefore require an additional and expensive manual annotation effort. Previous work has primarily focused on automatic content scoring of short answers, ranging from a few words to a few sentences (Pulman and Sukkarieh, 2005; Attali et al., 2008; Mohler et al., 2011; Ziai et al., 2012). On the other hand, scoring of individual sentences with respect to their linguistic quality, specifically in learner texts, has received considerably less attention. Higgins et al. (2004) devised guidelines for the manual annotation of sentences in learner texts, and evaluated a rule-based approach that classifies sentences with respect to clarity of expression based on grammar, mechanics and word usage errors; however, their system performs binary classification, whereas we are focusing on scoring sentences. Writing instruction tools, such as Criterion (Burstein et al.,"
W13-1704,C10-2103,0,0.079084,"imes and at least ninety per cent of the times they occur. This way, rules can be extracted from the existing error annotation in the corpus, obviating the need for manually constructed malrules, although the rules obtained by the two different methods may to some extent be complementary. In addition to corpus-derived rules, many classes of incorrect but plausible derivational and inflectional morphology are detected by means of rules derived from a machine-readable dictionary. Many mistakes are still not detected, but precision has been found to be more important in terms of learning effect (Nagata and Nakatani, 2010), and errors missed by this module will often give lower sentence scores. Figure 3 illustrates some types of error detected by the system. The feedback text is generated from a small number of templates corresponding to different categories of error marked up in the CLC. We are currently working on extending this part of the system with more general rules in addition to word n-grams, e.g., part-of-speech tags and grammatical relations, in order to detect more errors without loss in precision. 3 Trials After the SAT system had been developed, a series of trials were set up in order to test the"
W13-1704,W05-0202,0,0.0124168,"dividual sentences, independently of their context. The challenge of assessing intra-sentential quality lies in the limited linguistic evidence that can be extracted automatically from relatively short sentences for them to be assessed reliably, in addition to the difficulty in acquiring annotated data, since rating a response sentence by sentence is not something examiners typically do and would therefore require an additional and expensive manual annotation effort. Previous work has primarily focused on automatic content scoring of short answers, ranging from a few words to a few sentences (Pulman and Sukkarieh, 2005; Attali et al., 2008; Mohler et al., 2011; Ziai et al., 2012). On the other hand, scoring of individual sentences with respect to their linguistic quality, specifically in learner texts, has received considerably less attention. Higgins et al. (2004) devised guidelines for the manual annotation of sentences in learner texts, and evaluated a rule-based approach that classifies sentences with respect to clarity of expression based on grammar, mechanics and word usage errors; however, their system performs binary classification, whereas we are focusing on scoring sentences. Writing instruction t"
W13-1704,P11-1019,1,0.667281,"urstein, 2006; Briscoe et al., 2010). Existing systems, overviews of which have been published in various studies (Dikli, 2006; Williamson, 2009; Shermis and Hamner, 2012), involve a large range of techniques, such as discriminative and generative machine learning, clustering algorithms and vectorial semantics, as well as syntactic parsers. We approach automated text assessment as a supervised machine learning problem, which enables us to take advantage of existing annotated data. We use the publically-available First Certificate in English (FCE) dataset of upper-intermediate learner English (Yannakoudakis et al., 2011) and focus on assessing general linguistic competence. Systems that measure English competence directly are easier and faster to deploy, since they are more likely to be reusable and generalise better across different genres than topic-specific ones, which are not immediately usable when new tasks are added, since the model cannot be applied until a substantial amount of manually annotated responses have been collected for a specific prompt. Following previous research, we employ discriminative ranking, which has been shown to achieve state-of-the-art results on the task of assessing free-text"
W13-1704,W12-2022,0,0.0180762,"ssessing intra-sentential quality lies in the limited linguistic evidence that can be extracted automatically from relatively short sentences for them to be assessed reliably, in addition to the difficulty in acquiring annotated data, since rating a response sentence by sentence is not something examiners typically do and would therefore require an additional and expensive manual annotation effort. Previous work has primarily focused on automatic content scoring of short answers, ranging from a few words to a few sentences (Pulman and Sukkarieh, 2005; Attali et al., 2008; Mohler et al., 2011; Ziai et al., 2012). On the other hand, scoring of individual sentences with respect to their linguistic quality, specifically in learner texts, has received considerably less attention. Higgins et al. (2004) devised guidelines for the manual annotation of sentences in learner texts, and evaluated a rule-based approach that classifies sentences with respect to clarity of expression based on grammar, mechanics and word usage errors; however, their system performs binary classification, whereas we are focusing on scoring sentences. Writing instruction tools, such as Criterion (Burstein et al., 2003), give advice o"
W14-1702,J81-4005,0,0.762137,"Missing"
W14-1702,N12-1067,0,0.222936,"tives for ranking: 1) using the target LM embedded in our SMT system (described in Section 2.2) and 2) using a large n-gram LM built from web data. In the latter case, we used Microsoft Web N-gram Services, which provide access to large smoothed n-gram language models (with n=2,3,4,5) built from web documents (Gao et al., 2010). All our experiments are based on the 5-gram ‘bing-body:apr10’ model. The ranking performance of these two models was evaluated on the 10-best hypotheses generated by the SMT system for each sentence in the development set. Table 1 shows the results from the M2 Scorer (Dahlmeier and Ng, 2012), the official scorer for the shared task that, unlike previous versions, weights precision twice as much as recall. Results show that using Microsoft’s Web LM yields better performance, which is unsurprising given the vast amounts of data used to build that Candidate generation In order to integrate corrections from multiple systems, we developed a method to generate all the possible corrected versions of a sentence (candidates). Candidates are generated by computing all possible combinations of corrections (irrespective of the system from which they originate), including the original tokens"
W14-1702,W11-2841,0,0.0408429,"Missing"
W14-1702,W13-1703,0,0.554424,"Missing"
W14-1702,W11-2838,0,0.127059,"without harming recall. Our best hybrid system achieves state-of-the-art results, ranking first on the original test set and second on the test set with alternative annotations. 1 Introduction Grammatical error correction has attracted considerable interest in the last few years, especially through a series of ‘shared tasks’. These efforts have helped to provide a common ground for evaluating and comparing systems while encouraging research in the field. These shared tasks have primarily focused on English as a second or foreign language and addressed different error types. The HOO 2011 task (Dale and Kilgarriff, 2011), for example, included all error types whereas HOO 2012 (Dale et al., 2012) and the CoNLL 2013 shared task (Ng et al., 2013) were restricted to only two and five types respectively. In this paper, we describe our submission to the CoNLL 2014 shared task (Ng et al., 2014), which involves correcting all the errors in essays written in English by students at the National University of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as deter"
W14-1702,W12-2006,0,0.135741,"nking first on the original test set and second on the test set with alternative annotations. 1 Introduction Grammatical error correction has attracted considerable interest in the last few years, especially through a series of ‘shared tasks’. These efforts have helped to provide a common ground for evaluating and comparing systems while encouraging research in the field. These shared tasks have primarily focused on English as a second or foreign language and addressed different error types. The HOO 2011 task (Dale and Kilgarriff, 2011), for example, included all error types whereas HOO 2012 (Dale et al., 2012) and the CoNLL 2013 shared task (Ng et al., 2013) were restricted to only two and five types respectively. In this paper, we describe our submission to the CoNLL 2014 shared task (Ng et al., 2014), which involves correcting all the errors in essays written in English by students at the National University of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of"
W14-1702,P07-1003,0,0.0141757,"ed by Yuan and Felice (2013) in order to train an SMT 1 The latest version of the system, called ‘Write & Improve’, is available at http://www.cambridge english.org/writeandimprovebeta/. 2 More information at http://www.cambridge .org/elt/catalogue/subject/custom/item36 46603/ 3 Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&view=article&id=4&Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We then built a lexical reor"
W14-1702,W13-1704,1,0.758696,"t can ‘translate’ from incorrect into correct English. Our training data comprises a set of different parallel corpora, where the original (incorrect) sentences constitute the source side and corrected versions based on gold standard annotations constitute the target side. These corpora include: • the NUCLE v3.1 corpus (Dahlmeier et al., 2013), containing around 1,400 essays written in English by students at the National University of Singapore (approx. 1,220,257 tokens in 57,152 sentences), Figure 1: Overview of components and interactions in our final hybrid system. in their writing tasks1 (Andersen et al., 2013). The original SAT system provides three main functionalities: 1) text assessment, producing an overall score for a piece of text, 2) sentence evaluation, producing a sentence-level quality score, and 3) word-level feedback, suggesting specific corrections for frequent errors. Since the focus of the shared task is on strict correction (as opposed to detection), we only used the word-level feedback component of the SAT system. This module uses rules automatically derived from the Cambridge Learner Corpus2 (CLC) (Nicholls, 2003) that are aimed at detecting errorful unigrams, bigrams and trigrams"
W14-1702,W11-2839,0,0.0607249,"Missing"
W14-1702,W11-2840,0,0.0304679,"Missing"
W14-1702,W13-3601,0,0.248175,"the test set with alternative annotations. 1 Introduction Grammatical error correction has attracted considerable interest in the last few years, especially through a series of ‘shared tasks’. These efforts have helped to provide a common ground for evaluating and comparing systems while encouraging research in the field. These shared tasks have primarily focused on English as a second or foreign language and addressed different error types. The HOO 2011 task (Dale and Kilgarriff, 2011), for example, included all error types whereas HOO 2012 (Dale et al., 2012) and the CoNLL 2013 shared task (Ng et al., 2013) were restricted to only two and five types respectively. In this paper, we describe our submission to the CoNLL 2014 shared task (Ng et al., 2014), which involves correcting all the errors in essays written in English by students at the National University of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and 2 A"
W14-1702,W14-1701,0,0.226803,"specially through a series of ‘shared tasks’. These efforts have helped to provide a common ground for evaluating and comparing systems while encouraging research in the field. These shared tasks have primarily focused on English as a second or foreign language and addressed different error types. The HOO 2011 task (Dale and Kilgarriff, 2011), for example, included all error types whereas HOO 2012 (Dale et al., 2012) and the CoNLL 2013 shared task (Ng et al., 2013) were restricted to only two and five types respectively. In this paper, we describe our submission to the CoNLL 2014 shared task (Ng et al., 2014), which involves correcting all the errors in essays written in English by students at the National University of Singapore. An all-type task poses a greater challenge, since correcting open-class types (such as spelling or collocation errors) requires different correction strategies than those in closed classes (such as determiners or prepositions). In this scenario, hybrid systems or combinations of correction modules seem more appropriate and 2 Approach We tackle the error correction task using a pipeline of processes that combines results from multiple systems. Figure 1 shows the interacti"
W14-1702,N03-1017,0,0.0147365,"nt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We then built a lexical reordering model using the alignments created by pialign. The maximum phrase length was set to 7, as recommended in the SMT literature (Koehn et al., 2003; Koehn, 2014). The IRSTLM Toolkit (Federico et al., 2008) was used to build a 4-gram target language model with Kneser–Ney smoothing (Kneser and Ney, 1995) on the correct sentences from the NUCLE, full CLC and EVP corpora. Decoding was performed with Moses (Koehn et al., 2007), using the default settings and weights. No tuning process was applied. The resulting system was used to produce the 10 best correction candidates for each sentence in the dataset, which were further processed by other modules. Segmentation, tokenisation and part-of-speech tagging were performed using NLTK (Bird et al.,"
W14-1702,P07-2045,0,0.0118625,"hat, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We then built a lexical reordering model using the alignments created by pialign. The maximum phrase length was set to 7, as recommended in the SMT literature (Koehn et al., 2003; Koehn, 2014). The IRSTLM Toolkit (Federico et al., 2008) was used to build a 4-gram target language model with Kneser–Ney smoothing (Kneser and Ney, 1995) on the correct sentences from the NUCLE, full CLC and EVP corpora. Decoding was performed with Moses (Koehn et al., 2007), using the default settings and weights. No tuning process was applied. The resulting system was used to produce the 10 best correction candidates for each sentence in the dataset, which were further processed by other modules. Segmentation, tokenisation and part-of-speech tagging were performed using NLTK (Bird et al., 2009) for consistency with the shared task datasets. 2.3 Figure 2: An example showing the candidate generation process. Model SMT IRSTLM Microsoft Web N-grams CE 651 666 ME 2766 2751 UE 1832 1344 P 0.2621 0.3313 R 0.1905 0.1949 F0.5 0.2438 0.2907 Table 1: Performance of langua"
W14-1702,P00-1056,0,0.0342993,"an and Felice (2013). SMT system We follow a similar approach to the one described by Yuan and Felice (2013) in order to train an SMT 1 The latest version of the system, called ‘Write & Improve’, is available at http://www.cambridge english.org/writeandimprovebeta/. 2 More information at http://www.cambridge .org/elt/catalogue/subject/custom/item36 46603/ 3 Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&view=article&id=4&Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their corr"
W14-1702,J03-1002,0,0.0163956,"). SMT system We follow a similar approach to the one described by Yuan and Felice (2013) in order to train an SMT 1 The latest version of the system, called ‘Write & Improve’, is available at http://www.cambridge english.org/writeandimprovebeta/. 2 More information at http://www.cambridge .org/elt/catalogue/subject/custom/item36 46603/ 3 Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&view=article&id=4&Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal wei"
W14-1702,W11-2843,0,0.16668,"Missing"
W14-1702,N06-1014,0,0.0208391,"h to the one described by Yuan and Felice (2013) in order to train an SMT 1 The latest version of the system, called ‘Write & Improve’, is available at http://www.cambridge english.org/writeandimprovebeta/. 2 More information at http://www.cambridge .org/elt/catalogue/subject/custom/item36 46603/ 3 Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&view=article&id=4&Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate into themselves and errors are often similar to their correct forms. Equal weights were assigned to these features. We"
W14-1702,W13-3602,0,0.0366785,"Missing"
W14-1702,P11-1064,0,0.0134575,"the NUCLE corpus on error-free sentences, as described by Yuan and Felice (2013). SMT system We follow a similar approach to the one described by Yuan and Felice (2013) in order to train an SMT 1 The latest version of the system, called ‘Write & Improve’, is available at http://www.cambridge english.org/writeandimprovebeta/. 2 More information at http://www.cambridge .org/elt/catalogue/subject/custom/item36 46603/ 3 Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&view=article&id=4&Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; Och and Ney, 2003) and Berkeley Aligner (Liang et al., 2006; DeNero and Klein, 2007) in terms of precision and F0.5 on the development set. Instead of using heuristics to extract phrases from the word alignments learnt by GIZA++ or Berkerley Aligner, pialign created a phrase table directly from model probabilities. In addition to the features already defined by pialign, we added character-level Levenshtein distance to each mapping in the phrase table. This was done to allow for the fact that, in error correction, most words translate"
W14-1702,W13-3605,0,0.0223579,"r correction using hybrid systems and type filtering Mariano Felice Zheng Yuan Øistein E. Andersen Helen Yannakoudakis Ekaterina Kochmar Computer Laboratory, University of Cambridge, United Kingdom {mf501,zy249,oa223,hy260,ek358}@cl.cam.ac.uk Abstract typically produce good results. In fact, most of the participating teams in previous shared tasks have used a combination of modules or systems for their submissions, even for correcting closedclass types (Dahlmeier et al., 2011; Bhaskar et al., 2011; Rozovskaya et al., 2011; Ivanova et al., 2011; Rozovskaya et al., 2013; Yoshimoto et al., 2013; Xing et al., 2013; Kunchukuttan et al., 2013; Putra and Szabo, 2013; Xiang et al., 2013). In line with previous research, we present a hybrid approach that employs a rule-based error correction system and an ad-hoc statistical machine translation (SMT) system, as well as a large-scale language model to rank alternative corrections and an error type filtering technique. The remainder of this paper is organised as follows: Section 2 describes our approach and each component in detail, Section 3 presents our experiments using the CoNLL 2014 shared task development set and Section 4 reports our official results on"
W14-1702,P11-1019,1,0.583755,"st of suggested corrections. These corrections can either be applied to the original text or used to generate multiple correction candidates, as described in Section 2.3. 2.2 • phrase alignments involving corrections extracted automatically from the NUCLE corpus (with up to 7 tokens per side), which are used to boost the probability of phrase alignments that involve corrections so as to improve recall, • the CoNLL 2014 shared task development set, containing 50 essays from the previous year’s test set (approx. 29,207 tokens in 1,382 sentences), • the First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011), containing 1,244 exam scripts and 2 essays per script (approx. 532,033 tokens in 16,068 sentences), • a subset of the International English Language Testing System (IELTS) examination dataset extracted from the CLC corpus, containing 2,498 exam scripts and 2 essays per script (approx. 1,361,841 tokens in 64,628 sentences), and • a set of sentences from the English Vocabulary Profile3 (EVP), which have been modified to include artificially generated errors (approx. 351,517 tokens in 18,830 sentences). The original correct sentences are a subset of the CLC and come from examinations at differe"
W14-1702,W13-3607,1,0.747037,"amination dataset extracted from the CLC corpus, containing 2,498 exam scripts and 2 essays per script (approx. 1,361,841 tokens in 64,628 sentences), and • a set of sentences from the English Vocabulary Profile3 (EVP), which have been modified to include artificially generated errors (approx. 351,517 tokens in 18,830 sentences). The original correct sentences are a subset of the CLC and come from examinations at different proficiency levels. The artificial error generation method aims at replicating frequent error patterns observed in the NUCLE corpus on error-free sentences, as described by Yuan and Felice (2013). SMT system We follow a similar approach to the one described by Yuan and Felice (2013) in order to train an SMT 1 The latest version of the system, called ‘Write & Improve’, is available at http://www.cambridge english.org/writeandimprovebeta/. 2 More information at http://www.cambridge .org/elt/catalogue/subject/custom/item36 46603/ 3 Sentences were automatically scraped from http:// www.englishprofile.org/index.php?option= com_content&view=article&id=4&Itemid=5 16 Word alignment was carried out using pialign (Neubig et al., 2011), after we found it outperformed GIZA++ (Och and Ney, 2000; O"
W14-1702,W13-3611,0,\N,Missing
W14-1702,W13-3604,0,\N,Missing
W14-1702,W13-3616,0,\N,Missing
W14-1702,W13-3612,0,\N,Missing
W15-0625,J08-4004,0,0.164046,"and five distinct scores. In contrast, linearly weighted kappa appeared to be less affected, although a slight increase in value was observed as the range increased. The correction for chance agreement in Cohen’s kappa has been the subject of much controversy (Brennan and Prediger, 1981; Feinstein and Cicchetti, 1990; Uebersax, 1987; Byrt et al., 1993; 5 However, we would like to penalise trivial systems that e.g., always assign the most prevalent gold score, in which case the marginals are indeed fixed. 217 Gwet, 2002; Di Eugenio and Glass, 2004; Sim and Wright, 2005; Craggs and Wood, 2005; Artstein and Poesio, 2008; Powers, 2012). Firstly, it assumes that when assessors are unsure of a score, they guess at random according to a fixed prior distribution of scores. Secondly, it includes chance correction for every single prediction instance (i.e., not only when an assessor is in doubt). Many have argued (Brennan and Prediger, 1981; Uebersax, 1987) that this is a highly improbable model of assessor error and vastly over-estimates agreement due to chance, especially in the case when prior distributions are free to vary. Although it is likely that there is some agreement due to chance when an assessor is uns"
W15-0625,W13-3502,0,0.0217496,"ed coefficient that is defined in terms of concordant and discordant pairs; however, further experiments beyond the scope of this paper would be needed to confirm this. It is worth noting that given the generality of the ATS task setting as presented in this paper (i.e., aiming to predict gold standard scores on an ordinal scale) and the metric-evaluation setup (using synthetic data in addition to real output), the properties discussed and resulting recommendations may be more widely relevant within NLP and may serve as a useful benchmark for the wider community (Siddharthan and Katsos, 2010; Bloodgood and Grothendieck, 2013; Chen and He, 2013; Liu et al., 2013, among others) as well as for shared task organisers. An interesting direction for future work would be to explore the use of evaluation measures that lie outside of those commonly used by the ATS community, such as macro-averaged root mean squared error that has been argued as being suitable for ordinal regression tasks (Baccianella et al., 2009). 16 A low correlation could also point to effects of the underlying properties of the data as the metric is sensitive to trait prevalence (see Section 3.1.1). Acknowledgments We would like to thank Ted Briscoe fo"
W15-0625,D13-1180,0,0.577402,"more pronounced when it comes to scoring extended texts such as essays, a task prone to an element of subjectivity. Automated systems enable rigid application of scoring criteria, thus reducing the inconsistencies which may arise, in particular, when many human examiners are employed for large-scale assessment. There is a substantial literature describing and evaluating ATS systems (Page, 1968; Powers et al., 2002; Rudner and Liang, 2002; Burstein et al., 2003; Landauer et al., 2003; Higgins et al., 2004; Attali and Burstein, 2006; Attali et al., 2008; Williamson, 2009; Briscoe et al., 2010; Chen and He, 2013). Such systems are increasingly used but remain controversial. Although a comprehensive comparison of the capabilities of eight existing commercial essay scoring systems (Shermis and Hamner, 2012) across five different performance metrics in the recent ATS competition organised by Kaggle1 claimed that ATS systems grade similarly to humans, critics (Wang and Brown, 2007; Wang and Brown, 2008; Perelman, 2013) have continued to dispute this. For the evaluation of ATS systems (Williamson, 2009; Williamson et al., 2012), emphasis has been given to the “agreement” of machine-predicted scores (ordina"
W15-0625,J05-3001,0,0.0333798,"y in ranges between two and five distinct scores. In contrast, linearly weighted kappa appeared to be less affected, although a slight increase in value was observed as the range increased. The correction for chance agreement in Cohen’s kappa has been the subject of much controversy (Brennan and Prediger, 1981; Feinstein and Cicchetti, 1990; Uebersax, 1987; Byrt et al., 1993; 5 However, we would like to penalise trivial systems that e.g., always assign the most prevalent gold score, in which case the marginals are indeed fixed. 217 Gwet, 2002; Di Eugenio and Glass, 2004; Sim and Wright, 2005; Craggs and Wood, 2005; Artstein and Poesio, 2008; Powers, 2012). Firstly, it assumes that when assessors are unsure of a score, they guess at random according to a fixed prior distribution of scores. Secondly, it includes chance correction for every single prediction instance (i.e., not only when an assessor is in doubt). Many have argued (Brennan and Prediger, 1981; Uebersax, 1987) that this is a highly improbable model of assessor error and vastly over-estimates agreement due to chance, especially in the case when prior distributions are free to vary. Although it is likely that there is some agreement due to cha"
W15-0625,N04-1024,0,0.10742,"dress several issues with manual assessment (e.g., expense, speed, and consistency). Further advantages become more pronounced when it comes to scoring extended texts such as essays, a task prone to an element of subjectivity. Automated systems enable rigid application of scoring criteria, thus reducing the inconsistencies which may arise, in particular, when many human examiners are employed for large-scale assessment. There is a substantial literature describing and evaluating ATS systems (Page, 1968; Powers et al., 2002; Rudner and Liang, 2002; Burstein et al., 2003; Landauer et al., 2003; Higgins et al., 2004; Attali and Burstein, 2006; Attali et al., 2008; Williamson, 2009; Briscoe et al., 2010; Chen and He, 2013). Such systems are increasingly used but remain controversial. Although a comprehensive comparison of the capabilities of eight existing commercial essay scoring systems (Shermis and Hamner, 2012) across five different performance metrics in the recent ATS competition organised by Kaggle1 claimed that ATS systems grade similarly to humans, critics (Wang and Brown, 2007; Wang and Brown, 2008; Perelman, 2013) have continued to dispute this. For the evaluation of ATS systems (Williamson, 20"
W15-0625,O13-1025,0,0.0179013,"and discordant pairs; however, further experiments beyond the scope of this paper would be needed to confirm this. It is worth noting that given the generality of the ATS task setting as presented in this paper (i.e., aiming to predict gold standard scores on an ordinal scale) and the metric-evaluation setup (using synthetic data in addition to real output), the properties discussed and resulting recommendations may be more widely relevant within NLP and may serve as a useful benchmark for the wider community (Siddharthan and Katsos, 2010; Bloodgood and Grothendieck, 2013; Chen and He, 2013; Liu et al., 2013, among others) as well as for shared task organisers. An interesting direction for future work would be to explore the use of evaluation measures that lie outside of those commonly used by the ATS community, such as macro-averaged root mean squared error that has been argued as being suitable for ordinal regression tasks (Baccianella et al., 2009). 16 A low correlation could also point to effects of the underlying properties of the data as the metric is sensitive to trait prevalence (see Section 3.1.1). Acknowledgments We would like to thank Ted Briscoe for his valuable comments and suggestio"
W15-0625,E12-1035,0,0.0222245,"In contrast, linearly weighted kappa appeared to be less affected, although a slight increase in value was observed as the range increased. The correction for chance agreement in Cohen’s kappa has been the subject of much controversy (Brennan and Prediger, 1981; Feinstein and Cicchetti, 1990; Uebersax, 1987; Byrt et al., 1993; 5 However, we would like to penalise trivial systems that e.g., always assign the most prevalent gold score, in which case the marginals are indeed fixed. 217 Gwet, 2002; Di Eugenio and Glass, 2004; Sim and Wright, 2005; Craggs and Wood, 2005; Artstein and Poesio, 2008; Powers, 2012). Firstly, it assumes that when assessors are unsure of a score, they guess at random according to a fixed prior distribution of scores. Secondly, it includes chance correction for every single prediction instance (i.e., not only when an assessor is in doubt). Many have argued (Brennan and Prediger, 1981; Uebersax, 1987) that this is a highly improbable model of assessor error and vastly over-estimates agreement due to chance, especially in the case when prior distributions are free to vary. Although it is likely that there is some agreement due to chance when an assessor is unsure of a score"
W15-0625,N10-1144,0,0.0252069,"is a more effective tie-adjusted coefficient that is defined in terms of concordant and discordant pairs; however, further experiments beyond the scope of this paper would be needed to confirm this. It is worth noting that given the generality of the ATS task setting as presented in this paper (i.e., aiming to predict gold standard scores on an ordinal scale) and the metric-evaluation setup (using synthetic data in addition to real output), the properties discussed and resulting recommendations may be more widely relevant within NLP and may serve as a useful benchmark for the wider community (Siddharthan and Katsos, 2010; Bloodgood and Grothendieck, 2013; Chen and He, 2013; Liu et al., 2013, among others) as well as for shared task organisers. An interesting direction for future work would be to explore the use of evaluation measures that lie outside of those commonly used by the ATS community, such as macro-averaged root mean squared error that has been argued as being suitable for ordinal regression tasks (Baccianella et al., 2009). 16 A low correlation could also point to effects of the underlying properties of the data as the metric is sensitive to trait prevalence (see Section 3.1.1). Acknowledgments We"
W15-0625,P11-1019,1,0.938548,"ppropriate type of measurement to apply to the output of ATS systems to address the accuracy of the (numerical) solution compared to the gold standard. 3 Measuring Performance of ATS systems In this section, we review and critique metrics that have been frequently used in the literature to ascertain the performance of ATS systems. These performance metrics can be broadly categorised into measures of association and measures of agreement (e.g., see Williamson et al. (2012)). 3.1 Measures of Association Measures of association (i.e., correlation coefficients) have been widely used in ATS (e.g., Yannakoudakis et al. (2011)), with Pearson’s productmoment correlation coefficient being the most common. Pearson’s correlation is a parametric measure of association that quantifies the degree of linear dependence between two variables and, more specifically, describes the extent to which the variables co-vary relative to the degree to which they vary independently. The greater the association, the more accurately one can use the value of one variable to predict the other. As the data depart from the coefficient’s assumptions (e.g., unequal marginals), its maximum values may not be attainable (Carroll, 1961). For ordin"
W16-0510,P06-4020,1,0.803516,"Missing"
W16-0510,N04-1024,0,0.272002,"mber of existing automated textscoring systems (sometimes referred to as essay scoring systems). For an overview, the interested reader is directed to reviews and advances in the area (Shermis and Burstein, 2003; Landauer, 2003; Valenti et al., 2003; Dikli, 2006; Phillips, 2007; Briscoe et al., 2010; Shermis and Burstein, 2013). In this section, we review related research on topicalrelevance detection for automated writing assessment, and outline the key differences between our approach and that of existing work. A wide variety of computational approaches (Miller, 2003; Landauer et al., 2003; Higgins et al., 2004; Higgins and Burstein, 2007; Chen et al., 2010) have been used to automatically assess L2 texts. Early work on topical relevance (Higgins et al., 2006) posed the problem as one of binary classification and aimed to identify whether a text was either on or off-topic. The main motivation of the research was to detect off-topic text, text submitted mistakenly (within an online assessment setting), or 96 text submitted in bad faith (i.e., possibly memorised on an unrelated topic). They adopted an unsupervised approach to the problem, where they matched each text to its corresponding prompt using"
W16-0510,Q15-1016,0,0.0362934,"a lemmatised version of Wikipedia from 2013. We removed from the corpus all words that appeared less than 200 times and used the 96,811 remaining words as both potential expansion words w and as contexts c. We used a 5 word context window (2 words either side of the target word) and reduced the size of the resultant vectors by only storing dimensions that had a PPMI greater than 2.0 (Turney et al., 2010). The resultant vectors are competitive with the best reported results for traditional word vectors on a word–word similarity task (Spearman-ρ = 0.732 on 3000 word-pairs from the MEN dataset) (Levy et al., 2015). We create a vector representation for the prompt p in Rn by summing the PPMI word-vectors of the words occurring in the prompt. Finally, the |e |closest words to the prompt vector p, as measured by cosine similarity, can then be selected as expansion terms. 3.3 Random Indexing Random Indexing (RI) (Kanerva et al., 2000) is an approach which incrementally builds word vectors in a dimensionally-reduced space. Words are initially assigned a unique random index vector in a space Zn , where n is user-defined. These nearorthogonal vectors are updated by iterating over a corpus of text. In particul"
W16-0510,W10-1013,0,0.0313589,"es employed in that work was to calculate the similarity of an essay to a number of unrelated prompts. If the essay was closer to an unrelated prompt than the relevant one, the essay was deemed to be off-topic. Briscoe et al. (2010) tackle the problem of offtopic detection using more complex distributional semantic models that tend to overcome the problem of vocabulary mismatch. However, they frame the task as binary classification and evaluate their approach by determining if it can associate a learner text with the correct prompt. The work which is closest in spirit to that of our own is by Louis and Higgins (2010), who expand prompts using morphological variations, synonyms, and words that are distributionally similar to those that appear in the prompt. Their work builds on the earlier work by Higgins et al. (2006), and again pose the problem as one of binary classification. The most recent work of Persing and Ng (2014) involves scoring L2 learner texts for relevance on a seven-point scale using a feature-rich linear regression approach. While they demonstrate that learning one linear regression model per prompt is a useful supervised approach, it means that substantial training data is needed for each"
W16-0510,D09-1045,0,0.0471189,"Missing"
W16-0510,P14-1144,0,0.593295,"er the learner has understood the prompt and attempted a response with appropriate vocabulary. Other reasons for measuring the topical relevance of a text include the detection of malicious submissions, that is, detecting submissions that have been rote-learned or memorised specifically for assessment situations (Higgins et al., 2006). In this paper, we employ techniques from the area of distributional semantics and information retrieval (IR) to develop unsupervised promptrelevance models, and demonstrate that they correlate well with human judgements. In particu2 We note that a recent paper (Persing and Ng, 2014) has referred to this task as prompt adherence, while we use the terms prompt-relevance and topical-relevance interchangeably throughout this paper. 95 Proceedings of the 11th Workshop on Innovative Use of NLP for Building Educational Applications, pages 95–104, c San Diego, California, June 16, 2016. 2016 Association for Computational Linguistics lar, we study four different methods of expanding a prompt with with topically-related words and show that some are more beneficial than others at overcoming the ‘vocabulary mismatch’ problem which is typically present in free-text learner writing. T"
W16-0510,W09-3927,0,0.0198121,"r different methods of expanding a prompt with with topically-related words and show that some are more beneficial than others at overcoming the ‘vocabulary mismatch’ problem which is typically present in free-text learner writing. To the best of our knowledge, there have been no attempts at a comparative study investigating the effectiveness of such techniques on the automatic prediction of a topical-relevance score in the noisy domain of learner texts, where grammatical errors are common. In addition, we perform an external evaluation to measure the extent to which prompt-relevance informs (Rotaru and Litman, 2009) the holistic score. The remainder of the paper is outlined as follows: Section 2 discusses related work and outlines our contribution. Section 3 presents our framework and four unsupervised approaches to measuring semantic similarity. Section 4 presents both quantitative and qualitative evaluations for all of the methods employed in this paper. Section 5 performs an external evaluation by incorporating the best promptrelevance model as features into a supervised preference ranking approach. Finally, Section 6 concludes with a discussion and outline of future work. 2 Related Research There are"
W16-0510,W12-2004,1,0.904618,"y style) to those in the ICLE dataset. Candidates are assigned an overall score on a scale from 1 to 9. Prompt relevance is an aspect that is present in the marking criteria, and it is identified as a determinant of the overall score. We therefore hypothesise that adding prompt-relevance measures to the feature set of a prompt-independent essay scoring system (i.e. that is designed to assess linguistic competence only) would better reflect the evaluation performed by examiners and improve system performance. The baseline system is a linear preference ranking model (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012) and is trained to predict an overall essay score based on the following set of features: - word unigrams, bigrams, and trigrams POS (part-of-speech) counts grammatical relations essay length (# of unique words) 101 - counts of cohesive devices - max-word length and min-sentence length - number of errors based on a presence/absence trigram language model We divided the dataset into 5-folds in two separate ways. First, we created prompt-dependent folds, where essays associated with all 22 prompts appear in both the training and test data in the appropriate proportions. This scenario allows the"
W16-0510,W15-0625,1,0.851147,"ch on many of the prompts. However, to measure the topical quality of the expansion words selected by each approach in isolation, we removed the original prompt words from the expanded prompts and again calculated the performance of the different approaches. This more rigorous evaluation in Table 2 (Bottom) shows that the topical quality of the expansion words from the PRF approach tends to be better than the other approaches. We next look at the actual expansion words selected for two prompts. time, assessors graded within a point of each other. Furthermore, correlation is affected by scale (Yannakoudakis and Cummins, 2015). 9 The two remaining prompts have only three essays associated with them. Prompt # of essays length cos(p, s) dsp+e RIp+e cbow p+e skipp+e PRFp+e 1 237 -0.113 0.324 0.328 0.372 0.345 0.359 0.348 2 53 -0.026 0.120 0.141 0.098 0.125 0.160 0.188 3 64 -0.062 0.195 0.182 0.103 0.131 0.183 0.126 4 58 0.211 0.122 0.114 0.214 0.114 0.139 0.145 5 131 -0.023 0.205 0.208 0.192 0.209 0.245 0.260 6 43 -0.111 -0.019 -0.011 0.093 0.068 0.026 0.034 7 80 0.103 0.333 0.340 0.398 0.328 0.363 0.340 8 28 -0.115 0.511 0.519 0.720 0.581 0.571 0.598 9 49 -0.056 0.268 0.280 0.259 0.265 0.278 0.335 10 71 0.171 0.064 0"
W16-0510,P11-1019,1,0.899291,"similar in style (i.e. essay style) to those in the ICLE dataset. Candidates are assigned an overall score on a scale from 1 to 9. Prompt relevance is an aspect that is present in the marking criteria, and it is identified as a determinant of the overall score. We therefore hypothesise that adding prompt-relevance measures to the feature set of a prompt-independent essay scoring system (i.e. that is designed to assess linguistic competence only) would better reflect the evaluation performed by examiners and improve system performance. The baseline system is a linear preference ranking model (Yannakoudakis et al., 2011; Yannakoudakis and Briscoe, 2012) and is trained to predict an overall essay score based on the following set of features: - word unigrams, bigrams, and trigrams POS (part-of-speech) counts grammatical relations essay length (# of unique words) 101 - counts of cohesive devices - max-word length and min-sentence length - number of errors based on a presence/absence trigram language model We divided the dataset into 5-folds in two separate ways. First, we created prompt-dependent folds, where essays associated with all 22 prompts appear in both the training and test data in the appropriate prop"
W17-5004,D16-1195,0,0.0135021,"e parameters were optimised. contains more fine-grained labels per error. For example, the FCE (Yannakoudakis et al., 2011) training set has 75 different labels for individual error types, such as missing determiners or incorrect verb forms. By giving the model access to these labels, the system can learn more fine-grained error patterns that are based on the individual error types. 4 Evaluation setup and datasets • first language: Previous work has experimentally demonstrated that the distribution of writing errors depends on the first language (L1) of the learner (Rozovskaya and Roth, 2011; Chollampatt et al., 2016). We investigate the usefulness of L1 as an auxiliary objective during training. Rei and Yannakoudakis (2016) investigate a number of compositional architectures for error detection, and present state-of-the-art results using a bidirectional LSTM. We follow their experimental setup and investigate the impact of auxiliary loss functions on the same datasets: the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011) and the CoNLL-14 shared task test set (Ng et al., 2014b). FCE contains texts written by non-native learners of English in response to exam prompts eliciting free-te"
W17-5004,W13-1704,1,0.89379,"ls detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems were type specific and targeted closed-class errors. In the following year, Additional Training Data The main benefits of multi-task learning are expected in scenarios where the available taskspecific training data is limited. However, we also investigate the effect of auxiliary objectives when training on a substantially larger training set. More specifically, we follow Rei and Yannakoudakis (2016), who"
W17-5004,W13-1703,0,0.0458884,"Additional Training Data The main benefits of multi-task learning are expected in scenarios where the available taskspecific training data is limited. However, we also investigate the effect of auxiliary objectives when training on a substantially larger training set. More specifically, we follow Rei and Yannakoudakis (2016), who also experimented with augmenting the publicly available datasets with training data from a large proprietary corpus. In total, we train this large model on 17.8M tokens from the Cambridge Learner Corpus (CLC, Nicholls 2003), the NUS Corpus of Learner English (NUCLE, Dahlmeier et al. 2013), and the Lang-8 corpus (Mizumoto et al., 2011). We use the same model architecture as Rei and Yannakoudakis (2016), adding only the auxiliary objective of predicting the automatically generated POS tag, which was the most successful additional objective based on the development experiments. Table 6 contains results for evaluating this model, when trained on the large training set. On the FCE test data, the auxiliary objective does not provide an improvement and the model performance is comparable to the results by Rei and Yannakoudakis (2016) (R&Y). Since most of the 39 by randomly switching"
W17-5004,E17-2026,0,0.119476,"equence labeling datasets that have been manually annotated for different tasks: • The CoNLL 2000 dataset (Tjong Kim Sang and Buchholz, 2000) for chunking, containing sections of the Wall Street Journal and annotated with 22 different labels. • The CoNLL 2003 corpus (Tjong Kim Sang and De Meulder, 2003) contains texts from the Reuters Corpus and has been annotated with 8 labels for named entity recognition (NER). • The Penn Treebank (PTB) POS corpus (Marcus et al., 1993) contains texts from the Wall Street Journal and has been annotated with 48 POS tags. The CoNLL-00 dataset was identified by Bingel and Søgaard (2017) as being the most useful additional training resource in a multi-task setting; The CoNLL-03 NER dataset has a similar label density as the error detection task; and the PTB corpus was chosen as POS tags gave consistently good performance for error detection on both the development and test sets, as demonstrated in the previous section. In the first setting, each of these datasets is used to train a sequence labeling model for their respective tasks, and the resulting model is used to initialise a network for training an error detection system. While it is common to preload word embeddings fro"
W17-5004,W12-2006,0,0.0182062,"(Section 6). There has been some research on using auxiliary training objectives in the context of other tasks. Cheng et al. (2015) described a system for detecting out-of-vocabulary names by also predicting the next word in the sequence. Plank et al. (2016) predicted the frequency of each word together with the POS, and showed that this can improve tagging accuracy on low-frequency words. However, we are the first to explore the auxiliary objectives described in Section 3 in the context of error detection. the HOO 2012 shared task only focused on correcting preposition and determiner errors (Dale et al., 2012). The recent CoNLL shared tasks (Ng et al., 2013, 2014a) focused on error correction rather than detection: CoNLL-13 targeted correcting noun number, verb form and subjectverb agreement errors, in addition to preposition and determiner errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and ver"
W17-5004,P06-4020,0,0.0414369,"2,222 sentences for development and 2,720 sentences for testing. The development set was randomly sampled from the training data, and the test set contains texts from a different examination year. The CoNLL-14 test set contains 50 texts annotated by two experts. Compared to FCE, the texts are more technical and are written by higherproficiency learners. In order to make our results comparable to Rei and Yannakoudakis (2016), we • part-of-speech: POS tagging is a wellestablished sequence labeling task, requiring the model to disambiguate the word types based on their contexts. We use the RASP (Briscoe et al., 2006) parser to automatically generate POS labels for the training data, and include them as additional objectives. • grammatical relations: We include as an auxiliary objective the type of the Grammatical Relation (GR) in which the current token is a dependent, in order to incentivise the model to learn more about semantic composition. Again we use the RASP parser, which is unlexicalised and therefore more suitable for learner data where spelling and grammatical errors are common. Table 1 presents the labels for each of the auxiliary tasks for an example sentence from the FCE training data. The au"
W17-5004,W11-2838,0,0.0498802,", 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems were type specific and targeted closed-class errors. In the following year, Additional Training Data The main benefits of multi-task learning are expected in scenarios where the available taskspecific training data is limited. However, we also investigate the effect of auxiliary objectives when training on a substantially larger training set. More specifically, we follow Rei and Yannakoudakis (2016), who also experimented with augmenting the publicly available datasets with training data from a large proprietary corpus. In total, we train this large model on 17.8M tokens"
W17-5004,E14-3013,0,0.0755885,"resulting model has the same number of parameters, the additional objectives allow it to be optimised more efficiently and achieve better performance. 1 Introduction Automatic error detection systems for learner writing need to identify various types of error in text, ranging from incorrect uses of function words, such articles and prepositions, to semantic anomalies in content words, such as adjective– noun combinations. To tackle the scarcity of errorannotated training data, previous work has investigated the utility of automatically generated ungrammatical data (Foster and Andersen, 2009; Felice and Yuan, 2014), as well as explored learning from native well-formed data (Rozovskaya and Roth, 2016; Gamon, 2010). In this work, we investigate the utility of supplementing error detection frameworks with additional linguistic information that can be extracted from the available error-annotated learner data. We construct a neural sequence labeling system for error detection that allows us to learn better representations of language composition and de2 Error Detection Model In addition to the scarcity of errors in the training data (i.e., the majority of tokens are correct), recent research has highlighted"
W17-5004,W14-1702,1,0.873966,"to preposition and determiner errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2014), as well as phrase-based statistical machine translation, under the hypothesis that incorrect source sentences can be “translated” to correct target sentences (Felice et al., 2014; Grundkiewicz, 2014). 9 Conclusion We have described a method for integrating auxiliary loss functions with a neural sequence labeling framework, in order to improve error detection in learner writing. While predicting binary error labels, the model also learns to predict additional linguistic information for each token, allowing it to discover compositional features that can be exploited for error detection. We performed a systematic comparison of possible auxiliary labels, which are either available in existing annotations or can be generated automatically. Our experiments showed that POS t"
W17-5004,D15-1085,0,0.17153,"Missing"
W17-5004,C12-1038,0,0.0146383,"detection results on the CoNLL-14 test set using different auxiliary loss functions. also evaluate our models on the two CoNLL14 test annotations and train our models only on the public FCE dataset. This corresponds to their FCE-public model that treats the CoNLL-14 dataset as an out-of-domain test set corpus. Following the CoNLL-14 shared task, we also report F0.5 as the main evaluation metric. However, while the shared task focused on correction and calculated F0.5 over error spans using multiple annotations, we evaluate token-level error detection performance. Following recommendations by Chodorow et al. (2012), we also report the raw counts for predicted and correct tokens. For pre-processing, all the texts are lowercased and digits are replaced with zeros for the tokenlevel representations, although the character-based component has access to the original version of each token. Tokens that occur only once are mapped to a single OOV token, which is then used to represent previously unseen tokens during testing. The word embeddings have size 300 and are initialised with publicly available word2vec (Mikolov et al., 2013) embeddings trained on Google News. The LSTM hidden layers have size 200 and the"
W17-5004,J93-2004,0,0.0584361,"Missing"
W17-5004,N10-1019,0,0.171794,"ficiently and achieve better performance. 1 Introduction Automatic error detection systems for learner writing need to identify various types of error in text, ranging from incorrect uses of function words, such articles and prepositions, to semantic anomalies in content words, such as adjective– noun combinations. To tackle the scarcity of errorannotated training data, previous work has investigated the utility of automatically generated ungrammatical data (Foster and Andersen, 2009; Felice and Yuan, 2014), as well as explored learning from native well-formed data (Rozovskaya and Roth, 2016; Gamon, 2010). In this work, we investigate the utility of supplementing error detection frameworks with additional linguistic information that can be extracted from the available error-annotated learner data. We construct a neural sequence labeling system for error detection that allows us to learn better representations of language composition and de2 Error Detection Model In addition to the scarcity of errors in the training data (i.e., the majority of tokens are correct), recent research has highlighted the variability in manual correction of writing errors: re-annotation of the CoNLL 2014 shared task"
W17-5004,I08-1059,0,0.0333398,"004). More recent approaches have exploited errorannotated learner corpora and primarily treated the task as a classification problem over vectors of contextual, lexical and syntactic features extracted from a fixed window around the target token. Most work has focused on error-type specific detection models, and in particular on models detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems were type specific and targeted closed-class errors. In the followin"
W17-5004,I11-1017,0,0.0473082,"ulti-task learning are expected in scenarios where the available taskspecific training data is limited. However, we also investigate the effect of auxiliary objectives when training on a substantially larger training set. More specifically, we follow Rei and Yannakoudakis (2016), who also experimented with augmenting the publicly available datasets with training data from a large proprietary corpus. In total, we train this large model on 17.8M tokens from the Cambridge Learner Corpus (CLC, Nicholls 2003), the NUS Corpus of Learner English (NUCLE, Dahlmeier et al. 2013), and the Lang-8 corpus (Mizumoto et al., 2011). We use the same model architecture as Rei and Yannakoudakis (2016), adding only the auxiliary objective of predicting the automatically generated POS tag, which was the most successful additional objective based on the development experiments. Table 6 contains results for evaluating this model, when trained on the large training set. On the FCE test data, the auxiliary objective does not provide an improvement and the model performance is comparable to the results by Rei and Yannakoudakis (2016) (R&Y). Since most of the 39 by randomly switching between different tasks and updating parameters"
W17-5004,W14-1703,0,0.0349582,"terminer errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2014), as well as phrase-based statistical machine translation, under the hypothesis that incorrect source sentences can be “translated” to correct target sentences (Felice et al., 2014; Grundkiewicz, 2014). 9 Conclusion We have described a method for integrating auxiliary loss functions with a neural sequence labeling framework, in order to improve error detection in learner writing. While predicting binary error labels, the model also learns to predict additional linguistic information for each token, allowing it to discover compositional features that can be exploited for error detection. We performed a systematic comparison of possible auxiliary labels, which are either available in existing annotations or can be generated automatically. Our experiments showed that POS tags, grammatical rela"
W17-5004,W14-1701,0,0.0382055,"ion of writing errors depends on the first language (L1) of the learner (Rozovskaya and Roth, 2011; Chollampatt et al., 2016). We investigate the usefulness of L1 as an auxiliary objective during training. Rei and Yannakoudakis (2016) investigate a number of compositional architectures for error detection, and present state-of-the-art results using a bidirectional LSTM. We follow their experimental setup and investigate the impact of auxiliary loss functions on the same datasets: the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011) and the CoNLL-14 shared task test set (Ng et al., 2014b). FCE contains texts written by non-native learners of English in response to exam prompts eliciting free-text answers. The texts have been manually annotated with error types and error spans by professional examiners, which Rei and Yannakoudakis (2016) convert to a binary correct/incorrect token-level labeling for error detection. For missing-word errors, the error label is assigned to the next word in the sequence. The released version contains 28,731 sentences for training, 2,222 sentences for development and 2,720 sentences for testing. The development set was randomly sampled from the t"
W17-5004,N16-1179,0,0.0336032,"nce been applied to many language processing tasks and neural network architectures. For example, Collobert and Weston (2008) constructed a convolutional architecture that shared some weights between tasks such as POS tagging, NER and chunking. Whereas their model only shared word embeddings, our approach focuses on learning better compositional features through a shared bidirectional LSTM. Luong et al. (2016) explored a multi-task architecture for sequence-to-sequence learning where encoders and decoders in different languages are trained jointly using the same semantic representation space. Klerke et al. (2016) used eye tracking measurements as a secondary task in order to improve a model for sentence compression. Bingel and Søgaard (2017) explored beneficial task relationships for training multitask models on different datasets. All of these architectures are trained 40 functions and better word representations. The error detection model, which also learns to predict automatically generated POS tags, achieved improved performance on both CoNLL-14 benchmarks. A useful direction for future work would be to investigate dynamic weighting strategies for auxiliary objectives that allow the network to ini"
W17-5004,W13-3601,0,0.0234403,"auxiliary training objectives in the context of other tasks. Cheng et al. (2015) described a system for detecting out-of-vocabulary names by also predicting the next word in the sequence. Plank et al. (2016) predicted the frequency of each word together with the POS, and showed that this can improve tagging accuracy on low-frequency words. However, we are the first to explore the auxiliary objectives described in Section 3 in the context of error detection. the HOO 2012 shared task only focused on correcting preposition and determiner errors (Dale et al., 2012). The recent CoNLL shared tasks (Ng et al., 2013, 2014a) focused on error correction rather than detection: CoNLL-13 targeted correcting noun number, verb form and subjectverb agreement errors, in addition to preposition and determiner errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2014), a"
W17-5004,C14-1164,0,0.0136672,"exploited errorannotated learner corpora and primarily treated the task as a classification problem over vectors of contextual, lexical and syntactic features extracted from a fixed window around the target token. Most work has focused on error-type specific detection models, and in particular on models detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems were type specific and targeted closed-class errors. In the following year, Additional Training Data The mai"
W17-5004,P16-2067,0,0.408618,"not feasible. In addition, writing errors can be very sparse, leaving the system with very little useful training data for learning error patterns. In order to train models that generalise well with limited training examples, we would want to encourage them to learn more generic patterns of language, grammar, syntax and composition, which can then be exploited for error detection. Multi-task learning allows models to learn from multiple objectives via shared representations, using information from related tasks to boost performance on tasks for which there is limited target data. For example, Plank et al. (2016) explored the option of using word frequency as an auxiliary loss function for part-of-speech (POS) tagging. Rei (2017) describe a semi-supervised framework for multi-task learning, integrating language modeling as an additional objective. Following this work, we adapt auxiliary objectives for the task of error detection, and further experiNext, the network includes a tanh-activated feedforward layer, using the hidden states from both LSTMs as input, allowing the model to learn more complex higher-level features. By combining the hidden states from both directions, we are able to have a vector"
W17-5004,P11-1019,1,0.818124,"468 1461 1458 1584 1803 1654 1781.0 55.7 54.4 54.9 57.0 54.3 57.9 57.7 23.3 23.2 23.1 25.1 28.6 26.2 28.3 43.4 42.7 42.8 45.5 46.0 46.4 47.7 Table 2: Error detection results on the FCE dataset using different auxiliary loss functions. hidden and output layers. However, these components are required only during the training process; at testing time, these can be removed and the resulting model has the same architecture and number of parameters as the baseline, with the only difference being in how the parameters were optimised. contains more fine-grained labels per error. For example, the FCE (Yannakoudakis et al., 2011) training set has 75 different labels for individual error types, such as missing determiners or incorrect verb forms. By giving the model access to these labels, the system can learn more fine-grained error patterns that are based on the individual error types. 4 Evaluation setup and datasets • first language: Previous work has experimentally demonstrated that the distribution of writing errors depends on the first language (L1) of the learner (Rozovskaya and Roth, 2011; Chollampatt et al., 2016). We investigate the usefulness of L1 as an auxiliary objective during training. Rei and Yannakoud"
W17-5004,P17-1194,1,0.818541,"ing error patterns. In order to train models that generalise well with limited training examples, we would want to encourage them to learn more generic patterns of language, grammar, syntax and composition, which can then be exploited for error detection. Multi-task learning allows models to learn from multiple objectives via shared representations, using information from related tasks to boost performance on tasks for which there is limited target data. For example, Plank et al. (2016) explored the option of using word frequency as an auxiliary loss function for part-of-speech (POS) tagging. Rei (2017) describe a semi-supervised framework for multi-task learning, integrating language modeling as an additional objective. Following this work, we adapt auxiliary objectives for the task of error detection, and further experiNext, the network includes a tanh-activated feedforward layer, using the hidden states from both LSTMs as input, allowing the model to learn more complex higher-level features. By combining the hidden states from both directions, we are able to have a vector that represents a specific token but also takes into account context on both sides: dt = tanh(Wf ht XX (3) where Wf an"
W17-5004,C16-1030,1,0.870248,"as input to a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) moving through the sentence in both directions. At each step, the LSTM calculates a new hidden representation based on the current token embedding and the hidden state from the previous step. (f ) ht (f ) (1) (b) (2) = LSTM(xt , ht−1 ) (b) ht = LSTM(xt , ht+1 ) E=− t (f ) (b) + Wb ht ) yet,k log(yt,k ) (5) k where yt,k is the predicted probability of token t having label k, and yet,k has the value 1 if the correct label for token t is k, and the value 0 otherwise. We also make use of the character-level extension described by Rei et al. (2016). Each token is separated into individual characters and mapped to character embeddings. Using a bidirectional LSTM and a hidden feedforward component, the character vectors are composed into a characterbased token representation. Finally, a dynamic gating function is used to combine this representation with a regular token embedding, taking advantage of both approaches. This component allows the model to capture useful morphological and character-based patterns, in addition to learning individual token-level vectors of common tokens. 3 Auxiliary Loss Functions The model in Section 2 learns to"
W17-5004,W14-1704,0,0.0384609,"ared tasks (Ng et al., 2013, 2014a) focused on error correction rather than detection: CoNLL-13 targeted correcting noun number, verb form and subjectverb agreement errors, in addition to preposition and determiner errors made by non-native learners of English, whereas CoNLL-14 expanded to correction of all errors regardless of type. Core components of the top two systems across the CoNLL correction shared tasks include Average Perceptrons, L1 error correction priors in Naive Bayes models, and joint inference capturing interactions between errors (e.g., noun number and verb agreement errors) (Rozovskaya et al., 2014), as well as phrase-based statistical machine translation, under the hypothesis that incorrect source sentences can be “translated” to correct target sentences (Felice et al., 2014; Grundkiewicz, 2014). 9 Conclusion We have described a method for integrating auxiliary loss functions with a neural sequence labeling framework, in order to improve error detection in learner writing. While predicting binary error labels, the model also learns to predict additional linguistic information for each token, allowing it to discover compositional features that can be exploited for error detection. We per"
W17-5004,P11-1093,0,0.0610564,"difference being in how the parameters were optimised. contains more fine-grained labels per error. For example, the FCE (Yannakoudakis et al., 2011) training set has 75 different labels for individual error types, such as missing determiners or incorrect verb forms. By giving the model access to these labels, the system can learn more fine-grained error patterns that are based on the individual error types. 4 Evaluation setup and datasets • first language: Previous work has experimentally demonstrated that the distribution of writing errors depends on the first language (L1) of the learner (Rozovskaya and Roth, 2011; Chollampatt et al., 2016). We investigate the usefulness of L1 as an auxiliary objective during training. Rei and Yannakoudakis (2016) investigate a number of compositional architectures for error detection, and present state-of-the-art results using a bidirectional LSTM. We follow their experimental setup and investigate the impact of auxiliary loss functions on the same datasets: the First Certificate in English (FCE) dataset (Yannakoudakis et al., 2011) and the CoNLL-14 shared task test set (Ng et al., 2014b). FCE contains texts written by non-native learners of English in response to exa"
W17-5004,P16-1208,0,0.0676779,"it to be optimised more efficiently and achieve better performance. 1 Introduction Automatic error detection systems for learner writing need to identify various types of error in text, ranging from incorrect uses of function words, such articles and prepositions, to semantic anomalies in content words, such as adjective– noun combinations. To tackle the scarcity of errorannotated training data, previous work has investigated the utility of automatically generated ungrammatical data (Foster and Andersen, 2009; Felice and Yuan, 2014), as well as explored learning from native well-formed data (Rozovskaya and Roth, 2016; Gamon, 2010). In this work, we investigate the utility of supplementing error detection frameworks with additional linguistic information that can be extracted from the available error-annotated learner data. We construct a neural sequence labeling system for error detection that allows us to learn better representations of language composition and de2 Error Detection Model In addition to the scarcity of errors in the training data (i.e., the majority of tokens are correct), recent research has highlighted the variability in manual correction of writing errors: re-annotation of the CoNLL 201"
W17-5004,P10-2065,0,0.0271596,"ually constructed error grammars and mal-rules (e.g., Foster and Vogel 2004). More recent approaches have exploited errorannotated learner corpora and primarily treated the task as a classification problem over vectors of contextual, lexical and syntactic features extracted from a fixed window around the target token. Most work has focused on error-type specific detection models, and in particular on models detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems"
W17-5004,C08-1109,0,0.0623554,"ules (e.g., Foster and Vogel 2004). More recent approaches have exploited errorannotated learner corpora and primarily treated the task as a classification problem over vectors of contextual, lexical and syntactic features extracted from a fixed window around the target token. Most work has focused on error-type specific detection models, and in particular on models detecting preposition and article errors, which are among the most frequent ones in non-native English learner writing (Chodorow et al., 2007; De Felice and Pulman, 2008; Han et al., 2010; Tetreault et al., 2010; Han et al., 2006; Tetreault and Chodorow, 2008; Gamon et al., 2008; Gamon, 2010; Kochmar and Briscoe, 2014; Leacock et al., 2014). Maximum entropy models along with rule-based filters account for a substantial proportion of utilized techniques. Error detection models have also been an integral component of essay scoring systems and writing instruction tools (Burstein et al., 2004; Andersen et al., 2013; Attali and Burstein, 2006). The Helping Our Own (HOO) 2011 shared task on error detection and correction focused on a set of different errors (Dale and Kilgarriff, 2011), though most systems were type specific and targeted closed-class err"
W17-5004,W00-0726,0,0.0590858,"over the basic system. The inclusion of POS tags in the auxiliary objective consistently leads to the highest F0.5 . While GRs also improve performance over the main system, their overall contribution is less compared to the FCE test set, which can be explained by the different writing style in the CoNLL data. 6 effectiveness of our approach, we implement two alternative multi-task learning strategies for error detection. For these experiments, we make use of three established sequence labeling datasets that have been manually annotated for different tasks: • The CoNLL 2000 dataset (Tjong Kim Sang and Buchholz, 2000) for chunking, containing sections of the Wall Street Journal and annotated with 22 different labels. • The CoNLL 2003 corpus (Tjong Kim Sang and De Meulder, 2003) contains texts from the Reuters Corpus and has been annotated with 8 labels for named entity recognition (NER). • The Penn Treebank (PTB) POS corpus (Marcus et al., 1993) contains texts from the Wall Street Journal and has been annotated with 48 POS tags. The CoNLL-00 dataset was identified by Bingel and Søgaard (2017) as being the most useful additional training resource in a multi-task setting; The CoNLL-03 NER dataset has a simil"
W17-5004,W09-2112,0,\N,Missing
W17-5004,C08-1022,0,\N,Missing
W17-5004,W03-0419,0,\N,Missing
W17-5004,P15-1068,0,\N,Missing
W17-5004,P16-1112,1,\N,Missing
W18-5101,Q17-1010,0,0.0498626,"dding for the word ‘c1 . . . ck ’. Bi-directionality of the LSTM allows for the semantics of both the prefix and the suffix (last hidden forward and backward state) of the input word to be captured, which are then combined to form the hidden state for the input word. The model is trained by minimizing the mean squared error (MSE) between the embeddings that it produces and the task-tuned embeddings of words in the training set. This ensures that newly composed embeddings are endowed with characteristics from both the GLoVe space as well as the tasktuning process. While approaches like that of Bojanowski et al. (2017) can also compose embeddings for unseen words, they cannot endow the newly composed embeddings with characteristics from the task-tuning process; this may constitute a significant drawback (Kim, 2014). During the training of our character-based word composition model, to emphasize frequent words, we feed a word as many times as it appears in the training corpus. We note that a 1-layer CNN with global max-pooling in place of the 2-layer LSTM provides comparable performance while requiring significantly less time to train. This is expected since words are not very long sequences, and the filters"
W18-5101,D14-1181,0,0.0479864,"antic information. By not mapping these words to a single random embedding, we mitigate against the errors that may arise due to their conflation (Madhyastha et al., 2015). A special OOV (out of vocabulary) token is also initialized in the same range. All the embeddings are updated during training, allowing for some of the randomly-initialized ones to get 1 We also experimented with 1-layer GRU/LSTM and 1/2layer bi-directional GRUs/LSTMs but the performance only worsened or showed no gains; using sigmoid instead of softmax did not have any noteworthy effects on the results either. task-tuned (Kim, 2014); the ones that do not get tuned lie closely clustered around the OOV token to which unseen words in the test set are mapped. Word-sum (WS). The “LSTM+GLoVe+GBDT” method of Badjatiya et al. (2017) constitutes our second baseline. The authors first employ an LSTM to task-tune GLoVe-initialized word embeddings by propagating error back from an LR layer. They then train a gradient-boosted decision tree (GBDT) classifier to classify texts based on the average of the constituent word embeddings.2 We make two minor modifications to the original method: we utilize a 2-layer GRU3 instead of the LSTM t"
W18-5101,D15-1176,0,0.240267,"rams (AUGMENTED WS + CNG), and Context word-sum + char n-grams (CONTEXT WS + CNG ). These methods are identical to the (context/ augmented) hidden-state + char n-grams methods except that here we include the character n-grams and our character-based word composition model on top of the word-sum baseline. Char hidden-state (CHAR HS) and Char wordsum (CHAR WS). In all the methods described up till now, the input to the core RNN is word embeddings. To gauge whether character-level inputs are themselves sufficient or not, we construct two methods based on the character to word (C 2 W) approach of Ling et al. (2015). For the char hiddenstate method, the input is one-hot representations of characters from a fixed vocabulary. These representations are encoded into a sequence w1 , . . . , wn of intermediate word embeddings by a 2-layer bi-directional LSTM. The word embeddings are then fed into a 2-layer GRU that transforms them into hidden states h1 , . . . , hn . Finally, as in the hidden-state baseline, an LR layer with softmax activation uses the last hidden state hn to perform classification while propagating error backwards to train the network. The char word-sum method is similar except that once the"
W18-5101,C18-1093,1,0.914305,"s alone (Pavlopoulos et al., 2017; Badjatiya et al., 2017). Since the problem of deliberately 1 Proceedings of the Second Workshop on Abusive Language Online (ALW2), pages 1–10 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics noisy input is not explicitly accounted for, these approaches resort to the use of a generic OOV (out of vocabulary) embedding for words not seen in the training phase. However, in using a single embedding for all unseen words, such approaches lose the ability to distinguish obfuscated words from non-obfuscated or rare ones. Recently, Mishra et al. (2018) and Qian et al. (2018), working with the same Twitter dataset as we do, reported that many of the misclassifications by their RNN-based methods happen due to intentional misspellings and/or rare words. Our contributions are two-fold: first, we experimentally demonstrate that character n-gram features are complementary to the current state of the art RNN approaches to abusive language detection and can strengthen their performance. We then explicitly address the problem of deliberately noisy input by constructing a model that operates at the character level and learns to predict embeddings for"
W18-5101,W17-3006,0,0.0372037,"omposition model. In all the models, besides dropout regularization (Srivastava et al., 2014), we hold out a small part of the training set as validation data to prevent over-fitting. We use 300d embeddings and 1 to 5 character ngrams for Wikipedia and 200d embeddings and 1 to 4 character n-grams for Twitter. We implement the models in Keras (Chollet et al., 2015) with Theano back-end. We employ Lightgbm (Ke et al., 2017) as our GDBT classifier and tune its hyper-parameters using 5-fold grid search. 5.2 Twitter results For the Twitter dataset, unlike previous research (Badjatiya et al., 2017; Park and Fung, 2017), we report the macro precision, recall, and F1 averaged over 10 folds of stratified CV (Table 1). For a classification problem with N classes, macro precision (similarly, macro recall and macro F1 ) is given by: N 1 X M acro P = Pi N upon the results obtained with character n-grams. All the improvements are statistically significant with p &lt; 0.05 under 10-fold CV paired t-test. As Ling et al. (2015) noted in their POS tagging experiments, we observe that the CHAR HS and CHAR WS methods perform worse than their counterparts that use pre-trained word embeddings, i.e., the HS and WS baselines re"
W18-5101,W17-3004,0,0.56049,"uch provisions. Nobata et al. (2016) go on to show that simple character n-gram features prove to be highly promising for supervised classification approaches to abuse detection due to their robustness to spelling variations; however, they do not address obfuscations explicitly. Waseem and Hovy (2016) and Wulczyn et al. (2017) also use character ngrams to attain impressive results on their respective datasets. That said, the current state of the art methods do not exploit character-level information, but instead utilize recurrent neural network (RNN) models operating on word embeddings alone (Pavlopoulos et al., 2017; Badjatiya et al., 2017). Since the problem of deliberately 1 Proceedings of the Second Workshop on Abusive Language Online (ALW2), pages 1–10 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics noisy input is not explicitly accounted for, these approaches resort to the use of a generic OOV (out of vocabulary) embedding for words not seen in the training phase. However, in using a single embedding for all unseen words, such approaches lose the ability to distinguish obfuscated words from non-obfuscated or rare ones. Recently, Mishra et al. (2018) and Qian et"
W18-5101,D14-1162,0,0.0938289,"tate of the art results on the Wikipedia datasets. Given a text formed of a sequence w1 , . . . , wn of words (represented by ddimensional word embeddings), the method utilizes a 1-layer GRU to encode the words into hidden states h1 , . . . , hn . This is followed by an LR layer that classifies the text based on the last hidden state hn . We modify the authors’ original architecture in two minor ways: we extend the 1layer GRU to a 2-layer GRU and use softmax as the activation in the LR layer instead of sigmoid.1 Following Pavlopoulos et al., we initialize the word embeddings to GLoVe vectors (Pennington et al., 2014). In all our methods, words not present in the GLoVe set are randomly initialized in the range ±0.05, indicating the lack of semantic information. By not mapping these words to a single random embedding, we mitigate against the errors that may arise due to their conflation (Madhyastha et al., 2015). A special OOV (out of vocabulary) token is also initialized in the same range. All the embeddings are updated during training, allowing for some of the randomly-initialized ones to get 1 We also experimented with 1-layer GRU/LSTM and 1/2layer bi-directional GRUs/LSTMs but the performance only worse"
W18-5101,D17-1010,0,0.0215418,"imply mapped to the OOV token since we do not have a way of obtaining any semantic information about them. However, this is undesirable since racial slurs and expletives are often deliberately fudged by users to prevent detection. In using a single embedding for all unseen words, we lose the ability to distinguish such obfuscations from other non-obfuscated or rare words. Taking inspiration from the effectiveness of character-level features in abuse detection, we address this issue by having a character-based word composition model that can compose embeddings for unseen words in the test set (Pinter et al., 2017). We then augment the hidden-state + char n-grams method with it. 2 In their work, the authors report that initializing embeddings randomly rather than with GLoVe yields state of the art performance on the Twitter dataset that we are using. However, we found the opposite when performing 10-fold stratified cross-validation (CV). A possible explanation of this lies in the authors’ decision to not use stratification, which for such a highly imbalanced dataset can lead to unexpected outcomes (Forman and Scholz, 2010). Furthermore, the authors train their LSTM on the entire dataset including the te"
W18-5101,N18-2019,0,0.0437389,"l., 2017; Badjatiya et al., 2017). Since the problem of deliberately 1 Proceedings of the Second Workshop on Abusive Language Online (ALW2), pages 1–10 c Brussels, Belgium, October 31, 2018. 2018 Association for Computational Linguistics noisy input is not explicitly accounted for, these approaches resort to the use of a generic OOV (out of vocabulary) embedding for words not seen in the training phase. However, in using a single embedding for all unseen words, such approaches lose the ability to distinguish obfuscated words from non-obfuscated or rare ones. Recently, Mishra et al. (2018) and Qian et al. (2018), working with the same Twitter dataset as we do, reported that many of the misclassifications by their RNN-based methods happen due to intentional misspellings and/or rare words. Our contributions are two-fold: first, we experimentally demonstrate that character n-gram features are complementary to the current state of the art RNN approaches to abusive language detection and can strengthen their performance. We then explicitly address the problem of deliberately noisy input by constructing a model that operates at the character level and learns to predict embeddings for unseen words. We show"
W18-5101,W16-5618,0,0.039788,"s formed the optimal feature set for the task. On the other hand, geographic and word-length distribution features provided little to no improvement. Experimenting with the same dataset, Badjatiya et al. (2017) improved on their results by training a gradient-boosted decision tree (GBDT) classifier on averaged word embeddings learnt using a long short-term memory (LSTM) models initialized with random embeddings. Mishra et al. (2018) went on to incorporate community-based profiling features of users in their classification methods, which led to the state of the art performance on this dataset. Waseem (2016) studied the influence of annotators’ knowledge on the task of hate speech detection. For this, they sampled 7k tweets from the same corpus as Waseem and Hovy (2016) and recruited expert and amateur annotators to annotate the tweets as racism, sexism, both or neither. Combining this dataset with that of Waseem and Hovy (2016), Park et al. (2017) evaluated the efficacy of a 2-step classification process: they first used an LR classifier to separate abusive and nonabusive tweets, and then used another LR classifier to distinguish between the racist and sexist ones. They showed that this setup ha"
W18-5101,W17-3000,0,0.130449,"LR layer to classify the comments based on those representations. Davidson et al. (2017) produced a dataset of about 25k racist, offensive or clean tweets. They evaluated several multi-class classifiers with the aim of discerning clean tweets from racist and offensive tweets, while simultaneously being able to distinguish between the racist and offensive ones. Their best model was an LR classifier trained using TF – IDF and POS n-gram features coupled with features like count of hash tags and number of words. 3 Datasets Following the proceedings of the 1st Workshop on Abusive Language Online (Waseem et al., 2017), we use three datasets from two different domains. 3.1 Twitter Waseem and Hovy (2016) prepared a dataset of 16, 914 tweets from a corpus of approximately 136k tweets retrieved over a period of two months. They bootstrapped their collection process with a search for commonly used slurs and expletives related to religious, sexual, gender and ethnic minorities. After having manually annotated 16, 914 of the tweets as racism, sexism or neither, they asked an expert to review their annotations in order to mitigate against any biases. The inter-annotator agreement was reported at κ = 0.84, with fur"
W18-5101,N16-2013,0,0.451279,"ridge United Kingdom pm576@alumni.cam.ac.uk Helen Yannakoudakis The ALTA Institute University of Cambridge United Kingdom hy260@cl.cam.ac.uk Abstract Two conclusions can be drawn from these statistics: (i) abuse (a term we use henceforth to collectively refer to toxic language, hate speech, etc.) is prevalent in social media, and (ii) passive and/or manual techniques for curbing its propagation (such as flagging) are neither effective nor easily scalable (Pavlopoulos et al., 2017). Consequently, the efforts to automate the detection and moderation of such content have been gaining popularity (Waseem and Hovy, 2016; Wulczyn et al., 2017). The advent of social media in recent years has fed into some highly undesirable phenomena such as proliferation of offensive language, hate speech, sexist remarks, etc. on the Internet. In light of this, there have been several efforts to automate the detection and moderation of such abusive content. However, deliberate obfuscation of words by users to evade detection poses a serious challenge to the effectiveness of these efforts. The current state of the art approaches to abusive language detection, based on recurrent neural networks, do not explicitly address this p"
W19-4410,P17-1161,0,0.0466069,"tributions are fourfold: Grammatical error detection (GED) in nonnative writing requires systems to identify a wide range of errors in text written by language learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label distributions are highly imbalanced. Contextualized word representations offer a possible solution, as they can efficiently capture compositional information in language and can be optimized on large amounts of unsupervised data. In this paper, we perform a systematic comparison of ELMo, BERT and Flair embeddings (Peters et al., 2017; Devlin et al., 2018; Akbik et al., 2018) on a range of public GED datasets, and propose an approach to effectively integrate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors. 1 Introduction Detecting errors in text written by language learners is a key component of pedagogical applications for language learning and assessment. Supervised learning approaches to the task exploit publi"
W19-4410,han-etal-2004-detecting,0,0.0801466,"used to further improve performance on GEC (Yannakoudakis et al., 2017). Early approaches to GED and GEC relied upon handwritten rules and error grammars (e.g. Foster and Vogel (2004)), while later work focused on supervised learning from error-annotated corpora using feature engineering approaches and often utilizing maximum entropy-based classifiers (e.g. Chodorow et al. (2007); De Felice and Pulman (2008)). A large range of work has focused on the development of systems targeting specific error types, such as preposition (Tetreault and Chodorow, 2008; Chodorow et al., 2007), article usage (Han et al., 2004, 2006), and verb form errors (Lee and Seneff, 2008). Among others, errortype agnostic approaches have focused on generating synthetic ungrammatical data to augment the available training sets, or learning from native English datasets; for example, Foster and Andersen (2009) investigate rule-based error generation methods, while Gamon (2010) trains a language model (LM) on a large, general domain corpus, from which features (e.g. word likelihoods) are derived for use in error classification. As a distinct task, GEC has been formulated as a na¨ıve-bayes classification (Rozovskaya et al., 2013,"
W19-4410,N18-1202,0,0.612303,"ed based on the context in which the words appear. These embeddings are typically the output of a set of hidden layers of a large language modelling network, trained on large volumes of unlabeled and general domain data. As such, they are able to capture detailed information regarding language and composition from a wide range of data sources, and can help overcome resource limitations for supervised learning. We evaluate the use of contextual embeddings in the form of Bidirectional Encoder Representations from Transformers (BERT) (Devlin et al., 2018), embeddings from Language Models (ELMo) (Peters et al., 2018) and Flair embeddings (Akbik et al., 2018). To the best of our knowledge, this is the first evaluation of the use of contextual embeddings for the task of GED. Our contributions are fourfold: Grammatical error detection (GED) in nonnative writing requires systems to identify a wide range of errors in text written by language learners. Error detection as a purely supervised task can be challenging, as GED datasets are limited in size and the label distributions are highly imbalanced. Contextualized word representations offer a possible solution, as they can efficiently capture compositional inf"
W19-4410,P17-1194,1,0.608825,"e GED performance, recent work has investigated the use of artificially generated training data (Rei et al., 2017; Kasewa et al., 2018). On the related task of grammatical error correction (GEC), Junczys-Dowmunt et al. (2018) explore transfer learning approaches to tackle the low-resource bottleneck of the task and, among others, find substantially improved performance when incorporating pre-trained word embeddings (Mikolov et al., 2013), and importing network weights from a language model trained on a large unlabeled corpus. Herein, we extend the current state of the art for error detection (Rei, 2017) to effectively incorporate contextual embeddings: word representations that are constructed based on the context in which the words appear. These embeddings are typically the output of a set of hidden layers of a large language modelling network, trained on large volumes of unlabeled and general domain data. As such, they are able to capture detailed information regarding language and composition from a wide range of data sources, and can help overcome resource limitations for supervised learning. We evaluate the use of contextual embeddings in the form of Bidirectional Encoder Representation"
W19-4410,W14-1703,0,0.0274892,"mized to detect errors as well as predict their • We present a systematic comparison of different contextualized word representations for the task of GED; • We describe an approach for effectively integrating contextual representations to error detection models, achieving a new state of the 103 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 103–115 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics treated as the source “language” and the corrected text as its target counterpart) (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014; Rozovskaya and Roth, 2016; Yuan and Briscoe, 2016). Recently, Rei and Yannakoudakis (2016) presented the first approach towards neural GED, training a sequence labeling model based on word embeddings processed by a bidirectional LSTM (bi-LSTM), outputting a probability distribution over labels informed by the entire sentence as context. This approach achieves strong results when trained and evaluated on in-domain data, but shows weaker generalization performance on outof-domain data. Rei et al. (2016) extended this model to include character embeddings in order to capture morphological simil"
W19-4410,C16-1030,1,0.927485,"Missing"
W19-4410,W17-5032,1,0.934163,"bjective allows the network to learn more generic features about language and composition. At the same time, Rei and Yannakoudakis (2017) investigated the effectiveness of a number of auxiliary (morpho-syntactic) training objectives for the task of GED, finding that predicting part-ofspeech tags, grammatical relations or error types as auxiliary tasks yields improvements in performance over the single-task GED objective (though not as high as when utilizing an LM objective). The current state of the art on GED is based on augmenting neural approaches with artificially generated training data. Rei et al. (2017) showed improved GED performance using the bi-LSTM sequence labeler, by generating artificial errors in two different ways: 1) learning frequent error patterns from error-annotated corpora and applying these to error-free text; 2) using a statistical MT approach to “translate” correct text to its incorrect counterpart using parallel corpora. Recently, Kasewa et al. (2018) applied the latter approach using a neural MT system instead, and achieved a new state of the art on GED using the neural model of Rei (2017). art on a number of public GED datasets, and make our code and models publicly avai"
W19-4410,W17-5004,1,0.874647,"trong results when trained and evaluated on in-domain data, but shows weaker generalization performance on outof-domain data. Rei et al. (2016) extended this model to include character embeddings in order to capture morphological similarities such as word endings. Rei (2017) subsequently added a secondary LM objective to the neural sequence labeling architecture, operating on both word and character-level embeddings. This was found to be particularly useful for GED – introducing an LM objective allows the network to learn more generic features about language and composition. At the same time, Rei and Yannakoudakis (2017) investigated the effectiveness of a number of auxiliary (morpho-syntactic) training objectives for the task of GED, finding that predicting part-ofspeech tags, grammatical relations or error types as auxiliary tasks yields improvements in performance over the single-task GED objective (though not as high as when utilizing an LM objective). The current state of the art on GED is based on augmenting neural approaches with artificially generated training data. Rei et al. (2017) showed improved GED performance using the bi-LSTM sequence labeler, by generating artificial errors in two different wa"
W19-4410,W13-3602,0,0.0276958,"e usage (Han et al., 2004, 2006), and verb form errors (Lee and Seneff, 2008). Among others, errortype agnostic approaches have focused on generating synthetic ungrammatical data to augment the available training sets, or learning from native English datasets; for example, Foster and Andersen (2009) investigate rule-based error generation methods, while Gamon (2010) trains a language model (LM) on a large, general domain corpus, from which features (e.g. word likelihoods) are derived for use in error classification. As a distinct task, GEC has been formulated as a na¨ıve-bayes classification (Rozovskaya et al., 2013, 2014; Rozovskaya and Roth, 2016) or a monolingual (statistical or neural) machine translation (MT) problem (where uncorrected text is 3 Data In this section, we describe the different public datasets we use to train our models. The First Certificate in English (FCE) dataset 104 compared to systems that miss to detect some errors (Ng et al., 2014). We note that performance on the BEA shared task test set is conducted using the official evaluation tool in CodaLab. We also perform detailed analyses in order to evaluate the performance of our models per error type. As the datasets above either h"
W19-4410,W14-1704,0,0.0575574,"Missing"
W19-4410,P16-1208,0,0.111637,"t their • We present a systematic comparison of different contextualized word representations for the task of GED; • We describe an approach for effectively integrating contextual representations to error detection models, achieving a new state of the 103 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 103–115 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics treated as the source “language” and the corrected text as its target counterpart) (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014; Rozovskaya and Roth, 2016; Yuan and Briscoe, 2016). Recently, Rei and Yannakoudakis (2016) presented the first approach towards neural GED, training a sequence labeling model based on word embeddings processed by a bidirectional LSTM (bi-LSTM), outputting a probability distribution over labels informed by the entire sentence as context. This approach achieves strong results when trained and evaluated on in-domain data, but shows weaker generalization performance on outof-domain data. Rei et al. (2016) extended this model to include character embeddings in order to capture morphological similarities such as word ending"
W19-4410,C08-1109,0,0.0514333,"ion models can be complementary to error correction ones, and can be used to further improve performance on GEC (Yannakoudakis et al., 2017). Early approaches to GED and GEC relied upon handwritten rules and error grammars (e.g. Foster and Vogel (2004)), while later work focused on supervised learning from error-annotated corpora using feature engineering approaches and often utilizing maximum entropy-based classifiers (e.g. Chodorow et al. (2007); De Felice and Pulman (2008)). A large range of work has focused on the development of systems targeting specific error types, such as preposition (Tetreault and Chodorow, 2008; Chodorow et al., 2007), article usage (Han et al., 2004, 2006), and verb form errors (Lee and Seneff, 2008). Among others, errortype agnostic approaches have focused on generating synthetic ungrammatical data to augment the available training sets, or learning from native English datasets; for example, Foster and Andersen (2009) investigate rule-based error generation methods, while Gamon (2010) trains a language model (LM) on a large, general domain corpus, from which features (e.g. word likelihoods) are derived for use in error classification. As a distinct task, GEC has been formulated as"
W19-4410,P11-1019,1,0.944736,"k et al., 2018) on a range of public GED datasets, and propose an approach to effectively integrate such representations in current methods, achieving a new state of the art on GED. We further analyze the strengths and weaknesses of different contextual embeddings for the task at hand, and present detailed analyses of their impact on different types of errors. 1 Introduction Detecting errors in text written by language learners is a key component of pedagogical applications for language learning and assessment. Supervised learning approaches to the task exploit public error-annotated corpora (Yannakoudakis et al., 2011; Ng et al., 2014; Napoles et al., 2017) that are, however, limited in size, in addition to having a biased distribution of labels: the number of correct tokens in a text far outweighs the incorrect (Leacock et al., 2014). As such, Grammatical Error Detection (GED) can be considered a low/mid-resource task. The current state of the art explores error detection within a semi-supervised, multi-task learning framework, using a neural sequence labeler optimized to detect errors as well as predict their • We present a systematic comparison of different contextualized word representations for the ta"
W19-4410,D17-1297,1,0.815123,"ion to overall improvements in performance; • We perform a detailed analysis of the strengths and weaknesses of different contextual representations for the task of GED, presenting detailed results of their impact on different types of errors in order to guide future work. 2 Related work In this section, we describe previous work on GED and on the related task of GEC. While error correction systems can be used for error detection, previous work has shown that standalone error detection models can be complementary to error correction ones, and can be used to further improve performance on GEC (Yannakoudakis et al., 2017). Early approaches to GED and GEC relied upon handwritten rules and error grammars (e.g. Foster and Vogel (2004)), while later work focused on supervised learning from error-annotated corpora using feature engineering approaches and often utilizing maximum entropy-based classifiers (e.g. Chodorow et al. (2007); De Felice and Pulman (2008)). A large range of work has focused on the development of systems targeting specific error types, such as preposition (Tetreault and Chodorow, 2008; Chodorow et al., 2007), article usage (Han et al., 2004, 2006), and verb form errors (Lee and Seneff, 2008). A"
W19-4410,N16-1042,0,0.0130027,"ematic comparison of different contextualized word representations for the task of GED; • We describe an approach for effectively integrating contextual representations to error detection models, achieving a new state of the 103 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 103–115 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics treated as the source “language” and the corrected text as its target counterpart) (Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2014; Rozovskaya and Roth, 2016; Yuan and Briscoe, 2016). Recently, Rei and Yannakoudakis (2016) presented the first approach towards neural GED, training a sequence labeling model based on word embeddings processed by a bidirectional LSTM (bi-LSTM), outputting a probability distribution over labels informed by the entire sentence as context. This approach achieves strong results when trained and evaluated on in-domain data, but shows weaker generalization performance on outof-domain data. Rei et al. (2016) extended this model to include character embeddings in order to capture morphological similarities such as word endings. Rei (2017) subsequentl"
W19-4410,W09-2112,0,\N,Missing
W19-4410,C08-1022,0,\N,Missing
W19-4410,W14-1701,0,\N,Missing
W19-4410,W07-1604,0,\N,Missing
W19-4410,P08-1021,0,\N,Missing
W19-4410,W14-1702,1,\N,Missing
W19-4410,P16-1112,1,\N,Missing
W19-4410,E17-2037,0,\N,Missing
W19-4410,P17-1074,0,\N,Missing
W19-4410,N18-1055,0,\N,Missing
W19-4410,C18-1139,0,\N,Missing
W19-4410,D18-1541,0,\N,Missing
W19-4410,W19-4406,0,\N,Missing
W19-4424,D14-1082,0,0.0413013,"composition, we re-score the enriched input lattice I with the system described in Section 2.2. The FST-based system combination uses 7 different features: the convolutional system score, the LM and NMT scores from the Transformer• Two binary features indicating whether two publicly available spell-checkers – HunSpell2 and JamSpell3 – identify the target word as a spelling mistake. 1 https://github.com/marekrei/ sequence-labeler 2 http://hunspell.github.io/ 3 https://github.com/bakwc/JamSpell 231 • The POS tag, NER label and dependency relation of the target word based on the Stanford parser (Chen and Manning, 2014). false positives (FP) on token-level error detection by treating the error detection model as the “gold standard”. Specifically, we count how many times the candidate FST hypothesis disagrees with the detection model on the tokens identified as incorrect, and use as a 1.0 feature the following: FP+1.0 • The number of times the unigram, bigram, or trigram context of the target word appears in the BNC (Burnard, 2007) and in ukWaC (Ferraresi et al., 2008). We use a linear combination of the above three features together with the original score given by the FST system for each candidate hypothesi"
W19-4424,D14-1179,0,0.0326186,"Missing"
W19-4424,Q17-1010,0,0.00763353,"2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decoder model (Gehring et al., 2017), which employs convolutional neural networks (CNNs) to compute intermediate encoder and decoder states. The parameter settings follow Chollampatt and Ng (2018) and Ge et al. (2018). The source and target word embeddings have size 500, and are initialised with fastText embeddings (Bojanowski et al., 2017) trained on the native English Wikipedia corpus (2, 405, 972, 890 tokens). Each of the encoder and decoder is made up of seven convolutional layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybri"
W19-4424,W18-0529,0,0.105303,"max layer at the output. In addition to neural text representations, we also include several external features into the model, designed to help it learn more accurate error detection patterns from the limited amounts of training data available: Stahlberg et al. (2019) demonstrated the usefulness of FSTs for grammatical error correction. Their method starts with an input lattice I which is generated with a phrase-based statistical machine translation (SMT) system. The lattice I is composed with a number of FSTs that aim to enrich the search space with further possible corrections. Similarly to Bryant and Briscoe (2018), they rely on external knowledge sources like spell checkers and morphological databases to generate additional correction options for the input sentence. The enriched lattice is then mapped to the subword level by composition with a mapping transducer, and re-scored with neural machine translation models and neural LMs. In this work, rather than combining SMT and neural models, we use the framework of Stahlberg et al. (2019) to combine and enrich the outputs of two neural systems. The input lattice I is now the union of two n-best lists – one from the convolutional system (Section 2.1), and"
W19-4424,W19-4406,0,0.0263852,"as a feature the LD between those binary representations. Specifically, we would like to select the candidate FST sentence that has the smallest LD from the binary sequence created by the detection model, and therefore 1.0 use as a feature the following: LD+1.0 • Cambridge English W&I corpus Cambridge English Write & Improve (W&I)5 (Yannakoudakis et al., 2018) is an online web platform that assists non-native English learners with their writing. Learners from around the world submit letters, stories, articles and essays for automated assessment in response to various prompts. The W&I corpus (Bryant et al., 2019) contains 3, 600 annotated submissions across 3 different CEFR6 levels: A (beginner), B (intermediate), and C (advanced). The data has been 4 We note that there are no restrictions on the use of NLP tools (e.g., POS taggers, parsers, spellcheckers, etc.), nor on the amount of unannotated data that can be used, so long as such resources are publicly available. 5 https://writeandimprove.com/ 6 https://www.cambridgeenglish.org/ exams-and-tests/cefr/ 3. False positives: using the binary sequences described above, we count the number of 232 split into training (3, 000 essays), development (200 essa"
W19-4424,W13-1703,0,0.0241772,"atical error detection system was optimized separately as a sequence labeling model. Word embeddings were set to size 300 and initialized with pre-trained Glove embedding (Pennington et al., 2014). The bi-LSTM has 300dimensional hidden layers for each direction. Dropout was applied to word embeddings and LSTM outputs with probability 0.5. The model was optimized with Adam (Kingma and Ba, 2015), using a default learning rate 0.001. Training was stopped when performance on the development set did not improved over 7 epochs. The National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) contains 1, 400 essays written by undergraduate students at the National University of Singapore who are non-native English speakers. • Lang-8 Corpus of Learner English Lang-88 is an online language learning website which encourages users to correct each other’s grammar. The Lang-8 Corpus of Learner English (Mizumoto et al., 2011; Tajiri et al., 2012) refers to an English subsection of this website (can be quite noisy). Additional resources used in our system include: • English Wikipedia corpus The English Wikipedia corpus (2, 405, 972, 890 tokens in 110, 698, 467 sentences) is used to pre-tr"
W19-4424,P17-1074,0,0.263796,"tion 3 is a threshold used to filter out sentence pairs with unnecessary changes; e.g., [I look forward to hearing from you. → I am looking forward to hearing from you.]. It is an avA binary classification task is also introduced to predict whether the original source sentence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learne"
W19-4424,W12-2006,0,0.021788,"rrection (GEC) is the task of automatically correcting grammatical errors in written text. In this paper, we describe our submission to the restricted track of the BEA 2019 shared task on grammatical error correction (Bryant et al., 2019), where participating teams are constrained to using only the provided datasets as training data. Systems are expected to correct errors of all types, including grammatical, lexical and orthographical errors. Compared to previous shared tasks on GEC, which have primarily focused on correcting errors committed by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014), a new annotated dataset is introduced, consisting of essays produced by native and non-native English language learners, 228 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 228–239 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decoder model (Gehring e"
W19-4424,W11-2838,0,0.018545,"uction Grammatical error correction (GEC) is the task of automatically correcting grammatical errors in written text. In this paper, we describe our submission to the restricted track of the BEA 2019 shared task on grammatical error correction (Bryant et al., 2019), where participating teams are constrained to using only the provided datasets as training data. Systems are expected to correct errors of all types, including grammatical, lexical and orthographical errors. Compared to previous shared tasks on GEC, which have primarily focused on correcting errors committed by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014), a new annotated dataset is introduced, consisting of essays produced by native and non-native English language learners, 228 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 228–239 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decod"
W19-4424,P17-1070,0,0.0839707,"Missing"
W19-4424,W14-1702,1,0.843944,"der and decoder is made up of seven convolutional layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybrid systems that combine different approaches have been shown to yield improved performance (Felice et al., 2014; Rozovskaya and Roth, 2016; Grundkiewicz and Junczys-Dowmunt, 2018). In line with this work, we present a hybrid approach that 1) employs two NMT-based error correction systems: a neural convolutional system and a neural Transformerbased system; 2) a finite state transducer (FST) that combines and further enriches the n-best outputs of the NMT systems; 3) a re-ranking system that re-ranks the n-best output of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and pr"
W19-4424,N18-1055,0,0.0833184,"Missing"
W19-4424,D18-1541,0,0.18577,"ed by learners. Some researchers have investigated ways of incorporating task-specific knowledge, either by directly modifying the training objectives (Schmaltz et al., 2017; Sakaguchi et al., 2017; JunczysDowmunt et al., 2018) or by re-ranking machinetranslation-system correction hypotheses (Yannakoudakis et al., 2017; Chollampatt and Ng, 2018). To ameliorate the lack of large amounts of error-annotated learner data, various approaches have proposed to leverage unlabelled native data within a number of frameworks, including artificial error generation with back translation (Rei et al., 2017; Kasewa et al., 2018), fluency boost learning (Ge et al., 2018), and pre-training with denoising autoencoders (Zhao et al., 2019). Previous work has shown that a GEC system In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a system pipeline that utilises both error detection and correction models. The input text is first corrected by two complementary neural machine translation systems: one using convolutional networks and multi-task learning, and another using a neural Transformer-based system. Training is performed on publicly available data, along"
W19-4424,I11-1017,0,0.118142,"Missing"
W19-4424,N18-2046,0,0.036794,"l layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybrid systems that combine different approaches have been shown to yield improved performance (Felice et al., 2014; Rozovskaya and Roth, 2016; Grundkiewicz and Junczys-Dowmunt, 2018). In line with this work, we present a hybrid approach that 1) employs two NMT-based error correction systems: a neural convolutional system and a neural Transformerbased system; 2) a finite state transducer (FST) that combines and further enriches the n-best outputs of the NMT systems; 3) a re-ranking system that re-ranks the n-best output of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and presents our results on the shared task development set; Section 4 pre"
W19-4424,W14-1701,0,0.340725,"Missing"
W19-4424,P16-1154,0,0.0209614,"and target side of the parallel training data respectively. The same BPE operation is applied to the Wikipedia data before being used for training of our word embeddings. Approach We approach the error correction task using a pipeline of systems, as presented in Figure 1. In the following sections, we describe each of these components in detail. 2.1 p(yt |{y1 , ..., yt−1 }, x) t=1 Transformer 2 m Y Copying mechanism is a technique that has led to performance improvement on various monolingual sequence-to-sequence tasks, such as text summarisation, dialogue systems, and paraphrase generation (Gu et al., 2016; Cao et al., 2017). The idea is to allow the decoder to choose between simply copying an original input word and outputting a translation word. Since the source and target sentences are both in the same language (i.e., monolingual translation) and most words in The convolutional neural network (CNN) system We use a neural sequence-to-sequence model and an encoder–decoder architecture (Cho et al., 2014; Sutskever et al., 2014). An encoder first reads and encodes an entire input sequence x = (x1 , x2 , ..., xn ) into hidden state representations. A decoder then generates an output sequence y ="
W19-4424,W13-3601,0,0.0864338,"he task of automatically correcting grammatical errors in written text. In this paper, we describe our submission to the restricted track of the BEA 2019 shared task on grammatical error correction (Bryant et al., 2019), where participating teams are constrained to using only the provided datasets as training data. Systems are expected to correct errors of all types, including grammatical, lexical and orthographical errors. Compared to previous shared tasks on GEC, which have primarily focused on correcting errors committed by non-native speakers (Dale and Kilgarriff, 2011; Dale et al., 2012; Ng et al., 2013, 2014), a new annotated dataset is introduced, consisting of essays produced by native and non-native English language learners, 228 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 228–239 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics based on the input sequence x and all the previously generated tokens {y1 , y2 , ..., yt−1 }: CNN Input text FST Reranking Output text p(y) = (1) Our convolutional neural system is based on a multi-layer convolutional encoder–decoder model (Gehring et al., 2017), whi"
W19-4424,W11-2123,0,0.0115977,"providing error detection labels. Instead of only generating a corrected sentence, we extend the system to additionally predict whether a token in the source sentence is correct or incorrect. f (y) ≤σ f (yok ) (3) where f (y) is the normalised log probability of y: Pm f (y) = • Sentence-level labelling t=1 log(P (yt |y&lt;t )) m (4) This ensures that the quality of the artificially generated sentence, as estimated by a language model, is lower compared to the original sentence. We use a 5-gram language model (LM) trained on the One Billion Word Benchmark dataset (Chelba et al., 2014) with KenLM (Heafield, 2011) to compute P (yt |y&lt;t ). The σ in Equation 3 is a threshold used to filter out sentence pairs with unnecessary changes; e.g., [I look forward to hearing from you. → I am looking forward to hearing from you.]. It is an avA binary classification task is also introduced to predict whether the original source sentence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens usin"
W19-4424,D14-1162,0,0.0842984,"sterov’s Accelerated Gradient Descent (NAG) with a simplified formulation for Nesterov’s momentum (Bengio et al., 2013). The initial learning rate is set to 0.25, with a decaying factor of 0.1 and a momentum value of 0.99. We perform validation after every epoch, and select the best model based on the performance on the development set. During beam search, we keep a beam size of 12 and discard all other hypotheses. The grammatical error detection system was optimized separately as a sequence labeling model. Word embeddings were set to size 300 and initialized with pre-trained Glove embedding (Pennington et al., 2014). The bi-LSTM has 300dimensional hidden layers for each direction. Dropout was applied to word embeddings and LSTM outputs with probability 0.5. The model was optimized with Adam (Kingma and Ba, 2015), using a default learning rate 0.001. Training was stopped when performance on the development set did not improved over 7 epochs. The National University of Singapore Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) contains 1, 400 essays written by undergraduate students at the National University of Singapore who are non-native English speakers. • Lang-8 Corpus of Learner English Lan"
W19-4424,N18-1202,0,0.00953368,"grees with the detection model on the tokens identified as incorrect, and use as a 1.0 feature the following: FP+1.0 • The number of times the unigram, bigram, or trigram context of the target word appears in the BNC (Burnard, 2007) and in ukWaC (Ferraresi et al., 2008). We use a linear combination of the above three features together with the original score given by the FST system for each candidate hypothesis to re-rank the FST system’s 8-best list in an unsupervised way. The new 1-best correction hypothesis c∗ is then the one that maximises: • Contextualized word representations from ELMo (Peters et al., 2018). The discrete features are represented as 10dimensional embeddings and, together with the continuous features, concatenated to each word representation in the model. The overall architecture is optimized for error detection using crossentropy. Once trained, the model returns the predicted probabilities of each token in a sentence being correct or incorrect. c∗ = arg max c K X λi hi (c) (6) i=1 where h represents the score assigned to candidate hypothesis c according to feature i; λ is a weighting parameter that controls the effect feature i has on the final ranking; and K = 4 as we use a tota"
W19-4424,P16-1162,0,0.339169,"of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and presents our results on the shared task development set; Section 4 presents our official results on the shared task test set, including a detailed analysis of the performance of our final system; and, finally, Section 5 concludes the paper and provides an overview of our findings. BPE is introduced to alleviate the rare-word problem, and rare and unknown words are split into multiple frequent subword tokens (Sennrich et al., 2016b). NMT systems often limit vocabulary size on both source and target sides due to the computational complexity during training. Therefore, they are unable to translate out-of-vocabulary (OOV) words, which are treated as unknown tokens, resulting in poor translation quality. As noted by Yuan and Briscoe (2016), this problem is more serious for GEC as non-native text contains, not only rare words (e.g., proper nouns), but also misspelled words (i.e., spelling errors). In our model, each of the source and target vocabularies consist of the 30K most frequent BPE tokens from the source and target"
W19-4424,P17-1194,1,0.930878,"ing (NLP) (Collobert and Weston, 2008) and error generation system using the same network speech recognition (Deng et al., 2013) to computer architecture as the one described here, with errorvision (Girshick, 2015). Multi-task learning alcorrected sentences as the source and their correlows systems to use information from related tasks sponding uncorrected counterparts written by lanand learn from multiple objectives, which leads guage learners as the target. The system is then to performance improvement on individual tasks. used to collect the n-best outputs: yo1 , yo2 , ..., yon , Recently, Rei (2017) and Rei and Yannakoudakis for a given error-free native and/or learner sen(2017) investigated the use of different auxiliary tence y. Since there is no guarantee that the error objectives for the task of error detection in learner generation system will inject errors into the input writing. sentence y to make it less grammatically correct, In addition to our primary error correction task, we apply “quality control”. A pair of artificially we propose two related auxiliary objectives to generated sentences (yok , y), for k ∈ {1, 2, ..., n}, boost model performance: will be added to the training"
W19-4424,N19-1406,1,0.911367,"the original source sentence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previo"
W19-4424,W17-5032,1,0.94333,"andard language used by learners. Some researchers have investigated ways of incorporating task-specific knowledge, either by directly modifying the training objectives (Schmaltz et al., 2017; Sakaguchi et al., 2017; JunczysDowmunt et al., 2018) or by re-ranking machinetranslation-system correction hypotheses (Yannakoudakis et al., 2017; Chollampatt and Ng, 2018). To ameliorate the lack of large amounts of error-annotated learner data, various approaches have proposed to leverage unlabelled native data within a number of frameworks, including artificial error generation with back translation (Rei et al., 2017; Kasewa et al., 2018), fluency boost learning (Ge et al., 2018), and pre-training with denoising autoencoders (Zhao et al., 2019). Previous work has shown that a GEC system In this paper, we describe our submission to the BEA 2019 shared task on grammatical error correction. We present a system pipeline that utilises both error detection and correction models. The input text is first corrected by two complementary neural machine translation systems: one using convolutional networks and multi-task learning, and another using a neural Transformer-based system. Training is performed on publicly"
W19-4424,W19-4417,1,0.757125,"ence is grammatically correct or incorrect. We investigate the usefulness of sentencelevel classification as an auxiliary objective for training error correction models. Labels for both auxiliary error detection tasks are generated automatically by comparing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previous section, we also use the"
W19-4424,P16-1112,1,0.857116,"n SMT system based on error detection predictions. Following this work, we also deploy a re-ranking component which re-ranks the n-best correction hypotheses of the FST system (Section 2.3) based on error detection predictions output by an error detection system. FST-based system combination Error detection. Our system for grammatical error detection is based on the model described by Rei (2017).1 The task is formulated as a sequence labeling problem – given a sentence, the model assigns a probability to each token, indicating the likelihood of that token being incorrect in the given context (Rei and Yannakoudakis, 2016). The architecture maps words to distributed embeddings, while also constructing character-based representations for each word with a neural component. These are then passed through a bidirectional LSTM, followed by a feed-forward layer and a softmax layer at the output. In addition to neural text representations, we also include several external features into the model, designed to help it learn more accurate error detection patterns from the limited amounts of training data available: Stahlberg et al. (2019) demonstrated the usefulness of FSTs for grammatical error correction. Their method s"
W19-4424,D17-2005,1,0.86093,"ing source and target tokens using the ERRANT automatic alignment tool (Bryant et al., 2017). We first align each 230 based system, the edit distance of hypotheses in I to the input sentence, substitution and deletion penalties for the additional correction options from the FST framework, and the word count. Following Stahlberg et al. (2019); Stahlberg and Byrne (2019), we scale these features and tune the scaling weights on the BEA-2019 development set using a variant of Powell search (Powell, 1964). We use OpenFST (Allauzen et al., 2007) as backend for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previous section, we also use the purely neural Transformer-based system of Stahlberg and Byrne (2019). They use an ensemble of four Transformer (Vaswani et al., 2017) NMT and two Transformer LM models in Tensor2Tensor (Vaswani et al., 2018) transformer big configuration. The NMT mode"
W19-4424,W17-5004,1,0.919341,"Missing"
W19-4424,W18-1821,1,0.843308,"Missing"
W19-4424,P16-1208,0,0.0167945,"de up of seven convolutional layers, with a convolution window width of 3. We apply a left-to-right beam search to find a correction that approximately maximises the conditional probability in Equation 1. Figure 1: Overview of our best GEC system pipeline. targeting all errors may not necessarily be the best approach to the task, and that different GEC systems may be better suited to correcting different types of errors, and can therefore be complementary (Yuan, 2017). As such, hybrid systems that combine different approaches have been shown to yield improved performance (Felice et al., 2014; Rozovskaya and Roth, 2016; Grundkiewicz and Junczys-Dowmunt, 2018). In line with this work, we present a hybrid approach that 1) employs two NMT-based error correction systems: a neural convolutional system and a neural Transformerbased system; 2) a finite state transducer (FST) that combines and further enriches the n-best outputs of the NMT systems; 3) a re-ranking system that re-ranks the n-best output of the FST based on error detection features. The remainder of this paper is organised as follows: Section 2 describes our approach to the task; Section 3 describes the datasets used and presents our results on the s"
W19-4424,I17-2062,0,0.0663944,"Missing"
W19-4424,P12-2039,0,0.20414,"Missing"
W19-4424,D17-1298,0,0.0244066,"Missing"
W19-4424,W18-1819,0,0.0256159,"for FST operations, and the SGNMT decoder (Stahlberg et al., 2017, 2018) for neural decoding under FST constraints. eraged score learned on the development set: f (yi ) i=1 f (xi ) PN σ= N (5) where (x, y) is a pair of parallel sentences in the development set, and N is the total number of pairs. 2.2 The neural Transformer-based system Besides the convolutional system from the previous section, we also use the purely neural Transformer-based system of Stahlberg and Byrne (2019). They use an ensemble of four Transformer (Vaswani et al., 2017) NMT and two Transformer LM models in Tensor2Tensor (Vaswani et al., 2018) transformer big configuration. The NMT models are trained with backtranslation (Sennrich et al., 2016a; Rei et al., 2017; Kasewa et al., 2018) and fine-tuning through continued training. For a detailed description of this system we refer the reader to Stahlberg and Byrne (2019). 2.3 2.4 Re-ranking FST output Yannakoudakis et al. (2017) found that grammatical error detection systems can be used to improve error correction outputs. Specifically, they re-rank the n-best correction hypotheses of an SMT system based on error detection predictions. Following this work, we also deploy a re-ranking c"
W19-4424,P11-1019,1,0.784266,"section of 100 essays has been manually annotated, and equally partitioned into development and test sets. In order to cover the full range of English levels and abilities, the official development set consists of 300 essays from W&I (A: 130, B:100, and C:70) and 50 essays from LOCNESS (86, 973 tokens in 4, 384 sentences). The ERRANT scorer (Bryant et al., 2017) is used as the official scorer for the shared task. System performance is evaluated in terms of spanlevel correction using F0.5 , which emphasises precision twice as much as recall. • FCE The First Certificate in English (FCE) corpus (Yannakoudakis et al., 2011) is a subset of the Cambridge Learner Corpus (CLC) that consists of 1, 244 exam scripts written by learners of English sitting the FCE exam. 3.2 • NUCLE Training details The convolutional NMT model is trained with a hidden layer size of 1, 024 for both the encoder and the decoder. Dropout at a rate of 0.2 is applied to the embedding layers, convolutional layers and decoder output. The model is optimized using Nesterov’s Accelerated Gradient Descent (NAG) with a simplified formulation for Nesterov’s momentum (Bengio et al., 2013). The initial learning rate is set to 0.25, with a decaying factor"
W19-4424,D17-1297,1,0.858113,"ystem Besides the convolutional system from the previous section, we also use the purely neural Transformer-based system of Stahlberg and Byrne (2019). They use an ensemble of four Transformer (Vaswani et al., 2017) NMT and two Transformer LM models in Tensor2Tensor (Vaswani et al., 2018) transformer big configuration. The NMT models are trained with backtranslation (Sennrich et al., 2016a; Rei et al., 2017; Kasewa et al., 2018) and fine-tuning through continued training. For a detailed description of this system we refer the reader to Stahlberg and Byrne (2019). 2.3 2.4 Re-ranking FST output Yannakoudakis et al. (2017) found that grammatical error detection systems can be used to improve error correction outputs. Specifically, they re-rank the n-best correction hypotheses of an SMT system based on error detection predictions. Following this work, we also deploy a re-ranking component which re-ranks the n-best correction hypotheses of the FST system (Section 2.3) based on error detection predictions output by an error detection system. FST-based system combination Error detection. Our system for grammatical error detection is based on the model described by Rei (2017).1 The task is formulated as a sequence l"
W19-4424,N16-1042,1,0.819082,"set, including a detailed analysis of the performance of our final system; and, finally, Section 5 concludes the paper and provides an overview of our findings. BPE is introduced to alleviate the rare-word problem, and rare and unknown words are split into multiple frequent subword tokens (Sennrich et al., 2016b). NMT systems often limit vocabulary size on both source and target sides due to the computational complexity during training. Therefore, they are unable to translate out-of-vocabulary (OOV) words, which are treated as unknown tokens, resulting in poor translation quality. As noted by Yuan and Briscoe (2016), this problem is more serious for GEC as non-native text contains, not only rare words (e.g., proper nouns), but also misspelled words (i.e., spelling errors). In our model, each of the source and target vocabularies consist of the 30K most frequent BPE tokens from the source and target side of the parallel training data respectively. The same BPE operation is applied to the Wikipedia data before being used for training of our word embeddings. Approach We approach the error correction task using a pipeline of systems, as presented in Figure 1. In the following sections, we describe each of th"
W19-4424,N19-1014,0,0.20593,"Missing"
