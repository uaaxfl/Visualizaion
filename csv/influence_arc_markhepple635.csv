C90-2030,E89-1002,1,0.420481,"a c k &apos; the type and &apos; e q u a t e &apos; subformulas of this in further axiom instances. The l a m b d a expressions assigned by these two proofs are equivalent under r/-reduction. The existence of multiple equivalent proofs presents a problem for efficient t h e o r e m proving based on the calculus L. Search for proofs must be exhaustive to ensure t h a t all different &apos;readings&apos; for a given sequent are found, and a naive theorem prover will expend considerable effort constructing proofs that assign the same meaning. This radically reduces the efficiency of L a m b e k Calculus theorem proving. Hepple and Morrill (1989), working with a somewhat different CG framework t h a t also admits multiple equivalent proofs, suggest t h a t this problem be dealt with by deriving a notion of n o r m a l form (NF) for proofs, and then a d a p t i n g the parsing m e t h o d such t h a t this only returns N F proofs. Khnig (1989) takes this kind approach to handling the problem of multiple equivalent proofs for L a m b e k sequent theorem proving, generating a notion of N F for such proofs, and deriving from this a parsing strategy intended to c o m p u t e only NF proofs. :However, Khnig&apos;s parsing algorithm fails to excl"
C90-2030,P89-1033,0,0.3119,"ofs must be exhaustive to ensure t h a t all different &apos;readings&apos; for a given sequent are found, and a naive theorem prover will expend considerable effort constructing proofs that assign the same meaning. This radically reduces the efficiency of L a m b e k Calculus theorem proving. Hepple and Morrill (1989), working with a somewhat different CG framework t h a t also admits multiple equivalent proofs, suggest t h a t this problem be dealt with by deriving a notion of n o r m a l form (NF) for proofs, and then a d a p t i n g the parsing m e t h o d such t h a t this only returns N F proofs. Khnig (1989) takes this kind approach to handling the problem of multiple equivalent proofs for L a m b e k sequent theorem proving, generating a notion of N F for such proofs, and deriving from this a parsing strategy intended to c o m p u t e only NF proofs. :However, Khnig&apos;s parsing algorithm fails to exclude all n o n - N F proofs when used with a s t a n d a r d propositional L a m b e k Calculus. In this paper I define a NF system for the sequent formulation of the (product-free) Lambek Calculus, which gives rise to a parsing approach that yields only n o r m a l proofs. 3 A New Approach 3.1 H e a d"
C92-1024,E91-1035,1,0.837009,") a calculus which determines the set of admitted type combinations. The set of types (T) is defined recursivety in terms of a set of basic types (To) and a set of operators ( {  , / } for standard bidirectional CG), as the smallest set such t h a t (i) To C T, (ii) /f x,y E T, then x  y , x / y E T. 2 Intuitively, lexical types specify subcategorisation requirements of words, and requirements on constituent order. We here address a particular flexible CG, the (product-free) Lambek calculus (L: Lambek, 1958). The rules below provide a natural deduction formulation of L (Morrill et al. 1990; Barry et al. 1991), where dots above a type represent a proof of t h a t type. Proofs proceed from a number of initial assumptions, consisting of individual types, some of which m a y be ""discharged"" as the proof is constructed. Eact~ type in a proof is associated with a l a m b d a expression, corresponding to its meaning. The elimination r u l e / E states t h a t proofs of A / B and B may be combined to construct a proof of A. The introduction rule /l indicates t h a t we m a y discharge an assumption B within a proof of A to construct a proof of A / B (square brackets indicating the assumption's discharge)."
C92-1024,C90-2030,1,0.878584,"res this abstraction, e.g. the relative pronoun type rel/(s/np) abstracts over np. Note that this relative pronoun type cannot extract out of an embedded modal domain, because it abstracts over a bare (i.e. non-modal) np, whose presence would block O1 rule's use in deriving the overall modal constituent. However, a relative pronoun rel/(s/[]np), which abstracts over a modal type Drip, can extract out of an embedded modal domain. Including this operator presents considerable problems for efficient processing. Firstly, it excludes the use of the NF systems devised for the calculus (KSnig, 1989; Hepple, 1990a). As noted above~ spurious ambiguity makes ordinary (i.e. non-normal form) sequent theorem proving of L inefficient. This problem is greatly increased by inclusion of O, largely due to nondeterminism for use of the OE rule. 6 6Conaldrr a sequent S = Ox! ,l:]x:h..., nXn ::~ x0, where the r~l~ted amquent Ss ~ Xl,X2,... ,xn ~ x0 is a theorem. Nondeterminism for use of [[]L] means that there are n{ different paths of inference from S to St, so that there are at least n! proofs of S for each proof of SI. In fact, interaction of [OLl with other inference ruler means that there is typically many mo"
C92-1024,P91-1011,1,0.542666,"ble degree of inerementality, some conceivable incremental constituents will not be created that would be in parsing with alternative categorial frameworks. For example, rules of type raising and composition in Combinatory Catcgorial Grammar (Steedman, 1987; Szabolcsi, 1987) would allow incremental combination of types vp/s, np ::~ vp/(s
p), not allowed by the present approach. The modified chart method instead allows for the construction of incremental constituents in a manner that most closely relates to tim notion of dependency constituency argued for by Barry & Pickering (1990) (see also Hepple, 1991), although since the modified parser is still a complete parser for L it cannot be viewed as implementing a notion of dependency constituency. 7 Finally, it should be noted that the additional hypothetical reasoning allowed by emit* and combinations involving additional 'incremental constituents' result in many 'spurious' analyses, so that the incremental chart method is in general slower than the non-incremental chart method. Conclusion I have presented a chart parsing method for the Lambek calculus, which I would argue has several advantages over that of KSnig (1990, 1991). Firstly, I believ"
C92-1024,P89-1033,0,0.0982167,"t h a t requires this abstraction, e.g. the relative pronoun type rel/(s/np) abstracts over np. Note that this relative pronoun type cannot extract out of an embedded modal domain, because it abstracts over a bare (i.e. non-modal) np, whose presence would block O1 rule's use in deriving the overall modal constituent. However, a relative pronoun rel/(s/[]np), which abstracts over a modal type Drip, can extract out of an embedded modal domain. Including this operator presents considerable problems for efficient processing. Firstly, it excludes the use of the NF systems devised for the calculus (KSnig, 1989; Hepple, 1990a). As noted above~ spurious ambiguity makes ordinary (i.e. non-normal form) sequent theorem proving of L inefficient. This problem is greatly increased by inclusion of O, largely due to nondeterminism for use of the OE rule. 6 6Conaldrr a sequent S = Ox! ,l:]x:h..., nXn ::~ x0, where the r~l~ted amquent Ss ~ Xl,X2,... ,xn ~ x0 is a theorem. Nondeterminism for use of [[]L] means that there are n{ different paths of inference from S to St, so that there are at least n! proofs of S for each proof of SI. In fact, interaction of [OLl with other inference ruler means that there is typ"
C92-1024,C90-2041,0,0.4824,"Missing"
C94-2201,E91-1035,1,\N,Missing
C94-2201,E93-1034,0,\N,Missing
C96-1091,C90-2030,1,0.912115,"Missing"
C96-1091,C92-1024,1,0.881223,"Missing"
C96-1091,E95-1018,1,0.847376,"Missing"
C96-1091,P89-1033,0,0.101898,"Missing"
C96-1091,C90-2041,0,0.838372,"Missing"
C96-1091,E95-1019,0,0.653008,"results ti&apos;om a combination is inarked with the union of the index sets of the two formulae combined. 4 We. can ensure that no initial assumption contributes more than once to any deduction by requiring that wherever two tbrmulae are combined, their index sets must be disjoint. Thus, we require the following modified [o-El rule (where ¢, &apos;~/~,vr arc&apos;. index sets, and t0 denotes union of sets that are required to be disjoint): 4): A o - B : a &apos;~/~:B : b • - (bao&apos;,/~ 7r: A: (all) In proving I&apos; =&gt; A, a snccessflfl ow&apos;,rall analysis is recognised by the prescmee of a database formula 4See Llord & Morrill (1995) fbr a related use of indexing in ensuring linear use of resources. 539 A whose index set is the flfll set of indices assigned to the initial formulae in P. For&apos; example, to prove X o - X , X o - X , Xo--Y, Y =&gt; X, we might start with a database containing entries as fbllows (the tmmbering of entries is purely for exposition): I. i:Xo--X:v 2. j :Xo-X :w 3. k:Xo-Y:z 4. l:Y:y Use of the modified elimination rule gives additional fornmlae as follows: 5. { k , / } : X: z y [3+4] 6. {i, k, 1}: X: v(a:y) [1-t-5] 7. {j, k, l}: X: w(zy) [2-1-5] 8. {i,j,k,1}:X:v(w(xy)) [1+7] 9. {i,j,k,l}:X:w(v(a:y)) [2"
C98-1085,E93-1013,0,0.0756066,"Missing"
C98-1085,P97-1019,0,0.0319725,"Missing"
C98-1085,C92-1024,1,0.805548,"ved, with appropriate indexation being ensured by the condition rc = ¢t2¢ of tile rule (where t2 stands for disjoint union, which enforces linear usage). ¢ : a o - ( B : a ) : kv.a re: A: a[b//v] ¢ :B :b ~r = qSU¢ ~,c_¢ 2The key division here is between higher-order formulae, which a r e a r e functors that seek at least one argument that bears a a functional type (e.g. W o - ( X o - Z ) ) , and first-order formulae, which seek no such argument. aThis 'excision' step has parallels to the 'emit' step used in the chart-parsing approaches for the associative Lambek calculus of (KSnig, 1994) and (Hepple, 1992), although the latters differs in that there is no removal of the relevant subformula, i.e. tile 'emitting formula' is not simplified, remaining higher-order. Assumptions (1) and (4) both come from Xo-(Yo-Z): note how (1)'s argument is marked with (4)'s index (j). The condition a C ¢ of the rule ensures that (4) must contribute to the derivation of (1)'s argument. Finally, observe that the rule's semantics involves not simple application, but rather by direct substitution for the variable of a lambda expression, employing a special variant of substitution, notated _[_//_], which specifically d"
C98-1085,E95-1018,1,0.859838,"roof terms for successflfl analyses. 7 Application :#:1: Categorial Parsing The associative Lambek calculus (Lambek, 1958) is perhaps the most familiar representative of the class of categorial formalisms that fall within the 'type-logical' tradition. Recent work has seen proposals for a range of such systems, differing in their resource sensitivity (and hence, implicitly, their underlying notion of 'linguistic structure'), in some cases combining differing resource sensitivities in one system. 8 Many of SSee, for example, the formalisms developed in (Moortgat et al., 1994), (Morrill, 1994), (Hepple, 1995). 543 these proposals employ a 'labelled deductive system' methodology (Gabbay, 1996), whereby types in proofs are associated with labels which record proof information for use in ensuring correct inferencing. A natural 'base logic' on which to construct such systems is the multiplicatire fragment of linear logic, since (i) it stands above the various categorial systems in the hierarchy of substructural logics, and (ii) its operators correspond to precisely those appearing in any standard categorial logic. The key requirement for parsing categorial systems formulated in this way is some theore"
C98-1085,C96-1091,1,0.683988,"rmulating various categorial grammar systems. Linear deduction methods provide a common basis for parsing categorial systems formulated in this way. Secondly, the multiplicative fragment forms the core of the system used in work by Dalrymple and colleagues for handling the semantics of LFG derivations, providing a 'glue language' for assembling the meanings of sentences from those of words and phrases. Although there are a number of deduction methods for multiplicative linear logic, there is a notable absence of tabular methods, which, like chart parsing for CFGs, avoid redundant computation. Hepple (1996) presents a compilation method which allows for tabular deduction for 538 A:a A: (ab) o-I A o - B : Av.a [B:xl,[C:v] B®C: A:a A:a B:b QE A : E~,y(b,a) ~I A•B: (a ® b) The elimination (E) and introduction (I) rules for o-- correspond to steps of flmctional application and abstraction, respectively, as the term labelling reveals. The o-I rule discharges precisely one assumption (B) within the proof to which it applies. The ®I rule pairs together the premise terms, whereas ®E has a substitution like meaning. 1 Proofs that Wo---(Xo---Z), Xo-Y, Yo---Z =~ W and that Xo--yo-Z, Y®Z =~ X follow: Wo-(Xo"
C98-1085,E95-1019,0,0.0161616,"uch systems is the multiplicatire fragment of linear logic, since (i) it stands above the various categorial systems in the hierarchy of substructural logics, and (ii) its operators correspond to precisely those appearing in any standard categorial logic. The key requirement for parsing categorial systems formulated in this way is some theorem proving inethod that is sufficient for the fragment of linear logic employed (although some additional work will be required for managing labels), and a number of different approaches have been used, e.g. proof nets (Moortgat, 1992), and SLD resolution (Morrill, 1995). Hepple (1996) introduces first-order compilation for implicational linear logic, and shows how that m e t h o d can be used with labelling as a basis parsing implicational categorial systems. No further complications arise for combining the extended compilation approach described in this paper with labelling systems as a basis for efficient, non-redundant parsing of categorial formalisms in the core multiplicative fragment. See (Hepple, 1996) for a worked example. 8 Application 7~2: Glue Language Deduction In a line of research beginning with Dalrymple et al. (1993), a fragment of linear log"
C98-1111,P97-1021,0,0.0298271,"Missing"
C98-1111,P96-1025,0,0.0388392,"Missing"
C98-1111,P98-1115,1,0.0540771,"Missing"
C98-1111,H94-1020,0,0.0262288,"art-of-speech tags), we may be able to achieve parsing performance similar to the best results in the field obtained in (Collins, 1996). 2 Growth of the Rule Set One could investigate whether there is a finite g r a m m a r that should account for any text within a class of related texts (i.e. a domain oriented sub-grammar of English). If there is, the number of extracted rules will approach a limit as more sentences are processed, i.e. as the rule n u m b e r approaches the size of such an underlying and finite grammar. We had hoped that some approach to a limit would be seen using P T B II (Marcus et al., 1994), which larger and more consistent for bracketting than P T B I. As shown in Figure 1, however, the rule number growth continues unabated even after more t h a n 1 million part-ofspeech tokens have been processed. 700 Rule Growth and Partial Bracketting Why should the set of rules continue to grow in this way? P u t t i n g aside the possibility that natural languages do not have finite rule sets, we can think of two possible answers. First, it may be that the full ""underlying g r a m m a r "" is much larger than the rule set that has so far been produced, requiring a much larger tree-banked co"
C98-1111,1995.iwpt-1.26,0,0.0914475,"Missing"
C98-1111,J95-2002,0,0.0866185,"Missing"
D10-1026,W06-3113,0,0.178967,"j, 2001), which reduces the value information stored with n-grams to a limited set of discrete alternatives. It works by grouping together the values (probabilities or counts) associated with n-grams into clusters, and replacing the value to be stored for each n-gram with a code identifying its value’s cluster. For a scheme with n clusters, codes require log2 n bits. A common case is 8-bit quantization, allowing up to 256 distinct ‘quantum’ values. Different methods of dividing the range of values into clusters have been used, e.g. Whittaker and Raj (2001) used the Lloyd-Max algorithm, whilst Federico and Bertoldi (2006) use the simpler Binning method to quantize probabilities, and show that the LMs so produced out-perform those produced using the Lloyd-Max method on a phrase-based ma263 chine translation task. Binning partitions the range of values into regions that are uniformly populated, i.e. producing clusters that contain the same number of unique values. Functionality to perform uniform quantization of this kind is provided as part of various LM toolkits, such as IRSTLM. Some of the empirical storage results reported later in the paper relate to LMs recording n-gram count values which have been quantiz"
D10-1026,W07-0712,0,0.224934,"ntly represented by a single node within the trie. For language modeling purposes, the steps through the trie correspond to words of the vocabulary, although these are in practice usually represented by 24 or 32 bit integers (that have been uniquely assigned to each word). Nodes in the trie corresponding to complete n-grams can store other information, e.g. a probability or count value. Most modern language modeling toolkits employ some version of a trie structure for storage, including SRILM (Stolcke, 2002), CMU toolkit (Clarkson and Rosenfeld, 1997), MITLM (Hsu and Glass, 2008), and IRSTLM (Federico and Cettolo, 2007) and implementations exist which are very compact (Germann et al., 2009). An advantage of this structure is that it allows the stored n-grams to be enumerated. However, although this approach achieves a compact of representation of sequences, its memory costs are still such that very large language models require very large storage space, far more than the Bloom filter based methods described shortly, and far more than might be held in memory as a basis for more rapid access. The memory costs of such models have been addressed using compression methods, see Harb et al. (2009), but such extensi"
D10-1026,W09-1505,0,0.672891,"erico and Cettolo, 2007) and implementations exist which are very compact (Germann et al., 2009). An advantage of this structure is that it allows the stored n-grams to be enumerated. However, although this approach achieves a compact of representation of sequences, its memory costs are still such that very large language models require very large storage space, far more than the Bloom filter based methods described shortly, and far more than might be held in memory as a basis for more rapid access. The memory costs of such models have been addressed using compression methods, see Harb et al. (2009), but such extensions of the approach present further obstacles to rapid access. 2.2 Bloom Filter Based Language Models Recent randomized language models (Talbot and Osborne, 2007b; Talbot and Osborne, 2007a; Talbot and Brants, 2008; Talbot and Talbot, 2008; Talbot, 2009) make use of Bloom filter like structures to map n-grams to their associated probabilities or counts. These methods store language models in relatively little space by not actually keeping the ngram key in the structure and by allowing a small probability of returning a false positive, i.e. so that for an n-gram that is not in"
D10-1026,P07-2045,0,0.0038167,"therefore in this section we measure query speed on a large set of n-grams and compare it to that of modern language modeling toolkits. We build a model of all unigrams and bigrams in the Gigaword corpus (see Table 1) using the CMPHR method, SRILM (Stolcke, 2002), IRSTLM (Federico and Cettolo, 2007), and randLM3 (Talbot and Osborne, 2007a) toolkits. RandLM is a modern language modeling toolkit that uses Bloom filter based structures to store large language models and has been integrated so that it can be used as the language model storage for the Moses statistical machine translation system (Koehn et al., 2007). We use randLM with the BloomMap (Talbot and Talbot, 2008) storage structure option with 8 bit quantized values and an error rate equivalent to using 8 bit fingerprints (as recommended in the Moses documentation). All methods are implemented in C++ and are run on a machine with 2.80GHz Intel Xeon E5462 processor and 64 GB of RAM. In addition we show a comparison to using a modern database, MySQL 5.0, to store the same data. We measure the speed of querying all models for the 55 million distinct bigrams that occur in the Gigaword, 2 All T-MPHR results are for optimal configurations: Gigaword f"
D10-1026,P08-1058,0,0.463491,"and the Google Web1T 1-5gram corpus (Brants and Franz, 2006), have made it possible to build models incorporating counts of billions of n-grams. The storage of these language models, however, presents Mark Hepple Computer Science Department University of Sheffield M.Hepple@dcs.shef.ac.uk serious problems, given both their size and the need to provide rapid access. A prevalent approach for language model storage is the use of compact trie structures, but these structures do not scale well and require space proportional to both to the number of n-grams and the vocabulary size. Recent advances (Talbot and Brants, 2008; Talbot and Osborne, 2007b) involve the development of Bloom filter based models, which allow a considerable reduction in the space required to store a model, at the cost of allowing some limited extent of false positives when the model is queried with previously unseen n-grams. The aim is to achieve sufficiently compact representation that even very large language models can be stored totally within memory, avoiding the latencies of disk access. These Bloom filter based models exploit the idea that it is not actually necessary to store the n-grams of the model, as long as, when queried with"
D10-1026,P07-1065,0,0.569596,"gram corpus (Brants and Franz, 2006), have made it possible to build models incorporating counts of billions of n-grams. The storage of these language models, however, presents Mark Hepple Computer Science Department University of Sheffield M.Hepple@dcs.shef.ac.uk serious problems, given both their size and the need to provide rapid access. A prevalent approach for language model storage is the use of compact trie structures, but these structures do not scale well and require space proportional to both to the number of n-grams and the vocabulary size. Recent advances (Talbot and Brants, 2008; Talbot and Osborne, 2007b) involve the development of Bloom filter based models, which allow a considerable reduction in the space required to store a model, at the cost of allowing some limited extent of false positives when the model is queried with previously unseen n-grams. The aim is to achieve sufficiently compact representation that even very large language models can be stored totally within memory, avoiding the latencies of disk access. These Bloom filter based models exploit the idea that it is not actually necessary to store the n-grams of the model, as long as, when queried with an n-gram, the model retur"
D10-1026,D07-1049,0,0.493369,"gram corpus (Brants and Franz, 2006), have made it possible to build models incorporating counts of billions of n-grams. The storage of these language models, however, presents Mark Hepple Computer Science Department University of Sheffield M.Hepple@dcs.shef.ac.uk serious problems, given both their size and the need to provide rapid access. A prevalent approach for language model storage is the use of compact trie structures, but these structures do not scale well and require space proportional to both to the number of n-grams and the vocabulary size. Recent advances (Talbot and Brants, 2008; Talbot and Osborne, 2007b) involve the development of Bloom filter based models, which allow a considerable reduction in the space required to store a model, at the cost of allowing some limited extent of false positives when the model is queried with previously unseen n-grams. The aim is to achieve sufficiently compact representation that even very large language models can be stored totally within memory, avoiding the latencies of disk access. These Bloom filter based models exploit the idea that it is not actually necessary to store the n-grams of the model, as long as, when queried with an n-gram, the model retur"
E89-1002,P87-1012,0,0.0836986,"ivalent, the exponential figure signifies computational intractability. The next section gives a general outline of reduction and normal forms. This is followed by an illustration in relation to typed combinatory logic, where we emphasise that the reduction constitutes a proof-reduction. We then describe how the notions can be applied to combinatory grammar to handle the problem of parsing and derivational equivalence, and we again note that if derivations are regarded as proofs, the method is an instantiation of proof-reduction. Several suggestions have been made in relation to this problem. Pareschi and Steedman (1987) describe what they call a &apos;lazy chart parser&apos; intended to yield only one of each set of equivalent analyses by adopting a reduce-first parsing strategy, and invoking a special recovery procedure to avoid the backtracking that this strategy would otherwise necessitate. But Hepple (1987) shows that their algorithm is incomplete. Reduction and Normal Form Wittenburg (1987) presents an approach in which a combinatory grammar is compiled into one not exhibiting derivational equivalence. Such compilation seeks to avoid the problem of parsing with a grammar exhibiting derivational equivalence by arr"
E89-1002,P87-1011,0,0.336982,"parsing and derivational equivalence, and we again note that if derivations are regarded as proofs, the method is an instantiation of proof-reduction. Several suggestions have been made in relation to this problem. Pareschi and Steedman (1987) describe what they call a &apos;lazy chart parser&apos; intended to yield only one of each set of equivalent analyses by adopting a reduce-first parsing strategy, and invoking a special recovery procedure to avoid the backtracking that this strategy would otherwise necessitate. But Hepple (1987) shows that their algorithm is incomplete. Reduction and Normal Form Wittenburg (1987) presents an approach in which a combinatory grammar is compiled into one not exhibiting derivational equivalence. Such compilation seeks to avoid the problem of parsing with a grammar exhibiting derivational equivalence by arranging that the grammar used on-line does not have this property. The concern here however is management of parsing when the grammar used on-line does have the problematic property. It is a common state of affairs for some terms of a language to be equivalent in that for the intended semantics, their interpretations are the same in all models. In such a circumstance it c"
E91-1035,C90-2030,1,0.544259,"Missing"
E91-1035,E89-1002,1,\N,Missing
E91-1035,P89-1033,0,\N,Missing
E95-1018,E91-1035,1,0.779178,"Missing"
E95-1018,C92-1024,1,0.819064,"ll known that parsing (theorem proving) with sequent formalisms suffers efficiency problems as a consequence of derivational equivalence (or 'spurious ambiguity'), i.e. from the existence of multiple proofs that assign the same meaning for a given type combination. Alternative but equivalent formalisations of the above system are possible. Hepple (1993), for example, provides a natural deduction formalisation. Such a formalisation should readily provide the basis for a chart based approach to parsing hybrid logic grammars, after the manner of existing chart methods for use with L (KSnig 1990; Hepple 1992). A further promising possibility for efficient parsing of hybrid system grammars involves proof net reformulation, following a general scheme for such reformulation described in Moortgat (1992). However, the precise character of either chart or proof net based methods for parsing hybrid system grammars is a topic requiring further research. 9 Concluding Remarks As noted earlier, the approach described here has strong similarities to one developed independently by Moortgat & Oehrle (1993), although they take a precisely opposing view as to what constitute the appropriate directions of linkage"
E95-1018,C94-2201,1,0.737815,"ms that are in m a n y ways similar, but, interestingly, take precisely opposing views as to what are the 'natural relations' between levels. This difference of opinion has consequences for how the systems m a y be used as linguistic formalisms (requiring, for ex2Some early examples of muitimodal systems are logics that have coexistence, but without interlinkage, of associative and non-associative Lambek calculus (Oehrle & Zhang 1989; Morrill 1990). Further examples include systems that combine associative Lambek calculus with special connectives for discontinuity (e.g. Morrill & Solias 1993; Hepple 1994). 127 ample, 'additional apparatus' for handling word order in the second approach), and more crucially for the kind of linguistic accounts they allow to be formulated. In this paper, I will describe the approach taken in Hepple (1993) - - what I call the 'hybrid' approach, discuss the general linguistic model that it tends to foster and provide some linguistic illustration, and discuss possibilities for parsing hybrid systems. I will begin with discussion of substructural hierarchy and structural modalities, as it is the behaviour of systems with structural modalities that inspires the hybrid"
E95-1018,C90-2041,0,0.306156,"r s It is well known that parsing (theorem proving) with sequent formalisms suffers efficiency problems as a consequence of derivational equivalence (or 'spurious ambiguity'), i.e. from the existence of multiple proofs that assign the same meaning for a given type combination. Alternative but equivalent formalisations of the above system are possible. Hepple (1993), for example, provides a natural deduction formalisation. Such a formalisation should readily provide the basis for a chart based approach to parsing hybrid logic grammars, after the manner of existing chart methods for use with L (KSnig 1990; Hepple 1992). A further promising possibility for efficient parsing of hybrid system grammars involves proof net reformulation, following a general scheme for such reformulation described in Moortgat (1992). However, the precise character of either chart or proof net based methods for parsing hybrid system grammars is a topic requiring further research. 9 Concluding Remarks As noted earlier, the approach described here has strong similarities to one developed independently by Moortgat & Oehrle (1993), although they take a precisely opposing view as to what constitute the appropriate directio"
E95-1018,E93-1034,0,0.0122976,"es propose formal systems that are in m a n y ways similar, but, interestingly, take precisely opposing views as to what are the 'natural relations' between levels. This difference of opinion has consequences for how the systems m a y be used as linguistic formalisms (requiring, for ex2Some early examples of muitimodal systems are logics that have coexistence, but without interlinkage, of associative and non-associative Lambek calculus (Oehrle & Zhang 1989; Morrill 1990). Further examples include systems that combine associative Lambek calculus with special connectives for discontinuity (e.g. Morrill & Solias 1993; Hepple 1994). 127 ample, 'additional apparatus' for handling word order in the second approach), and more crucially for the kind of linguistic accounts they allow to be formulated. In this paper, I will describe the approach taken in Hepple (1993) - - what I call the 'hybrid' approach, discuss the general linguistic model that it tends to foster and provide some linguistic illustration, and discuss possibilities for parsing hybrid systems. I will begin with discussion of substructural hierarchy and structural modalities, as it is the behaviour of systems with structural modalities that inspi"
guthrie-etal-2010-efficient,P07-1065,0,\N,Missing
guthrie-etal-2010-efficient,D07-1090,0,\N,Missing
guthrie-etal-2010-efficient,D07-1049,0,\N,Missing
guthrie-etal-2010-efficient,W07-0712,0,\N,Missing
guthrie-etal-2010-efficient,P08-1058,0,\N,Missing
guthrie-etal-2010-efficient,W09-1505,0,\N,Missing
harkema-etal-2004-large,A00-1026,0,\N,Missing
harkema-etal-2004-large,W02-0312,0,\N,Missing
hepple-etal-2004-nlp,W02-2004,0,\N,Missing
jabbari-etal-2010-evaluating,W02-0816,0,\N,Missing
jabbari-etal-2010-evaluating,S07-1009,0,\N,Missing
L16-1494,W15-4631,0,0.150136,"ary of reader comments. Furthermore, the evaluations proposed so far, despite in several cases being called user studies, are not task-based evaluations that might let us understand how well systems are meeting user needs. A different, but promising, line of work, not yet deployed in summarization systems, is that on argument mining. Much of reader comment is argumentative and one appealing type of summary is one that would summarise the main points of contention in comments, something it is not clear an extractive summary could do. Work by e.g. Ghosh et al., (2014) Habernal et al. (2014) and Swanson et al. (2015) amongst others, focuses on defining and identifying key argumentative units and their relations. They mention summarization of argumentative texts as one potential application of their work. However, they do not specify what an end-user summary of reader comment on news might be like. In this paper we make three contributions to advancing work in this area. First, we offer a specification of one possible summary type for reader comment based on the notions of viewpoint and issue, which we define below (Section 2.). Second, we propose a task-based evaluation framework in which users are offere"
L16-1494,W14-2106,0,\N,Missing
L16-1694,P13-1035,0,0.0448237,"Missing"
L16-1694,J90-1003,0,0.389551,"tion scores for each word pair in separate time intervals by splitting our dataset based on the timestamp of the texts. PMI is an information theoretic measure that indicates which words tend to often co-occur in a context. It measures the relative difference between observed word cooccurrences, and their expected co-occurrence assuming independence, PMI(X, Y ) = α · log P (x, y) , P (x) · P (y) (1) where α is a normalisation factor, here set to α = − log P (x, y) following (Bouma, 2009) to address issues with interpretability and sensitivity to low-count events in regular PMI (where α = 1). (Church and Hanks, 1990). This normalised variant of PMI (NPMI) is bounded in the [−1, 1] interval and can be easily interpreted: word pairs with a negative NPMI co-occur less often than expected under independence, a positive NPMI means more often, and 0 denotes equality. The maximum value NPMI=1 implies that both words exclusively appear together. We use Word1 arrests publish bestfriends g-slate activist china’s blake magazines activist actors cameras angeles Word2 yemen trailers forming spotted arrests stealth griffin merchandise yemen showcase g-slate los NPMI 0.699 0.678 0.678 0.675 0.674 0.674 0.672 0.669 0.667"
L16-1694,J93-1003,0,0.449795,"automatically select relevant messages that can be used as labels when presenting the clusters to end users. Our results on event detection tasks using tweets, show that our method rivals state-of-the-art message based event detection techniques. Our method is especially useful for downstream applications where higher recall is desirable, such as time dependent information retrieval. The data from this study is freely available.1 2. Related Work The study of word co-occurrences has a long tradition in Natural Language Processing. Measures of co-occurrence have been studied by Fano (1961) and Dunning (1993). In NLP, they have been used for finding collocations or multiword expressions in documents (Sag et al., 2002; Evert, 2005), for weighting vectors for measuring distributional semantic similarity (Turney and Pantel, 2010) or for finding the sentiment polarity of words (Turney, 2002). More related to this study, Newman et al. (2010) shows that the best performance for measuring topic coherence is obtained using the Pointwise Mutual Information co-occurrence metric. Spectral clustering is a state-of-the-art clustering method that has been used for various tasks like image segmentation (Shi and"
L16-1694,D08-1038,0,0.042352,"e is obtained using the Pointwise Mutual Information co-occurrence metric. Spectral clustering is a state-of-the-art clustering method that has been used for various tasks like image segmentation (Shi and Malik, 2000) or detecting communities in networks (Newman, 2006). The application of spectral clustering methods in NLP has been limited because of increased storage space and runtime when faced with large-scale text datasets (Lin and Cohen, 2010). 1 http://www.sas.upenn.edu/˜danielpr/ clusters.html 4380 Cluster analysis and modelling events over time have been studied in different contexts. Hall et al. (2008) studies the evolution and trends of topics by using topic modeling and matching the topics obtained independently at different time intervals. Wang and McCallum (2006) develop a topic model that explicitly embeds time as an observed variable. Several other approaches have integrated time into a probabilistic graphical model of text (Al Sumait et al., 2008; Gohr et al., 2009; Wang et al., 2008). These papers used datasets of long and well structured documents on a restricted set of topics (e.g. conference proceedings or political addresses). In social media, event detection represents an extre"
L16-1694,N13-1090,0,0.0142806,"Missing"
L16-1694,N10-1012,0,0.0133696,"is desirable, such as time dependent information retrieval. The data from this study is freely available.1 2. Related Work The study of word co-occurrences has a long tradition in Natural Language Processing. Measures of co-occurrence have been studied by Fano (1961) and Dunning (1993). In NLP, they have been used for finding collocations or multiword expressions in documents (Sag et al., 2002; Evert, 2005), for weighting vectors for measuring distributional semantic similarity (Turney and Pantel, 2010) or for finding the sentiment polarity of words (Turney, 2002). More related to this study, Newman et al. (2010) shows that the best performance for measuring topic coherence is obtained using the Pointwise Mutual Information co-occurrence metric. Spectral clustering is a state-of-the-art clustering method that has been used for various tasks like image segmentation (Shi and Malik, 2000) or detecting communities in networks (Newman, 2006). The application of spectral clustering methods in NLP has been limited because of increased storage space and runtime when faced with large-scale text datasets (Lin and Cohen, 2010). 1 http://www.sas.upenn.edu/˜danielpr/ clusters.html 4380 Cluster analysis and modelli"
L16-1694,D14-1162,0,0.0746894,"Missing"
L16-1694,N10-1021,0,0.0556854,"Missing"
L16-1694,N12-1034,0,0.0354,"Missing"
L16-1694,P02-1053,0,0.0243471,"that these groups identify key real world events as they occur in time, despite no explicit supervision. The performance of our method rivals state-of-the-art methods for event detection on F-score, obtaining higher recall at the expense of precision. Keywords: Topic Detection & Tracking, Information Extraction, Information Retrieval, Text Mining 1. Introduction Algorithms based on word co-occurrences have a long tradition in NLP and have been used successfully for applications ranging from sentiment analysis to thesaurus learning, collocation extraction and discovering multiword expressions (Turney, 2002; Curran, 2004; Sag et al., 2002; Evert, 2005). However, in most cases, these scores have been computed using underlying static corpora and ignoring any temporal variation. In this paper we study the changing behaviour of word co-occurrences over time using social media data. We hypothesise that co-occurrences between words will change over time as a response to real world events. Increased social media usage enables us to extract for analysis large scale streaming data, previously largely unavailable to researchers. Data arising from these sources, specifically Twitter, has been shown to refl"
N10-1037,jabbari-etal-2010-evaluating,1,0.848855,"t to their ultimate practical utility. The view of a ‘good’ answer set described above suggests a comparison of Ai to Hi using versions of ‘recall’ and ‘precision’ metrics, that incorporate the ‘weighting’ of human answers via freqi . Let us begin by noting the obvious definitions for recall and 3 We do not consider here a related task which assesses whether the mode answer mi is found within an answer set of up to 10 guesses. We do not favour the use of this metric for reasons parallel to those discussed for the mode metric of the previous section, i.e. brittleness and information loss. 4 In Jabbari et al. (2010), we define a metric that directly addresses the ability of systems to achieve good ranking of substitution candidates. This is not itself a measure of lexical substitution task performance, but addresses a component ability that is key to the achievement of lexical substitution tasks. 291 precision metrics without count-weighting: R(i) = |Hi ∩ Ai | |Hi | P (i) = |Hi ∩ Ai | |Ai | Our definitions of these metrics, given below, do include count-weighting, and require some explanation. The numerator of our recall definition is |Ai |i not |Hi ∩ Ai |i as |Ai |i = |Hi ∩ Ai |i (as f reqi assigns 0 to"
N10-1037,S07-1044,0,0.0211417,"actor k = 1). compare the indication it provides of relative system performance to that of the oot metric. We consider three systems described in Jabbari (2010), developed as part of an investigation into the means and benefits of combining models of lexical context: (i) bow: a system using a bag-of-words model to rank candidates, (ii) lm: using a (simple) n-gram language model, and (iii) cmlc: using a model that combines bow and lm models into one. We also consider the system KU, which uses a very large language model and an advanced treatment of smoothing, and which performed well at ELS07 (Yuret, 2007).5 Table 1 shows the oot scores for these systems, including a breakdown by part-of-speech, which indicate a performance ranking: bow < lm < cmlc < KU Our first problem is that these systems are developed for the oot task, not coverage, so after rank5 We thank Deniz Yuret for allowing us to use his system’s outputs in this analysis. 292 ing their candidates, they do not attempt to draw a boundary between the candidates worth returning and those not. Instead, we here use the oot outputs to compute an optimal performance for each system, i.e. we find, for the ranked candidates of each question,"
N10-1037,S07-1009,0,\N,Missing
N18-4008,P15-1119,0,0.0780832,"Missing"
N18-4008,U11-1016,0,0.0404524,"been applied by Simard (1998) to well resourced languages (French and Spanish) which generally involved pre-processing, candidate generation and disambiguation. Hybrid techniques are common with this task e.g. Yarowsky (1999) used decision list, Bayesian classification and Viterbi decoding while Crandall (2005) applied Bayesianand HMM-based methods. Tufis¸ and Chit¸u (1999) used a hybrid approach that backs off to character-based method when dealing with “unknown words”. Electronic dictionaries, where available, often ˇ augment the substitution schemes used (Santi´ c et al., 2009). On Maori, Cocks and Keegan (2011) used na¨ıve Bayes algorithms with word n-grams to improve on the character based approach by Scannell (2011). For Igbo, however, one major challenge to applying most of the techniques mentioned above that depend on annotated datasets is the lack of these datasets for Igbo e.g tagged corpora, morphologically segmented corpora or dictionaries. This work aims at using a resource-light approach that is based on a more generalisable state-of-theart representation model like word-embeddings Table 2: Disambiguation challenge for Google Translate 2 A wordkey is a “latinized” form of a word i.e. a wor"
N18-4008,wagacha-etal-2006-grapheme,0,0.119633,"Missing"
P00-1036,W96-0102,0,\N,Missing
P00-1036,W99-0705,0,\N,Missing
P00-1036,W94-0111,0,\N,Missing
P00-1036,W96-0213,0,\N,Missing
P00-1036,A92-1018,0,\N,Missing
P00-1036,A88-1019,0,\N,Missing
P00-1036,J95-4004,0,\N,Missing
P00-1036,P98-1081,0,\N,Missing
P00-1036,C98-1078,0,\N,Missing
P00-1036,J95-2004,0,\N,Missing
P91-1011,E91-1035,1,0.603855,"Missing"
P91-1011,C90-2030,1,0.888446,"unctions from y into x, a n d a d o p t a convention of left association, so t h a t , e.g. ( ( s  n p ) / p p ) / n p may b e written s  n p / p p / n p . 2See Lambek (1958) a n d M o o r t g a t (1989) for a sequent formulation of the LC. See Morrill, Leslie, Hepple & Barry (1990), a n d Barry, Hepple, Leslie & Morrill (1991) for a natural deduction formulation. Zielonka (1981) provides a LC formulation in terms of (recursively defined) reduction schema. Various extensions of the LC are currently u n d e r investigation, a l t h o u g h we shall not have space to discuss t h e m here. See Hepple (1990), Morrill (1990) a n d M o o r t g a t (1990b). 79 type combinations - - the other calculi which we consider admit only a subset of the Lambek type combinations, s T h e flexibility of the LC is such that, for any combination x l , . . , x , ==~ x0, a fully left-branching derivation is always possible (i.e. combining xl and x2, then combining the result with x3, and so on). However, the properties of the LC make it useless for practical incremental processing. Under the LC, there is always an infinite number of result types for any combination, and we can only in practice address the possibili"
P91-1011,E89-1002,1,0.835545,"be the case with unrestricted type-raising). Some problems for efficient processing of CCGs arise from what has been termed 'spurious ambiguity' or 'derivational equivalence', i.e. the existence of multiple distinct proofs which assign the same reading for some combination of types. For example, the proofs (6a,b) assign the same reading for the combination. Since search for proofs must be exhaustive to ensure that all distinct readings for a combination are found, effort will be wasted constructing proofs which a . . . . ~ ~he same meaning, considerably reducing the elficiency of processing. Hepple & Morrill (1989) suggest a solution to this problem that involves specifying a notion of normal form (NF) for CCG proofs, and ensuring that the parser returns only NF proofs. 4 However, their method has a number of limitations. (i) They considered a 'toy g r a m m a r ' involving only the CCG rules stated above. For a grammar involving further combination rules, normalisation would need to be completely reworked, and it remains to be shown that this task can be successfully done. (ii) Combinatory Categorial Grammars (CCGs - Steedman, 1987; Szabolcsi, 1987) are formulated by adding a number of type combination"
P91-1011,P89-1033,0,0.411254,"Missing"
P97-1044,P96-1011,0,0.0132733,"W Wo-Z Z Au.yu Av.wv z Yo--Z : )~v.y(wv) [1,1] Y: y(wz) x : z( az y( wz ) ) [1,0] [1,0] The solution to this problem involves specifying a normal form for deductions, and allowing that only normal form proofs are constructed) Our route to specifying a normal form for proofs exploits a correspondence between proofs and dependency structures. Dependency grammar (DG) takes as fundamental ~This approach of 'normal form parsing' has been applied to the associative Lambek calculus in (K6nig, 1989), (Hepple, 1990), (Hendriks, 1992), and to Combinatory Categorial Grammar in (Hepple & Morrill, 1989), (Eisner, 1996). 347 the notions of head and dependent. An analogy is often drawn between CG and DG based on equating categorial functors with heads, whereby the arguments sought by a functor are seen as its dependents. The two approaches have some obvious differences. Firstly, the argument requirements of a categorial functor are ordered. Secondly, arguments in CG are phrasal, whereas in DG dependencies are between words. However, to identify the dependency relations entailed by a proof, we may simply ignore argument ordering, and we can trace through the proof to identify those initial assumptions ('words'"
P97-1044,C90-2030,1,0.797823,"the proof (9), we have also the equivalent proof (10). (10) (i) (ii) Xo--Y At.x(Az.t) (iii) (iv) Yo-W Wo-Z Z Au.yu Av.wv z Yo--Z : )~v.y(wv) [1,1] Y: y(wz) x : z( az y( wz ) ) [1,0] [1,0] The solution to this problem involves specifying a normal form for deductions, and allowing that only normal form proofs are constructed) Our route to specifying a normal form for proofs exploits a correspondence between proofs and dependency structures. Dependency grammar (DG) takes as fundamental ~This approach of 'normal form parsing' has been applied to the associative Lambek calculus in (K6nig, 1989), (Hepple, 1990), (Hendriks, 1992), and to Combinatory Categorial Grammar in (Hepple & Morrill, 1989), (Eisner, 1996). 347 the notions of head and dependent. An analogy is often drawn between CG and DG based on equating categorial functors with heads, whereby the arguments sought by a functor are seen as its dependents. The two approaches have some obvious differences. Firstly, the argument requirements of a categorial functor are ordered. Secondly, arguments in CG are phrasal, whereas in DG dependencies are between words. However, to identify the dependency relations entailed by a proof, we may simply ignore"
P97-1044,C92-1024,1,0.813277,"duction method which, like chart parsing for phrase-structure grammar, avoids the need to recompute intermediate results when searching exhaustively for all possible analyses, i.e. where any combination of types contributes to more than one overall analysis, it need only be computed once. The incremental system to be developed in this paper is similarly compatible with a 'chart-like' processing approach, although this issue will not be further addressed within this paper. For earlier work on chart-parsing type-logical formalisms, specifically the associative Lambek calculus, see KSnig (1990), Hepple (1992), K5nig (1994). 345 identifier for that assumption. The index sets of a derived formula identify precisely those assumptions from which it is derived. The rule (4) ensures appropriate indexation, i.e. via the condition rr = ¢~¢, where t~ stands for disjoint union (ensuring linear usage). The common origin of assumptions (i) and (iv) (i.e. from Xo--(Yo-Z)) is recorded by the fact that (i)'s argument is marked with (iv)'s index (j). The condition a C ~b of (4) ensures that (iv) must contribute to the derivation of (i)'s argument (which is needed to ensure correct inferencing). Finally, observe t"
P97-1044,E95-1018,1,0.854573,"duction implemented via a version of SLD resolution. Hepple (1996) introduces a linear deduction method, involving compilation to first order formulae, which can be combined with various labelling disciplines. These approaches, however, are not directed toward incremental processing. In what follows, we show how the method of (Hepple, 1996) can be modified to allow processing which has a high degree of incrementality. These modifications, however, give a system which suffers 2See, for example, the formalisms developed in (Moortgat & Morrill, 1991), (Moortgat & Oehrle, 1994), (Morrill, 1994), (Hepple, 1995). the problem of 'derivational equivalence', also called 'spurious ambiguity', i.e. allowing multiple proofs which assign the same reading for some combination, a fact which threatens processing efficiency. We show how this problem is solved via normalisation. 2 Implicational Linear Logic Linear logic is an example of a &quot;resource-sensitive&quot; logic, requiring that each assumption ('resource') is used precisely once in any deduction. For the implicational fragment, the set of formulae ~ are defined by 5r ::= A [ ~'o-~- (with A a nonempty set of atomic types). A natural deduction formulation requi"
P97-1044,C96-1091,1,0.287036,"Recent work has seen the emergence of a common framework for parsing categorial grammar (CG) formalisms that fall within the 'type-logical' tradition (such as the Lambek calculus and related systems), whereby some method of linear logic theorem proving is used in combination with a system of labelling that ensures only deductions appropriate to the relevant grammatical logic are allowed. The approaches realising this framework, however, have not so far addressed the task of incremental parsing - - a key issue in earlier work with 'flexible' categorial grammars. In this paper, the approach of (Hepple, 1996) is modified to yield a linear deduction system that does allow flexible deduction and hence incremental processing, but that hence also suffers the problem of 'spurious ambiguity'. This problem is avoided via normalisation. 1 Introduction A key attraction of the class of formalisms known as 'flexible' categorial grammars is their compatibility with an incremental style of processing, in allowing sentences to be assigned analyses that are fully or primarily left-branching. Such analyses designate many initial substrings of a sentence as interpretable constituents, allowing its interpretation t"
P97-1044,E89-1002,1,0.799883,"At.x(Az.t) (iii) (iv) Yo-W Wo-Z Z Au.yu Av.wv z Yo--Z : )~v.y(wv) [1,1] Y: y(wz) x : z( az y( wz ) ) [1,0] [1,0] The solution to this problem involves specifying a normal form for deductions, and allowing that only normal form proofs are constructed) Our route to specifying a normal form for proofs exploits a correspondence between proofs and dependency structures. Dependency grammar (DG) takes as fundamental ~This approach of 'normal form parsing' has been applied to the associative Lambek calculus in (K6nig, 1989), (Hepple, 1990), (Hendriks, 1992), and to Combinatory Categorial Grammar in (Hepple & Morrill, 1989), (Eisner, 1996). 347 the notions of head and dependent. An analogy is often drawn between CG and DG based on equating categorial functors with heads, whereby the arguments sought by a functor are seen as its dependents. The two approaches have some obvious differences. Firstly, the argument requirements of a categorial functor are ordered. Secondly, arguments in CG are phrasal, whereas in DG dependencies are between words. However, to identify the dependency relations entailed by a proof, we may simply ignore argument ordering, and we can trace through the proof to identify those initial assu"
P97-1044,P89-1033,0,0.0726916,"Missing"
P97-1044,C90-2041,0,0.0184653,"ate a deduction method which, like chart parsing for phrase-structure grammar, avoids the need to recompute intermediate results when searching exhaustively for all possible analyses, i.e. where any combination of types contributes to more than one overall analysis, it need only be computed once. The incremental system to be developed in this paper is similarly compatible with a 'chart-like' processing approach, although this issue will not be further addressed within this paper. For earlier work on chart-parsing type-logical formalisms, specifically the associative Lambek calculus, see KSnig (1990), Hepple (1992), K5nig (1994). 345 identifier for that assumption. The index sets of a derived formula identify precisely those assumptions from which it is derived. The rule (4) ensures appropriate indexation, i.e. via the condition rr = ¢~¢, where t~ stands for disjoint union (ensuring linear usage). The common origin of assumptions (i) and (iv) (i.e. from Xo--(Yo-Z)) is recorded by the fact that (i)'s argument is marked with (iv)'s index (j). The condition a C ~b of (4) ensures that (iv) must contribute to the derivation of (i)'s argument (which is needed to ensure correct inferencing). Fin"
P97-1044,E95-1017,0,0.0233736,"e n t a l i t y in relation to allowing 'sem a n t i c a l l y contentful' combinations. In d e p e n d e n c y terms, t h e s y s t e m allows any set of initial formulae to combine to a single result iff t h e y form a connected g r a p h u n d e r t h e d e p e n d e n c y relations t h a t o b t a i n a m o n g s t them. Note t h a t t h e e x t e n t of i n c r e m e n t a l i t y allowed by using 'generalised c o m p o s i t i o n ' in the compiled firstorder s y s t e m should not be e q u a t e d with t h a t which 7For an example of a system allowing word-by-word incrementality, see (Milward, 1995). SNote that this is not to say that the system is unable to combine these two types, e.g. a combination so--s, np =~ s o - ( s o - n p ) is derivable, with appropriate compilation. The point rather is that such a combination will typically not happen as a component in a proof of some other overall deduction. would be allowed by such a rule in the original (noncompiled) system. We can illustrate this point using the following type combination, which is not an instance of even 'generalised' composition. Xo-(Yo-Z), Yo--W =~ X o - ( W o - Z ) Compilation of the higher-order assumption would yield"
P97-1044,E95-1019,0,0.0132578,"labelled deduction idea. Approaches within this framework employ a theorem proving method that is appropriate for use with linear logic, and combine it with a labelling system that restricts admitted deductions to be those of a weaker system. Crucially, linear logic stands above all of the type-logical formalisms proposed in the hierarchy of substructural logics, and hence linear logic deduction methods can provide a common basis for parsing all of these systems. For example, Moortgat (1992) combines a linear proof net method with labelling to provide deduction for several categorial systems. Morrill (1995) shows how types of the associative Lambek calculus may be translated to labelled implicational linear types, with deduction implemented via a version of SLD resolution. Hepple (1996) introduces a linear deduction method, involving compilation to first order formulae, which can be combined with various labelling disciplines. These approaches, however, are not directed toward incremental processing. In what follows, we show how the method of (Hepple, 1996) can be modified to allow processing which has a high degree of incrementality. These modifications, however, give a system which suffers 2Se"
P97-1044,P87-1011,0,0.0777105,"Missing"
P98-1088,E93-1013,0,0.0728613,"Missing"
P98-1088,P97-1019,0,0.0295761,"Missing"
P98-1088,C92-1024,1,0.807517,"from which they are derived, with appropriate indexation being ensured by the condition 7r = ¢ ~ ¢ of the rule (where t2 stands for disjoint union, which enforces linear usage). ¢:Ao--(B:a):)~v.a rr: A : a[b//v] ¢:B:b 7r = ¢ ~ ¢ 2The key division here is between higher-order formulae, which are are functors t h a t seek at least one argument that bears a a functional type (e.g. Wo--(Xo--Z)), and first-order formulae, which seek no such argument. 3This 'excision' step has parallels to the 'emit' step used in the chart-parsing approaches for the associative Lambek calculus of (KSnig, 1994) and (Hepple, 1992), although the latters differs in that there is no removal of the relevant subformula, i.e. the 'emitting formula' is not simplified, remaining higher-order. Assumptions (1) and (4) both come from Xo-(Yo--Z): note how (1)'s argument is marked with (4)'s index (j). The condition c~ C ¢ of the rule ensures that (4) must contribute to the derivation of (1)'s argument. Finally, observe that the rule's semantics involves not simple application, but rather by direct substitution for the variable of a lambda expression, employing a special variant of substitution, notated _[_//_], which specifically"
P98-1088,E95-1018,1,0.857672,"e proof terms for successful analyses. 7 Application ~1: Categorial Parsing The associative Lambek calculus (Lambek, 1958) is perhaps the most familiar representative of the class of categorial formalisms that fall within the 'type-logical' tradition. Recent work has seen proposals for a range of such systems, differing in their resource sensitivity (and hence, implicitly, their underlying notion of 'linguistic structure'), in some cases combining differing resource sensitivities in one system, s Many of SSee, for example, the formalisms developed in (Moortgat et al., 1994), (Morrill, 1994), (Hepple, 1995). 543 these proposals employ a 'labelled deductive system' methodology (Gabbay, 1996), whereby types in proofs are associated with labels which record proof information for use in ensuring correct inferencing. A natural 'base logic' on which to construct such systems is the multiplicative fragment of linear logic, since (i) it stands above the various categorial systems in the hierarchy of substructural logics, and (ii) its operators correspond to precisely those appearing in any standard categorial logic. The key requirement for parsing categorial systems formulated in this way is some theore"
P98-1088,C96-1091,1,0.684369,"rmulating various categorial grammar systems. Linear deduction methods provide a common basis for parsing categorial systems formulated in this way. Secondly, the multiplicative fragment forms the core of the system used in work by Dalrymple and colleagues for handling the semantics of LFG derivations, providing a 'glue language' for assembling the meanings of sentences from those of words and phrases. Although there are a number of deduction methods for multiplicative linear logic, there is a notable absence of tabular methods, which, like chart parsing for CFGs, avoid redundant computation. Hepple (1996) presents a compilation method which allows for tabular deduction for 538 implicational linear logic (i.e. the fragment with only o--). This paper develops that method to cover the fragment that includes the multiplicative. The use of this method for the applications mentioned above is discussed. 2 Multiplicative Linear Logic Linear logic is a 'resource-sensitive' logic: in any deduction, each assumption ('resource') is used precisely once• The formulae of the multiplicative fragment of (intuitionistic) linear logic are defined by ~"" ::= A I ~'o-~"" J 9v ® ~ (A a nonempty set of atomic types)."
P98-1088,E95-1019,0,0.016153,"systems is the multiplicative fragment of linear logic, since (i) it stands above the various categorial systems in the hierarchy of substructural logics, and (ii) its operators correspond to precisely those appearing in any standard categorial logic. The key requirement for parsing categorial systems formulated in this way is some theorem proving m e t h o d that is sufficient for the fragment of linear logic employed (although some additional work will be required for managing labels), and a number of different approaches have been used, e.g. proof nets (Moortgat, 1992), and SLD resolution (Morrill, 1995). Hepple (1996) introduces first-order compilation for implicational linear logic, and shows how that method can be used with labelling as a basis parsing implicational categorial systems. No further complications arise for combining the extended compilation approach described in this paper with labelling systems as a basis for efficient, non-redundant parsing of categorial formalisms in the core multiplicative fragment. See (Hepple, 1996) for a worked example. 8 Application Deduction ~2: Glue Language In a line of research beginning with Dalrymple et al. (1993), a fragment of linear logic is"
P98-1115,P97-1021,0,0.136357,"Missing"
P98-1115,P96-1025,0,0.0607813,"Missing"
P98-1115,P98-1115,1,0.0512067,"Missing"
P98-1115,H94-1020,0,0.0495892,"ay be able to achieve parsing performance similar to the best results in the field obtained in (Collins, 1996). 2 Rule Growth and Partial Bracketting Growth of the Rule Set One could investigate whether there is a finite grammar that should account for any text within a class of related texts (i.e. a domain oriented sub-grammar of English). If there is, the number of extracted rules will approach a limit as more sentences are processed, i.e. as the rule number approaches the size of such an underlying and finite grammar. We had hoped that some approach to a limit would be seen using P T B II (Marcus et al., 1994), which larger and more consistent for bracketting than P T B I. As shown in Figure 1, however, the rule number growth continues unabated even after more than 1 million part-ofspeech tokens have been processed. 700 Why should the set of rules continue to grow in this way? P u t t i n g aside the possibility that natural languages do not have finite rule sets, we can think of two possible answers. First, it may be that the full &quot;underlying grammar&quot; is much larger than the rule set that has so far been produced, requiring a much larger tree-banked corpus than is now available for its extraction."
P98-1115,1995.iwpt-1.26,0,0.255817,"Missing"
P98-1115,J95-2002,0,0.140312,"Missing"
P99-1060,C92-1024,1,0.787932,"ingle ordering over lexical categories by which standard charts are organised. 1 The previous chart methods for the Lambek calculus deal with this problem in different ways. The method of K6nig (1990, 1994) places hypotheticals on separate 'minicharts' which can attach into other (mini)charts where combinations are 1In effect, hypotheticals belong on additional suborderings, which can connect into the main ordering of the chart at various positions, generating a branching, multi-dimensional ordering scheme. possible. The m e t h o d requires rather complicated book-keeping. The m e t h o d of Hepple (1992) avoids this complicated book-keeping, and also rules out some useless subderivations allowed by Khnig's method, but does so at the cost of computing a representation of all the possible category sequences that might be tested in an exhaustive sequent proof search. Neither of these methods exhibits performance that would be satisfactory for practical use. 2 3 Some Preliminaries First-order Compilation for Categorial Parsing Hepple (1996) introduces a m e t h o d of firstorder compilation for implicational linear logic, to provide a basis for efficient theorem proving of various categorial form"
P99-1060,C96-1091,1,0.892872,"ties between categorial grammars and the D-Tree grammar (DTG) formalism of Rambow et al. (1995a). On this basis, we have explored adapting the DTG parsing approach of Rambow et al. (1995b) for use with the Lambek calculus. The resulting method is one in which the formulae of a Lambek sequent that is to be proven are first converted to produce rules of a formalism which combines ideas from the multiset-valued linear indexed grammar formalism of Rainbow (1994), with the Lambek calculus span labelling scheme of Morrill (1995), and with the first-order compilation method for categorial parsing of Hepple (1996). The resulting 'grammar' is then parsed using an Earley-style predictive chart algorithm which is adapted from Rambow et al. (1995b). 2 The Lambek Calculus We are concerned with the implicational (or 'product-free') fragment of the associative Lambek calculus (Lambek, 1958). A natural deduction formulation is provided by the following rules of elimination and introduction, which correspond to steps of functional application and 465 A / B :a B :b A : (ab) /E • ....[B: v] A:a A / B : Av.a (which) rel/(s/np) /I (mary) np B:b BA a E A : (ab) [B: v].:. A:a B  A : Av.a (ate) (nps)/np nps I [n"
P99-1060,W98-0117,1,0.8093,"alism, which are used in an Earley-style predictive chart algorithm. The method is non-polynomial, but performs well for practical purposes - - much better than previous chart methods for Lambek grammars. abstraction, respectively (as the term labelling reveals). The rules are sensitive to the order of assumptions. In the [/I] (resp. [I]) rule, [B] indicates a discharged or withdrawn assumption, which is required to be the rightmost (resp. leftmost) of the proof. 1 Introduction We present a new chart parsing method for Lambek grammars. The starting point for this work is the observation, in (Hepple, 1998), of certain similarities between categorial grammars and the D-Tree grammar (DTG) formalism of Rambow et al. (1995a). On this basis, we have explored adapting the DTG parsing approach of Rambow et al. (1995b) for use with the Lambek calculus. The resulting method is one in which the formulae of a Lambek sequent that is to be proven are first converted to produce rules of a formalism which combines ideas from the multiset-valued linear indexed grammar formalism of Rainbow (1994), with the Lambek calculus span labelling scheme of Morrill (1995), and with the first-order compilation method for c"
P99-1060,C90-2041,0,0.0366688,"Missing"
P99-1060,E95-1019,0,0.0657025,"tarting point for this work is the observation, in (Hepple, 1998), of certain similarities between categorial grammars and the D-Tree grammar (DTG) formalism of Rambow et al. (1995a). On this basis, we have explored adapting the DTG parsing approach of Rambow et al. (1995b) for use with the Lambek calculus. The resulting method is one in which the formulae of a Lambek sequent that is to be proven are first converted to produce rules of a formalism which combines ideas from the multiset-valued linear indexed grammar formalism of Rainbow (1994), with the Lambek calculus span labelling scheme of Morrill (1995), and with the first-order compilation method for categorial parsing of Hepple (1996). The resulting 'grammar' is then parsed using an Earley-style predictive chart algorithm which is adapted from Rambow et al. (1995b). 2 The Lambek Calculus We are concerned with the implicational (or 'product-free') fragment of the associative Lambek calculus (Lambek, 1958). A natural deduction formulation is provided by the following rules of elimination and introduction, which correspond to steps of functional application and 465 A / B :a B :b A : (ab) /E • ....[B: v] A:a A / B : Av.a (which) rel/(s/np) /I"
P99-1060,P88-1033,0,0.044269,"d using a modified indexation scheme to record dependencies 3The constants produced in the translation correspond to 'new' string positions, which make up the additional suborderings on which hypotheticals are located. The variables produced in the translation become instantiated to some string constant during an analysis, fixing the position at which an additional subordering becomes 'attached to' another (sub)ordering. 4The idea of implementing categorial grammar as a non-directional logic, but associating atomic types with string position pairs (i.e. spans) to handle word order, is used in Pareschi (1988), although in that approach all string positions instantiate to values on a single ordering (i.e. integers 0 - n for a string of length n), which is not sufficient for Lambek calculus deductions. 467 between residue formulae and excised hypotheticals (one where both the residue and hypothetical record the dependency). For this procedure, the 'atomic type plus span label' units that result from the previous stage are treated as atomic units. The procedure T is defined by the cases shown in Figure 2 (although the method is perhaps best understood from the example also shown there). Its input is"
P99-1060,P95-1021,0,0.137054,"ms well for practical purposes - - much better than previous chart methods for Lambek grammars. abstraction, respectively (as the term labelling reveals). The rules are sensitive to the order of assumptions. In the [/I] (resp. [I]) rule, [B] indicates a discharged or withdrawn assumption, which is required to be the rightmost (resp. leftmost) of the proof. 1 Introduction We present a new chart parsing method for Lambek grammars. The starting point for this work is the observation, in (Hepple, 1998), of certain similarities between categorial grammars and the D-Tree grammar (DTG) formalism of Rambow et al. (1995a). On this basis, we have explored adapting the DTG parsing approach of Rambow et al. (1995b) for use with the Lambek calculus. The resulting method is one in which the formulae of a Lambek sequent that is to be proven are first converted to produce rules of a formalism which combines ideas from the multiset-valued linear indexed grammar formalism of Rainbow (1994), with the Lambek calculus span labelling scheme of Morrill (1995), and with the first-order compilation method for categorial parsing of Hepple (1996). The resulting 'grammar' is then parsed using an Earley-style predictive chart a"
P99-1060,1995.iwpt-1.30,0,0.136437,"ms well for practical purposes - - much better than previous chart methods for Lambek grammars. abstraction, respectively (as the term labelling reveals). The rules are sensitive to the order of assumptions. In the [/I] (resp. [I]) rule, [B] indicates a discharged or withdrawn assumption, which is required to be the rightmost (resp. leftmost) of the proof. 1 Introduction We present a new chart parsing method for Lambek grammars. The starting point for this work is the observation, in (Hepple, 1998), of certain similarities between categorial grammars and the D-Tree grammar (DTG) formalism of Rambow et al. (1995a). On this basis, we have explored adapting the DTG parsing approach of Rambow et al. (1995b) for use with the Lambek calculus. The resulting method is one in which the formulae of a Lambek sequent that is to be proven are first converted to produce rules of a formalism which combines ideas from the multiset-valued linear indexed grammar formalism of Rainbow (1994), with the Lambek calculus span labelling scheme of Morrill (1995), and with the first-order compilation method for categorial parsing of Hepple (1996). The resulting 'grammar' is then parsed using an Earley-style predictive chart a"
P99-1060,P94-1036,0,\N,Missing
roberts-etal-2008-combining,E99-1001,0,\N,Missing
roberts-etal-2008-combining,W03-1309,0,\N,Missing
roberts-etal-2008-combining,P05-3007,0,\N,Missing
roberts-etal-2008-combining,harkema-etal-2004-large,1,\N,Missing
roberts-etal-2008-combining,A00-1040,0,\N,Missing
S07-1014,P06-1095,1,0.245989,"ovsky et al., 2003a) is an emerging ISO standard for annotation of events, temporal expressions and the anchoring and ordering relations between them. TimeBank (Pustejovsky et al., 2003b; Boguraev et al., forthcoming) was originally conceived of as a proof of concept that illustrates the TimeML language, but has since gone through several rounds of revisions and can now be considered a gold standard for temporal information. TimeML and TimeBank have already been used as the basis for automatic time, event and temporal relation annotation tasks in a number of research projects in recent years (Mani et al., 2006; Boguraev et al., forthcoming). An open evaluation challenge in the area of temporal annotation should serve to drive research forward, as it has in other areas of NLP. The automatic identification of all temporal referring expressions, events and temporal relations within a text is the ultimate aim of research in this area. However, addressing this aim in a first evaluation challenge was judged to be too difficult, both for organizers and participants, and a staged approach was deemed more effective. Thus we here present an initial evaluation exercise based on three limited tasks that we bel"
S07-1098,P06-1095,0,0.144129,"‘lite’ approach of this kind perform? (2) Which features contribute positively to system performance? (3) Which ML algorithm is better suited for the TempEval tasks? We used the Weka ML workbench to facilitate experimenting with different ML algorithms. The paper describes our system and supplies preliminary answers to the above questions. 1 Introduction The Sheffield team were involved in TempEval as co-proposers/co-organisers of the task.1 For our participation in the task, we decided to pursue an MLbased approach, the benefits of which have been explored elsewhere (Boguraev and Ando, 2005; Mani et al., 2006). For the TempEval tasks, this is easily done by treating the assignment of temporal relation types as a simple classification task, using readily available information for the instance features. More specifically, the features used were ones provided as 1 We maintained a strict separation between persons assisting in annotation of the test corpus and those involved in system development. In what follows, we will first describe how our system was constructed, before going on to discuss our main observations around the key aims mentioned above. For example, in regard to our ‘lite’ approach, we"
S07-1098,S07-1014,1,0.902509,"for the task. (These were produced using WEKA’s rules.ZeroR algorithm, which does exactly that.) The best results observed for each task are shown in bold in the table. These best performing algorithms were used for the corresponding tasks in the competition. Observe that the lazy.KStar 2 These scores are computed under the ‘strict’ requirement that key and response labels should be identical. The TempEval competition also uses a ‘relaxed’ metric which gives partial credit when one (or both) label is disjunctive and there is a partial match, e.g. between labels AFTER and OVERLAP-ORAFTER. See (Verhagen et al., 2007) for details. USFD ave. max. Task A FS FR 0.59 0.60 0.56 0.59 0.62 0.64 Task B FS FR 0.73 0.74 0.74 0.75 0.80 0.81 Task C FS FR 0.54 0.59 0.51 0.60 0.55 0.66 Table 3: Competition task scores for Sheffield system (USFD), plus average/max scores across all competing systems method, which gives the best performance for Task A, gives a rather ‘middling’ performance for Task B. Similarly, the SVM method that gives the best results for Task C falls quite a way below the performance of KStar on Task A. A more extreme case is seen with the results for rules.JRip (Weka’s implementation of the RIPPER al"
W04-3110,O98-4002,1,0.749256,"base for storing terminological information and compiling finite state machines from this database to do term look-up. 1.2 Context Termino is being developed in the context of two ongoing projects: CLEF, for Clinical E-Science Framework (Rector et al., 2003) and myGrid (Goble et al., 2003). Both these projects involve an Information Extraction component. Information Extraction is the activity of identifying pre-defined classes of entities and relationships in natural language texts and storing this information in a structured format enabling rapid and effective access to the information, e.g. Gaizauskas and Wilks (1998), Grishman (1997). The goal of the CLEF project is to extract information from patient records regarding the treatment of cancer. The treatment of cancer patients may extend over several years and the resulting clinical record may include many documents, such as clinic letters, case notes, lab reports, discharge summaries, etc. These documents are generally full of medical terms naming entities such as body parts, drugs, problems (i.e. symptoms and diseases), investigations and interventions. Some of these terms are particular to the hospital from which the document originates. We aim to ident"
W04-3110,W02-0312,0,0.0259296,"Missing"
W04-3110,A00-1026,0,0.198023,"ized that the biomedical literature is now so large, and growing so quickly, that it is becoming increasingly difficult for researchers to access the published results that are relevant to their research. Consequently, any technology that can facilitate this access should help to increase research productivity. This has led to an increased interest in the application of natural language processing techniques for the automatic capture of biomedical content from journal abstracts, complete papers, and other textual documents (Gaizauskas et al., 2003; Hahn et al., 2002; Pustejovsky et al., 2002; Rindflesch et al., 2000). An essential processing step in these applications is the identification and semantic classification of technical terms in text, since these terms often point to entities about which information should be extracted. Proper semantic classification of terms also helps in resolving anaphora and extracting relations whose arguments are restricted semantically. 1.1 Challenge Any technical domain generates very large numbers of terms – single or multiword expressions that have some specialised use or meaning in that domain. For example, the UMLS Metathesaurus (Humphreys et al., 1998), which provid"
W05-1527,J93-2004,0,\N,Missing
W08-0602,P05-3007,0,0.019812,"spread: the application of statistical machine learning techniques to feature-based models of the text. Such approaches have typically been applied to journal texts. They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example, Grover et al (2007)). This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)). Statistical machine learning has also been applied to clinical text, but its use has generally been limited to entity recognition. The Mayo Clinic text analysis system (Pakhomov et al., 2005), for example, uses a combination of dictionary lookup and a Na¨ıve Bayes classifier to identify entities for information retrieval applications. To the best of our knowledge, statistical methods have not been previously applied to extraction of clinical relationships from text. This paper describes experiments in the statistical machine learning of relationships from a novel text type: oncology narratives. The set of relationships extracted are considered to be of interest for clinical and research applications down line of IE, such as querying to support clinical research. We apply Support V"
W08-0602,roberts-etal-2008-combining,1,0.805113,"ts. CLEF uses information extraction (IE) technology to make information from the textual portion of the medical record available for integration with the structured record, and thus available for clinical care and research. The CLEF IE system analyses the textual records to extract entities, events and the relationships between them. These relationships give information that is often not available in the structured record. Why was a drug given? What were the results of a physical examination? What problems were not present? We have previously reported entity extraction in the CLEF IE system (Roberts et al., 2008b). This paper examines relationship extraction. Extraction of relationships from clinical text is usually carried out as part of a full clinical IE system. Several such systems have been described. They generally use a syntactic parse with domainspecific grammar rules. The Linguistic String project (Sager et al., 1994) used a full syntactic and 10 BioNLP 2008: Current Trends in Biomedical Natural Language Processing, pages 10–18, c Columbus, Ohio, USA, June 2008. 2008 Association for Computational Linguistics clinical sublanguage parse to fill template data structures corresponding to medical"
W08-0602,P05-1053,0,0.396045,"m et al., 1995) also used a full parse, a conceptual representation of the text, and a large scale knowledge base. In other applications of biomedical NLP, a second paradigm has become widespread: the application of statistical machine learning techniques to feature-based models of the text. Such approaches have typically been applied to journal texts. They have been used both for entity recognition and extraction of various relations, such as protein-protein interactions (see, for example, Grover et al (2007)). This follows on from the success of these methods in general NLP (see for example Zhou et al (2005)). Statistical machine learning has also been applied to clinical text, but its use has generally been limited to entity recognition. The Mayo Clinic text analysis system (Pakhomov et al., 2005), for example, uses a combination of dictionary lookup and a Na¨ıve Bayes classifier to identify entities for information retrieval applications. To the best of our knowledge, statistical methods have not been previously applied to extraction of clinical relationships from text. This paper describes experiments in the statistical machine learning of relationships from a novel text type: oncology narrati"
W08-0602,P02-1022,0,\N,Missing
W14-4914,dione-etal-2010-design,0,0.0471047,"bo language. The starting point for this project is the development of a new part-of-speech tagging scheme based on the EAGLES tagset guidelines, adapted to incorporate additional language internal features. The tags are currently being used in a part-of-speech annotation task for the development of POS tagged Igbo corpus. The proposed tagset has 59 tags. 1 Introduction Supervised machine learning methods in NLP require an adequate amount of training data. The first crucial step for a part-of-speech (POS) tagging system for a language is a well designed, consistent, and complete tagset (Bamba Dione et al., 2010) which must be preceded by a detailed study and analysis of the language. Our tagset was developed from scratch through the study of linguistics and electronic texts in Igbo, using the EAGLES recommendations. This initial manual annotation is important. Firstly, information dealing with challenging phenomena in a language is expressed in the tagging guideline; secondly, computational POS taggers require annotated text as training data. Even in unsupervised methods, some annotated texts are still required as a benchmark in evaluation. With this in mind, our tagset design follows three main goal"
W14-4914,W06-1009,0,0.0354089,"e sentence. This particular example has already been cited as one of the uses of tone mark in the language. Apart from such instances, the sentences in the Bible are not tone marked. As such, one cannot rely on such restricted use of tone marks for any major conclusions on the grammar of the language. With regard to corpus work in general, the Bible has been described as consistent in its orthography, most easily accessible, carefully translated (most translators believe it is the word of God), and well structured (books, chapters, verses), etc. (Resnik et al., 1999; Kanungo and Resnik, 1999; Chew et al., 2006). The NWT Bible is generally written in standard Igbo. 4 Tokenization We outline here the method we used in the tokenization of the text. For the sake of a start-up, we tokenized based on the whitespace. The Igbo language uses whitespace to represent lexical boundaries; we used the following regex: Separate characters if the string matches: • “ga-” or “n”’ or “N”’ or “na-” or “Na-” or “ana-” or “i.na-”; for example, the following samples n’elu, na–erughari., .ina-akwa, ana-egbu in the Bible will be separated into n’, elu, na–, erughari., .ina-, akwa, ana-, egbu tokens. • Any non-zero length se"
W15-4635,W09-4613,0,0.0161401,"rsection set between the terms/words in the segment and in the comment. len returns the number of entries in the given set. • Jaccard: jaccard = len(I(S, C)) len(U (S, C)) (2) where U (S, C) is the union set between the terms/words in the segment and comment. • NE overlap: N Eoverlap = len(I(S, C)) len(U (S, C)) (3) where I(S, C) is the intersection set between the named entities (NEs) in the segment and in the comment and U (S, C) is the NEs union set. • DISCO 1 + DISCO 2: DISCO (DIStributionally similar words using CO-occurrences) assumes words with similar meaning occur in similar context (Kolb, 2009). Using large text collections such as the BNC corpora or Wikipedia, distributional similarity between words is computed by using a simple context window of size ±3 words for counting co-occurrences. DISCO computes two different similarities between words: DISCO1 and DISCO2. In DISCO1 when two words are directly compared for exact similarity DISCO simply retrieves their word vectors from the large text collections and computes the similarity according to Lin’s information theoretic measure (Lin, 1998). DISCO2 compares words based on their sets of distributional similar words. 3.2.2 Computing S"
W15-4635,P98-2127,0,0.0148338,"lar words using CO-occurrences) assumes words with similar meaning occur in similar context (Kolb, 2009). Using large text collections such as the BNC corpora or Wikipedia, distributional similarity between words is computed by using a simple context window of size ±3 words for counting co-occurrences. DISCO computes two different similarities between words: DISCO1 and DISCO2. In DISCO1 when two words are directly compared for exact similarity DISCO simply retrieves their word vectors from the large text collections and computes the similarity according to Lin’s information theoretic measure (Lin, 1998). DISCO2 compares words based on their sets of distributional similar words. 3.2.2 Computing Similarity Linking Score Using a linear function, we combine the scores of each of these features (cosine to DISCO) to produce a final similarity score for a commentsegment pair: Score = n X f eaturei ∗ weighti (4) i=1 where weighti is the weight associated with the ith feature. The weights are trained based on linear regression using the Weka package and the training data described in the following section. 4 3.2.3 Training Data Obtaining training data requires manual effort and human involvement and"
W15-4635,C98-2122,0,\N,Missing
W15-5405,J08-4004,0,0.122425,"Missing"
W15-5405,H92-1022,0,0.292715,"is developed mostly through hand-coded means, so it is important to measure the reliability of the tagset that produced it. The fundamental assumption of this exercise, as discussed in (Artstein and Massimo, 2007; Raquel, 2011), is that the output of manual annotation is considered reliable if it can be computed that 24 Proceedings of the Joint Workshop on Language Technology for Closely Related Languages, Varieties and Dialects, pages 24–33, c Hissar, Bulgaria, September 10, 2015. 2015 Association for Computational Linguistics TBL is a machine learning (ML) algorithm originally developed by Brill (1992). It starts with an initial state and requires a correctly tagged text, called truth, for training. The training process iteratively acquires an ordered list of rules that correct the errors found in the initial state until this initial state resembles the truth to some acceptable degree. Igbo word order is Subject-Verb-Object (SVO), with a complement to the right of the head in all types of phrases, for example, “Okeke killed a snake” is written: 2 Finding and correcting errors to make more accurate annotated data as found in Loftsson (2009) and Helgad´ottir et al., (2012) and our work are re"
W15-5405,E09-1060,0,0.0319359,"achine learning (ML) algorithm originally developed by Brill (1992). It starts with an initial state and requires a correctly tagged text, called truth, for training. The training process iteratively acquires an ordered list of rules that correct the errors found in the initial state until this initial state resembles the truth to some acceptable degree. Igbo word order is Subject-Verb-Object (SVO), with a complement to the right of the head in all types of phrases, for example, “Okeke killed a snake” is written: 2 Finding and correcting errors to make more accurate annotated data as found in Loftsson (2009) and Helgad´ottir et al., (2012) and our work are relatively similar in the aspect of inspecting marked data positions, but entirely different in methods. Loftsson (2009) and Helgad´ottir et al., (2012) applied trained POS-taggers singly and combined, respectively, then the outputs were compared with the gold standard and differences found were marked as error candidates for verification. Whereas our method projects changes made in the IAA into the main tagged corpus, and all positions where these changes occurred are inspected further. Okeke Okeke 3 The Igbo Language Igbo is one of the major"
W15-5405,W14-4914,1,0.867602,"e quality of the initial tagged Igbo corpus (ITC0), instead of ignoring them and tagging new text, which saves effort, time and money. A quality tagged corpus can help to maximize the performance of automatic POS-taggers used for tagging similar texts. We employ both manual and automatic processes in a semi-automatic method for this work. Our semi-automatic annotation method uses Transformation-based Learning (TBL) and a human expert, who is involved in several stages of the process. First, an initial Igbo tagged corpus (ITC0) was developed in a distributed manner using the tagset reported in Onyenwe et al., (2014). Through an inter-annotation agreement (IAA) exercise, this tagset (TS0) was evaluated and revised to ensure a more reliable and reproducible result. Then we use TBL to find and propagate changes from the IAA to this initial tagged corpus in an automated manner; an expert human annotator verifies locations TBL has marked for changes instead going through the entire text. Through this semiautomated process, the quality of the tagged corpus is increased with minimum expense. TBL is suitable for this because its inductive method performs very well using annotated corpora whose sizes are smaller"
W16-3605,W16-6610,1,0.619014,"of comments in themselves, and as a target for what an automated system might deliver online. Misra et al. (2015) have created manual summaries of short dialogue sequences, extracted from different conversations on similar issues on debat49 relating it with human judgements on system output quality. If it cannot be validated, the challenge arises to develop a metric better suited to this evaluation need. Our summary corpus has already proved useful in providing insights for system development, and for training and evaluation. We have used group annotations to evaluate a clustering algorithm (Aker et al., 2016a); used back-links to inform the training of a cluster labeling algorithm (Aker et al., 2016b); used the summaries as references in evaluating system outputs (with ROUGE as metric), and to inform human assessors in a task-based system evaluation (Barker et al., 2016). Acknowledgments The authors would like to thank the European Commission for supporting this work, carried out as part of the FP7 SENSEI project, grant reference: FP7-ICT-610916. We would also like to thank all the annotators without whose work the SENSEI corpus would not have been created, Jonathan Foster for his help in recruit"
W16-3605,W16-2802,1,0.902481,"oint or assertion already expressed. Issues are questions on which multiple viewpoints are possible; e.g., the issue of whether reducing bin collection to once every three weeks is a good idea, or whether reducing bin collection will lead to an increase in vermin. Issues are very often implicit, i.e not directly expressed in the comments (e.g., the issue of whether reducing bin collection will lead to an increase in vermin is never explicitly mentioned yet this is clearly what comments 1-4 are addressing). A fuller account of this issue-based framework for analysing reader comment is given in Barker and Gaizauskas (2016). Aside from argumentative content, reader comments exhibit other features as well. For example, commenters may seek clarification about facts (e.g. comment 4 where the commenter asks Is Bury going to provide larger bins for families . . . ?). But these clarifications are typically carried out in the broader context of making an argument, i.e. advancing evidence to support a viewpoint. Comments may also express jokes or emotion, though these too are often in the service of advancing some viewpoint (e.g. sarcasm or as in comments 4 and 6 emotive terms like lamebrained and crazy clearly indicati"
W16-3605,L16-1494,1,0.816562,"judgements on system output quality. If it cannot be validated, the challenge arises to develop a metric better suited to this evaluation need. Our summary corpus has already proved useful in providing insights for system development, and for training and evaluation. We have used group annotations to evaluate a clustering algorithm (Aker et al., 2016a); used back-links to inform the training of a cluster labeling algorithm (Aker et al., 2016b); used the summaries as references in evaluating system outputs (with ROUGE as metric), and to inform human assessors in a task-based system evaluation (Barker et al., 2016). Acknowledgments The authors would like to thank the European Commission for supporting this work, carried out as part of the FP7 SENSEI project, grant reference: FP7-ICT-610916. We would also like to thank all the annotators without whose work the SENSEI corpus would not have been created, Jonathan Foster for his help in recruiting annotators and The Guardian for allowing us access to their materials. Finally, thanks to our anonymous reviewers whose comments and suggestions have helped us to improve the paper. Even so, there are limitations to the work done which give pointers to further wor"
W16-3605,W14-2106,0,0.0607293,"ctive summary. A significant weakness of such summaries is that they fail to capture the essential argument-oriented nature of these multiway conversations, since single comments taken from topically distinct clusters do not reflect the argumentative structure of the conversation. In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015). If such elements and relations can be automatically extracted then they could serve as the basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. To the best of our knowledge, however, none have proposed how, given an analysis in terms of their theory, one might produce a summary of a full reader comment set. Researchers are beginning to explore how to generate summaries of extended ar"
W16-3605,W15-4631,0,0.029459,"ch summaries is that they fail to capture the essential argument-oriented nature of these multiway conversations, since single comments taken from topically distinct clusters do not reflect the argumentative structure of the conversation. In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015). If such elements and relations can be automatically extracted then they could serve as the basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. To the best of our knowledge, however, none have proposed how, given an analysis in terms of their theory, one might produce a summary of a full reader comment set. Researchers are beginning to explore how to generate summaries of extended argumentative conversations in social media, su"
W16-3605,W02-0406,0,0.189098,"Missing"
W16-3605,W04-1013,0,0.0550558,"Missing"
W16-3605,N15-1046,0,0.194431,"hey fail to capture the essential argument-oriented nature of these multiway conversations, since single comments taken from topically distinct clusters do not reflect the argumentative structure of the conversation. In the second approach, which might be characterised as argument-theory-driven, researchers working on argument mining from social media have articulated various schemes defining argument elements and relations in argumentative discourse and in some cases begun work on computational methods to identify them in text (Ghosh et al., 2014; Habernal et al., 2014; Swanson et al., 2015; Misra et al., 2015). If such elements and relations can be automatically extracted then they could serve as the basis for generating a summary that better reflects the argumentative content of reader comment. Indeed, several of these authors have cited summarization as a motivating application for their work. To the best of our knowledge, however, none have proposed how, given an analysis in terms of their theory, one might produce a summary of a full reader comment set. Researchers are beginning to explore how to generate summaries of extended argumentative conversations in social media, such as those found in"
W16-3605,D08-1081,0,0.0695176,"Missing"
W16-6610,aker-etal-2014-bootstrapping,1,0.783301,"ferences. From this pool, 20 comment clusters were randomly selected. Table 1 provides statistics on the two evaluation sets. 3 Method Our labeling approach is supervised and we refer to it as SCL (Supervised Cluster Labeler). Using the entire set of manually annotated Guardian articles, we collect training data to build a regression model for extracting labels for automatic clusters. To do this we first extract terms2 from the arti2 Terms are noun phrase-like word sequences and are extracted using POS-tag grammars such as NN NN. We use the automatically generated POS-tag grammars reported by Aker et al. (2014). cle as well as comments and represent them with features. Each term is assigned a score between 0 and 1, where 0 indicates a term that is a poor label for a cluster, and 1 a term that makes an excellent label. We obtain the score using human summaries generated for the Guardian articles. For these human summaries we have the information about which sentences in the summary links to which human clusters. If the question is to answer whether the term X is a good label for the Y cluster, then we collect the sentences from the human summaries that are linked to that Y cluster and compare that te"
W16-6610,W16-3605,1,0.738625,"present the topic (Lau et al., 2011; Mei et al., 2007). In most studies, such textual labels are still extractive, i.e. the methods rely on labels being present within the textual sources (Lau et al., 2011; Mei et al., 2007). To overcome this limitation, many studies use external resources, most notably Wikipedia, for deriving topic labels. Hulpus et al. (2013), for example, present a graph-based approach to labeling using DBpedia concepts. An advantage of such approaches is the potential to provide labels that are more abstract, and hence more akin to labels humans might produce. Aker et al. (2016) apply such an approach to the online news domain, and evaluate it via an information retrieval task (similar to the evaluation in Aletras et al. (2014)). However, low recall figures were reported due to the abstractedness of the labels. Joty et al. (2013) also argue that external resources like Wikipedia titles are too broad for their e-mail and blog domain, as shown by the fact that none of the human-created labels in their development set appears in a Wikipedia title. Chang et al. (2015) use human generated labels for social media posts in Google+, suggesting that post-internal information"
W16-6610,P14-1023,0,0.00994184,"raph): the number of occurrences of a term in the article sentences 2–6. • #Term in sentences after 6 (main text body): the number of occurrences of a term in the final portion of the article (from the 7th sentence to the end of the article). • #Term in the entire article: the number of occurrences of a term in the entire article. • Article centroid similarity: the cosine similarity (Salton and Lesk, 1968) between the term and the article centroid. The similarity is based on Word2Vec word embeddings: each word is represented by a 400-dimensional word embedding. We use the vectors published by Baroni et al. (2014). To compute the similarity of term:document pair, we remove stop-words and punctuation from each, then query for each remaining word’s vector representation using the Word2Vec, and create a sum of the word vectors. We use the resulting sum vectors to compute their cosine similarity. Features In the cluster labeling approach we use several features extracted from the news article and the comments. To investigate to what extent our intuition about the relevance of the news article for labelling comment clusters is justified and craft features, we analysed a set of 1.7K Guardian news articles al"
W16-6610,P11-1154,0,0.0310324,". Identifying topics in comment streams is vitally important to providing an overview of what readers are saying. However, merely clustering comments is not enough: topic clusters should also be given labels that accurately reflect their content, and that are accessible to users. Producing “good labels” is challenging, as what constitutes a good label is not well defined. A 61 common method of labelling topic clusters with the top-n key terms characterising the topic is reported as less suitable than generating “textual labels” not consisting of key terms, to meaningfully represent the topic (Lau et al., 2011; Mei et al., 2007). In most studies, such textual labels are still extractive, i.e. the methods rely on labels being present within the textual sources (Lau et al., 2011; Mei et al., 2007). To overcome this limitation, many studies use external resources, most notably Wikipedia, for deriving topic labels. Hulpus et al. (2013), for example, present a graph-based approach to labeling using DBpedia concepts. An advantage of such approaches is the potential to provide labels that are more abstract, and hence more akin to labels humans might produce. Aker et al. (2016) apply such an approach to th"
W17-1907,wagacha-etal-2006-grapheme,0,0.106399,"Missing"
W17-1907,U11-1016,0,0.0882331,"94a) observed that, although diacritic restoration is not a hugely popular task in NLP research, it shares similar properties with 54 language models for French. On the Croatian ˇ language, Santi´ c et al. (2009) used substitution schemes, a dictionary and language models in implementing a similar architecture. For Spanish, Yarowsky (1999) used dictionaries with decision lists, Bayesian classification and Viterbi decoding the surrounding context. Crandall (2005), using Bayesian approach, HMM and a hybrid of both, as well as different evaluation method, attempted to improve on Yarowsky’s work. Cocks and Keegan (2011) worked on M¯aori using na¨ıve Bayes and wordbased n-grams relating to the target word as instance features. Tufis¸ and Chit¸u (1999) used POS tagging to restore Romanian texts but backed off to character-based approach to deal with “unknown words”. Generally, there seems to be a consensus on the superiority of the word-based approach for well resourced languages. Figure 1: Illustrative View of the Diacritic Restoration Process (Ezeani et al., 2016) 2.1 Missing orthographic diacritics 1. Nwanyi ahu banyere n’ugbo ya. (The woman entered her [farm|boat/craft]) 2. O kwuru banyere olu ya. (He/she"
W17-1907,P94-1013,0,0.855033,"ndi m’ozi, ihu ya dika anyanwu ututu,ahu ya n’achakwa bara bara ka mmiri si n’okwute. ka ihe niile no n’agbam n’obi,o sim na ohuru m n’anya.na ochoro k’anyi buru enyi,a hukwuru m ya n’anya.anyi wee kwekorita wee buru enyi onye’a m n’ekwu maka ya bu odinobi m,onye ihe ya n’amasi m such tasks as word sense disambiguation with regards to resolving both syntactic and semantic ambiguities. Indeed it was referred to as an instance of a closely related class of problems which includes word choice selection in machine translation, homograph and homophone disambiguation and capitalisation restoration (Yarowsky, 1994b). Diacritic restoration, like sense disambiguation, is not an end in itself but an “intermediate task” (Wilks and Stevenson, 1996) which supports better understanding and representation of meanings in human-machine interactions. In most nondiacritic languages, sense disambiguation systems can directly support such tasks as machine translation, information retrieval, text processing, speech processing etc. (Ide and V´eronis, 1998). But it takes more for diacritic languages, where possible, to produce standard texts. So for those languages, to achieve good results with such systems as listed a"
W17-1907,J98-1001,0,\N,Missing
W17-1907,W02-2021,0,\N,Missing
W18-4004,J10-4006,0,0.0412575,"t efforts to develop resources for Igbo include the design of Igbo POS tagset (Onyenwe et al., 2014), and the tagset refinement (Onyenwe et al., 2015) as well as the development of Igbo POS-tagger (Onyenwe, 2017). Works are also on-going with its automatic diacritic restoration and lexical disambiguation (Ezeani et al., 2016) (Ezeani et al., 2017) and morphological segmentation (Enemouh et al., 2017). 1.1 Embedding Models Word embeddings are generic semantic representations from corpus. It enhances the concept of distributional hypothesis (Harris, 1954) and count-based distributional vectors (Baroni and Lenci, 2010) and provides an alternative to the one task, one model approach. Their application areas span most NLP tasks and other fields such as biomedical, psychiatry, psychology, philology, cognitive science and social science (Altszyler et al., 2016). There are many approaches to training embedding models, however predictive (Mikolov et al., 2013a) and count-based (Pennington et al., 2014) models are very commonly used. Ideally, a model trained in one language should capture similar semantic distribution in other languages. Since the large amount of data required to train such a model are not often a"
W18-4004,D16-1136,0,0.0302356,"cross languages in the form of word-aligned data, sentence-aligned data (e.g. Europarl corpus), documentaligned data (e.g. Wikipedia), lexicon (bilingual or cross-lingual dictionary) or even zero-shot learning with no parallel data. In a survey of cross-lingual embedding models (Ruder, 2017), four different approaches were identified, monolingual mapping (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Guo et al., 2015) which trains embeddings on large monolingual corpora and then linearly maps a target language word to its corresponding source language embedding vectors; pseudo-cross-lingual (Duong et al., 2016; Gouws and Søgaard, 2015; Xiao and Guo, 2014) which trains embeddings with a pseudo-cross-lingual corpus i.e mixing contexts from different languages; cross-lingual (Hermann and Blunsom, 2013; Hermann and Blunsom, 2014; Koˇcisk`y et al., 2014) trains embeddings on a parallel corpus constraining similar words to be close to each other in a shared vector space; joint optimization (Klementiev et al., 2012; Luong et al., 2015; Gouws et al., 2015) trains models on parallel or monolingual data but jointly optimise a combination of monolingual and cross-lingual losses. In this paper, we will adopt t"
W18-4004,E14-1049,0,0.0368877,"domain in solving a problem in another domain. It is commonly applied when the target domain training data is limited (Weiss et al., 2016). With transfer learning we could take advantage of a parallel data that exist across languages in the form of word-aligned data, sentence-aligned data (e.g. Europarl corpus), documentaligned data (e.g. Wikipedia), lexicon (bilingual or cross-lingual dictionary) or even zero-shot learning with no parallel data. In a survey of cross-lingual embedding models (Ruder, 2017), four different approaches were identified, monolingual mapping (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Guo et al., 2015) which trains embeddings on large monolingual corpora and then linearly maps a target language word to its corresponding source language embedding vectors; pseudo-cross-lingual (Duong et al., 2016; Gouws and Søgaard, 2015; Xiao and Guo, 2014) which trains embeddings with a pseudo-cross-lingual corpus i.e mixing contexts from different languages; cross-lingual (Hermann and Blunsom, 2013; Hermann and Blunsom, 2014; Koˇcisk`y et al., 2014) trains embeddings on a parallel corpus constraining similar words to be close to each other in a shared vector space; joint optimization (Kl"
W18-4004,N15-1157,0,0.0185375,"he form of word-aligned data, sentence-aligned data (e.g. Europarl corpus), documentaligned data (e.g. Wikipedia), lexicon (bilingual or cross-lingual dictionary) or even zero-shot learning with no parallel data. In a survey of cross-lingual embedding models (Ruder, 2017), four different approaches were identified, monolingual mapping (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Guo et al., 2015) which trains embeddings on large monolingual corpora and then linearly maps a target language word to its corresponding source language embedding vectors; pseudo-cross-lingual (Duong et al., 2016; Gouws and Søgaard, 2015; Xiao and Guo, 2014) which trains embeddings with a pseudo-cross-lingual corpus i.e mixing contexts from different languages; cross-lingual (Hermann and Blunsom, 2013; Hermann and Blunsom, 2014; Koˇcisk`y et al., 2014) trains embeddings on a parallel corpus constraining similar words to be close to each other in a shared vector space; joint optimization (Klementiev et al., 2012; Luong et al., 2015; Gouws et al., 2015) trains models on parallel or monolingual data but jointly optimise a combination of monolingual and cross-lingual losses. In this paper, we will adopt the projection approach de"
W18-4004,P15-1119,0,0.122434,"blem in another domain. It is commonly applied when the target domain training data is limited (Weiss et al., 2016). With transfer learning we could take advantage of a parallel data that exist across languages in the form of word-aligned data, sentence-aligned data (e.g. Europarl corpus), documentaligned data (e.g. Wikipedia), lexicon (bilingual or cross-lingual dictionary) or even zero-shot learning with no parallel data. In a survey of cross-lingual embedding models (Ruder, 2017), four different approaches were identified, monolingual mapping (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Guo et al., 2015) which trains embeddings on large monolingual corpora and then linearly maps a target language word to its corresponding source language embedding vectors; pseudo-cross-lingual (Duong et al., 2016; Gouws and Søgaard, 2015; Xiao and Guo, 2014) which trains embeddings with a pseudo-cross-lingual corpus i.e mixing contexts from different languages; cross-lingual (Hermann and Blunsom, 2013; Hermann and Blunsom, 2014; Koˇcisk`y et al., 2014) trains embeddings on a parallel corpus constraining similar words to be close to each other in a shared vector space; joint optimization (Klementiev et al., 20"
W18-4004,P14-1006,0,0.0238936,"th no parallel data. In a survey of cross-lingual embedding models (Ruder, 2017), four different approaches were identified, monolingual mapping (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Guo et al., 2015) which trains embeddings on large monolingual corpora and then linearly maps a target language word to its corresponding source language embedding vectors; pseudo-cross-lingual (Duong et al., 2016; Gouws and Søgaard, 2015; Xiao and Guo, 2014) which trains embeddings with a pseudo-cross-lingual corpus i.e mixing contexts from different languages; cross-lingual (Hermann and Blunsom, 2013; Hermann and Blunsom, 2014; Koˇcisk`y et al., 2014) trains embeddings on a parallel corpus constraining similar words to be close to each other in a shared vector space; joint optimization (Klementiev et al., 2012; Luong et al., 2015; Gouws et al., 2015) trains models on parallel or monolingual data but jointly optimise a combination of monolingual and cross-lingual losses. In this paper, we will adopt the projection approach described in (Guo et al., 2015). 2 Experimental Setup Our experimental data consists of a collection of Igbo texts from the Igbo Bible and the translation of the Universal Declaration of Human Rig"
W18-4004,C12-1089,0,0.0211962,"14; Guo et al., 2015) which trains embeddings on large monolingual corpora and then linearly maps a target language word to its corresponding source language embedding vectors; pseudo-cross-lingual (Duong et al., 2016; Gouws and Søgaard, 2015; Xiao and Guo, 2014) which trains embeddings with a pseudo-cross-lingual corpus i.e mixing contexts from different languages; cross-lingual (Hermann and Blunsom, 2013; Hermann and Blunsom, 2014; Koˇcisk`y et al., 2014) trains embeddings on a parallel corpus constraining similar words to be close to each other in a shared vector space; joint optimization (Klementiev et al., 2012; Luong et al., 2015; Gouws et al., 2015) trains models on parallel or monolingual data but jointly optimise a combination of monolingual and cross-lingual losses. In this paper, we will adopt the projection approach described in (Guo et al., 2015). 2 Experimental Setup Our experimental data consists of a collection of Igbo texts from the Igbo Bible and the translation of the Universal Declaration of Human Rights, two short novels: an Igbo version of Eze Goes to School and another Igbo novel Mmadu. Ka A Na Ari.a. The pipeline has three stages. It starts with building the embedding models using"
W18-4004,P14-2037,0,0.0405368,"Missing"
W18-4004,W15-1521,0,0.0190573,"ch trains embeddings on large monolingual corpora and then linearly maps a target language word to its corresponding source language embedding vectors; pseudo-cross-lingual (Duong et al., 2016; Gouws and Søgaard, 2015; Xiao and Guo, 2014) which trains embeddings with a pseudo-cross-lingual corpus i.e mixing contexts from different languages; cross-lingual (Hermann and Blunsom, 2013; Hermann and Blunsom, 2014; Koˇcisk`y et al., 2014) trains embeddings on a parallel corpus constraining similar words to be close to each other in a shared vector space; joint optimization (Klementiev et al., 2012; Luong et al., 2015; Gouws et al., 2015) trains models on parallel or monolingual data but jointly optimise a combination of monolingual and cross-lingual losses. In this paper, we will adopt the projection approach described in (Guo et al., 2015). 2 Experimental Setup Our experimental data consists of a collection of Igbo texts from the Igbo Bible and the translation of the Universal Declaration of Human Rights, two short novels: an Igbo version of Eze Goes to School and another Igbo novel Mmadu. Ka A Na Ari.a. The pipeline has three stages. It starts with building the embedding models using training or project"
W18-4004,W15-5405,1,0.847308,"llenge to adapting these systems for low resource languages is lack of good quality data. Such languages often rely on poor quality webcrawled data. In our case the target language is Igbo, a language spoken by over 30 million indigenes who live mainly in the south-eastern part of Nigeria but also in different parts of the world. Inspite of the relatively large number of speakers, Igbo is critically low-resourced in terms of NLP research (Onyenwe et al., 2018). Recent efforts to develop resources for Igbo include the design of Igbo POS tagset (Onyenwe et al., 2014), and the tagset refinement (Onyenwe et al., 2015) as well as the development of Igbo POS-tagger (Onyenwe, 2017). Works are also on-going with its automatic diacritic restoration and lexical disambiguation (Ezeani et al., 2016) (Ezeani et al., 2017) and morphological segmentation (Enemouh et al., 2017). 1.1 Embedding Models Word embeddings are generic semantic representations from corpus. It enhances the concept of distributional hypothesis (Harris, 1954) and count-based distributional vectors (Baroni and Lenci, 2010) and provides an alternative to the one task, one model approach. Their application areas span most NLP tasks and other fields"
W18-4004,D14-1162,0,0.0831315,"emouh et al., 2017). 1.1 Embedding Models Word embeddings are generic semantic representations from corpus. It enhances the concept of distributional hypothesis (Harris, 1954) and count-based distributional vectors (Baroni and Lenci, 2010) and provides an alternative to the one task, one model approach. Their application areas span most NLP tasks and other fields such as biomedical, psychiatry, psychology, philology, cognitive science and social science (Altszyler et al., 2016). There are many approaches to training embedding models, however predictive (Mikolov et al., 2013a) and count-based (Pennington et al., 2014) models are very commonly used. Ideally, a model trained in one language should capture similar semantic distribution in other languages. Since the large amount of data required to train such a model are not often available for low resource languages, transfer learning techniques could be used to project learned knowledge from one language to another. This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 30 Proceedings of SemDeep-3, the 3rd Workshop on Semantic Deep Learning, pages 30–38 Santa Fe, Ne"
W18-4004,wagacha-etal-2006-grapheme,0,0.109023,"Missing"
W18-4004,W14-1613,0,0.0256038,"ata, sentence-aligned data (e.g. Europarl corpus), documentaligned data (e.g. Wikipedia), lexicon (bilingual or cross-lingual dictionary) or even zero-shot learning with no parallel data. In a survey of cross-lingual embedding models (Ruder, 2017), four different approaches were identified, monolingual mapping (Mikolov et al., 2013b; Faruqui and Dyer, 2014; Guo et al., 2015) which trains embeddings on large monolingual corpora and then linearly maps a target language word to its corresponding source language embedding vectors; pseudo-cross-lingual (Duong et al., 2016; Gouws and Søgaard, 2015; Xiao and Guo, 2014) which trains embeddings with a pseudo-cross-lingual corpus i.e mixing contexts from different languages; cross-lingual (Hermann and Blunsom, 2013; Hermann and Blunsom, 2014; Koˇcisk`y et al., 2014) trains embeddings on a parallel corpus constraining similar words to be close to each other in a shared vector space; joint optimization (Klementiev et al., 2012; Luong et al., 2015; Gouws et al., 2015) trains models on parallel or monolingual data but jointly optimise a combination of monolingual and cross-lingual losses. In this paper, we will adopt the projection approach described in (Guo et al"
W98-0117,C96-1091,1,0.350205,"roviding a functional semantics for DTG derivations, or rather of some DTG-like formalism, in a manner akin to that of categorial grammar. The approach envisaged is one in which each tree fragment (i.e. maximal unit containing no dominance links) of an initial dtree is associated with a lambda term. At the end of a derivation, the meaning of the resulting tree would be computed by working bottom up, applying 6 The relevant subformulae can be precisely characterised in terms of a notion polarity: hypothetica.ls correspond to maximal positive-polarity subformulae of hlgher-order forrnulae. See (Hepple, 1996) for details. · 67 the meaning term of each basic tree fragment to the meanings computed for each complete subtree added in at the fragment's frontier nodes, in some fixed fashion (e.g. such as in their right-to-left order). Strictly, terms would be combined using the special bt...~stitution operation of rule ( 4) (allowing variable capture in the manner discussed). Suitable terms to associate with tree fragments will be arrived at by exploiting the analogy between d-trees and higherorder formulae under compilation. (7) s NP NP VP :j John /""'-..NP V Re! /""-.... : &gt;.u.which(&gt;.:.u) NP...,h S 1 :"
W98-0117,P95-1021,0,0.305689,"Missing"
webb-etal-2008-cross,W04-2319,0,\N,Missing
webb-etal-2008-cross,J93-3003,0,\N,Missing
webb-etal-2008-cross,N06-1036,0,\N,Missing
webb-etal-2008-cross,J00-3003,0,\N,Missing
webb-etal-2008-cross,J96-2004,0,\N,Missing
webb-etal-2008-cross,J86-3001,0,\N,Missing
webb-etal-2008-cross,P98-2188,0,\N,Missing
webb-etal-2008-cross,C98-2183,0,\N,Missing
webb-etal-2008-cross,H01-1073,0,\N,Missing
wilks-etal-2004-human,J00-3003,0,\N,Missing
wilks-etal-2004-human,P98-2188,0,\N,Missing
wilks-etal-2004-human,C98-2183,0,\N,Missing
