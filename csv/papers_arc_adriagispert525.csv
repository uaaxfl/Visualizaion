W19-5340,{CUED}@{WMT}19:{EWC}{\\&}{LM}s,2019,60,0,3,0.531583,12216,felix stahlberg,"Proceedings of the Fourth Conference on Machine Translation (Volume 2: Shared Task Papers, Day 1)",0,"Two techniques provide the fabric of the Cambridge University Engineering Department{'}s (CUED) entry to the WMT19 evaluation campaign: elastic weight consolidation (EWC) and different forms of language modelling (LMs). We report substantial gains by fine-tuning very strong baselines on former WMT test sets using a combination of checkpoint averaging and EWC. A sentence-level Transformer LM and a document-level LM based on a modified Transformer architecture yield further gains. As in previous years, we also extract n-gram probabilities from SMT lattices which can be seen as a source-conditioned n-gram LM."
P19-1022,Domain Adaptive Inference for Neural Machine Translation,2019,0,5,3,0.925926,13929,danielle saunders,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"We investigate adaptive ensemble weighting for Neural Machine Translation, addressing the case of improving performance on a new and potentially unknown domain without sacrificing performance on the original domain. We adapt sequentially across two Spanish-English and three English-German tasks, comparing unregularized fine-tuning, L2 and Elastic Weight Consolidation. We then report a novel scheme for adaptive NMT ensemble decoding by extending Bayesian Interpolation with source information, and report strong improvements across test domains without access to the domain label."
D19-5203,Controlling {J}apanese Honorifics in {E}nglish-to-{J}apanese Neural Machine Translation,2019,0,0,3,0,14102,weston feely,Proceedings of the 6th Workshop on Asian Translation,0,"In the Japanese language different levels of honorific speech are used to convey respect, deference, humility, formality and social distance. In this paper, we present a method for controlling the level of formality of Japanese output in English-to-Japanese neural machine translation (NMT). By using heuristics to identify honorific verb forms, we classify Japanese sentences as being one of three levels of informal, polite, or formal speech in parallel text. The English source side is marked with a feature that identifies the level of honorific speech present in the Japanese target side. We use this parallel text to train an English-Japanese NMT model capable of producing Japanese translations in different honorific speech styles for the same English input sentence."
W18-6427,The {U}niversity of {C}ambridge{'}s Machine Translation Systems for {WMT}18,2018,54,1,2,0.625,12216,felix stahlberg,Proceedings of the Third Conference on Machine Translation: Shared Task Papers,0,"The University of Cambridge submission to the WMT18 news translation task focuses on the combination of diverse models of translation. We compare recurrent, convolutional, and self-attention-based neural models on German-English, English-German, and Chinese-English. Our final system combines all neural models together with a phrase-based SMT system in an MBR-based scheme. We report small but consistent gains on top of strong Transformer ensembles."
W18-1914,Turning {NMT} Research into Commercial Products,2018,0,0,2,0,28507,dragos munteanu,Proceedings of the 13th Conference of the Association for Machine Translation in the {A}mericas (Volume 2: User Track),0,None
P18-2051,Multi-representation ensembles and delayed {SGD} updates improve syntax-based {NMT},2018,15,4,3,0.925926,13929,danielle saunders,Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We explore strategies for incorporating target syntax into Neural Machine Translation. We specifically focus on syntax in ensembles containing multiple sentence representations. We formulate beam search over such ensembles using WFSTs, and describe a delayed SGD update training procedure that is especially effective for long representations like linearized syntax. Our approach gives state-of-the-art performance on a difficult Japanese-English task."
N18-3013,Accelerating {NMT} Batched Beam Decoding with {LMBR} Posteriors for Deployment,2018,18,2,3,1,28540,gonzalo iglesias,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 3 (Industry Papers)",0,"We describe a batched beam decoding algorithm for NMT with LMBR n-gram posteriors, showing that LMBR techniques still yield gains on top of the best recently reported results with Transformers. We also discuss acceleration strategies for deployment, and the effect of the beam size and batching on memory and speed."
N18-2081,Neural Machine Translation Decoding with Terminology Constraints,2018,4,3,2,1,9981,eva hasler,"Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 2 (Short Papers)",0,"Despite the impressive quality improvements yielded by neural machine translation (NMT) systems, controlling their translation output to adhere to user-provided terminology constraints remains an open problem. We describe our approach to constrained neural decoding based on finite-state machines and multi-stack decoding which supports target-side constraints as well as constraints with corresponding aligned input text spans. We demonstrate the performance of our framework on multiple translation tasks and motivate the need for constrained decoding with attentions as a means of reducing misplacement and duplication when translating user constraints."
W17-3531,A Comparison of Neural Models for Word Ordering,2017,18,4,4,1,9981,eva hasler,Proceedings of the 10th International Conference on Natural Language Generation,0,We compare several language models for the word-ordering task and propose a new bag-to-sequence neural model based on attention-based sequence-to-sequence models. We evaluate the model on a large German WMT data set where it significantly outperforms existing models. We also describe a novel search strategy for LM-based word ordering and report results on the English Penn Treebank. Our best model setup outperforms prior work both in terms of speed and quality.
E17-2058,Neural Machine Translation by Minimising the {B}ayes-risk with Respect to Syntactic Translation Lattices,2017,16,14,2,0.625,12216,felix stahlberg,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,We present a novel scheme to combine neural machine translation (NMT) with traditional statistical machine translation (SMT). Our approach borrows ideas from linearised lattice minimum Bayes-risk decoding for SMT. The NMT score is combined with the Bayes-risk of the translation according the SMT lattice. This makes our approach much more flexible than n-best list or lattice rescoring as the neural decoder is not restricted to the SMT search space. We show an efficient and simple way to integrate risk estimation into the NMT decoder which is suitable for word-level as well as subword-unit-level NMT. We test our method on English-German and Japanese-English and report significant gains over lattice rescoring on several data sets for both single and ensembled NMT. The MBR decoder produces entirely new hypotheses far beyond simply rescoring the SMT search space or fixing UNKs in the NMT output.
N16-1100,Speed-Constrained Tuning for Statistical Machine Translation Using {B}ayesian Optimization,2016,28,0,2,0,5907,daniel beck,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"xc2xa92016 Association for Computational Linguistics. We address the problem of automatically finding the parameters of a statistical machine translation system that maximize BLEU scores while ensuring that decoding speed exceeds a minimum value. We propose the use of Bayesian Optimization to efficiently tune the speed-related decoding parameters by easily incorporating speed as a noisy constraint function. The obtained parameter values are guaranteed to satisfy the speed constraint with an associated confidence margin. Across three language pairs and two speed constraint values, we report overall optimization time reduction compared to grid and random search. We also show that Bayesian Optimization can decouple speed and BLEU measurements, resulting in a further reduction of overall optimization time as speed is measured over a small subset of sentences."
N15-1105,Fast and Accurate Preordering for {SMT} using Neural Networks,2015,25,16,1,1,23877,adria gispert,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose the use of neural networks to model source-side preordering for faster and better statistical machine translation. The neural network trains a logistic regression model to predict whether two sibling nodes of the source-side parse tree should be swapped in order to obtain a more monotonic parallel corpus, based on samples extracted from the word-aligned parallel corpus. For multiple language pairs and domains, we show that this yields the best reordering performance against other state-of-the-art techniques, resulting in improved translation quality and very fast decoding."
D15-1273,Transducer Disambiguation with Sparse Topological Features,2015,16,5,2,1,28540,gonzalo iglesias,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We describe a simple and efficient algorithm to disambiguate non-functional weighted finite state transducers (WFSTs), i.e. to generate a new WFST that contains a unique, best-scoring path for each hypothesis in the input labels along with the best output labels. The algorithm uses topological features combined with a tropical sparse tuple vector semiring. We empirically show that our algorithm is more efficient than previous work in a PoStagging disambiguation task. We use our method to rescore very large translation lattices with a bilingual neural network language model, obtaining gains in line with the literature."
J14-3008,Pushdown Automata in Statistical Machine Translation,2014,50,12,3,0.807115,24531,cyril allauzen,Computational Linguistics,0,"This article describes the use of pushdown automata (PDA) in the context of statistical machine translation and alignment under a synchronous context-free grammar. We use PDAs to compactly represent the space of candidate translations generated by the grammar when applied to an input sentence. General-purpose PDA algorithms for replacement, composition, shortest path, and expansion are presented. We describe HiPDT, a hierarchical phrase-based decoder using the PDA representation and these algorithms. We contrast the complexity of this decoder with a decoder based on a finite state automata representation, showing that PDAs provide a more suitable framework to achieve exact decoding for larger synchronous context-free grammars and smaller language models. We assess this experimentally on a large-scale Chinese-to-English alignment and translation task. In translation, we propose a two-pass decoding strategy involving a weaker language model in the first-pass to address the results of PDA complexity analysis. We study in depth the experimental conditions and tradeoffs in which HiPDT can achieve state-of-the-art performance for large-scale SMT."
E14-1026,Source-side Preordering for Translation using Logistic Regression and Depth-first Branch-and-Bound Search,2014,29,14,2,0,25409,laura jehl,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We present a simple preordering approach for machine translation based on a featurerich logistic regression model to predict whether two children of the same node in the source-side parse tree should be swapped or not. Given the pair-wise children regression scores we conduct an efficient depth-first branch-and-bound search through the space of possible children permutations, avoiding using a cascade of classifiers or limiting the list of possible ordering outcomes. We report experiments in translating English to Japanese and Korean, demonstrating superior performance as (a) the number of crossing links drops by more than 10% absolute with respect to other state-of-the-art preordering approaches, (b) BLEU scores improve on 2.2 points over the baseline with lexicalised reordering model, and (c) decoding can be carried out 80 times faster."
E14-1028,Word Ordering with Phrase-Based Grammars,2014,40,10,1,1,23877,adria gispert,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"We describe an approach to word ordering using modelling techniques from statistical machine translation. The system incorporates a phrase-based model of string generation that aims to take unordered bags of words and produce fluent, grammatical sentences. We describe the generation grammars and introduce parsing procedures that address the computational complexity of generation under permutation of phrases. Against the best previous results reported on this task, obtained using syntax driven models, we report huge quality improvements, with BLEU score gains of 20 which we confirm with human fluency judgements. Our system incorporates dependency language models, large n-gram language models, and minimum Bayes risk decoding."
C14-1195,Effective Incorporation of Source Syntax into Hierarchical Phrase-based Translation,2014,34,4,2,0.833333,4608,tong xiao,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,In this paper we explicitly consider source language syntactic information in both rule extraction and decoding for hierarchical phrase-based translation. We obtain tree-to-string rules by the GHKM method and use them to complement Hiero-style rules. All these rules are then employed to decode new sentences with source language parse trees. We experiment with our approach in a state-of-the-art Chinese-English system and demonstrate 1.2 and 0.8 BLEU improvements on the NIST newswire and web evaluation data of MT08 and MT12.
W13-2225,The {U}niversity of {C}ambridge {R}ussian-{E}nglish System at {WMT}13,2013,22,8,4,1,5715,juan pino,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes the University of Cambridge submission to the Eighth Workshop on Statistical Machine Translation. We report results for the RussianEnglish translation task. We use multiple segmentations for the Russian input language. We employ the Hadoop framework to extract rules. The decoder is HiFST, a hierarchical phrase-based decoder implemented using weighted finitestate transducers. Lattices are rescored with a higher order language model and minimum Bayes-risk objective."
2012.eamt-1.34,Can Automatic Post-Editing Make {MT} More Meaningful,2012,22,11,5,0,43862,kristen parton,Proceedings of the 16th Annual conference of the European Association for Machine Translation,0,"Automatic post-editors (APEs) enable the re-use of black box machine translation (MT) systems for a variety of tasks where different aspects of translation are important. In this paper, we describe APEs that target adequacy errors, a critical problem for tasks such as cross-lingual question-answering, and compare different approaches for post-editing: a rule-based system and a feedback approach that uses a computer in the loop to suggest improvements to the MT system. We test the APEs on two different MT systems and across two different genres. Human evaluation shows that the APEs significantly improve adequacy, regardless of approach, MT system or genre: 30-56% of the post-edited sentences have improved adequacy compared to the original MT."
D11-1127,Hierarchical Phrase-based Translation Representations,2011,28,23,4,1,28540,gonzalo iglesias,Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing,0,"This paper compares several translation representations for a synchronous context-free grammar parse including CFGs/hypergraphs, finite-state automata (FSA), and pushdown automata (PDA). The representation choice is shown to determine the form and complexity of target LM intersection and shortest-path algorithms that follow. Intersection, shortest path, FSA expansion and RTN replacement algorithms are presented for PDAs. Chinese-to-English translation experiments using HiFST and HiPDT, FSA and PDA-based decoders, are presented using admissible (or exact) search, possible for HiFST with compact SCFG rulesets and HiPDT with compact LMs. For large rulesets with large LMs, we introduce a two-pass search strategy which we then analyze in terms of search errors and translation performance."
W10-1722,The {CUED} {H}i{FST} System for the {WMT}10 Translation Shared Task,2010,21,4,3,1,5715,juan pino,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes the Cambridge University Engineering Department submission to the Fifth Workshop on Statistical Machine Translation. We report results for the French-English and Spanish-English shared translation tasks in both directions. The CUED system is based on HiFST, a hierarchical phrase-based decoder implemented using weighted finite-state transducers. In the French-English task, we investigate the use of context-dependent alignment models. We also show that lattice minimum Bayes-risk decoding is an effective framework for multi-source translation, leading to large gains in BLEU score."
P10-2006,Efficient Path Counting Transducers for Minimum {B}ayes-Risk Decoding of Statistical Machine Translation Lattices,2010,13,14,2,1,30901,graeme blackwood,Proceedings of the {ACL} 2010 Conference Short Papers,0,This paper presents an efficient implementation of linearised lattice minimum Bayes-risk decoding using weighted finite state transducers. We introduce transducers to efficiently count lattice paths containing n-grams and use these to gather the required statistics. We show that these procedures can be implemented exactly through simple transformations of word sequences to sequences of n-grams. This yields a novel implementation of lattice minimum Bayes-risk decoding which is fast and exact even for very large lattices.
J10-3008,Hierarchical Phrase-Based Translation with Weighted Finite-State Transducers and Shallow-n Grammars,2010,34,47,1,1,23877,adria gispert,Computational Linguistics,0,"In this article we describe HiFST, a lattice-based decoder for hierarchical phrase-based translation and alignment. The decoder is implemented with standard Weighted Finite-State Transducer (WFST) operations as an alternative to the well-known cube pruning procedure. We find that the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in fewer search errors, better parameter optimization, and improved translation performance. The direct generation of translation lattices in the target language can improve subsequent rescoring procedures, yielding further gains when applying long-span language models and Minimum Bayes Risk decoding. We also provide insights as to how to control the size of the search space defined by hierarchical rules. We show that shallow-n grammars, low-level rule catenation, and other search constraints can help to match the power of the translation system to specific language pairs."
D10-1053,Hierarchical Phrase-Based Translation Grammars Extracted from Alignment Posterior Probabilities,2010,27,12,1,1,23877,adria gispert,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We report on investigations into hierarchical phrase-based translation grammars based on rules extracted from posterior distributions over alignments of the parallel text. Rather than restrict rule extraction to a single alignment, such as Viterbi, we instead extract rules based on posterior distributions provided by the HMM word-to-word alignment model. We define translation grammars progressively by adding classes of rules to a basic phrase-based system. We assess these grammars in terms of their expressive power, measured by their ability to align the parallel text from which their rules are extracted, and the quality of the translations they yield. In Chinese-to-English translation, we find that rule extraction from posteriors gives translation improvements. We also find that grammars with rules with only one nonterminal, when extracted from posteriors, can outperform more complex grammars extracted from Viterbi alignments. Finally, we show that the best way to exploit source-to-target and target-to-source alignment models is to build two separate systems and combine their output translation lattices."
C10-1009,Fluency Constraints for Minimum {B}ayes-Risk Decoding of Statistical Machine Translation Lattices,2010,35,10,2,1,30901,graeme blackwood,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,A novel and robust approach to improving statistical machine translation fluency is developed within a minimum Bayes-risk decoding framework. By segmenting translation lattices according to confidence measures over the maximum likelihood translation hypothesis we are able to focus on regions with potential translation errors. Hypothesis space constraints based on monolingual coverage are applied to the low confidence regions to improve overall translation fluency.
N09-2019,Minimum {B}ayes Risk Combination of Translation Hypotheses from Alternative Morphological Decompositions,2009,24,32,1,1,23877,adria gispert,"Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics, Companion Volume: Short Papers",0,We describe a simple strategy to achieve translation performance improvements by combining output from identical statistical machine translation systems trained on alternative morphological decompositions of the source language. Combination is done by means of Minimum Bayes Risk decoding over a shared N-best list. When translating into English from two highly inflected languages such as Arabic and Finnish we obtain significant improvements over simply selecting the best morphological decomposition.
N09-1013,Context-Dependent Alignment Models for Statistical Machine Translation,2009,29,17,2,0,45415,jamie brunning,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We introduce alignment models for Machine Translation that take into account the context of a source word when determining its translation. Since the use of these contexts alone causes data sparsity problems, we develop a decision tree algorithm for clustering the contexts based on optimisation of the EM auxiliary function. We show that our context-dependent models lead to an improvement in alignment quality, and an increase in translation quality when the alignments are used in Arabic-English and Chinese-English translation."
N09-1049,Hierarchical Phrase-Based Translation with Weighted Finite State Transducers,2009,95,31,2,1,28540,gonzalo iglesias,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper describes a lattice-based decoder for hierarchical phrase-based translation. The decoder is implemented with standard WFST operations as an alternative to the well-known cube pruning procedure. We find that the use of WFSTs rather than k-best lists requires less pruning in translation search, resulting in fewer search errors, direct generation of translation lattices in the target language, better parameter optimization, and improved translation performance when rescoring with long-span language models and MBR decoding. We report translation experiments for the Arabic-to-English and Chinese-to-English NIST translation tasks and contrast the WFST-based hierarchical decoder with hierarchical translation under cube pruning."
E09-1044,Rule Filtering by Pattern for Efficient Hierarchical Translation,2009,29,41,2,1,28540,gonzalo iglesias,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We describe refinements to hierarchical translation search procedures intended to reduce both search errors and memory usage through modifications to hypothesis expansion in cube pruning and reductions in the size of the rule sets used in translation. Rules are put into syntactic classes based on the number of non-terminals and the pattern, and various filtering strategies are then applied to assess the impact on translation speed and quality. Results are reported on the 2008 NIST Arabic-to-English evaluation task."
W08-0316,{E}uropean Language Translation with Weighted Finite State Transducers: The {CUED} {MT} System for the 2008 {ACL} Workshop on {SMT},2008,14,10,2,1,30901,graeme blackwood,Proceedings of the Third Workshop on Statistical Machine Translation,0,"We describe the Cambridge University Engineering Department phrase-based statistical machine translation system for Spanish-English and French-English translation in the ACL 2008 Third Workshop on Statistical Machine Translation Shared Task. The CUED system follows a generative model of translation and is implemented by composition of component models realised as Weighted Finite State Transducers, without the use of a special-purpose decoder. Details of system tuning for both Europarl and News translation tasks are provided."
C08-2005,Phrasal Segmentation Models for Statistical Machine Translation,2008,9,10,2,1,30901,graeme blackwood,Coling 2008: Companion volume: Posters,0,Phrasal segmentation models define a mapping from the words of a sentence to sequences of translatable phrases. We discuss the estimation of these models from large quantities of monolingual training text and describe their realization as weighted finite state transducers for incorporation into phrase-based statistical machine translation systems. Results are reported on the NIST Arabic-English translation tasks showing significant complementary gains in BLEU score with large 5-gram and 6-gram language models.
W06-3101,Morpho-syntactic Information for Automatic Error Analysis of Statistical Machine Translation Output,2006,16,33,2,0,5059,maja popovic,Proceedings on the Workshop on Statistical Machine Translation,0,"Evaluation of machine translation output is an important but difficult task. Over the last years, a variety of automatic evaluation measures have been studied, some of them like Word Error Rate (WER), Position Independent Word Error Rate (PER) and BLEU and NIST scores have become widely used tools for comparing different systems as well as for evaluating improvements within one system. However, these measures do not give any details about the nature of translation errors. Therefore some analysis of the generated output is needed in order to identify the main problems and to focus the research efforts. On the other hand, human evaluation is a time consuming and expensive task. In this paper, we investigate methods for using of morpho-syntactic information for automatic evaluation: standard error measures WER and PER are calculated on distinct word classes and forms in order to get a better idea about the nature of translation errors and possibilities for improvements."
W06-3120,{TALP} Phrase-based statistical translation system for {E}uropean language pairs,2006,15,9,3,0,5326,marta costajussa,Proceedings on the Workshop on Statistical Machine Translation,0,This paper reports translation results for the Exploiting Parallel Texts for Statistical Machine Translation (HLT-NAACL Workshop on Parallel Texts 2006). We have studied different techniques to improve the standard Phrase-Based translation system. Mainly we introduce two reordering approaches and add morphological information.
W06-3125,N-gram-based {SMT} System Enhanced with Reordering Patterns,2006,13,20,2,0.8,836,josep crego,Proceedings on the Workshop on Statistical Machine Translation,0,"This work presents translation results for the three data sets made available in the shared task Exploiting Parallel Texts for Statistical Machine Translation of the HLT-NAACL 2006 Workshop on Statistical Machine Translation. All results presented were generated by using the N-gram-based statistical machine translation system which has been enhanced from the last year's evaluation with a tagged target language model (using Part-Of-Speech tags). For both Spanish-English translation directions and the English-to-French translation task, the baseline system allows for linguistically motivated source-side reorderings."
J06-4004,N-gram-based Machine Translation,2006,36,210,4,1,40951,jose marino,Computational Linguistics,0,"This article describes in detail an n-gram approach to statistical machine translation. This approach consists of a log-linear combination of a translation model based on n-grams of bilingual units, which are referred to as tuples, along with four specific feature functions. Translation performance, which happens to be in the state of the art, is demonstrated with Spanish-to-English and English-to-Spanish translations of the European Parliament Plenary Sessions (EPPS)."
2006.iwslt-evaluation.17,The {TALP} Ngram-based {SMT} systems for {IWSLT} 2006,2006,37,25,2,0.8,836,josep crego,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes TALPtuples, the 2006 Ngrambased statistical machine translation system developed at the TALP Research Center of the UPC (Universitat Polit ecnica de Catalunya) in Barcelona. Emphasis is put on improvements and extensions of the system of previous years, being highlighted and empirically compared. Mainly, these include a novel and much more efcient word ordering strategy based on reordering patterns, a linguistically-guided tuple segmentation criterion and improved optimization procedures. The paper provides details of this system participation in the third International Workshop on Spoken Language Translation (IWSLT) held in Kyoto, Japan in November 2006. Results on four translation directions are reported, namely from Arabic, Chinese, Italian and Japanese into English for the open data track, thoroughly explaining all language-related preprocessing and optimization schemes."
2006.iwslt-evaluation.18,{TALP} phrase-based system and {TALP} system combination for {IWSLT} 2006,2006,14,7,3,0,5326,marta costajussa,Proceedings of the Third International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper describes the TALP phrase-based statistical machine translation system, enriched with the statistical machine reordering technique. We also report the combination of this system and the TALP-tuple, the n-gram-based statistical machine translation system. We report the results for all the tasks (Chinese, Arabic, Italian and Japanese to English) in the framework of the third evaluation campaign of the International Workshop on Spoken Language Translation."
W05-0823,Statistical Machine Translation of {E}uparl Data by using Bilingual N-grams,2005,10,18,3,0.740741,28438,rafael banchs,Proceedings of the {ACL} Workshop on Building and Using Parallel Texts,0,This work discusses translation results for the four Euparl data sets which were made available for the shared task Exploiting Parallel Texts for Statistical Machine Translation. All results presented were generated by using a statistical machine translation system which implements a log-linear combination of feature functions along with a bilingual n-gram translation model.
P05-2012,Phrase Linguistic Classification and Generalization for Improving Statistical Machine Translation,2005,12,12,1,1,23877,adria gispert,Proceedings of the {ACL} Student Research Workshop,0,"In this paper a method to incorporate linguistic information regarding single-word and compound verbs is proposed, as a first step towards an SMT model based on linguistically-classified phrases. By substituting these verb structures by the base form of the head verb, we achieve a better statistical word alignment performance, and are able to better estimate the translation model and generalize to unseen verb forms during translation. Preliminary experiments for the English - Spanish language pair are performed, and future research lines are detailed."
2005.mtsummit-papers.36,Bilingual N-gram Statistical Machine Translation,2005,22,37,4,1,40951,jose marino,Proceedings of Machine Translation Summit X: Papers,0,"This paper describes a statistical machine translation system that uses a translation model which is based on bilingual n-grams. When this translation model is log-linearly combined with four specific feature functions, state of the art translations are achieved for Spanish-to-English and English-to-Spanish translation tasks. Some specific results obtained for the EPPS (European Parliament Plenary Sessions) data are presented and discussed. Finally, future research issues are depicted."
2005.mtsummit-papers.37,"Reordered Search, and Tuple Unfolding for Ngram-based {SMT}",2005,-1,-1,3,1,836,josep crego,Proceedings of Machine Translation Summit X: Papers,0,"In Statistical Machine Translation, the use of reordering for certain language pairs can produce a significant improvement on translation accuracy. However, the search problem is shown to be NP-hard when arbitrary reorderings are allowed. This paper addresses the question of reordering for an Ngram-based SMT approach following two complementary strategies, namely reordered search and tuple unfolding. These strategies interact to improve translation quality in a Chinese to English task. On the one hand, we allow for an Ngram-based decoder (MARIE) to perform a reordered search over the source sentence, while combining a translation tuples Ngram model, a target language model, a word penalty and a word distance model. Interestingly, even though the translation units are learnt sequentially, its reordered search produces an improved translation. On the other hand, we allow for a modification of the translation units that unfolds the tuples, so that shorter units are learnt from a new parallel corpus, where the source sentences are reordered according to the target language. This tuple unfolding technique reduces data sparseness and, when combined with the reordered search, further boosts translation performance. Translation accuracy and efficency results are reported for the IWSLT 2004 Chinese to English task."
2005.iwslt-1.25,The {TALP} Ngram-based {SMT} System for {IWSLT}{'}05,2005,-1,-1,2,1,836,josep crego,Proceedings of the Second International Workshop on Spoken Language Translation,0,None
arranz-etal-2004-bilingual,Bilingual Connections for Trilingual Corpora: An {XML} Approach,2004,10,2,5,0,11078,victoria arranz,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,None
2004.iwslt-papers.3,Phrase-based alignment combining corpus cooccurrences and linguistic knowledge,2004,17,13,1,1,23877,adria gispert,Proceedings of the First International Workshop on Spoken Language Translation: Papers,0,"This paper introduces a phrase alignment strategy that seeks phrase and word links in two stages using cooccurrence measures and linguistic information. On a first stage, the algorithm finds high-precision links involving a linguistically-derived set of phrases, leaving word alignment to be performed in a second phase. Experiments have been carried out for an English-Spanish parallel corpus, and we show how phrase cooccurrence measures convey a complementary information to word cooccurrences, and a stronger evidence of a good alignment. Alignment Error Rate (AER) results are presented, being competitive with and even outperforming state-of-the-art alignment algorithms."
2004.iwslt-evaluation.14,{TALP}: Xgram-based spoken language translation system,2004,7,12,1,1,23877,adria gispert,Proceedings of the First International Workshop on Spoken Language Translation: Evaluation Campaign,0,"This paper introduces TALP, a speech-to-speech statistical machine translation system developed at the TALP Research Center (Barcelona, Spain). TALP generates translations by searching for the best scoring path through a Finite-State Transducers (FSTs), which models an Xgram of the bilingual language defined by tuples. A detailed description of the system and the core processes to train it from a parallel corpus are presented. Results on the Chinese-English supplied task of the Int. Workshop on Spoken Language Translation (IWSLTxe2x80x9904) Evaluation Campaign are shown and discussed. 1. Overview of the system TALP (Traduccio Automatica del Llenguatge Parlat) is a speech-to-speech statistical machine translation system developed at the TALP Research Center (Barcelona, Spain) during the last years. It implements an integrated architecture by joining speech recognition and translation in one single step. Mathematically, the system produces a translation by maximizing the joint probability between source and target languages, which is equivalent to a language model of an special language with bilingual units (called tuples). TALP implements this tuple language model by means of a Finite-State Transducer (FST) considering an Xgram memory, that is, a variablelength N-gram model which adapts its length to evidence in the data. Xgrams have proved good results in speech recognition tasks in the past [1]. Given such a bilingual FST, the search for a translation becomes the search for the best-scoring path among the transducerxe2x80x99s edges. This search can be performed by dynamic programming, using well-known decoding techniques from the speech recognition domain. This way, the Viterbi algorithm and a beam search can be used forwards taking only source-language words into account (first part of each tuple), reading words in the target language during trace-back to produce the translation. Using This work has been partially supported by the Spanish Government under grant TIC2002-04447-C02 (ALIADO project), the European Union under grant FP6-506738 (TC-STAR project) and the Dep. of Universities, Research and Information Society (Generalitat de Catalunya). Figure 1: A translation FST from Spanish to English the same structure and search method, acoustic models can be omitted to perform text translation tasks only. This translation FST is learned automatically from a parallel corpus in three main steps (and an optional preprocessing). First, an automatic word alignment is produced. Currently this is done by the freely-available GIZA software [2], implementing well-known IBM and HMM translation models [3, 4]. From this alignment, a tuple extraction algorithm generates the set tuples that induces a sequential segmentation of both source and target sentences. These tuples must respect word order in both languages, as this is necessary for the transducer to produce a correct-order translated output. Finally, Xgrams are learned using standard language modeling techniques. Previous publications on this system include [5] and [6]. The organization of the paper is as follows. Section 2 offers an overview of the system architecture, whereas sections 2 and 3 deepen into details on translation generation and training issues. Section 4 presents the experimental framework used to evaluate the system, whose results are discussed in section 5. Finally, section 6 concludes and outlines future research lines. 2. Translation generation Statistical machine translation is based on the assumption that every sentence e in the target language is a possible translation of a given sentence f in the source language. The main difference between two possible translations of a given sentence is a probability assigned to each, which is to be learned from a bilingual text corpus. This probability can be modeled by a joint probability model of source and target languages. In this case, solving the translation problem is finding the sentence in the target language that maximises equation 1. This probability can be approximated by an Xgram of a joint or bilingual language model, learned from a set of tuples, as expressed in equation 2. e = arg max e {p(e, f)} = xc2xb7 xc2xb7 xc2xb7 = (1)"
W03-1504,Low-cost Named Entity Classification for {C}atalan: Exploiting Multilingual Resources and Unlabeled Data,2003,8,4,2,0,25372,lluis marquez,Proceedings of the {ACL} 2003 Workshop on Multilingual and Mixed-language Named Entity Recognition,0,"This work studies Named Entity Classification (NEC) for Catalan without making use of large annotated resources of this language. Two views are explored and compared, namely exploiting solely the Catalan resources, and a direct training of bilingual classification models (Spanish and Catalan), given that a large collection of annotated examples is available for Spanish. The empirical results obtained on real data point out that multilingual models clearly outperform monolingual ones, and that the resulting Catalan NEC models are easier to improve by bootstrapping on unlabelled data."
