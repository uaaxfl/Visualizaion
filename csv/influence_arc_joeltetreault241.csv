2020.argmining-1.13,L16-1704,0,0.137328,"rt” annotators, simplifying the T VS P task design through their feedback and observations, as they provided both a deep understanding of the argumentation theory and practical experience annotating the arguments. Each expert annotator was a fluent or native English speaker with an advanced degree in linguistics. Experts underwent training, which included studying guidelines and participating in calibration tasks to analyze debate arguments from three sources: Dagstuhl-ArgQuality-Corpus-V22 , originally from UKPConvArgRank (Habernal and Gurevych, 2016); the Internet Argument corpus V23 (IAC) (Abbott et al., 2016); and ChangeMyView,4 a Reddit forum. Through the pilots and subsequent debriefs with the experts, we made the following modifications to the annotation task of Wachsmuth et al. (2017a): (1) Reduce taxonomy complexity. While T VS P defined the task to score all 11 AQ subaspects (Local Acceptability, Local Relevance, etc.), 3 dimensions (Cogency, Effectiveness, Reasonableness), and overall AQ, we reduced the number of qualities scored by only focusing on the 3 higher-level dimensions plus overall AQ. As a result, annotators assessed an argumentative text in terms of 4 scores instead of 15 scores"
2020.argmining-1.13,P16-1150,0,0.180443,"otations, we conducted 14 pilot experiments with a group of four “expert” annotators, simplifying the T VS P task design through their feedback and observations, as they provided both a deep understanding of the argumentation theory and practical experience annotating the arguments. Each expert annotator was a fluent or native English speaker with an advanced degree in linguistics. Experts underwent training, which included studying guidelines and participating in calibration tasks to analyze debate arguments from three sources: Dagstuhl-ArgQuality-Corpus-V22 , originally from UKPConvArgRank (Habernal and Gurevych, 2016); the Internet Argument corpus V23 (IAC) (Abbott et al., 2016); and ChangeMyView,4 a Reddit forum. Through the pilots and subsequent debriefs with the experts, we made the following modifications to the annotation task of Wachsmuth et al. (2017a): (1) Reduce taxonomy complexity. While T VS P defined the task to score all 11 AQ subaspects (Local Acceptability, Local Relevance, etc.), 3 dimensions (Cogency, Effectiveness, Reasonableness), and overall AQ, we reduced the number of qualities scored by only focusing on the 3 higher-level dimensions plus overall AQ. As a result, annotators assessed a"
2020.argmining-1.13,2020.coling-main.402,1,0.564322,"retz et al. (2020)). In a small study, Wachsmuth et al. (2017a) demonstrate that theory-based AQ annotations can be done both by trained experts and by crowd annotators, though the authors acknowledge the high complexity and subjectivity of the problem and call for the simplification of theory-based AQ annotation in order to reliably create larger-scale corpora. To date, no work has tackled this challenge and accordingly, no larger-scale and no domain-diverse corpus of this kind exists. We aim to close this gap by describing our efforts to create Grammarly Argument Quality Corpus (GAQCorpus) (Lauscher et al., 2020), the largest and the only domain-diverse corpus consisting of 5,285 English arguments annotated with theory-based AQ scores across four dimensions. ∗ Equal contribution. This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 In the following, we adopt the term “theory-based AQ,” which was proposed by Wachsmuth et al. (2017b) to indicate that the conception of AQ is specifically grounded in argumentation theoretic literature (and not in CL or NLP). 117 Proceedings of the 7th Workshop on Argument Min"
2020.argmining-1.13,P13-1026,0,0.704809,"th crowdsourcing, formulating theory-based guidelines that helped make subjective judgments of AQ more objective. We demonstrate how to identify arguments and adapt the annotation task for three diverse domains. Our work will inform research on theory-based argumentation annotation and enable the creation of more diverse corpora to support computational AQ assessment. 1 Introduction The notion of Argumentation Quality (AQ) plays an important role in many existing argument-related downstream applications, such as argumentative writing support (Stab and Gurevych, 2017), automatic essay grading (Persing and Ng, 2013), and debate systems (Toledo et al., 2019). For some of these applications, the idea is to automatically give feedback to users to help them improve their writing skills or assess their writing capabilities. For others, assessing AQ is an important step in a more complex pipeline for retrieving high-quality arguments. While grading overall AQ (Toledo et al., 2019) or a specific conceptualization of AQ, such as prompt adherence (Persing and Ng, 2014) is relatively well explored, researchers have noted the lack of work in so-called theory-based AQ1 (Wachsmuth et al., 2017b), which can be represe"
2020.argmining-1.13,P14-1144,0,0.2947,"many existing argument-related downstream applications, such as argumentative writing support (Stab and Gurevych, 2017), automatic essay grading (Persing and Ng, 2013), and debate systems (Toledo et al., 2019). For some of these applications, the idea is to automatically give feedback to users to help them improve their writing skills or assess their writing capabilities. For others, assessing AQ is an important step in a more complex pipeline for retrieving high-quality arguments. While grading overall AQ (Toledo et al., 2019) or a specific conceptualization of AQ, such as prompt adherence (Persing and Ng, 2014) is relatively well explored, researchers have noted the lack of work in so-called theory-based AQ1 (Wachsmuth et al., 2017b), which can be represented with a taxonomy characterizing overall AQ into several subdimensions and aspects, for instance, as logic and rhetoric, which therefore provides a more informative and targeted perspective. However, this holistic approach comes with the downside of higher complexity, especially when it comes to annotating textual corpora, which are required for training and developing common computational approaches (see, e.g., Gretz et al. (2020)). In a small s"
2020.argmining-1.13,P15-1053,0,0.0874784,"nnotation task (§3), which is followed by a discussion of the data domains (§4). §5 presents an analysis of the resulting corpus. Finally, we conclude our work and provide directions for future research (§6). 2 Local acceptability Related Work Local relevance Most argumentation annotation studies have been Cogency Local sufficiency conducted on student essays or web debates. Student essays have been annotated for thesis clarArrangement ity (Persing and Ng, 2013), organization (PersAppropriateness ing et al., 2010), and prompt adherence (PersOverall Clarity Effectiveness ing and Ng, 2014), and Persing and Ng (2015) Argument Quality model argument strength rated on a 4-point Likert Credibility scale. Similarly, Stab and Gurevych (2016) annoEmotional appeal tate the absence of opposing arguments and Stab Global acceptability Reasonableness and Gurevych (2017) predict insufficient premise support in arguments. For web debates, HaberGlobal relevance nal and Gurevych (2016) conduct an annotation Global sufficiency study in which they present debate arguments pairwise to crowd annotators, who then can choose Figure 1: The taxonomy of theory-based argument the more convincing argument. Persing and Ng quality a"
2020.argmining-1.13,D10-1023,0,0.449265,"Missing"
2020.argmining-1.13,W16-2813,0,0.335655,"ng corpus. Finally, we conclude our work and provide directions for future research (§6). 2 Local acceptability Related Work Local relevance Most argumentation annotation studies have been Cogency Local sufficiency conducted on student essays or web debates. Student essays have been annotated for thesis clarArrangement ity (Persing and Ng, 2013), organization (PersAppropriateness ing et al., 2010), and prompt adherence (PersOverall Clarity Effectiveness ing and Ng, 2014), and Persing and Ng (2015) Argument Quality model argument strength rated on a 4-point Likert Credibility scale. Similarly, Stab and Gurevych (2016) annoEmotional appeal tate the absence of opposing arguments and Stab Global acceptability Reasonableness and Gurevych (2017) predict insufficient premise support in arguments. For web debates, HaberGlobal relevance nal and Gurevych (2016) conduct an annotation Global sufficiency study in which they present debate arguments pairwise to crowd annotators, who then can choose Figure 1: The taxonomy of theory-based argument the more convincing argument. Persing and Ng quality aspects (Wachsmuth et al., 2017b). (2017) also annotate the reasons why an argument receives a low persuasive power score."
2020.argmining-1.13,E17-1092,0,0.112422,"to reliably collect a large number of judgments with crowdsourcing, formulating theory-based guidelines that helped make subjective judgments of AQ more objective. We demonstrate how to identify arguments and adapt the annotation task for three diverse domains. Our work will inform research on theory-based argumentation annotation and enable the creation of more diverse corpora to support computational AQ assessment. 1 Introduction The notion of Argumentation Quality (AQ) plays an important role in many existing argument-related downstream applications, such as argumentative writing support (Stab and Gurevych, 2017), automatic essay grading (Persing and Ng, 2013), and debate systems (Toledo et al., 2019). For some of these applications, the idea is to automatically give feedback to users to help them improve their writing skills or assess their writing capabilities. For others, assessing AQ is an important step in a more complex pipeline for retrieving high-quality arguments. While grading overall AQ (Toledo et al., 2019) or a specific conceptualization of AQ, such as prompt adherence (Persing and Ng, 2014) is relatively well explored, researchers have noted the lack of work in so-called theory-based AQ1"
2020.argmining-1.13,D19-1564,0,0.259864,"guidelines that helped make subjective judgments of AQ more objective. We demonstrate how to identify arguments and adapt the annotation task for three diverse domains. Our work will inform research on theory-based argumentation annotation and enable the creation of more diverse corpora to support computational AQ assessment. 1 Introduction The notion of Argumentation Quality (AQ) plays an important role in many existing argument-related downstream applications, such as argumentative writing support (Stab and Gurevych, 2017), automatic essay grading (Persing and Ng, 2013), and debate systems (Toledo et al., 2019). For some of these applications, the idea is to automatically give feedback to users to help them improve their writing skills or assess their writing capabilities. For others, assessing AQ is an important step in a more complex pipeline for retrieving high-quality arguments. While grading overall AQ (Toledo et al., 2019) or a specific conceptualization of AQ, such as prompt adherence (Persing and Ng, 2014) is relatively well explored, researchers have noted the lack of work in so-called theory-based AQ1 (Wachsmuth et al., 2017b), which can be represented with a taxonomy characterizing overal"
2020.argmining-1.13,P17-2039,0,0.135456,"automatic essay grading (Persing and Ng, 2013), and debate systems (Toledo et al., 2019). For some of these applications, the idea is to automatically give feedback to users to help them improve their writing skills or assess their writing capabilities. For others, assessing AQ is an important step in a more complex pipeline for retrieving high-quality arguments. While grading overall AQ (Toledo et al., 2019) or a specific conceptualization of AQ, such as prompt adherence (Persing and Ng, 2014) is relatively well explored, researchers have noted the lack of work in so-called theory-based AQ1 (Wachsmuth et al., 2017b), which can be represented with a taxonomy characterizing overall AQ into several subdimensions and aspects, for instance, as logic and rhetoric, which therefore provides a more informative and targeted perspective. However, this holistic approach comes with the downside of higher complexity, especially when it comes to annotating textual corpora, which are required for training and developing common computational approaches (see, e.g., Gretz et al. (2020)). In a small study, Wachsmuth et al. (2017a) demonstrate that theory-based AQ annotations can be done both by trained experts and by crow"
2020.argmining-1.13,E17-1017,0,0.22181,"automatic essay grading (Persing and Ng, 2013), and debate systems (Toledo et al., 2019). For some of these applications, the idea is to automatically give feedback to users to help them improve their writing skills or assess their writing capabilities. For others, assessing AQ is an important step in a more complex pipeline for retrieving high-quality arguments. While grading overall AQ (Toledo et al., 2019) or a specific conceptualization of AQ, such as prompt adherence (Persing and Ng, 2014) is relatively well explored, researchers have noted the lack of work in so-called theory-based AQ1 (Wachsmuth et al., 2017b), which can be represented with a taxonomy characterizing overall AQ into several subdimensions and aspects, for instance, as logic and rhetoric, which therefore provides a more informative and targeted perspective. However, this holistic approach comes with the downside of higher complexity, especially when it comes to annotating textual corpora, which are required for training and developing common computational approaches (see, e.g., Gretz et al. (2020)). In a small study, Wachsmuth et al. (2017a) demonstrate that theory-based AQ annotations can be done both by trained experts and by crow"
2020.coling-main.180,D10-1049,0,0.0401963,"ion procedure where appositives are identified by locating instances of the appos dependency label (Nivre et al., 2020) in parsed text, and used it to build a dataset of appositives for PERSON entities in English news articles. The candidate appositives were cross-referenced with the WikiData knowledge base (Vrandeˇci´c and Kr¨otzsch, 2014) through word matching, and only those appositives were included in the final dataset which matched a fact from WikiData. More generally, appositive generation relates to work on joint fact selection and generation (Liang et al., 2009; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2013). 2.2 A shift in terminology Kang et al. (2019) actually called the phrases in question post-modifiers, rather than appositives. The linguistic term post-modifier can be seen as subsuming appositives, but it is much broader, including also prepositional, non-finite and dependent clauses that appear in postposition. Meanwhile, appositives come in two forms, nominal appositives, where a single noun identifies or qualifies another noun, e.g. President Washington, and explicative appositives, where a pronoun, an infinitive or a noun phrase is used to explain or specify t"
2020.coling-main.180,W14-3348,0,0.0485641,"en providing the model with three sentences of preceding context, one or zero had little impact on its performance, so all results reported below use one sentence of preceding context, following Kang et al. (2019). Further details on the implementations and the hyperparameters we used can be found in Appendix A.3. 6.2 Evaluation We follow Kang et al. (2019) in the choice of performance metrics for predictions over the positive instances in the data: we measure F1 score of the predicted bag-of-words excluding stopwords; BLEU (Papineni et al., 2002) over n-grams, where n = 1, 2, 3;8 and METEOR (Denkowski and Lavie, 2014), which supports stemming and synonymy only in English, Spanish and German, so these features are not used for Polish. We use accuracy to measure the models’ ability to determine when an appositive is due. 7 Results We view the results of our experiments from two angles: one concerns the expansion of the task definition we achieve with ApposCorpus, from a constrained scenario to an end-to-end one; the other concerns the increased coverage of the dataset, which allows us to compare and contrast appositive generation across different languages and named entity types. 7.1 Constrained v. end-to-en"
2020.coling-main.180,N19-1423,0,0.0688443,"Missing"
2020.coling-main.180,P16-1154,0,0.0323524,"w frequency for the remaining fact types (cf. has quality and capital of ), on the other hand, poses a challenge whose solution would require deeper natural language understanding and, possibly, explicit domain transfer techniques. 6 Experiments To show how the new task formulation can be used, we experiment with three established language generation methods: the main method of Kang et al. (2019), which we refer to as base; an extension of base with external knowledge injected through embeddings with knowledge-base grounding (KB); and a model enhanced with an explicit copy mechanism (copynet, Gu et al. (2016)). Notice that our goal here is not to build the best model for this task, but to develop reasonable models which can serve as baselines for future work in this area.7 6.1 Architectures LSTM baseline, base Kang et al. (2019) introduced an LSTM-based encoder-decoder architecture with an auxiliary objective used to guide the attention of the decoder towards the WikiData facts that were matched during the data collection process. Input sentences and facts are represented with the same word embeddings and encoded by separate biLSTMs. The decoder is initialized with the encoding of the input and at"
2020.coling-main.180,D18-1330,0,0.0201788,"Missing"
2020.coling-main.180,N19-1089,0,0.0730947,"es that appear next to a noun phrase and serve an explicative function (Bauer, 2017). Adding such explanations to text is a multi-step process. First, one has to decide whether an entity mention needs an appositive. That may not be the case for entities that are sufficiently well-known or that have been introduced earlier in the text. In case an appositive is indeed needed, the next step is to choose what information about the entity to disclose. If the information is to be of a factual nature, the writer needs to have prior knowledge of the entity, or access to an external knowledge resource–Kang et al. (2019) found appositives to be frequently based on facts of particular relevance to the context of the mention. Lastly, the surface form of the appositive, well-fitted to the surrounding context, needs to be produced. Viewed from the perspective of NLP, appositive generation is therefore an This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 1989 Proceedings of the 28th International Conference on Computational Linguistics, pages 1989–2003 Barcelona, Spain (Online), December 8-13, 2020 interesting and ch"
2020.coling-main.180,C10-2062,0,0.0761306,"Missing"
2020.coling-main.180,P09-1011,0,0.0195642,"on this task. They designed a data collection procedure where appositives are identified by locating instances of the appos dependency label (Nivre et al., 2020) in parsed text, and used it to build a dataset of appositives for PERSON entities in English news articles. The candidate appositives were cross-referenced with the WikiData knowledge base (Vrandeˇci´c and Kr¨otzsch, 2014) through word matching, and only those appositives were included in the final dataset which matched a fact from WikiData. More generally, appositive generation relates to work on joint fact selection and generation (Liang et al., 2009; Kim and Mooney, 2010; Angeli et al., 2010; Konstas and Lapata, 2013). 2.2 A shift in terminology Kang et al. (2019) actually called the phrases in question post-modifiers, rather than appositives. The linguistic term post-modifier can be seen as subsuming appositives, but it is much broader, including also prepositional, non-finite and dependent clauses that appear in postposition. Meanwhile, appositives come in two forms, nominal appositives, where a single noun identifies or qualifies another noun, e.g. President Washington, and explicative appositives, where a pronoun, an infinitive or a"
2020.coling-main.180,2020.lrec-1.497,0,0.0615007,"Missing"
2020.coling-main.180,P02-1040,0,0.108934,"y experiments with the base architecture showed that the choice between providing the model with three sentences of preceding context, one or zero had little impact on its performance, so all results reported below use one sentence of preceding context, following Kang et al. (2019). Further details on the implementations and the hyperparameters we used can be found in Appendix A.3. 6.2 Evaluation We follow Kang et al. (2019) in the choice of performance metrics for predictions over the positive instances in the data: we measure F1 score of the predicted bag-of-words excluding stopwords; BLEU (Papineni et al., 2002) over n-grams, where n = 1, 2, 3;8 and METEOR (Denkowski and Lavie, 2014), which supports stemming and synonymy only in English, Spanish and German, so these features are not used for Polish. We use accuracy to measure the models’ ability to determine when an appositive is due. 7 Results We view the results of our experiments from two angles: one concerns the expansion of the task definition we achieve with ApposCorpus, from a constrained scenario to an end-to-end one; the other concerns the increased coverage of the dataset, which allows us to compare and contrast appositive generation across"
2020.coling-main.180,2020.acl-demos.14,0,0.0197894,"e the quality of the outcome.5 Below, we describe in detail the data collection procedure. 3.1 Preprocessing We processed every article as follows: (1) tokenize the text and segment sentences; (2) normalize mentions of the entity in the article’s title and annotate them with internal links; (3) identify sentences which contain a linked named entity listed as an instance of type human or of (a subclass of) type organization in WikiData (corresponding to the PERSON and ORGANIZATION named entity types); (4) run a dependency parser on these sentences. Steps (1) and (4) were performed with Stanza (Qi et al., 2020). 3.2 Detecting appositives Any instance of the appos label that depends on a linked named entity and is separated from it with a comma or an opening parenthesis was considered a valid candidate. In this case, we recorded the source sentence, replacing the appositive with special token &lt;appos&gt;, as input data, and the appositive as a target. The beginning of the appositive was taken to be the first token after the comma or opening parenthesis, and the end is taken to be the last token before the next comma/semicolon/full stop (if beginning was marked by a comma) or closing parenthesis (if begin"
2020.coling-main.180,tiedemann-2012-parallel,0,0.0101132,"an added token &lt;appos&gt; just after the PERSON/ORGANIZATION entity, and the target is a special &lt;EMPTY&gt; token. The procedure described above was used to collect training data for factual appositive generation. As it is our goal to study the potential of a cross-domain approach to appositive generation, the ApposCorpus also contains an out-of-domain test set, sourced from newswire. 4 Dataset collection: News We sourced our data for cross-domain evaluation from the news domain, following previous work (Kang et al., 2019), using these news corpora: Global Voices (English, Spanish, German, Polish) (Tiedemann, 2012), News Commentary (English, Spanish, German) (Tiedemann, 2012) and Paralela (Pezik, 2016). 4.1 Entity linking Unlike Wikipedia, where entities are explicitly linked to WikiData entries through internal links, here, we had to perform additional entity linking. We did so in the following manner: (1) extract candidates from WikiData based on exact match between the full span of the named entity and all aliases of entities in the respective subset of WikiData (instances of type human if NER label is PERSON , else organization), (2) obtain relative alias frequency distributions from Wikipedia, and"
2020.coling-main.180,Q17-1028,0,0.0612005,"Missing"
2020.coling-main.402,L16-1704,0,0.0755014,"ome categories contain a relatively high proportion of argumentative posts, like Politics & Government → Law & Ethics, from which we select posts. We only include posts marked as best answer for a question and exclude posts containing uniform resource identifiers or media content. From these, we select posts that were labeled as argumentative by a majority of 10 raters in a secondary experiment (see Appendix). Debates. To reflect online debate forums-style argumentation, we include 1,337 arguments from Change My View (CMV) (Tan et al., 2016) and 766 from the Internet Argument corpus V2 (IAC) (Abbott et al., 2016) resulting in 2,103 arguments in total. CMV is an internet forum in which users post their opinion and ask others to challenge their beliefs on the topic. The IAC is composed of posts retrieved from three online debate forums, and in this study we include only arguments from the ConvinceMe subset. We try to restrict the sample to instances that do not require much background knowledge or thread-level context. From CMV, we include original posts only and for ConvinceMe, we include the first post reacting to the topic. From CMV we also exclude posts tagged [MOD], which indicate moderator posts."
2020.coling-main.402,Q17-1010,0,0.00989535,"Missing"
2020.coling-main.402,N19-1423,0,0.0451899,"xploited. In addition to simple single-task learning approaches, we study the effect of jointly predicting AQ dimensions in two variants (flat vs. hierarchical) and find that combining the training signals of all four aspects benefits theory-based AQ assessment. RQ4: Does the corpus support training a single unified model for multi-domain evaluation? When enough data from a single domain is available, training on in-domain data is typically preferred over multi-domain. However, larger amounts of data are especially useful for complex model architectures currently prominent in NLP (e.g., BERT (Devlin et al., 2019), GPT2 (Radford et al., 2019)). We study these two mutually opposing effects on GAQCorpus and show that our corpus supports training a single unified model across all three domains, with improved performances in individual domains. RQ5: Can we empirically substantiate the idea that theory-based and practical AQ assessment can learn from each other? Wachsmuth et al. (2017a) suggest that both the practical and the theory-based views can learn from each other, but so far, this has been only tested manually. Employing our models, we go one step further and conduct a bi-directional experiment emplo"
2020.coling-main.402,N18-1094,0,0.0141347,"in arguments. Another well-studied domain is web debates. Wachsmuth et al. (2017c) adapt PageRank to identify argument relevance. Pairwise comparison of the convincingness of debate arguments has been conducted (Habernal and Gurevych, 2016). Persing and Ng (2017) additionally predict why an argument receives a low persuasive power score. By explaining flaws in argumentation, they highlight the importance of explainability and specific author feedback. Other approaches take into account properties of the source, i.e., the author (Durmus and Cardie, 2019) or the audience (El Baff et al., 2018; Durmus and Cardie, 2018). In contrast, we assume that a system may not have much knowledge about the authors or audience and thus our models operate solely on the text. Toledo et al. (2019) and Gretz et al. (2020) present large corpora of crowd-sourced arguments and their quality. These corpora cover a variety of topics, but only within single domains. The authors emphasize 4564 Title: Should ‘blogging’ be a capital crime? Iran is considering it... Stance: A government has the right to censor speech (...) Overall Argument Quality Text: My government doesn’t give me freedom of speech, so I have to argue for this side."
2020.coling-main.402,K18-1044,0,0.0288156,"Missing"
2020.coling-main.402,P16-1150,0,0.682301,"t features of everyday writing. These are components of Argument Quality (AQ), which has been studied in many domains, such as student essays (Wachsmuth et al., 2016), news editorials (El Baff et al., 2018), and debate forums (Lukin et al., 2017). Preceding work in natural language processing (NLP) and computational linguistics (CL) has mostly focused on practical AQ assessment1 , considering either the overall quality of arguments (Toledo et al., 2019; Gretz et al., 2020, inter alia) or a single specific conceptualization of AQ, e.g., argument strength (Persing and Ng, 2015), convincingness (Habernal and Gurevych, 2016), and relevance (Wachsmuth et al., 2017c). However, Gretz et al. (2020) note the need to predict quality in terms of fine-grained aspects. Fine-grained prediction enables a deeper understanding of argumentation and offers specific feedback to authors aiming to improve their argumentative writing skills. For instance, authors might want to know whether their premises are sufficient with regard to their claim(s) or whether their language is appropriate. Wachsmuth et al. (2017b) surveyed and synthesized theory-based dimensions of AQ into a taxonomy of three main dimensions: Cogency (Logic), Effec"
2020.coling-main.402,E17-1070,0,0.149611,"e work. We demonstrate the feasibility of large-scale AQ annotation, show that exploiting relations between dimensions yields performance improvements, and explore the synergies between theory-based prediction and practical AQ assessment. 1 Introduction Providing relevant and sufficient justifications for a claim and using clear language to express reasoning are important features of everyday writing. These are components of Argument Quality (AQ), which has been studied in many domains, such as student essays (Wachsmuth et al., 2016), news editorials (El Baff et al., 2018), and debate forums (Lukin et al., 2017). Preceding work in natural language processing (NLP) and computational linguistics (CL) has mostly focused on practical AQ assessment1 , considering either the overall quality of arguments (Toledo et al., 2019; Gretz et al., 2020, inter alia) or a single specific conceptualization of AQ, e.g., argument strength (Persing and Ng, 2015), convincingness (Habernal and Gurevych, 2016), and relevance (Wachsmuth et al., 2017c). However, Gretz et al. (2020) note the need to predict quality in terms of fine-grained aspects. Fine-grained prediction enables a deeper understanding of argumentation and off"
2020.coling-main.402,2020.argmining-1.13,1,0.421537,"he scheme to Overall AQ and the three higher level dimensions and represent the finer-grained sub-dimensions as questions to guide the annotators’ judgments. We additionally use a five-point scale (very low, low, medium, high, very high, plus “cannot judge”), which simplifies the task according to feedback from our expert annotators and previous findings (Cox III, 1980). We experimented with both the five-point and the original three-point rating scale (low, medium, high) used by Wachsmuth et al. (2017b), and found that switching the scales did not negatively affect inter-annotator agreement. Ng et al. (2020) describe the annotation design and guidelines in more detail. 3.2 Data We investigate different domains to obtain a deeper understanding of real-world AQ and the feasibility of the annotation scheme in different settings. We include three domains in our study: Community questions and answers forum posts (CQA), debate forum posts (Debates), and business review forum posts (Reviews). Figures 2 and 3 display an example text for each domain. CQA. We include 2,088 arguments from Yahoo! Answers,2 a community questions and answers forum where users ask questions and answer questions posted by others"
2020.coling-main.402,P13-1026,0,0.339048,"y-based approaches. Practical approaches. Recently, the field of computational AQ research has been mostly driven by practical approaches that each target an individual domain. Accordingly, past approaches tackle either overall quality (Toledo et al., 2019) or specific subqualities of argumentation, such as convincingness (Habernal and Gurevych, 2016) and relevance (Wachsmuth et al., 2017c). The popularity of practical approaches can partly be attributed to the relative simplicity of crowd-sourcing annotations. Much prior work has focused on aspects of student essays, including essay clarity (Persing and Ng, 2013), organization (Persing et al., 2010), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). Later, Wachsmuth et al. (2016) present an approach driven by detecting argumentative units, thereby demonstrating the usefulness of argument mining techniques to the problem. Similarly, Stab and Gurevych (2016) predict the absence of opposing arguments and Stab and Gurevych (2017) predict insufficient premise support in arguments. Another well-studied domain is web debates. Wachsmuth et al. (2017c) adapt PageRank to identify argument relevance. Pairwise comparison of th"
2020.coling-main.402,P14-1144,0,0.0957402,"AQ research has been mostly driven by practical approaches that each target an individual domain. Accordingly, past approaches tackle either overall quality (Toledo et al., 2019) or specific subqualities of argumentation, such as convincingness (Habernal and Gurevych, 2016) and relevance (Wachsmuth et al., 2017c). The popularity of practical approaches can partly be attributed to the relative simplicity of crowd-sourcing annotations. Much prior work has focused on aspects of student essays, including essay clarity (Persing and Ng, 2013), organization (Persing et al., 2010), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). Later, Wachsmuth et al. (2016) present an approach driven by detecting argumentative units, thereby demonstrating the usefulness of argument mining techniques to the problem. Similarly, Stab and Gurevych (2016) predict the absence of opposing arguments and Stab and Gurevych (2017) predict insufficient premise support in arguments. Another well-studied domain is web debates. Wachsmuth et al. (2017c) adapt PageRank to identify argument relevance. Pairwise comparison of the convincingness of debate arguments has been conducted (Habernal and Gurevych"
2020.coling-main.402,P15-1053,0,0.364623,"guage to express reasoning are important features of everyday writing. These are components of Argument Quality (AQ), which has been studied in many domains, such as student essays (Wachsmuth et al., 2016), news editorials (El Baff et al., 2018), and debate forums (Lukin et al., 2017). Preceding work in natural language processing (NLP) and computational linguistics (CL) has mostly focused on practical AQ assessment1 , considering either the overall quality of arguments (Toledo et al., 2019; Gretz et al., 2020, inter alia) or a single specific conceptualization of AQ, e.g., argument strength (Persing and Ng, 2015), convincingness (Habernal and Gurevych, 2016), and relevance (Wachsmuth et al., 2017c). However, Gretz et al. (2020) note the need to predict quality in terms of fine-grained aspects. Fine-grained prediction enables a deeper understanding of argumentation and offers specific feedback to authors aiming to improve their argumentative writing skills. For instance, authors might want to know whether their premises are sufficient with regard to their claim(s) or whether their language is appropriate. Wachsmuth et al. (2017b) surveyed and synthesized theory-based dimensions of AQ into a taxonomy of"
2020.coling-main.402,D10-1023,0,0.127785,"hes. Recently, the field of computational AQ research has been mostly driven by practical approaches that each target an individual domain. Accordingly, past approaches tackle either overall quality (Toledo et al., 2019) or specific subqualities of argumentation, such as convincingness (Habernal and Gurevych, 2016) and relevance (Wachsmuth et al., 2017c). The popularity of practical approaches can partly be attributed to the relative simplicity of crowd-sourcing annotations. Much prior work has focused on aspects of student essays, including essay clarity (Persing and Ng, 2013), organization (Persing et al., 2010), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). Later, Wachsmuth et al. (2016) present an approach driven by detecting argumentative units, thereby demonstrating the usefulness of argument mining techniques to the problem. Similarly, Stab and Gurevych (2016) predict the absence of opposing arguments and Stab and Gurevych (2017) predict insufficient premise support in arguments. Another well-studied domain is web debates. Wachsmuth et al. (2017c) adapt PageRank to identify argument relevance. Pairwise comparison of the convincingness of debate arguments"
2020.coling-main.402,W16-2813,0,0.17304,"and Gurevych, 2016) and relevance (Wachsmuth et al., 2017c). The popularity of practical approaches can partly be attributed to the relative simplicity of crowd-sourcing annotations. Much prior work has focused on aspects of student essays, including essay clarity (Persing and Ng, 2013), organization (Persing et al., 2010), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). Later, Wachsmuth et al. (2016) present an approach driven by detecting argumentative units, thereby demonstrating the usefulness of argument mining techniques to the problem. Similarly, Stab and Gurevych (2016) predict the absence of opposing arguments and Stab and Gurevych (2017) predict insufficient premise support in arguments. Another well-studied domain is web debates. Wachsmuth et al. (2017c) adapt PageRank to identify argument relevance. Pairwise comparison of the convincingness of debate arguments has been conducted (Habernal and Gurevych, 2016). Persing and Ng (2017) additionally predict why an argument receives a low persuasive power score. By explaining flaws in argumentation, they highlight the importance of explainability and specific author feedback. Other approaches take into account"
2020.coling-main.402,E17-1092,0,0.110648,"arity of practical approaches can partly be attributed to the relative simplicity of crowd-sourcing annotations. Much prior work has focused on aspects of student essays, including essay clarity (Persing and Ng, 2013), organization (Persing et al., 2010), prompt adherence (Persing and Ng, 2014), and argument strength (Persing and Ng, 2015). Later, Wachsmuth et al. (2016) present an approach driven by detecting argumentative units, thereby demonstrating the usefulness of argument mining techniques to the problem. Similarly, Stab and Gurevych (2016) predict the absence of opposing arguments and Stab and Gurevych (2017) predict insufficient premise support in arguments. Another well-studied domain is web debates. Wachsmuth et al. (2017c) adapt PageRank to identify argument relevance. Pairwise comparison of the convincingness of debate arguments has been conducted (Habernal and Gurevych, 2016). Persing and Ng (2017) additionally predict why an argument receives a low persuasive power score. By explaining flaws in argumentation, they highlight the importance of explainability and specific author feedback. Other approaches take into account properties of the source, i.e., the author (Durmus and Cardie, 2019) or"
2020.coling-main.402,D19-1564,0,0.224459,"and practical AQ assessment. 1 Introduction Providing relevant and sufficient justifications for a claim and using clear language to express reasoning are important features of everyday writing. These are components of Argument Quality (AQ), which has been studied in many domains, such as student essays (Wachsmuth et al., 2016), news editorials (El Baff et al., 2018), and debate forums (Lukin et al., 2017). Preceding work in natural language processing (NLP) and computational linguistics (CL) has mostly focused on practical AQ assessment1 , considering either the overall quality of arguments (Toledo et al., 2019; Gretz et al., 2020, inter alia) or a single specific conceptualization of AQ, e.g., argument strength (Persing and Ng, 2015), convincingness (Habernal and Gurevych, 2016), and relevance (Wachsmuth et al., 2017c). However, Gretz et al. (2020) note the need to predict quality in terms of fine-grained aspects. Fine-grained prediction enables a deeper understanding of argumentation and offers specific feedback to authors aiming to improve their argumentative writing skills. For instance, authors might want to know whether their premises are sufficient with regard to their claim(s) or whether the"
2020.coling-main.402,D15-1072,0,0.0133084,"clude original posts only and for ConvinceMe, we include the first post reacting to the topic. From CMV we also exclude posts tagged [MOD], which indicate moderator posts. Reviews. Yelp is an online platform where users publish business reviews and rate their experience from 1 (poor) to 5 (excellent) stars. From the Yelp-Challenge-Dataset3 , we sampled 1,104 arguments reviewing restaurants. While the review texts often do not appear as “classic” arguments, i.e., with a dedicated claim and premises supporting this claim, the texts can indeed be considered argumentative (Wachsmuth et al., 2014; Wachsmuth et al., 2015); The star rating corresponds to a claim a user is making about the business and the review text is intended to support this claim. For Debate and Review posts, we include the star rating and stance (if provided) with the text. Across all domains, we filter for posts with text length between 70 and 200 words. To ensure high quality annotations, we first ran 13 pilot studies in two flavors: (1) with three of the linguistic expert annotators (§3.1), and (2) with a crowd-sourced workforce of 24 contributors from Appen.4 Both groups used the same annotation guidelines and interface, which we itera"
2020.coling-main.402,C16-1158,0,0.575084,"approaches to theory-based assessment, which can serve as strong baselines for future work. We demonstrate the feasibility of large-scale AQ annotation, show that exploiting relations between dimensions yields performance improvements, and explore the synergies between theory-based prediction and practical AQ assessment. 1 Introduction Providing relevant and sufficient justifications for a claim and using clear language to express reasoning are important features of everyday writing. These are components of Argument Quality (AQ), which has been studied in many domains, such as student essays (Wachsmuth et al., 2016), news editorials (El Baff et al., 2018), and debate forums (Lukin et al., 2017). Preceding work in natural language processing (NLP) and computational linguistics (CL) has mostly focused on practical AQ assessment1 , considering either the overall quality of arguments (Toledo et al., 2019; Gretz et al., 2020, inter alia) or a single specific conceptualization of AQ, e.g., argument strength (Persing and Ng, 2015), convincingness (Habernal and Gurevych, 2016), and relevance (Wachsmuth et al., 2017c). However, Gretz et al. (2020) note the need to predict quality in terms of fine-grained aspects."
2020.coling-main.402,P17-2039,0,0.122137,"ponents of Argument Quality (AQ), which has been studied in many domains, such as student essays (Wachsmuth et al., 2016), news editorials (El Baff et al., 2018), and debate forums (Lukin et al., 2017). Preceding work in natural language processing (NLP) and computational linguistics (CL) has mostly focused on practical AQ assessment1 , considering either the overall quality of arguments (Toledo et al., 2019; Gretz et al., 2020, inter alia) or a single specific conceptualization of AQ, e.g., argument strength (Persing and Ng, 2015), convincingness (Habernal and Gurevych, 2016), and relevance (Wachsmuth et al., 2017c). However, Gretz et al. (2020) note the need to predict quality in terms of fine-grained aspects. Fine-grained prediction enables a deeper understanding of argumentation and offers specific feedback to authors aiming to improve their argumentative writing skills. For instance, authors might want to know whether their premises are sufficient with regard to their claim(s) or whether their language is appropriate. Wachsmuth et al. (2017b) surveyed and synthesized theory-based dimensions of AQ into a taxonomy of three main dimensions: Cogency (Logic), Effectiveness (Rhetoric), and Reasonableness"
2020.coling-main.402,E17-1017,0,0.11995,"ponents of Argument Quality (AQ), which has been studied in many domains, such as student essays (Wachsmuth et al., 2016), news editorials (El Baff et al., 2018), and debate forums (Lukin et al., 2017). Preceding work in natural language processing (NLP) and computational linguistics (CL) has mostly focused on practical AQ assessment1 , considering either the overall quality of arguments (Toledo et al., 2019; Gretz et al., 2020, inter alia) or a single specific conceptualization of AQ, e.g., argument strength (Persing and Ng, 2015), convincingness (Habernal and Gurevych, 2016), and relevance (Wachsmuth et al., 2017c). However, Gretz et al. (2020) note the need to predict quality in terms of fine-grained aspects. Fine-grained prediction enables a deeper understanding of argumentation and offers specific feedback to authors aiming to improve their argumentative writing skills. For instance, authors might want to know whether their premises are sufficient with regard to their claim(s) or whether their language is appropriate. Wachsmuth et al. (2017b) surveyed and synthesized theory-based dimensions of AQ into a taxonomy of three main dimensions: Cogency (Logic), Effectiveness (Rhetoric), and Reasonableness"
2020.coling-main.402,E17-1105,0,0.0610078,"ponents of Argument Quality (AQ), which has been studied in many domains, such as student essays (Wachsmuth et al., 2016), news editorials (El Baff et al., 2018), and debate forums (Lukin et al., 2017). Preceding work in natural language processing (NLP) and computational linguistics (CL) has mostly focused on practical AQ assessment1 , considering either the overall quality of arguments (Toledo et al., 2019; Gretz et al., 2020, inter alia) or a single specific conceptualization of AQ, e.g., argument strength (Persing and Ng, 2015), convincingness (Habernal and Gurevych, 2016), and relevance (Wachsmuth et al., 2017c). However, Gretz et al. (2020) note the need to predict quality in terms of fine-grained aspects. Fine-grained prediction enables a deeper understanding of argumentation and offers specific feedback to authors aiming to improve their argumentative writing skills. For instance, authors might want to know whether their premises are sufficient with regard to their claim(s) or whether their language is appropriate. Wachsmuth et al. (2017b) surveyed and synthesized theory-based dimensions of AQ into a taxonomy of three main dimensions: Cogency (Logic), Effectiveness (Rhetoric), and Reasonableness"
2021.emnlp-main.100,S16-1081,0,0.0946627,"this dimension is performed using a wider spectrum of approaches, as presented in the third column of Table 1. The most frequently used metric is referenceBLEU (r- BLEU ), which is based on the n-gram precision of the system output compared to human rewrites of the desired formality. Other approaches include self-BLEU (s-BLEU), where the system output is compared to its input, measuring the semantic similarity between the system input and its output, or regression models (e.g., CNN, BERT ) trained on data annotated for similaritybased tasks, such as the Semantic Textual Similarity task (STS) (Agirre et al., 2016). while similar resources are not available for other languages. Different model architectures have been used by prior work (e.g., CNN, LSTM, GRU, finetuning on pre-trained language models such as RoBERTa and BERT; Table 1). In most papers, Fluency Fluency is typically evaluated with the resulting classifier is evaluated on the test side model-based approaches (see fourth column of Taof the GYAFC corpus, reporting accuracy scores ble 1). Among those, the most frequent method is 1323 that of computing perplexity (PPL) under a language model. The latter is either trained from scratch on the same"
2021.emnlp-main.100,2020.findings-emnlp.263,0,0.0349961,"Missing"
2021.emnlp-main.100,2021.eacl-main.202,0,0.035073,"ee dimensions, or one could develop a singular metric. In line with Briakou et al. (2021a), our study also calls for releasing more human evaluations and more system outputs to enable robust evaluation. Finally, there is still room for improvement in assessing how fluent a rewrite is. Our study provides a framework to address these questions systematically and calls for ST papers to standardize and release data to support larger-scale evaluations. Conclusions Automatic (and human) evaluation processes are well-known problems for the field of Natural Language Generation (Howcroft et al., 2020; Clinciu et al., 2021) and the burgeoning subfield of ST is not immune. ST, in particular, has suffered from a lack of standardization of automatic metrics, a lack of agreement between human judgments and automatics metrics, as well as a blindspot to developing metrics for languages other than English. We address these issues by conducting the first controlled multilingual evaluation for leading ST metrics with a focus on formality, covering metrics for 3 evaluation dimensions and overall ranking for 4 languages. Given our findings, we recommend the formality style transfer community adopt the following best practi"
2021.emnlp-main.100,W11-2123,0,0.0252155,"), METEOR (Banerjee and Lavie, 2005) based on the harmonic mean of unigram precision and recall while accounting for synonym matches, and chrF (Popovi´c, 2015) based on the character n-gram F-score; 3. semantic textual similarity (STS) models constitute supervised methods that we model via fine-tuning multilingual pre-trained language models (i.e., XLM - R, mBERT) to predict a semantic similarity score for a pair of texts on an ordinal scale. Fluency We experiment with perplexity (PPL) and likelihood (LL) scores based on probability scores of language models trained from scratch (e.g., KenLM (Heafield, 2011)), as well as pseudolikelihood scores (PSEUDO - LL) extracted from pre-trained masked language models similarly to Salazar et al. (2020), by masking sentence tokens one by one. 4 Experiment Settings Supervised Metrics For all supervised modelbased approaches, we experiment with fine-tuning two multilingual pre-trained language models: 1. multilingual BERT, dubbed mBERT (Devlin et al., 2019)—a transformer-based model pretrained with a masked language model objective on the concatenation of monolingual Wikipedia corpora from the 104 languages with the largest Wikipedias. 2. XLM - R (Conneau et a"
2021.emnlp-main.100,P14-2029,1,0.912318,"uency Fluency is typically evaluated with the resulting classifier is evaluated on the test side model-based approaches (see fourth column of Taof the GYAFC corpus, reporting accuracy scores ble 1). Among those, the most frequent method is 1323 that of computing perplexity (PPL) under a language model. The latter is either trained from scratch on the same corpus used to train the FoST models (i.e., GYAFC) using different underlying architectures (e.g., KenLM, LSTM), or employ large pre-trained language models (e.g., GPT). A few other works train models on EN data annotated for grammaticality (Heilman et al., 2014) or linguistic acceptability (Warstadt et al., 2019) instead. Overall Systems’ overall quality (see fifth column of Table 2) is mostly evaluated using r-BLEU or by combining independently computed metrics into a single score (e.g., geometric mean - GM(.), harmonic mean - HM(.), F1(.)). Moreover, 6 out of 8 approaches that rely on combined scores do not include fluency scores in their overall evaluation. English Focus Since most of the current work on FoST and ST is in EN, prior work relies heavily on EN resources for designing automatic evaluation methods. For instance, resources for training"
2021.emnlp-main.100,2020.inlg-1.23,0,0.0721364,"Missing"
2021.emnlp-main.100,P19-1607,0,0.0434717,"Missing"
2021.emnlp-main.100,2020.emnlp-main.55,0,0.0747529,"Missing"
2021.emnlp-main.100,D19-1325,0,0.0464824,"Missing"
2021.emnlp-main.100,2021.naacl-main.337,0,0.0625398,"Missing"
2021.emnlp-main.100,2021.naacl-main.171,0,0.0678424,"Missing"
2021.emnlp-main.100,2020.wmt-1.77,0,0.027738,"imension, along and is now confirmed in a multilingual setting. with leading metrics from the other dimensions: This trend might be explained by chrF’s ability to XLM - R formality regression models, chr F and XLM match spelling errors within words via character n- R pseudo-likelihood. r- BLEU gets 30 out of 40 grams. XLM - R trained on STS with zero-shot trans- comparisons correct while the other metrics get 1328 25, 22, and 19 respectively. This indicates that r- BLEU correlates with human judgments better at the corpus-level than at the sentence-level, as in machine translation evaluation (Mathur et al., 2020). We caution that these results are not definitive but rather suggestive of the best performing metric, given the ideal evaluation would be a larger number of systems with which to perform a rank correlation. The complete analysis for each language is in Appendix B. 4. System-level Ranking chrF and XLM - R are the best metrics using a pairwise comparison evaluation. However, an ideal evaluation would be to have a large number of systems with which to draw reliable correlations. 6 We view this work as a strong point of departure for future investigations of ST evaluation. Our work first calls f"
2021.emnlp-main.100,N19-1049,0,0.0345068,"Missing"
2021.emnlp-main.100,D19-5557,0,0.0211767,"treats it as a regression task to better mirror human evaluation. • Our analysis code and meta-evaluation files with system outputs are made public to facilitate further work in developing better automatic metrics for ST: https://github.com/ Elbria/xformal-FoST-meta. 2 2.1 Background Limitations of Automatic Evaluation Recent work highlights the need for research to improve evaluation practices for ST along multiple directions. Not only does ST lack standardized evaluation practices (Yamshchikov et al., 2021), but commonly used methods have major drawbacks which hamper progress in this field. Pang (2019) and Pang and Gimpel (2019) show that the most widely adopted automatic metric, BLEU, can be gamed. They observe that untransferred text achieves the highest BLEU score for the task of sentiment transfer, questioning complex models’ ability to surpass this trivial baseline. Mir et al. (2019) discuss the inherent trade-off between ST evaluation aspects and propose that models are evaluated at specific points of their trade-off plots. Tikhonov et al. (2019) argue that, despite their cost, human-written references are needed for future experiments with style transfer. They also show that comparin"
2021.emnlp-main.100,L16-1147,0,0.0329103,"stics on the training data used for supervised and unsupervised models across the 3 ST evaluation aspects. For datasets that are only available for EN, we use the already available machine translated resources for STS 10 and formality datasets (Briakou et al., 2021b). The former employs the DeepL service (no information of translation quality is available) while the latter uses the AWS translation service11 (with reported BLEU scores of 37.16 (BR - 33.79 (FR), and 32.67 (IT)).12 The KenLM models for all the languages are trained on 1M randomly sampled sentences from the OpenSubtitles dataset (Lison and Tiedemann, 2016). PT ), 5 Experimental Results We analyze the results of comparing the outputs from the several automatic metrics to their humangenerated counterparts for formality style transfer (§5.1), meaning preservation (§5.2), fluency (§5.3) via conducting segment-level analysis—and then, turn into analyzing system-level rankings to evaluation overall task success (§5.4). 5.1 Formality Transfer Metrics The field is divided on the best way to evaluate the style dimension – formality in our case. Practitioners use either a binary approach (is the new sentence formal or informal?) or a regression approach"
2021.emnlp-main.100,P02-1040,0,0.122269,"FoST evaluation aspects described below, we cover a broad spectrum of approaches that range from dedicated models for the tasks at hand to more lightweight methods relying on unsupervised approaches and automated metrics. Formality We benchmark model-based approaches that fine-tune multilingual pre-trained language models (i.e., XLM - R, mBERT), where the task of formality detection is modeled either as a binary classification task (i.e., formal vs. informal), or as a regression task that predicts different formality levels on an ordinal scale. Meaning Preservation We evaluate the BLEU score (Papineni et al., 2002) of the system output compared to the reference rewrite (r-BLEU) since it is the dominant metric in prior work. Prior reviews of meaning preservation metrics for paraphrase and sentiment ST tasks in EN (Yamshchikov et al., 2021) cover n-gram metrics and embeddingbased approaches. We consider three additional metric classes to compare system outputs with inputs, as human annotators do: 1. n-gram based metrics include: s-BLEU (selfBLEU that compares system outputs with their inputs as opposed to references, i.e., r-BLEU), METEOR (Banerjee and Lavie, 2005) based on the harmonic mean of unigram pr"
2021.emnlp-main.100,Q16-1005,1,0.888573,"Missing"
2021.emnlp-main.100,D14-1162,0,0.0860249,"dam optimizer (Kingma and Ba, 2015), a batch size of 32, and a learning rate of 5e−5 for 3 and 5 epochs for classification and regression tasks, respectively. We perform a grid search on held-out validation sets over learning rate with values: 2e−3, 2e−4, 2e−5, and 5e−5 and over number of epochs with values: 3, 5, and 8. 2. embedding-based methods fall under the category of unsupervised evaluation approaches that rely on either contextual word representations extracted from pre-trained language models or non-contextual pre-trained word embeddings (e.g., word2vec (Mikolov et al., 2013); Glove (Pennington et al., 2014)). For the former, we use BERT-score (Zhang et al., 2020a) which computes the similarity Cross-lingual Transfer For supervised modelbetween each output token and each reference based methods that rely on the availability of token based on BERT contextual embeddings. human-annotated instances to train dedicated modFor the latter, we experiment with two simiels for specific tasks, we experiment with three larity metrics: the first is the cosine distance between the sentence-level feature represen- standard cross-lingual transfer approaches (e.g., Hu et al. (2020)): 1. ZERO - SHOT trains a single"
2021.emnlp-main.100,W15-3049,0,0.0734115,"Missing"
2021.emnlp-main.100,W18-6319,0,0.0164979,"Details on training data used for model-based metrics across the three ST evaluation aspects. on the EN training data and evaluates it on the original test data for each language; 2. TRANSLATE TRAIN uses machine translation ( MT ) to obtain training data in each language through translating the EN training set—and trains independent systems for each language; 3. TRANSLATE - TEST trains a single model on the EN training data and evaluates it on the test data that are translated into EN using MT . Unsupervised Metrics For meaning preservation metrics, we use the open-sourced implementations of: Post (2018) for BLEU (Papineni et al., 2002); Banerjee and Lavie (2005) for METEOR; Popovi´c (2015) for chrF.3,4,5 For BERT-score we use the implementation of Zhang et al. (2020a);6 non-contextualized embeddings-based approaches are based on fastText pre-trained embeddings.7 For fluency metrics, we use the implementation of Salazar et al. (2020) for computing pseudolikelihood.8 PPL and LL scores are extracted from a 5-gram KenLM model (Heafield, 2011).9 Training Data Table 3 presents statistics on the training data used for supervised and unsupervised models across the 3 ST evaluation aspects. For datase"
2021.emnlp-main.100,N18-1012,1,0.912261,"esses are, and how they might generalize to languages other than English. 2.3 Automatic Metrics for FoST Formality Style transfer is often evaluated using model-based approaches. The most frequent method consists of training a binary classifier on human written formal vs. informal pairs. The classifier is later used to predict the percentage of generated outputs that match the desired attribute per evaluated system—the system with the highest percentage is considered the best performing with respect to style. Across methods, the corpus used to train the classifier is the GYAFC parallelcorpus (Rao and Tetreault, 2018) consisting of 1 The complete list is hosted at: https://github. 105K parallel informal-formal human-generated com/fuzhenxin/Style-Transfer-in-Text excerpts. This corpus is curated for FoST in EN, 1322 We systematically review automatic evaluation practices in ST with formality as a case study. We select FoST for this work since it is one of the most frequently studied styles (Jin et al., 2020) and there is human annotated data including human references available for these evaluations (Rao and Tetreault, 2018; Briakou et al., 2021b). Tables 1 and 2 summarize evaluation details for all FoST me"
2021.emnlp-main.100,2020.acl-main.240,0,0.119895,"es, and chrF (Popovi´c, 2015) based on the character n-gram F-score; 3. semantic textual similarity (STS) models constitute supervised methods that we model via fine-tuning multilingual pre-trained language models (i.e., XLM - R, mBERT) to predict a semantic similarity score for a pair of texts on an ordinal scale. Fluency We experiment with perplexity (PPL) and likelihood (LL) scores based on probability scores of language models trained from scratch (e.g., KenLM (Heafield, 2011)), as well as pseudolikelihood scores (PSEUDO - LL) extracted from pre-trained masked language models similarly to Salazar et al. (2020), by masking sentence tokens one by one. 4 Experiment Settings Supervised Metrics For all supervised modelbased approaches, we experiment with fine-tuning two multilingual pre-trained language models: 1. multilingual BERT, dubbed mBERT (Devlin et al., 2019)—a transformer-based model pretrained with a masked language model objective on the concatenation of monolingual Wikipedia corpora from the 104 languages with the largest Wikipedias. 2. XLM - R (Conneau et al., 2020)—a transformer-based masked language model trained on 100 languages using monolingual CommonCrawl data. All models are based on"
2021.emnlp-main.100,D19-1499,0,0.0385034,"Missing"
2021.emnlp-main.100,D19-1365,0,0.034082,"Missing"
2021.emnlp-main.100,2020.coling-main.203,0,0.0808187,"Missing"
2021.emnlp-main.100,Q19-1040,0,0.0143541,"ting classifier is evaluated on the test side model-based approaches (see fourth column of Taof the GYAFC corpus, reporting accuracy scores ble 1). Among those, the most frequent method is 1323 that of computing perplexity (PPL) under a language model. The latter is either trained from scratch on the same corpus used to train the FoST models (i.e., GYAFC) using different underlying architectures (e.g., KenLM, LSTM), or employ large pre-trained language models (e.g., GPT). A few other works train models on EN data annotated for grammaticality (Heilman et al., 2014) or linguistic acceptability (Warstadt et al., 2019) instead. Overall Systems’ overall quality (see fifth column of Table 2) is mostly evaluated using r-BLEU or by combining independently computed metrics into a single score (e.g., geometric mean - GM(.), harmonic mean - HM(.), F1(.)). Moreover, 6 out of 8 approaches that rely on combined scores do not include fluency scores in their overall evaluation. English Focus Since most of the current work on FoST and ST is in EN, prior work relies heavily on EN resources for designing automatic evaluation methods. For instance, resources for training stylistic classifiers or regression models are not a"
2021.emnlp-main.100,2020.emnlp-demos.6,0,0.0279677,"Missing"
2021.emnlp-main.100,2020.acl-main.294,0,0.106522,"learning rate of 5e−5 for 3 and 5 epochs for classification and regression tasks, respectively. We perform a grid search on held-out validation sets over learning rate with values: 2e−3, 2e−4, 2e−5, and 5e−5 and over number of epochs with values: 3, 5, and 8. 2. embedding-based methods fall under the category of unsupervised evaluation approaches that rely on either contextual word representations extracted from pre-trained language models or non-contextual pre-trained word embeddings (e.g., word2vec (Mikolov et al., 2013); Glove (Pennington et al., 2014)). For the former, we use BERT-score (Zhang et al., 2020a) which computes the similarity Cross-lingual Transfer For supervised modelbetween each output token and each reference based methods that rely on the availability of token based on BERT contextual embeddings. human-annotated instances to train dedicated modFor the latter, we experiment with two simiels for specific tasks, we experiment with three larity metrics: the first is the cosine distance between the sentence-level feature represen- standard cross-lingual transfer approaches (e.g., Hu et al. (2020)): 1. ZERO - SHOT trains a single model tations of the compared texts extracted via 2 ave"
2021.emnlp-main.100,2020.acl-main.639,0,0.042812,"Missing"
2021.emnlp-main.419,N19-1422,0,0.0191533,". (2020) introduced the NYTimes800k dataset, a guided process following news image caption- and a model named Transform and Tell, which we 5163 GoodNews NYTimes800k who 93.02 93.77 when 44.06 41.54 where 58.59 51.08 misc 31.69 30.92 context 78.44 77.07 Table 2: Template components percentage in the GoodNews and NYTimes800k datasets. refer to as Tell. This model exploits a Transformer decoder and byte-pair-encoding (BPE) (Sennrich et al., 2016) allowing to generate captions with unseen or rare named entities from common tokens. As in other multimodal tasks, where studies (Shekhar et al., 2019; Caglayan et al., 2019; Li et al., 2020) have shown that the exploitation of both modalities is essential for achieving a good performance, Tran et al. (2020) evaluated a text only model showing that it performs worse than the multimodal model. We will also evaluate single visual and text modality models in our experiments. Our work differs from previous work in news image captioning in that JoGANIC is an end-toend framework that (i) integrates journalistic guidelines through a template guided caption generation process; and (ii) exploits a dedicated named entity representation and a long text encoding mechanism. O"
2021.emnlp-main.419,W14-3348,0,0.0176558,"ws and 763K training, 8K validation and 22K test captions for NYTimes800k. We provide further details about the datasets in the supplementary material. uses RoBERTa and ResNet-152 as the encoders and Transformer as the decoder, it is equivalent to JoGANIC without template guidance as they use the same encoders and training settings. (b) Tell (full), which includes two additional visual encoders: YOLOv3 and MTCNN, and LocationAware and Weighted RoBERTa for text encoding. For the general caption generation quality evaluation, we use the BLEU-4 (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014) and CIDEr (Vedantam et al., 2015) metrics. We also use named entity precision/recall to evaluate the named entity generation quality. To better understand how well the generated captions follow the ground truth templates, we calculate precision and recall for the five components who, when, where, context and misc and use the averaged precision and recall6 as the final metric. 5.2 Implementation and Training details Following Tran et al. (2020), we set the hidden size of the input features dI = 2048, dT = 1024 and dE = 300 and the number of heads H = 16. We use the Adam optimizer (Kingma and B"
2021.emnlp-main.419,P02-1040,0,0.110014,"g, 18K validation, and 23K test captions for GoodNews and 763K training, 8K validation and 22K test captions for NYTimes800k. We provide further details about the datasets in the supplementary material. uses RoBERTa and ResNet-152 as the encoders and Transformer as the decoder, it is equivalent to JoGANIC without template guidance as they use the same encoders and training settings. (b) Tell (full), which includes two additional visual encoders: YOLOv3 and MTCNN, and LocationAware and Weighted RoBERTa for text encoding. For the general caption generation quality evaluation, we use the BLEU-4 (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Denkowski and Lavie, 2014) and CIDEr (Vedantam et al., 2015) metrics. We also use named entity precision/recall to evaluate the named entity generation quality. To better understand how well the generated captions follow the ground truth templates, we calculate precision and recall for the five components who, when, where, context and misc and use the averaged precision and recall6 as the final metric. 5.2 Implementation and Training details Following Tran et al. (2020), we set the hidden size of the input features dI = 2048, dT = 1024 and dE = 300 and the number o"
2021.emnlp-main.419,D14-1162,0,0.0846998,"in news article image captioning, Ramisa et al. (2018), proposed an encoderdecoder architecture with a deep convolutional model VGG (Simonyan and Zisserman, 2015) and Word2Vec (Mikolov et al., 2013) as the image and text feature encoder, and an LSTM as the decoder. Biten et al. (2019) introduced the GoodNews dataset, and proposed a two-step caption generation 2 Related Work process using ResNet-152 (He et al., 2016) as the 2.1 Generic Image Captioning image representation and a sentence-level aggreState-of-the-art approaches (Johnson et al., 2016; gated representation using GloVe embeddings (Pennington et al., 2014). First, a caption is generated Wang et al., 2020; He et al., 2020; Sammani and with placeholders for the different types of named Melas-Kyriazi, 2020) mainly use encoder-decoder entities: PERSON, ORGANIZATION, etc. shown frameworks with attention to generate captions in the left column of Table 1. Then, the placefor images. Xu et al. (2015) developed soft and hard attention mechanisms to focus on different re- holders are filled in by matching entities from the gions in the image when generating different words. best ranked sentences of the article. This two-step Similarly, Anderson et al. (2"
2021.emnlp-main.419,D18-1435,0,0.0313992,"Missing"
2021.emnlp-main.419,N19-4009,0,0.0212606,"Missing"
2021.emnlp-main.419,P16-1162,0,0.00876387,"u et al. (2020); in three aspects: (i) our model’s input consists of Liu et al. (2020) used LSTM as the decoder. Tran image-article pairs; (ii) our caption generation is et al. (2020) introduced the NYTimes800k dataset, a guided process following news image caption- and a model named Transform and Tell, which we 5163 GoodNews NYTimes800k who 93.02 93.77 when 44.06 41.54 where 58.59 51.08 misc 31.69 30.92 context 78.44 77.07 Table 2: Template components percentage in the GoodNews and NYTimes800k datasets. refer to as Tell. This model exploits a Transformer decoder and byte-pair-encoding (BPE) (Sennrich et al., 2016) allowing to generate captions with unseen or rare named entities from common tokens. As in other multimodal tasks, where studies (Shekhar et al., 2019; Caglayan et al., 2019; Li et al., 2020) have shown that the exploitation of both modalities is essential for achieving a good performance, Tran et al. (2020) evaluated a text only model showing that it performs worse than the multimodal model. We will also evaluate single visual and text modality models in our experiments. Our work differs from previous work in news image captioning in that JoGANIC is an end-toend framework that (i) integrates"
2021.emnlp-main.419,W19-0418,0,0.0513491,"Missing"
2021.gem-1.6,S16-1081,0,0.0479348,"Missing"
2021.gem-1.6,2020.cl-1.4,0,0.0238665,"ems as their evaluation is primarily based on human judgments. Concretely, out of the 97 papers we reviewed, 69 of them resort to human evaluation (Figure 1), where it is treated either as a substitute for automatic metrics or as a more reliable evaluation. This paper summarizes the findings of the review and raises the following concerns on current human evaluation practices: they conduct either human or automatic evaluation on system outputs for ST, and therefore should be included in our structured review. We did not review papers for text simplification, as it has been studied separately (Alva-Manchego et al., 2020; Sikka et al., 2020) and metrics for automatic evaluation have been widely adopted (Xu et al., 2016). Our final list consists of 97 papers: 86 of them are from top-tier NLP and AI venues: ACL , EACL , EMNLP , NAACL , TACL , IEEE , AAAI , Neur IPS, ICML, and ICLR, and the remaining 11 are pre-prints which have not been peer-reviewed. Review Structure We review each paper based on a predefined set of criteria (Table 2). The rationale behind their choice is to collect information on the evaluation aspects that are underspecified in NLP in general as well as those specific to the ST task. For thi"
2021.gem-1.6,P09-1032,0,0.0315401,"Missing"
2021.gem-1.6,2021.eacl-main.29,0,0.0808413,"Missing"
2021.gem-1.6,2020.inlg-1.24,0,0.0421945,"utes studied and the lack of naturally occurring references for the associated ST tasks, prior work emphasizes the limitations of automatic evaluation. As a result, progress in this growing field relies heavily on human evaluations to quantify progress among the three evaluation aspects. 58 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 58–67 August 5–6, 2021. ©2021 Association for Computational Linguistics Inspired by recent critiques of human evaluations of Natural Language Generation (NLG) systems (Howcroft et al., 2020; Lee, 2020; Belz et al., 2020, 2021; Shimorina and Belz, 2021), we conduct a structured review of human evaluation for neural style transfer systems as their evaluation is primarily based on human judgments. Concretely, out of the 97 papers we reviewed, 69 of them resort to human evaluation (Figure 1), where it is treated either as a substitute for automatic metrics or as a more reliable evaluation. This paper summarizes the findings of the review and raises the following concerns on current human evaluation practices: they conduct either human or automatic evaluation on system outputs for ST, and therefore should be incl"
2021.gem-1.6,W07-0718,0,0.120501,"Missing"
2021.gem-1.6,2020.isa-1.5,0,0.0110905,"stic attributes studied and the lack of naturally occurring references for the associated ST tasks, prior work emphasizes the limitations of automatic evaluation. As a result, progress in this growing field relies heavily on human evaluations to quantify progress among the three evaluation aspects. 58 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 58–67 August 5–6, 2021. ©2021 Association for Computational Linguistics Inspired by recent critiques of human evaluations of Natural Language Generation (NLG) systems (Howcroft et al., 2020; Lee, 2020; Belz et al., 2020, 2021; Shimorina and Belz, 2021), we conduct a structured review of human evaluation for neural style transfer systems as their evaluation is primarily based on human judgments. Concretely, out of the 97 papers we reviewed, 69 of them resort to human evaluation (Figure 1), where it is treated either as a substitute for automatic metrics or as a more reliable evaluation. This paper summarizes the findings of the review and raises the following concerns on current human evaluation practices: they conduct either human or automatic evaluation on system outputs for ST, and there"
2021.gem-1.6,W13-2305,0,0.0750907,"Missing"
2021.gem-1.6,D19-1325,0,0.0440185,"Missing"
2021.gem-1.6,2021.humeval-1.8,0,0.0802246,"Missing"
2021.gem-1.6,P16-1094,0,0.0179725,"Missing"
2021.gem-1.6,P14-2029,1,0.864675,"Missing"
2021.gem-1.6,N18-1169,0,0.0658276,"Missing"
2021.gem-1.6,2020.inlg-1.23,0,0.0746458,"Missing"
2021.gem-1.6,2020.emnlp-main.602,0,0.0250282,"Missing"
2021.gem-1.6,N19-1049,0,0.0225152,"across three dimensions: style transfer (has the desired attributed been changed FORMALITY Gotta see both sides of the story. (informal) You have to consider both sides of the story. (formal) SENTIMENT The screen is just the right size. (positive) The screen is too small. (negative) AUTHOR IMITATION Bring her out to me. (modern) Call her forth to me. (shakespearean) Table 1: Examples of three ST attributes: formality, sentiment and Shakespearean transfer. as intended?), meaning preservation (are the other attributes preserved?), and fluency (is the output well-formed?) (Pang and Gimpel, 2019; Mir et al., 2019). Given the large spectrum of stylistic attributes studied and the lack of naturally occurring references for the associated ST tasks, prior work emphasizes the limitations of automatic evaluation. As a result, progress in this growing field relies heavily on human evaluations to quantify progress among the three evaluation aspects. 58 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 58–67 August 5–6, 2021. ©2021 Association for Computational Linguistics Inspired by recent critiques of human evaluations of Natural Language Generation (NL"
2021.gem-1.6,D19-1306,0,0.046338,"Missing"
2021.gem-1.6,2021.humeval-1.15,0,0.0677632,"Missing"
2021.gem-1.6,2020.emnlp-main.55,0,0.0597224,"Missing"
2021.gem-1.6,D19-5614,0,0.0189124,"y is usually evaluated across three dimensions: style transfer (has the desired attributed been changed FORMALITY Gotta see both sides of the story. (informal) You have to consider both sides of the story. (formal) SENTIMENT The screen is just the right size. (positive) The screen is too small. (negative) AUTHOR IMITATION Bring her out to me. (modern) Call her forth to me. (shakespearean) Table 1: Examples of three ST attributes: formality, sentiment and Shakespearean transfer. as intended?), meaning preservation (are the other attributes preserved?), and fluency (is the output well-formed?) (Pang and Gimpel, 2019; Mir et al., 2019). Given the large spectrum of stylistic attributes studied and the lack of naturally occurring references for the associated ST tasks, prior work emphasizes the limitations of automatic evaluation. As a result, progress in this growing field relies heavily on human evaluations to quantify progress among the three evaluation aspects. 58 Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021), pages 58–67 August 5–6, 2021. ©2021 Association for Computational Linguistics Inspired by recent critiques of human evaluations of Natural Lang"
2021.gem-1.6,N18-1012,1,0.902839,"Missing"
2021.gem-1.6,2020.evalnlgeval-1.2,0,0.0384816,"nd of 64 4.2 Releasing Annotations of the style dimension targeted. Furthermore, since ST includes rewriting text according to pragmatic aspects of language use, who the human judgments are matters since differences in communication norms and expectations might result in different judgments for the same text. Standardizing and describing protocols is also key to assessing the alignment of the evaluation with the models and task proposed (H¨am¨al¨ainen and Alnajjar, 2021), and to understand potential biases and ethical issues that might arise from, e.g., compensation mechanisms (Vaughan, 2018; Schoch et al., 2020; Shmueli et al., 2021). Making human-annotated judgments available would enable the development of better automatic metrics for ST. If all annotations had been released with the papers reviewed, we estimate that more than 10K human judgments per evaluation aspect would be available. Today this would suffice to train and evaluate dedicated evaluation models. In addition, raw annotations can shed light on the difficulty of the task and nature of the data: they can be aggregated in multiple ways (Oortwijn et al., 2021), or used to account for annotator bias in model training (Beigman and Beigman"
2021.gem-1.6,2021.naacl-main.295,0,0.027822,"g Annotations of the style dimension targeted. Furthermore, since ST includes rewriting text according to pragmatic aspects of language use, who the human judgments are matters since differences in communication norms and expectations might result in different judgments for the same text. Standardizing and describing protocols is also key to assessing the alignment of the evaluation with the models and task proposed (H¨am¨al¨ainen and Alnajjar, 2021), and to understand potential biases and ethical issues that might arise from, e.g., compensation mechanisms (Vaughan, 2018; Schoch et al., 2020; Shmueli et al., 2021). Making human-annotated judgments available would enable the development of better automatic metrics for ST. If all annotations had been released with the papers reviewed, we estimate that more than 10K human judgments per evaluation aspect would be available. Today this would suffice to train and evaluate dedicated evaluation models. In addition, raw annotations can shed light on the difficulty of the task and nature of the data: they can be aggregated in multiple ways (Oortwijn et al., 2021), or used to account for annotator bias in model training (Beigman and Beigman Klebanov, 2009). Final"
2021.gem-1.6,Q16-1029,0,0.0303553,"of them resort to human evaluation (Figure 1), where it is treated either as a substitute for automatic metrics or as a more reliable evaluation. This paper summarizes the findings of the review and raises the following concerns on current human evaluation practices: they conduct either human or automatic evaluation on system outputs for ST, and therefore should be included in our structured review. We did not review papers for text simplification, as it has been studied separately (Alva-Manchego et al., 2020; Sikka et al., 2020) and metrics for automatic evaluation have been widely adopted (Xu et al., 2016). Our final list consists of 97 papers: 86 of them are from top-tier NLP and AI venues: ACL , EACL , EMNLP , NAACL , TACL , IEEE , AAAI , Neur IPS, ICML, and ICLR, and the remaining 11 are pre-prints which have not been peer-reviewed. Review Structure We review each paper based on a predefined set of criteria (Table 2). The rationale behind their choice is to collect information on the evaluation aspects that are underspecified in NLP in general as well as those specific to the ST task. For this work, we call the former global criteria. The latter is called dimension-specific criteria and is m"
2021.naacl-main.256,S16-1081,0,0.0538204,"Missing"
2021.naacl-main.256,2020.emnlp-main.5,0,0.023242,"man correlations of 64 (BR - PT), 70 (IT), 71 (FR), and 81 (EN). Fluency We compute the logarithm of each sentence’s probability—computed by a 5-gram KneserNey language model (Kneser and Ney, 1995)—and normalize it by the sequence length. We train each 13 https://github.com/xingniu/ multitask-ft-fsmt 14 https://github.com/cindyxinyiwang/ deep-latent-sequence-model 15 See A.A for discussion on this assumption. language model on 2M random sample of the nonEnglish side of OpenSubtitles formal data. Overall We compute multi-BLEU (Post, 2018) via comparing with multiple formal rewrites on XFORMAL. Freitag et al. (2020) shows that correlation with human judgments improves when considering multiple references for MT evaluation. 5.4 Human evaluation Given that automatic evaluation of ST lacks standard evaluation practices—even in cases when EN is considered—we turn to human evaluation to reliably assess our baselines following the protocols of RT18. We sample a subset of 100 sentences from XFORMAL per language, evaluate outputs of 5 systems, and collect 5 judgments per instance.We open the task to all workers passing QC 2 in Table 3. We include inter-annotator agreement results in A.E. Formality We collect for"
2021.naacl-main.256,N19-1320,0,0.0190756,"ls perform on par with a simple rule-based system consisting of handcrafted transformations. We make XFORMAL, our annotations protocols, and analysis code publicly available and hope that this study facilitates and encourages more research towards Multilingual ST. that use reconstruction and back-translation losses (e.g., Logeswaran et al. (2018); Prabhumoye et al. (2018)). Another line of work, focuses on manipulation methods that remove the style-specific attribute of text (e.g., Li et al. (2018); Xu et al. (2018)), while recent approaches use reinforcement learning (e.g., Wu et al. (2019); Gong et al. (2019), probabilistic formulations (He et al., 2020), and masked language models (Malmi et al., 2020). 2 Inter-language ST is introduced by Mirkin and Meunier (2015) who proposed personalized MT for EN-French and EN-German. Subsequent MT works control for politeness (Sennrich et al., 2016a), voice (Yamagishi et al., 2016), personality traits (Rabinovich et al., 2017), user-provided terminology (Hasler et al., 2018), gender (Vanmassenhove et al., 2018), formality (Niu et al., 2017; Feely et al., 2019), morphological variations (Moryossef et al., 2019), complexity (Agrawal and Carpuat, 2019) and readi"
2021.naacl-main.256,2020.acl-main.100,0,0.0168936,"ling style aspects in generation tasks is studied in monolingual settings with an English-centric focus (intra-language) and cross-lingual settings together with Machine Translation (MT) (interlanguage). Our work rests in intra-language ST with a multilingual focus, in contrast to prior work. ST datasets that consist of parallel pairs in different styles include: GYAFC for formality (Rao and Tetreault, 2018), Yelp (Shen et al., 2017) and Amazon Product Reviews for sentiment (He and McAuley, 2016), political slant and gender controlled datasets (Prabhumoye et al., 2018), Expert Style Transfer (Cao et al., 2020), PASTEL for imitating personal (Kang et al., 2019), SIMILE for simile generation (Chakrabarty et al., 2020), and others. 3 XFORMAL Collection We describe the process of collecting formal rewrites using data statements protocols (Bender and Friedman, 2018; Gebru et al., 2018). Curation rational To collect XFORMAL, we firstly curate informal excerpts in multiple lanIntra-language ST was first cast as generation guages. To this end, we follow the procedures task by Xu et al. (2012) and is addressed through described in Rao and Tetreault (2018) (hencemethods that use either parallel data or unpai"
2021.naacl-main.256,N18-2081,0,0.0309629,"Missing"
2021.naacl-main.256,2020.emnlp-main.524,0,0.0365836,"cus (intra-language) and cross-lingual settings together with Machine Translation (MT) (interlanguage). Our work rests in intra-language ST with a multilingual focus, in contrast to prior work. ST datasets that consist of parallel pairs in different styles include: GYAFC for formality (Rao and Tetreault, 2018), Yelp (Shen et al., 2017) and Amazon Product Reviews for sentiment (He and McAuley, 2016), political slant and gender controlled datasets (Prabhumoye et al., 2018), Expert Style Transfer (Cao et al., 2020), PASTEL for imitating personal (Kang et al., 2019), SIMILE for simile generation (Chakrabarty et al., 2020), and others. 3 XFORMAL Collection We describe the process of collecting formal rewrites using data statements protocols (Bender and Friedman, 2018; Gebru et al., 2018). Curation rational To collect XFORMAL, we firstly curate informal excerpts in multiple lanIntra-language ST was first cast as generation guages. To this end, we follow the procedures task by Xu et al. (2012) and is addressed through described in Rao and Tetreault (2018) (hencemethods that use either parallel data or unpaired corpora of different styles. Parallel corpora de- forth RT18) who create a corpus of informalsigned for"
2021.naacl-main.256,P14-2029,1,0.839819,"Missing"
2021.naacl-main.256,W18-1820,0,0.0627641,"Missing"
2021.naacl-main.256,D19-1306,0,0.0457652,"Missing"
2021.naacl-main.256,D19-1179,0,0.0182722,"in monolingual settings with an English-centric focus (intra-language) and cross-lingual settings together with Machine Translation (MT) (interlanguage). Our work rests in intra-language ST with a multilingual focus, in contrast to prior work. ST datasets that consist of parallel pairs in different styles include: GYAFC for formality (Rao and Tetreault, 2018), Yelp (Shen et al., 2017) and Amazon Product Reviews for sentiment (He and McAuley, 2016), political slant and gender controlled datasets (Prabhumoye et al., 2018), Expert Style Transfer (Cao et al., 2020), PASTEL for imitating personal (Kang et al., 2019), SIMILE for simile generation (Chakrabarty et al., 2020), and others. 3 XFORMAL Collection We describe the process of collecting formal rewrites using data statements protocols (Bender and Friedman, 2018; Gebru et al., 2018). Curation rational To collect XFORMAL, we firstly curate informal excerpts in multiple lanIntra-language ST was first cast as generation guages. To this end, we follow the procedures task by Xu et al. (2012) and is addressed through described in Rao and Tetreault (2018) (hencemethods that use either parallel data or unpaired corpora of different styles. Parallel corpora d"
2021.naacl-main.256,P07-2045,0,0.00759793,"each model. 8 Data are available at: http://opus.nlpl.eu/. We use the publicly available pretrained Bicleaner models: https://github.com/bitextor/bicleaner, and discard sentences with a score lower than 0.5. 10 https://webscope.sandbox.yahoo.com/ catalog.php?datatype=l&did=74 11 Split into 6.2M/6.6M formal/informal sentences. 12 Bilingual data statistics are in A.K. 9 Unsupervised approaches We benchmark two unsupervised methods that are used for EN ST: 1. UNPSUPERVISED NEURAL MACHINE TRANS - Experimental setting 3203 Preprocessing We preprocess data consistently across languages using MOSES (Koehn et al., 2007). Our pipeline consists of three steps: a) normalization; b) tokenization; c) true-casing. For NMTbased approaches, we also learn joint source-target BPE with 32K operations (Sennrich et al., 2016b). Model Implementations For NMT-based and unsupervised models we use the open-sourced impementations of Niu et al. (2018) and He et al. (2020), respectively.13,14 We include more details on model architectures in A.D. 5.3 Automatic Evaluation Recent work on ST evaluation highlights the lack of standard evaluation practices (Yamshchikov et al., 2020; Pang, 2019; Pang and Gimpel, 2019; Mir et al., 201"
2021.naacl-main.256,2020.emnlp-main.55,0,0.0626299,"Missing"
2021.naacl-main.256,D19-1325,0,0.0181585,"rs in English (EN) entitled Grammarly’s Yahoo Answers Formality Corpus ditional encoder-decoder architectures (Rao and ( GYAFC ). Tetreault, 2018), learn mappings between latent repConcretely, we use the L6 Yahoo! Answers corresentation of different styles (Shang et al., 2019), or fine-tune pre-trained models (Wang et al., 2019). pus that consists of questions and answers posted to the Yahoo! Answers platform.2 The corpus conOther approaches use parallel data from similar tasks to facilitate transfer in the target style via do- tains a large number of informal text and allows main adaptation (Li et al., 2019), multi-task learn- control for different languages and different domains.3 Similar to the collection of GYAFC, we ing (Niu et al., 2018; Niu and Carpuat, 2020), and extract all answers from the Family & Relationzero-shot transfer (Korotkova et al., 2019) or create pseudo-parallel data via data augmentation tech- ships (F&R) topic that correspond to the three niques (Zhang et al., 2020; Krishna et al., 2020). languages of interest: Família e Relacionamentos (BR - PT), Relazioni e famiglia (IT ), and Amour Approaches that rely on non-parallel data include disentanglement methods based on the id"
2021.naacl-main.256,P02-1040,0,0.115166,"he open-sourced impementations of Niu et al. (2018) and He et al. (2020), respectively.13,14 We include more details on model architectures in A.D. 5.3 Automatic Evaluation Recent work on ST evaluation highlights the lack of standard evaluation practices (Yamshchikov et al., 2020; Pang, 2019; Pang and Gimpel, 2019; Mir et al., 2019). We follow the most frequent evaluation metrics used in EN tasks and measure the quality of the system’s outputs with respect to four dimensions, while we leave an extensive evaluation of automatic metrics for future work. Meaning Preservation We compute selfBLEU (Papineni et al., 2002) which compares system outputs with the informal sentences. Formality We average the style transfer score of transferred sentences computed by a formality regression model. We fine-tune mBERT (Devlin et al., 2019) pre-trained language models on the machinetranslated answers genre from Pavlick and Tetreault (2016) that consists of about 4K human-annotated sentences rated on a 7-point formality scale. To acquire an annotated corpus in the languages of interest, we follow the TRANSLATE TRAIN transfer approach: we propagate the original EN training data’s human ratings to their corresponding trans"
2021.naacl-main.256,Q16-1005,1,0.925805,"rkers’ demographics We recruit Turkers from Brazil, France/Canada, and Italy for BR - PT, FR , and IT , respectively. Beyond their country of residence, no further information is available. Compensation We compensate at a rate of $0.10 per HIT with additional one-time bonuses that bumps them up to a target rate of over $10/hour. After this entire process, we have constructed a high-quality corpus of formality rewrites of 1,000 sentences for three languages. In the next section, we provide statistics and an analysis of XFORMAL. 4 XFORMAL Statistics & Analysis Types of formality edits Following Pavlick and Tetreault (2016), we analyze the most frequent edit operations Turkers perform when formalizing the informal sentences. We conduct both an automatic analysis (details in A.I) of the whole set of rewrites, and a human analysis (details in A.H) of a random sample of 200 rewrites per language (we recruited a native speaker for each language). Table 4 presents both analyses’ results, where we also include the corresponding statistics for the English language (GYAFC). In general, we observe similar trends across languages: humans make edits covering both the &quot;noisy-text&quot; sense of formality (e.g., fixing punctuatio"
2021.naacl-main.256,W18-6319,0,0.0126349,"dgments of 5 Turkers for 200 sentences per language. We report Spearman correlations of 64 (BR - PT), 70 (IT), 71 (FR), and 81 (EN). Fluency We compute the logarithm of each sentence’s probability—computed by a 5-gram KneserNey language model (Kneser and Ney, 1995)—and normalize it by the sequence length. We train each 13 https://github.com/xingniu/ multitask-ft-fsmt 14 https://github.com/cindyxinyiwang/ deep-latent-sequence-model 15 See A.A for discussion on this assumption. language model on 2M random sample of the nonEnglish side of OpenSubtitles formal data. Overall We compute multi-BLEU (Post, 2018) via comparing with multiple formal rewrites on XFORMAL. Freitag et al. (2020) shows that correlation with human judgments improves when considering multiple references for MT evaluation. 5.4 Human evaluation Given that automatic evaluation of ST lacks standard evaluation practices—even in cases when EN is considered—we turn to human evaluation to reliably assess our baselines following the protocols of RT18. We sample a subset of 100 sentences from XFORMAL per language, evaluate outputs of 5 systems, and collect 5 judgments per instance.We open the task to all workers passing QC 2 in Table 3."
2021.naacl-main.256,P18-1080,0,0.0711974,"3199–3216 June 6–11, 2021. ©2021 Association for Computational Linguistics of leading ST baselines through automatic and human evaluation methods. Our results show that Fo ST in non-English languages is particularly challenging as complex neural models perform on par with a simple rule-based system consisting of handcrafted transformations. We make XFORMAL, our annotations protocols, and analysis code publicly available and hope that this study facilitates and encourages more research towards Multilingual ST. that use reconstruction and back-translation losses (e.g., Logeswaran et al. (2018); Prabhumoye et al. (2018)). Another line of work, focuses on manipulation methods that remove the style-specific attribute of text (e.g., Li et al. (2018); Xu et al. (2018)), while recent approaches use reinforcement learning (e.g., Wu et al. (2019); Gong et al. (2019), probabilistic formulations (He et al., 2020), and masked language models (Malmi et al., 2020). 2 Inter-language ST is introduced by Mirkin and Meunier (2015) who proposed personalized MT for EN-French and EN-German. Subsequent MT works control for politeness (Sennrich et al., 2016a), voice (Yamagishi et al., 2016), personality traits (Rabinovich et al."
2021.naacl-main.256,E17-1101,0,0.0184006,"oye et al. (2018)). Another line of work, focuses on manipulation methods that remove the style-specific attribute of text (e.g., Li et al. (2018); Xu et al. (2018)), while recent approaches use reinforcement learning (e.g., Wu et al. (2019); Gong et al. (2019), probabilistic formulations (He et al., 2020), and masked language models (Malmi et al., 2020). 2 Inter-language ST is introduced by Mirkin and Meunier (2015) who proposed personalized MT for EN-French and EN-German. Subsequent MT works control for politeness (Sennrich et al., 2016a), voice (Yamagishi et al., 2016), personality traits (Rabinovich et al., 2017), user-provided terminology (Hasler et al., 2018), gender (Vanmassenhove et al., 2018), formality (Niu et al., 2017; Feely et al., 2019), morphological variations (Moryossef et al., 2019), complexity (Agrawal and Carpuat, 2019) and reading level (Marchisio et al., 2019). Related Work Controlling style aspects in generation tasks is studied in monolingual settings with an English-centric focus (intra-language) and cross-lingual settings together with Machine Translation (MT) (interlanguage). Our work rests in intra-language ST with a multilingual focus, in contrast to prior work. ST datasets th"
2021.naacl-main.256,N18-1012,1,0.936782,"hese examples relate to the notion of deep formality (Heylighen et al., 1999), where the ultimate goal is that of adding the context needed to disambiguate an expression. On the other hand, variations in formality might just reflect different situational and personal factors, as shown in the Italian (IT) example. This work takes the first step towards a more language-inclusive direction for the field of ST by building the first corpus of style transfer for non-English languages. In particular, we make the following contributions: 1. Building upon prior work on Formality Style Transfer (FoST) (Rao and Tetreault, 2018), we contribute an evaluation dataset, XFORMAL that consists of multiple formal rewrites of informal sentences in three Romance languages: Brazilian Portuguese (BR - PT), French (FR), and Italian (IT); 2. Without assum∗ ing access to any gold-standard training data for Work done as a Research Intern at Dataminr, Inc. 1 Code and data: https://github.com/Elbria/xformal-FoST the languages at hand, we benchmark a myriad 3199 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 3199–3216 June 6–11, 2021"
2021.naacl-main.256,C12-1177,0,0.0278278,"d McAuley, 2016), political slant and gender controlled datasets (Prabhumoye et al., 2018), Expert Style Transfer (Cao et al., 2020), PASTEL for imitating personal (Kang et al., 2019), SIMILE for simile generation (Chakrabarty et al., 2020), and others. 3 XFORMAL Collection We describe the process of collecting formal rewrites using data statements protocols (Bender and Friedman, 2018; Gebru et al., 2018). Curation rational To collect XFORMAL, we firstly curate informal excerpts in multiple lanIntra-language ST was first cast as generation guages. To this end, we follow the procedures task by Xu et al. (2012) and is addressed through described in Rao and Tetreault (2018) (hencemethods that use either parallel data or unpaired corpora of different styles. Parallel corpora de- forth RT18) who create a corpus of informalsigned for the task at hand are used to train tra- formal sentence-pairs in English (EN) entitled Grammarly’s Yahoo Answers Formality Corpus ditional encoder-decoder architectures (Rao and ( GYAFC ). Tetreault, 2018), learn mappings between latent repConcretely, we use the L6 Yahoo! Answers corresentation of different styles (Shang et al., 2019), or fine-tune pre-trained models (Wang"
2021.naacl-main.256,W16-4620,0,0.0177668,"sses (e.g., Logeswaran et al. (2018); Prabhumoye et al. (2018)). Another line of work, focuses on manipulation methods that remove the style-specific attribute of text (e.g., Li et al. (2018); Xu et al. (2018)), while recent approaches use reinforcement learning (e.g., Wu et al. (2019); Gong et al. (2019), probabilistic formulations (He et al., 2020), and masked language models (Malmi et al., 2020). 2 Inter-language ST is introduced by Mirkin and Meunier (2015) who proposed personalized MT for EN-French and EN-German. Subsequent MT works control for politeness (Sennrich et al., 2016a), voice (Yamagishi et al., 2016), personality traits (Rabinovich et al., 2017), user-provided terminology (Hasler et al., 2018), gender (Vanmassenhove et al., 2018), formality (Niu et al., 2017; Feely et al., 2019), morphological variations (Moryossef et al., 2019), complexity (Agrawal and Carpuat, 2019) and reading level (Marchisio et al., 2019). Related Work Controlling style aspects in generation tasks is studied in monolingual settings with an English-centric focus (intra-language) and cross-lingual settings together with Machine Translation (MT) (interlanguage). Our work rests in intra-language ST with a multilingual fo"
2021.naacl-main.256,2020.acl-main.294,0,0.0814976,"he Yahoo! Answers platform.2 The corpus conOther approaches use parallel data from similar tasks to facilitate transfer in the target style via do- tains a large number of informal text and allows main adaptation (Li et al., 2019), multi-task learn- control for different languages and different domains.3 Similar to the collection of GYAFC, we ing (Niu et al., 2018; Niu and Carpuat, 2020), and extract all answers from the Family & Relationzero-shot transfer (Korotkova et al., 2019) or create pseudo-parallel data via data augmentation tech- ships (F&R) topic that correspond to the three niques (Zhang et al., 2020; Krishna et al., 2020). languages of interest: Família e Relacionamentos (BR - PT), Relazioni e famiglia (IT ), and Amour Approaches that rely on non-parallel data include disentanglement methods based on the idea of learn- et relations (FR) (Step 1). We follow the same pre-processing steps as described in RT18 for coning style-agnostic latent representations (e.g., Shen sistency (Step 2). We filter out answers that: a) et al. (2017); Hu et al. (2017)). However, they consist of questions; b) include URLs; c) have fewer are recently criticized for resulting in poor content preservation (Xu et"
2021.naacl-main.256,D18-1334,0,0.011822,"ve the style-specific attribute of text (e.g., Li et al. (2018); Xu et al. (2018)), while recent approaches use reinforcement learning (e.g., Wu et al. (2019); Gong et al. (2019), probabilistic formulations (He et al., 2020), and masked language models (Malmi et al., 2020). 2 Inter-language ST is introduced by Mirkin and Meunier (2015) who proposed personalized MT for EN-French and EN-German. Subsequent MT works control for politeness (Sennrich et al., 2016a), voice (Yamagishi et al., 2016), personality traits (Rabinovich et al., 2017), user-provided terminology (Hasler et al., 2018), gender (Vanmassenhove et al., 2018), formality (Niu et al., 2017; Feely et al., 2019), morphological variations (Moryossef et al., 2019), complexity (Agrawal and Carpuat, 2019) and reading level (Marchisio et al., 2019). Related Work Controlling style aspects in generation tasks is studied in monolingual settings with an English-centric focus (intra-language) and cross-lingual settings together with Machine Translation (MT) (interlanguage). Our work rests in intra-language ST with a multilingual focus, in contrast to prior work. ST datasets that consist of parallel pairs in different styles include: GYAFC for formality (Rao and"
2021.naacl-main.256,D19-1365,0,0.0127701,"2012) and is addressed through described in Rao and Tetreault (2018) (hencemethods that use either parallel data or unpaired corpora of different styles. Parallel corpora de- forth RT18) who create a corpus of informalsigned for the task at hand are used to train tra- formal sentence-pairs in English (EN) entitled Grammarly’s Yahoo Answers Formality Corpus ditional encoder-decoder architectures (Rao and ( GYAFC ). Tetreault, 2018), learn mappings between latent repConcretely, we use the L6 Yahoo! Answers corresentation of different styles (Shang et al., 2019), or fine-tune pre-trained models (Wang et al., 2019). pus that consists of questions and answers posted to the Yahoo! Answers platform.2 The corpus conOther approaches use parallel data from similar tasks to facilitate transfer in the target style via do- tains a large number of informal text and allows main adaptation (Li et al., 2019), multi-task learn- control for different languages and different domains.3 Similar to the collection of GYAFC, we ing (Niu et al., 2018; Niu and Carpuat, 2020), and extract all answers from the Family & Relationzero-shot transfer (Korotkova et al., 2019) or create pseudo-parallel data via data augmentation tech-"
2021.naacl-main.256,P19-1482,0,0.0235048,"omplex neural models perform on par with a simple rule-based system consisting of handcrafted transformations. We make XFORMAL, our annotations protocols, and analysis code publicly available and hope that this study facilitates and encourages more research towards Multilingual ST. that use reconstruction and back-translation losses (e.g., Logeswaran et al. (2018); Prabhumoye et al. (2018)). Another line of work, focuses on manipulation methods that remove the style-specific attribute of text (e.g., Li et al. (2018); Xu et al. (2018)), while recent approaches use reinforcement learning (e.g., Wu et al. (2019); Gong et al. (2019), probabilistic formulations (He et al., 2020), and masked language models (Malmi et al., 2020). 2 Inter-language ST is introduced by Mirkin and Meunier (2015) who proposed personalized MT for EN-French and EN-German. Subsequent MT works control for politeness (Sennrich et al., 2016a), voice (Yamagishi et al., 2016), personality traits (Rabinovich et al., 2017), user-provided terminology (Hasler et al., 2018), gender (Vanmassenhove et al., 2018), formality (Niu et al., 2017; Feely et al., 2019), morphological variations (Moryossef et al., 2019), complexity (Agrawal and Carp"
2021.textgraphs-1.13,2020.emnlp-main.49,0,0.0121475,"y are 1 Introduction connected. However, in the first sentence, “fired” is Event detection is an important task in natural lan- an event trigger of type “Conflict:Attack,” whereas guage processing, which encompasses predicting in the second sentence, it is of type “Personnel:End important incidents in texts, e.g., news, tweets, mes- Position.” The fact that the edge label between “posages, and manuscripts (Yang and Mitchell, 2016; lice” and “fired” is a nominal-subject or an object Nguyen et al., 2016; Feng et al., 2016; Zhang et al., relation serves as an indicator of the type of event 2020; Du and Cardie, 2020; McClosky et al., 2011; trigger. Hence, leveraging the dependency labels Ji and Grishman, 2008; Liao and Grishman, 2010; can help improve the event detection performance. Li et al., 2013; Yang et al., 2019). As an examIn this work, we propose a simple method to ple, consider the following sentence: The plane employ the dependency labels into existing models arrived back to base safely. Here, the word ar- inspired from a recently proposed technique called rived is an event trigger that denotes an event of Graph Transformer Networks (GTN) (Yun et al., the type “Movement:Transport,” while “The p"
2021.textgraphs-1.13,P16-2011,0,0.0175115,"es not take into account dependency labels will only have access to the information that they are 1 Introduction connected. However, in the first sentence, “fired” is Event detection is an important task in natural lan- an event trigger of type “Conflict:Attack,” whereas guage processing, which encompasses predicting in the second sentence, it is of type “Personnel:End important incidents in texts, e.g., news, tweets, mes- Position.” The fact that the edge label between “posages, and manuscripts (Yang and Mitchell, 2016; lice” and “fired” is a nominal-subject or an object Nguyen et al., 2016; Feng et al., 2016; Zhang et al., relation serves as an indicator of the type of event 2020; Du and Cardie, 2020; McClosky et al., 2011; trigger. Hence, leveraging the dependency labels Ji and Grishman, 2008; Liao and Grishman, 2010; can help improve the event detection performance. Li et al., 2013; Yang et al., 2019). As an examIn this work, we propose a simple method to ple, consider the following sentence: The plane employ the dependency labels into existing models arrived back to base safely. Here, the word ar- inspired from a recently proposed technique called rived is an event trigger that denotes an even"
2021.textgraphs-1.13,P08-1030,0,0.0788393,"an important task in natural lan- an event trigger of type “Conflict:Attack,” whereas guage processing, which encompasses predicting in the second sentence, it is of type “Personnel:End important incidents in texts, e.g., news, tweets, mes- Position.” The fact that the edge label between “posages, and manuscripts (Yang and Mitchell, 2016; lice” and “fired” is a nominal-subject or an object Nguyen et al., 2016; Feng et al., 2016; Zhang et al., relation serves as an indicator of the type of event 2020; Du and Cardie, 2020; McClosky et al., 2011; trigger. Hence, leveraging the dependency labels Ji and Grishman, 2008; Liao and Grishman, 2010; can help improve the event detection performance. Li et al., 2013; Yang et al., 2019). As an examIn this work, we propose a simple method to ple, consider the following sentence: The plane employ the dependency labels into existing models arrived back to base safely. Here, the word ar- inspired from a recently proposed technique called rived is an event trigger that denotes an event of Graph Transformer Networks (GTN) (Yun et al., the type “Movement:Transport,” while “The plane” 2019). GTNs enable us to learn a soft selection of and “base” are its arguments. Given a"
2021.textgraphs-1.13,P13-1008,0,0.030909,"ssing, which encompasses predicting in the second sentence, it is of type “Personnel:End important incidents in texts, e.g., news, tweets, mes- Position.” The fact that the edge label between “posages, and manuscripts (Yang and Mitchell, 2016; lice” and “fired” is a nominal-subject or an object Nguyen et al., 2016; Feng et al., 2016; Zhang et al., relation serves as an indicator of the type of event 2020; Du and Cardie, 2020; McClosky et al., 2011; trigger. Hence, leveraging the dependency labels Ji and Grishman, 2008; Liao and Grishman, 2010; can help improve the event detection performance. Li et al., 2013; Yang et al., 2019). As an examIn this work, we propose a simple method to ple, consider the following sentence: The plane employ the dependency labels into existing models arrived back to base safely. Here, the word ar- inspired from a recently proposed technique called rived is an event trigger that denotes an event of Graph Transformer Networks (GTN) (Yun et al., the type “Movement:Transport,” while “The plane” 2019). GTNs enable us to learn a soft selection of and “base” are its arguments. Given a sentence, the edge-types and composite relations (e.g., multi-hop objective of the event det"
2021.textgraphs-1.13,P10-1081,0,0.0496057,"atural lan- an event trigger of type “Conflict:Attack,” whereas guage processing, which encompasses predicting in the second sentence, it is of type “Personnel:End important incidents in texts, e.g., news, tweets, mes- Position.” The fact that the edge label between “posages, and manuscripts (Yang and Mitchell, 2016; lice” and “fired” is a nominal-subject or an object Nguyen et al., 2016; Feng et al., 2016; Zhang et al., relation serves as an indicator of the type of event 2020; Du and Cardie, 2020; McClosky et al., 2011; trigger. Hence, leveraging the dependency labels Ji and Grishman, 2008; Liao and Grishman, 2010; can help improve the event detection performance. Li et al., 2013; Yang et al., 2019). As an examIn this work, we propose a simple method to ple, consider the following sentence: The plane employ the dependency labels into existing models arrived back to base safely. Here, the word ar- inspired from a recently proposed technique called rived is an event trigger that denotes an event of Graph Transformer Networks (GTN) (Yun et al., the type “Movement:Transport,” while “The plane” 2019). GTNs enable us to learn a soft selection of and “base” are its arguments. Given a sentence, the edge-types"
2021.textgraphs-1.13,D18-1156,0,0.0470076,"Missing"
2021.textgraphs-1.13,P14-5010,0,0.0127037,"k+1 0 , h1 , . . . , hn−1 be the output of the k-th layer of this module with H 0 = P . Given any adjacency matrix A and input H k , consider the following operation at layer k: fu (H k , A) = n−1 X k k GkA (u, v)(WA hv + bkA ). (1) v=0 k and bk are the weight matrix and Here, WA A bias item for the adjacency matrix A at layer k, and GkA (u, v) is the gated-importance, given by k GkA (u, v) = A(u, v)σ(watt,A hkv + katt,A ), where σ(·) is an activation function, and watt,A and att,A are the attention weight vector and bias item. A dependency parser, e.g., Stanford Core 2 Proposed Method NLP (Manning et al., 2014), generates a directed In this work, we incorporate GTNs onto two heterogeneous graph G for each sentence (recall homogeneous-graph-based models: (i) Model I: Fig. 1). Existing works typically do not use the dea gated-graph-convolution-based model inspired pendency labels (e.g., nominal-subject); they only by Nguyen and Grishman (2018); Liu et al. (2018); derive three homogeneous adjacency matrices from Balali et al. (2020); and (ii) Model II: MOGANED G as follows: (i) Af wd where Af wd (i, j) = 1 if model (Yan et al., 2019). Both models have a sim- there is an edge from node i to j; (ii) Arev"
2021.textgraphs-1.13,N16-1034,0,0.0190713,"red”. A model that does not take into account dependency labels will only have access to the information that they are 1 Introduction connected. However, in the first sentence, “fired” is Event detection is an important task in natural lan- an event trigger of type “Conflict:Attack,” whereas guage processing, which encompasses predicting in the second sentence, it is of type “Personnel:End important incidents in texts, e.g., news, tweets, mes- Position.” The fact that the edge label between “posages, and manuscripts (Yang and Mitchell, 2016; lice” and “fired” is a nominal-subject or an object Nguyen et al., 2016; Feng et al., 2016; Zhang et al., relation serves as an indicator of the type of event 2020; Du and Cardie, 2020; McClosky et al., 2011; trigger. Hence, leveraging the dependency labels Ji and Grishman, 2008; Liao and Grishman, 2010; can help improve the event detection performance. Li et al., 2013; Yang et al., 2019). As an examIn this work, we propose a simple method to ple, consider the following sentence: The plane employ the dependency labels into existing models arrived back to base safely. Here, the word ar- inspired from a recently proposed technique called rived is an event trigger t"
2021.textgraphs-1.13,2020.findings-emnlp.326,0,0.0751477,"ependency relations for a graph-based model. While the goal of this paper is not to establish a state-ofthe-art (SOTA) method, but rather to show the merit of our approach, we do note that the improvements with our method approach the current SOTA (Cui et al., 2020) (which leverages dependency relations using embeddings instead of GTNs). To summarize, our main contribution is a method of enabling existing homogeneous-graph-based models to exploit dependency labels for event detection, inspired from GTNs. Incorporating GTNs in NLP tasks has received less attention (also see recent related work Veyseh et al. (2020)). Notations: We denote matrices and vectors in bold, e.g., A (matrix) or a (vector). Note that, A(u, v) denotes the element at index (u, v) in matrix A. ← [LST M (xi )||LST M (xi )], and ||denotes the concatenation operation. P is then fed to the graphbased module, as discussed next. Graph-Based Module: We first introduce the basic unit of both Model I and II, i.e., gatedgraph-convolution network (see Fig. 3). Let H k = hk0 , hk1 , . . . , hkn−1 be the input and H k+1 = k+1 k+1 hk+1 0 , h1 , . . . , hn−1 be the output of the k-th layer of this module with H 0 = P . Given any adjacency matrix"
2021.textgraphs-1.13,D19-1582,0,0.267254,"rigger that denotes an event of Graph Transformer Networks (GTN) (Yun et al., the type “Movement:Transport,” while “The plane” 2019). GTNs enable us to learn a soft selection of and “base” are its arguments. Given a sentence, the edge-types and composite relations (e.g., multi-hop objective of the event detection task is to predict connections, called meta-paths) among the words, all such event triggers and their respective types. thus producing heterogeneous adjacency matrices. Recent works on event detection (Nguyen and We integrate GTNs into two homogeneousGrishman, 2018; Liu et al., 2018; Yan et al., 2019; graph-based models (that previously ignored the deBalali et al., 2020) employ graph based methods pendency relations), namely, a simple gated-graph(Graph Convolution Networks (Kipf and Welling, convolution-based model inspired by Nguyen and 2017)) using the dependency graph (shown in Grishman (2018); Liu et al. (2018); Balali et al. Fig. 1) generated from syntactic dependency- (2020), and the near-state-of-the-art MOGANED parsers. These methods are able to capture use- model (Yan et al., 2019), enabling them to now ful non-local dependencies between words that are leverage the dependency rel"
2021.textgraphs-1.13,N16-1033,0,0.0167003,"s in Fig. 1. In both the sentences, there is an edge between “police” and “fired”. A model that does not take into account dependency labels will only have access to the information that they are 1 Introduction connected. However, in the first sentence, “fired” is Event detection is an important task in natural lan- an event trigger of type “Conflict:Attack,” whereas guage processing, which encompasses predicting in the second sentence, it is of type “Personnel:End important incidents in texts, e.g., news, tweets, mes- Position.” The fact that the edge label between “posages, and manuscripts (Yang and Mitchell, 2016; lice” and “fired” is a nominal-subject or an object Nguyen et al., 2016; Feng et al., 2016; Zhang et al., relation serves as an indicator of the type of event 2020; Du and Cardie, 2020; McClosky et al., 2011; trigger. Hence, leveraging the dependency labels Ji and Grishman, 2008; Liao and Grishman, 2010; can help improve the event detection performance. Li et al., 2013; Yang et al., 2019). As an examIn this work, we propose a simple method to ple, consider the following sentence: The plane employ the dependency labels into existing models arrived back to base safely. Here, the word ar- inspi"
2021.textgraphs-1.13,P19-1522,0,0.0133505,"mpasses predicting in the second sentence, it is of type “Personnel:End important incidents in texts, e.g., news, tweets, mes- Position.” The fact that the edge label between “posages, and manuscripts (Yang and Mitchell, 2016; lice” and “fired” is a nominal-subject or an object Nguyen et al., 2016; Feng et al., 2016; Zhang et al., relation serves as an indicator of the type of event 2020; Du and Cardie, 2020; McClosky et al., 2011; trigger. Hence, leveraging the dependency labels Ji and Grishman, 2008; Liao and Grishman, 2010; can help improve the event detection performance. Li et al., 2013; Yang et al., 2019). As an examIn this work, we propose a simple method to ple, consider the following sentence: The plane employ the dependency labels into existing models arrived back to base safely. Here, the word ar- inspired from a recently proposed technique called rived is an event trigger that denotes an event of Graph Transformer Networks (GTN) (Yun et al., the type “Movement:Transport,” while “The plane” 2019). GTNs enable us to learn a soft selection of and “base” are its arguments. Given a sentence, the edge-types and composite relations (e.g., multi-hop objective of the event detection task is to pr"
2021.textgraphs-1.13,P11-1163,0,0.0262622,"connected. However, in the first sentence, “fired” is Event detection is an important task in natural lan- an event trigger of type “Conflict:Attack,” whereas guage processing, which encompasses predicting in the second sentence, it is of type “Personnel:End important incidents in texts, e.g., news, tweets, mes- Position.” The fact that the edge label between “posages, and manuscripts (Yang and Mitchell, 2016; lice” and “fired” is a nominal-subject or an object Nguyen et al., 2016; Feng et al., 2016; Zhang et al., relation serves as an indicator of the type of event 2020; Du and Cardie, 2020; McClosky et al., 2011; trigger. Hence, leveraging the dependency labels Ji and Grishman, 2008; Liao and Grishman, 2010; can help improve the event detection performance. Li et al., 2013; Yang et al., 2019). As an examIn this work, we propose a simple method to ple, consider the following sentence: The plane employ the dependency labels into existing models arrived back to base safely. Here, the word ar- inspired from a recently proposed technique called rived is an event trigger that denotes an event of Graph Transformer Networks (GTN) (Yun et al., the type “Movement:Transport,” while “The plane” 2019). GTNs enabl"
2021.wnut-1.28,P98-1013,0,0.26263,"route kill avalanche earthquake destroyed rescue camp1 stuck climbers 100 Figure 2: Initial semantic graph, based on subset of Nepal 2015 earthquake tweets (tweets 3, 4, 5) initial semantic graph is shown in Figure 2, which is constructed based on tweets 3, 4 and 5 in Table 1. Formally, given a tweet ti , we use a dependency parser to extract the part-of-speech tags from tweets. Based on these tags we form two groups: (1) verbs and nominalized verbs (i.e., nouns that are derived from verbs, like explosion) Vi = {v1 , v2 , ...}, by matching the tokens to the Lexical Units provided in FrameNet (Baker et al., 1998) and (2) nouns (excluding nominalized verbs) Ni = {n1 , n2 , ..}. The output graph has as nodes ∪i Ni ∪Vi for all tweets ti . We form weighted edges only across the two groups (verbs V and nouns N ), which are initialized based on the PMI of each pair (Church and Hanks, 1990). More specifically, for each tweet ti , we have (vi , nj )∀i, j but no (ni , nj ) edges. This ensures that we link the sentence’s predicate with its arguments, while avoiding to link arguments that appear together under different relations/predicates. As shown in Figure 2, this results in a graph that combines information"
2021.wnut-1.28,J90-1003,0,0.131266,"le 1. Formally, given a tweet ti , we use a dependency parser to extract the part-of-speech tags from tweets. Based on these tags we form two groups: (1) verbs and nominalized verbs (i.e., nouns that are derived from verbs, like explosion) Vi = {v1 , v2 , ...}, by matching the tokens to the Lexical Units provided in FrameNet (Baker et al., 1998) and (2) nouns (excluding nominalized verbs) Ni = {n1 , n2 , ..}. The output graph has as nodes ∪i Ni ∪Vi for all tweets ti . We form weighted edges only across the two groups (verbs V and nouns N ), which are initialized based on the PMI of each pair (Church and Hanks, 1990). More specifically, for each tweet ti , we have (vi , nj )∀i, j but no (ni , nj ) edges. This ensures that we link the sentence’s predicate with its arguments, while avoiding to link arguments that appear together under different relations/predicates. As shown in Figure 2, this results in a graph that combines information across tweets in a more explainable way compared to previous approaches (Deng et al., 2019), while maintaining semantic relations from text. Given a large-scale crisis event, our first step is to represent the content of the related tweets in a graph structure by merging inf"
2021.wnut-1.28,L16-1259,0,0.0251932,"ypes of sentence classification for disaster response, such as determining if a message is Dynamic Semantic Graphs): a novel framework related to a specific crisis event (Caragea et al., 250 2016; Kruspe, 2019; Nguyen et al., 2016; Neubig et al., 2011), if it is actionable (Leavitt and Robinson, 2017) or critical (Mccreadie et al., 2019; Spiliopoulou et al., 2020). Other work classifies tweets with respect to the type of information they contain, a problem that is formulated as a multiclass tweet classification (typically five major information types) (Burel et al., 2017; Nguyen et al., 2017; Imran et al., 2016; Miyazaki et al., 2019; Padhee et al., 2020). Related work outside of Crisis NLP can also be used to extract information from tweets, in the form of events. Chen et al. (2018) use an encoderdecoder framework to extract sub-events from each tweet, while Rudra et al. (2018) use noun-verb pairs to represent sub-events, where each pair is ranked based on their overlap score in tweets. Some approaches outside the crisis domain that focus on extracting textual sub-events from tweets or documents, in a sequence classification setup (Bekoulis et al., 2019). Other related work includes Open IE methods"
2021.wnut-1.28,D19-1660,0,0.0186393,"ssification for disaster response, such as determining if a message is Dynamic Semantic Graphs): a novel framework related to a specific crisis event (Caragea et al., 250 2016; Kruspe, 2019; Nguyen et al., 2016; Neubig et al., 2011), if it is actionable (Leavitt and Robinson, 2017) or critical (Mccreadie et al., 2019; Spiliopoulou et al., 2020). Other work classifies tweets with respect to the type of information they contain, a problem that is formulated as a multiclass tweet classification (typically five major information types) (Burel et al., 2017; Nguyen et al., 2017; Imran et al., 2016; Miyazaki et al., 2019; Padhee et al., 2020). Related work outside of Crisis NLP can also be used to extract information from tweets, in the form of events. Chen et al. (2018) use an encoderdecoder framework to extract sub-events from each tweet, while Rudra et al. (2018) use noun-verb pairs to represent sub-events, where each pair is ranked based on their overlap score in tweets. Some approaches outside the crisis domain that focus on extracting textual sub-events from tweets or documents, in a sequence classification setup (Bekoulis et al., 2019). Other related work includes Open IE methods (open information extr"
2021.wnut-1.28,I11-1108,0,0.043058,"n across documents / posts. Information Extraction & Classification in Tweets. Given the large volume of noisy data from social media, most tasks focus on sentence classification problems, where the goal is to filter only the most important posts that might be helpful for first responders. As discussed by Imran et al. (2015); Tapia et al. (2011), there are several types of sentence classification for disaster response, such as determining if a message is Dynamic Semantic Graphs): a novel framework related to a specific crisis event (Caragea et al., 250 2016; Kruspe, 2019; Nguyen et al., 2016; Neubig et al., 2011), if it is actionable (Leavitt and Robinson, 2017) or critical (Mccreadie et al., 2019; Spiliopoulou et al., 2020). Other work classifies tweets with respect to the type of information they contain, a problem that is formulated as a multiclass tweet classification (typically five major information types) (Burel et al., 2017; Nguyen et al., 2017; Imran et al., 2016; Miyazaki et al., 2019; Padhee et al., 2020). Related work outside of Crisis NLP can also be used to extract information from tweets, in the form of events. Chen et al. (2018) use an encoderdecoder framework to extract sub-events fro"
2021.wnut-1.28,2020.acl-main.521,0,0.0484298,"Missing"
2021.wnut-1.28,D14-1162,0,0.0920566,"Missing"
2021.wnut-1.28,2020.findings-emnlp.344,1,0.767431,"sy data from social media, most tasks focus on sentence classification problems, where the goal is to filter only the most important posts that might be helpful for first responders. As discussed by Imran et al. (2015); Tapia et al. (2011), there are several types of sentence classification for disaster response, such as determining if a message is Dynamic Semantic Graphs): a novel framework related to a specific crisis event (Caragea et al., 250 2016; Kruspe, 2019; Nguyen et al., 2016; Neubig et al., 2011), if it is actionable (Leavitt and Robinson, 2017) or critical (Mccreadie et al., 2019; Spiliopoulou et al., 2020). Other work classifies tweets with respect to the type of information they contain, a problem that is formulated as a multiclass tweet classification (typically five major information types) (Burel et al., 2017; Nguyen et al., 2017; Imran et al., 2016; Miyazaki et al., 2019; Padhee et al., 2020). Related work outside of Crisis NLP can also be used to extract information from tweets, in the form of events. Chen et al. (2018) use an encoderdecoder framework to extract sub-events from each tweet, while Rudra et al. (2018) use noun-verb pairs to represent sub-events, where each pair is ranked bas"
2021.wnut-1.28,N18-1081,0,0.0878613,"n et al. (2018) use an encoderdecoder framework to extract sub-events from each tweet, while Rudra et al. (2018) use noun-verb pairs to represent sub-events, where each pair is ranked based on their overlap score in tweets. Some approaches outside the crisis domain that focus on extracting textual sub-events from tweets or documents, in a sequence classification setup (Bekoulis et al., 2019). Other related work includes Open IE methods (open information extraction), which extract tuples of expressions from text that represent the events of the sentence. Such work includes Open IE by AllenNLP (Stanovsky et al., 2018), which uses a deep BiLSTM sequence prediction model and systems that combine BERT embeddings with other neural models, such as a BiLSTM encoder (Kolluru et al., 2020). ~Predicate: rescue Tweets Time Arguments: hundred, climbers, need, avalanche Output: Extracted ~Predicate: evacuate sub-events Arguments: helicopter, IndianArmy, Everest, Input: Tweets from crisis event Construct Initial Graph camp1 DGCN (or GCN) Represents tweet content t-1 t Learns edge weights t-1 t Extract Sub-events Pattern matching on sub-graphs Predicate: nucleus Arguments: semantically connected to trigger / nucleus Fig"
C08-1109,P96-1042,0,0.0113352,"produce an overall Hit rate (0.08). Overall rates for FPs and Misses are calculated in a similar manner. 7. Using the values from step 6, calculate Precision (Hits/(Hits + FP)) and Recall (Hits/(Hits + Misses)). These are shown in the last two rows of Table 3. Hits FP Misses Precision Recall Estimated Overall Rates Sample Proportion * Sub-Corpus Proportion 0.80 * 0.10 = 0.08 0.20 * 0.10 = 0.02 0.30 * 0.90 = 0.27 0.08/(0.08 + 0.02) = 0.80 0.08/(0.08 + 0.27) = 0.23 Table 3: Sampling Calculations (Hypothetical) This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). In addition, while our methodology is used for evaluating a system, active learning is commonly used for training a system. 4.2 Application Next, we tested whether our proposed sampling approach provides good estimates of a system’s performance. For this task, we used the +Combo:word model to separate a large corpus of student essays into the “Error” and “OK” subcorpora. The original corpus total"
C08-1109,I08-1059,0,0.655628,"Missing"
C08-1109,C94-1042,0,0.332691,"ng 2008), pages 865–872 Manchester, August 2008 development and evaluation of such a system. First, we describe a machine learning system that detects preposition errors in essays of ESL writers. To date there have been relatively few attempts to address preposition error detection, though the sister task of detecting determiner errors has been the focus of more research. Our system performs comparably with other leading systems. We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al., 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. While the need for annotation by multiple raters has been well established in NLP tasks (Carletta, 1996), most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors, or to check the system’s output. Some grammatical errors, such as number disagreement between subject and verb, no doubt show very high reliability, but others, such as usage errors involving preposit"
C08-1109,P03-2026,0,0.798728,"Missing"
C08-1109,izumi-etal-2004-overview,0,0.148294,"Missing"
C08-1109,W00-0708,0,0.0557462,"Missing"
C08-1109,P06-1031,0,0.132405,"Missing"
C08-1109,J96-2004,0,0.024601,"ror detection, though the sister task of detecting determiner errors has been the focus of more research. Our system performs comparably with other leading systems. We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al., 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. While the need for annotation by multiple raters has been well established in NLP tasks (Carletta, 1996), most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors, or to check the system’s output. Some grammatical errors, such as number disagreement between subject and verb, no doubt show very high reliability, but others, such as usage errors involving prepositions or determiners are likely to be much less reliable. Our results show that relying on one rater for system evaluation can be problematic, and we provide a sampling approach which can facilitate using multiple raters for this task. In the next section, we des"
C08-1109,A00-2019,1,0.860565,"Missing"
C08-1109,W08-1205,1,0.547726,"Missing"
C08-1109,W07-1604,1,0.733173,"r detection system. This paper addresses both the 865 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 865–872 Manchester, August 2008 development and evaluation of such a system. First, we describe a machine learning system that detects preposition errors in essays of ESL writers. To date there have been relatively few attempts to address preposition error detection, though the sister task of detecting determiner errors has been the focus of more research. Our system performs comparably with other leading systems. We extend our previous work (Chodorow et al., 2007) by experimenting with combination features, as well as features derived from the Google N-Gram corpus and Comlex (Grishman et al., 1994). Second, we discuss drawbacks in current methods of annotating ESL data and evaluating error detection systems, which are not limited to preposition errors. While the need for annotation by multiple raters has been well established in NLP tasks (Carletta, 1996), most previous work in error detection has surprisingly relied on only one rater to either create an annotated corpus of learner errors, or to check the system’s output. Some grammatical errors, such"
C08-1109,N07-2045,0,0.0614255,"Missing"
C12-1038,boyd-2010-eagle,0,0.0439876,"of string distance, there is no way to talk about true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into ac"
C12-1038,D11-1010,0,0.0149267,"12: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential for progress in the field. The goal of this p"
C12-1038,N12-1067,0,0.0368454,"a string-based definition can refer to both words as a single error. (2) a. The book in my class inspire me. b. The book in my class inspires me. c. The books in my class inspire me. There are several issues involved in choosing the unit size. First, the definition of unit size affects whether a system is given credit for finding all and only the errors, as these examples illustrate. Identifying inspire as an error may or may not be sufficient; identifying both words may be overkill. Defining error detection metrics in terms of edit distance mitigates this problem for evaluation comparisons (Dahlmeier and Ng, 2012), as correcting inspire to inspires is handled the same as book ... inspire corrected to book ... inspires. In essence, edit distance measures (EDMs) compare the system output to the correct string, ignoring exactly how it was derived. 619 Moreover, EDMs naturally handle multiple, overlapping errors, a problem for systems that target only a specific error type (Gamon, 2010; Rozovskaya and Roth, 2010a). Taking an example from Dahlmeier and Ng (2012), the sequence ... development set similar with test set ... can be corrected as a preposition selection error with → to and an adjacent article omi"
C12-1038,W12-2006,0,0.47295,"et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential for progress in the field. The goal of this paper is to draw attention to the many evaluation issues in error detection that have largely been overlooked in the past and which make it hard to draw meaningful comparisons between"
C12-1038,W10-4236,0,0.0232724,"r even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential for progress in the field. The goal of this paper is to draw attention to the many evaluation issues in error detection that have largely been overlooked in the past and which make it hard to draw meaning"
C12-1038,dale-narroway-2012-framework,0,0.0329995,"Missing"
C12-1038,W11-1410,1,0.932988,"ror detection, system evaluation, evaluation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and com"
C12-1038,dickinson-ledbetter-2012-annotating,1,0.843068,"t true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into account separately (which is largely done in Dale and Narroway (2011))."
C12-1038,N10-1019,0,0.0422643,"se in real world applications. 3.4 Variety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy,"
C12-1038,W11-1422,0,0.0290787,"2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon (2011). In the two HOO Shared Tasks, P, R and F1 were all reported, but no preference was given to which one to use to rank the systems in the end. 618 Recommendations It is clear that no single measure of performance is best for all purposes; each one has its own set of advantages and disadvantages. However, all of the measures, aside from BLEU and METEOR, are based on the same four values, the counts for TP, FP, FN, and TN. We recommend reporting these four in addition to any metrics derived from them. This will enable readers to calculate other measures that the authors of a particular paper did"
C12-1038,han-etal-2010-using,1,0.627698,"e, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon (2011). In the two HOO Shared Tasks, P, R and F1 were all reported, but no preference was given to which one to use to rank the systems in the end. 618 Recommendations It is clear that no single"
C12-1038,W10-1802,0,0.053335,"istance, there is no way to talk about true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into account separately (w"
C12-1038,N12-1029,1,0.845321,"ccuracy (A), Precision (P), Recall (R), true-negative rate (TNR), and F-score (F1 ), as shown in Figure 2. System prediction Comma (+) No comma (-) Annotation (Gold Standard) Comma (+) No comma (-) TP FP FN TN Figure 1: The basis for typical NLP system evaluation T P+T N T P+T N +F P+F N TP T P+F P TP T P+F N Accuracy (A) = Precision (P)= Recall (R)= True Negative Rate (TNR) = F-measure (F1 ) = 2 · P·R P+R TN T N +F P Figure 2: Evaluation metrics 3.2 Error Detection and the Three-Way Contingency Table Now consider a task that is similar to comma restoration, the task of comma error detection (Israel et al., 2012), in which a system seeks to find and correct errors in the writer’s usage of commas. For this task, the positive class is not the presence of a comma but rather an error of the writer’s that involves a comma. Therefore, it is necessary to compare what the writer has written to an annotator’s judgment, and only if there is a mismatch between the two do we have an error (the positive class); when writer and annotator agree, the case is a non-error (the negative class). The traditional 2x2 table is no longer sufficient to represent all of the contingencies, which must instead be laid out in the"
C12-1038,P08-1021,0,0.0160292,"luation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes it hard to measure performance and compare systems, which is essential fo"
C12-1038,W12-3617,1,0.849254,"o way to talk about true negatives. Errors can be of variable sizes, but this means that the size of non-errors is, in a sense, undefined. Thus, alternative measures like κ cannot be calculated. The issue of unit size becomes more complicated when error types interact. In (3), for instance, if a learner’s preposition in should be corrected to to, then the article can be dropped. If, however, the preposition is not changed, then dropping the article makes the sentence worse. This is because one correction leads to another, as noted by many doing error annotation (Boyd, 2010; Hana et al., 2010; Lee et al., 2012; Dickinson and Ledbetter, 2012). As far as we aware, no system evaluation accounts for this problem, and we do not solve it, either. (3) Every day we go { in the school → to school } Recommendations How one treats positives depends upon one’s purpose(s). If matching to a correct string and P/R are all that is required, EDMs have several excellent properties, including being able to account for errors of variable type and size (above the level of the base unit). If feedback is desired, EDMs are not a good choice because each error type must be taken into account separately (which is largely do"
C12-1038,W03-0209,0,0.0191481,"task the system is used for, so we begin with a brief overview of applications of grammatical error detection. 2 Applications It is not surprising that most applications of grammatical error detection are in education, where it has been used for student assessment and to support language learning. A more recent set of applications focuses on improving systems within the domain of NLP itself. Automatically scoring essays Error detection is a fundamental component in most systems which perform automated essay scoring (Yannakoudakis et al., 2011; Attali and Burstein, 2006; Burstein et al., 2003; Lonsdale and Strong-Krause, 2003). The goal here is to find aspects of grammar and word usage related to overall text quality so that a holistic score, usually on a 5or 6-point scale, can be generated. Essay scoring systems also measure the range of vocabulary, discourse structure, and the mechanics of writing (e.g., spelling) as predictors of the writing score. These systems are used for large scale high-stakes tests taken by native and non-native speakers, such as the Graduate Record Exam (GRE), and for tests of non-native proficiency, such as the Test of English as a Foreign Language (TOEFL). Improving writing quality To d"
C12-1038,P11-2089,1,0.69726,"Missing"
C12-1038,P11-1094,0,0.0174037,". Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon (2011). In the two HOO Shared Tasks, P, R and F1 were all reported, but no preference was given to which one to use to rank the systems in the end. 618 Recommendations It is clear that no single measure of performance is best for all purposes; each one has its own set of advantages and disadvantages. However, all of the measures, aside from BLEU and METEOR, are based on the same four values, the counts for TP, FP, FN, and TN. We recommend reporting these four in addition to any metrics derived from them. This will e"
C12-1038,W11-2111,1,0.821062,"n order to properly model behavior (Amaral and Meurers, 2007). For both purposes, error detection is needed to detect errors, suggest corrections, and provide information about the linguistic properties of the writer’s mistakes. Applications within NLP Grammatical error detection can be a useful component in correcting and evaluating text generated in various NLP applications. Among other applications, it can be used to detect errors in machine translation (MT) output (such as in Knight and Chander (1994) and Peng and Araki (2005)) and can even be incorporated in quality metrics to assess MT (Parton et al., 2011). In these contexts, an NLP system takes the place of the writer, but the goal is similar, namely to produce more error-free language. 613 3 Measuring Performance 3.1 Traditional Evaluation Measures To illustrate the traditional measures of NLP system evaluation, consider as an example the task of comma restoration (see, e.g. Shieber and Tao, 2003), in which the commas are removed from a well-edited text (the gold standard) and a system attempts to restore them by predicting their locations. For evaluations involving a binary distinction such as this one (the presence vs. absence of a comma),"
C12-1038,E12-1035,0,0.325261,"ects (TP) to the total number of errors in the Annotated gold standard (TP+FN); P compares TP to the total number of errors that the System reports (TP+FP); and F1 is the harmonic mean of R and P. Unfortunately, all three measures are affected by the proportion of cases that are annotated as errors in the gold standard (referred to as the prevalence of the errors, which is equal to (TP+FN)/N, where N is the total number of cases, i.e., N = TP+TN+FP+FN) and by the proportion of cases that are reported by the System as errors (referred to as the bias of the System, which is equal to (TP+FP)/N). Powers (2012) demonstrates how a system that performs no better than chance will nonetheless show an increase in R when prevalence increases and an increase in P when bias increases. To understand this behavior, we must consider what it means to perform at chance. If the class labels Error and No Error are assigned to cases independently by the Annotator and the System, then these labels are expected to match a proportion of the time by chance alone a proportion equal to the product of their probabilities. For example, the expected proportion of TP matches is equal to the product of the proportion of cases"
C12-1038,W10-1004,0,0.314583,"iety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consi"
C12-1038,N10-1018,0,0.101343,"iety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consi"
C12-1038,P11-1093,0,0.0376992,"sures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some researchers have even looked towards more holistic, sentence-level metrics like BLEU and METEOR (see, e.g. Park and Levy, 2011), or modifying measures like P and R to consider entire sentences, as in Gamon"
C12-1038,N03-1029,0,0.0228136,"ed in various NLP applications. Among other applications, it can be used to detect errors in machine translation (MT) output (such as in Knight and Chander (1994) and Peng and Araki (2005)) and can even be incorporated in quality metrics to assess MT (Parton et al., 2011). In these contexts, an NLP system takes the place of the writer, but the goal is similar, namely to produce more error-free language. 613 3 Measuring Performance 3.1 Traditional Evaluation Measures To illustrate the traditional measures of NLP system evaluation, consider as an example the task of comma restoration (see, e.g. Shieber and Tao, 2003), in which the commas are removed from a well-edited text (the gold standard) and a system attempts to restore them by predicting their locations. For evaluations involving a binary distinction such as this one (the presence vs. absence of a comma), a comparison between the system’s output and the annotator’s judgments (the gold standard) can be organized as a two-by-two contingency table, shown in Figure 1. Presence of a comma is the target or positive class, and absence is the negative class. Positions in the text where both the system and the gold standard indicate that there should be a co"
C12-1038,W08-1205,1,0.759732,"e to move forward. KEYWORDS: grammatical error detection, system evaluation, evaluation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes"
C12-1038,C08-1109,1,0.657801,"e to move forward. KEYWORDS: grammatical error detection, system evaluation, evaluation metrics. Proceedings of COLING 2012: Technical Papers, pages 611–628, COLING 2012, Mumbai, December 2012. 611 1 Introduction With hundreds of millions of people worldwide learning second, or even third, languages (Leacock et al., 2010), there is a large and growing need for NLP systems that automatically detect and correct the grammar and word usage errors that learners make. In response to this need, NLP researchers have developed tools to target errors involving articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008b), particles (Dickinson et al., 2011), verb forms (Lee and Seneff, 2008), and collocations (Dahlmeier and Ng, 2011). Research in this field has surged over the last few years, culminating notably in two recent “Helping Our Own” (HOO) Shared Tasks (Dale and Kilgarriff, 2010) and (Dale et al., 2012), which are concerned with automated error correction in texts authored by non-native speakers of English working in NLP. However, despite this high level of activity and interest, there is relatively little consensus on how best to evaluate grammatical error detection/correction systems. This makes"
C12-1038,P10-2065,1,0.845196,"use, as shown above, it does not reflect A, P, or R. This limits its value for decisions about whether a system is ready for use in real world applications. 3.4 Variety in Reporting Evaluation Measures As a result of the many issues involved in evaluating grammatical error correction systems, it is perhaps unsurprising that there is little consensus about which evaluation metric to use. For example, Leacock et al. (2010) describe how, in 2008, there were three papers on preposition error detection and each used slightly different evaluation metrics making comparison impossible. More recently, Tetreault et al. (2010) and Dickinson et al. (2011) report P and R for work done on prepositions and particles, respectively. Gamon (2010) also used P and R, while Rozovskaya and Roth (2010b) and Rozovskaya and Roth (2011) reported A of usage in a test corpus before and after corrections were made by their system. Sometimes, different metrics have been used for two aspects of the same task, as in Han et al. (2010), where A was reported for determining the presence vs. absence of a preposition, but P and R were used to evaluate performance in detecting when an incorrect preposition was used by the writer. Some resear"
C12-1038,P11-1019,0,0.0332827,"uation. Certain metrics are more or less appropriate depending on the type of task the system is used for, so we begin with a brief overview of applications of grammatical error detection. 2 Applications It is not surprising that most applications of grammatical error detection are in education, where it has been used for student assessment and to support language learning. A more recent set of applications focuses on improving systems within the domain of NLP itself. Automatically scoring essays Error detection is a fundamental component in most systems which perform automated essay scoring (Yannakoudakis et al., 2011; Attali and Burstein, 2006; Burstein et al., 2003; Lonsdale and Strong-Krause, 2003). The goal here is to find aspects of grammar and word usage related to overall text quality so that a holistic score, usually on a 5or 6-point scale, can be generated. Essay scoring systems also measure the range of vocabulary, discourse structure, and the mechanics of writing (e.g., spelling) as predictors of the writing score. These systems are used for large scale high-stakes tests taken by native and non-native speakers, such as the Graduate Record Exam (GRE), and for tests of non-native proficiency, such"
C12-1158,N12-1033,0,0.0956259,"Missing"
C12-1158,brooke-hirst-2012-measuring,0,0.126261,"ich features actually perform best. A second problem is that there is no consensus on the scope of the evaluation. The ICLE contains English essays written by native speakers of 16 languages. Typically a subset of 7 languages is used in the evaluations, although more recently some work has reported results for a larger set. Moreover, when researchers report results for 7 languages, they are not always reporting on the same 7 languages. For example, in the work of Wong and Dras (2011) the 7 native languages (L1s) are Bulgarian, Chinese, Czech, French, Japanese, Russian, and Spanish. Whereas in Brooke and Hirst (2012), Italian and Polish are used instead of Bulgarian and Czech. In addition, different researchers have split the corpus in different ways when training and evaluating their systems, making it even more difficult to compare results across experiments. 1 2 Note that Kochmar (2011) used a subsection of the Cambridge Learner Corpus. Throughout this paper, we will refer to ICLE version 2 as ICLE. In this paper, we first provide an automatic method for extracting data from the ICLE corpus to remove some of the corpus-specific idiosyncracies that automatic Native Language Identification classifiers cu"
C12-1158,D11-1010,0,0.020331,"sification, Corpora. 1 Introduction One growing NLP field is that of Native Language Identification (NLI), which is the task of automatically identifying a speaker’s first language based solely on the speaker’s writing in another language. NLI can be useful for a number of applications. Native language is often used as a feature in machine learning approaches to authorship profiling (Estival et al., 2007), which is frequently used in forensic linguistics. NLI can also be used in educational settings to provide more targeted feedback to language learners about their errors (Chang et al., 2008; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011). It is well known that speakers of different languages make different kinds of errors when learning a language (Swan and Smith, 2001). For example, a French speaker learning English might write sentence (1), which contains a verb tense error. On the other hand, a Japanese speaker learning English might make the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achi"
C12-1158,de-marneffe-etal-2006-generating,0,0.0145892,"Missing"
C12-1158,P11-2038,0,0.0920066,"Missing"
C12-1158,P09-2012,0,0.0654302,"Missing"
C12-1158,P11-1093,0,0.0289759,"ntroduction One growing NLP field is that of Native Language Identification (NLI), which is the task of automatically identifying a speaker’s first language based solely on the speaker’s writing in another language. NLI can be useful for a number of applications. Native language is often used as a feature in machine learning approaches to authorship profiling (Estival et al., 2007), which is frequently used in forensic linguistics. NLI can also be used in educational settings to provide more targeted feedback to language learners about their errors (Chang et al., 2008; Dahlmeier and Ng, 2011; Rozovskaya and Roth, 2011). It is well known that speakers of different languages make different kinds of errors when learning a language (Swan and Smith, 2001). For example, a French speaker learning English might write sentence (1), which contains a verb tense error. On the other hand, a Japanese speaker learning English might make the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They"
C12-1158,1996.amta-1.36,0,0.411296,"Missing"
C12-1158,P12-2038,0,0.285243,"the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They also said to have great curiosity. There has been a great deal of work on NLI in recent years. The methods employed have ranged from some combination of lexical, part-of-speech and n-gram features (Koppel et al., 2005), to syntactic features (Wong and Dras, 2011) including Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), to topic models (Wong et al., 2011). Despite these research efforts, it has been somewhat hard to compare different approaches for a number of reasons. The first difficulty is with the evaluation data set. Evaluating an NLI system requires a corpus containing texts in a language other than the native language of the writer. Because of a scarcity of such corpora, most work1 has used the ICLEv22 for training and evaluation since it contains several hundred essays written by college-level English language learners. However, this corpus is quite small for training and testing statistical systems"
C12-1158,C08-1109,1,0.893112,"Missing"
C12-1158,N03-1033,0,0.070628,"Missing"
C12-1158,W07-0602,0,0.367516,"Missing"
C12-1158,U09-1008,0,0.299281,"Missing"
C12-1158,D11-1148,0,0.390776,"On the other hand, a Japanese speaker learning English might make the verb tense error shown in (2). A writing tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They also said to have great curiosity. There has been a great deal of work on NLI in recent years. The methods employed have ranged from some combination of lexical, part-of-speech and n-gram features (Koppel et al., 2005), to syntactic features (Wong and Dras, 2011) including Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), to topic models (Wong et al., 2011). Despite these research efforts, it has been somewhat hard to compare different approaches for a number of reasons. The first difficulty is with the evaluation data set. Evaluating an NLI system requires a corpus containing texts in a language other than the native language of the writer. Because of a scarcity of such corpora, most work1 has used the ICLEv22 for training and evaluation since it contains several hundred essays written by college-level English language learners. However"
C12-1158,U11-1015,0,0.0665164,"tutor system which can detect the native language of the learner will be able to tailor the feedback about the error and contrast it with common properties of the learner’s language. (1) She knows that she hasn’t achieve it completely. (2) They also said to have great curiosity. There has been a great deal of work on NLI in recent years. The methods employed have ranged from some combination of lexical, part-of-speech and n-gram features (Koppel et al., 2005), to syntactic features (Wong and Dras, 2011) including Tree Substitution Grammars (TSGs) (Swanson and Charniak, 2012), to topic models (Wong et al., 2011). Despite these research efforts, it has been somewhat hard to compare different approaches for a number of reasons. The first difficulty is with the evaluation data set. Evaluating an NLI system requires a corpus containing texts in a language other than the native language of the writer. Because of a scarcity of such corpora, most work1 has used the ICLEv22 for training and evaluation since it contains several hundred essays written by college-level English language learners. However, this corpus is quite small for training and testing statistical systems which makes it difficult to tell whe"
C12-1158,D12-1064,0,0.18574,"Missing"
C14-3004,W11-2838,0,0.0277637,"there has been a surge in interest in using NLP to address educational needs, which in turn, has spawned the recurring ACL/NAACL workshop “Innovative Use of Natural Language Processing for Building Educational Applications” that had its 9th edition at ACL 2014. The last three years, in particular, have been pivotal for GEC. Papers on the topic have become more commonplace at main conferences such as ACL, NAACL and EMNLP, as well as two editions of a Morgan Claypool Synthesis Series book on the topic (Leacock et al., 2010; Leacock et al., 2014). In 2011 and 2012, the first shared tasks in GEC (Dale and Kilgarriff, 2011; Dale et al., 2012) were created, and dozens of teams from all over the world participated. This was followed by two successful CoNLL Shared Tasks on the topic in 2013 and 2014 (Ng et al., 2013; Ng et al., 2014). While there have been many exciting developments in GEC over the last few years, there is still considerable room for improvement as state-of-the-art performance in detecting and correcting several important error types is still inadequate for real world applications. We hope to engage researchers from other NLP fields to develop novel and effective approaches to these problems. Our"
C14-3004,W12-2006,0,0.0154378,"nterest in using NLP to address educational needs, which in turn, has spawned the recurring ACL/NAACL workshop “Innovative Use of Natural Language Processing for Building Educational Applications” that had its 9th edition at ACL 2014. The last three years, in particular, have been pivotal for GEC. Papers on the topic have become more commonplace at main conferences such as ACL, NAACL and EMNLP, as well as two editions of a Morgan Claypool Synthesis Series book on the topic (Leacock et al., 2010; Leacock et al., 2014). In 2011 and 2012, the first shared tasks in GEC (Dale and Kilgarriff, 2011; Dale et al., 2012) were created, and dozens of teams from all over the world participated. This was followed by two successful CoNLL Shared Tasks on the topic in 2013 and 2014 (Ng et al., 2013; Ng et al., 2014). While there have been many exciting developments in GEC over the last few years, there is still considerable room for improvement as state-of-the-art performance in detecting and correcting several important error types is still inadequate for real world applications. We hope to engage researchers from other NLP fields to develop novel and effective approaches to these problems. Our tutorial is specific"
C14-3004,W13-3601,1,0.77859,"ional Applications” that had its 9th edition at ACL 2014. The last three years, in particular, have been pivotal for GEC. Papers on the topic have become more commonplace at main conferences such as ACL, NAACL and EMNLP, as well as two editions of a Morgan Claypool Synthesis Series book on the topic (Leacock et al., 2010; Leacock et al., 2014). In 2011 and 2012, the first shared tasks in GEC (Dale and Kilgarriff, 2011; Dale et al., 2012) were created, and dozens of teams from all over the world participated. This was followed by two successful CoNLL Shared Tasks on the topic in 2013 and 2014 (Ng et al., 2013; Ng et al., 2014). While there have been many exciting developments in GEC over the last few years, there is still considerable room for improvement as state-of-the-art performance in detecting and correcting several important error types is still inadequate for real world applications. We hope to engage researchers from other NLP fields to develop novel and effective approaches to these problems. Our tutorial is specifically designed to: • Introduce an NLP audience to the challenges that language learners face and thus the challenges of designing NLP tools to assist in language acquisition •"
C14-3004,W14-1701,0,0.0278812,"s” that had its 9th edition at ACL 2014. The last three years, in particular, have been pivotal for GEC. Papers on the topic have become more commonplace at main conferences such as ACL, NAACL and EMNLP, as well as two editions of a Morgan Claypool Synthesis Series book on the topic (Leacock et al., 2010; Leacock et al., 2014). In 2011 and 2012, the first shared tasks in GEC (Dale and Kilgarriff, 2011; Dale et al., 2012) were created, and dozens of teams from all over the world participated. This was followed by two successful CoNLL Shared Tasks on the topic in 2013 and 2014 (Ng et al., 2013; Ng et al., 2014). While there have been many exciting developments in GEC over the last few years, there is still considerable room for improvement as state-of-the-art performance in detecting and correcting several important error types is still inadequate for real world applications. We hope to engage researchers from other NLP fields to develop novel and effective approaches to these problems. Our tutorial is specifically designed to: • Introduce an NLP audience to the challenges that language learners face and thus the challenges of designing NLP tools to assist in language acquisition • Provide a history"
D11-1119,P10-1089,0,0.400416,"y slightly. 1 Introduction The function of context-sensitive text correction is to identify word-choice errors in text (Bergsma et al., 2009). It can be viewed as a lexical disambiguation task (Lapata and Keller, 2005), where a system selects from a predefined confusion word set, such as {affect, effect} or {complement, compliment}, and provides the most appropriate word choice given the context. Typically, one determines if a word has been used correctly based on lexical, syntactic and semantic information from the context of the word. One of the top performing models of spelling correction (Bergsma et al., 2010) is based on web-scale n-gram counts, which reflect both syntax and meaning. However, even with a large-scale n-gram corpus, data sparsity can hurt performance in two ways. ∗ This work was done when the first author was an intern for Educational Testing Service. Take a sentence from The New York Times (NYT) for example: “‘This fellow’s won a war,’ the dean of the capital’s press corps, David Broder, announced on ‘Meet the Press’ after complimenting the president on the ‘great sense of authority and command’ he exhibited in a flight suit.” Unfortunately, neither the phrase “complementing the pr"
D11-1119,W07-1604,1,0.878565,"antic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for"
D11-1119,W02-1005,0,0.0287773,"udanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model"
D11-1119,de-marneffe-etal-2006-generating,0,0.0106983,"Missing"
D11-1119,P08-2008,0,0.0201236,"ey focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et al., 2010) and verb sense disambiguation (Dligach and Palmer, 2008). In those instances where parsers have been used, they have mainly provided shallow analyses or relations involving specific target words, such as a preposition or verb. Unlike preposition errors, spelling errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the tes"
D11-1119,P97-1067,0,0.0409743,"mplimenting the president” exists in the web-scale Google N-gram corpus (Brants and Franz, 2006). The n-gram models decide solely based on the frequency of the bi-grams “after comple(i)menting” and “comple(i)menting the”, which are common usages for both words. The real question is whether we are more likely to “compliment” or “complement” a person, the “president”. Several clues could help us answer that question. A dependency parser can identify the word “president” as the subject of “compliment” or “complement” which also may be the case in some of the training data. Lexical co-occurrence (Edmonds, 1997) and semantic word relatedness measurements, such as Random Indexing (Sahlgren, 2006), could provide evidence that “compliment” is more likely to co-occur with “president” than “complement”. Fur1291 Proceedings of the 2011 Conference on Empirical Methods in Natural Language Processing, pages 1291–1300, c Edinburgh, Scotland, UK, July 27–31, 2011. 2011 Association for Computational Linguistics thermore, some important clues can be quite distant from the target word, e.g. outside the 9-word context window Bergsma et al. (2010) and Carlson (2007) used. Consider another sentence in the NYT corpus,"
D11-1119,P98-1059,0,0.0306819,"sight, site}, {peace, piece} and {raise, rise}. They reported that the SVM model with NG features outperformed its unsupervised version, sumLM. However, the limited confusion word sets they evaluated may not comprehensively represent the word usage errors that writers typically make. In this paper, we test nine additional commonly confused word pairs to expand the scope of the evaluation. These words were selected based on their lower frequencies compared to the five pairs in the above work (as shown later in Table 2). 1293 3 Enhanced N-gram Models with Parse Features To our knowledge, only (Elmi and Evans, 1998) have used parsing for spell correction. They focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et"
D11-1119,C08-1022,0,0.0241586,"Missing"
D11-1119,I08-1059,0,0.0242171,"use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In the prior work, the words for distributional similarity are constrained to the local window, and positional information of the words is not encoded. Recent work (Carlson and Fette, 2007; Gamon et al., 2008; Bergsma et al., 2009) has demonstrated that large-scale language modeling is extremely helpful for contextual spelling correction and other lexical disambiguation tasks. These systems make the word choice depending on how frequently each candidate word has been seen in the given context in web-scale data. As n-gram data has become more readily available, such as the Google N-gram Corpus, the likelihood of a word being used in a certain context can be better estimated. Bergsma et al. (2009; 2010) presented a series of simple but powerful models which relied heavily on web-scale n-gram counts."
D11-1119,W95-0104,0,0.419802,"e intended word is more likely to be semantically coherent with the context than is a spelling error. Jones and Martin (1997) made use of the semantic similarity produced by Latent Semantic Analysis. Budanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has be"
D11-1119,hermet-etal-2008-using,0,0.0660695,"Missing"
D11-1119,P03-2026,0,0.0382898,"redicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to"
D11-1119,A97-1025,0,0.0775056,"and how our approach differs from these approaches. In Sections 3 and 4, we discuss our methods for using parse features and word co-occurrence information. In Section 5, we present experimental results and analysis. 2 Related Work A variety of approaches have been proposed for context-sensitive spelling correction ranging from semantic methods to machine learning classifiers to large-scale n-gram models. Some semantics-based systems have been developed based on an intuitive assumption that the intended word is more likely to be semantically coherent with the context than is a spelling error. Jones and Martin (1997) made use of the semantic similarity produced by Latent Semantic Analysis. Budanitsky and Hirst (2001) investigated the effectiveness of predicting words based on different semantic 1292 similarity/distance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), trans"
D11-1119,W06-1605,0,0.0690266,"Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In the prior work, the words for distributional similarity are constrained to the local window, and positional information of the words is not encoded. Recent work (Carlson and Fette, 2007; Gamon et al., 2008; Bergsma et al., 2009) has demonstrated that large-scale language modeling is extremely helpful for conte"
D11-1119,N10-1018,0,0.0119634,"ny word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the test phase, we do not know which word should be put in the sentences to create parse features for test examples. Previous studies (Tetreault et al., 2010) failed to discuss this issue. 2) Some existing work (Whitelaw et al., 2009; Rozovskaya and Roth, 2010) in the text correction field introduced artificial errors into training data to adapt the system to better handle ill-formed text. But this method will encounter serious data sparsity problems when facing rare words. 3.1 Baseline System We chose one of the leading spelling correction systems, (Bergsma et al., 2010), as our primary baseline. As noted earlier, it is an SVM-based system combining web-scale n-gram counts (NG) and contextual words (LEX) as features. To simplify the explanation, throughout the paper, we will only consider the situation with two confusion words. The problem with mor"
D11-1119,C08-1109,1,0.808685,"istance measures in WordNet. Both systems report performance that is lower than systems developed more recently. A variety of machine-learning methods have been proposed in spelling correction and preposition and article error correction fields, such as Bayesian classifiers (Golding, 1995; Golding and Roth, 1996), Winnow-based learning (Golding and Roth, 1999), decision lists (Golding, 1995), transformation-based learning (Mangu and Brill, 1997), augmented mixture models (Cucerzan and Yarowsky, 2002) and maximum entropy classifiers (Izumi et al., 2003; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008; Felice and Pulman, 2008). Despite their differences, these approaches mainly use contextual features to capture the lexical, semantic and/or syntactic environment of the target word. The use of distributional similarity measures for spelling correction has been previously explored in (Mohammad and Hist, 2006). In our work, distributional similarity is not the primary contribution but we show the impact it can have when used in conjunction with a large scale n-gram model and with parse features, which allows the system to select words outside the local window for distributional similarity. In"
D11-1119,P10-2065,1,0.802253,"Evans, 1998) have used parsing for spell correction. They focus on using a parser as a filter to discriminate between possible real-world corrections where the part-ofspeech differs. In our work, we show that parse features are effective when used directly in the classification mode (as opposed to as a final filter) to select the best correction regardless of whether or not the part-of-speech of the choices differ. Statistical parsers have also seen limited use in the sister tasks of preposition and article error detection (Hermet et al., 2008; Lee and Knutsson, 2008; Felice and Pulman, 2009; Tetreault et al., 2010) and verb sense disambiguation (Dligach and Palmer, 2008). In those instances where parsers have been used, they have mainly provided shallow analyses or relations involving specific target words, such as a preposition or verb. Unlike preposition errors, spelling errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by differ"
D11-1119,D09-1093,0,0.0288853,"g errors can occur in any word. In this paper, we propose a novel way to incorporate the parse into spelling correction, applying the parser to sentences filled by each candidate word equivalently and extracting salient features. This overcomes two problem in the existing methods: 1) the parse trees of the same sentence filled by different confusion words can be different. However, in the test phase, we do not know which word should be put in the sentences to create parse features for test examples. Previous studies (Tetreault et al., 2010) failed to discuss this issue. 2) Some existing work (Whitelaw et al., 2009; Rozovskaya and Roth, 2010) in the text correction field introduced artificial errors into training data to adapt the system to better handle ill-formed text. But this method will encounter serious data sparsity problems when facing rare words. 3.1 Baseline System We chose one of the leading spelling correction systems, (Bergsma et al., 2010), as our primary baseline. As noted earlier, it is an SVM-based system combining web-scale n-gram counts (NG) and contextual words (LEX) as features. To simplify the explanation, throughout the paper, we will only consider the situation with two confusion"
D11-1119,C98-1057,0,\N,Missing
D13-1013,J13-1002,0,0.021583,"parser is useful for spoken dialogue systems which typically encounter disfluent speech and require accurate syntactic structures. The model is completely flexible with adding other features (either text or speech features). There are still many ways of improving this framework such as using k-beam training and decoding, using prosodic and acoustic features, using out of domain data for improving the language and parsing models, and merging the two classifiers into one through better feature engineering. It is worth noting that we put the dummy root word in the first position of the sentence. Ballesteros and Nivre (2013) show that parser accuracy can improve by changing that position for English. One of the main challenges in this problem is that most of the training instances are not disfluent and thus the sample space is very sparse. As seen in the experiments, we can get further improvements by modifying the weight updates in the Perceptron learner. In future work, we will explore different learning algorithms which can help us address the sparsity problem and improve the model accuracy. Another challenge is related to the parser speed, since the number of candidates and features are much greater than the"
D13-1013,N01-1016,0,0.651849,"and speech first (Nakatani and Hirschberg, 1993). In the first approach, all prosodic and acoustic cues are ignored while in the second approach both grammatical and acoustic features are considered. For this paper, we focus on developing a text-first approach but our model is easily flexible with speech-first features because there is no restriction on the number and types of features in our model. Among text-first approaches, the work is split between developing systems which focus specifically on disfluency detection and those which couple disfluency detection with parsing. For the former, Charniak and Johnson (2001) employ a linear classifier to predict the edited phrases in Switchboard corpus (Godfrey et al., 1992). Johnson and Charniak (2004) use a TAG-based noisy channel model to detect disfluencies while parsing with getting nbest parses from each sentence and re-ranking with a language model. The original TAG parser is not used for parsing itself and it is used just to find rough copies in the sentence. Their method achieves promising results on detecting edited words but at the expense of speed (the parser has a complexity of O(N 5 ). Kahn et al. (2005) use the same TAG model and add semi-automatic"
D13-1013,W02-1001,0,0.0563939,"heir size. Unfortunately, the disfluencies marked in the dps files are not exactly the same as those marked in the corresponding mrg files. Hence, our result is not completely comparable to previous work except for (Kahn et al., 2005; Lease and Johnson, 2006; Miller and Schuler, 2008). We use Tsurgeon (Levy and Andrew, 2006) for extracting sentences from mrg files and use the Penn2Malt tool5 to convert them to dependencies. Afterwards, we provide dependency trees with disfluent words being the dependent of nothing. Learning For the first classifier, we use averaged structured Perceptron (AP) (Collins, 2002) with a minor modification. Since the first classifier data is heavily biased towards the “regular label”, we modify the weight updates in the original algorithm to 2 (original is 1) for the cases where a “reparandum” is wrongly recognized as another label. We call the modified version “weighted averaged Perceptron (WAP)”. We see that this simple modification improves the model accuracy.6 For the second classifier (parser), we use the original averaged structured Perceptron algorithm. We report results on both AP and WAP versions of the parser. Features Since for every state in the parser conf"
D13-1013,N09-2028,0,0.349221,"tence and re-ranking with a language model. The original TAG parser is not used for parsing itself and it is used just to find rough copies in the sentence. Their method achieves promising results on detecting edited words but at the expense of speed (the parser has a complexity of O(N 5 ). Kahn et al. (2005) use the same TAG model and add semi-automatically extracted prosodic features. Zwarts and Johnson (2011) improve the performance of TAG model by adding external language modeling information from data sets such as Gigaword in addition to using minimal expected Floss in n-best re-ranking. Georgila (2009) uses integer linear programming combined with CRF for learning disfluencies. That work shows that ILP can learn local and global constraints to improve the performance significantly. Qian and Liu (2013) achieve the best performance on the Switchboard corpus (Godfrey et al., 1992) without any additional data. They use three steps for detecting disfluencies using weighted Max-Margin Markov (M 3 ) network: detecting fillers, detecting edited words, and refining errors in previous steps. Some text-first approaches treat parsing and disfluency detection jointly, though the models differ in the typ"
D13-1013,P04-1005,0,0.899263,"ith its repair (“to Denver”), a filled pause (“uh”) and discourse marker (“I ∗ The first author worked on this project while he was a research intern in CoreNL research group, NLU lab, Nuance Communications, Sunnyvale, CA. FP DM Repair Filled pauses and discourse markers are to some extent a fixed and closed set. The main challenge in finding disfluencies is the case where the edited phrase is neither a rough copy of its repair or has any repair phrase (i.e. discarded edited phrase). Hence, in previous work, researchers report their method performance on detecting edited phrases (reparandum) (Johnson and Charniak, 2004). In contrast to most previous work which focuses solely on either detection or on parsing, we introduce a novel framework for jointly parsing sentences with disfluencies. To our knowledge, our work is the first model that is based on joint dependency and disfluency detection. We show that our model is robust enough to detect disfluencies with high accuracy, while still maintaining a high level of dependency parsing accuracy that approaches the upper bound. Additionally, our model outperforms prior work on joint parsing and disfluency detection on the disfluency detection task, and improves up"
D13-1013,H05-1030,0,0.545709,"ection with parsing. For the former, Charniak and Johnson (2001) employ a linear classifier to predict the edited phrases in Switchboard corpus (Godfrey et al., 1992). Johnson and Charniak (2004) use a TAG-based noisy channel model to detect disfluencies while parsing with getting nbest parses from each sentence and re-ranking with a language model. The original TAG parser is not used for parsing itself and it is used just to find rough copies in the sentence. Their method achieves promising results on detecting edited words but at the expense of speed (the parser has a complexity of O(N 5 ). Kahn et al. (2005) use the same TAG model and add semi-automatically extracted prosodic features. Zwarts and Johnson (2011) improve the performance of TAG model by adding external language modeling information from data sets such as Gigaword in addition to using minimal expected Floss in n-best re-ranking. Georgila (2009) uses integer linear programming combined with CRF for learning disfluencies. That work shows that ILP can learn local and global constraints to improve the performance significantly. Qian and Liu (2013) achieve the best performance on the Switchboard corpus (Godfrey et al., 1992) without any a"
D13-1013,N06-2019,0,0.807775,"g combined with CRF for learning disfluencies. That work shows that ILP can learn local and global constraints to improve the performance significantly. Qian and Liu (2013) achieve the best performance on the Switchboard corpus (Godfrey et al., 1992) without any additional data. They use three steps for detecting disfluencies using weighted Max-Margin Markov (M 3 ) network: detecting fillers, detecting edited words, and refining errors in previous steps. Some text-first approaches treat parsing and disfluency detection jointly, though the models differ in the type of parse formalism employed. Lease and Johnson (2006) use a PCFG-based parser to parse 125 sentences along with finding edited phrases. Miller and Schuler (2008) use a right-corner transform of binary branching structures on bracketed sentences but their results are much worse than (Johnson and Charniak, 2004). To date, none of the prior joint approaches have used a dependency formalism. 3 Joint Parsing Model We model the problem using a deterministic transition-based parser (Nivre, 2008). These parsers have the advantage of being very accurate while being able to parse a sentence in linear time. An additional advantage is that they can use as m"
D13-1013,levy-andrew-2006-tregex,0,0.012486,"turned into lower-case. The main difference with previous work is that we use Switchboard mrg files for training and testing our model (since they contain parse trees) instead of the more commonly used Swithboard dps text files. Mrg files are a subset of dps files with about more than half of their size. Unfortunately, the disfluencies marked in the dps files are not exactly the same as those marked in the corresponding mrg files. Hence, our result is not completely comparable to previous work except for (Kahn et al., 2005; Lease and Johnson, 2006; Miller and Schuler, 2008). We use Tsurgeon (Levy and Andrew, 2006) for extracting sentences from mrg files and use the Penn2Malt tool5 to convert them to dependencies. Afterwards, we provide dependency trees with disfluent words being the dependent of nothing. Learning For the first classifier, we use averaged structured Perceptron (AP) (Collins, 2002) with a minor modification. Since the first classifier data is heavily biased towards the “regular label”, we modify the weight updates in the original algorithm to 2 (original is 1) for the cases where a “reparandum” is wrongly recognized as another label. We call the modified version “weighted averaged Percep"
D13-1013,P08-2027,0,0.481809,"nts to improve the performance significantly. Qian and Liu (2013) achieve the best performance on the Switchboard corpus (Godfrey et al., 1992) without any additional data. They use three steps for detecting disfluencies using weighted Max-Margin Markov (M 3 ) network: detecting fillers, detecting edited words, and refining errors in previous steps. Some text-first approaches treat parsing and disfluency detection jointly, though the models differ in the type of parse formalism employed. Lease and Johnson (2006) use a PCFG-based parser to parse 125 sentences along with finding edited phrases. Miller and Schuler (2008) use a right-corner transform of binary branching structures on bracketed sentences but their results are much worse than (Johnson and Charniak, 2004). To date, none of the prior joint approaches have used a dependency formalism. 3 Joint Parsing Model We model the problem using a deterministic transition-based parser (Nivre, 2008). These parsers have the advantage of being very accurate while being able to parse a sentence in linear time. An additional advantage is that they can use as many nonlocal and local features as needed. Arc-Eager Algorithm We use the arc-eager algorithm (Nivre, 2004)"
D13-1013,P93-1007,0,0.20117,"ious work on disfluency detection. §3 describes our model. Experiments are described in §4 and Conclusions are made in §5. 1 In the literature, edited words are also known as “reparandum”, and the fillers are known as “interregnum”. Filled pauses are also called “Interjections”. 124 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 124–129, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics 2 Related Work Disfluency detection approaches can be divided into two different groups: text-first and speech first (Nakatani and Hirschberg, 1993). In the first approach, all prosodic and acoustic cues are ignored while in the second approach both grammatical and acoustic features are considered. For this paper, we focus on developing a text-first approach but our model is easily flexible with speech-first features because there is no restriction on the number and types of features in our model. Among text-first approaches, the work is split between developing systems which focus specifically on disfluency detection and those which couple disfluency detection with parsing. For the former, Charniak and Johnson (2001) employ a linear clas"
D13-1013,W04-0308,0,0.108267,"chuler (2008) use a right-corner transform of binary branching structures on bracketed sentences but their results are much worse than (Johnson and Charniak, 2004). To date, none of the prior joint approaches have used a dependency formalism. 3 Joint Parsing Model We model the problem using a deterministic transition-based parser (Nivre, 2008). These parsers have the advantage of being very accurate while being able to parse a sentence in linear time. An additional advantage is that they can use as many nonlocal and local features as needed. Arc-Eager Algorithm We use the arc-eager algorithm (Nivre, 2004) which is a bottom-up parsing strategy that is used in greedy and k-beam transitionbased parsers. One advantage of this strategy is that the words can get a head from their left side, before getting right dependents. This is particularly beneficial for our task, since we know that reparanda are similar to their repairs. Hence, a reparandum may get its head but whenever the parser faces a repair, it removes the reparandum from the sentence and continues its actions. The actions in an arc-eager parsing algorithm are: • Left-arc (LA): The first word in the buffer becomes the head of the top word"
D13-1013,J08-4003,0,0.0214778,"steps. Some text-first approaches treat parsing and disfluency detection jointly, though the models differ in the type of parse formalism employed. Lease and Johnson (2006) use a PCFG-based parser to parse 125 sentences along with finding edited phrases. Miller and Schuler (2008) use a right-corner transform of binary branching structures on bracketed sentences but their results are much worse than (Johnson and Charniak, 2004). To date, none of the prior joint approaches have used a dependency formalism. 3 Joint Parsing Model We model the problem using a deterministic transition-based parser (Nivre, 2008). These parsers have the advantage of being very accurate while being able to parse a sentence in linear time. An additional advantage is that they can use as many nonlocal and local features as needed. Arc-Eager Algorithm We use the arc-eager algorithm (Nivre, 2004) which is a bottom-up parsing strategy that is used in greedy and k-beam transitionbased parsers. One advantage of this strategy is that the words can get a head from their left side, before getting right dependents. This is particularly beneficial for our task, since we know that reparanda are similar to their repairs. Hence, a re"
D13-1013,N13-1102,0,0.722919,"-ofdomain data. We show that our method, based on transition-based parsing, performs at a high level of accuracy for both the parsing and disfluency detection tasks. Additionally, our method is the fastest for the joint task, running in linear time. 1 Joel Tetreault Nuance Communications, Inc. Sunnyvale, CA joel.tetreault@nuance.com Interregnum }| { z I mean uh I want a flight |to Boston {z } |{z} |{z } to |Denver {z } Reparandum Introduction Detecting disfluencies in spontaneous speech has been widely studied by researchers in different communities including natural language processing (e.g. Qian and Liu (2013)), speech processing (e.g. Wang et al. (2013)) and psycholinguistics (e.g. Finlayson and Corley (2012)). While the percentage of spoken words which are disfluent is typically not more than ten percent (Bortfeld et al., 2001), this additional “noise” makes it much harder for spoken language systems to predict the correct structure of the sentence. Disfluencies can be filled pauses (e.g. “uh”, “um”, “huh”), discourse markers (e.g. “you know”, “I mean”) or edited words which are repeated or corrected by the speaker. For example, in the following sentence, an edited phrase or reparandum interval ("
D13-1013,N13-1083,0,0.0587479,"on transition-based parsing, performs at a high level of accuracy for both the parsing and disfluency detection tasks. Additionally, our method is the fastest for the joint task, running in linear time. 1 Joel Tetreault Nuance Communications, Inc. Sunnyvale, CA joel.tetreault@nuance.com Interregnum }| { z I mean uh I want a flight |to Boston {z } |{z} |{z } to |Denver {z } Reparandum Introduction Detecting disfluencies in spontaneous speech has been widely studied by researchers in different communities including natural language processing (e.g. Qian and Liu (2013)), speech processing (e.g. Wang et al. (2013)) and psycholinguistics (e.g. Finlayson and Corley (2012)). While the percentage of spoken words which are disfluent is typically not more than ten percent (Bortfeld et al., 2001), this additional “noise” makes it much harder for spoken language systems to predict the correct structure of the sentence. Disfluencies can be filled pauses (e.g. “uh”, “um”, “huh”), discourse markers (e.g. “you know”, “I mean”) or edited words which are repeated or corrected by the speaker. For example, in the following sentence, an edited phrase or reparandum interval (“to Boston”) occurs with its repair (“to Denv"
D13-1013,P11-2033,0,0.0662816,"sifier (parser), we use the original averaged structured Perceptron algorithm. We report results on both AP and WAP versions of the parser. Features Since for every state in the parser configuration, there are many candidates for being disfluent; we use local features as well as global features for the first classifier. Global features are mostly useful for discriminating between the four actions and local features are mostly useful for choosing a phrase as a candidate for being a disfluent phrase. The features are described in Figure 2. For the second classifier, we use the same features as (Zhang and Nivre, 2011, Table 1) except that we train our 4 E.g. I want t- go to school. 5 http://stp.lingfil.uu.se/˜nivre/ research/Penn2Malt.html 6 This is similar to WM3 N in (Qian and Liu, 2013). Global Features First n words inside/outside buffer (n=1:4) First n POS i/o buffer (n=1:6) Are n words i/o buffer equal? (n=1:4) Are n POS i/o buffer equal? (n=1:4) n last FG transitions (n=1:5) n last transitions (n=1:5) n last FG transitions + first POS in the buffer (n=1:5) n last transitions + first POS in the buffer (n=1:5) (n+m)-gram of m/n POS i/o buffer (n,m=1:4) Refined (n+m)-gram of m/n POS i/o buffer (n,m=1:"
D13-1013,P11-1071,0,0.404779,"dict the edited phrases in Switchboard corpus (Godfrey et al., 1992). Johnson and Charniak (2004) use a TAG-based noisy channel model to detect disfluencies while parsing with getting nbest parses from each sentence and re-ranking with a language model. The original TAG parser is not used for parsing itself and it is used just to find rough copies in the sentence. Their method achieves promising results on detecting edited words but at the expense of speed (the parser has a complexity of O(N 5 ). Kahn et al. (2005) use the same TAG model and add semi-automatically extracted prosodic features. Zwarts and Johnson (2011) improve the performance of TAG model by adding external language modeling information from data sets such as Gigaword in addition to using minimal expected Floss in n-best re-ranking. Georgila (2009) uses integer linear programming combined with CRF for learning disfluencies. That work shows that ILP can learn local and global constraints to improve the performance significantly. Qian and Liu (2013) achieve the best performance on the Switchboard corpus (Godfrey et al., 1992) without any additional data. They use three steps for detecting disfluencies using weighted Max-Margin Markov (M 3 ) n"
D16-1228,K16-1021,0,0.025738,"This supports the use of interpolation to improve GEC evaluation in any setting. This work is the first exploration into applying fluency-based metrics to GEC evaluation. We believe that, for future work, fluency measures could be further improved with other methods, such as using existing GEC systems to detect errors, or even using an ensemble of systems to improve coverage (indeed, ensembles have been useful in the GEC task itself (Susanto et al., 2014)). There is also recent work from the MT community, such as the use of confidence bounds (Graham and Liu, 2016) or uncertainty measurement (Beck et al., 2016), which could be adopted by the GEC community. Finally, in the course of our experiments, we determined that metrics calculated on the sentencelevel is more reliable for evaluating GEC output, and we suggest that the GEC community adopt this modification to better assess systems. To facilitate GEC evaluation, we have set up an online platform8 for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available.9 Acknowledgments We would like to thank Matt Post, Martin Chodorow, and the three anonymous reviews for"
D16-1228,P15-1068,0,0.309246,"ated based on comparison to reference corrections. These Reference-Based Metrics (RBMs) credit corrections seen in the references and penalize systems for ignoring errors and making bad changes (changing a span of text in an ungrammatical way or introducing errors to grammatical text). However, RBMs make two strong assumptions: that the annotations in the references are correct and that they are complete. We argue that these assumptions are invalid and point to a deficit in current evaluation practices. In GEC, the agreement between raters can be low due to the challenging nature of the task (Bryant and Ng, 2015; Rozovskaya and Roth, 2010; Tetreault and Chodorow, 2008), indicating that annotations may not be correct or complete. An exhaustive list of all possible corrections would be time-consuming, if not impossible. As a result, RBMs penalize output that has a valid correction that is not present in the references or that addresses an error not corrected in the references. The example in §1 has low GLEU and M2 scores, even though the output addresses two errors (GLEU=0.43 and M2 = 0.00, in the bottom half and quartile of 15k system outputs, respectively). To address these concerns, we propose three"
D16-1228,W12-3102,0,0.0600429,"Missing"
D16-1228,N12-1067,0,0.132679,"ormance by including features derived from MT metrics (BLEU, TERp, and METEOR). Within the GEC field, recent shared tasks have prompted the development and scrutiny of new metrics for evaluating GEC systems. The Helping Our Own shared tasks evaluated systems using precision, recall, and F-score against annotated gold-standard corrections (Dale and Kilgarriff, 2011; Dale et al., 2012). The subsequent CoNLL Shared Tasks on GEC (Ng et al., 2013; Ng et al., 2014) were scored with the MaxMatch metric (M2 ), which captures word- and phrase-level edits by calculating the Fscore over an edit lattice (Dahlmeier and Ng, 2012). Felice and Briscoe (2015) identified shortcomings of M2 and proposed I-measure to address these issues. I-measure computes the accuracy of a token-level alignment between the original, generated, and goldstandard sentences. These precision- and recallbased metrics measure fluency and adequacy by penalizing inappropriate changes, which alter meaning or introduce other errors. Changes consistent with the annotations indicate improved fluency and no change in meaning. Unlike these metrics, GLEU scores output by penalizing n-grams found in the input and output but not the reference (Napoles et a"
D16-1228,W11-2838,0,0.0715121,"the output (Specia et al., 2013). Most closely related to our work, Parton et al. (2011) applied features from Educational Testing Service’s R e-rater (Attali and Burstein, 2006) to evaluate MT output with a ranking SVM, without references, and improved performance by including features derived from MT metrics (BLEU, TERp, and METEOR). Within the GEC field, recent shared tasks have prompted the development and scrutiny of new metrics for evaluating GEC systems. The Helping Our Own shared tasks evaluated systems using precision, recall, and F-score against annotated gold-standard corrections (Dale and Kilgarriff, 2011; Dale et al., 2012). The subsequent CoNLL Shared Tasks on GEC (Ng et al., 2013; Ng et al., 2014) were scored with the MaxMatch metric (M2 ), which captures word- and phrase-level edits by calculating the Fscore over an edit lattice (Dahlmeier and Ng, 2012). Felice and Briscoe (2015) identified shortcomings of M2 and proposed I-measure to address these issues. I-measure computes the accuracy of a token-level alignment between the original, generated, and goldstandard sentences. These precision- and recallbased metrics measure fluency and adequacy by penalizing inappropriate changes, which alte"
D16-1228,W12-2006,0,0.0377927,"2013). Most closely related to our work, Parton et al. (2011) applied features from Educational Testing Service’s R e-rater (Attali and Burstein, 2006) to evaluate MT output with a ranking SVM, without references, and improved performance by including features derived from MT metrics (BLEU, TERp, and METEOR). Within the GEC field, recent shared tasks have prompted the development and scrutiny of new metrics for evaluating GEC systems. The Helping Our Own shared tasks evaluated systems using precision, recall, and F-score against annotated gold-standard corrections (Dale and Kilgarriff, 2011; Dale et al., 2012). The subsequent CoNLL Shared Tasks on GEC (Ng et al., 2013; Ng et al., 2014) were scored with the MaxMatch metric (M2 ), which captures word- and phrase-level edits by calculating the Fscore over an edit lattice (Dahlmeier and Ng, 2012). Felice and Briscoe (2015) identified shortcomings of M2 and proposed I-measure to address these issues. I-measure computes the accuracy of a token-level alignment between the original, generated, and goldstandard sentences. These precision- and recallbased metrics measure fluency and adequacy by penalizing inappropriate changes, which alter meaning or introdu"
D16-1228,N15-1060,0,0.279212,"ures derived from MT metrics (BLEU, TERp, and METEOR). Within the GEC field, recent shared tasks have prompted the development and scrutiny of new metrics for evaluating GEC systems. The Helping Our Own shared tasks evaluated systems using precision, recall, and F-score against annotated gold-standard corrections (Dale and Kilgarriff, 2011; Dale et al., 2012). The subsequent CoNLL Shared Tasks on GEC (Ng et al., 2013; Ng et al., 2014) were scored with the MaxMatch metric (M2 ), which captures word- and phrase-level edits by calculating the Fscore over an edit lattice (Dahlmeier and Ng, 2012). Felice and Briscoe (2015) identified shortcomings of M2 and proposed I-measure to address these issues. I-measure computes the accuracy of a token-level alignment between the original, generated, and goldstandard sentences. These precision- and recallbased metrics measure fluency and adequacy by penalizing inappropriate changes, which alter meaning or introduce other errors. Changes consistent with the annotations indicate improved fluency and no change in meaning. Unlike these metrics, GLEU scores output by penalizing n-grams found in the input and output but not the reference (Napoles et al., 2015). Like BLEU (Papin"
D16-1228,finch-etal-2004-automatic,0,0.0340237,"to the metric rankings (Grundkiewicz et al., 2015; Napoles et al., 2015). In this section, we compare each metric’s ranking to the human ranking of Grundkiewicz et al. (2015, Table 3c). We use 20 references for scoring with RBMs: 2 original references, 10 references collected by Bryant and Ng (2015), and 8 references collected by Sakaguchi et al. (2016). The motivations for using 20 references are twofold: the best GEC evaluation method uses these 20 references with the GLEU metric (Sakaguchi et al., 2016), and work in machine translation shows that more references are better for evaluation (Finch et al., 2004). Due to the low agreement discussed in §3, having more references can be beneficial for evaluating a system when there are multiple viable ways of correcting a sentence. Unlike previous GEC evaluations, all metrics reported here use the mean of the sentence-level scores for each system. Results are presented in Table 1. The error-count metrics, ER and LT, have stronger correlation than all RBMs except for GLEU, which is the current state of the art. GLEU has the strongest correlation with the human ranking (ρ = 0.852, r = 0.838), followed closely by ER, which has slightly lower Pearson correl"
D16-1228,N16-1001,0,0.0329128,"ronger correlation than any uninterpolated metric. This supports the use of interpolation to improve GEC evaluation in any setting. This work is the first exploration into applying fluency-based metrics to GEC evaluation. We believe that, for future work, fluency measures could be further improved with other methods, such as using existing GEC systems to detect errors, or even using an ensemble of systems to improve coverage (indeed, ensembles have been useful in the GEC task itself (Susanto et al., 2014)). There is also recent work from the MT community, such as the use of confidence bounds (Graham and Liu, 2016) or uncertainty measurement (Beck et al., 2016), which could be adopted by the GEC community. Finally, in the course of our experiments, we determined that metrics calculated on the sentencelevel is more reliable for evaluating GEC output, and we suggest that the GEC community adopt this modification to better assess systems. To facilitate GEC evaluation, we have set up an online platform8 for benchmarking system output on the same set of sentences evaluated using different metrics and made the code for calculating LT and LFM available.9 Acknowledgments We would like to thank Matt Post, Martin"
D16-1228,D15-1052,0,0.73243,"M2 0.21 ± 0.34, IM 0.10±0.30, ER 0.91±0.10, LFM 0.50±0.16, LT 1.00±0.01. 2111 Metric GLEU ER LT I-measure LFM M2 Spearman’s ρ 0.852 0.852 0.808 0.769 0.780 0.648 Pearson’s r 0.838 0.829 0.811 0.753 0.742 0.641 Table 1: Correlation between the human and metric rankings. 4 Experiments To assess the proposed metrics, we apply the RBMs, GBMs, and interpolated metrics to score the output of 12 systems participating in the CoNLL-2014 Shared Task on GEC (Ng et al., 2014). Recent works have evaluated RBMs by collecting human rankings of these system outputs and comparing them to the metric rankings (Grundkiewicz et al., 2015; Napoles et al., 2015). In this section, we compare each metric’s ranking to the human ranking of Grundkiewicz et al. (2015, Table 3c). We use 20 references for scoring with RBMs: 2 original references, 10 references collected by Bryant and Ng (2015), and 8 references collected by Sakaguchi et al. (2016). The motivations for using 20 references are twofold: the best GEC evaluation method uses these 20 references with the GLEU metric (Sakaguchi et al., 2016), and work in machine translation shows that more references are better for evaluation (Finch et al., 2004). Due to the low agreement disc"
D16-1228,P14-2029,1,0.823958,"Missing"
D16-1228,P15-2097,1,0.876426,"nd Ng, 2012). Felice and Briscoe (2015) identified shortcomings of M2 and proposed I-measure to address these issues. I-measure computes the accuracy of a token-level alignment between the original, generated, and goldstandard sentences. These precision- and recallbased metrics measure fluency and adequacy by penalizing inappropriate changes, which alter meaning or introduce other errors. Changes consistent with the annotations indicate improved fluency and no change in meaning. Unlike these metrics, GLEU scores output by penalizing n-grams found in the input and output but not the reference (Napoles et al., 2015). Like BLEU (Papineni et al., 2002), GLEU captures both fluency and adequacy with n-gram overlap. Recent work has shown that GLEU has the strongest correlation with human judgments compared to the GEC metrics described above (Sakaguchi et al., 2016). These GEC metrics are all defined at the corpus level, meaning that the statistics are accumulated over the entire output and then used to calculate a single system score. 2110 3 Explicitly evaluating grammaticality GLEU, I-measure, and M2 are calculated based on comparison to reference corrections. These Reference-Based Metrics (RBMs) credit corr"
D16-1228,W13-3601,1,0.785495,"applied features from Educational Testing Service’s R e-rater (Attali and Burstein, 2006) to evaluate MT output with a ranking SVM, without references, and improved performance by including features derived from MT metrics (BLEU, TERp, and METEOR). Within the GEC field, recent shared tasks have prompted the development and scrutiny of new metrics for evaluating GEC systems. The Helping Our Own shared tasks evaluated systems using precision, recall, and F-score against annotated gold-standard corrections (Dale and Kilgarriff, 2011; Dale et al., 2012). The subsequent CoNLL Shared Tasks on GEC (Ng et al., 2013; Ng et al., 2014) were scored with the MaxMatch metric (M2 ), which captures word- and phrase-level edits by calculating the Fscore over an edit lattice (Dahlmeier and Ng, 2012). Felice and Briscoe (2015) identified shortcomings of M2 and proposed I-measure to address these issues. I-measure computes the accuracy of a token-level alignment between the original, generated, and goldstandard sentences. These precision- and recallbased metrics measure fluency and adequacy by penalizing inappropriate changes, which alter meaning or introduce other errors. Changes consistent with the annotations in"
D16-1228,W14-1701,0,0.612106,"ng reference-based evaluation metrics. By interpolating both methods, we achieve state-of-the-art correlation with human judgments. Finally, we show that GEC metrics are much more reliable when they are calculated at the sentence level instead of the corpus level. We have set up a CodaLab site for benchmarking GEC output using a common dataset and different evaluation metrics. 1 Introduction Grammatical error correction (GEC) has been evaluated by comparing the changes made by a system to the corrections made in gold-standard annotations. Following the recent shared tasks in this field (e.g., Ng et al. (2014)), several papers have critiqued GEC metrics and proposed new methods. Existing metrics depend on gold-standard corrections and therefore have a notable weakness: systems are penalized for making corrections that do not appear in the references.1 For example, the following output has low metric scores even though three appropriate corrections were made to the input: 1 We refer to the gold-standard corrections as references because gold standard suggests just one accurate correction. These changes (in red) were not seen in the references and therefore the metrics GLEU and M2 (described in §2) s"
D16-1228,P02-1040,0,0.120543,"2015) identified shortcomings of M2 and proposed I-measure to address these issues. I-measure computes the accuracy of a token-level alignment between the original, generated, and goldstandard sentences. These precision- and recallbased metrics measure fluency and adequacy by penalizing inappropriate changes, which alter meaning or introduce other errors. Changes consistent with the annotations indicate improved fluency and no change in meaning. Unlike these metrics, GLEU scores output by penalizing n-grams found in the input and output but not the reference (Napoles et al., 2015). Like BLEU (Papineni et al., 2002), GLEU captures both fluency and adequacy with n-gram overlap. Recent work has shown that GLEU has the strongest correlation with human judgments compared to the GEC metrics described above (Sakaguchi et al., 2016). These GEC metrics are all defined at the corpus level, meaning that the statistics are accumulated over the entire output and then used to calculate a single system score. 2110 3 Explicitly evaluating grammaticality GLEU, I-measure, and M2 are calculated based on comparison to reference corrections. These Reference-Based Metrics (RBMs) credit corrections seen in the references and"
D16-1228,W11-2111,1,0.84553,"–2115, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics Quality estimation targets fluency by estimating the amount of post-editing needed by the output. This has been the topic of recent shared tasks, e.g. Bojar et al. (2015). Specia et al. (2010) evaluated the quality of translations using sentence-level features from the output but not the references, predicting discrete and continuous scores. A strong baseline, QuEst, uses support vector regression trained over 17 features extracted from the output (Specia et al., 2013). Most closely related to our work, Parton et al. (2011) applied features from Educational Testing Service’s R e-rater (Attali and Burstein, 2006) to evaluate MT output with a ranking SVM, without references, and improved performance by including features derived from MT metrics (BLEU, TERp, and METEOR). Within the GEC field, recent shared tasks have prompted the development and scrutiny of new metrics for evaluating GEC systems. The Helping Our Own shared tasks evaluated systems using precision, recall, and F-score against annotated gold-standard corrections (Dale and Kilgarriff, 2011; Dale et al., 2012). The subsequent CoNLL Shared Tasks on GEC ("
D16-1228,W10-1004,0,0.0260497,"son to reference corrections. These Reference-Based Metrics (RBMs) credit corrections seen in the references and penalize systems for ignoring errors and making bad changes (changing a span of text in an ungrammatical way or introducing errors to grammatical text). However, RBMs make two strong assumptions: that the annotations in the references are correct and that they are complete. We argue that these assumptions are invalid and point to a deficit in current evaluation practices. In GEC, the agreement between raters can be low due to the challenging nature of the task (Bryant and Ng, 2015; Rozovskaya and Roth, 2010; Tetreault and Chodorow, 2008), indicating that annotations may not be correct or complete. An exhaustive list of all possible corrections would be time-consuming, if not impossible. As a result, RBMs penalize output that has a valid correction that is not present in the references or that addresses an error not corrected in the references. The example in §1 has low GLEU and M2 scores, even though the output addresses two errors (GLEU=0.43 and M2 = 0.00, in the bottom half and quartile of 15k system outputs, respectively). To address these concerns, we propose three metrics to evaluate the gr"
D16-1228,Q16-1013,1,0.911412,"cision- and recallbased metrics measure fluency and adequacy by penalizing inappropriate changes, which alter meaning or introduce other errors. Changes consistent with the annotations indicate improved fluency and no change in meaning. Unlike these metrics, GLEU scores output by penalizing n-grams found in the input and output but not the reference (Napoles et al., 2015). Like BLEU (Papineni et al., 2002), GLEU captures both fluency and adequacy with n-gram overlap. Recent work has shown that GLEU has the strongest correlation with human judgments compared to the GEC metrics described above (Sakaguchi et al., 2016). These GEC metrics are all defined at the corpus level, meaning that the statistics are accumulated over the entire output and then used to calculate a single system score. 2110 3 Explicitly evaluating grammaticality GLEU, I-measure, and M2 are calculated based on comparison to reference corrections. These Reference-Based Metrics (RBMs) credit corrections seen in the references and penalize systems for ignoring errors and making bad changes (changing a span of text in an ungrammatical way or introducing errors to grammatical text). However, RBMs make two strong assumptions: that the annotatio"
D16-1228,P13-4014,0,0.0612146,"Missing"
D16-1228,D14-1102,0,0.0285394,"large number of references (20), however Figure 1 shows that interpolating GLEU using just one reference has stronger correlation than any uninterpolated metric. This supports the use of interpolation to improve GEC evaluation in any setting. This work is the first exploration into applying fluency-based metrics to GEC evaluation. We believe that, for future work, fluency measures could be further improved with other methods, such as using existing GEC systems to detect errors, or even using an ensemble of systems to improve coverage (indeed, ensembles have been useful in the GEC task itself (Susanto et al., 2014)). There is also recent work from the MT community, such as the use of confidence bounds (Graham and Liu, 2016) or uncertainty measurement (Beck et al., 2016), which could be adopted by the GEC community. Finally, in the course of our experiments, we determined that metrics calculated on the sentencelevel is more reliable for evaluating GEC output, and we suggest that the GEC community adopt this modification to better assess systems. To facilitate GEC evaluation, we have set up an online platform8 for benchmarking system output on the same set of sentences evaluated using different metrics an"
D16-1228,W08-1205,1,0.535486,"s. These Reference-Based Metrics (RBMs) credit corrections seen in the references and penalize systems for ignoring errors and making bad changes (changing a span of text in an ungrammatical way or introducing errors to grammatical text). However, RBMs make two strong assumptions: that the annotations in the references are correct and that they are complete. We argue that these assumptions are invalid and point to a deficit in current evaluation practices. In GEC, the agreement between raters can be low due to the challenging nature of the task (Bryant and Ng, 2015; Rozovskaya and Roth, 2010; Tetreault and Chodorow, 2008), indicating that annotations may not be correct or complete. An exhaustive list of all possible corrections would be time-consuming, if not impossible. As a result, RBMs penalize output that has a valid correction that is not present in the references or that addresses an error not corrected in the references. The example in §1 has low GLEU and M2 scores, even though the output addresses two errors (GLEU=0.43 and M2 = 0.00, in the bottom half and quartile of 15k system outputs, respectively). To address these concerns, we propose three metrics to evaluate the grammaticality of output without"
D19-5504,J82-2005,0,0.510863,"Missing"
D19-5504,D18-1274,0,0.0654412,"ingle model on all L1–Level subsets, with a margin of 5 CoNLL14 Evaluation We compare our adapted models on the CoNLL14 testset (Ng et al., 2014) in Table 3. The model adapted to Chinese-B2 improves the most over the baseline, achieving 55.1 F0.5 . This result aligns with how the test set was constructed: it consists of essays written by university students, mostly Chinese native speakers. When we pre-process the evaluation set before decoding with a commercial spellchecker5 , our adapted model scores 57.0 which places it near other leading models, trained on a similar amount of data, such as Chollampatt and Ng (2018) (56.52) and Junczys-Dowmunt et al. (2018)6 (57.53) even though we do not use the CoNLL14 in-domain training data. We note that the most recent state-of-the-art models (Zhao et al., 2019; Grundkiewicz et al., 2019), are trained on up to one hundred million additional synthetic parallel sentences, while we adapt models with only eight thousand parallel sentences. 5 Details removed for anonymity. We call their ensemble of four models with language model re-scoring JD ensemble and their single best model without language model re-scoring JD single 6 30 Adapt CN-C1 FR-B1 DE-B1 IT-B1 ES-A2 Det 3.53"
D19-5504,W18-1817,0,0.0299984,"the CLC, however, it only covers one proficiency level and there are not enough sentences for each L1 for our experiments. Previous work on adapting GEC classifiers to L1 (Rozovskaya et al., 2017) used the FCE corpus, and thus did not Experimental Setup Our baseline neural GEC system is an RNN-based encoder-decoder neural network with attention and LSTM units (Bahdanau et al., 2015). The system takes as input an English sentence which may contain grammatical errors and decodes the corrected sentence. We train the system on the parallel corpus extracted from the CLC with the OpenNMTpy toolkit (Klein et al., 2018) using the hyperparameters listed in the Appendix. To increase the coverage of the neural network’s vocabulary, without hurting efficiency, we break source and target words into sub-word units. The segmentation into sub-word units is learned from unlabeled data using the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016). The vocabulary, consisting of 20,000 BPE sub-units, is shared between the encoder and decoder.3 We truncate sentences longer than 60 BPE sub-units and train the baseline system with early stopping on a development set sampled from the base dataset.4 To train and evalu"
D19-5504,P17-2061,0,0.11958,"(3.6 F0.5 ) relative to a strong baseline. 1 Joel Tetreault∗ Dataminr jtetreault@dataminr.com Introduction We believe the future of GEC lies in providing users with feedback that is personalized to their proficiency level and native language (L1). In this work, we present the first results on adapting a general purpose neural GEC system for English to both of these characteristics by using fine-tuning, a transfer learning method for neural networks, which has been extensively explored for domain adaptation of machine translation systems (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Miceli Barone et al., 2017; Thompson et al., 2018). We show that a model adapted to both L1 and proficiency level outperforms models adapted to only one of these characteristics. Our contributions also include the first results on adapting GEC systems to proficiency levels and the broadest study of adapting GEC to L1 which includes twelve different languages. Guides for English teachers have extensively documented how grammatical errors made by learners are influenced by their native language (L1). Swan and Smith (2001) attribute some of the errors to “transfer” or “interference” between lan"
D19-5504,2015.iwslt-evaluation.11,0,0.0454098,"scenarios achieves the largest performance improvement (3.6 F0.5 ) relative to a strong baseline. 1 Joel Tetreault∗ Dataminr jtetreault@dataminr.com Introduction We believe the future of GEC lies in providing users with feedback that is personalized to their proficiency level and native language (L1). In this work, we present the first results on adapting a general purpose neural GEC system for English to both of these characteristics by using fine-tuning, a transfer learning method for neural networks, which has been extensively explored for domain adaptation of machine translation systems (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Miceli Barone et al., 2017; Thompson et al., 2018). We show that a model adapted to both L1 and proficiency level outperforms models adapted to only one of these characteristics. Our contributions also include the first results on adapting GEC systems to proficiency levels and the broadest study of adapting GEC to L1 which includes twelve different languages. Guides for English teachers have extensively documented how grammatical errors made by learners are influenced by their native language (L1). Swan and Smith (2001) attribute some of the er"
D19-5504,N12-1067,0,0.0133995,"2.8 43.8 45.9 44.1 Table 1: Adaptation to Proficiency Level in F0.5 Results Adaptation by L1 We adapt GEC models to twelve L1s: Arabic, Chinese, French, German, Greek, Italian, Polish, Portuguese, Russian, Spanish, Swiss-German and Turkish. The results in Table 2 (top) show that all L1-adapted models are better than the baseline, with improvements ranging from 1.2 F0.5 for Chinese and French, up We report the results for the three adaptation scenarios: adapting to Level only, adapting to L1 only, and adapting to both L1 and Level. We summarize the results by showing the average M2 F0.5 score (Dahlmeier and Ng, 2012) across all the test sets included in the respective scenario. 29 Adapt No Random L1 JD single Adapt No Random Level L1 L1 & Level JD single AR 37.5 46.3 48.3 47.0 CN 36.2 45.0 46.2 44.7 CN-B2 36.1 42.7 43.4 44.1 45.5 43.0 FR 32.7 44.9 46.1 44.2 CN-C1 32.5 39.1 41.0 40.9 43.1 35.8 DE 31.4 44.7 47.1 41.4 FR-B1 31.8 45.3 46.5 46.5 48.1 46.9 GR 32.7 46.4 49.0 44.1 IT 29.3 44.9 46.8 40.7 DE-B1 31.2 46.1 46.9 48.1 50.2 43.8 PL 36.0 46.2 48.4 46.0 IT-B1 28.1 43.5 45.3 46.5 47.3 41.6 PT 31.7 45.2 47.6 44.6 PT-B1 31.4 45.2 46.1 46.2 47.9 46.7 RU 35.8 45.3 47.8 43.7 ES 32.1 47.6 49.8 44.8 ES-A2 28.9 50"
D19-5504,D17-1156,0,0.100588,"a strong baseline. 1 Joel Tetreault∗ Dataminr jtetreault@dataminr.com Introduction We believe the future of GEC lies in providing users with feedback that is personalized to their proficiency level and native language (L1). In this work, we present the first results on adapting a general purpose neural GEC system for English to both of these characteristics by using fine-tuning, a transfer learning method for neural networks, which has been extensively explored for domain adaptation of machine translation systems (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Miceli Barone et al., 2017; Thompson et al., 2018). We show that a model adapted to both L1 and proficiency level outperforms models adapted to only one of these characteristics. Our contributions also include the first results on adapting GEC systems to proficiency levels and the broadest study of adapting GEC to L1 which includes twelve different languages. Guides for English teachers have extensively documented how grammatical errors made by learners are influenced by their native language (L1). Swan and Smith (2001) attribute some of the errors to “transfer” or “interference” between languages. For example, German"
D19-5504,I11-1017,0,0.145233,"nguages. Each essay is corrected by one annotator, who also identifies the minimal error spans and labels them using about 80 error types. From this annotated corpus we extract a parallel corpus comprising of source sentences with grammatical errors and the corresponding corrected target sentences. We do note the proprietary nature of the CLC which makes reproducibility difficult, though it has been used in prior research, such as Rei and Yannakoudakis (2016). It was necessary for this study as the other GEC corpora available are not annotated for both L1 and level. The Lang8 Learner Corpora (Mizumoto et al., 2011) also provides information about L1, but it has no information about proficiency levels. The FCE dataset (Yannakoudakis et al., 2011) is a subset of the CLC, however, it only covers one proficiency level and there are not enough sentences for each L1 for our experiments. Previous work on adapting GEC classifiers to L1 (Rozovskaya et al., 2017) used the FCE corpus, and thus did not Experimental Setup Our baseline neural GEC system is an RNN-based encoder-decoder neural network with attention and LSTM units (Bahdanau et al., 2015). The system takes as input an English sentence which may contain"
D19-5504,W14-1701,0,0.0842121,"for the baseline model. Similarly, among the models adapted to different levels of Spanish native speakers, the one adapted to Spanish-A2 achieves the largest gains of 8 F0.5 points. The Spanish-A2 testset has the highest number of errors per 100 words among all the L1-Level testsets, as shown in Table 1 in the Appendix. Furthermore, the A2 level is only half as frequent as the B1 level in the random sample of CLC. Finally, our adapted models are better than the JD single model on all L1–Level subsets, with a margin of 5 CoNLL14 Evaluation We compare our adapted models on the CoNLL14 testset (Ng et al., 2014) in Table 3. The model adapted to Chinese-B2 improves the most over the baseline, achieving 55.1 F0.5 . This result aligns with how the test set was constructed: it consists of essays written by university students, mostly Chinese native speakers. When we pre-process the evaluation set before decoding with a commercial spellchecker5 , our adapted model scores 57.0 which places it near other leading models, trained on a similar amount of data, such as Chollampatt and Ng (2018) (56.52) and Junczys-Dowmunt et al. (2018)6 (57.53) even though we do not use the CoNLL14 in-domain training data. We no"
D19-5504,N18-2046,0,0.019795,"s. Mizumoto et al. (2011) showed for the first time that a statistical machine translation (SMT) system applied to GEC performs better when the training and test data have the same L1. Chollampatt et al. (2016) extend this work by adapting a neural language model to three different L1s and use it as a feature in SMT-based GEC system. However, we are not aware of prior work addressing the impact of both proficiency level and native language on the performance of GEC systems. Furthermore, neural GEC systems, which have become state-of-the-art (Gehring et al., 2017; Junczys-Dowmunt et al., 2018; Grundkiewicz and Junczys-Dowmunt, 2018), are general purpose and domain agnostic. Grammar error correction (GEC) systems have become ubiquitous in a variety of software applications, and have started to approach human-level performance for some datasets. However, very little is known about how to efficiently personalize these systems to the user’s characteristics, such as their proficiency level and first language, or to emerging domains of text. We present the first results on adapting a general purpose neural GEC system to both the proficiency level and the first language of a writer, using only a few thousand annotated sentences"
D19-5504,P16-1112,0,0.0237808,"ge Learner Corpus (CLC) (Nicholls, 2003) comprising examination essays written by English learners with six proficiency levels2 and more than 100 different native languages. Each essay is corrected by one annotator, who also identifies the minimal error spans and labels them using about 80 error types. From this annotated corpus we extract a parallel corpus comprising of source sentences with grammatical errors and the corresponding corrected target sentences. We do note the proprietary nature of the CLC which makes reproducibility difficult, though it has been used in prior research, such as Rei and Yannakoudakis (2016). It was necessary for this study as the other GEC corpora available are not annotated for both L1 and level. The Lang8 Learner Corpora (Mizumoto et al., 2011) also provides information about L1, but it has no information about proficiency levels. The FCE dataset (Yannakoudakis et al., 2011) is a subset of the CLC, however, it only covers one proficiency level and there are not enough sentences for each L1 for our experiments. Previous work on adapting GEC classifiers to L1 (Rozovskaya et al., 2017) used the FCE corpus, and thus did not Experimental Setup Our baseline neural GEC system is an R"
D19-5504,W19-4427,0,0.0164849,"the baseline, achieving 55.1 F0.5 . This result aligns with how the test set was constructed: it consists of essays written by university students, mostly Chinese native speakers. When we pre-process the evaluation set before decoding with a commercial spellchecker5 , our adapted model scores 57.0 which places it near other leading models, trained on a similar amount of data, such as Chollampatt and Ng (2018) (56.52) and Junczys-Dowmunt et al. (2018)6 (57.53) even though we do not use the CoNLL14 in-domain training data. We note that the most recent state-of-the-art models (Zhao et al., 2019; Grundkiewicz et al., 2019), are trained on up to one hundred million additional synthetic parallel sentences, while we adapt models with only eight thousand parallel sentences. 5 Details removed for anonymity. We call their ensemble of four models with language model re-scoring JD ensemble and their single best model without language model re-scoring JD single 6 30 Adapt CN-C1 FR-B1 DE-B1 IT-B1 ES-A2 Det 3.53 2.34 8.85 2.37 6.06 Prep 5.90 1.99 1.77 5.32 12.52 Verb 2.99 12.54 2.04 12.48 7.51 Tense 1.77 5.16 2.37 6.74 8.54 NNum 8.28 9.16 3.86 4.40 8.73 Noun 8.02 3.48 7.18 3.29 12.39 Pron 22.78 1.13 22.75 8.99 10.57 Table"
D19-5504,P11-1093,0,0.0471004,"Missing"
D19-5504,P16-1162,0,0.0410806,"work with attention and LSTM units (Bahdanau et al., 2015). The system takes as input an English sentence which may contain grammatical errors and decodes the corrected sentence. We train the system on the parallel corpus extracted from the CLC with the OpenNMTpy toolkit (Klein et al., 2018) using the hyperparameters listed in the Appendix. To increase the coverage of the neural network’s vocabulary, without hurting efficiency, we break source and target words into sub-word units. The segmentation into sub-word units is learned from unlabeled data using the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016). The vocabulary, consisting of 20,000 BPE sub-units, is shared between the encoder and decoder.3 We truncate sentences longer than 60 BPE sub-units and train the baseline system with early stopping on a development set sampled from the base dataset.4 To train and evaluate the adapted models, we extract subsets of sentences from the CLC that have been written by learners having a particular Level, L1, or L1-Level combination. We consider all subsets having at least 11,000 sentences, such that we can allocate 8,000 sentences for training, 1,000 for tuning and 2,000 for testing. We compare adapt"
D19-5504,W18-6313,0,0.080483,"Joel Tetreault∗ Dataminr jtetreault@dataminr.com Introduction We believe the future of GEC lies in providing users with feedback that is personalized to their proficiency level and native language (L1). In this work, we present the first results on adapting a general purpose neural GEC system for English to both of these characteristics by using fine-tuning, a transfer learning method for neural networks, which has been extensively explored for domain adaptation of machine translation systems (Luong and Manning, 2015; Freitag and Al-Onaizan, 2016; Chu et al., 2017; Miceli Barone et al., 2017; Thompson et al., 2018). We show that a model adapted to both L1 and proficiency level outperforms models adapted to only one of these characteristics. Our contributions also include the first results on adapting GEC systems to proficiency levels and the broadest study of adapting GEC to L1 which includes twelve different languages. Guides for English teachers have extensively documented how grammatical errors made by learners are influenced by their native language (L1). Swan and Smith (2001) attribute some of the errors to “transfer” or “interference” between languages. For example, German native speakers are more"
D19-5504,P11-1019,0,0.123118,"ror types. From this annotated corpus we extract a parallel corpus comprising of source sentences with grammatical errors and the corresponding corrected target sentences. We do note the proprietary nature of the CLC which makes reproducibility difficult, though it has been used in prior research, such as Rei and Yannakoudakis (2016). It was necessary for this study as the other GEC corpora available are not annotated for both L1 and level. The Lang8 Learner Corpora (Mizumoto et al., 2011) also provides information about L1, but it has no information about proficiency levels. The FCE dataset (Yannakoudakis et al., 2011) is a subset of the CLC, however, it only covers one proficiency level and there are not enough sentences for each L1 for our experiments. Previous work on adapting GEC classifiers to L1 (Rozovskaya et al., 2017) used the FCE corpus, and thus did not Experimental Setup Our baseline neural GEC system is an RNN-based encoder-decoder neural network with attention and LSTM units (Bahdanau et al., 2015). The system takes as input an English sentence which may contain grammatical errors and decodes the corrected sentence. We train the system on the parallel corpus extracted from the CLC with the Ope"
D19-5504,N19-1014,0,0.0527002,"oves the most over the baseline, achieving 55.1 F0.5 . This result aligns with how the test set was constructed: it consists of essays written by university students, mostly Chinese native speakers. When we pre-process the evaluation set before decoding with a commercial spellchecker5 , our adapted model scores 57.0 which places it near other leading models, trained on a similar amount of data, such as Chollampatt and Ng (2018) (56.52) and Junczys-Dowmunt et al. (2018)6 (57.53) even though we do not use the CoNLL14 in-domain training data. We note that the most recent state-of-the-art models (Zhao et al., 2019; Grundkiewicz et al., 2019), are trained on up to one hundred million additional synthetic parallel sentences, while we adapt models with only eight thousand parallel sentences. 5 Details removed for anonymity. We call their ensemble of four models with language model re-scoring JD ensemble and their single best model without language model re-scoring JD single 6 30 Adapt CN-C1 FR-B1 DE-B1 IT-B1 ES-A2 Det 3.53 2.34 8.85 2.37 6.06 Prep 5.90 1.99 1.77 5.32 12.52 Verb 2.99 12.54 2.04 12.48 7.51 Tense 1.77 5.16 2.37 6.74 8.54 NNum 8.28 9.16 3.86 4.40 8.73 Noun 8.02 3.48 7.18 3.29 12.39 Pron 22.78"
E06-1037,2005.sigdial-1.10,1,0.732524,"TS) community. Student Moves refer to the type of answer a student gives. Answers that involve a concept already introduced in the dialogue are called Shallow, answers that involve a novel concept are called Novel, “I don’t know” type answers are called Assertions (As), and Deep answers refer to answers that involve linking two concepts through reasoning. In our study, we merge all non-Shallow moves into a new move “Other.” In addition to Student Moves, we annotated five other features to include in our representation of the student state. Two emotion related features were annotated manually (Forbes-Riley and Litman, 2005): certainty and frustration. Certainty describes how confident a student seemed to be in his answer, while frustration describes how frustrated the student seemed to be in his last response. We include three other features for the Student state that were extracted automatically. Correctness says if the last student answer was correct or incorrect. As noted above, this is what most current tutoring systems use as their state. Percent Correct is the percentage of questions in the current problem the student has answered correctly so far. Finally, if a student performs poorly when it comes to a c"
E06-1037,2005.sigdial-1.5,0,0.0851696,"ving enough data to train on. Our results here indicate that a small training corpus is actually acceptable to use in a MDP framework as long as the state and action features are pruned effectively. The use of features such as context and student moves is nothing new to the ITS community however, such as the BEETLE system (Zinn et al., 2005), but very little work has been done using RL in developing tutoring systems. 7 Related Work RL has been applied to improve dialogue systems in past work but very few approaches have looked at which features are important to include in the dialogue state. (Paek and Chickering, 2005) showed how the state space can be learned from 8 Discussion In this paper we showed that incorporating more information into a representation of the student state has an impact on what actions the tutor should take. We first showed that despite not be295 ing able to test on real users or simulated users just yet, that our generated policies were indeed reliable since they converged in terms of the V-values of each state and the policy for each state. Next, we showed that all five features investigated in this study were indeed important to include when constructing an estimation of the studen"
E06-1037,W03-2111,0,0.0659231,"Missing"
E14-4010,N06-2019,0,0.054362,"Missing"
E14-4010,P04-1015,0,0.0558677,"regular parser to parse the disfluent sentences, the UAS for correct words would be 70.7. As seen in Table 1, the best parser UAS is 88.4 (M6) which is very close to the upperbound, however RT13, M2E and M6 are nearly indistinguishable in terms of parser performance. 5 Conclusion and Future Directions In this paper, we build on our prior work by introducing rich and novel features to better handle the detection of reparandum and by introducing an improved classifier structure to decrease the uncertainty in decision-making and to improve parser speed and accuracy. We could use early updating (Collins and Roark, 2004) for learning the greedy parser which is shown to be useful in greedy parsing (Huang and Sagae, 2010). K-beam parsing is a way to improve the model though at the expense of speed. The main problem with k-beam parsers is that it is complicated to combine classifier scores from different classifiers. One possible solution is to modify the three actions to work on just one word per action, thus the system will run in completely linear time with one classifier and k-beam parsing can be done by choosing better features for the joint parser. A model similar to this idea is designed by Honnibal and J"
E14-4010,W04-0308,0,0.08988,")). High performance disfluency detection methods can greatly enhance 1 Honnibal and Johnson (2014) have a forthcoming paper based on a similar idea but with a higher performance. 48 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 48–53, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics which is manipulated by a set of actions. When an action is made, the parser goes to a new state. State State C1 Other DM IJ C4 RP C2 C3 Parse C6 DM[i] IJ[i] C5 RP[i:j] RA LA R SH C1 The arc-eager algorithm (Nivre, 2004) is a transition-based algorithm for dependency parsing. In the initial state of the algorithm, the buffer contains all words in the order in which they appear in the sentence and the stack contains the artificial root token. The actions in arc-eager parsing are left-arc (LA), right-arc (RA), reduce (R) and shift (SH). LA removes the top word in the stack by making it the dependent of the first word in the buffer; RA shifts the first word in the buffer to the stack by making it the dependent of the top stack word; R pops the top stack word and SH pushes the first buffer word into the stack. DM"
E14-4010,W02-1001,0,0.0548682,"n/dev/test splits from Johnson and Charniak (2004) (JC04). All experimental settings are the same as RT13. We compare our new models against this prior work in terms of disfluency detection performance and parsing accuracy. In the second evaluation (Eval 2), we compare our work against the current best disfluency detection method (QL13) on the JC04 split as well as on a 10 fold cross-validation of the parsed section of the Switchboard. We use gold POS tags for all evaluations. For all of the joint parsing models we use the weighted averaged Perceptron which is the same as averaged Perceptron (Collins, 2002) but with a 50 uated the QL13 system with optimal number of training iterations (10 iterations). As seen in Table 2, although the annotation in the mrg files is less precise than in the dps files, M6 outperforms all models on the JC04 split thus showing the power of the new features and new classifier structure. loss weight of two for reparandum candidates as done in prior work. The standard arc-eager parser is first trained on a “cleaned” Switchboard corpus (i.e. after removing disfluent words) with 3 training iterations. Next, it is updated by training it on the real corpus with 3 additional"
E14-4010,J08-4003,0,0.0262699,"2003; Merlo and Mansur, 2004). Disfluencies are often decomposed into three types: filled pauses (IJ) such as “uh” or “huh”, discourse markers (DM) such as “you know” and “I mean” and edited words (reparandum) which are repeated or corrected by the speaker (repair). The following sentence illustrates the three types: 2 IJ DM Non-monotonic Disfluency Parsing In transition-based dependency parsing, a syntactic tree is constructed by a set of stack and buffer actions where the parser greedily selects an action at each step until it reaches the end of the sentence with an empty buffer and stack (Nivre, 2008). A state in a transition-based system has a stack of words, a buffer of unprocessed words and a set of arcs that have been produced in the parser history. The parser consists of a state (or a configuration) I want a flight |to Boston uh I |mean {z } |{z} {z } to |Denver {z } Reparandum Joel Tetreault Yahoo Labs New York, NY, USA tetreaul@yahoo-inc.com Repair To date, there have been many studies on disfluency detection (Hough and Purver, 2013; Rasooli and Tetreault, 2013; Qian and Liu, 2013; Wang et al., 2013) such as those based on TAGs and the noisy channel model (e.g. Johnson and Charniak"
E14-4010,N09-2028,0,0.0935796,"ed system has a stack of words, a buffer of unprocessed words and a set of arcs that have been produced in the parser history. The parser consists of a state (or a configuration) I want a flight |to Boston uh I |mean {z } |{z} {z } to |Denver {z } Reparandum Joel Tetreault Yahoo Labs New York, NY, USA tetreaul@yahoo-inc.com Repair To date, there have been many studies on disfluency detection (Hough and Purver, 2013; Rasooli and Tetreault, 2013; Qian and Liu, 2013; Wang et al., 2013) such as those based on TAGs and the noisy channel model (e.g. Johnson and Charniak (2004), Zhang et al. (2006), Georgila (2009), and Zwarts and Johnson (2011)). High performance disfluency detection methods can greatly enhance 1 Honnibal and Johnson (2014) have a forthcoming paper based on a similar idea but with a higher performance. 48 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 48–53, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics which is manipulated by a set of actions. When an action is made, the parser goes to a new state. State State C1 Other DM IJ C4 RP C2 C3 Parse C6 DM[i] IJ[i] C5 RP[i:j] RA LA R S"
E14-4010,N13-1102,0,0.741959,"ging the disfluency and parsing steps into one. However, joint parsing and disfluency detection models, such as Lease and Johnson (2006), based on these approaches have only achieved moderate performance in the disfluency detection task. Our aim in this paper is to show that a high performance joint approach is viable. We build on our previous work (Rasooli and Tetreault, 2013) (henceforth RT13) to jointly detect disfluencies while producing dependency parses. While this model produces parses at a very high accuracy, it does not perform as well as the state-of-the-art in disfluency detection (Qian and Liu, 2013) (henceforth QL13). In this paper, we extend RT13 in two important ways: 1) we show that by adding a set of novel features selected specifically for disfluency detection we can outperform the current state of the art in disfluency detection in two evaluations1 and 2) we show that by extending the architecture from two to six classifiers, we can drastically increase the speed and reduce the memory usage of the model without a loss in performance. Parsing disfluent sentences is a challenging task which involves detecting disfluencies as well as identifying the syntactic structure of the sentence"
E14-4010,D13-1013,1,0.773763,"tract the linguistic processing pipeline of a spoken dialogue system by first “cleaning” the speaker’s utterance, making it easier for a parser to process correctly. A joint parsing and disfluency detection model can also speed up processing by merging the disfluency and parsing steps into one. However, joint parsing and disfluency detection models, such as Lease and Johnson (2006), based on these approaches have only achieved moderate performance in the disfluency detection task. Our aim in this paper is to show that a high performance joint approach is viable. We build on our previous work (Rasooli and Tetreault, 2013) (henceforth RT13) to jointly detect disfluencies while producing dependency parses. While this model produces parses at a very high accuracy, it does not perform as well as the state-of-the-art in disfluency detection (Qian and Liu, 2013) (henceforth QL13). In this paper, we extend RT13 in two important ways: 1) we show that by adding a set of novel features selected specifically for disfluency detection we can outperform the current state of the art in disfluency detection in two evaluations1 and 2) we show that by extending the architecture from two to six classifiers, we can drastically in"
E14-4010,Q14-1011,0,0.415318,"history. The parser consists of a state (or a configuration) I want a flight |to Boston uh I |mean {z } |{z} {z } to |Denver {z } Reparandum Joel Tetreault Yahoo Labs New York, NY, USA tetreaul@yahoo-inc.com Repair To date, there have been many studies on disfluency detection (Hough and Purver, 2013; Rasooli and Tetreault, 2013; Qian and Liu, 2013; Wang et al., 2013) such as those based on TAGs and the noisy channel model (e.g. Johnson and Charniak (2004), Zhang et al. (2006), Georgila (2009), and Zwarts and Johnson (2011)). High performance disfluency detection methods can greatly enhance 1 Honnibal and Johnson (2014) have a forthcoming paper based on a similar idea but with a higher performance. 48 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 48–53, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics which is manipulated by a set of actions. When an action is made, the parser goes to a new state. State State C1 Other DM IJ C4 RP C2 C3 Parse C6 DM[i] IJ[i] C5 RP[i:j] RA LA R SH C1 The arc-eager algorithm (Nivre, 2004) is a transition-based algorithm for dependency parsing. In the initial state of the al"
E14-4010,W13-3518,0,0.0128524,"in the buffer; RA shifts the first word in the buffer to the stack by making it the dependent of the top stack word; R pops the top stack word and SH pushes the first buffer word into the stack. DM[i] Parse RP[i:j] IJ[i] C2 SH LA RA (a) A structure with two classifiers. R (b) A structure with six classifiers. Figure 1: Two kinds of cascades for disfluency learning. Circles are classifiers and light-colored blocks show the final decision by the system. 3 The arc-eager algorithm is a monotonic parsing algorithm, i.e. once an action is performed, subsequent actions should be consistent with it (Honnibal et al., 2013). In monotonic parsing, if a word becomes a dependent of another word or acquires a dependent, other actions shall not change those dependencies that have been constructed for that word in the action history. Disfluency removal is an issue for monotonic parsing in that if an action creates a dependency relation, the other actions cannot repair that dependency relation. The main idea proposed by RT13 is to change the original arc-eager algorithm to a non-monotonic one so it is possible to repair a dependency tree while detecting disfluencies by incorporating three new actions (one for each disf"
E14-4010,N13-1083,0,0.0226117,"ion at each step until it reaches the end of the sentence with an empty buffer and stack (Nivre, 2008). A state in a transition-based system has a stack of words, a buffer of unprocessed words and a set of arcs that have been produced in the parser history. The parser consists of a state (or a configuration) I want a flight |to Boston uh I |mean {z } |{z} {z } to |Denver {z } Reparandum Joel Tetreault Yahoo Labs New York, NY, USA tetreaul@yahoo-inc.com Repair To date, there have been many studies on disfluency detection (Hough and Purver, 2013; Rasooli and Tetreault, 2013; Qian and Liu, 2013; Wang et al., 2013) such as those based on TAGs and the noisy channel model (e.g. Johnson and Charniak (2004), Zhang et al. (2006), Georgila (2009), and Zwarts and Johnson (2011)). High performance disfluency detection methods can greatly enhance 1 Honnibal and Johnson (2014) have a forthcoming paper based on a similar idea but with a higher performance. 48 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 48–53, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics which is manipulated by a set of actions. When an"
E14-4010,P06-1071,0,0.0284195,"e in a transition-based system has a stack of words, a buffer of unprocessed words and a set of arcs that have been produced in the parser history. The parser consists of a state (or a configuration) I want a flight |to Boston uh I |mean {z } |{z} {z } to |Denver {z } Reparandum Joel Tetreault Yahoo Labs New York, NY, USA tetreaul@yahoo-inc.com Repair To date, there have been many studies on disfluency detection (Hough and Purver, 2013; Rasooli and Tetreault, 2013; Qian and Liu, 2013; Wang et al., 2013) such as those based on TAGs and the noisy channel model (e.g. Johnson and Charniak (2004), Zhang et al. (2006), Georgila (2009), and Zwarts and Johnson (2011)). High performance disfluency detection methods can greatly enhance 1 Honnibal and Johnson (2014) have a forthcoming paper based on a similar idea but with a higher performance. 48 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 48–53, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics which is manipulated by a set of actions. When an action is made, the parser goes to a new state. State State C1 Other DM IJ C4 RP C2 C3 Parse C6 DM[i] IJ[i] C5"
E14-4010,P11-1071,0,0.0573231,"of words, a buffer of unprocessed words and a set of arcs that have been produced in the parser history. The parser consists of a state (or a configuration) I want a flight |to Boston uh I |mean {z } |{z} {z } to |Denver {z } Reparandum Joel Tetreault Yahoo Labs New York, NY, USA tetreaul@yahoo-inc.com Repair To date, there have been many studies on disfluency detection (Hough and Purver, 2013; Rasooli and Tetreault, 2013; Qian and Liu, 2013; Wang et al., 2013) such as those based on TAGs and the noisy channel model (e.g. Johnson and Charniak (2004), Zhang et al. (2006), Georgila (2009), and Zwarts and Johnson (2011)). High performance disfluency detection methods can greatly enhance 1 Honnibal and Johnson (2014) have a forthcoming paper based on a similar idea but with a higher performance. 48 Proceedings of the 14th Conference of the European Chapter of the Association for Computational Linguistics, pages 48–53, c Gothenburg, Sweden, April 26-30 2014. 2014 Association for Computational Linguistics which is manipulated by a set of actions. When an action is made, the parser goes to a new state. State State C1 Other DM IJ C4 RP C2 C3 Parse C6 DM[i] IJ[i] C5 RP[i:j] RA LA R SH C1 The arc-eager algorithm (N"
E14-4010,P11-2033,0,\N,Missing
E14-4010,P10-1110,0,\N,Missing
E14-4010,P04-1005,0,\N,Missing
E17-2037,P15-1068,0,0.0955748,"Missing"
E17-2037,P15-2097,1,0.820565,"Missing"
E17-2037,D16-1195,0,0.339887,"Missing"
E17-2037,W13-3601,1,0.9381,"Missing"
E17-2037,N12-1067,0,0.619851,"Missing"
E17-2037,W14-1701,0,0.855532,"o handle and sets a gold standard to which the field should aim. We overview the current state of GEC by evaluating the performance of four leading systems on this new dataset. We analyze the edits made in JFLEG and summarize which types of changes the systems successfully make, and which they need to address. JFLEG will enable the field to move beyond minimal error corrections. Introduction Automatic grammatical error correction (GEC) progress is limited by the corpora available for developing and evaluating systems. Following the release of the test set of the CoNLL–2014 Shared Task on GEC (Ng et al., 2014), systems have been compared and new evaluation techniques proposed on this single dataset. This corpus has enabled substantial advancement in GEC beyond the shared tasks, but we are concerned that the field is over-developing on this dataset. This is problematic for two reasons: 1) it represents one specific population of language learners; and 2) the corpus only contains minimal edits, which correct the grammaticality of a sentence but do not necessarily make it fluent or native-sounding. To illustrate the need for fluency edits, consider the example in Table 1. The correction with only mini"
E17-2037,W13-1703,0,0.799358,"Missing"
E17-2037,W14-3301,1,0.454778,"Missing"
E17-2037,W14-1702,0,0.285777,"Missing"
E17-2037,Q16-1013,1,0.836222,"as in which they do well and how they can improve. JFLEG fulfills the need for a new gold standard to properly assess the current state of GEC. 1 Table 1: A sentence corrected with just minimal edits compared to fluency edits. tence sounds more natural and its intended meaning is more clear. It is not unrealistic to expect these changes from automatic GEC: the current best systems use machine translation (MT) and are therefore capable of making broader sentential rewrites but, until now, there has not been a gold standard against which they could be evaluated. Following the recommendations of Sakaguchi et al. (2016), we release a new corpus for GEC, the JHU FLuency-Extended GUG corpus (JFLEG), which adds a layer of annotation to the GUG corpus (Heilman et al., 2014). GUG represents a cross-section of ungrammatical data, containing sentences written by English language learners with different L1s and proficiency levels. For each of 1,511 GUG sentences, we have collected four human-written corrections which contain holistic fluency rewrites instead of just minimal edits. This corpus represents the diversity of edits that GEC needs to handle and sets a gold standard to which the field should aim. We overvie"
E17-2037,P12-2039,0,0.314604,"Missing"
E17-2037,P14-2029,1,0.874996,"A sentence corrected with just minimal edits compared to fluency edits. tence sounds more natural and its intended meaning is more clear. It is not unrealistic to expect these changes from automatic GEC: the current best systems use machine translation (MT) and are therefore capable of making broader sentential rewrites but, until now, there has not been a gold standard against which they could be evaluated. Following the recommendations of Sakaguchi et al. (2016), we release a new corpus for GEC, the JHU FLuency-Extended GUG corpus (JFLEG), which adds a layer of annotation to the GUG corpus (Heilman et al., 2014). GUG represents a cross-section of ungrammatical data, containing sentences written by English language learners with different L1s and proficiency levels. For each of 1,511 GUG sentences, we have collected four human-written corrections which contain holistic fluency rewrites instead of just minimal edits. This corpus represents the diversity of edits that GEC needs to handle and sets a gold standard to which the field should aim. We overview the current state of GEC by evaluating the performance of four leading systems on this new dataset. We analyze the edits made in JFLEG and summarize wh"
E17-2037,P11-1019,0,0.373617,"Missing"
E17-2037,N16-1042,0,0.565553,"Missing"
E17-2037,D16-1161,0,0.596066,"Missing"
E99-1031,C88-1021,0,0.0841452,"Missing"
E99-1031,W98-1119,0,0.0370519,"les for two evaluation corpora: written news stories from the Penn Treebank corpus (Marcus et al., 1993) and spoken task-oriented dialogues from the TRAINS93 corpus (Heeman and Allen, 1995). The input format and features added onto DEs from these two corpora are very different, but by encapsulating the translation layer, the same pronoun resolution code can be used for both domains. In both of our experiments only simple noun phrases in the surface form triggered DEs. Treebank texts contain complete structural parsers, POS tags, and annotation of the antecedents of definite pronouns (added by Ge et al. 1998). Because of the thorough syntactic information, DEs can be attributed with explicit phrase structure information. This corpus contains unconstrained news stories, so semantic type information is not available. The Treebank translator module adds the following features to each DE: 1. Whether its surface constituent is contained in reported speech; 2. A list of parent nodes containing its surface constituent in the parse tree. Each node's unique identifier encodes the phrase type (i.e. VB, NP, ADJP); 3. Whether the surface constituent is in the second half of a compound sentence; 4. The referen"
E99-1031,J93-2004,0,0.0282502,"onoun resolution modules. 3.2 Translation layer Translation layer modules are responsible for all syntactic and semantic analysis of the input text. There are a number of design features that must be controlled in this layer, such as how the discourse structure affects antecedent accessibility and which surface constituents trigger DEs. All these design decisions should be implemented as independent modules so that they can be turned on or off for particular experiments. Our experiments created translation modules for two evaluation corpora: written news stories from the Penn Treebank corpus (Marcus et al., 1993) and spoken task-oriented dialogues from the TRAINS93 corpus (Heeman and Allen, 1995). The input format and features added onto DEs from these two corpora are very different, but by encapsulating the translation layer, the same pronoun resolution code can be used for both domains. In both of our experiments only simple noun phrases in the surface form triggered DEs. Treebank texts contain complete structural parsers, POS tags, and annotation of the antecedents of definite pronouns (added by Ge et al. 1998). Because of the thorough syntactic information, DEs can be attributed with explicit phra"
E99-1031,P98-2143,0,0.0439455,"ra resolution techniques for new language understanding applications. Our implementation of the architecture in a pronoun resolution testing platform demonstrates the flexibility of the approach. 1 Introduction When building natural language understanding systems, choosing the best technique for anaphora resolution is a challenging task. The system builder must decide whether to adopt an existing technique or design a new approach. A huge variety of techniques are described in the literature, many of them achieving high success rates on their own evaluation texts (cf. Hobbs 1986; Strube 1998; Mitkov 1998). Each technique makes different assumptions about the data available to reference resolution, for example, some assume perfect parses, others assume only POS-tagged input, some assume semantic information is available, etc. The chances are high that no published technique will exactly match the data available to a particular system's reference resolution component, so it may The authors thank James Allen for help on this project, as well as the anonymous reviewers for helpful comments on the paper. This material is based on work supported by USAF/Rome Labs contract F30602-95-1-0025, ONR grant"
E99-1031,A88-1003,0,0.0167303,"4. Whether its surface constituent is definite or indefinite; 5. Whether its surface constituent is contained in quoted speech; 6. For pronoun DEs, the id of the correct antecedent (used for evaluation). 3.3 Anaphora resolution layer Modules within this layer can be coded to resolve a variety of anaphoric phenomena in a variety of ways. For example, a particular experiment may be concerned only with resolving pronouns or it might also require determination of coreference between definite noun phrases. This layer is reminiscent of the independent anaphora resolution modules in the Lucy system (Rich and LuperFoy, 1988), except that modules in that system were not designed to be easily turned on or off. For our testbed, we implemented a variety of pronoun resolution techniques. Each technique 231 Proceedings of EACL '99 Pronoun resolution module Baseline most-recent technique that chooses closest entity to the left of the pronoun Choose most recent entity that matches sub-categorization restrictions on the verb Strobe's s-list algorithm (Strube, 1998) Boost salience for the first entity in each sentence Decrease salience for entities in prepositional phrases or relative clauses Increase the salience for non-"
E99-1031,P98-2204,0,0.0567608,"ompare anaphora resolution techniques for new language understanding applications. Our implementation of the architecture in a pronoun resolution testing platform demonstrates the flexibility of the approach. 1 Introduction When building natural language understanding systems, choosing the best technique for anaphora resolution is a challenging task. The system builder must decide whether to adopt an existing technique or design a new approach. A huge variety of techniques are described in the literature, many of them achieving high success rates on their own evaluation texts (cf. Hobbs 1986; Strube 1998; Mitkov 1998). Each technique makes different assumptions about the data available to reference resolution, for example, some assume perfect parses, others assume only POS-tagged input, some assume semantic information is available, etc. The chances are high that no published technique will exactly match the data available to a particular system's reference resolution component, so it may The authors thank James Allen for help on this project, as well as the anonymous reviewers for helpful comments on the paper. This material is based on work supported by USAF/Rome Labs contract F30602-95-1-0"
E99-1031,P99-1079,1,0.838312,"for pronoun resolution. Several experiments were run to demonstrate the flexibility of the architecture. The purpose of this paper is not to compare the pronoun resolution results for the techniques we implemented, so pronoun resolution accuracy of particular techniques will not be discussed here.l Instead, our implementation is described to provide some examples of how the architecture can be put to use. 3.1 Supervisor layer The supervisor layer controls which modules within layers 2 and 3 execute for a particular experiment. We created two different supervisor t See (Byron and Allen. 1999; Tetreault, 1999) for results of pronoun resolution experiments run within the testbed. 230 Proceedings of EACL '99 modules in the testbed. One of them simply reads a configuration file with runtime flags hard-coded by the user. This allows the user to explicitly control which parts of the system execute, and will be used when a final reference resolution techniques is chosen for integration into the TRIPS system parser (Ferguson and Allen, 1998). The second supervisor layer was coded as a genetic algorithm (Byron and Allen, 1999). In this module, the selection of translation layer modules to execute was hard-"
E99-1031,C98-2138,0,\N,Missing
E99-1031,C98-2199,0,\N,Missing
garg-etal-2004-evaluation,robinson-etal-2004-issues,1,\N,Missing
garg-etal-2004-evaluation,bernsen-etal-2002-nite,0,\N,Missing
han-etal-2010-using,W08-1205,1,\N,Missing
han-etal-2010-using,P00-1067,0,\N,Missing
han-etal-2010-using,N07-2045,0,\N,Missing
han-etal-2010-using,C08-1022,0,\N,Missing
han-etal-2010-using,W07-1607,0,\N,Missing
han-etal-2010-using,P03-1054,0,\N,Missing
han-etal-2010-using,W07-1604,1,\N,Missing
han-etal-2010-using,P03-2026,0,\N,Missing
han-etal-2010-using,P08-1021,0,\N,Missing
han-etal-2010-using,P06-1132,0,\N,Missing
han-etal-2010-using,izumi-etal-2004-overview,0,\N,Missing
han-etal-2010-using,I08-1059,0,\N,Missing
han-etal-2010-using,han-etal-2004-detecting,1,\N,Missing
J01-4003,P87-1022,0,0.849008,"Missing"
J01-4003,E99-1031,1,0.872236,"Missing"
J01-4003,W98-1119,0,0.312425,"Missing"
J01-4003,J95-2003,0,0.788201,"Missing"
J01-4003,J86-3001,0,0.0361313,"Missing"
J01-4003,J97-3006,0,0.210086,"Missing"
J01-4003,J93-2004,0,0.0485927,"Missing"
J01-4003,P98-2143,0,0.105314,"sycholinguistic claims on Cf-list ranking will actually improve pronoun resolution accuracy. Our results from this investigation lead to the development of a new syntaxbased ranking of the Cf-list and corpus-based evidence that contradicts the psycholinguistic claims. 1. Introduction The aims of this paper are to compare implementations of pronoun resolution algorithms automatically on a common corpus and to see if results from psycholinguistic experiments can be used to improve pronoun resolution. Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora. While manual evaluations have the advantage of allowing the researcher to examine the data closely, they are problematic because they can be time consuming, generally making it difficult to process corpora that are large enough to provide reliable, broadly based statistics. With a system that can run various pronoun resolution algorithms, one can easily and quickly analyze large amounts of data and generate more reliable results. In this study, this ability to alter an algorithm slightly and test its pe"
J01-4003,mitkov-2000-towards,0,0.0991385,"Missing"
J01-4003,P98-2204,0,0.683448,"see if two psycholinguistic claims on Cf-list ranking will actually improve pronoun resolution accuracy. Our results from this investigation lead to the development of a new syntaxbased ranking of the Cf-list and corpus-based evidence that contradicts the psycholinguistic claims. 1. Introduction The aims of this paper are to compare implementations of pronoun resolution algorithms automatically on a common corpus and to see if results from psycholinguistic experiments can be used to improve pronoun resolution. Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora. While manual evaluations have the advantage of allowing the researcher to examine the data closely, they are problematic because they can be time consuming, generally making it difficult to process corpora that are large enough to provide reliable, broadly based statistics. With a system that can run various pronoun resolution algorithms, one can easily and quickly analyze large amounts of data and generate more reliable results. In this study, this ability to alter an algorithm slightly an"
J01-4003,P96-1036,0,0.0119593,"s, which is substantial (p (1.4e-06) over a corpus of 1,691 pronouns. Likewise, for the fictional texts, 1 extra pronoun is resolved incorrectly when using Gordon et al.'s method. Looking at the difference in what each algorithm gets right and wrong, it seems that type of referring expression and mention count play a role in which entity should be selected from the complex NP. If an entity has been mentioned previously or is realized as a pronoun, it is more likely to be the referent of a following pronoun. This would lend support to Strube and Hahn's S-list and functional centering theories (Strube and Hahn 1996), which maintain that type of referring expression and previous mention influence the salience of each entity with the S-list or Cf-list. 6. C o n c l u s i o n s In this paper we first presented a new pronoun resolution algorithm, Left-Right Centering, which adheres to the constraints of centering theory and was inspired by the need to remedy a lack of incremental processing in Brennan, Friedman, and Pollard's (1987) method. Second, we compared LRC's performance with that of three other leading pronoun resolution algorithms, each one restricted to using only syntactic information. This compar"
J01-4003,J99-3001,0,0.158952,"Missing"
J01-4003,J99-2001,0,0.0204283,"Missing"
J01-4003,P99-1079,1,0.855329,"tions have the advantage of allowing the researcher to examine the data closely, they are problematic because they can be time consuming, generally making it difficult to process corpora that are large enough to provide reliable, broadly based statistics. With a system that can run various pronoun resolution algorithms, one can easily and quickly analyze large amounts of data and generate more reliable results. In this study, this ability to alter an algorithm slightly and test its performance is central. We first show the attractiveness of the Left-Right Centering algorithm (henceforth LRC) (Tetreault 1999) given its incremental processing of utterances, psycholinguistic plausibility, and good performance in finding the antecedents of pronouns. The algorithm is tested against three other leading pronoun resolution algorithms: Hobbs's naive algorithm (1978), S-list (Strube 1998), and BFP (Brennan, Friedman, and Pollard 1987). Next we use the conclusions from two psycholinguistic experiments on ranking the Cf-list, the salience of discourse entities in prepended phrases (Gordon, Grosz, and Gilliom 1993) and the ordering of possessor and possessed in complex NPs (Gordon et al. 1999), to try to impr"
J01-4003,P89-1031,0,0.195281,"algorithm to see if two psycholinguistic claims on Cf-list ranking will actually improve pronoun resolution accuracy. Our results from this investigation lead to the development of a new syntaxbased ranking of the Cf-list and corpus-based evidence that contradicts the psycholinguistic claims. 1. Introduction The aims of this paper are to compare implementations of pronoun resolution algorithms automatically on a common corpus and to see if results from psycholinguistic experiments can be used to improve pronoun resolution. Many hand-tested corpus evaluations have been done in the past (e.g., Walker 1989; Strube 1998; Mitkov 1998; Strube and Hahn 1999), but these have the drawback of being carried out on small corpora. While manual evaluations have the advantage of allowing the researcher to examine the data closely, they are problematic because they can be time consuming, generally making it difficult to process corpora that are large enough to provide reliable, broadly based statistics. With a system that can run various pronoun resolution algorithms, one can easily and quickly analyze large amounts of data and generate more reliable results. In this study, this ability to alter an algorith"
J01-4003,J94-2003,0,0.221062,"Missing"
J01-4003,C98-2138,0,\N,Missing
J01-4003,C98-2199,0,\N,Missing
L16-1076,P05-1022,0,0.259903,"Missing"
L16-1076,P12-2071,0,0.0384176,"Missing"
L16-1076,P13-2045,1,0.879947,"Missing"
L16-1076,P14-5010,0,0.00506755,"Missing"
L16-1076,H05-1067,0,0.613636,"to gain insights into what differentiates funny captions from the rest. We developed a set of unsupervised methods for ranking captions based on features such as originality, centrality, sentiment, concreteness, grammaticality, humancenteredness, etc. We used each of these methods to independently rank all captions from our corpus and selected the top captions for each method. Then, we performed Amazon Mechanical Turk experiments in which we asked Turkers to judge which of the selected captions is funnier. Figure 1: Cartoon number 31 Figure 2: Cartoon number 32 2. Related Work In early work, Mihalcea and Strapparava (2005) investigate whether classification techniques can distinguish between humorous and non-humorous text. Training data consisted of humorous one-liners (15 words or less), and non-humorous one-liners, which are derived from Reuters news titles, proverbs, and sentences from the British National Corpus. They looked at features such as alliteration, antonymy and adult slang. Mihalcea and Pullman (2007) took this work further. They looked at four semantic classes relevant to humancenteredness: persons, social groups, social relationships, and personal pronouns. They showed that social relationships"
L16-1076,N12-2012,0,0.0309884,"Missing"
N06-1035,N04-3002,1,0.878088,"Missing"
N06-1035,2005.sigdial-1.5,0,0.0698775,"Missing"
N06-1035,E06-1037,1,0.904803,"on in prior RL approaches to dialogue systems. In this paper, we use a corpus of dialogues of humans interacting with a spoken dialogue tutoring system to show the comparative utility of adding the three features of concept repetition, frustration level, and student performance. These features are not just unique to the tutoring domain but are important to dialogue systems in general. Our empirical results show that these features all lead to changes in what action the system should take, with concept repetition and frustration having the largest effects. This paper extends our previous work (Tetreault and Litman, 2006) which first presented a methodology for exploring whether adding more complex features to a representation of student state will beneficially alter tutor actions with respect to feedback. Here we present an empirical method of comparing the effects of each feature while also generalizing our findings to a different action choice of what type of follow-up question should a tutor ask the student (as opposed to what type of feedback should the tutor give). In complex domains such as tutoring, testing different policies with real or simulated students can be time consuming and costly so it is imp"
N06-1035,2005.sigdial-1.4,0,0.0642802,"n dialogue tutoring system. In addition, we also look at the effects of these features on what type of a question a tutoring system should ask at any state and compare it with our previous work on using feedback as the system action. 1 Introduction A host of issues confront spoken dialogue system designers, such as choosing the best system action to perform given any user state, and also selecting the right features to best represent the user state. While recent work has focused on using Reinforcement Learning (RL) to address the first issue (such as (Walker, 2000), (Henderson et al., 2005), (Williams et al., 2005a)), there has been very little empirical work on the issue of feature selection in prior RL approaches to dialogue systems. In this paper, we use a corpus of dialogues of humans interacting with a spoken dialogue tutoring system to show the comparative utility of adding the three features of concept repetition, frustration level, and student performance. These features are not just unique to the tutoring domain but are important to dialogue systems in general. Our empirical results show that these features all lead to changes in what action the system should take, with concept repetition and"
N07-1035,2005.sigdial-1.5,0,0.063739,"Missing"
N07-1035,N06-1035,1,0.81042,"ess the reliability of the expected cumulative reward for a given policy, and (2) perform a refined comparison between policies derived from different MDP models. NLP researchers frequently have to deal with issues of data sparsity. Whether the task is machine translation or named-entity recognition, the amount of data one has to train or test with can greatly impact the reliability and robustness of one’s models, results and conclusions. One research area that is particularly sensitive to the data sparsity issue is machine learning, specifiWe apply the proposed approach to our previous work (Tetreault and Litman, 2006) in using RL to improve a spoken dialogue tutoring system. In that work, a dataset of 100 dialogues was used to develop a methodology for selecting which user state features should be included in the MDP state-space. But are 100 dialogues enough to generate reliable policies? In this paper we apply our confidence inAbstract Past approaches for using reinforcement learning to derive dialog control policies have assumed that there was enough collected data to derive a reliable policy. In this paper we present a methodology for numerically constructing confidence intervals for the expected cumula"
N07-2001,2005.sigdial-1.6,0,0.0590345,"ulates realistic user behaviors in a statistical way. 1 Introduction Recently, user simulation has been used in the development of spoken dialog systems. In contrast to experiments with human subjects, which are usually expensive and time consuming, user simulation generates a large corpus of user behaviors in a low-cost and time-efficient manner. For example, user simulation has been used in evaluation of spoken dialog systems (L´opez-C´ozar et al., 2003) and to learn dialog strategies (Scheffler, 2002). However, these studies do not systematically evaluate how helpful a user simulation is. (Schatzmann et al., 2005) propose a set of evaluation measures to assess the realness of the simulated corpora (i.e. how similar are the simulated behaviors and human behaviors). Nevertheless, how realistic a simulated corpus needs to be for different tasks is still an open question. We hypothesize that for tasks like system evaluation, a more realistic simulated corpus is preferable. Since the system strategies are evaluated and adapted based on the analysis of these simulated dialog behaviors, we would expect that these behaviors are what we are going to see in the test phase when the systems interact with human use"
N07-2001,N06-1035,1,0.860394,"Missing"
N07-2011,P04-1045,1,0.785094,"tman@cs.pitt.edu Abstract We use χ2 to investigate the context dependency of student affect in our computer tutoring dialogues, targeting uncertainty in student answers in 3 automatically monitorable contexts. Our results show significant dependencies between uncertain answers and specific contexts. Identification and analysis of these dependencies is our first step in developing an adaptive version of our dialogue system. 1 Introduction Detecting and adapting to user affect is being explored by many researchers to improve dialogue system quality. Detection has received much attention (e.g., (Litman and Forbes-Riley, 2004; Lee and Narayanan, 2005)), but less work has been done on adaptation, due to the difficulty of developing responses and applying them at the right time. Most work on adaptation takes a context-independent approach: use the same type of response after all instances of an affective state. For example, Liu and Picard (2005)’s health assessment system responds with empathy to all instances of user stress. Research suggests, however, that it may be more effective to take a context-dependent approach: develop multiple responses for each affective state, whose use depends on the state’s context. E."
N07-2011,W06-1611,1,0.889642,"Missing"
N10-1099,N04-1024,1,\N,Missing
N10-1099,J97-1003,0,\N,Missing
N10-1099,D08-1020,0,\N,Missing
N10-1099,J95-2003,0,\N,Missing
N10-1099,J08-1001,0,\N,Missing
N12-1003,W01-1605,0,0.103125,"nent’s claims (e.g., “The argument states that”) • to evaluate an opponent’s claims (e.g., “It may seem reasonable at first glance, but actually, there are some logical mistakes in it”) • to present evidence and relate it to specific claims (e.g., “To illustrate my point, I will now give the example of”) There are many ways of analyzing discourse. The most relevant is perhaps rhetorical structure theory (RST) (Mann and Thompson, 1988). To our knowledge, the RST parser from Marcu (2000) is the only RST parser readily available for experimentation. The parser is trained to model the RST corpus (Carlson et al., 2001), which treats complete clauses (i.e., clauses with their obligatory complements) as the elementary units of analysis. Thus, the parser treats the first sentence in example 1 as a single unit and does not differentiate between the main and subordinate clauses. In contrast, our approach distinguishes 21 the sequence “The argument states that . . . ” as shell (which is used here to restate the external claim). Furthermore, we identify the entire second sentence as shell (here, used to evaluate the external claim), whereas the RST parser splits the sentence into two clauses, “It may seem . . .” a"
N12-1003,1993.eamt-1.1,0,0.348313,"Missing"
N12-1003,P11-1099,0,0.0677393,"mber of relationships. On the other hand, shell can capture longer sequences that express more complex relationships between the components of an argumentative discourse (e.g., “But let’s get back to the core issue here” signals that the following point is more important than the previous one). Acknowledgments There are also various other approaches to analyzing arguments. Notably, much recent theoretical research on argumentation has focused on argumentation schemes (Walton et al., 2008), which are high-level strategies for constructing arguments (e.g., argument from consequences). Recently, Feng and Hirst (2011) developed automated methods for classifying texts by argumentation scheme. In similar work, Anand et al. (2011) use argumentation schemes to identify tactics in blog posts (e.g., moral appeal, social generalization, appeals to external authorities etc.). Although shell language can certainly be found in persuasive writing, it is used to organize the persuader’s tactics and claims rather than to express them. For example, consider the following sentence: “It must be the case that this diet works since it was recommended by someone who lost 20 pounds on it.” In shell detection, we focus on the"
N12-1003,P04-1087,0,0.0886083,"e opponents claims, connect ones own claims, etc., may be seen as determining what Grosz and Sidener call “discourse segment purposes” (i.e., the intentions underlying the segments containing the shell spans). We can also view shell detection as the task of identifying phrases that indicate certain types of speech acts (Searle, 1975). In particular, we aim to identify markers of assertive speech acts, which declare that the speaker believes a certain proposition, and expressive speech acts, which express attitudes toward propositions. Shell also overlaps with the concept of discourse markers (Hutchinson, 2004), such as “however” or TP L INCOLN (L) — D OUGLAS (D) DEBATES L: Now, I say that there is no charitable way to look at that statement, except to conclude that he is actually crazy. L: The first thing I see fit to notice is the fact that . . . FP D: He became noted as the author of the scheme to . . . D: . . . such amendments were to be made to it as would render it useless and inefficient . . . FN D: I wish to impress it upon you, that every man who voted for those resolutions . . . L: That statement he makes, too, in the teeth of the knowledge that I had made the stipulation to come down here"
N12-1003,J86-3001,0,\N,Missing
N12-1019,P08-1007,0,0.058236,"a syntactically-aware metric designed to focus on structural n-grams with long surface spans that cannot be captured efficiently with surface ngram metrics. Like BLEU, it is a precisionbased metric and requires a length penalty to minimize the effects of length. 7. BADGER (Parker, 2008) is a language independent metric based on compression and information theory. It computes a compression distance between the two sentences that utilizes the Burrows Wheeler Transformation (BWT). The BWT enables taking into account common sentence contexts with no limit on the size of these contexts. 8. MAXSIM (Chan and Ng, 2008) treats the problem as one of bipartite graph matching and maps each word in one sentence to at most one word in the other sentence. It allows the use of arbitrary similarity functions between words.2 Our choice of metrics was based on their popularity in the MT community, their performance in open competitions such as the NIST MetricsMATR challenge (NIST, 2008) and the WMT shared evaluation task (Callison-Burch et al., 2010), their availability, and their relative complementarity. 3.3 Datasets In this section, we describe the two datasets that we used to evaluate our approach. 3.3.1 Microsoft"
N12-1019,P09-1053,0,0.648384,"based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for paraphrase identification. Fernando and Stevenson (2008) construct a matrix of word similarities between all pairs of words in both sentences instead of relying only on the maximal similarities. Das and Smith (2009) use an explicit model of alignment between the corresponding parts of two paraphrastic sentences and combine it with a logistic regression classifier built from n-gram overlap features. Most recently, Socher et al. (2011) employ a joint model that incorporates the similarities between both single word features as well as multi-word phrases extracted from the parse trees of the two sentences. We compare our results to those from all the approaches described in this section later in §3.4. 3 Classifying with MT Metrics In this section, we first describe our overall approach to paraphrase identif"
N12-1019,N10-1031,0,0.034254,"ording to its informativeness as indicated by its frequency. We use NIST1 through NIST5 as 5 different features for our classifier (hereafter NIST(1-5)). 3. TER (Snover et al., 2006) is defined as the number of edits needed to “fix” the translation output so that it matches the reference. TER differs from WER in that it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 4. TERp (TER-Plus) (Snover et al., 2009) builds upon the core TER algorithm by providing additional edit operations based on stemming, synonymy and paraphrase. 5. METEOR (Denkowski and Lavie, 2010) uses a combination of both precision and recall unlike BLEU which focuses on precision. Furthermore, it incorporates stemming, synonymy (via WordNet) and paraphrase (via a lookup table). 6. SEPIA (Habash and El Kholy, 2008) is a syntactically-aware metric designed to focus on structural n-grams with long surface spans that cannot be captured efficiently with surface ngram metrics. Like BLEU, it is a precisionbased metric and requires a length penalty to minimize the effects of length. 7. BADGER (Parker, 2008) is a language independent metric based on compression and information theory. It com"
N12-1019,C04-1051,0,0.850577,"anslation hypothesis produced by a system is semantically equivalent to the source sentence that was translated. However, cross-lingual semantic equivalence is even harder to assess than monolingual, therefore, most MT metrics instead try to measure whether the hypothesis is semantically equivalent to a human-authored reference translation of the same source sentence. Using such automated metrics as This paper describes such a re-examination. We employ 8 different MT metrics for identifying paraphrases across two different datasets - the well-known Microsoft Research paraphrase corpus (MSRP) (Dolan et al., 2004) and the plagiarism detection corpus (PAN) from the 2010 Uncovering Plagiarism, Authorship and Social Software Misuse shared task (Potthast et al., 2010). We include both MSRP and PAN in our study because they represent two very different sources of paraphrased text. The creation of MSRP relied on the massive redundancy of news articles on the web and extracted sentential paraphrases from different stories written about the same topic. In the case of PAN, humans consciously paraphrased existing text to generate new, 182 2012 Conference of the North American Chapter of the Association for Compu"
N12-1019,I05-5003,0,0.777794,"olely on MT metrics. We release both the new dataset and the error analysis annotations for use by the community. 1 In the last 5 years, several shared tasks and competitions have led to the development of increasingly sophisticated metrics that go beyond the computation of n-gram overlaps (BLEU, NIST) or edit distances (TER, WER, PER etc.). Note that the task of an MT metric is essentially one of identifying whether the translation produced by a system is a paraphrase of the reference translation. Although the notion of using MT metrics for the task of paraphrase identification is not novel (Finch et al., 2005; Wan et al., 2006), it merits a re-examination in the light of the development of these novel MT metrics for which we can ask “How much better, if at all, do these newer metrics perform for the task of paraphrase identification?” Introduction One of the most important reasons for the recent advances made in Statistical Machine Translation (SMT) has been the development of automated metrics for evaluation of translation quality. The goal of any such metric is to assess whether the translation hypothesis produced by a system is semantically equivalent to the source sentence that was translated."
N12-1019,P11-2089,1,0.745416,"1 indicating the degree to which two pairs are paraphrastic is more suitable for most approaches. However, rather than asking annotators to rate pairs on a scale, a better idea might be to show the sentence pairs to a large number of Turkers (≥ 20) on Amazon Mechanical Turk and ask them to classify it as either a paraphrase or a non-paraphrase. A simple estimate of the degree of semantic equivalence of the pair is simply the proportion of the Turkers who classified the pair as paraphrastic. An example of such an approach, as applied to the task of grammatical error detection, can be found in (Madnani et al., 2011).3 Second, we believe that the PAN corpus— with Turker simulated plagiarism—contains much more realistic examples of paraphrase and should be incorporated into future evaluations of paraphrase identification. In order to encourage this, we are releasing our PAN dataset containing 13,000 sentence pairs. We are also releasing our error analysis data (100 pairs for MSRP and 100 pairs for PAN) since they might prove useful to other researchers as well. Note that the annotations for this analysis were produced by the authors themselves and, although, they attempted to accurately identify all error"
N12-1019,P02-1040,0,0.0953774,"r BLEU(1-4)). Our best system utilized a classifier combination approach. We used a simple meta-classifier that uses the average of the unweighted probability estimates from the constituent classifiers to make its final decision. We used three constituent classifiers: Logistic regression, the SMO implementation of a support vector machine (Platt, 1999; Keerthi et al., 2001) and a lazy, instance-based classifier that extends the nearest neighbor algorithm (Aha et al., 1991). We used the WEKA machine learning toolkit to perform our experiments (Hall et al., 2009). 1 3.2 MT metrics used 1. BLEU (Papineni et al., 2002) is the most commonly used metric for MT evaluation. It is computed as the amount of n-gram overlap— for different values of n—between the system output and the reference translation, tempered by a penalty for translations that might be too short. BLEU relies on exact matching and has no concept of synonymy or paraphrasing. We use BLEU1 through BLEU4 as 4 different fea1 These constituent classifiers were chosen since they were the top 3 performers in 5-fold cross-validation experiments conducted on both MSRP and PAN training sets. The metaclassifier was chosen similarly once the constituent cl"
N12-1019,C10-2115,0,0.0351299,"Missing"
N12-1019,W06-1603,0,0.304937,"rase and can prove useful to the 183 community for future evaluations of paraphrase identification. BLEU-based features were also employed by Wan et al. (2006) who use them in combination with several other features based on dependency relations and tree edit-distance inside an SVM. There are several other supervised approaches to paraphrase identification that do not use any features based on MT metrics. Mihalcea et al. (2006) combine pointwise mutual information, latent semantic analysis and WordNet-based measures of word semantic similarity into an arbitrary text-to-text similarity metric. Qiu et al. (2006) build a framework that detects dissimilarities between sentences and makes its paraphrase judgment based on the significance of such dissimilarities. Kozareva and Montoyo (2006) use features based on LCS, skip n-grams and WordNet with a meta-classifier composed of SVM, k-nearest neighbor and maximum entropy classifiers. Islam and Inkpen (2007) measure semantic similarity using a corpus-based measure and a modified version of the Longest Common Subsequence (LCS) algorithm. Rus et al. (2008) take a graph-based approach originally developed for recognizing textual entailment and adapt it for par"
N12-1019,2006.amta-papers.25,0,0.0825834,"s 4 different fea1 These constituent classifiers were chosen since they were the top 3 performers in 5-fold cross-validation experiments conducted on both MSRP and PAN training sets. The metaclassifier was chosen similarly once the constituent classifiers had been chosen. 184 2. NIST (Doddington, 2002) is a variant of BLEU that uses the arithmetic mean of n-gram overlaps, rather than the geometric mean. It also weights each n-gram according to its informativeness as indicated by its frequency. We use NIST1 through NIST5 as 5 different features for our classifier (hereafter NIST(1-5)). 3. TER (Snover et al., 2006) is defined as the number of edits needed to “fix” the translation output so that it matches the reference. TER differs from WER in that it includes a heuristic algorithm to deal with shifts in addition to insertions, deletions and substitutions. 4. TERp (TER-Plus) (Snover et al., 2009) builds upon the core TER algorithm by providing additional edit operations based on stemming, synonymy and paraphrase. 5. METEOR (Denkowski and Lavie, 2010) uses a combination of both precision and recall unlike BLEU which focuses on precision. Furthermore, it incorporates stemming, synonymy (via WordNet) and p"
N12-1019,U06-1019,0,0.930952,"We release both the new dataset and the error analysis annotations for use by the community. 1 In the last 5 years, several shared tasks and competitions have led to the development of increasingly sophisticated metrics that go beyond the computation of n-gram overlaps (BLEU, NIST) or edit distances (TER, WER, PER etc.). Note that the task of an MT metric is essentially one of identifying whether the translation produced by a system is a paraphrase of the reference translation. Although the notion of using MT metrics for the task of paraphrase identification is not novel (Finch et al., 2005; Wan et al., 2006), it merits a re-examination in the light of the development of these novel MT metrics for which we can ask “How much better, if at all, do these newer metrics perform for the task of paraphrase identification?” Introduction One of the most important reasons for the recent advances made in Statistical Machine Translation (SMT) has been the development of automated metrics for evaluation of translation quality. The goal of any such metric is to assess whether the translation hypothesis produced by a system is semantically equivalent to the source sentence that was translated. However, cross-lin"
N12-1019,O97-1002,0,\N,Missing
N12-1029,P06-2001,0,0.0611946,"Missing"
N12-1029,1995.iwpt-1.8,0,0.277359,"Missing"
N12-1029,P11-1092,0,0.0405999,"Missing"
N12-1029,W10-4236,0,0.0979335,"Missing"
N12-1029,C08-1022,0,0.0517037,"Missing"
N12-1029,W11-1410,1,0.721123,"ster task of restoring commas in well-formed text. For both tasks, we show that the use of novel features which encode long-distance information improves upon the more lexically-driven features used in prior work. 1 Introduction Automatically detecting and correcting grammatical errors in learner language is a growing sub-field of Natural Language Processing. As the field has progressed, we have seen research focusing on a range of grammatical phenomena including English articles and prepositions (c.f. Tetreault et al., 2010; De Felice and Pulman, 2008), particles in Korean and Japanese (c.f. Dickinson et al., 2011; Oyama, 2010), and broad approaches that aim to find multiple error types (c.f Rozovskaya et al., 2011; Gamon, 2011). However, to the best of our knowledge, there has not been any research published specifically on correcting erroneous comma usage in English (though there have been efforts such as the MS Word grammar checker, and products like Grammarly and White Smoke that include comma checking). There are a variety of reasons that motivate our interest in attempting to correct comma errors. First of all, a review of error typologies in Leacock et al. (2010) reveals that comma usage errors"
N12-1029,N10-1019,0,0.0386973,"e POS features abstract away from the words and avoid the problem of data sparseness by allowing the classifier to focus on the categories of the words, rather than the lexical items themselves. The combination (combo) feature is a unigram of the word+pos for every word in the sliding window. It reinforces the relationship between the lexical items and their POS tags, further strengthening the evidence of entries like then RB. All of these features have been used in previous grammatical error detection tasks which target particle, article, and preposition errors (c.f., Dickinson et al., 2011; Gamon, 2010; Tetreault and Chodorow, 2008). The first combo feature keeps track of the first combination feature of the sentence so that it can be referred to by the classifier throughout processing the entire sentence. This feature is helpful when an introductory phrase is longer than the classifier’s five word window. Figure 1 provides a good example of the utility of this feature, as If the teacher easily gets mad is so long that by the time the window has moved to the target position of the space following mad, the first word and POS, If RB, which can often indicate an introductory phrase, is beyond"
N12-1029,W11-1422,0,0.0692504,"tance information improves upon the more lexically-driven features used in prior work. 1 Introduction Automatically detecting and correcting grammatical errors in learner language is a growing sub-field of Natural Language Processing. As the field has progressed, we have seen research focusing on a range of grammatical phenomena including English articles and prepositions (c.f. Tetreault et al., 2010; De Felice and Pulman, 2008), particles in Korean and Japanese (c.f. Dickinson et al., 2011; Oyama, 2010), and broad approaches that aim to find multiple error types (c.f Rozovskaya et al., 2011; Gamon, 2011). However, to the best of our knowledge, there has not been any research published specifically on correcting erroneous comma usage in English (though there have been efforts such as the MS Word grammar checker, and products like Grammarly and White Smoke that include comma checking). There are a variety of reasons that motivate our interest in attempting to correct comma errors. First of all, a review of error typologies in Leacock et al. (2010) reveals that comma usage errors are the fourth most common error type among non-native writers in the Cambridge Learner Corpus (Nicholls, 1999), whic"
N12-1029,C94-1069,0,0.286386,"Missing"
N12-1029,D10-1018,0,0.0213926,"words and POS tags in a sliding 5 word window (target word, +/- 2 words). The lexical items help to encode any idiosyncratic relationships between words and commas that might not be exploited through the examination of more in-depth linguistic features. For example, then is a special case of an adverb (RB) that is often preceded by a comma, even if other adverbs are not, so POS tags might not capture this relationship. The lexical items also provide an approximation of a language model or hidden event language model approach, which has proven to be useful in comma restoration tasks (see e.g. Lu and Ng, 2010). The POS features abstract away from the words and avoid the problem of data sparseness by allowing the classifier to focus on the categories of the words, rather than the lexical items themselves. The combination (combo) feature is a unigram of the word+pos for every word in the sliding window. It reinforces the relationship between the lexical items and their POS tags, further strengthening the evidence of entries like then RB. All of these features have been used in previous grammatical error detection tasks which target particle, article, and preposition errors (c.f., Dickinson et al., 20"
N12-1029,W96-0213,0,0.35576,"omma error correction, and have not, as far as we know, been utilized in previous research. 5 Comma Restoration Before applying our system to the task of error correction, we tested its utility in restoring commas in newswire texts. Specifically, we evaluate on section 23 of the WSJ, training on sections 02-22. Here, the task is straightforward: we remove all commas from the test data and performance is measured on the system’s ability to put the commas back in the right places. After stripping all commas from our test data, the text is tokenized and POS tagged using a maximum entropy tagger (Ratnaparkhi, 1996) and every token is considered by the classifier as either requiring a following comma or not. Out of 53,640 tokens, 3062 should be followed by a comma. We provide accuracy, precision, recall, F1 -score, and sentence accuracy (S Acc.) for these tests, along with results from Gravano et al. (2009) and Shieber and Tao (2003) in Table 2. The first system (LexSyn) includes only the lexical and syntactic features from Figure 1; the second (LexSyn+Dist) includes all of the features. System LexSyn LexSyn+Dist Shieber & Tao Gravano et al. Acc. 97.4 97.5 97.0 N.A. P 85.8 85.8 79.7 57 R 64.9 66.3 62.6 6"
N12-1029,W11-2843,0,0.0477105,"res which encode long-distance information improves upon the more lexically-driven features used in prior work. 1 Introduction Automatically detecting and correcting grammatical errors in learner language is a growing sub-field of Natural Language Processing. As the field has progressed, we have seen research focusing on a range of grammatical phenomena including English articles and prepositions (c.f. Tetreault et al., 2010; De Felice and Pulman, 2008), particles in Korean and Japanese (c.f. Dickinson et al., 2011; Oyama, 2010), and broad approaches that aim to find multiple error types (c.f Rozovskaya et al., 2011; Gamon, 2011). However, to the best of our knowledge, there has not been any research published specifically on correcting erroneous comma usage in English (though there have been efforts such as the MS Word grammar checker, and products like Grammarly and White Smoke that include comma checking). There are a variety of reasons that motivate our interest in attempting to correct comma errors. First of all, a review of error typologies in Leacock et al. (2010) reveals that comma usage errors are the fourth most common error type among non-native writers in the Cambridge Learner Corpus (Nicholl"
N12-1029,N03-1029,0,0.301522,"Missing"
N12-1029,C08-1109,1,0.866655,"s abstract away from the words and avoid the problem of data sparseness by allowing the classifier to focus on the categories of the words, rather than the lexical items themselves. The combination (combo) feature is a unigram of the word+pos for every word in the sliding window. It reinforces the relationship between the lexical items and their POS tags, further strengthening the evidence of entries like then RB. All of these features have been used in previous grammatical error detection tasks which target particle, article, and preposition errors (c.f., Dickinson et al., 2011; Gamon, 2010; Tetreault and Chodorow, 2008). The first combo feature keeps track of the first combination feature of the sentence so that it can be referred to by the classifier throughout processing the entire sentence. This feature is helpful when an introductory phrase is longer than the classifier’s five word window. Figure 1 provides a good example of the utility of this feature, as If the teacher easily gets mad is so long that by the time the window has moved to the target position of the space following mad, the first word and POS, If RB, which can often indicate an introductory phrase, is beyond the scope of the sliding window"
N12-1029,P10-2065,1,0.918,"Missing"
N12-1029,W08-1703,0,\N,Missing
N13-1055,C12-1038,1,0.818668,"ion corrections Total number of corrections suggested Number of correct preposition corrections Total number of corrections in test set Note that due to the high volume of unchanged prepositions in the test corpus, we obtain very high accuracies, which are not indicative of true performance, and are not included in our results. The results of our experiments are presented in Table 2.12 The first part of the table shows the fscores of preposition error correction systems that 10 We use liblinear (Fan et al., 2008) with the L1-regularized logistic regression solver and default parameters. 11 As Chodorow et al. (2012) note, it is not clear how to handle cases where the system predicts a preposition that is neither the same as the writer preposition nor the correct preposition. We count these cases as false positives. 12 No thresholds were used in the systems that were trained on well-edited text. Traditionally, thresholds are applied so as to only predict a correction when the system is highly confident. This has the effect of increasing precision at the cost of recall, and sometimes leads to an overall improved f-score. Here we take the prediction of the system, regardless of the confidence, reflecting a"
N13-1055,P11-1092,0,0.466589,"he usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and"
N13-1055,W11-2838,0,0.0132963,"on either side of the preposition (∼clean). The third contains all corrections regardless of any other changes in the surrounding context (all). Lang-8 The Lang-8 website contains journals written by language learners, where native speakers highlight and correct errors on a sentence-bysentence basis. As a result, it contains typical grammatical mistakes made by language learners, which can be easily downloaded. We automatically extract 75,622 sentences with preposition errors and corrections from the first million journal entries.7 HOO 2011 We take the test set from the HOO 2011 shared task (Dale and Kilgarriff, 2011) and extract all examples of preposition selection errors. The texts are fragments of ACL papers that have been manually annotated for grammatical errors.8 It is important to note that the three test sets we use are from entirely different domains: exam scripts from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault"
N13-1055,W12-2006,0,0.0438256,"and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction"
N13-1055,P11-4017,0,0.0264218,"al., 2012). To our knowledge, no one has previously extracted data for training a grammatical error detection system from Wikipedia revisions. 3.2 Extracting Preposition Correction Data from Wikipedia Revisions As the source of our Wikipedia revisions, we used an XML snapshot of Wikipedia generated in July 2011 containing 8,735,890 articles and 288,583,063 revisions.1 We then used the following process to extract preposition errors and their corresponding corrections from this snapshot: Step 1: Extract the plain text versions of all revisions of all articles using the Java Wikipedia Library (Ferschke et al., 2011). Step 2: For each Wikipedia article, compare each revision with the revision immediately preceding it using an efficient diff algorithm.2 Step 3: Compute all 1-word edit chains for the article, i.e., sequences of related edits derived from all revisions of the same article. For example, say revision 10 of an article inserts the preposition of into a sentence and revision 12 changes that preposition to on. Assuming that no other revisions change this sentence, the corresponding edit chain would contain the following 3 elements: →of→on. The extracted chains contain the full context on either s"
N13-1055,W09-2112,0,0.633063,"Missing"
N13-1055,I08-1059,0,0.0273622,"n welledited text? 3. What is the impact of having a mismatch in the error distributions of the training and test sets? 2 Related Work In this section, we only review work in preposition error correction in terms of the three training paradigms and refer the reader to Leacock et al. (2010) for a more comprehensive review of the field. 2.1 Training on Well-Edited Text Early approaches to error detection and correction did not have access to large amounts of errorannotated data to train statistical models and thus, systems were trained on millions of well-edited examples from news text instead (Gamon et al., 2008; Tetreault and Chodorow, 2008; De Felice and Pulman, 2009). Feature sets usually consisted of ngrams around the preposition, POS sequences, syntactic features and semantic information. Since the model only had knowledge of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for ma"
N13-1055,N10-1019,0,0.0565523,"dressing domain adaptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical learner errors. Han et al. (2010) showed that a preposition error detection and correction system trained on 100,000 annotated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of correct usage) outperformed a model trained only on 5 million examples of correct usage. Gamon (2010) and Dahlmeier and Ng (2011) showed that combining models trained separately on examples of correct and incorrect usage could also improve the performance of a preposition error correction system. 3 3.1 Mining Wikipedia Revisions for Grammatical Error Corrections Related Work Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types"
N13-1055,P12-2076,0,0.0554385,"rror distributions derived from the error-annotated learner corpora available at the time. Izumi et al. (2003) was the first to evaluate a model trained on incorrect usage as well as artificial errors for the task of correcting several different 508 error types, including prepositions. However, with limited training data, system performance was quite poor. Rozovskaya and Roth (2010) evaluated different ways of generating artificial errors and found that a system trained on artificial errors could outperform the more traditional training paradigm of using only well-edited texts. Most recently, Imamura et al. (2012) showed that performance could be improved by training a model on artificial errors and addressing domain adaptation for the task of Japanese particle correction. 2.3 Error-Annotated Learner Corpora Recently, error-annotated learner data has become more readily and publicly available allowing models to be trained on both examples of correct usage as well typical learner errors. Han et al. (2010) showed that a preposition error detection and correction system trained on 100,000 annotated preposition errors from the Chungdahm Corpus of Korean Learner English (in addition to 1 million examples of"
N13-1055,P03-2026,0,0.440635,"of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for many years, such error-annotated corpora were not available. Instead, several researchers generated artificial errors based on the error distributions derived from the error-annotated learner corpora available at the time. Izumi et al. (2003) was the first to evaluate a model trained on incorrect usage as well as artificial errors for the task of correcting several different 508 error types, including prepositions. However, with limited training data, system performance was quite poor. Rozovskaya and Roth (2010) evaluated different ways of generating artificial errors and found that a system trained on artificial errors could outperform the more traditional training paradigm of using only well-edited texts. Most recently, Imamura et al. (2012) showed that performance could be improved by training a model on artificial errors and a"
N13-1055,max-wisniewski-2010-mining,0,0.390095,"ect usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction (Zesch, 2012), this resource has not been used for the task of grammatical error correction. To evaluate the usefulness of Wikipedia revision history for grammatical error correction, we address the task of correcting errors in preposition selection (i.e., where the context licenses the use of a preposition, but the writer selects the wrong one). We first train a model directly on instances of correct and incorrect preposition usage extracted from the Wikipedia revision data. We also generate artificial errors using the confusion distributio"
N13-1055,D10-1094,0,0.513155,"using rules and regular expressions (Leacock et al., 2010). On the other hand, errors involving the usage of prepositions and articles are influenced by several factors including the local context, the prior discourse and semantics. These errors are better handled by statistical models which potentially require millions of training examples. Most statistical approaches to grammatical error correction have used one of the following training paradigms: 1) training solely on examples of correct usage (Han et al., 2006); 2) training on examples of correct usage and artificially generated errors (Rozovskaya and Roth, 2010); and 3) training on examples of correct usage and real learner errors (Dahlmeier and Ng, 2011; Dale et al., 2012). The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP ta"
N13-1055,P12-2039,0,0.0232089,"ish speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7 Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8 The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results. 9 Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itself does not change. 511 and De Felice and Pulman (2009). In short, the method models the problem o"
N13-1055,C08-1109,1,0.960607,". What is the impact of having a mismatch in the error distributions of the training and test sets? 2 Related Work In this section, we only review work in preposition error correction in terms of the three training paradigms and refer the reader to Leacock et al. (2010) for a more comprehensive review of the field. 2.1 Training on Well-Edited Text Early approaches to error detection and correction did not have access to large amounts of errorannotated data to train statistical models and thus, systems were trained on millions of well-edited examples from news text instead (Gamon et al., 2008; Tetreault and Chodorow, 2008; De Felice and Pulman, 2009). Feature sets usually consisted of ngrams around the preposition, POS sequences, syntactic features and semantic information. Since the model only had knowledge of correct usage, an error was flagged if the system’s prediction for a particular preposition context differed from the preposition the writer used. 2.2 Artificial Errors The issue with training solely on correct usage was that the systems had no knowledge of typical learner errors. Ideally, a system would be trained on examples of correct and incorrect usage, however, for many years, such error-annotated"
N13-1055,P10-2065,1,0.572162,"s we use are from entirely different domains: exam scripts from non-native English speakers (FCE), essays by highly proficient college students in Singapore (NUCLE) and ACL papers (HOO). In addition, they have a different number of total prepositions as well as erroneous prepositions. 5 Preposition Error Correction Experiments We use the preposition error correction model described in Tetreault and Chodorow (2008)9 to evaluate the many ways of using Wikipedia error corrections as described in the Section 4. We use this system since it has been recreated for other work (Dahlmeier and Ng, 2011; Tetreault et al., 2010) and is similar in methodology to Gamon et al. (2008) 7 Tajiri et al. (2012) extract a corpus of English verb phrases corrected for tense/aspect errors from Lang-8. They kindly provided us with their scripts to carry out the scraping of Lang-8. 8 The results of the HOO 2011 shared task were not reported at level of preposition selection error, therefore it is not possible to compare the results presented in this paper with those results. 9 Note that in that work, the model was evaluated in terms of preposition error detection rather than correction, however the model itself does not change. 51"
N13-1055,N03-1033,0,0.00614282,"here a preposition is replaced with another preposition. If the preposition edit is the only edit in the sentence, we convert the chain into a sentence pair and label it clean. If there are other 1-word edits but not within 5 words of the preposition edit on either side, we label the sentence somewhat clean. Otherwise, we label it dirty. The motivation is that the presence of other nearby edits make the preposition correction less reliable when used in isolation, due to the possible dependencies between corrections. All extracted sentences were part-of-speech tagged using the Stanford Tagger (Toutanova et al., 2003). Using the above process, we are able to extract approximately 2 million sentences containing prepositions errors and their corrections. Some examples of the sentences we extracted are given in Figure 1. Example (4) shows an example of a bad correction. 4 Corpora We use several corpora for training and testing our preposition error correction system. The properties of each are outlined in Table 1, organized by paradigm. For each corpus we report the total number of prepositions used for training, as well as the number and percentage of preposition corrections. 4.1 Well-edited Text We train ou"
N13-1055,P08-2035,0,0.019275,"Many NLP researchers have taken advantage of the wealth of information available in Wikipedia revisions. Dutrey et al. (2011) define a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at → in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of → on) those dates supports Ranneft’s claims. (3) [Wiki dirty] . . . cirque has a permanent production (to → at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his performances for French touris"
N13-1055,P11-1019,0,0.0895178,"hanged. 4.3 Naturally Occurring Errors We have a number of corpora that contain annotated preposition errors. Note that we are only considering incorrectly selected prepositions, we do not consider missing or extraneous. NUCLE The NUS Corpus of Learner English (NUCLE)5 contains one million words of learner essay text, manually annotated with error tags and corrections. We use the same training, dev and test splits as Dahlmeier and Ng (2011). FCE The CLC FCE Dataset6 is a collection of 1,244 exam scripts written by learners of English as part of the Cambridge ESOL First Certificate in English (Yannakoudakis et al., 2011). It includes demographic metadata about the candidate, a grade for each essay and manuallyannotated error corrections. Wikipedia We use three versions of the preposition errors extracted from the Wikipedia revisions as described in Section 3.2. The first includes corrections where the preposition was the only word corrected in the entire sentence 5 6 http://bit.ly/nuclecorpus http://ilexir.co.uk/applications/clc-fce-dataset/ (clean). The second contains all clean corrections, as well as all corrections where there were no other edits within a five-word span on either side of the preposition ("
N13-1055,N10-1056,0,0.0707676,"ine a typology of modifications found in the French Wikipedia (WiCoPaCo). They show that the kinds of edits made range from specific lexical changes to more general rewrite edits. Similar types of edits are found in the English Wikipedia. The data extracted from Wikipedia revisions has been used for a wide variety of tasks including spelling correction (Max and Wisniewski, 2010; Zesch, 2012), lexical error detection (Nelken and Yamangil, 2008), sentence compression (Yamangil and Nelken, 2008), paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008), lexical simplification (Yatskar et al., 2010) and entailment (Zanzotto and Pennacchiotti, 2010; (1) [Wiki clean] In addition, sometimes it is also left to stand overnight (at → in) the refrigerator. (2) [Wiki clean] Also none of the witnesses present (of → on) those dates supports Ranneft’s claims. (3) [Wiki dirty] . . . cirque has a permanent production (to → at) the Mirage, love. (4) [Wiki dirty] In the late 19th century Vasilli Andreyev a salon violinist took up the balalaika in his performances for French tourists (in → to) Petersburg. Figure 1: Example sentences with preposition errors extracted from Wikipedia revisions. The second"
N13-1055,E12-1054,0,0.192817,"The latter two methods require annotated corpora of errors, and while they have shown great promise, manually annotating grammatical errors in a large enough corpus of learner writing is often a costly and time-consuming endeavor. In order to efficiently and automatically acquire a very large corpus of annotated learner errors, we investigate the use of error corrections extracted from Wikipedia revision history. While Wikipedia revision history has shown promise for other NLP tasks including paraphrase generation (Max and Wisniewski, 2010; Nelken and Yamangil, 2008) and spelling correction (Zesch, 2012), this resource has not been used for the task of grammatical error correction. To evaluate the usefulness of Wikipedia revision history for grammatical error correction, we address the task of correcting errors in preposition selection (i.e., where the context licenses the use of a preposition, but the writer selects the wrong one). We first train a model directly on instances of correct and incorrect preposition usage extracted from the Wikipedia revision data. We also generate artificial errors using the confusion distributions derived from this data. We compare both of these approaches to"
N13-1055,han-etal-2010-using,1,\N,Missing
N13-1055,W10-3504,0,\N,Missing
N18-1012,P14-2029,1,0.844587,"3 Formality: Following PT16, workers rate the formality of the source style sentence, the target style reference rewrite and the target style model outputs on a discrete scale of -3 to +3 described as: -3: Very Informal, -2: Informal, -1: Somewhat Informal, 0: Neutral, 1: Somewhat Formal, 2: Formal and 3: Very Formal. cases the annotations looked correct. But as is common in any such crowdsourced data collection process, there were some errors, especially in the overall ranking of the systems. 5.2 We cover each of the human evaluations with a corresponding automatic metric: Fluency: Following Heilman et al. (2014), workers rate the fluency of the source style sentence, the target style reference rewrite and the target style model outputs on a discrete scale of 1 to 5 described as: 5: Perfect, 4: Comprehensible, 3: Somewhat Comprehensible, 2: Incomprehensible. We additionally provide an option of 1: Other for sentences that are incomplete or just a fragment. Formality: We use the formality classifier described in PT16. We find that the classifier trained on the answers genre of PT16 dataset does not perform well when tested on our datasets. Hence, we collect formality judgments for an additional 5000 se"
N18-1012,S16-1081,0,0.0364145,"es not perform well when tested on our datasets. Hence, we collect formality judgments for an additional 5000 sentences and use the formality classifier re-trained on this in-domain data. Fluency: We use the reimplementation13 of Heilman et al. (2014) (H14 in Table 4) which is a statistical model for predicting the grammaticality of a sentence on a scale of 0 to 4 previously shown to be effective for other generation tasks like grammatical error correction (Napoles et al., 2016). Meaning Preservation: Following the annotation scheme developed for the Semantic Textual Similarity (STS) dataset (Agirre et al., 2016), given two sentences i.e. the source style sentence and the target style reference rewrite or the target style model output, workers rate the meaning similarity of the two sentences on a scale of 1 to 6 described as: 6: Completely equivalent, 5: Mostly equivalent, 4: Roughly equivalent, 3: Not equivalent but share some details, 2: Not equivalent but on same topic, 1: Completely dissimilar. Meaning Preservation: Modeling semantic similarity at a sentence level is a fundamental language processing task, and one that is a wide open field of research. Recently, He et al., (2015) (H E 15 in Table"
N18-1012,C14-1205,0,0.0359221,"model outperforms X U 12 on the same set. In text simplification, the availability of parallel data extracted from English Wikipedia and Simple Wikipedia (Zhu et al., 2010) led to the application of PBMT (Wubben et al., 2012a) and more recently NMT (Wang et al., 2016) models. We take inspiration from both the PBMT and NMT models and apply several modifications to these approaches for our task of transforming the formality style of the text. Identifying Formality: There has been previous work on detecting formality of a given text at the lexical level (Brooke et al., 2010; Lahiri et al., 2011; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015), at the sentence level (Pavlick and Tetreault, 2016) and at the document level (Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer (Xu"
N18-1012,C10-2011,0,0.0765582,"anism enriched sequenceto-sequence neural model outperforms X U 12 on the same set. In text simplification, the availability of parallel data extracted from English Wikipedia and Simple Wikipedia (Zhu et al., 2010) led to the application of PBMT (Wubben et al., 2012a) and more recently NMT (Wang et al., 2016) models. We take inspiration from both the PBMT and NMT models and apply several modifications to these approaches for our task of transforming the formality style of the text. Identifying Formality: There has been previous work on detecting formality of a given text at the lexical level (Brooke et al., 2010; Lahiri et al., 2011; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015), at the sentence level (Pavlick and Tetreault, 2016) and at the document level (Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasi"
N18-1012,D08-1021,0,0.0278407,"pweight the rule-based GYAFC data via duplication.12 5 As discussed earlier, there has been very little research into best practices for style transfer evaluation. Only a few works have included a human evaluation (Xu et al., 2012; Jhamtani et al., 2017), and automatic evaluations have employed BLEU or PINC (Xu et al., 2012; Chen and Dolan, 2011), which have been borrowed from other fields and not vetted for this task. In our work, we conduct a more thorough and detailed evaluation using both humans and automatic metrics to assess transformations. Inspired by work in the paraphrase community (Callison-Burch, 2008), we solicit ratings on how formal, how fluent and how meaning-preserving a rewrite is. Additionally, we look at the correlation between the human judgments and the automatic metrics. Neural Machine Translation While encoder-decoder based neural network models have become quite successful for MT(Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), the field of style transfer, has not yet been able to fully take advantage of these advances owing to the lack of availability of large parallel data. With GYAFC we can now show how well NMT techniques fare for style transfer. We experim"
N18-1012,P11-1020,0,0.708189,"Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has re-purposed the MT metric BLEU (Papineni et al., 2002) and the paraphrase metric PINC (Chen and Dolan, 2011) for evaluation. Additionally, X U 12 introduce three new automatic style metrics based on cosine similarity, language model and logistic regression that measure the degree to which the output matches the target style. Under human based evaluation, on the other hand, there has been work on a more fine grained evaluation where human judgments were separately collected for adequacy, fluency and style (Xu et al., 2012; Niu et al., 2017). In our work, we conduct a more thorough evaluation where we evaluate model outputs on the three criteria of formality, fluency and meaning using both automatic m"
N18-1012,C16-1109,0,0.0386185,"Missing"
N18-1012,W14-4012,0,0.0232034,"Missing"
N18-1012,P07-2045,0,0.00493226,"del to learn generalizations outside the rules. To increase the data size, we use self-training (Ueffing, 2006), where we use the PBMT model to translate the large number of in-domain sentences from GYAFC belonging to the the source style and use the resultant output to retrain the PBMT model. Using subselection, we only select rewrites that have an Levenshtein edit distance of over 10 characters when compared to the source to encourage the model to be less conservative. Finally, we upweight the rule-based GYAFC data via duplication (Sennrich et al., 2016b). For our experiments, we use Moses (Koehn et al., 2007). We train a 5-gram language model using KenLM (Heafield et al., 2013), and use target style sentences from GYAFC and the sub-sampled target style sentences from out-ofdomain Yahoo Answers, as in Moore and Lewis (2010), to create a large language model. 4.3 NMT Combined: The size of our parallel data is smaller than the size typically used to train NMT models. Motivated by this fact, we propose several variants to the baseline models that we find helps minimize this issue. We augment the data used to train NMT Copy via two techniques: 1) we run the PBMT model on additional source data, and 2)"
N18-1012,W17-4912,0,0.132132,"ments were separately collected for adequacy, fluency and style (Xu et al., 2012; Niu et al., 2017). In our work, we conduct a more thorough evaluation where we evaluate model outputs on the three criteria of formality, fluency and meaning using both automatic metrics and human judgments. Style Transfer without Parallel Data: Another direction of research directly controls certain attributes of the generated text without using parallel data. Hu et al. (2017) control the sentiment and the tense of the generated text by learning a disentangled latent representation in a neural generative model. Ficler and Goldberg (2017) control several linguistic style aspects simultaneously by conditioning a recurrent neural network language model on specific style (professional, personal, length) and content (theme, sentiment) parameters. Under NMT models, Sennrich et al. (2016a) control the politeness of the translated text via side constraints, Niu et al. (2017) control the level of formality of MT output 3 https://github.com/cocoxu/Shakespeare 130 Domain All Yahoo Answers Entertainment & Music Family & Relationships Total 40M 3.8M 7.8M Informal 24M 2.7M 5.6M Formal 16M 700K 1.8M Informal to Formal Formal to Informal Tra"
N18-1012,D15-1181,0,0.0481085,"Missing"
N18-1012,P13-2121,0,0.0214318,"Missing"
N18-1012,P10-2041,0,0.0264454,"to the the source style and use the resultant output to retrain the PBMT model. Using subselection, we only select rewrites that have an Levenshtein edit distance of over 10 characters when compared to the source to encourage the model to be less conservative. Finally, we upweight the rule-based GYAFC data via duplication (Sennrich et al., 2016b). For our experiments, we use Moses (Koehn et al., 2007). We train a 5-gram language model using KenLM (Heafield et al., 2013), and use target style sentences from GYAFC and the sub-sampled target style sentences from out-ofdomain Yahoo Answers, as in Moore and Lewis (2010), to create a large language model. 4.3 NMT Combined: The size of our parallel data is smaller than the size typically used to train NMT models. Motivated by this fact, we propose several variants to the baseline models that we find helps minimize this issue. We augment the data used to train NMT Copy via two techniques: 1) we run the PBMT model on additional source data, and 2) we use back-translation (Sennrich et al., 2016c) of the PBMT model to translate the large number of in-domain target style sentences from GYAFC. To balance the over one million artificially generated pairs from the res"
N18-1012,D16-1228,1,0.830072,"Formality: We use the formality classifier described in PT16. We find that the classifier trained on the answers genre of PT16 dataset does not perform well when tested on our datasets. Hence, we collect formality judgments for an additional 5000 sentences and use the formality classifier re-trained on this in-domain data. Fluency: We use the reimplementation13 of Heilman et al. (2014) (H14 in Table 4) which is a statistical model for predicting the grammaticality of a sentence on a scale of 0 to 4 previously shown to be effective for other generation tasks like grammatical error correction (Napoles et al., 2016). Meaning Preservation: Following the annotation scheme developed for the Semantic Textual Similarity (STS) dataset (Agirre et al., 2016), given two sentences i.e. the source style sentence and the target style reference rewrite or the target style model output, workers rate the meaning similarity of the two sentences on a scale of 1 to 6 described as: 6: Completely equivalent, 5: Mostly equivalent, 4: Roughly equivalent, 3: Not equivalent but share some details, 2: Not equivalent but on same topic, 1: Completely dissimilar. Meaning Preservation: Modeling semantic similarity at a sentence leve"
N18-1012,W11-2826,0,0.21862,"-best list during decoding. In the field of text simplification, more recently, Xu et al. (2016) learn large-scale paraphrase rules using bilingual texts whereas Kajiwara and Komachi (2016) build a monolingual parallel corpus using sentence similarity based on alignment between word embeddings. Our work differs from these methods in that we mainly address the question of how much leverage we can derive by collecting a large amount of informal-formal sentence pairs and build models that learn to transfer style directly using this parallel corpus. Related Work Style Transfer with Parallel Data: Sheikha and Inkpen (2011) collect pairs of formal and informal words and phrases from different sources and use a natural language generation system to generate informal and formal texts by replacing lexical items based on user preferences. Xu et al. (2012) (henceforth X U 12) was one of the first works to treat style transfer as a sequence to sequence task. They generate a parallel corpus of 30K sentence pairs by scraping the modern translations of Shakespeare plays and train a PBMT system to translate from modern English to Shakespearean English.3 More recently, Jhamtani et al. (2017) show that a copy-mechanism enri"
N18-1012,D17-1299,0,0.217865,"sentence level (Pavlick and Tetreault, 2016) and at the document level (Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has re-purposed the MT metric BLEU (Papineni et al., 2002) and the paraphrase metric PINC (Chen and Dolan, 2011) for evaluation. Additionally, X U 12 introduce three new automatic style metrics based on cosine similarity, language model and logistic regression that measure the degree to which the output matches the target style. Under human based evaluation, on the other hand, there has been work on a more fine grained evaluation where human judgments were separately collected for adequacy, fluency and style (Xu et al., 2012; Niu et al., 2017). In our work, we conduct"
N18-1012,W09-0441,0,0.0242672,"all Ranking: In addition to the finegrained human judgments, we collect judgments to assess the overall ranking of the systems. Given the original source style sentence, the target style reference rewrite and the target style model outputs, we ask workers to rank the rewrites in the order of their overall formality, taking into account both fluency and meaning preservation. We then rank the model using the equation below: rank(model) = Overall Ranking: We experiment with BLEU (Papineni et al., 2002) and PINC (Chen and Dolan, 2011) as both were used in prior style evaluations, as well as TERp (Snover et al., 2009). 1 X 1 X rank(smodel , j) |S| |J| s∈S Automatic Metrics 6 j∈J Results In this section, we discuss how well the five models perform in the informal to formal style transfer task using human judgments (§6.1) and automatic metrics (§6.2), the correlation of the automatic metrics and human judgments to determine the ef(1) where, model is the one of our models, S is a subset of 500 test set sentences, J is the set of five judgments, smodel is the model rewrite for sentence s, and rank(smodel , j) is the rank of smodel in judgment j. The two authors of the paper reviewed these human judgments and f"
N18-1012,P02-1040,0,0.107205,"a and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has re-purposed the MT metric BLEU (Papineni et al., 2002) and the paraphrase metric PINC (Chen and Dolan, 2011) for evaluation. Additionally, X U 12 introduce three new automatic style metrics based on cosine similarity, language model and logistic regression that measure the degree to which the output matches the target style. Under human based evaluation, on the other hand, there has been work on a more fine grained evaluation where human judgments were separately collected for adequacy, fluency and style (Xu et al., 2012; Niu et al., 2017). In our work, we conduct a more thorough evaluation where we evaluate model outputs on the three criteria of"
N18-1012,N15-1023,0,0.0406022,"on the same set. In text simplification, the availability of parallel data extracted from English Wikipedia and Simple Wikipedia (Zhu et al., 2010) led to the application of PBMT (Wubben et al., 2012a) and more recently NMT (Wang et al., 2016) models. We take inspiration from both the PBMT and NMT models and apply several modifications to these approaches for our task of transforming the formality style of the text. Identifying Formality: There has been previous work on detecting formality of a given text at the lexical level (Brooke et al., 2010; Lahiri et al., 2011; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015), at the sentence level (Pavlick and Tetreault, 2016) and at the document level (Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer (Xu et al., 2012; Jhamtani et al"
N18-1012,Q16-1005,1,0.863835,"ability of parallel data extracted from English Wikipedia and Simple Wikipedia (Zhu et al., 2010) led to the application of PBMT (Wubben et al., 2012a) and more recently NMT (Wang et al., 2016) models. We take inspiration from both the PBMT and NMT models and apply several modifications to these approaches for our task of transforming the formality style of the text. Identifying Formality: There has been previous work on detecting formality of a given text at the lexical level (Brooke et al., 2010; Lahiri et al., 2011; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015), at the sentence level (Pavlick and Tetreault, 2016) and at the document level (Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) ha"
N18-1012,D14-1162,0,0.0846207,"odels have become quite successful for MT(Sutskever et al., 2014; Bahdanau et al., 2014; Cho et al., 2014), the field of style transfer, has not yet been able to fully take advantage of these advances owing to the lack of availability of large parallel data. With GYAFC we can now show how well NMT techniques fare for style transfer. We experiment with three NMT models: 5.1 NMT baseline: Our baseline model is a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) encoder-decoder model with attention (Bahdanau et al., 2014).11 We pretrain the input word embeddings on Yahoo Answers using GloVE (Pennington et al., 2014). As in our PBMT based approach, we train our NMT baseline model on the output of the rule-based approach when applied to GYAFC. 11 Evaluation Human-based Evaluation We perform human-based evaluation to assess model outputs on the four criteria: formality, fluency, meaning and overall. For a subset of 500 sentences from the test sets of both Entertainment & Music and Family & Relationship domains, we collect five human judgments per sentence per criteria using Amazon Mechanical Turk as follows: 12 Training data sizes for different methods are summarized in the supplementary material. Details a"
N18-1012,P12-1107,0,0.0870489,"Missing"
N18-1012,W11-0711,0,0.343423,"u et al., 2010) led to the application of PBMT (Wubben et al., 2012a) and more recently NMT (Wang et al., 2016) models. We take inspiration from both the PBMT and NMT models and apply several modifications to these approaches for our task of transforming the formality style of the text. Identifying Formality: There has been previous work on detecting formality of a given text at the lexical level (Brooke et al., 2010; Lahiri et al., 2011; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015), at the sentence level (Pavlick and Tetreault, 2016) and at the document level (Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has re-purposed the MT metric BLEU (Papineni et al., 2002) and the paraphrase"
N18-1012,N16-1005,0,0.471522,"vlick and Tetreault, 2016) and at the document level (Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has re-purposed the MT metric BLEU (Papineni et al., 2002) and the paraphrase metric PINC (Chen and Dolan, 2011) for evaluation. Additionally, X U 12 introduce three new automatic style metrics based on cosine similarity, language model and logistic regression that measure the degree to which the output matches the target style. Under human based evaluation, on the other hand, there has been work on a more fine grained evaluation where human judgments were separately collected for adequacy, fluency and style (Xu et al., 2012; Niu et al., 2017). In our work, we conduct a more thorough evalua"
N18-1012,W16-2323,0,0.165635,"vlick and Tetreault, 2016) and at the document level (Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has re-purposed the MT metric BLEU (Papineni et al., 2002) and the paraphrase metric PINC (Chen and Dolan, 2011) for evaluation. Additionally, X U 12 introduce three new automatic style metrics based on cosine similarity, language model and logistic regression that measure the degree to which the output matches the target style. Under human based evaluation, on the other hand, there has been work on a more fine grained evaluation where human judgments were separately collected for adequacy, fluency and style (Xu et al., 2012; Niu et al., 2017). In our work, we conduct a more thorough evalua"
N18-1012,Q16-1029,0,0.214336,"c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics In the following two sections we discuss related work and the GYAFC dataset. In §4, we detail our rule-based and MT-based approaches. In §5, we describe our human and automatic metric based evaluation. In §6, we describe the results of our models using both human and automatic evaluation and discuss how well the automatic metrics correlate with human judgments. 2 by selecting phrases of a requisite formality level from the k-best list during decoding. In the field of text simplification, more recently, Xu et al. (2016) learn large-scale paraphrase rules using bilingual texts whereas Kajiwara and Komachi (2016) build a monolingual parallel corpus using sentence similarity based on alignment between word embeddings. Our work differs from these methods in that we mainly address the question of how much leverage we can derive by collecting a large amount of informal-formal sentence pairs and build models that learn to transfer style directly using this parallel corpus. Related Work Style Transfer with Parallel Data: Sheikha and Inkpen (2011) collect pairs of formal and informal words and phrases from different"
N18-1012,C12-1177,0,0.537933,"may put you in a good light. Hovy (1987) argues that by varying the style of a text, people convey more information than is present in the literal meaning of the words. One particularly important dimension of style is formality (Heylighen and Dewaele, 1999). Automatically changing the style of a given content to make it more formal can be a useful addition to any writing assistance tool. In the field of style transfer, to date, the only available dataset has been for the transformation of modern English to Shakespeare, and it led to the application of phrase-based machine translation (PBMT) (Xu et al., 2012) and neural machine translation (NMT) (Jhamtani et al., 2017) models to the task. The lack of an equivalent or larger dataset for any other form of style transfer has blocked progress in this field. Moreover, prior Informal: Formal: Informal: Formal: I’d say it is punk though. However, I do believe it to be punk. Gotta see both sides of the story. You have to consider both sides of the story. Table 1: Informal sentences with formal rewrites. In this paper, we primarily focus on the informal to formal direction since we collect our dataset for this direction. However, we evaluate our models on"
N18-1012,P16-1009,0,0.48298,"vlick and Tetreault, 2016) and at the document level (Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012). In our work, we reproduce the sentence-level formality classifier introduced in Pavlick and Tetreault (2016) (PT16) to extract informal sentences for GYAFC creation and to automatically evaluate system outputs. Evaluating Style Transfer: The problem of style transfer falls under the category of natural language generation tasks such as machine translation, paraphrasing, etc. Previous work on style transfer (Xu et al., 2012; Jhamtani et al., 2017; Niu et al., 2017; Sennrich et al., 2016a) has re-purposed the MT metric BLEU (Papineni et al., 2002) and the paraphrase metric PINC (Chen and Dolan, 2011) for evaluation. Additionally, X U 12 introduce three new automatic style metrics based on cosine similarity, language model and logistic regression that measure the degree to which the output matches the target style. Under human based evaluation, on the other hand, there has been work on a more fine grained evaluation where human judgments were separately collected for adequacy, fluency and style (Xu et al., 2012; Niu et al., 2017). In our work, we conduct a more thorough evalua"
N18-1012,P17-4012,0,\N,Missing
N18-1012,W17-4902,0,\N,Missing
N19-1373,Q16-1026,0,0.0358867,"nce representations; and (3) a conversation-level RNN that operates on the utterance encoding output of the attention mechanism, followed by a CRF layer to predict utterance labels. We describe them in detail below. 3.1 Utterance-level RNN For each word in an utterance, we combine two different word embeddings: GloVe (Pennington et al., 2014) and pre-trained ELMo representations (Peters et al., 2018) with fine-tuned task-specific parameters, which have shown superior performance in a wide range of tasks. The word embedding is then concatenated with its CNN-based 50D character-level embedding (Chiu and Nichols, 2016; Ma and Hovy, 2016) to get the complete word-level representations. The motivation behind incorporating subword-level information is to infer the lexical features of utterances and named entities better. 3728 The word representation layer is followed by a bidirectional GRU (Bi-GRU) layer. Concatenating the forward and backward outputs of the Bi-GRU generates the utterance embedding that serves as input to the utterance-level context-aware selfattention mechanism which learns the final utterance representation. 3.2 Context-aware Self-attention Self-attentive representations encode a variablele"
N19-1373,N16-1037,0,0.0903089,"we focus on two main classes of approaches which have dominated recent research: those that treat DA classification as a text classification problem, where each utterance is classified in isolation, and those that treat it as a sequence labeling problem. Text Classification: Lee and Dernoncourt (2016) build a vector representation for each utterance, using either a CNN or RNN, and use the preceding utterance(s) as context to classify it. Their model was extended by Khanpour et al. (2016) and Ortega and Vu (2017). Shen and Lee (2016) used a variant of the attention-based encoder for the task. Ji et al. (2016) use a hybrid architecture, combining an RNN language model with a latent variable model. Sequence Labeling: Kalchbrenner and Blunsom (2013) used a mixture of sentence-level CNNs and discourse-level RNNS to achieve state-of-the-art results on the task. Recent works (Li and Wu, 2016; Liu et al., 2017) have increasingly employed hierarchical architectures to learn and model multiple levels of utterance and DA dependencies. Kumar et al. (2018), Chen et al. (2018) and Tran et al. (2017) used RNN-based hierarchical neural networks, using different combinations of techniques like last-pooling or att"
N19-1373,N16-1062,0,0.314733,"answering, conversational agents, speech recognition, etc. Examples of DAs can be found in Table 1. Here we have a conversation of 7 utterances between two speakers. Each utterance has a corresponding label such as Question or Backchannel. Early work in this field made use of statistical machine learning methods and approached the task as either a structured prediction or text classification problem (Stolcke et al., 2000; Ang et al., 2005; Zimmermann, 2009; Surendran and Levow, 2006). Many recent studies have proposed deep learning models for the DA classification task with promising results (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Ortega and Vu, 2017). However, most of these approaches treat the task as a text classification problem, treating each utterance in isolation, rendering them unable to leverage the conversation-level contextual dependence among utterances. Knowing the text and/or the DA labels of the previous utterances can assist in predicting the current DA state. For instance, in Table 1, the Answer or Statement dialog acts often follow Question type utterances. This work draws from recent advances in NLP such as self-attention, hierarchical deep learning models, and contextual depe"
N19-1373,C16-1185,0,0.0180897,"d Dernoncourt (2016) build a vector representation for each utterance, using either a CNN or RNN, and use the preceding utterance(s) as context to classify it. Their model was extended by Khanpour et al. (2016) and Ortega and Vu (2017). Shen and Lee (2016) used a variant of the attention-based encoder for the task. Ji et al. (2016) use a hybrid architecture, combining an RNN language model with a latent variable model. Sequence Labeling: Kalchbrenner and Blunsom (2013) used a mixture of sentence-level CNNs and discourse-level RNNS to achieve state-of-the-art results on the task. Recent works (Li and Wu, 2016; Liu et al., 2017) have increasingly employed hierarchical architectures to learn and model multiple levels of utterance and DA dependencies. Kumar et al. (2018), Chen et al. (2018) and Tran et al. (2017) used RNN-based hierarchical neural networks, using different combinations of techniques like last-pooling or attention mechanism to encode sentences, coupled with CRF decoders. Chen et al. (2018) achieved the highest performance to date on the two datasets for this task. Our work extends these hierarchical models and leverages a combination of techniques proposed across these prior works (CR"
N19-1373,D17-1231,0,0.0565717,"16) build a vector representation for each utterance, using either a CNN or RNN, and use the preceding utterance(s) as context to classify it. Their model was extended by Khanpour et al. (2016) and Ortega and Vu (2017). Shen and Lee (2016) used a variant of the attention-based encoder for the task. Ji et al. (2016) use a hybrid architecture, combining an RNN language model with a latent variable model. Sequence Labeling: Kalchbrenner and Blunsom (2013) used a mixture of sentence-level CNNs and discourse-level RNNS to achieve state-of-the-art results on the task. Recent works (Li and Wu, 2016; Liu et al., 2017) have increasingly employed hierarchical architectures to learn and model multiple levels of utterance and DA dependencies. Kumar et al. (2018), Chen et al. (2018) and Tran et al. (2017) used RNN-based hierarchical neural networks, using different combinations of techniques like last-pooling or attention mechanism to encode sentences, coupled with CRF decoders. Chen et al. (2018) achieved the highest performance to date on the two datasets for this task. Our work extends these hierarchical models and leverages a combination of techniques proposed across these prior works (CRF decoding, context"
N19-1373,W13-3214,0,0.731959,"t classification problem, where each utterance is classified in isolation, and those that treat it as a sequence labeling problem. Text Classification: Lee and Dernoncourt (2016) build a vector representation for each utterance, using either a CNN or RNN, and use the preceding utterance(s) as context to classify it. Their model was extended by Khanpour et al. (2016) and Ortega and Vu (2017). Shen and Lee (2016) used a variant of the attention-based encoder for the task. Ji et al. (2016) use a hybrid architecture, combining an RNN language model with a latent variable model. Sequence Labeling: Kalchbrenner and Blunsom (2013) used a mixture of sentence-level CNNs and discourse-level RNNS to achieve state-of-the-art results on the task. Recent works (Li and Wu, 2016; Liu et al., 2017) have increasingly employed hierarchical architectures to learn and model multiple levels of utterance and DA dependencies. Kumar et al. (2018), Chen et al. (2018) and Tran et al. (2017) used RNN-based hierarchical neural networks, using different combinations of techniques like last-pooling or attention mechanism to encode sentences, coupled with CRF decoders. Chen et al. (2018) achieved the highest performance to date on the two data"
N19-1373,P16-1101,0,0.0468806,"(3) a conversation-level RNN that operates on the utterance encoding output of the attention mechanism, followed by a CRF layer to predict utterance labels. We describe them in detail below. 3.1 Utterance-level RNN For each word in an utterance, we combine two different word embeddings: GloVe (Pennington et al., 2014) and pre-trained ELMo representations (Peters et al., 2018) with fine-tuned task-specific parameters, which have shown superior performance in a wide range of tasks. The word embedding is then concatenated with its CNN-based 50D character-level embedding (Chiu and Nichols, 2016; Ma and Hovy, 2016) to get the complete word-level representations. The motivation behind incorporating subword-level information is to infer the lexical features of utterances and named entities better. 3728 The word representation layer is followed by a bidirectional GRU (Bi-GRU) layer. Concatenating the forward and backward outputs of the Bi-GRU generates the utterance embedding that serves as input to the utterance-level context-aware selfattention mechanism which learns the final utterance representation. 3.2 Context-aware Self-attention Self-attentive representations encode a variablelength sequence into a"
N19-1373,C16-1189,0,0.353118,"gents, speech recognition, etc. Examples of DAs can be found in Table 1. Here we have a conversation of 7 utterances between two speakers. Each utterance has a corresponding label such as Question or Backchannel. Early work in this field made use of statistical machine learning methods and approached the task as either a structured prediction or text classification problem (Stolcke et al., 2000; Ang et al., 2005; Zimmermann, 2009; Surendran and Levow, 2006). Many recent studies have proposed deep learning models for the DA classification task with promising results (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Ortega and Vu, 2017). However, most of these approaches treat the task as a text classification problem, treating each utterance in isolation, rendering them unable to leverage the conversation-level contextual dependence among utterances. Knowing the text and/or the DA labels of the previous utterances can assist in predicting the current DA state. For instance, in Table 1, the Answer or Statement dialog acts often follow Question type utterances. This work draws from recent advances in NLP such as self-attention, hierarchical deep learning models, and contextual dependencies to produce a d"
N19-1373,W17-5530,0,0.159251,"on, etc. Examples of DAs can be found in Table 1. Here we have a conversation of 7 utterances between two speakers. Each utterance has a corresponding label such as Question or Backchannel. Early work in this field made use of statistical machine learning methods and approached the task as either a structured prediction or text classification problem (Stolcke et al., 2000; Ang et al., 2005; Zimmermann, 2009; Surendran and Levow, 2006). Many recent studies have proposed deep learning models for the DA classification task with promising results (Lee and Dernoncourt, 2016; Khanpour et al., 2016; Ortega and Vu, 2017). However, most of these approaches treat the task as a text classification problem, treating each utterance in isolation, rendering them unable to leverage the conversation-level contextual dependence among utterances. Knowing the text and/or the DA labels of the previous utterances can assist in predicting the current DA state. For instance, in Table 1, the Answer or Statement dialog acts often follow Question type utterances. This work draws from recent advances in NLP such as self-attention, hierarchical deep learning models, and contextual dependencies to produce a dialogue act classifica"
N19-1373,D14-1162,0,0.0930772,"the overall architecture of our model, which involves three main components: (1) an utterance-level RNN that encodes the information within the utterances at the word and character-level; (2) a context-aware selfattention mechanism that aggregates word representations into utterance representations; and (3) a conversation-level RNN that operates on the utterance encoding output of the attention mechanism, followed by a CRF layer to predict utterance labels. We describe them in detail below. 3.1 Utterance-level RNN For each word in an utterance, we combine two different word embeddings: GloVe (Pennington et al., 2014) and pre-trained ELMo representations (Peters et al., 2018) with fine-tuned task-specific parameters, which have shown superior performance in a wide range of tasks. The word embedding is then concatenated with its CNN-based 50D character-level embedding (Chiu and Nichols, 2016; Ma and Hovy, 2016) to get the complete word-level representations. The motivation behind incorporating subword-level information is to infer the lexical features of utterances and named entities better. 3728 The word representation layer is followed by a bidirectional GRU (Bi-GRU) layer. Concatenating the forward and b"
N19-1373,N18-1202,0,0.0469146,"n components: (1) an utterance-level RNN that encodes the information within the utterances at the word and character-level; (2) a context-aware selfattention mechanism that aggregates word representations into utterance representations; and (3) a conversation-level RNN that operates on the utterance encoding output of the attention mechanism, followed by a CRF layer to predict utterance labels. We describe them in detail below. 3.1 Utterance-level RNN For each word in an utterance, we combine two different word embeddings: GloVe (Pennington et al., 2014) and pre-trained ELMo representations (Peters et al., 2018) with fine-tuned task-specific parameters, which have shown superior performance in a wide range of tasks. The word embedding is then concatenated with its CNN-based 50D character-level embedding (Chiu and Nichols, 2016; Ma and Hovy, 2016) to get the complete word-level representations. The motivation behind incorporating subword-level information is to infer the lexical features of utterances and named entities better. 3728 The word representation layer is followed by a bidirectional GRU (Bi-GRU) layer. Concatenating the forward and backward outputs of the Bi-GRU generates the utterance embed"
N19-1373,D18-1092,0,0.0641522,"Missing"
N19-1373,N16-1174,0,0.422824,"tropy vs. Accuracy on the SwDA Dataset Table 4: Performance of utterance representation methods when integrated with the hierarchical model pus, where we first learn utterance representations on the corpus using Skip-Thought Vectors (Kiros et al., 2015) and Paragraph Vectors (Le and Mikolov, 2014), and then use them with the rest of the model; (iii) jointly learned with the DA classification task. Table 4 describes the performance of different utterance representation learning methods when combined with the overall architecture on both datasets. Introducing the word-level attention mechanism (Yang et al., 2016) enables the model to learn better representations by attending to more informative words in an utterance, resulting in better performance (Bi-RNN + Attention). The self-attention mechanism (Bi-RNN + Selfattention) leads to even greater overall improvements. Adding context information (previous recurrent state of the conversation) boosts performance significantly. A notable aspect of our model is how contextual information is leveraged at different levels of the sequence modeling task. The combination of conversation-level contextual states for utterancerepresentation learning (+ Context) and"
N19-1373,J00-3003,0,0.932472,"Missing"
N19-1373,E17-1041,0,0.140798,". (2016) and Ortega and Vu (2017). Shen and Lee (2016) used a variant of the attention-based encoder for the task. Ji et al. (2016) use a hybrid architecture, combining an RNN language model with a latent variable model. Sequence Labeling: Kalchbrenner and Blunsom (2013) used a mixture of sentence-level CNNs and discourse-level RNNS to achieve state-of-the-art results on the task. Recent works (Li and Wu, 2016; Liu et al., 2017) have increasingly employed hierarchical architectures to learn and model multiple levels of utterance and DA dependencies. Kumar et al. (2018), Chen et al. (2018) and Tran et al. (2017) used RNN-based hierarchical neural networks, using different combinations of techniques like last-pooling or attention mechanism to encode sentences, coupled with CRF decoders. Chen et al. (2018) achieved the highest performance to date on the two datasets for this task. Our work extends these hierarchical models and leverages a combination of techniques proposed across these prior works (CRF decoding, contextual attention, and character-level word embeddings) with self-attentive representation learning, and is able to achieve state-of-the-art performance. 3 1 Model The task of DA classificat"
N19-1373,K19-1036,0,\N,Missing
P10-2065,W07-1604,1,0.645202,"nd Knutsson (2008) show that knowledge of the PP attachment site helps in the task of preposition selection by comparing a classifier trained on lexical features (the verb before the preposition, the noun between the verb and the preposition, if any, and the noun after the preposition) to a classifier trained on attachment features which explicitly state whether the preposition is attached to the preceding noun or verb. They also argue that a parser which is capable of distinguishing between arguments and adjuncts is useful for generating the correct preposition. 3 Baseline System The work of Chodorow et al. (2007) and T&C08 treat the tasks of preposition selection and error detection as a classification problem. That is, given the context around a preposition and a model of correct usage, a classifier determines which of the 34 prepositions covered by the model is most appropriate for the context. A model of correct preposition usage is constructed by training a Maximum Entropy classifier (Ratnaparkhi, 1998) on millions of preposition contexts from well-formed text. A context is represented by 25 lexical features and 4 combination features: Lexical Token and POS n-grams in a 2 word window around the pr"
P10-2065,C08-1022,0,0.602123,"Missing"
P10-2065,W08-1301,0,0.00717771,"Missing"
P10-2065,de-marneffe-etal-2006-generating,0,0.028425,"Missing"
P10-2065,I08-1059,0,0.0586698,"modeling preposition usage in well-formed text and learner text? • We demonstrate that parse features have a significant impact on preposition selection in well-formed text. We also show which features have the greatest effect on performance. • We show that, despite the noisiness of learner text, parse features can actually make small, albeit non-significant, improvements to the performance of a state-of-the-art preposition error detection system. • We evaluate the accuracy of parsing and especially preposition attachment in learner texts. 2 Related Work T&C08, De Felice and Pulman (2008) and Gamon et al. (2008) describe very similar preposition error detection systems in which a model of correct prepositional usage is trained from wellformed text and a writer’s preposition is compared with the predictions of this model. It is difficult to directly compare these systems since they are trained and tested on different data sets 353 Proceedings of the ACL 2010 Conference Short Papers, pages 353–358, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics 3.1 but they achieve accuracy in a similar range. Of these systems, only the DAPPER system (De Felice and Pulman, 2008; De F"
P10-2065,hermet-etal-2008-using,0,0.0431457,"Missing"
P10-2065,P09-2079,0,0.0107573,"Missing"
P10-2065,P03-1054,0,0.00558171,"Detection Joel Tetreault Educational Testing Service Princeton NJ, USA Jennifer Foster NCLT Dublin City University Ireland Martin Chodorow Hunter College of CUNY New York, NY, USA JTetreault@ets.org jfoster@computing.dcu.ie @hunter.cuny.edu Abstract We recreate a state-of-the-art preposition usage system (Tetreault and Chodorow (2008), henceforth T&C08) originally trained with lexical features and augment it with parser output features. We employ the Stanford parser in our experiments because it consists of a competitive phrase structure parser and a constituent-to-dependency conversion tool (Klein and Manning, 2003a; Klein and Manning, 2003b; de Marneffe et al., 2006; de Marneffe and Manning, 2008). We compare the original model with the parser-augmented model on the tasks of preposition selection in wellformed text (fluent writers) and preposition error detection in learner texts (ESL writers). This paper makes the following contributions: We evaluate the effect of adding parse features to a leading model of preposition usage. Results show a significant improvement in the preposition selection task on native speaker text and a modest increment in precision and recall in an ESL error detection task. Ana"
P10-2065,P08-1006,0,0.014116,"Missing"
P10-2065,C08-1109,1,0.709664,"t test sets and also training sizes. Given the time required to train large models, we report here experiments with a relatively small model. 355 Model T&C08 +Phrase Structure Only +Dependency Only +Parse +head-tag+comp-tag +left +grandparent +head-token+comp-tag +head-tag +head-token +head-tag+comp-token Accuracy 65.2 67.1 68.2 68.5 66.9 66.8 66.6 66.6 66.5 66.4 66.1 Method T&C08 +Parse native speakers for the Test of English as a Foreign R Language (TOEFL ). The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). 4,881 of the prepositions were judged to be correct and the remaining 302 were judged to be incorrect. The writer’s preposition is flagged as an error by the system if its likelihood according to the model satisfied a set of criteria (e.g., the difference between the probability of the system’s choice and the writer’s preposition is 0.8 or higher). Unlike the selection task where we use accuracy as the metric, we use precision and recall with respect to error detection. To date, performance figures that have been reported in the literature have been quite low, reflecting the difficulty of"
P10-2065,W08-1205,1,0.791661,"t test sets and also training sizes. Given the time required to train large models, we report here experiments with a relatively small model. 355 Model T&C08 +Phrase Structure Only +Dependency Only +Parse +head-tag+comp-tag +left +grandparent +head-token+comp-tag +head-tag +head-token +head-tag+comp-token Accuracy 65.2 67.1 68.2 68.5 66.9 66.8 66.6 66.6 66.5 66.4 66.1 Method T&C08 +Parse native speakers for the Test of English as a Foreign R Language (TOEFL ). The prepositions were judged by two trained annotators and checked by the authors using the preposition annotation scheme described in Tetreault and Chodorow (2008b). 4,881 of the prepositions were judged to be correct and the remaining 302 were judged to be incorrect. The writer’s preposition is flagged as an error by the system if its likelihood according to the model satisfied a set of criteria (e.g., the difference between the probability of the system’s choice and the writer’s preposition is 0.8 or higher). Unlike the selection task where we use accuracy as the metric, we use precision and recall with respect to error detection. To date, performance figures that have been reported in the literature have been quite low, reflecting the difficulty of"
P10-2065,J07-4004,0,\N,Missing
P10-2065,J03-4003,0,\N,Missing
P11-2089,W10-0731,0,0.0164388,"lice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in"
P11-2089,D09-1030,0,0.0334174,"ferent groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in ne"
P11-2089,W10-0708,0,0.00908944,"e there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of"
P11-2089,C08-1022,0,0.0718184,"Missing"
P11-2089,W10-0713,0,0.00614558,"s the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s A"
P11-2089,I08-1059,0,0.0380613,"of crowdsourcing. 1 Motivation and Contributions to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambigu"
P11-2089,N10-1019,0,0.0860787,"Missing"
P11-2089,W10-0717,0,0.0141234,"ated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) a"
P11-2089,N10-1024,0,0.0278613,"orpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL)."
P11-2089,W10-1004,1,0.378088,"Missing"
P11-2089,D10-1094,1,0.419692,"Missing"
P11-2089,C08-1109,1,0.664593,"ies, both based on a novel use of crowdsourcing. 1 Motivation and Contributions to have different judgments of usage. Therefore, an appropriate evaluation should take this into account by not only enlisting multiple human judges but also aggregating these judgments in a graded manner. Second, systems are hardly ever compared to each other. In fact, to our knowledge, no two systems developed by different groups have been compared directly within the field primarily because there is no common corpus or shared task—both commonly found in other NLP areas such as machine translation.1 For example, Tetreault and Chodorow (2008), Gamon et al. (2008) and Felice and Pulman (2008) developed preposition error detection systems, but evaluated on three different corpora using different evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009),"
P11-2089,W10-1006,1,0.836109,"allison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 2010a), all in the last five years. In this year’s ACL conference, there are four long papers devoted to this topic. Despite the growing interest, two major factors encumber the growth of this subfield. First, the lack of consistent and appropriate score reporting is an issue. Most work reports results in the form of precision and recall as measured against the judgment of a single human rater. This is problematic because most usage errors (such as those in article and preposition usage) are a matter of degree rather than sim1 ple rule violations such as number agreement. As a There has been a rec"
P11-2089,W10-0725,0,0.0169367,"rent evaluation measures. The goal of this paper is to address the above issues by using crowdsourcing, which has been proven effective for collecting multiple, reliable judgments in other NLP tasks: machine translation (Callison-Burch, 2009; Zaidan and CallisonBurch, 2010), speech recognition (Evanini et al., 2010; Novotney and Callison-Burch, 2010), automated paraphrase generation (Madnani, 2010), anaphora resolution (Chamberlain et al., 2009), word sense disambiguation (Akkaya et al., 2010), lexicon construction for less commonly taught languages (Irvine and Klementiev, 2010), fact mining (Wang and Callison-Burch, 2010) and named entity recognition (Finin et al., 2010) among several others. In particular, we make a significant contribution to the field by showing how to leverage crowdsourcOne of the fastest growing areas in need of NLP tools is the field of grammatical error detection for learners of English as a Second Language (ESL). According to Guo and Beckett (2007), “over a billion people speak English as their second or foreign language.” This high demand has resulted in many NLP research papers on the topic, a Synthesis Series book (Leacock et al., 2010) and a recurring workshop (Tetreault et al., 20"
P11-2089,N10-1057,0,0.046017,"Missing"
P11-2089,W10-4236,0,\N,Missing
P14-2029,P03-1054,0,0.0255583,"tem for the binary task by binarizing its predictions.12 We compare our work to a modified version of the publicly available13 system from Post (2011), which performed very well on an artificial dataset. To our knowledge, it is the only publicly available system for grammaticality prediction. It is very Table 1: Pearson’s r on the development set, for our full system and variations excluding each feature type. “− X” indicates the full model without the “X” features. 3.2.4 PCFG Parsing Features We find phrase structure trees and basic dependencies with the Stanford Parser’s English PCFG model (Klein and Manning, 2003; de Marneffe et al., 2006).11 We then compute the following: • the parse score as provided by the Stanford PCFG Parser, normalized for sentence length, later referred to as parse prob • a binary feature that captures whether the top node of the tree is sentential or not (i.e. the assumption is that if the top node is nonsentential, then the sentence is a fragment) • features binning the number of dep relations returned by the dependency conversion. These dep relations are underspecified for function and indicate that the parser was unable to find a standard relation such as subj, possibly ind"
P14-2029,N07-2024,0,0.0193024,"eilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our syste"
P14-2029,W11-2111,1,0.923297,"Missing"
P14-2029,copestake-flickinger-2000-open,0,0.0167849,"iss + 1) as features. To identify misspellings, we use a freely available spelling dictionary for U.S. English.5 3.2.3 Precision Grammar Features Following Wagner et al. (2007) and Wagner et al. (2009), we use features extracted from precision grammar parsers. These grammars have been hand-crafted and designed to only provide complete syntactic analyses for grammatically correct sentences. This is in contrast to treebanktrained grammars, which will generally provide some analysis regardless of grammaticality. Here, we use (1) the Link Grammar Parser8 and (2) the HPSG English Resource Grammar (Copestake and Flickinger, 2000) and PET parser.9 We use a binary feature, complete link, from the Link grammar that indicates whether at least one complete linkage can be found for a sentence. We also extract several features from the HPSG analyses.10 They mostly reflect information about unification success or failure and the associated costs. In each instance, we use the logarithm of one plus the frequency. 3.2.2 n-gram Count and Language Model Features Given each sentence, the model obtains the counts of n-grams (n = 1 . . . 3) from English Gigaword and computes the following features:6 • of to X log(count(s) + 1) kSn k"
P14-2029,2003.mtsummit-papers.9,0,0.0574979,"Missing"
P14-2029,P11-2038,0,0.180454,"sa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our system to the one from Post (2011) and"
P14-2029,D11-1010,0,0.0137361,"resent a dataset of learner sentences rated for grammaticality. Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation. It could also be used in educational applications such as essay scoring. Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammatical2 Dataset Description We created a dataset consisting of 3,129 sentences randomly selected from essays written by nonnative speakers of English as part of a test of English language proficiency. We oversampled lower-scoring essays to increase the chances of finding ungrammatical sentences. Two of the a"
P14-2029,P07-1011,0,0.0284516,"al Scale Michael Heilman Aoife Cahill Nitin Madnani Melissa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we"
P14-2029,de-marneffe-etal-2006-generating,0,0.0433313,"Missing"
P14-2029,1993.eamt-1.1,0,0.520675,"Missing"
P14-2029,C08-1109,1,0.271581,"lop a system for the task of predicting the grammaticality of sentences, and present a dataset of learner sentences rated for grammaticality. Such a system could be used, for example, to check or to rank outputs from systems for text summarization, natural language generation, or machine translation. It could also be used in educational applications such as essay scoring. Much of the previous research on predicting grammaticality has focused on identifying (and possibly correcting) specific types of grammatical errors that are typically made by English language learners, such as prepositions (Tetreault and Chodorow, 2008), articles (Han et al., 2006), and collocations (Dahlmeier and Ng, 2011). While some applications (e.g., grammar checking) rely on such fine-grained predictions, others might be better addressed by sentence-level grammaticality judgments (e.g., machine translation evaluation). Regarding sentence-level grammaticality, there has been much work on rating the grammatical2 Dataset Description We created a dataset consisting of 3,129 sentences randomly selected from essays written by nonnative speakers of English as part of a test of English language proficiency. We oversampled lower-scoring essays"
P14-2029,2005.eamt-1.15,0,0.0187352,"Missing"
P14-2029,D07-1012,0,0.257114,"Missing"
P14-2029,U10-1011,0,0.017105,"l Nitin Madnani Melissa Lopez Matthew Mulholland Educational Testing Service Princeton, NJ, USA {mheilman,acahill,nmadnani,mlopez002,mmulholland}@ets.org Joel Tetreault Yahoo! Research New York, NY, USA tetreaul@yahoo-inc.com Abstract ity of machine translation outputs (Gamon et al., 2005; Parton et al., 2011), such as the MT Quality Estimation Shared Tasks (Bojar et al., 2013, §6), but relatively little on evaluating the grammaticality of naturally occurring text. Also, most other research on evaluating grammaticality involves artificial tasks or datasets (Sun et al., 2007; Lee et al., 2007; Wong and Dras, 2010; Post, 2011). Here, we make the following contributions. Automated methods for identifying whether sentences are grammatical have various potential applications (e.g., machine translation, automated essay scoring, computer-assisted language learning). In this work, we construct a statistical model of grammaticality using various linguistic features (e.g., misspelling counts, parser outputs, n-gram language model scores). We also present a new publicly available dataset of learner sentences judged for grammaticality on an ordinal scale. In evaluations, we compare our system to the one from Pos"
P14-2029,W13-2201,0,\N,Missing
P15-1038,D14-1082,0,0.0182931,"http://www.clearnlp.com 5 388 Parser ClearNLP v2,37 GN138 LTDP v2.0.39 Mate v3.6.110 RBG11 Redshift12 spaCy13 SNN14 Turbo v2.215 Yara16 Approach Transition-based, selectional branching (Choi and McCallum, 2013) Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Maximum spanning tree, 3rd-order features (Bohnet, 2010) Tensor decomposition, randomized hill-climb (Lei et al., 2014) Transition-based, non-monotonic (Honnibal et al., 2013) Transition-based, greedy, dynamic oracle, Brown clusters Transition-based, word embeddings (Chen and Manning, 2014) Dual decomposition, 3rd-order features (Martins et al., 2013) Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Language Java Python Python Java Java Cython Cython Java C++ Java License Apache GPL v2 n/a GPL v2 MIT FOSS Dual GPL v2 GPL v2 Apache Table 3: Dependency parsers used in our experiments. 4 Parsers 64 produced the best accuracy, while a beam size of 1 for LTDT, ClearNLP, and Yara produced the best speed performance. Given this trend, we also include how those three parsers perform at beam 1 in our analyses. We compared ten state of the art parsers representi"
P15-1038,P13-1104,1,0.926373,"Missing"
P15-1038,P12-2071,1,0.342324,"Missing"
P15-1038,W08-1301,0,0.108481,"Missing"
P15-1038,Q13-1033,0,0.0475998,"Missing"
P15-1038,P11-2125,0,0.09249,"Missing"
P15-1038,W09-1201,0,0.0163968,"Missing"
P15-1038,D11-1037,0,0.019356,": newswire, PT : pivot text, TC : telephone conversation, WB : web text, ALL : all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 3.1 ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only. Several programs are available for converting constituency trees into dependency trees. Table 2 shows a comparison between three of the most widely used: the LTH (Johansson and Nugues, 2007),4 , Stanford (de Marneffe and Manning, 2008),5 and ClearNLP (Choi and Palmer, 2012b)6 dependency converters. Compared to the Stanford converter,"
P15-1038,W13-3518,0,0.028072,"2012). 3 conll.cemantix.org/2012/download/ids/ 4 http://nlp.cs.lth.se/software http://nlp.stanford.edu/software 6 http://www.clearnlp.com 5 388 Parser ClearNLP v2,37 GN138 LTDP v2.0.39 Mate v3.6.110 RBG11 Redshift12 spaCy13 SNN14 Turbo v2.215 Yara16 Approach Transition-based, selectional branching (Choi and McCallum, 2013) Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Maximum spanning tree, 3rd-order features (Bohnet, 2010) Tensor decomposition, randomized hill-climb (Lei et al., 2014) Transition-based, non-monotonic (Honnibal et al., 2013) Transition-based, greedy, dynamic oracle, Brown clusters Transition-based, word embeddings (Chen and Manning, 2014) Dual decomposition, 3rd-order features (Martins et al., 2013) Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Language Java Python Python Java Java Cython Cython Java C++ Java License Apache GPL v2 n/a GPL v2 MIT FOSS Dual GPL v2 GPL v2 Apache Table 3: Dependency parsers used in our experiments. 4 Parsers 64 produced the best accuracy, while a beam size of 1 for LTDT, ClearNLP, and Yara produced the best speed performance. Given this trend, we also in"
P15-1038,W14-6110,0,0.0237582,"Missing"
P15-1038,N12-1015,0,0.0184437,"Missing"
P15-1038,W07-2416,0,0.0132148,"Missing"
P15-1038,D12-1096,0,0.0247064,"Missing"
P15-1038,D10-1069,0,0.0128203,"L : all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 3.1 ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only. Several programs are available for converting constituency trees into dependency trees. Table 2 shows a comparison between three of the most widely used: the LTH (Johansson and Nugues, 2007),4 , Stanford (de Marneffe and Manning, 2008),5 and ClearNLP (Choi and Palmer, 2012b)6 dependency converters. Compared to the Stanford converter, the ClearNLP converter produces a similar set of dependency labels but ge"
P15-1038,D13-1116,0,0.029659,"Missing"
P15-1038,W13-3516,0,0.0362406,"Missing"
P15-1038,P14-1130,0,0.0156994,"cy parsing was performed by (Kummerfeld and others, 2012). 3 conll.cemantix.org/2012/download/ids/ 4 http://nlp.cs.lth.se/software http://nlp.stanford.edu/software 6 http://www.clearnlp.com 5 388 Parser ClearNLP v2,37 GN138 LTDP v2.0.39 Mate v3.6.110 RBG11 Redshift12 spaCy13 SNN14 Turbo v2.215 Yara16 Approach Transition-based, selectional branching (Choi and McCallum, 2013) Easy-first, dynamic oracle (Goldberg and Nivre, 2013) Transition-based, beam-search + dynamic prog. (Huang et al., 2012) Maximum spanning tree, 3rd-order features (Bohnet, 2010) Tensor decomposition, randomized hill-climb (Lei et al., 2014) Transition-based, non-monotonic (Honnibal et al., 2013) Transition-based, greedy, dynamic oracle, Brown clusters Transition-based, word embeddings (Chen and Manning, 2014) Dual decomposition, 3rd-order features (Martins et al., 2013) Transition-based, beam-search, dynamic oracle (Rasooli and Tetreault, 2015) Language Java Python Python Java Java Cython Cython Java C++ Java License Apache GPL v2 n/a GPL v2 MIT FOSS Dual GPL v2 GPL v2 Apache Table 3: Dependency parsers used in our experiments. 4 Parsers 64 produced the best accuracy, while a beam size of 1 for LTDT, ClearNLP, and Yara produced"
P15-1038,N06-2033,0,0.0191273,"Missing"
P15-1038,J93-2004,0,0.0541048,"rted in these shared tasks are: labeled attachment score (LAS) – the percentage of predicted dependencies where the arc and the label are assigned correctly; unlabeled attachment score (UAS) – where the arc is assigned correctly; label accuracy score (LS) – where the label is assigned correctly; and exact match (EM) – the percentage of sentences whose predicted trees are entirely correct. Although shared tasks have been tremendously useful for advancing the state of the art in dependency parsing, most English evaluation has employed a single-genre corpus, the WSJ portion of the Penn Treebank (Marcus et al., 1993), so it is not immediately clear how these results genIntroduction Dependency parsing is a valuable form of syntactic processing for NLP applications due to its transparent lexicalized representation and robustness with respect to flexible word order languages. Thanks to over a decade of research on statistical dependency parsing, many dependency parsers are now publicly available. In this paper, we report on a comparative analysis of leading statistical dependency parsers using a multi-genre corpus. Our purpose is not to introduce a new parsing algorithm but to assess the performance of exist"
P15-1038,P13-2109,0,0.137596,"Missing"
P15-1038,W13-4917,1,0.529379,"Missing"
P15-1038,D07-1013,0,0.0163942,"6 11,467 10,976 8,969 1,634 1,366 WB 284,975 36,351 38,490 12,452 1,797 1,787 ALL 2,084,081 291,640 216,357 105,179 15,161 11,697 Table 1: Distribution of data used for our experiments. The first three/last three rows show the number of tokens/trees in each genre. BC: broadcasting conversation, BN: broadcasting news, MZ: news magazine, NW : newswire, PT : pivot text, TC : telephone conversation, WB : web text, ALL : all genres combined. eralize.1 Furthermore, a detailed comparative error analysis is typically lacking. The most detailed comparison of dependency parsers to date was performed by McDonald and Nivre (2007; 2011); they analyzed accuracy as a function of sentence length, dependency distance, valency, non-projectivity, part-of-speech tags and dependency labels.2 Since then, additional analyses of dependency parsers have been performed, but either with respect to specific linguistic phenomena (e.g. (Nivre et al., 2010; Bender et al., 2011)) or to downstream tasks (e.g. (Miwa and others, 2010; Petrov et al., 2010; Yuret et al., 2013)). 3 3.1 ture from the input, as these are often not available in non-annotated data. 3.2 Dependency Conversion OntoNotes provides annotation of constituency trees only"
P15-1038,E12-2021,0,0.0425666,"Missing"
P15-1038,J11-1007,0,0.0218317,"Missing"
P15-1038,W08-2121,0,0.021027,"Missing"
P15-1038,W10-1905,0,0.0467351,"Missing"
P15-1038,D11-1036,0,0.0159019,"ate non-projective dependencies. Development data ClearNLP, LTDP, SNN and Yara make use of the development data (for parameter tuning). Mate and Turbo self-tune parameter settings using the training data. The others were trained using their default/“standard” parameter settings. 5 D EPENDA BLE: Web-based Evaluation and Visualization Tool There are several very useful tools for evaluating the output of dependency parsers, including the venerable eval.pl18 script used in the CoNLL shared tasks, and newer Java-based tools that support visualization of and search over parse trees such as TedEval (Tsarfaty et al., 2011),19 MaltEval (Nilsson and Nivre, 2008)20 and “What’s wrong with my NLP?”.21 Recently, there is momentum towards web-based tools for annotation and visualization of NLP pipelines (Stenetorp and others, 2012). For this work, we used a new webbased tool, D EPENDA BLE, developed by the first author of this paper. It requires no installation and so provides a convenient way to evaluate and compare dependency parsers. The following are key features of D EPENDA BLE: Beam search ClearNLP, LTDP, Redshift and Yara have the option of different beam settings. The higher the beam size, the more accurate th"
P15-1038,nilsson-nivre-2008-malteval,0,0.0972031,"elopment data ClearNLP, LTDP, SNN and Yara make use of the development data (for parameter tuning). Mate and Turbo self-tune parameter settings using the training data. The others were trained using their default/“standard” parameter settings. 5 D EPENDA BLE: Web-based Evaluation and Visualization Tool There are several very useful tools for evaluating the output of dependency parsers, including the venerable eval.pl18 script used in the CoNLL shared tasks, and newer Java-based tools that support visualization of and search over parse trees such as TedEval (Tsarfaty et al., 2011),19 MaltEval (Nilsson and Nivre, 2008)20 and “What’s wrong with my NLP?”.21 Recently, there is momentum towards web-based tools for annotation and visualization of NLP pipelines (Stenetorp and others, 2012). For this work, we used a new webbased tool, D EPENDA BLE, developed by the first author of this paper. It requires no installation and so provides a convenient way to evaluate and compare dependency parsers. The following are key features of D EPENDA BLE: Beam search ClearNLP, LTDP, Redshift and Yara have the option of different beam settings. The higher the beam size, the more accurate the parser usually becomes, but typicall"
P15-1038,C10-1094,0,0.0281924,"Missing"
P15-1038,S14-2008,0,\N,Missing
P15-1038,W06-2920,0,\N,Missing
P15-1038,D07-1096,0,\N,Missing
P15-2097,W13-3601,1,0.745346,"l shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by comparing its output against that of the official Helping Our Own (HOO) scorer (Dale and Kilgarriff, 2011), itself based on the GNU wdiff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can be reduced to checking whether a set of predefined errors have been changed into a set of associated corrections. M2 is not without its own issues. First, phraselevel edits can be gamed because the lattice treats a long phrase deletion as one edit.3 Second, the F-score does not capture the difference between “no change” and “wrong ed"
P15-2097,P15-1068,0,0.296006,"tion between the M2 and I-measure scores (Pearson’s r = −0.694). A difficulty with all these metrics is that they require detailed annotations of the location and erBLEU I-Measure 87 2 1 86 0 85 -1 84 -2 -3 83 -4 82 -5 81 -6 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2 , I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response to an explicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, mentioned above, it is difficult for annotators to reliably produce these annotations (Bryant and Ng, 2015). However, this requirement can be relinquished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine translation, as Park and Levy (2011) did with BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). As we will show in more detail in Section 5, taking the twelve publicly released system outputs from the CoNLL-2014 Shared Task,4 we actually find a negative correlation between the M2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) (Figure 1). With the earlier-reported negative correlation between I-m"
P15-2097,W14-1701,0,0.457886,"ale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by comparing its output against that of the official Helping Our Own (HOO) scorer (Dale and Kilgarriff, 2011), itself based on the GNU wdiff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can be reduced to checking whether a set of predefined errors have been changed into a set of associated corrections. M2 is not without its own issues. First, phraselevel edits can be gamed because the lattice treats a long phrase deletion as one edit.3 Second, the F-score does not capture the difference between “no change” and “wrong edits” made by syste"
P15-2097,C12-1038,1,0.763083,"trics should be compared. That is, what is the metric metric? The answer is that it should be rooted in the end-use case for the task under consideration. This could be some other metric further downstream of the task, or something simpler like direct human evaluation. This latter approach is the one often taken in machine translation; for example, the organizers of the Workshop on Statistical Machine 2 Grammatical error correction metrics GEC is often viewed as a matter of correcting isolated grammatical errors, but is much more complicated, nuanced, and subjective than that. As discussed in Chodorow et al. (2012), there is often no single correction for an error (e.g., whether to correct a subject-verb agreement error by changing the number of the subject or the verb), and errors cover a range of factors including style, register, venue, audience, and usage questions, about 1 Our code and rankings of the CoNLL-2014 Shared Task system outputs can be downloaded from github.com/ cnap/gec-ranking/. 588 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Short Papers), pages 588–593, c Beijing, Ch"
P15-2097,P02-1040,0,0.124111,"-3 83 -4 82 -5 81 -6 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2 , I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response to an explicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, mentioned above, it is difficult for annotators to reliably produce these annotations (Bryant and Ng, 2015). However, this requirement can be relinquished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine translation, as Park and Levy (2011) did with BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). As we will show in more detail in Section 5, taking the twelve publicly released system outputs from the CoNLL-2014 Shared Task,4 we actually find a negative correlation between the M2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) (Figure 1). With the earlier-reported negative correlation between I-measure and M2 , we have a troubling picture: which of these metrics is best? Which one actually captures and rewards the behaviors we would like our systems to report? Despite these many proposed metrics, n"
P15-2097,N12-1067,0,0.459535,"existence of different style manuals at newspapers, and questions about the legitimacy of prescriptivist grammar conventions. Several automatic metrics have been used for evaluating GEC systems. F-score, the harmonic mean of precision and recall, is one of the most commonly used metrics. It was used as an official evaluation metric for several shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by comparing its output against that of the official Helping Our Own (HOO) scorer (Dale and Kilgarriff, 2011), itself based on the GNU wdiff utility.2 In other words, it was evaluated under the assumption that evaluating GEC can"
P15-2097,P11-1094,0,0.0260818,"U I-Measure 87 2 1 86 0 85 -1 84 -2 -3 83 -4 82 -5 81 -6 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2 , I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response to an explicit error annotation scheme. Due to the inherent subjectivity and poor definition of the task, mentioned above, it is difficult for annotators to reliably produce these annotations (Bryant and Ng, 2015). However, this requirement can be relinquished by treating GEC as a text-to-text rewriting task and borrowing metrics from machine translation, as Park and Levy (2011) did with BLEU (Papineni et al., 2002) and METEOR (Lavie and Agarwal, 2007). As we will show in more detail in Section 5, taking the twelve publicly released system outputs from the CoNLL-2014 Shared Task,4 we actually find a negative correlation between the M2 and BLEU scores (r = −0.772) and positive correlation between I-measure and BLEU scores (r = 0.949) (Figure 1). With the earlier-reported negative correlation between I-measure and M2 , we have a troubling picture: which of these metrics is best? Which one actually captures and rewards the behaviors we would like our systems to report?"
P15-2097,W11-2838,0,0.028626,"g (Short Papers), pages 588–593, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics which there can be much disagreement. In addition, errors are not always errors, as can be seen from the existence of different style manuals at newspapers, and questions about the legitimacy of prescriptivist grammar conventions. Several automatic metrics have been used for evaluating GEC systems. F-score, the harmonic mean of precision and recall, is one of the most commonly used metrics. It was used as an official evaluation metric for several shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014). M2 was assessed by compari"
P15-2097,W14-3301,1,0.884876,"Missing"
P15-2097,W12-2006,0,0.0374068,"Language Processing (Short Papers), pages 588–593, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics which there can be much disagreement. In addition, errors are not always errors, as can be seen from the existence of different style manuals at newspapers, and questions about the legitimacy of prescriptivist grammar conventions. Several automatic metrics have been used for evaluating GEC systems. F-score, the harmonic mean of precision and recall, is one of the most commonly used metrics. It was used as an official evaluation metric for several shared tasks (Dale et al., 2012; Dale and Kilgarriff, 2011), where participants were asked to detect and correct closedclass errors (i.e., determiners and prepositions). One of the issues with F-score is that it fails to capture phrase-level edits. Thus Dahlmeier and Ng (2012) proposed the MaxMatch (M2 ) scorer, which calculates the F-score over an edit lattice that captures phrase-level edits. For GEC, M2 is the standard, having been used to rank error correction systems in the 2013 and 2014 CoNLL shared tasks, where the error types to be corrected were not limited to closed-class errors. (Ng et al., 2013; Ng et al., 2014)"
P15-2097,N15-1060,0,0.534138,"f utility.2 In other words, it was evaluated under the assumption that evaluating GEC can be reduced to checking whether a set of predefined errors have been changed into a set of associated corrections. M2 is not without its own issues. First, phraselevel edits can be gamed because the lattice treats a long phrase deletion as one edit.3 Second, the F-score does not capture the difference between “no change” and “wrong edits” made by systems. Chodorow et al. (2012) also list other complications arising from using F-score or M2 , depending on the application of GEC. Considering these problems, Felice and Briscoe (2015) proposed a new metric, I-measure, which is based on accuracy computed by edit distance between the source, reference, and system output. Their results are striking: there is a negative correlation between the M2 and I-measure scores (Pearson’s r = −0.694). A difficulty with all these metrics is that they require detailed annotations of the location and erBLEU I-Measure 87 2 1 86 0 85 -1 84 -2 -3 83 -4 82 -5 81 -6 0 10 20 30 40 50 M2 Score Figure 1: Correlation among M2 , I-measure, and BLEU scores: M2 score shows negative correlations to other metrics. ror type of each correction in response"
P15-2097,W07-0734,0,\N,Missing
P19-1043,W17-2408,0,0.158932,"velop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. Our system outperforms competitive baselines given both automatic and human evaluations. To our knowledge, this is the first work to tackle the problem of effective email subject line generation. 1 Table 1: An email with three possible subject lines. 2018), and email classification (Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017), to our knowledge there is no previous work on generating email subjects. In this paper, we propose the task of Subject Line Generation (SLG): automatically producing email subjects given the email body. While this is similar to email summarization, the two tasks serve different purposes in the process of email composition and consumption. A subject line is required when the sender writes the email, while a summary is more useful for long emails to benefit the recipient. An automatically generated email subject can also be used for downstream applications such as email triaging to help people"
P19-1043,D14-1181,0,0.00235813,"ion or METEOR for machine translation, there is no available automatic metric designed for email subject generation. Motivated by recent work on regression-based metrics for machine translation (Shimanaka et al., 2018) and dialog response generation (Lowe et al., 2017), we build a neural network (ESQE) to estimate the quality of an email subject given the email body (§3.3). The estimator is pretrained and fixed during RL training phase to provide rewards for the extractor agent. While our model is based on Chen and Bansal D = [d1 , d2 , . . . , dj , . . . , d|D |] We first use a temporal CNN (Kim, 2014) to build individual sentence representations. For each sentence, we feed the sequence of its word vectors into 1-D convolutional filters with various window sizes. We then apply ReLU activation and then max-over-time pooling. The sentence representation is a concatenation of activations from all filters cj = CNN(dj ), j = 1, . . . , |D| (1) Then we use a bidirectional LSTM (Hochreiter and Schmidhuber, 1997) to capture documentlevel inter-sentence information over CNN outputs: → − → − d j = LSTMforward ( d j−1 , cj ) ← − ← − d j = LSTMbackward ( d j+1 , cj ) → − ← − dj = [ d j , d j ] (2) For"
P19-1043,P08-1041,0,0.0246458,"ard. Our model outperforms several competitive baselines and approaches human-level performance. In the future, we would like to generalize it to multiple domains and datasets. We are also interested in generating more effective and appropriate subjects by incorporating prior email conversations, social context, the goal and style of emails, personality, among others. Related Work Past NLP email research has focused on summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004; CorstonOliver et al., 2004; Wan and McKeown, 2004; Carenini et al., 2007; Zajic et al., 2008; Carenini et al., 2008; Ulrich et al., 2009), keyword extraction and action detection (Turney, 2000; Bennett and Carbonell, 2005; Dredze et al., 2008; Scerri et al., 2010; Loza et al., 2014; Lahiri et al., 2017; Lin et al., 2018), and classification (Prabhakaran et al., 2014; Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017). However, we could not find any previous work on email subject line generation. The very first study on email summarization is Muresan et al. (2001) who reduce the problem to extracting salient phrases. Later, Nenkova and Bagga (2003), Rambow et al. (2004), Wan and McKeown (2004) deal w"
P19-1043,P18-1063,0,0.0119666,"he email body which contain the necessary information for writing a subject. This task can be formulated as a sequence-to-sequence learning problem where the output sequence corresponds to the position of “positive” sentences in the input email body. Therefore, we use a pointer network (Vinyals et al., 2015) to first build hierarchical sentence representations during encoding and then extract “positive” sentences during decoding. Suppose our input is an email body D which consists of |D |sentences: Our Model Our model is illustrated in Figure 1. Based on recent progress in news summarization (Chen and Bansal, 2018), our model generates email subjects in two stages: (1) The extractor selects multiple sentences containing salient information for writing a subject (§3.1). (2) The abstractor rewrites multiple selected sentences into a succinct subject line while preserving key information (§3.2). We employ a multi-stage training strategy (§3.4) including a Reinforcement Learning (RL) phase because of its usefulness for text generation tasks (Ranzato et al., 2016; Bahdanau et al., 2017) to optimize the non-differentiable metrics such as ROUGE and METEOR. However, unlike ROUGE for summarization or METEOR for"
P19-1043,P16-1046,0,0.0785311,"Missing"
P19-1043,W04-1000,0,0.533926,"Missing"
P19-1043,P17-1103,0,0.0210667,"ject line while preserving key information (§3.2). We employ a multi-stage training strategy (§3.4) including a Reinforcement Learning (RL) phase because of its usefulness for text generation tasks (Ranzato et al., 2016; Bahdanau et al., 2017) to optimize the non-differentiable metrics such as ROUGE and METEOR. However, unlike ROUGE for summarization or METEOR for machine translation, there is no available automatic metric designed for email subject generation. Motivated by recent work on regression-based metrics for machine translation (Shimanaka et al., 2018) and dialog response generation (Lowe et al., 2017), we build a neural network (ESQE) to estimate the quality of an email subject given the email body (§3.3). The estimator is pretrained and fixed during RL training phase to provide rewards for the extractor agent. While our model is based on Chen and Bansal D = [d1 , d2 , . . . , dj , . . . , d|D |] We first use a temporal CNN (Kim, 2014) to build individual sentence representations. For each sentence, we feed the sequence of its word vectors into 1-D convolutional filters with various window sizes. We then apply ReLU activation and then max-over-time pooling. The sentence representation is a"
P19-1043,W14-3348,0,0.00867843,", negative otherwise. The multi-sentence extractor is trained to predict “positive” sentences by minimizing the cross-entropy loss. For the multi-sentence abstractor, we create training examples by pairing the “positive” sentences and the 3.3 Email Subject Quality Estimator Since there is no established automatic metric for SLG, we build our own Email Subject Quality Estimator (ESQE). Given an email body D and a potential subject for the subject s, our quality estimator outputs a real-valued Subject Quality score 449 (Lin, 2004) including F1 scores of ROUGE1, ROUGE-2, and ROUGE-L. (2) METEOR (Denkowski and Lavie, 2014). They all rely on one or more references and measure the similarity between the output and the reference. In addition, we include ESQE, which is a reference-less metric. Human Evaluation. While those automatic scores are quick and inexpensive to calculate, only our quality estimator is designed for evaluation of subject line generation. Therefore, we also conduct an extensive human evaluation on the overall score and two aspects of email quality: informativeness and fluency. An email subject is informative if it contains accurate and consistent details with the body, and it is fluent if free"
P19-1043,loza-etal-2014-building,0,0.0177156,"sets. We are also interested in generating more effective and appropriate subjects by incorporating prior email conversations, social context, the goal and style of emails, personality, among others. Related Work Past NLP email research has focused on summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004; CorstonOliver et al., 2004; Wan and McKeown, 2004; Carenini et al., 2007; Zajic et al., 2008; Carenini et al., 2008; Ulrich et al., 2009), keyword extraction and action detection (Turney, 2000; Bennett and Carbonell, 2005; Dredze et al., 2008; Scerri et al., 2010; Loza et al., 2014; Lahiri et al., 2017; Lin et al., 2018), and classification (Prabhakaran et al., 2014; Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017). However, we could not find any previous work on email subject line generation. The very first study on email summarization is Muresan et al. (2001) who reduce the problem to extracting salient phrases. Later, Nenkova and Bagga (2003), Rambow et al. (2004), Wan and McKeown (2004) deal with the problem of email thread summarization by the sentence extraction approach. Acknowledgements We would like to thank Jimmy Nguyen and Vipul Raheja for their help"
P19-1043,D15-1166,0,0.0145755,"t sentences: SQ(D, s) = FFNN([D, s]) oˆtj = vo |tanh(Wo dj + Uo et ) P (ot |o1 , o2 , . . . , ot−1 ) = softmax(ˆ ot ) (4) where {v, W, U} are trainable parameters. We also add a trainable “stop” vector with the same dimension as the sentence representation. The decoder can choose to stop by pointing to this “stop” sentence. 3.2 Multi-sentence Abstractor In the second stage, the abstractor takes the selected sentences from the extractor and rewrites them into an email subject. We implement the abstractor as a sequence-to-sequence encoderdecoder model with the bilinear multiplicative attention (Luong et al., 2015) and copy mechanism (See et al., 2017). The copy mechanism enables the decoder to copy words directly from the input document, which is helpful to generate accurate information verbatim even for out-of-vocabulary words. (6) To train the estimator, we collect human evaluations on 3,490 email subjects. In order to expose the estimator to both good and bad examples, 2,278 of the 3,490 are the original subjects and the remaining 1,212 subjects are generated by an existing summarization system. Each subject has 3 human evaluation scores (the same human evaluation as explained in §4.1) and we train"
P19-1043,W04-3252,0,0.257472,"by the extractor: r(a1:T ) = SQ(D, s) (8) For training, we maximize the expected reward: 4.2 L(θ) = Ea1:T ∼πθ [r(a1:T )] (9) To benchmark our method, we use several methods from the summarization field, including some recent state-of-the-art systems, because the email subject line can be viewed as a short summary of the email content. They can be clustered into two groups. (1) Unsupervised extractive or/and abstractive summarization. LEAD-2 directly uses the first two sentences as the subject line. We choose lead2 to include both the greeting and the first sentence of main content. TextRank (Mihalcea and Tarau, 2004) and LexRank (Erkan and Radev, 2004) are two graph-based ranking models to extract the most salient sentence as the subject line. Shang et al. (2018) use a graph-based framework to extract topics and then generate a single abstractive sentence for each topic under a budget constraint. (2) Neural summarization using encoderdecoder networks with attention mechanisms. (Sutskever et al., 2014; Bahdanau et al., 2015). The Pointer-Generator Network from See et al. (2017) augments the standard encoder-decoder network by adding the ability to copy words from the source text and using the coverage loss"
P19-1043,C10-1037,0,0.0312835,"ole training takes about 4 hours. 4.3 Baselines. For TextRank and LexRank, we use the sumy2 implementation which uses the snowball stemmer, the sentence and word tokenizer from NLTK3 . For Shang et al. (2018), we Implementation Details Our Model. We pretrain 128-dimensional word2vec (Mikolov et al., 2013) on our corpus as initialization and update word embeddings during training. We use single layer bidirectional LSTMs with 256 hidden units in all models. The 2 3 451 https://github.com/miso-belica/sumy https://www.nltk.org/ use their extension of the Multi-Sentence Compression Graph (MSCG) of Filippova (2010) and a budget of 10 words in the submodular maximization. We choose the number of communities from [1,2,3,4,5] based on the dev set and we find that 1 works best. For the Pointer-Generator Network from See et al. (2017), we follow their implementation4 and use a batch size 16. For Paulus et al. (2018), we use an implementation from Keneshloo et al. (2018)5 . We did not include the intra-temporal attention and the intra-decoder attention because they hurt the performance. For Hsu et al. (2018), we follow their code6 with a batch size 16. All training is early stopped based on the dev set perfor"
P19-1043,P18-1013,0,0.233096,"ients. The baseline network has the same architecture as the decoder of the extractor. But it has another set of trainable parameters θb and predicts the reward by minimizing the following mean squared error: L(θb ) = (bt − r)2 4 4.1 Baselines (11) Experimental Setup Evaluation Automatic Evaluation. Since SLG is a new task, we analyze the usefulness of automatic metrics from sister tasks, and also use human evaluation. We first use automatic metrics from text summarization and machine translation: (1) ROUGE 450 LEAD-2 TextRank LexRank Shang et al. (2018) See et al. (2017) Paulus et al. (2018) Hsu et al. (2018) Narayan et al. (2018a) Our System Human Annotation R-1 11.28 11.12 13.02 10.56 18.02 14.08 16.59 13.52 25.41 23.43∗ R-2 4.61 3.75 4.96 3.28 5.73 5.09 4.67 3.27 11.34 9.71∗ Dev R-L 10.48 10.15 11.89 9.92 16.63 13.36 15.12 13.33 25.07 22.17 METEOR 10.76 9.19 10.84 6.17 10.83 9.07 13.22∗ 4.64 9.83 10.87∗ R-1 11.00 11.32 12.46 10.40 17.02 13.49 15.75 12.60 23.67 23.90∗ R-2 4.33 3.88 4.62 3.09 5.45 4.55 4.54 3.09 10.29 10.09∗ Test R-L 10.20 10.14 11.37 9.77 15.78 12.83 14.41 12.52 23.44 22.75∗ METEOR 11.27∗ 10.64∗ 11.56∗ 6.15 10.31 8.65 12.49∗ 4.66 9.37 11.04∗ Test R-L 16.62 16.00 18.68 14.88 20.8"
P19-1043,W01-0719,0,0.456874,"tion Email is a ubiquitous form of online communication. An email message consists of two basic elements: an email subject line and an email body. The subject line, which is displayed to the recipient in the list of inbox messages, should tell what the email body is about and what the sender wants to convey. An effective email subject line becomes essential since it can help people manage a large number of emails. Table 1 shows an email body with three possible subject lines. There have been several research tracks around email usage. While much effort has been focused on email summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004), email keyword extraction and action detection (Turney, 2000; Lahiri et al., 2017; Lin et al., ∗ Work done during the internship at Grammarly. 446 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446–456 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset CNN (Cheng and Lapata, 2016) XSum (Narayan et al., 2018a) Gigaword News Headline (Rush et al., 2015) Annotated Enron Subject Line Corpus domain News News News Business/Personal docs (train/val/test) 90,"
P19-1043,N18-4015,0,0.0145992,"rewrites multiple selected sentences into a succinct subject line while preserving key information (§3.2). We employ a multi-stage training strategy (§3.4) including a Reinforcement Learning (RL) phase because of its usefulness for text generation tasks (Ranzato et al., 2016; Bahdanau et al., 2017) to optimize the non-differentiable metrics such as ROUGE and METEOR. However, unlike ROUGE for summarization or METEOR for machine translation, there is no available automatic metric designed for email subject generation. Motivated by recent work on regression-based metrics for machine translation (Shimanaka et al., 2018) and dialog response generation (Lowe et al., 2017), we build a neural network (ESQE) to estimate the quality of an email subject given the email body (§3.3). The estimator is pretrained and fixed during RL training phase to provide rewards for the extractor agent. While our model is based on Chen and Bansal D = [d1 , d2 , . . . , dj , . . . , d|D |] We first use a temporal CNN (Kim, 2014) to build individual sentence representations. For each sentence, we feed the sequence of its word vectors into 1-D convolutional filters with various window sizes. We then apply ReLU activation and then max-"
P19-1043,D18-1206,0,0.36017,"ree possible subject lines. There have been several research tracks around email usage. While much effort has been focused on email summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004), email keyword extraction and action detection (Turney, 2000; Lahiri et al., 2017; Lin et al., ∗ Work done during the internship at Grammarly. 446 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446–456 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset CNN (Cheng and Lapata, 2016) XSum (Narayan et al., 2018a) Gigaword News Headline (Rush et al., 2015) Annotated Enron Subject Line Corpus domain News News News Business/Personal docs (train/val/test) 90,266/1,220/1,093 204,045/11,332/11,334 3,799,588/394,622/381,197 14,436/1,960/1,906 avg doc words 760 431 31 75 avg summary words 46 23 8 4 Table 2: Annotated Enron Subject Line Corpus compared with other datasets. to properly evaluate the subject, we use a combination of automatic metrics from the text summarization and machine translation fields, in addition to building our own regression-based Email Subject Quality Estimator (ESQE). Third, to gene"
P19-1043,N18-1158,0,0.19802,"ree possible subject lines. There have been several research tracks around email usage. While much effort has been focused on email summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004), email keyword extraction and action detection (Turney, 2000; Lahiri et al., 2017; Lin et al., ∗ Work done during the internship at Grammarly. 446 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446–456 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset CNN (Cheng and Lapata, 2016) XSum (Narayan et al., 2018a) Gigaword News Headline (Rush et al., 2015) Annotated Enron Subject Line Corpus domain News News News Business/Personal docs (train/val/test) 90,266/1,220/1,093 204,045/11,332/11,334 3,799,588/394,622/381,197 14,436/1,960/1,906 avg doc words 760 431 31 75 avg summary words 46 23 8 4 Table 2: Annotated Enron Subject Line Corpus compared with other datasets. to properly evaluate the subject, we use a combination of automatic metrics from the text summarization and machine translation fields, in addition to building our own regression-based Email Subject Quality Estimator (ESQE). Third, to gene"
P19-1043,W17-4503,0,0.0470077,"Missing"
P19-1043,P14-2056,0,0.121458,"ment summarization. We then develop a novel deep learning method and compare it to several baselines as well as recent state-of-the-art text summarization systems. We also investigate the efficacy of several automatic metrics based on correlations with human judgments and propose a new automatic evaluation metric. Our system outperforms competitive baselines given both automatic and human evaluations. To our knowledge, this is the first work to tackle the problem of effective email subject line generation. 1 Table 1: An email with three possible subject lines. 2018), and email classification (Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017), to our knowledge there is no previous work on generating email subjects. In this paper, we propose the task of Subject Line Generation (SLG): automatically producing email subjects given the email body. While this is similar to email summarization, the two tasks serve different purposes in the process of email composition and consumption. A subject line is required when the sender writes the email, while a summary is more useful for long emails to benefit the recipient. An automatically generated email subject can also be used for downstream applications such as"
P19-1043,D14-1211,0,0.0207832,"cts by incorporating prior email conversations, social context, the goal and style of emails, personality, among others. Related Work Past NLP email research has focused on summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004; CorstonOliver et al., 2004; Wan and McKeown, 2004; Carenini et al., 2007; Zajic et al., 2008; Carenini et al., 2008; Ulrich et al., 2009), keyword extraction and action detection (Turney, 2000; Bennett and Carbonell, 2005; Dredze et al., 2008; Scerri et al., 2010; Loza et al., 2014; Lahiri et al., 2017; Lin et al., 2018), and classification (Prabhakaran et al., 2014; Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017). However, we could not find any previous work on email subject line generation. The very first study on email summarization is Muresan et al. (2001) who reduce the problem to extracting salient phrases. Later, Nenkova and Bagga (2003), Rambow et al. (2004), Wan and McKeown (2004) deal with the problem of email thread summarization by the sentence extraction approach. Acknowledgements We would like to thank Jimmy Nguyen and Vipul Raheja for their help in the data creation process. We also thank Dragomir Radev, Courtney Napoles, Dimitri"
P19-1043,C04-1079,0,0.102346,"Selection and Rewriting with Email Subject Quality Estimation Reward. Our model outperforms several competitive baselines and approaches human-level performance. In the future, we would like to generalize it to multiple domains and datasets. We are also interested in generating more effective and appropriate subjects by incorporating prior email conversations, social context, the goal and style of emails, personality, among others. Related Work Past NLP email research has focused on summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004; CorstonOliver et al., 2004; Wan and McKeown, 2004; Carenini et al., 2007; Zajic et al., 2008; Carenini et al., 2008; Ulrich et al., 2009), keyword extraction and action detection (Turney, 2000; Bennett and Carbonell, 2005; Dredze et al., 2008; Scerri et al., 2010; Loza et al., 2014; Lahiri et al., 2017; Lin et al., 2018), and classification (Prabhakaran et al., 2014; Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017). However, we could not find any previous work on email subject line generation. The very first study on email summarization is Muresan et al. (2001) who reduce the problem to extracting salient phrases. Later, Nenkova and"
P19-1043,N04-4027,0,0.359232,"nication. An email message consists of two basic elements: an email subject line and an email body. The subject line, which is displayed to the recipient in the list of inbox messages, should tell what the email body is about and what the sender wants to convey. An effective email subject line becomes essential since it can help people manage a large number of emails. Table 1 shows an email body with three possible subject lines. There have been several research tracks around email usage. While much effort has been focused on email summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004), email keyword extraction and action detection (Turney, 2000; Lahiri et al., 2017; Lin et al., ∗ Work done during the internship at Grammarly. 446 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446–456 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset CNN (Cheng and Lapata, 2016) XSum (Narayan et al., 2018a) Gigaword News Headline (Rush et al., 2015) Annotated Enron Subject Line Corpus domain News News News Business/Personal docs (train/val/test) 90,266/1,220/1,093 204,045/11,332/11,334 3,799,588"
P19-1043,D15-1044,0,0.485597,"eral research tracks around email usage. While much effort has been focused on email summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004), email keyword extraction and action detection (Turney, 2000; Lahiri et al., 2017; Lin et al., ∗ Work done during the internship at Grammarly. 446 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 446–456 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Dataset CNN (Cheng and Lapata, 2016) XSum (Narayan et al., 2018a) Gigaword News Headline (Rush et al., 2015) Annotated Enron Subject Line Corpus domain News News News Business/Personal docs (train/val/test) 90,266/1,220/1,093 204,045/11,332/11,334 3,799,588/394,622/381,197 14,436/1,960/1,906 avg doc words 760 431 31 75 avg summary words 46 23 8 4 Table 2: Annotated Enron Subject Line Corpus compared with other datasets. to properly evaluate the subject, we use a combination of automatic metrics from the text summarization and machine translation fields, in addition to building our own regression-based Email Subject Quality Estimator (ESQE). Third, to generate effective email subjects, we propose a m"
P19-1043,I17-2062,0,0.0543644,"Missing"
P19-1043,D17-1062,0,0.014181,"document summarization because the email subject line can be viewed as a short summary of the email content. Therefore, we use different summarization models as baselines with techniques such as graph-based extraction and compression, sequence-to-sequence neural abstractive summarization with the hierarchical attention, copy, and coverage mechanisms. In addition, RL has become increasingly popular for text generation to optimize the non-differentiable metrics and to reduce the exposure bias in the traditional “teaching forcing” supervised training (Ranzato et al., 2016; Bahdanau et al., 2017; Zhang and Lapata, 2017; Sakaguchi et al., 2017). For example, Narayan et al. (2018b) use RL for ranking sentences in pure extractive summarization. Furthermore, current methods on news headline generation (Lopyrev, 2015; Tilk and Alum¨ae, 2017; Kiyono et al., 2017; Tan et al., 2017; Shen et al., 2017) most follow the encoder-decoder model, while our model uses a multi-sentence selection and rewriting framework. different automatic scores and the average human rating. We also report the inter-rater agreement in the last row by checking the correlation between the third human rating and the average of the other two."
P19-1043,scerri-etal-2010-classifying,0,0.0203545,"iple domains and datasets. We are also interested in generating more effective and appropriate subjects by incorporating prior email conversations, social context, the goal and style of emails, personality, among others. Related Work Past NLP email research has focused on summarization (Muresan et al., 2001; Nenkova and Bagga, 2003; Rambow et al., 2004; CorstonOliver et al., 2004; Wan and McKeown, 2004; Carenini et al., 2007; Zajic et al., 2008; Carenini et al., 2008; Ulrich et al., 2009), keyword extraction and action detection (Turney, 2000; Bennett and Carbonell, 2005; Dredze et al., 2008; Scerri et al., 2010; Loza et al., 2014; Lahiri et al., 2017; Lin et al., 2018), and classification (Prabhakaran et al., 2014; Prabhakaran and Rambow, 2014; Alkhereyf and Rambow, 2017). However, we could not find any previous work on email subject line generation. The very first study on email summarization is Muresan et al. (2001) who reduce the problem to extracting salient phrases. Later, Nenkova and Bagga (2003), Rambow et al. (2004), Wan and McKeown (2004) deal with the problem of email thread summarization by the sentence extraction approach. Acknowledgements We would like to thank Jimmy Nguyen and Vipul Ra"
P19-1043,P17-1099,0,0.693041,"j = vo |tanh(Wo dj + Uo et ) P (ot |o1 , o2 , . . . , ot−1 ) = softmax(ˆ ot ) (4) where {v, W, U} are trainable parameters. We also add a trainable “stop” vector with the same dimension as the sentence representation. The decoder can choose to stop by pointing to this “stop” sentence. 3.2 Multi-sentence Abstractor In the second stage, the abstractor takes the selected sentences from the extractor and rewrites them into an email subject. We implement the abstractor as a sequence-to-sequence encoderdecoder model with the bilinear multiplicative attention (Luong et al., 2015) and copy mechanism (See et al., 2017). The copy mechanism enables the decoder to copy words directly from the input document, which is helpful to generate accurate information verbatim even for out-of-vocabulary words. (6) To train the estimator, we collect human evaluations on 3,490 email subjects. In order to expose the estimator to both good and bad examples, 2,278 of the 3,490 are the original subjects and the remaining 1,212 subjects are generated by an existing summarization system. Each subject has 3 human evaluation scores (the same human evaluation as explained in §4.1) and we train our estimator to regress the average."
P19-1043,W04-1013,0,\N,Missing
P19-1043,W04-1008,0,\N,Missing
P99-1079,P87-1022,0,0.946916,"-Right Centering Algorithm (LRC), lie in its incremental processing of utterances and in its low computational overhead. The algorithm is compared with three other pronoun resolution methods: Hobbs' syntax-based algorithm, Strube's S-list approach, and the B F P Centering algorithm. All four methods were implemented in a system and tested on an annotated subset of the Treebank corpus consisting of 2026 pronouns. The noteworthy results were that Hobbs and LRC performed the best. 1 2 Introduction The aim of this project is to develop a pronoun resolution algorithm which performs better than the Brennan et al. 1987 algorithm 1 as a cognitive model while also performing well empirically. A revised algorithm (Left-Right Centering) was motivated by the fact that the B F P algorithm did not allow for incremental processing of an utterance and hence of its pronouns, and also by the fact that it occasionally imposes a high computational load, detracting from its psycholinguistic plausibility. A second motivation for the project is to remedy the dearth of empirical results on pronoun resolution methods. Many small comparisons of methods have been made, such as by Strube (1998) and Walker (1989), but those usua"
P99-1079,W98-1119,0,0.545098,"Missing"
P99-1079,J95-2003,0,0.505556,"statistics. By creating a system that can run algorithms, one can easily and quickly analyze large amounts of data and generate more reliable results. In this project, the new algorithm is tested against three leading syntax-based pronoun resolution methods: Hobbs' naive algorithm (1977), S-list (Strube 1998), and BFP. Section 2 presents the motivation and algorithm for Left-Right Centering. In Section 3, the results of the algorithms are presented and then discussed in Section 4. This paper presents a pronoun resolution algorithm that adheres to the constraints and rules of Centering Theory (Grosz et al., 1995) and is an alternative to Brennan et al.'s 1987 algorithm. The advantages of this new model, the Left-Right Centering Algorithm (LRC), lie in its incremental processing of utterances and in its low computational overhead. The algorithm is compared with three other pronoun resolution methods: Hobbs' syntax-based algorithm, Strube's S-list approach, and the B F P Centering algorithm. All four methods were implemented in a system and tested on an annotated subset of the Treebank corpus consisting of 2026 pronouns. The noteworthy results were that Hobbs and LRC performed the best. 1 2 Introduction"
P99-1079,J97-3006,0,0.254959,"arisons of methods have been made, such as by Strube (1998) and Walker (1989), but those usually consist of statistics based on a small handtested corpus. The problem with evaluating 1Henceforth BFP 602 Left-Right Centering A l g o r i t h m Left-Right Centering (LRC) is a formalized algorithm built u p o n centering theory's constraints and rules as detailed in Grosz et. al (1995). The creation of the LRC Algorithm is motivated by two drawbacks found in the B F P method. The first is B F P ' s limitation as a cognitive model since it makes no provision for incremental resolution of pronouns (Kehler 1997). Psycholinguistic research support the claim that listeners process utterances one word at a time, so when they hear a pronoun they will try to resolve it immediately. If new information comes into play which makes the resolution incorrect (such as a violation of binding constraints), the listener will go back and find a correct antecedent. This incremental resolution problem also motivates Strube's S-list approach. The second drawback to the B F P algorithm is the computational explosion of generating and filtering anchors. In utterances with two or more pronouns and a Cf-list with several c"
P99-1079,J93-2004,0,0.0315705,"tial(Un) 3 that meets feature and binding constraints. If one is found proceed to the next pronoun within utterance. Else go to (b). (b) Search for an antecedent intersententially in Cf(Un-1) that meets feature and binding constraints. 3. C r e a t e C f - create Cf-list of Un by ranking discourse entities of Un according to grammatical function. Our implementation used a left-right breadth-first walk of the parse tree to approximate sorting by grammatical function. Evaluation of A l g o r i t h m s All four algorithms were run on a 3900 utterance subset of the Penn Treebank annotated corpus (Marcus et al., 1993) provided by Charniak and Ge (1998). The corpus consists of 195 different newspaper articles. Sentences are fully bracketed and have labels that indicate word-class and features. Because the S-list and BFP algorithms do not allow resolution of quoted text, all quoted expressions were removed from the corpus, leaving 1696 pronouns (out of 2026) to be resolved. For analysis, the algorithms were broken up into two classes. The ""N"" group consists of algorithms that search intersententially through all Cf-lists for an antecedent. The ""1"" group consists of algorithms that can only search for an ante"
P99-1079,P96-1036,0,0.0762156,"available, Strube's S-list algorithm and LRC prove more useful since grammatical function can be approximated by using surface order. 6 Future Work The next step is t o test all four algorithms on a novel or short stories. Statistics from the Walker and Strube studies suggest that BFP will perform better in these cases. Other future work includes constructing a hybrid algorithm of LRC and S-list in which entities are ranked both by the familiarity scale and by grammatical function. Research into how transitions and the Cb can be used in a pronoun resolution algorithm should also be examined. Strube and Hahn (1996) developed a heuristic of ranking transition pairs by cost to evaluate different Cfranking schemes. Perhaps this heuristic could be used to constrain the search for antecedents. It is quite possible that hybrid algorithms (i.e. using Hobbs for intrasentential resolution, LRC for intersentential) may not produce any significant improvement over the current systems. If so, this might indicate that purely syntactic methods cannot be pushed much farther, and the upper limit reached can serve as a base line for approaches that combine syntax and semantics. 7 and coding of the Hobbs algorithm. Speci"
P99-1079,P98-2204,0,0.677484,"el R. Tetreault University of R o c h e s t e r D e p a r t m e n t of C o m p u t e r Science Rochester, NY, 14627 tetreaul@cs, r o c h e s t e r , e d u Abstract algorithms by hand is that it is time consuming and difficult to process corpora that are large enough to provide reliable, broadly based statistics. By creating a system that can run algorithms, one can easily and quickly analyze large amounts of data and generate more reliable results. In this project, the new algorithm is tested against three leading syntax-based pronoun resolution methods: Hobbs' naive algorithm (1977), S-list (Strube 1998), and BFP. Section 2 presents the motivation and algorithm for Left-Right Centering. In Section 3, the results of the algorithms are presented and then discussed in Section 4. This paper presents a pronoun resolution algorithm that adheres to the constraints and rules of Centering Theory (Grosz et al., 1995) and is an alternative to Brennan et al.'s 1987 algorithm. The advantages of this new model, the Left-Right Centering Algorithm (LRC), lie in its incremental processing of utterances and in its low computational overhead. The algorithm is compared with three other pronoun resolution methods"
P99-1079,P89-1031,0,0.481421,"than the Brennan et al. 1987 algorithm 1 as a cognitive model while also performing well empirically. A revised algorithm (Left-Right Centering) was motivated by the fact that the B F P algorithm did not allow for incremental processing of an utterance and hence of its pronouns, and also by the fact that it occasionally imposes a high computational load, detracting from its psycholinguistic plausibility. A second motivation for the project is to remedy the dearth of empirical results on pronoun resolution methods. Many small comparisons of methods have been made, such as by Strube (1998) and Walker (1989), but those usually consist of statistics based on a small handtested corpus. The problem with evaluating 1Henceforth BFP 602 Left-Right Centering A l g o r i t h m Left-Right Centering (LRC) is a formalized algorithm built u p o n centering theory's constraints and rules as detailed in Grosz et. al (1995). The creation of the LRC Algorithm is motivated by two drawbacks found in the B F P method. The first is B F P ' s limitation as a cognitive model since it makes no provision for incremental resolution of pronouns (Kehler 1997). Psycholinguistic research support the claim that listeners proc"
P99-1079,C98-2199,0,\N,Missing
Q16-1005,W11-2826,0,0.236687,"1987; Lahiri et al., 2011). Other recent work adopts a less abstract definition which is similar to the notion of “noisy text”– e.g. use of slang and poor grammar (Mosquera and Moreda, 2012a; Peterson et al., 2011). As a result, many rules have been explored for recognizing and generating informal language. Some of these rules are abstract, such as the level of implicature (Heylighen and Dewaele, 1999; Lahiri, 2015) or the degree of subjectivity (Mosquera and Moreda, 2012a), while others are much more concrete, such as the number of adjectives (Fang and Cao, 2009) or use of contractions (Abu Sheikha and Inkpen, 2011). Much prior work on detecting formality has focused on the lexical level (Brooke et al., 2010; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015). For larger units of text, perhaps the best-known method for measuring formality is the F-score1 (Heylighen and Dewaele, 1999), which is based on relative part-of-speech frequencies. F-score and its more recent variants (Li et al., 2013) provide a coarse measure of formality, but are designed to work at the genrelevel, making them less reliable for shorter units of text such as sentences (Lahiri, 2015). Exist1 We use special font to denote Heylighen"
Q16-1005,C14-1205,0,0.490597,"text”– e.g. use of slang and poor grammar (Mosquera and Moreda, 2012a; Peterson et al., 2011). As a result, many rules have been explored for recognizing and generating informal language. Some of these rules are abstract, such as the level of implicature (Heylighen and Dewaele, 1999; Lahiri, 2015) or the degree of subjectivity (Mosquera and Moreda, 2012a), while others are much more concrete, such as the number of adjectives (Fang and Cao, 2009) or use of contractions (Abu Sheikha and Inkpen, 2011). Much prior work on detecting formality has focused on the lexical level (Brooke et al., 2010; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015). For larger units of text, perhaps the best-known method for measuring formality is the F-score1 (Heylighen and Dewaele, 1999), which is based on relative part-of-speech frequencies. F-score and its more recent variants (Li et al., 2013) provide a coarse measure of formality, but are designed to work at the genrelevel, making them less reliable for shorter units of text such as sentences (Lahiri, 2015). Exist1 We use special font to denote Heylighen and Dewaele’s F-score to avoid confusion with F1 measure. 62 ing statistical approaches to detecting formality (Abu S"
Q16-1005,C10-2011,0,0.109136,"ute this score in the same way as Sidhaye and Cheung (2015), who used it to measure the formality of tweets. • Ngram classifier: As our final baseline, we train a ridge regression model which uses only ngrams (unigrams, bigrams, and trigrams) as features. Comparison against previously published models. Note that we are not able to make a meaningful comparison against against any of the previously published statistical models for formality detection. To our knowledge, there are three relevant previous publications that produced statistical models for detecting formality: Abu Sheikha and Inkpen (2010), Peterson et al. (2011), and Mosquera and Moreda (2012b). All three of these models performed a binary classification (as opposed to regression) and operated at the document (as opposed to sentence level). We were able to closely reimplement the model of Peterson et al. (2011), but we choose not to include the results here since their model was designed for binary email-level classification and thus relies on domain-specific features (e.g. casing in the subject line), that are not available in our real-valued, sentence-level datasets. The other models and the data/lexicons on which they relie"
Q16-1005,P13-1025,0,0.688285,"nits of text such as sentences (Lahiri, 2015). Exist1 We use special font to denote Heylighen and Dewaele’s F-score to avoid confusion with F1 measure. 62 ing statistical approaches to detecting formality (Abu Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012b) have treated the problem as a binary classification task and relied heavily on word lists to differentiate the two classes. Linguistics literature supports treating formality as a continuum (Irvine, 1979; Heylighen and Dewaele, 1999), as has been done in studies of other pragmatic dimensions such as politeness (Danescu-Niculescu-Mizil et al., 2013) and emotiveness (Walker et al., 2012). Lahiri et al. (2011) provided a preliminary investigation of annotating formality on an ordinal scale and released a dataset of sentence-level formality annotations (Lahiri, 2015), but did not use their data in any computational tasks. This paper extends prior work by (i) introducing a statistical regression model of formality which is based on an empirical analysis of human perceptions rather than on heuristics and (ii) by applying that model to a linguistic analysis of online discussions. 3 Human perceptions of formality Before we can automatically rec"
Q16-1005,Y09-1015,0,0.0264926,"stance and shared knowledge (Sigley, 1997; Hovy, 1987; Lahiri et al., 2011). Other recent work adopts a less abstract definition which is similar to the notion of “noisy text”– e.g. use of slang and poor grammar (Mosquera and Moreda, 2012a; Peterson et al., 2011). As a result, many rules have been explored for recognizing and generating informal language. Some of these rules are abstract, such as the level of implicature (Heylighen and Dewaele, 1999; Lahiri, 2015) or the degree of subjectivity (Mosquera and Moreda, 2012a), while others are much more concrete, such as the number of adjectives (Fang and Cao, 2009) or use of contractions (Abu Sheikha and Inkpen, 2011). Much prior work on detecting formality has focused on the lexical level (Brooke et al., 2010; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015). For larger units of text, perhaps the best-known method for measuring formality is the F-score1 (Heylighen and Dewaele, 1999), which is based on relative part-of-speech frequencies. F-score and its more recent variants (Li et al., 2013) provide a coarse measure of formality, but are designed to work at the genrelevel, making them less reliable for shorter units of text such as sentences (Lahiri,"
Q16-1005,N15-1185,0,0.0196514,"1979; Brown and Fraser, 1979). The formal-informal dimension has even been called the “most important dimension of variation between styles” (Heylighen and Dewaele, 1999). A speaker’s level of formality can reveal information about their familiarity with a person, opinions of a topic, and goals for an interaction (Hovy, 1987; Endrass et al., 2011). As a result, the ability to recognize formality is an integral part of dialogue systems (Mairesse, 2008; Mairesse and Walker, 2011; Battaglino and Bickmore, 2015), sociolinguistic analyses (Danescu-Niculescu-Mizil et al., 2012; Justo et al., 2014; Krishnan and Eisenstein, 2015), human-computer interaction (Johnson et al., 2005; Khosmood and Walker, 2010), summarization (Sidhaye and Cheung, 2015), and automatic writing assessment (Felice and Deane, 2012). Formality can also indicate contextindependent, universal statements (Heylighen and Dewaele, 1999), making formality detection relevant for tasks such as knowledge base population (Suh et al., 2006; Reiter and Frank, 2010) and textual entailment (Dagan et al., 2006). This paper investigates formality in online written communication. The contributions are as follows: 1) We provide an analysis of humans’ subjective pe"
Q16-1005,J11-3002,0,0.0303536,"have observed that it subsumes a range of dimensions of style including serious-trivial, polite-casual, and level of shared knowledge (Irvine, 1979; Brown and Fraser, 1979). The formal-informal dimension has even been called the “most important dimension of variation between styles” (Heylighen and Dewaele, 1999). A speaker’s level of formality can reveal information about their familiarity with a person, opinions of a topic, and goals for an interaction (Hovy, 1987; Endrass et al., 2011). As a result, the ability to recognize formality is an integral part of dialogue systems (Mairesse, 2008; Mairesse and Walker, 2011; Battaglino and Bickmore, 2015), sociolinguistic analyses (Danescu-Niculescu-Mizil et al., 2012; Justo et al., 2014; Krishnan and Eisenstein, 2015), human-computer interaction (Johnson et al., 2005; Khosmood and Walker, 2010), summarization (Sidhaye and Cheung, 2015), and automatic writing assessment (Felice and Deane, 2012). Formality can also indicate contextindependent, universal statements (Heylighen and Dewaele, 1999), making formality detection relevant for tasks such as knowledge base population (Suh et al., 2006; Reiter and Frank, 2010) and textual entailment (Dagan et al., 2006). Thi"
Q16-1005,N15-1023,1,0.847193,"g and poor grammar (Mosquera and Moreda, 2012a; Peterson et al., 2011). As a result, many rules have been explored for recognizing and generating informal language. Some of these rules are abstract, such as the level of implicature (Heylighen and Dewaele, 1999; Lahiri, 2015) or the degree of subjectivity (Mosquera and Moreda, 2012a), while others are much more concrete, such as the number of adjectives (Fang and Cao, 2009) or use of contractions (Abu Sheikha and Inkpen, 2011). Much prior work on detecting formality has focused on the lexical level (Brooke et al., 2010; Brooke and Hirst, 2014; Pavlick and Nenkova, 2015). For larger units of text, perhaps the best-known method for measuring formality is the F-score1 (Heylighen and Dewaele, 1999), which is based on relative part-of-speech frequencies. F-score and its more recent variants (Li et al., 2013) provide a coarse measure of formality, but are designed to work at the genrelevel, making them less reliable for shorter units of text such as sentences (Lahiri, 2015). Exist1 We use special font to denote Heylighen and Dewaele’s F-score to avoid confusion with F1 measure. 62 ing statistical approaches to detecting formality (Abu Sheikha and Inkpen, 2010; Pet"
Q16-1005,W11-0711,0,0.242466,"Missing"
Q16-1005,P10-1005,0,0.0369924,"gral part of dialogue systems (Mairesse, 2008; Mairesse and Walker, 2011; Battaglino and Bickmore, 2015), sociolinguistic analyses (Danescu-Niculescu-Mizil et al., 2012; Justo et al., 2014; Krishnan and Eisenstein, 2015), human-computer interaction (Johnson et al., 2005; Khosmood and Walker, 2010), summarization (Sidhaye and Cheung, 2015), and automatic writing assessment (Felice and Deane, 2012). Formality can also indicate contextindependent, universal statements (Heylighen and Dewaele, 1999), making formality detection relevant for tasks such as knowledge base population (Suh et al., 2006; Reiter and Frank, 2010) and textual entailment (Dagan et al., 2006). This paper investigates formality in online written communication. The contributions are as follows: 1) We provide an analysis of humans’ subjective perceptions of formality in four different genres. We highlight areas of high and low agreement and extract patterns that consis61 Transactions of the Association for Computational Linguistics, vol. 4, pp. 61–74, 2016. Action Editor: Janyce Wiebe and Kristina Toutanova. Submission batch: 10/2015; Revision batch: 12/2015; Published 3/2016. c 2016 Association for Computational Linguistics. Distributed un"
Q16-1005,D15-1014,0,0.0562636,"between styles” (Heylighen and Dewaele, 1999). A speaker’s level of formality can reveal information about their familiarity with a person, opinions of a topic, and goals for an interaction (Hovy, 1987; Endrass et al., 2011). As a result, the ability to recognize formality is an integral part of dialogue systems (Mairesse, 2008; Mairesse and Walker, 2011; Battaglino and Bickmore, 2015), sociolinguistic analyses (Danescu-Niculescu-Mizil et al., 2012; Justo et al., 2014; Krishnan and Eisenstein, 2015), human-computer interaction (Johnson et al., 2005; Khosmood and Walker, 2010), summarization (Sidhaye and Cheung, 2015), and automatic writing assessment (Felice and Deane, 2012). Formality can also indicate contextindependent, universal statements (Heylighen and Dewaele, 1999), making formality detection relevant for tasks such as knowledge base population (Suh et al., 2006; Reiter and Frank, 2010) and textual entailment (Dagan et al., 2006). This paper investigates formality in online written communication. The contributions are as follows: 1) We provide an analysis of humans’ subjective perceptions of formality in four different genres. We highlight areas of high and low agreement and extract patterns that"
Q16-1005,walker-etal-2012-corpus,0,0.155054,"e use special font to denote Heylighen and Dewaele’s F-score to avoid confusion with F1 measure. 62 ing statistical approaches to detecting formality (Abu Sheikha and Inkpen, 2010; Peterson et al., 2011; Mosquera and Moreda, 2012b) have treated the problem as a binary classification task and relied heavily on word lists to differentiate the two classes. Linguistics literature supports treating formality as a continuum (Irvine, 1979; Heylighen and Dewaele, 1999), as has been done in studies of other pragmatic dimensions such as politeness (Danescu-Niculescu-Mizil et al., 2013) and emotiveness (Walker et al., 2012). Lahiri et al. (2011) provided a preliminary investigation of annotating formality on an ordinal scale and released a dataset of sentence-level formality annotations (Lahiri, 2015), but did not use their data in any computational tasks. This paper extends prior work by (i) introducing a statistical regression model of formality which is based on an empirical analysis of human perceptions rather than on heuristics and (ii) by applying that model to a linguistic analysis of online discussions. 3 Human perceptions of formality Before we can automatically recognize formality, we need an understan"
Q16-1005,W15-0515,0,\N,Missing
Q16-1013,P15-1068,0,0.817081,"ion between them, if possible, would be very complex. Additionally, it is difficult for annotators to agree on error annotations, which complicates the annotation validity as a gold standard (Leacock et al., 2014). This is due to the nature of grammatical error correction, where there can be diverse correct edits for a sentence (Figure 1). In other words, there is no single gold-standard correction. The variety of error types and potential correct edits result in very low inter-annotator agreement (IAA), as reported in previous studies (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010; Bryant and Ng, 2015). This leads to a more fundamental question: why do we depend so much on fine-grained, lowconsensus error-type annotations as a gold standard for evaluating GEC systems? One answer is that error tags can be informative and useful to provide feedback to language learners, especially for specific closed-class error types fluency evaluation. As the development of the technology , social media becomes more and more significant role in the whole world . With the development of technology As the technology develops As technology develops plays a more and more significant role becomes more and more s"
Q16-1013,A00-2019,0,0.131062,"with our new annotation scheme has very strong correlation with expert rankings (ρ = 0.82). As a result, we advocate for a fundamental and necessary shift in the goal of GEC, from correcting small, labeled error types, to producing text that has native fluency. 1 Introduction What is the purpose of grammatical error correction (GEC)? One response is that GEC aims to help people become better writers by correcting grammatical mistakes in their writing. In the NLP community, the original scope of GEC was correcting targeted error types with the goal of providing feedback to non-native writers (Chodorow and Leacock, 2000; Dale and Kilgarriff, 2011; Leacock et al., 2014). As systems improved and more advanced methods were applied to the task, the definition evolved to wholesentence correction, or correcting all errors of every error type (Ng et al., 2014). With this pivot, we urge the community to revisit the original question. It is often the case that writing exhibits problems that are difficult to ascribe to specific grammatical categories. Consider the following example: Original: From this scope , social media has shorten our distance . Corrected: From this scope , social media has shortened our distance"
Q16-1013,C12-1038,1,0.764396,"mmatical errors. The ability to do this with the non-error-coded, fluent annotations we advocate here is no longer direct, but is not lost entirely. For this purpose, some recent studies have proposed post hoc automated errortype classification methods (Swanson and Yamangil, 2012; Xue and Hwa, 2014), which compare the original sentence to its correction and deduce the error types. We speculate that, by removing the error-coding restraint, we can obtain edits that sound more fluent to native speakers while also reducing the expense of annotation, with diminished time and training requirements. Chodorow et al. (2012) and Tetreault et al. (2014) suggested that it is better to have a large number of annotators to reduce bias in automatic evaluation. Following this recommendation, we collected additional annotations without error codes, written by both experts and non-experts. 5 It is important to note that both grammaticality and fluency are determined with respect to a particular speaker population and setting. In this paper, we focus on Standard Written English, which is the standard used in education, business, and journalism. While judgments of individual sentences would differ for other populations and"
Q16-1013,N12-1067,0,0.315246,"pora— and more of them—could be created. There has been some work exploring this, namely Tetreault and Chodorow (2008), which used a sampling approach that would only work for errors involving closed-class words. Pavlick et al. (2014) also describe preliminary work into designing an improved crowdsourcing interface to expedite data collection of coded errors. Section 3 outlines our annotation approach, which is faster and cheaper than previous approaches because it does not make use of error coding. 2.2 Evaluation practices Three evaluation metrics3 have been proposed for GEC: MaxMatch (M2 ) (Dahlmeier and Ng, 2012), I-measure (Felice and Briscoe, 2015), and GLEU (Napoles et al., 2015). The first two compare the changes made in the output to error-coded spans of the reference corrections. M2 was the metric used 3 Not including the metrics of the HOO shared tasks, which were precision, recall, and F-score. for the 2013 and 2014 CoNLL GEC shared tasks (Ng et al., 2013; Ng et al., 2014). It captures wordand phrase-level edits by building an edit lattice and calculating an F-score over the lattice. Felice and Briscoe (2015) note problems with M2 : specifically, it does not distinguish between a “do-nothing b"
Q16-1013,W13-1703,0,0.887528,"provide corrections to those spans for each error in the sentence. One of the main issues with coded annotation schemes is the difficulty of defining the granularity of error types. These sets of error tags are not easily interchangeable between different corpora. Specifically, two major GEC corpora have different taxonomies: the Cambridge Learner Corpus (CLC) (Nicholls, 2003) has 80 tags, which generally represent the word class of the error and the type of error (such as replace preposition, unnecessary pronoun, or missing determiner). In contrast, the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013) has only 27 error types. A direct conversion between them, if possible, would be very complex. Additionally, it is difficult for annotators to agree on error annotations, which complicates the annotation validity as a gold standard (Leacock et al., 2014). This is due to the nature of grammatical error correction, where there can be diverse correct edits for a sentence (Figure 1). In other words, there is no single gold-standard correction. The variety of error types and potential correct edits result in very low inter-annotator agreement (IAA), as reported in previous studies (Tetreault and C"
Q16-1013,W11-2838,0,0.199732,"eme has very strong correlation with expert rankings (ρ = 0.82). As a result, we advocate for a fundamental and necessary shift in the goal of GEC, from correcting small, labeled error types, to producing text that has native fluency. 1 Introduction What is the purpose of grammatical error correction (GEC)? One response is that GEC aims to help people become better writers by correcting grammatical mistakes in their writing. In the NLP community, the original scope of GEC was correcting targeted error types with the goal of providing feedback to non-native writers (Chodorow and Leacock, 2000; Dale and Kilgarriff, 2011; Leacock et al., 2014). As systems improved and more advanced methods were applied to the task, the definition evolved to wholesentence correction, or correcting all errors of every error type (Ng et al., 2014). With this pivot, we urge the community to revisit the original question. It is often the case that writing exhibits problems that are difficult to ascribe to specific grammatical categories. Consider the following example: Original: From this scope , social media has shorten our distance . Corrected: From this scope , social media has shortened our distance . If the goal is to correct"
Q16-1013,W12-2006,0,0.0226628,"terminers and prepositions). Indeed, the CLC, the first large-scale corpus of annotated grammatical errors, was coded specifically with the intent of gathering statistics about errors to inform the development of tools to help English language learners (Nicholls, 2003). Later GEC corpora adhered to the same error-coding template, if not the same error types (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). The first shared task in GEC aspired to the CLC’s same objective: to develop tools for language learners (Dale and Kilgarriff, 2011). Subsequent shared tasks (Dale et al., 2012; Ng et al., 2013) followed suit, targeting specific error types. Error-coded corpora are effective training and evaluation data for targeted error correction, and statistical classifiers have been developed to handle errors involving closed-class words (Rozovskaya and Roth, 2014). However, the 2014 CoNLL shared task engendered a sea change in GEC: in this shared task, systems needed to correct all errors in a sentence, of all error types, including ones more stylistic in nature (Ng et al., 2014). The evaluation metrics and annotated data from the previous shared task were used; however we arg"
Q16-1013,N15-1060,0,0.590262,"antly, as we will show in this paper, annotating for explicit error codes places a downward pressure on annotators to find and fix concrete, easily-identifiable grammatical errors (such as wrong verb tense) in lieu of addressing the native fluency of the text. A related problem is the presence of multiple evaluation metrics computed over error-annotated corpora. Recent work has shown that metrics like M2 and I-measure, both of which require errorcoded corpora, produce dramatically different results when used to score system output and produce a ranking of systems in conventional competitions (Felice and Briscoe, 2015). In light of all of this, we suggest that the GEC task has overlooked a fundamental question: What are the best practices for corpus annotation and system evaluation? This work attempts to answer this question. We show that native speakers prefer text that exhibits fluent sentences over ones that have only minimal grammatical corrections. We explore different methods for corpus annotation (with and without error codes, written by experts and non-experts) and different evaluation metrics to determine which configuration of annotated corpus and metric has the strongest correlation with the huma"
Q16-1013,D15-1052,0,0.201953,"14). The evaluation metrics and annotated data from the previous shared task were used; however we argue that they do not align with the use case of this reframed task. What is the use case of whole-sentence correction? It should not be to provide specific targeted feedback on error types, but rather to rewrite sentences as a proofreader would. The community has already begun to view wholesentence correction as a task, with the yet unstated goal of improving the overall fluency of sentences. Independent papers published human evaluations of the shared task system output (Napoles et al., 2015; Grundkiewicz et al., 2015), asking judges to rank systems based on their grammaticality. As GEC moves toward correcting an entire sentence instead of targeted error types, the myriad acceptable edits will result in much lower IAA, compromising evaluation metrics based on the precision and recall of 171 coded errors. At this juncture, it is crucial that we examine whether error-coded corpora and evaluation are necessary for this new direction of GEC. Finally, it would be remiss not to address the cost and time of corpus annotation. Tetreault and Chodorow (2008) noted that it would take 80 hours to correct 1,000 preposit"
Q16-1013,2012.iwslt-papers.5,0,0.0181853,"r knowledge, this is the first time that the interplay of annotation scheme and evaluation metric, as well as the rater expertise, has been evaluated jointly for GEC. 4.1 Table 5: Human ranking of the new annotations by grammaticality. Lines between systems indicate clusters according to bootstrap resampling at p ≤ 0.05. Systems in the same cluster are considered to be tied. tems. By running TrueSkill 1,000 times using bootstrap resampling and producing a system ranking each time, we collect a range of ranks for each system. We can then cluster systems according to nonoverlapping rank ranges (Koehn, 2012) to produce the final ranking, allowing ties. Table 5 shows the ranking of “grammatical” judgments for the additional annotations and the original NUCLE annotations. While the score of the expert fluency edits is higher than the non-expert fluency, they are within the same cluster, suggesting that the judges perceived them to be just as good. The fluency rewrites by both experts and nonexperts are clearly preferable over the minimal edit corrections, although the error-coded NUCLE corrections are perceived as more grammatical than the minimal corrections. 4 Automatic metrics We have demonstrat"
Q16-1013,P11-2089,1,0.931933,"Missing"
Q16-1013,P07-1044,0,0.0326389,"annotation and evaluation. As we will show, the two areas are intimately related. Fundamentally, this work reframes grammatical error correction as a fluency task. Our proposed evaluation framework produces system rankings with strong to very strong correlations with human judgments (Spearman’s ρ = 0.82, Pearson’s r = 0.73), using a variation of the GLEU metric (Napoles et al., 2015)2 and two sets of “fluent” sen1 All the scripts and new data we collected are available at https://github.com/keisks/reassess-gec. 2 This metric should not be confused with the method of the same name presented in Mutton et al. (2007) for sentence-level 170 tence rewrites as a gold standard, which are simpler and cheaper to collect than previous annotations. 2 Current issues in GEC In this section, we will address issues of the GEC task, reviewing previous work with respect to error annotation and evaluation metrics. 2.1 Annotation methodologies Existing corpora for GEC are annotated for errors using fine-grained coding schemes. To create error-coded corpora, trained annotators must identify spans of text containing an error, assign codes corresponding to the error type, and provide corrections to those spans for each erro"
Q16-1013,P15-2097,1,0.127014,"rrelation with the human ranking. In so doing, we establish a reliable and replicable evaluation procedure to help further the advancement of GEC methods.1 To date, this is the only work to undertake a comprehensive empirical study of annotation and evaluation. As we will show, the two areas are intimately related. Fundamentally, this work reframes grammatical error correction as a fluency task. Our proposed evaluation framework produces system rankings with strong to very strong correlations with human judgments (Spearman’s ρ = 0.82, Pearson’s r = 0.73), using a variation of the GLEU metric (Napoles et al., 2015)2 and two sets of “fluent” sen1 All the scripts and new data we collected are available at https://github.com/keisks/reassess-gec. 2 This metric should not be confused with the method of the same name presented in Mutton et al. (2007) for sentence-level 170 tence rewrites as a gold standard, which are simpler and cheaper to collect than previous annotations. 2 Current issues in GEC In this section, we will address issues of the GEC task, reviewing previous work with respect to error annotation and evaluation metrics. 2.1 Annotation methodologies Existing corpora for GEC are annotated for error"
Q16-1013,W13-3601,1,0.930945,"sitions). Indeed, the CLC, the first large-scale corpus of annotated grammatical errors, was coded specifically with the intent of gathering statistics about errors to inform the development of tools to help English language learners (Nicholls, 2003). Later GEC corpora adhered to the same error-coding template, if not the same error types (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). The first shared task in GEC aspired to the CLC’s same objective: to develop tools for language learners (Dale and Kilgarriff, 2011). Subsequent shared tasks (Dale et al., 2012; Ng et al., 2013) followed suit, targeting specific error types. Error-coded corpora are effective training and evaluation data for targeted error correction, and statistical classifiers have been developed to handle errors involving closed-class words (Rozovskaya and Roth, 2014). However, the 2014 CoNLL shared task engendered a sea change in GEC: in this shared task, systems needed to correct all errors in a sentence, of all error types, including ones more stylistic in nature (Ng et al., 2014). The evaluation metrics and annotated data from the previous shared task were used; however we argue that they do no"
Q16-1013,W14-1701,0,0.739859,"ative fluency. 1 Introduction What is the purpose of grammatical error correction (GEC)? One response is that GEC aims to help people become better writers by correcting grammatical mistakes in their writing. In the NLP community, the original scope of GEC was correcting targeted error types with the goal of providing feedback to non-native writers (Chodorow and Leacock, 2000; Dale and Kilgarriff, 2011; Leacock et al., 2014). As systems improved and more advanced methods were applied to the task, the definition evolved to wholesentence correction, or correcting all errors of every error type (Ng et al., 2014). With this pivot, we urge the community to revisit the original question. It is often the case that writing exhibits problems that are difficult to ascribe to specific grammatical categories. Consider the following example: Original: From this scope , social media has shorten our distance . Corrected: From this scope , social media has shortened our distance . If the goal is to correct verb errors, the grammatical mistake in the original sentence has been addressed and we can move on. However, when we aim to correct the sentence as a whole, a more vexing problem remains. The more prominent er"
Q16-1013,P02-1040,0,0.105581,"nd Briscoe (2015) note problems with M2 : specifically, it does not distinguish between a “do-nothing baseline” and systems that only propose wrong corrections; also, phrase-level edits can be easily gamed because the lattice treats the deletion of a long phrase as a single edit. To address these issues, they propose I-measure, which generates a token-level alignment between the source sentence, system output, and gold-standard sentences, and then computes accuracy based on the alignment. Unlike these approaches, GLEU does not use error-coded references4 (Napoles et al., 2015). Based on BLEU (Papineni et al., 2002), it computes n-gram precision of the system output against reference sentences. GLEU additionally penalizes text in the output that was unchanged from the source but changed in the reference sentences. Recent work by Napoles et al. (2015) and Grundkiewicz et al. (2015) evaluated these metrics against human evaluations obtained using methods borrowed from the Workshop on Statistical Machine Translation (Bojar et al., 2014). Both papers found a moderate to strong correlation with human judgments for GLEU and M2 , and a slightly negative correlation for I-measure. Importantly, however, none of t"
Q16-1013,W10-1004,0,0.0643602,"ror types. A direct conversion between them, if possible, would be very complex. Additionally, it is difficult for annotators to agree on error annotations, which complicates the annotation validity as a gold standard (Leacock et al., 2014). This is due to the nature of grammatical error correction, where there can be diverse correct edits for a sentence (Figure 1). In other words, there is no single gold-standard correction. The variety of error types and potential correct edits result in very low inter-annotator agreement (IAA), as reported in previous studies (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010; Bryant and Ng, 2015). This leads to a more fundamental question: why do we depend so much on fine-grained, lowconsensus error-type annotations as a gold standard for evaluating GEC systems? One answer is that error tags can be informative and useful to provide feedback to language learners, especially for specific closed-class error types fluency evaluation. As the development of the technology , social media becomes more and more significant role in the whole world . With the development of technology As the technology develops As technology develops plays a more and more significant role b"
Q16-1013,Q14-1033,0,0.0593669,"ater GEC corpora adhered to the same error-coding template, if not the same error types (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). The first shared task in GEC aspired to the CLC’s same objective: to develop tools for language learners (Dale and Kilgarriff, 2011). Subsequent shared tasks (Dale et al., 2012; Ng et al., 2013) followed suit, targeting specific error types. Error-coded corpora are effective training and evaluation data for targeted error correction, and statistical classifiers have been developed to handle errors involving closed-class words (Rozovskaya and Roth, 2014). However, the 2014 CoNLL shared task engendered a sea change in GEC: in this shared task, systems needed to correct all errors in a sentence, of all error types, including ones more stylistic in nature (Ng et al., 2014). The evaluation metrics and annotated data from the previous shared task were used; however we argue that they do not align with the use case of this reframed task. What is the use case of whole-sentence correction? It should not be to provide specific targeted feedback on error types, but rather to rewrite sentences as a proofreader would. The community has already begun to v"
Q16-1013,W14-3301,1,0.231647,"Missing"
Q16-1013,2006.amta-papers.25,0,0.0758344,"-experts taking more liberties than experts with both the number of sentences changed and the degree of change within each sentence (see Table 2 for an extreme example of this phenomenon). In order to quantify the extent of changes made in the different annotations, we look at the percent of sentences that were left unchanged as well as the number of changes needed to transform the original sentence into the corrected annotation. To calculate the number of changes, we used a modified Translation Edit Rate (TER), which measures the number of edits needed to transform one sentence into another (Snover et al., 2006). An edit can be an insertion, deletion, substitution, or shift. We chose this metric because it counts the movement of a phrase (a shift) as one change, which the Levenshtein distance would heavily penalize. TER is calculated as the number of changes per token, but instead we report the number of changes per sentence for ease of interpretation, which we call the sTER. We compare the original set of sentences to the new annotations and the existing NUCLE and BN15 reference sets to determine the relative extent of changes made by the fluency and minimal edits (Figure 2). Compared to the origina"
Q16-1013,N12-1037,0,0.0690572,"address overlapping and interacting errors. For example, the annotators of the NUCLE corpus, which was used for the recent shared tasks, were explicitly instructed to select the minimal text span of possible alternatives (Dahlmeier et al., 2013). There are situations where error-coded annotations are useful to help students correct specific grammatical errors. The ability to do this with the non-error-coded, fluent annotations we advocate here is no longer direct, but is not lost entirely. For this purpose, some recent studies have proposed post hoc automated errortype classification methods (Swanson and Yamangil, 2012; Xue and Hwa, 2014), which compare the original sentence to its correction and deduce the error types. We speculate that, by removing the error-coding restraint, we can obtain edits that sound more fluent to native speakers while also reducing the expense of annotation, with diminished time and training requirements. Chodorow et al. (2012) and Tetreault et al. (2014) suggested that it is better to have a large number of annotators to reduce bias in automatic evaluation. Following this recommendation, we collected additional annotations without error codes, written by both experts and non-expe"
Q16-1013,W08-1205,1,0.952379,"r et al., 2013) has only 27 error types. A direct conversion between them, if possible, would be very complex. Additionally, it is difficult for annotators to agree on error annotations, which complicates the annotation validity as a gold standard (Leacock et al., 2014). This is due to the nature of grammatical error correction, where there can be diverse correct edits for a sentence (Figure 1). In other words, there is no single gold-standard correction. The variety of error types and potential correct edits result in very low inter-annotator agreement (IAA), as reported in previous studies (Tetreault and Chodorow, 2008; Rozovskaya and Roth, 2010; Bryant and Ng, 2015). This leads to a more fundamental question: why do we depend so much on fine-grained, lowconsensus error-type annotations as a gold standard for evaluating GEC systems? One answer is that error tags can be informative and useful to provide feedback to language learners, especially for specific closed-class error types fluency evaluation. As the development of the technology , social media becomes more and more significant role in the whole world . With the development of technology As the technology develops As technology develops plays a more"
Q16-1013,W10-1006,1,0.882862,"Missing"
Q16-1013,P14-2098,0,0.0349226,"racting errors. For example, the annotators of the NUCLE corpus, which was used for the recent shared tasks, were explicitly instructed to select the minimal text span of possible alternatives (Dahlmeier et al., 2013). There are situations where error-coded annotations are useful to help students correct specific grammatical errors. The ability to do this with the non-error-coded, fluent annotations we advocate here is no longer direct, but is not lost entirely. For this purpose, some recent studies have proposed post hoc automated errortype classification methods (Swanson and Yamangil, 2012; Xue and Hwa, 2014), which compare the original sentence to its correction and deduce the error types. We speculate that, by removing the error-coding restraint, we can obtain edits that sound more fluent to native speakers while also reducing the expense of annotation, with diminished time and training requirements. Chodorow et al. (2012) and Tetreault et al. (2014) suggested that it is better to have a large number of annotators to reduce bias in automatic evaluation. Following this recommendation, we collected additional annotations without error codes, written by both experts and non-experts. 5 It is importa"
Q16-1013,P11-1019,0,0.0837216,"e technology develops As technology develops plays a more and more significant role becomes more and more significant world Figure 1: An ungrammatical sentence that can be corrected in different ways. (such as determiners and prepositions). Indeed, the CLC, the first large-scale corpus of annotated grammatical errors, was coded specifically with the intent of gathering statistics about errors to inform the development of tools to help English language learners (Nicholls, 2003). Later GEC corpora adhered to the same error-coding template, if not the same error types (Rozovskaya and Roth, 2010; Yannakoudakis et al., 2011; Dahlmeier et al., 2013). The first shared task in GEC aspired to the CLC’s same objective: to develop tools for language learners (Dale and Kilgarriff, 2011). Subsequent shared tasks (Dale et al., 2012; Ng et al., 2013) followed suit, targeting specific error types. Error-coded corpora are effective training and evaluation data for targeted error correction, and statistical classifiers have been developed to handle errors involving closed-class words (Rozovskaya and Roth, 2014). However, the 2014 CoNLL shared task engendered a sea change in GEC: in this shared task, systems needed to correct"
Q19-1032,W17-4755,0,0.0314663,"Missing"
Q19-1032,D18-1274,0,0.0297013,"Missing"
Q19-1032,W16-2302,0,0.0324677,"Missing"
Q19-1032,C18-1231,0,0.194947,"s false negatives. I-measure calculates the weighted accuracy of correction and detection, indicating how much better or worse a candidate system is than the original text (Felice and Briscoe, 2015). ERRANT is a rule-based error-type classifier (Bryant et al., 2017) that can be used as an evaluation metric by calculating the F -score of changes in a candidate text compared to a reference. It is the metric proposed for the 2019 shared task in GEC (Bryant et al., 2019). Other efforts have focused on reference-less metrics (Napoles et al., 2016b; Choshen and Abend, 2018b) and quality estimation (Chollampatt and Ng, 2018b). Related Work This paper represents an exploration of several components of GEC: corpora, metrics, and metaevaluation. We summarize work in these areas. 2.1 Corpora Prior work on evaluating GEC systems was performed on text written by primarily non-native English speakers, focusing on student essays by English language learners (ELLs). The NUS Corpus of Learner English (NUCLE) comprises essays written by mostly Chinese native speakers (Dahlmeier et al., 2013), and was the data set for the 2013 and 2014 CoNLL Shared Tasks in GEC (Ng et al., 2013, 2014). After the 2014 Shared Task, 16 additio"
Q19-1032,W19-4406,0,0.177108,"Missing"
Q19-1032,P18-1127,0,0.0569683,"sion of BLEU (Papineni et al., 2002) that penalizes false negatives. I-measure calculates the weighted accuracy of correction and detection, indicating how much better or worse a candidate system is than the original text (Felice and Briscoe, 2015). ERRANT is a rule-based error-type classifier (Bryant et al., 2017) that can be used as an evaluation metric by calculating the F -score of changes in a candidate text compared to a reference. It is the metric proposed for the 2019 shared task in GEC (Bryant et al., 2019). Other efforts have focused on reference-less metrics (Napoles et al., 2016b; Choshen and Abend, 2018b) and quality estimation (Chollampatt and Ng, 2018b). Related Work This paper represents an exploration of several components of GEC: corpora, metrics, and metaevaluation. We summarize work in these areas. 2.1 Corpora Prior work on evaluating GEC systems was performed on text written by primarily non-native English speakers, focusing on student essays by English language learners (ELLs). The NUS Corpus of Learner English (NUCLE) comprises essays written by mostly Chinese native speakers (Dahlmeier et al., 2013), and was the data set for the 2013 and 2014 CoNLL Shared Tasks in GEC (Ng et al.,"
Q19-1032,P17-1074,0,0.0574901,"n the candidate and the reference sentences (Dahlmeier and Ng, 2012). This was the official metric of the 2013 and 2014 Shared Tasks. The General Language Evaluation Understanding (GLEU) metric captures fluency rewrites in addition to grammatical corrections (Napoles et al., 2015, 2016a). It is an extension of BLEU (Papineni et al., 2002) that penalizes false negatives. I-measure calculates the weighted accuracy of correction and detection, indicating how much better or worse a candidate system is than the original text (Felice and Briscoe, 2015). ERRANT is a rule-based error-type classifier (Bryant et al., 2017) that can be used as an evaluation metric by calculating the F -score of changes in a candidate text compared to a reference. It is the metric proposed for the 2019 shared task in GEC (Bryant et al., 2019). Other efforts have focused on reference-less metrics (Napoles et al., 2016b; Choshen and Abend, 2018b) and quality estimation (Chollampatt and Ng, 2018b). Related Work This paper represents an exploration of several components of GEC: corpora, metrics, and metaevaluation. We summarize work in these areas. 2.1 Corpora Prior work on evaluating GEC systems was performed on text written by prim"
Q19-1032,N18-2020,0,0.0320618,"Missing"
Q19-1032,P15-1068,0,0.121301,"l components of GEC: corpora, metrics, and metaevaluation. We summarize work in these areas. 2.1 Corpora Prior work on evaluating GEC systems was performed on text written by primarily non-native English speakers, focusing on student essays by English language learners (ELLs). The NUS Corpus of Learner English (NUCLE) comprises essays written by mostly Chinese native speakers (Dahlmeier et al., 2013), and was the data set for the 2013 and 2014 CoNLL Shared Tasks in GEC (Ng et al., 2013, 2014). After the 2014 Shared Task, 16 additional references were released for that test set, 8 from each of Bryant and Ng (2015) and Sakaguchi et al. (2016). The Cambridge Learner Corpus First Certificate in English (FCE) data set includes essays for the B2 qualification exams (Yannakoudakis et al., 2011). The Johns Hopkins Fluency-Extended GUG corpus (JFLEG) contains text from the TOEFL exam (originally collected in the GUG corpus [Heilman et al., 2014]), with fluency corrections. Fluency corrections are rewrites needed to make a text sound natural to a native English speaker (Sakaguchi et al., 2016), in contrast to only making minimal corrections to grammatical errors as in FCE and NUCLE (Napoles et al., 2017). The A"
Q19-1032,N12-1067,0,0.0371937,"). In metrics, we use these judgments to analyze how automatic evaluation metrics fare. We describe the existing metrics examined in this study (§5.1) and propose a new ensemble metric GMEG-Metric (§5.2). Finally, we evaluate and analyze metric performance on GMEG-Data and on the CoNLL-2014 Shared Task test set (§6). 2 Table 1: Summary of existing GEC test sets (size in number of sentences). 2.2 Metrics The most commonly used automatic GEC metrics are MaxMatch (M2 ) and GLEU. M2 reports the F -score of edits over the optimal phrasal alignment between the candidate and the reference sentences (Dahlmeier and Ng, 2012). This was the official metric of the 2013 and 2014 Shared Tasks. The General Language Evaluation Understanding (GLEU) metric captures fluency rewrites in addition to grammatical corrections (Napoles et al., 2015, 2016a). It is an extension of BLEU (Papineni et al., 2002) that penalizes false negatives. I-measure calculates the weighted accuracy of correction and detection, indicating how much better or worse a candidate system is than the original text (Felice and Briscoe, 2015). ERRANT is a rule-based error-type classifier (Bryant et al., 2017) that can be used as an evaluation metric by cal"
Q19-1032,W07-0718,0,0.0604377,"es have been proposed for human evaluation of system outputs. For this task, we use a hybrid approach that combines judgments on a continuous scale with relative ranking (partial ranking with scalars or PRWS). PRWS was advocated for in EASL (Sakaguchi and Van Durme, 2018) and RankME (Novikova et al., 2018), two recent works that investigated reliable methods for collecting human ratings of competing systems’ outputs. Those studies both found PRWS to be more reliable than the direct assessment framework used in WMT (Bojar et al., 2016), the relative-ranking methodology formerly applied by WMT (Callison-Burch et al., 2007), and earlier GEC human evaluation work (Grundkiewicz et al., 2015; Napoles et al., 2015). Unlike relative ranking, PRWS does not explicitly ask raters to rank the sentences, although a ranking can be inferred from the relative scores. Raters collected human ratings of 13 system outputs from CoNLL-14 and calculated the correlation between automatic metric scores and those human judgments. This section describes assembling ground-truth human judgments of GMEG-Data. We first outline the GEC systems included and then describe how we conduct human evaluation of the system outputs. 4.1 GEC Systems"
Q19-1032,W13-1703,0,0.405383,"al text by native English speakers, and a diverse set of student writing. Introduction Grammatical error correction (GEC) systems are evaluated with automatic metrics that compare their output to gold-standard corrections from reference corpora. Having automatic metrics that correlate well with human judgments allows rapid system development and reliable evaluations across the field. Although the GEC community has benefited from several evaluation sets over the past several years, they are primarily composed of student essays written by non-native English speakers (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Napoles et al., 2017; Bryant et al., 2019). As of yet, we do not know how well GEC systems do on other domains of text or how reliable automatic evaluation is when we move to other domains. We tackle this issue head-on by creating a new test set for GEC that represents diverse domains • The output of 6 GEC systems with varying modern architectures and training data sizes, and human judgments of the entire set of system outputs. • Evaluation of 4 standard GEC metrics and a leading machine translation (MT) metric on the new test set. • GMEG-Metric: A new ensemble metric for GEC that is robust"
Q19-1032,W08-0309,0,0.0596727,") shared task test set (Daudaravicius et al., 2016) has text from scientific publications written by proficient non-native and native English speakers, but is not widely used. We report the number of sentences and reference corrections for these four data sets in Table 1. 2.3 Meta-evaluation of Metrics Since 2006, the Workshop (now Conference) on Machine Translation (WMT) has conducted largescale human evaluation of MT systems for its annual shared task (Koehn and Monz, 2006). The parallel Metrics track is a shared task for automatic MT metrics, evaluating performance against human judgments (Callison-Burch et al., 2008). Researchers in GEC have adopted this practice following the CoNLL-2014 Shared Task on Grammatical Error Correction (henceforth CoNLL-14) (Ng et al., 2014), for which all results are publicly available, including the references and 13 552 system outputs. Grundkiewicz et al. (2015) and Napoles et al. (2015) simultaneously performed a human evaluation of the system outputs inspired by WMT. Heretofore, all work in GEC evaluation has been conducted on this data. Grunkdkiewicz et al. and Napoles et al. calculated the correlation of the human scores with M2 , I-measure, and BLEU, finding that M2 mo"
Q19-1032,W16-0506,1,0.877117,"icate in English (FCE) data set includes essays for the B2 qualification exams (Yannakoudakis et al., 2011). The Johns Hopkins Fluency-Extended GUG corpus (JFLEG) contains text from the TOEFL exam (originally collected in the GUG corpus [Heilman et al., 2014]), with fluency corrections. Fluency corrections are rewrites needed to make a text sound natural to a native English speaker (Sakaguchi et al., 2016), in contrast to only making minimal corrections to grammatical errors as in FCE and NUCLE (Napoles et al., 2017). The Automatic Evaluation of Scientific Writing (AESW) shared task test set (Daudaravicius et al., 2016) has text from scientific publications written by proficient non-native and native English speakers, but is not widely used. We report the number of sentences and reference corrections for these four data sets in Table 1. 2.3 Meta-evaluation of Metrics Since 2006, the Workshop (now Conference) on Machine Translation (WMT) has conducted largescale human evaluation of MT systems for its annual shared task (Koehn and Monz, 2006). The parallel Metrics track is a shared task for automatic MT metrics, evaluating performance against human judgments (Callison-Burch et al., 2008). Researchers in GEC ha"
Q19-1032,N15-1060,0,0.02211,"2 reports the F -score of edits over the optimal phrasal alignment between the candidate and the reference sentences (Dahlmeier and Ng, 2012). This was the official metric of the 2013 and 2014 Shared Tasks. The General Language Evaluation Understanding (GLEU) metric captures fluency rewrites in addition to grammatical corrections (Napoles et al., 2015, 2016a). It is an extension of BLEU (Papineni et al., 2002) that penalizes false negatives. I-measure calculates the weighted accuracy of correction and detection, indicating how much better or worse a candidate system is than the original text (Felice and Briscoe, 2015). ERRANT is a rule-based error-type classifier (Bryant et al., 2017) that can be used as an evaluation metric by calculating the F -score of changes in a candidate text compared to a reference. It is the metric proposed for the 2019 shared task in GEC (Bryant et al., 2019). Other efforts have focused on reference-less metrics (Napoles et al., 2016b; Choshen and Abend, 2018b) and quality estimation (Chollampatt and Ng, 2018b). Related Work This paper represents an exploration of several components of GEC: corpora, metrics, and metaevaluation. We summarize work in these areas. 2.1 Corpora Prior"
Q19-1032,D15-1052,0,0.129338,"aluation of Metrics Since 2006, the Workshop (now Conference) on Machine Translation (WMT) has conducted largescale human evaluation of MT systems for its annual shared task (Koehn and Monz, 2006). The parallel Metrics track is a shared task for automatic MT metrics, evaluating performance against human judgments (Callison-Burch et al., 2008). Researchers in GEC have adopted this practice following the CoNLL-2014 Shared Task on Grammatical Error Correction (henceforth CoNLL-14) (Ng et al., 2014), for which all results are publicly available, including the references and 13 552 system outputs. Grundkiewicz et al. (2015) and Napoles et al. (2015) simultaneously performed a human evaluation of the system outputs inspired by WMT. Heretofore, all work in GEC evaluation has been conducted on this data. Grunkdkiewicz et al. and Napoles et al. calculated the correlation of the human scores with M2 , I-measure, and BLEU, finding that M2 moderately correlated with human judgments, I-measure had very weak negative correlation, and BLEU negatively correlated. Sakaguchi et al. (2016) analyzed the combination of available reference sets and metrics to identify the best evaluation configuration. Later, Chollampatt and Ng"
Q19-1032,P14-2029,1,0.832405,"essays written by mostly Chinese native speakers (Dahlmeier et al., 2013), and was the data set for the 2013 and 2014 CoNLL Shared Tasks in GEC (Ng et al., 2013, 2014). After the 2014 Shared Task, 16 additional references were released for that test set, 8 from each of Bryant and Ng (2015) and Sakaguchi et al. (2016). The Cambridge Learner Corpus First Certificate in English (FCE) data set includes essays for the B2 qualification exams (Yannakoudakis et al., 2011). The Johns Hopkins Fluency-Extended GUG corpus (JFLEG) contains text from the TOEFL exam (originally collected in the GUG corpus [Heilman et al., 2014]), with fluency corrections. Fluency corrections are rewrites needed to make a text sound natural to a native English speaker (Sakaguchi et al., 2016), in contrast to only making minimal corrections to grammatical errors as in FCE and NUCLE (Napoles et al., 2017). The Automatic Evaluation of Scientific Writing (AESW) shared task test set (Daudaravicius et al., 2016) has text from scientific publications written by proficient non-native and native English speakers, but is not widely used. We report the number of sentences and reference corrections for these four data sets in Table 1. 2.3 Meta-"
Q19-1032,D16-1161,0,0.0155348,"tputs. 4.1 GEC Systems We select 6 GEC systems that differ in 3 aspects: the type of system (statistical or neural), the neural network architecture, and the amount of data used for training. We give only a brief overview of the systems below (summarized in Table 5) because the goal of this paper is not to identify the best GEC system, and furthermore the systems are trained on a combination of public data sets and propriety data. AMU A statistical MT model trained using a modified version of the Moses toolkit (Koehn et al., 2007). We use the pre-trained model published in Junczys-Dowmunt and Grundkiewicz (2016). LSTM A RNN-based sequence-to-sequence neural network with bi-directional encoder and LSTM units, trained with the OpenNMT-py toolkit (Klein et al., 2018). We train a second high-recall system using the same architecture, but changing the data sampling strategy (called LSTM-R). Marian A RNN-based sequence-to-sequence neural network with deep-transition architecture (Barone et al., 2017) trained with the Marian toolkit (Junczys-Dowmunt et al., 2018). We use the WMT17 system parameters (Sennrich et al., 2017), excluding ensembles and left-to-right re-ranking. 8 https://github.com/pytorch/fairse"
Q19-1032,D14-1020,0,0.0487557,"Missing"
Q19-1032,N15-1124,0,0.0588459,"Missing"
Q19-1032,W18-1817,0,0.0250994,"he amount of data used for training. We give only a brief overview of the systems below (summarized in Table 5) because the goal of this paper is not to identify the best GEC system, and furthermore the systems are trained on a combination of public data sets and propriety data. AMU A statistical MT model trained using a modified version of the Moses toolkit (Koehn et al., 2007). We use the pre-trained model published in Junczys-Dowmunt and Grundkiewicz (2016). LSTM A RNN-based sequence-to-sequence neural network with bi-directional encoder and LSTM units, trained with the OpenNMT-py toolkit (Klein et al., 2018). We train a second high-recall system using the same architecture, but changing the data sampling strategy (called LSTM-R). Marian A RNN-based sequence-to-sequence neural network with deep-transition architecture (Barone et al., 2017) trained with the Marian toolkit (Junczys-Dowmunt et al., 2018). We use the WMT17 system parameters (Sennrich et al., 2017), excluding ensembles and left-to-right re-ranking. 8 https://github.com/pytorch/fairseq/ tree/master/examples/translation 9 These sentences and an explanation of the rule-based error-insertion method are included with the released data set."
Q19-1032,N16-1001,0,0.0461465,"Missing"
Q19-1032,P07-2045,0,0.00622959,"systems included and then describe how we conduct human evaluation of the system outputs. 4.1 GEC Systems We select 6 GEC systems that differ in 3 aspects: the type of system (statistical or neural), the neural network architecture, and the amount of data used for training. We give only a brief overview of the systems below (summarized in Table 5) because the goal of this paper is not to identify the best GEC system, and furthermore the systems are trained on a combination of public data sets and propriety data. AMU A statistical MT model trained using a modified version of the Moses toolkit (Koehn et al., 2007). We use the pre-trained model published in Junczys-Dowmunt and Grundkiewicz (2016). LSTM A RNN-based sequence-to-sequence neural network with bi-directional encoder and LSTM units, trained with the OpenNMT-py toolkit (Klein et al., 2018). We train a second high-recall system using the same architecture, but changing the data sampling strategy (called LSTM-R). Marian A RNN-based sequence-to-sequence neural network with deep-transition architecture (Barone et al., 2017) trained with the Marian toolkit (Junczys-Dowmunt et al., 2018). We use the WMT17 system parameters (Sennrich et al., 2017), ex"
Q19-1032,W06-3114,0,0.0482221,"ng minimal corrections to grammatical errors as in FCE and NUCLE (Napoles et al., 2017). The Automatic Evaluation of Scientific Writing (AESW) shared task test set (Daudaravicius et al., 2016) has text from scientific publications written by proficient non-native and native English speakers, but is not widely used. We report the number of sentences and reference corrections for these four data sets in Table 1. 2.3 Meta-evaluation of Metrics Since 2006, the Workshop (now Conference) on Machine Translation (WMT) has conducted largescale human evaluation of MT systems for its annual shared task (Koehn and Monz, 2006). The parallel Metrics track is a shared task for automatic MT metrics, evaluating performance against human judgments (Callison-Burch et al., 2008). Researchers in GEC have adopted this practice following the CoNLL-2014 Shared Task on Grammatical Error Correction (henceforth CoNLL-14) (Ng et al., 2014), for which all results are publicly available, including the references and 13 552 system outputs. Grundkiewicz et al. (2015) and Napoles et al. (2015) simultaneously performed a human evaluation of the system outputs inspired by WMT. Heretofore, all work in GEC evaluation has been conducted on"
Q19-1032,W14-1701,0,0.105202,"y used. We report the number of sentences and reference corrections for these four data sets in Table 1. 2.3 Meta-evaluation of Metrics Since 2006, the Workshop (now Conference) on Machine Translation (WMT) has conducted largescale human evaluation of MT systems for its annual shared task (Koehn and Monz, 2006). The parallel Metrics track is a shared task for automatic MT metrics, evaluating performance against human judgments (Callison-Burch et al., 2008). Researchers in GEC have adopted this practice following the CoNLL-2014 Shared Task on Grammatical Error Correction (henceforth CoNLL-14) (Ng et al., 2014), for which all results are publicly available, including the references and 13 552 system outputs. Grundkiewicz et al. (2015) and Napoles et al. (2015) simultaneously performed a human evaluation of the system outputs inspired by WMT. Heretofore, all work in GEC evaluation has been conducted on this data. Grunkdkiewicz et al. and Napoles et al. calculated the correlation of the human scores with M2 , I-measure, and BLEU, finding that M2 moderately correlated with human judgments, I-measure had very weak negative correlation, and BLEU negatively correlated. Sakaguchi et al. (2016) analyzed the"
Q19-1032,W18-6450,0,0.0230148,"Missing"
Q19-1032,W13-3601,1,0.822888,"Abend, 2018b) and quality estimation (Chollampatt and Ng, 2018b). Related Work This paper represents an exploration of several components of GEC: corpora, metrics, and metaevaluation. We summarize work in these areas. 2.1 Corpora Prior work on evaluating GEC systems was performed on text written by primarily non-native English speakers, focusing on student essays by English language learners (ELLs). The NUS Corpus of Learner English (NUCLE) comprises essays written by mostly Chinese native speakers (Dahlmeier et al., 2013), and was the data set for the 2013 and 2014 CoNLL Shared Tasks in GEC (Ng et al., 2013, 2014). After the 2014 Shared Task, 16 additional references were released for that test set, 8 from each of Bryant and Ng (2015) and Sakaguchi et al. (2016). The Cambridge Learner Corpus First Certificate in English (FCE) data set includes essays for the B2 qualification exams (Yannakoudakis et al., 2011). The Johns Hopkins Fluency-Extended GUG corpus (JFLEG) contains text from the TOEFL exam (originally collected in the GUG corpus [Heilman et al., 2014]), with fluency corrections. Fluency corrections are rewrites needed to make a text sound natural to a native English speaker (Sakaguchi et"
Q19-1032,W17-4768,0,0.0329136,"Missing"
Q19-1032,N18-2012,0,0.0555963,"Missing"
Q19-1032,W13-2202,0,0.0268357,"t the annotated data in half, so that there are approximately 1,000 sentences from each domain for training/development and 1,000 6 System-level Correlation with Human Judgments Each metric is evaluated on 3k sentences in the test split. We report the Pearson correlation and the Spearman rank coefficients between the system scores predicted by each metric and the ground-truth scores from the human rating. The Spearman coefficient is appropriate for differentiating between two systems, although it too harshly penalizes metrics that change the order of systems with similar performance (Mach´acˇ ek and Bojar, 2013), so we also report the Pearson correlation. Because we only have a small number of systems, and the correlation can change based on the inclusion or exclusion of one additional data point, we include the artificial systems (§5.2) for more robust correlation calculations. The upperand lower-bounds (Reference and Source+error) could artificially inflate the correlation values and are therefore not included in any of the correlation calculations. The results on the test set for all metrics and domains are in Table 8. Following Graham and Baldwin (2014), we apply the Williams statistical test12 o"
Q19-1032,P02-1040,0,0.11471,"on the CoNLL-2014 Shared Task test set (§6). 2 Table 1: Summary of existing GEC test sets (size in number of sentences). 2.2 Metrics The most commonly used automatic GEC metrics are MaxMatch (M2 ) and GLEU. M2 reports the F -score of edits over the optimal phrasal alignment between the candidate and the reference sentences (Dahlmeier and Ng, 2012). This was the official metric of the 2013 and 2014 Shared Tasks. The General Language Evaluation Understanding (GLEU) metric captures fluency rewrites in addition to grammatical corrections (Napoles et al., 2015, 2016a). It is an extension of BLEU (Papineni et al., 2002) that penalizes false negatives. I-measure calculates the weighted accuracy of correction and detection, indicating how much better or worse a candidate system is than the original text (Felice and Briscoe, 2015). ERRANT is a rule-based error-type classifier (Bryant et al., 2017) that can be used as an evaluation metric by calculating the F -score of changes in a candidate text compared to a reference. It is the metric proposed for the 2019 shared task in GEC (Bryant et al., 2019). Other efforts have focused on reference-less metrics (Napoles et al., 2016b; Choshen and Abend, 2018b) and qualit"
Q19-1032,P15-2097,1,0.865957,"e evaluate and analyze metric performance on GMEG-Data and on the CoNLL-2014 Shared Task test set (§6). 2 Table 1: Summary of existing GEC test sets (size in number of sentences). 2.2 Metrics The most commonly used automatic GEC metrics are MaxMatch (M2 ) and GLEU. M2 reports the F -score of edits over the optimal phrasal alignment between the candidate and the reference sentences (Dahlmeier and Ng, 2012). This was the official metric of the 2013 and 2014 Shared Tasks. The General Language Evaluation Understanding (GLEU) metric captures fluency rewrites in addition to grammatical corrections (Napoles et al., 2015, 2016a). It is an extension of BLEU (Papineni et al., 2002) that penalizes false negatives. I-measure calculates the weighted accuracy of correction and detection, indicating how much better or worse a candidate system is than the original text (Felice and Briscoe, 2015). ERRANT is a rule-based error-type classifier (Bryant et al., 2017) that can be used as an evaluation metric by calculating the F -score of changes in a candidate text compared to a reference. It is the metric proposed for the 2019 shared task in GEC (Bryant et al., 2019). Other efforts have focused on reference-less metrics"
Q19-1032,D16-1228,1,0.950971,"2016a). It is an extension of BLEU (Papineni et al., 2002) that penalizes false negatives. I-measure calculates the weighted accuracy of correction and detection, indicating how much better or worse a candidate system is than the original text (Felice and Briscoe, 2015). ERRANT is a rule-based error-type classifier (Bryant et al., 2017) that can be used as an evaluation metric by calculating the F -score of changes in a candidate text compared to a reference. It is the metric proposed for the 2019 shared task in GEC (Bryant et al., 2019). Other efforts have focused on reference-less metrics (Napoles et al., 2016b; Choshen and Abend, 2018b) and quality estimation (Chollampatt and Ng, 2018b). Related Work This paper represents an exploration of several components of GEC: corpora, metrics, and metaevaluation. We summarize work in these areas. 2.1 Corpora Prior work on evaluating GEC systems was performed on text written by primarily non-native English speakers, focusing on student essays by English language learners (ELLs). The NUS Corpus of Learner English (NUCLE) comprises essays written by mostly Chinese native speakers (Dahlmeier et al., 2013), and was the data set for the 2013 and 2014 CoNLL Shared"
Q19-1032,W17-4770,0,0.0320834,"Missing"
Q19-1032,E17-2037,1,0.940418,"h speakers, and a diverse set of student writing. Introduction Grammatical error correction (GEC) systems are evaluated with automatic metrics that compare their output to gold-standard corrections from reference corpora. Having automatic metrics that correlate well with human judgments allows rapid system development and reliable evaluations across the field. Although the GEC community has benefited from several evaluation sets over the past several years, they are primarily composed of student essays written by non-native English speakers (Yannakoudakis et al., 2011; Dahlmeier et al., 2013; Napoles et al., 2017; Bryant et al., 2019). As of yet, we do not know how well GEC systems do on other domains of text or how reliable automatic evaluation is when we move to other domains. We tackle this issue head-on by creating a new test set for GEC that represents diverse domains • The output of 6 GEC systems with varying modern architectures and training data sizes, and human judgments of the entire set of system outputs. • Evaluation of 4 standard GEC metrics and a leading machine translation (MT) metric on the new test set. • GMEG-Metric: A new ensemble metric for GEC that is robust across all three domai"
Q19-1032,Q16-1013,1,0.93075,"ora, metrics, and metaevaluation. We summarize work in these areas. 2.1 Corpora Prior work on evaluating GEC systems was performed on text written by primarily non-native English speakers, focusing on student essays by English language learners (ELLs). The NUS Corpus of Learner English (NUCLE) comprises essays written by mostly Chinese native speakers (Dahlmeier et al., 2013), and was the data set for the 2013 and 2014 CoNLL Shared Tasks in GEC (Ng et al., 2013, 2014). After the 2014 Shared Task, 16 additional references were released for that test set, 8 from each of Bryant and Ng (2015) and Sakaguchi et al. (2016). The Cambridge Learner Corpus First Certificate in English (FCE) data set includes essays for the B2 qualification exams (Yannakoudakis et al., 2011). The Johns Hopkins Fluency-Extended GUG corpus (JFLEG) contains text from the TOEFL exam (originally collected in the GUG corpus [Heilman et al., 2014]), with fluency corrections. Fluency corrections are rewrites needed to make a text sound natural to a native English speaker (Sakaguchi et al., 2016), in contrast to only making minimal corrections to grammatical errors as in FCE and NUCLE (Napoles et al., 2017). The Automatic Evaluation of Scien"
Q19-1032,P18-1020,0,0.0303722,"Missing"
Q19-1032,W14-3354,0,0.0583843,"Missing"
Q19-1032,W15-3053,0,0.0599878,"Missing"
Q19-1032,W16-0511,0,\N,Missing
swift-etal-2004-semi,brants-plaehn-2000-interactive,0,\N,Missing
swift-etal-2004-semi,J93-2004,0,\N,Missing
swift-etal-2004-semi,A00-2008,0,\N,Missing
swift-etal-2004-semi,H94-1020,0,\N,Missing
W04-0214,swift-etal-2004-semi,1,\N,Missing
W04-0214,J98-2001,0,\N,Missing
W04-0214,J96-3006,0,\N,Missing
W04-0214,brants-plaehn-2000-interactive,0,\N,Missing
W04-0214,J93-2004,0,\N,Missing
W04-0214,W98-1119,0,\N,Missing
W04-0214,J01-4003,1,\N,Missing
W04-0214,P02-1011,0,\N,Missing
W04-0214,J86-3001,0,\N,Missing
W04-0214,poesio-2000-annotating,0,\N,Missing
W04-0304,H92-1026,0,0.0243884,"phrases. Experiments with incremental feedback from a referAdvisor (reference) Inform Feedback Mediator Inform Client (parser) Modify Chart Figure 1: A General Architecture for Incremental Parsing ence resolution module and an NP suitability oracle are reported, and the ability of the implementation to incrementally instantiate semantically underspecified pronouns is outlined. We believe this research provides an important start towards developing endto-end continuous understanding models. 2 An Incremental Parsing Architecture Many current parsers fall into the class of historybased grammars (Black et al., 1992). The independence assumptions of these models make the parsing problem both stochastically and computationally tractable, but represent a simplification and may therefore be a source of error. In a continuous understanding framework, higher-level modules may have additional information that suggests loci for improvement, recognizing either invalid independence assumptions or errors in the underlying probability model. We have designed a general incremental parsing architecture (Figure 1) in which the Client, a dynamic programming parser, performs its calculations, the results of which are inc"
W04-0304,C98-1028,0,0.0112875,"s of natural human speech. Continuous understanding is necessary if the system is to respond before the entire utterance is analyzed, a prerequisite for incremental confirmation and clarification. The major computational advantage of continuous understanding models is that high-level expectations and feedback should be able to influence the search of lowerlevel processes, thus leading to a focused search through hypotheses that are plausible at all levels of processing. One of the major current applications of parsers that operate incrementally is for language modelling in speech recognition (Brill et al., 1998; Jelinek and Chelba, 1999). This work is important not only for its ability to improve performance on the speech recognition task; it also models the interactions between speech recognition and parsing in a continuous understanding system. Our research attempts to further the quest for continuous understanding by moving one step up the hierarchy, building an incremental parser which is the advisee rather than the advisor. We begin by presenting a general architecture for incremental interaction between the parser and higher-level modules, and then discuss a specific instantiation of this gene"
W04-0304,P93-1008,0,0.0222601,"n-path phenomenon and parsing preferences (Altmann and Steedman, 1988; Konieczny, 1996; Phillips, 1996). Moreover, a variety of eye-tracking experiments (Cooper, 1974; Tanenhaus and Spivey, 1996; Allopenna et al., 1998; Sedivy et al., 1999) suggest that complex semantic and referential constraints are incorporated on an incremental basis in human parsing decisions. Computational parsers, however, still tend to operate an entire sentence at a time, despite the advent of speech-to-intention dialogue systems such as Verbmobil (Kasper et al., 1996; Noth et al., 2000; Pinkal et al., 2000), Gemini (Dowding et al., 1993; Dowding et al., 1994; Moore et al., 1995) and TRIPS (Allen et al., 1996; Ferguson et al., 1996; Ferguson and Allen, 1998). Naturalness, robustness, and interactivity are goals of such systems, but control flow is typically the sequential execution of modules, each operating on the output of its predecessor; only after the entire sentence has been parsed do higher-level modules such as intention recognition and reference resolution get involved. In contrast to this sequential model is the continuous understanding approach, in which all levels of language analysis occur simultaneously, from sp"
W04-0304,P94-1016,0,0.0209813,"parsing preferences (Altmann and Steedman, 1988; Konieczny, 1996; Phillips, 1996). Moreover, a variety of eye-tracking experiments (Cooper, 1974; Tanenhaus and Spivey, 1996; Allopenna et al., 1998; Sedivy et al., 1999) suggest that complex semantic and referential constraints are incorporated on an incremental basis in human parsing decisions. Computational parsers, however, still tend to operate an entire sentence at a time, despite the advent of speech-to-intention dialogue systems such as Verbmobil (Kasper et al., 1996; Noth et al., 2000; Pinkal et al., 2000), Gemini (Dowding et al., 1993; Dowding et al., 1994; Moore et al., 1995) and TRIPS (Allen et al., 1996; Ferguson et al., 1996; Ferguson and Allen, 1998). Naturalness, robustness, and interactivity are goals of such systems, but control flow is typically the sequential execution of modules, each operating on the output of its predecessor; only after the entire sentence has been parsed do higher-level modules such as intention recognition and reference resolution get involved. In contrast to this sequential model is the continuous understanding approach, in which all levels of language analysis occur simultaneously, from speech recognition to in"
W04-0304,A00-2041,0,0.0542565,"ault, underspecified pronoun, and so cannot apply these restrictions to discriminate between referents. Our implementation performs the semantic vetting discussed above, but we have done no largescale experiments in this area. 6 Related Work There are instances in the literature of incremental parsers that pass forward information to higher-level modules, but none, to our knowledge, are designed as continuous understanding systems, where all levels of language analysis occur (virtually) simultaneously. For example, there are a number of robust semantic processing systems (Pinkal et al., 2000; Rose, 2000; Worm, 1998; Zechner, 1998) which contain incremental parsers that pass on partial results immediately to the robust semantic analysis component, which begins to work on combining these sentence fragments. If the parser cannot find a parse, then the semantic analysis program has already done at least part of its work. However, none of the above systems have a feedback loop between the semantic analysis component and the incremental parser. So, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models. Schuler (2002) describes a parser w"
W04-0304,C02-1024,0,0.103995,"ms (Pinkal et al., 2000; Rose, 2000; Worm, 1998; Zechner, 1998) which contain incremental parsers that pass on partial results immediately to the robust semantic analysis component, which begins to work on combining these sentence fragments. If the parser cannot find a parse, then the semantic analysis program has already done at least part of its work. However, none of the above systems have a feedback loop between the semantic analysis component and the incremental parser. So, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models. Schuler (2002) describes a parser which builds both a syntactic tree and a denotation-based semantic analysis as it parses. The denotations of constituents in the environment are used to inform parsing decisions, much as we use the static database of place names. However, the feedback in our system is richer, based on the context provided by the preceding discourse. Furthermore, as an instantiation of the general architecture presented in Section 2, our system is more easily extensible to other forms of feedback. 7 Future Work There is a catch-22 in that the accurate reference information necessary to impro"
W04-0304,W04-0214,1,0.799355,"hat a noun phrase constituent c would be in the final parse, conditioned on R and N , or Pr = p(c in final parse|R, N ). This probability was then linearly combined with the parser’s constituent probability, n Pp = p(c → wm ), according to the equation P (c) = (1 − λ) · Pp + λ · Pr for various values of λ. Evaluation using held-out data suggested that a value of λ = 0.2 would be optimal. This style of feedback is an example of chart subversion, as it is a direct modification of constituent probabilities by the Mediator, defining a new probability distribution. 4 Experiments The Monroe domain (Tetreault et al., 2004; Stent, 2001) is a series of task-oriented dialogues between human participants set in a simulated rescue operation domain, where participants collaboratively plan responses to emergency calls. Dialogues were recorded, broken up into utterances, and then transcribed by hand, removing speech repairs from the parser input. These transcriptions served as input for all experiments reported below. A probabilistic grammar was trained from supervised data, assigning PCFG probabilities for the rule expansions in the CFG backbone of the handcrafted, semantically constrained grammar. The parser was run"
W04-0304,P98-2229,0,0.0290917,"pecified pronoun, and so cannot apply these restrictions to discriminate between referents. Our implementation performs the semantic vetting discussed above, but we have done no largescale experiments in this area. 6 Related Work There are instances in the literature of incremental parsers that pass forward information to higher-level modules, but none, to our knowledge, are designed as continuous understanding systems, where all levels of language analysis occur (virtually) simultaneously. For example, there are a number of robust semantic processing systems (Pinkal et al., 2000; Rose, 2000; Worm, 1998; Zechner, 1998) which contain incremental parsers that pass on partial results immediately to the robust semantic analysis component, which begins to work on combining these sentence fragments. If the parser cannot find a parse, then the semantic analysis program has already done at least part of its work. However, none of the above systems have a feedback loop between the semantic analysis component and the incremental parser. So, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models. Schuler (2002) describes a parser which builds"
W04-0304,P98-2236,0,0.199507,"noun, and so cannot apply these restrictions to discriminate between referents. Our implementation performs the semantic vetting discussed above, but we have done no largescale experiments in this area. 6 Related Work There are instances in the literature of incremental parsers that pass forward information to higher-level modules, but none, to our knowledge, are designed as continuous understanding systems, where all levels of language analysis occur (virtually) simultaneously. For example, there are a number of robust semantic processing systems (Pinkal et al., 2000; Rose, 2000; Worm, 1998; Zechner, 1998) which contain incremental parsers that pass on partial results immediately to the robust semantic analysis component, which begins to work on combining these sentence fragments. If the parser cannot find a parse, then the semantic analysis program has already done at least part of its work. However, none of the above systems have a feedback loop between the semantic analysis component and the incremental parser. So, while all of these are in some sense examples of incremental parsing, they are not continuous understanding models. Schuler (2002) describes a parser which builds both a syntactic"
W04-0304,H93-1008,0,\N,Missing
W04-0304,P98-1028,0,\N,Missing
W04-0304,C98-2231,0,\N,Missing
W04-0304,P93-1005,0,\N,Missing
W04-0304,C98-2224,0,\N,Missing
W07-1604,P03-2026,0,0.608351,"Missing"
W07-1604,izumi-etal-2004-overview,0,0.173212,"Missing"
W07-1604,Y04-1032,0,0.0517139,"Missing"
W08-1205,P96-1042,0,0.06125,"FPs in the sample from the “Error” sub-corpus. For the hypothetical data in Figure 1, these values are 600/750 = 0.80 for Hits, and 150/750 = 0.20 for FPs. Calculate the proportion of Misses in the sample from the “OK” subcorpus. For the hypothetical data, this is 450/1500 = 0.30 for Misses. Estimated Overall Rates Sample Proportion * Sub-Corpus Proportion 0.80 * 0.10 = 0.08 0.20 * 0.10 = 0.02 0.30 * 0.90 = 0.27 0.08/(0.08 + 0.02) = 0.80 0.08/(0.08 + 0.27) = 0.23 Table 5: Sampling Calculations (Hypothetical) This method is similar in spirit to active learning ((Dagan and Engelson, 1995) and (Engelson and Dagan, 1996)), which has been used to iteratively build up an annotated corpus, but it differs from active learning applications in that there are no iterative loops between the system and the human annotator(s). In addition, while our methodology is used for evaluating a system, active learning is commonly used for training a system. 6. The values computed in step 5 are conditional proportions based on the sub-corpora. To calculate the overall proportions in the test corpus, it is necessary to multiply each value by the relative size of its sub-corpus. This is shown in Table 5, where the proportion of Hi"
W08-1205,W07-1607,0,0.161831,"Missing"
W08-1205,I08-1059,0,0.506964,"notator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1 . Although there are several learner cor• Sampling Approach Multiple annotation can be very costly and time-consuming, which may explain why previous work employed only one rater. As an alternative to the standard exhaustive annotation, we propose a sampling approach in which estimates of the rates of hits, false positives, and misses are derived from random samples of the system’s output, and then precision and recall of the system can be calculated. We show that estimates of system performance derived c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unp"
W08-1205,izumi-etal-2004-overview,0,0.436773,"ments in rating preposition usage. While one tends to think of annotator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1 . Although there are several learner cor• Sampling Approach Multiple annotation can be very costly and time-consuming, which may explain why previous work employed only one rater. As an alternative to the standard exhaustive annotation, we propose a sampling approach in which estimates of the rates of hits, false positives, and misses are derived from random samples of the system’s output, and then precision and recall of the system can be calculated. We show that estimates of system performance derived c 2008. Licensed under t"
W08-1205,P06-1031,0,0.082225,"ne tends to think of annotator disagreements about discourse and semantics as being quite common, our studies show that judgments of preposition usage, which is largely lexically driven, can be just as contentious. As a result, this unreliability poses a serious issue for the development and evaluation of NLP tools in the task of automatically detecting preposition usage errors in the writing of non-native speakers of English. To date, single human annotation has typically been the gold standard for grammatical error detection, such as in the work of (Izumi et al., 2004), (Han et al., 2006), (Nagata et al., 2006), (Gamon et al., 2008)1 . Although there are several learner cor• Sampling Approach Multiple annotation can be very costly and time-consuming, which may explain why previous work employed only one rater. As an alternative to the standard exhaustive annotation, we propose a sampling approach in which estimates of the rates of hits, false positives, and misses are derived from random samples of the system’s output, and then precision and recall of the system can be calculated. We show that estimates of system performance derived c 2008. Licensed under the Creative Commons Attribution-Noncommerci"
W08-1205,A00-2019,1,0.851772,"Missing"
W08-1205,W07-1604,1,0.675429,"Missing"
W08-1205,C08-1109,1,0.46915,"ns, determiners and collocations. In the work discussed here, we target preposition usage errors, specifically those of incorrect selection (“we arrived to the station”) and 3 Automatically Detecting Preposition Usage Errors In this section, we give a description of our system and compare its performance to other systems. Although the focus of this paper is on human judgments in the task of error detection, we describe our system to show that variability in human judgments can impact the evaluation of a system in this task. A full description of our system and its performance can be found in (Tetreault and Chodorow, 2008). 3.1 System Our approach treats preposition error detection as a classification problem: that is, given a context of two words before and two words after the writer’s preposition, what is the best preposition to use? 4 There is a third error type, omission (“we are fond null beer”), that is a topic for our future research. 25 Prep in for of on to with at by as from about An error is marked when the system’s suggestion differs from the writer’s by a certain threshold amount. We have used a maximum entropy (ME) classifier (Ratnaparkhi, 1998) to select the most probable preposition for a given c"
W08-1205,P03-2026,0,\N,Missing
W09-3010,N04-2006,1,0.842388,"non-native writing is difficult because of the possibility of multiple correct answers and the variability in human agreement. This paper seeks to improve the best practice of such evaluation by analyzing the frequency of multiple correct answers and identifying factors that influence agreement levels in judging the usage of articles and noun number. 1 Introduction In recent years, systems have been developed with the long-term goal of detecting, in the writing of non-native speakers, usage errors involving articles, prepositions and noun number (Knight and Chander, 1994; Minnen et al., 2000; Lee, 2004; Han et al., 2005; Peng and Araki, 2005; Brockett et al., 2006; Turner and Charniak, 2007). These systems should, ideally, be evaluated on a corpus of learners’ writing, annotated with acceptable corrections. However, since such corpora are expensive to compile, many researchers have instead resorted to measuring the accuracy of predicting what a native writer originally wrote in well-formed text. This type of evaluation effectively makes the assumption that there is one correct form of native usage per context, which may not always be the case. Two studies have already challenged this “singl"
W09-3010,W00-0708,0,0.300286,"Missing"
W09-3010,H94-1048,0,0.0215294,"ld be annotated. Using this annotation scheme, however, raises two questions that have not yet been thoroughly researched: (1) what is the human agreement level on such annotation? (2) what factors might influence the agreement level? In this paper, we consider two factors: the context of a word, and the variability of its usage. In the two studies cited above, the human judges were shown only the target sentence and did not take into account any constraint on the choice of word that might be imposed by the larger context. For PP attachment, human performance improves when given more context (Ratnaparkhi et al., 1994). For other linguistic phenomena, such as article/number selection for nouns, a larger context window of at least several sentences may be required, even though some automatic methods for exploiting context have not been shown to boost performance (Han et al., 2005). The second factor, variability of usage, may be 60 Proceedings of the Third Linguistic Annotation Workshop, ACL-IJCNLP 2009, pages 60–63, c Suntec, Singapore, 6-7 August 2009. 2009 ACL and AFNLP null Three years ago John Small, a sheep farmer in the Mendip Hills, read an editorial in his local newspaper which claimed that foxes ne"
W09-3010,N07-2045,0,0.134007,"Missing"
W09-3010,W08-1205,1,\N,Missing
W09-3010,P06-1032,0,\N,Missing
W10-1006,D09-1030,0,0.0747838,"Missing"
W10-1006,I08-1059,0,0.0420656,"but at a fraction of the time and cost. Based on these results, we propose a new evaluation method which makes it feasible to compare two error detection systems tested on different learner data sets. 1 Introduction The last few years have seen an explosion in the development of NLP tools to detect and correct errors made by learners of English as a Second Language (ESL). While there has been considerable emphasis placed on the system development aspect of the field, with researchers tackling some of the toughest ESL errors such as those involving articles (Han et al., 2006) and prepositions (Gamon et al., 2008), (Felice and Pullman, 2009), there has been a woeful lack of attention paid to developing best practices for annotation and evaluation. Annotation in the field of ESL error detection has typically relied on just one trained rater, and that rater’s judgments then become the gold standard for evaluating a system. So it is very rare that inter-rater reliability is reported, although, in other NLP subfields, reporting reliability is the norm. Time and cost are probably the two most important reasons why past work has relied on only one rater because using multiple annotators on the same ESL texts"
W10-1006,D08-1027,0,0.245786,"Missing"
W10-1006,C08-1109,1,0.85996,"ts of what is acceptable. One way to address this is by aggregating a multitude of judgments for each preposition and treating this as the gold standard, however such a tactic has been impractical due to time and cost limitations. While annotation is a problem in this field, comparing one system to another has also been a major issue. To date, none of the preposition and article error detection systems in the literature have been evaluated on the same corpus. This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–48, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics • In terms of cost and time, AMT is an effective alternative to trained raters on the tasks of preposition selection in well-formed text and preposition error annotation in ESL text. • With AMT, it is possible to efficiently collect multiple judgments for a target construction."
W10-1006,W08-1205,1,0.867989,"ts of what is acceptable. One way to address this is by aggregating a multitude of judgments for each preposition and treating this as the gold standard, however such a tactic has been impractical due to time and cost limitations. While annotation is a problem in this field, comparing one system to another has also been a major issue. To date, none of the preposition and article error detection systems in the literature have been evaluated on the same corpus. This is mostly due to the fact that learner corpora are difficult to acquire (and then annotate), but also to the fact that they are 1 (Tetreault and Chodorow, 2008b) report that it would take 80hrs for one of their trained raters to find and mark 1,000 preposition errors. Proceedings of the NAACL HLT 2010 Fifth Workshop on Innovative Use of NLP for Building Educational Applications, pages 45–48, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics • In terms of cost and time, AMT is an effective alternative to trained raters on the tasks of preposition selection in well-formed text and preposition error annotation in ESL text. • With AMT, it is possible to efficiently collect multiple judgments for a target construction."
W11-2111,P07-1111,0,0.0171842,"tions using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations and derive the regression features from them. Gamon et al. (2005) build a classifier to distinguish machine-generated translations from human 108 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108–115, c"
W11-2111,P07-1038,0,0.0169553,"tions using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations and derive the regression features from them. Gamon et al. (2005) build a classifier to distinguish machine-generated translations from human 108 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108–115, c"
W11-2111,W10-1703,0,0.313827,"ver et al. (2009) were used to produce scores tuned for fluency and adequacy. 3. METEOR (Lavie and Denkowski, 2009): Meteor scores were produced using Meteor-next v1.2. All types of matches were allowed (exact, stem, synonym and paraphrase) and scores tuned specifically to rank, HTER and adequacy were produced using the “-t” flag in the tool. We also implemented features closely related to or inspired by other MT metrics. The set of these auxiliary features is referred to as “Aux”. 1. Character-level statistics: Based on the success of the i-letter-BLEU and i-letter-recall metrics from WMT10 (Callison-Burch et al., 2010), we added the harmonic mean of precision (or recall) for character n-grams (from 1 to 10) as features. 2. Raw n-gram matches: We calculated the precision and precision for word n-grams (up to n=6) and added each as a separate feature (for a total of 12). Although these statistics are also calculated as part of the MT metrics above, breaking them into separate features gives the model more information. 3. Length ratios: The ratio between the lengths of the MT output and the reference translation was calculated on a character level and a word level. These ratios were also calculated between the"
W11-2111,A00-2019,1,0.607784,"ary automated essay scoring system developed by Educational Testing Service (ETS) to assess writing quality.1 The system has been used operationally for over 10 years in highstakes exams such as the GRE and TOEFL given its speed, reliability and high agreement with human raters. E-rater combines 8 main features using linear regression to produce a numerical score for an essay. These features are grammar, usage, mechanics, style, organization, development, lexical complexity and vocabulary usage. The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors (Chodorow and Leacock, 2000). The usage feature detects errors related to articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al., 2008). The mechanics feature checks for spelling, punctuation and capitalization errors. The style feature checks for passive constructions and word repetition, among others. Organization and development tabulate the presence or absence of discourse elements and the length of each element. Finally, the lexical complexity feature details how complex the writer’s words are based on frequency indices and writing scales, and the vocabulary feature"
W11-2111,P01-1020,0,0.105398,"compare translation hypotheses to a set of human-authored reference translations. However, there has also been some work on methods that are not dependent on human-authored translations. One subset of such methods is task-based in that the methods determine the quality of a translation in terms of how well it serves the need of an extrinsic task. These tasks can either be downstream NLP Besides extrinsic evaluation, there is another set of methods that attempt to “learn” what makes a good translation and then predict the quality of new translations without comparing to reference translations. Corston-Oliver et al. (2001) proposed the idea of building a decision tree classifier to simply distinguish between machine and human translations using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidenc"
W11-2111,W08-0331,0,0.0463365,"cs above, breaking them into separate features gives the model more information. 3. Length ratios: The ratio between the lengths of the MT output and the reference translation was calculated on a character level and a word level. These ratios were also calculated between the MT output and the source sentence. 4. OOV heuristic: The percentage of tokens in the MT that match the source sentence. This is a low-precision heuristic for counting out of vocabulary (OOV) words, since it also counts named entities and words that happen to be the same in different languages. 4.3 Ranking Model Following (Duh, 2008), we represent sentence-level MT evaluation as a ranking problem. For a particular source sentence, there are N machine translations and one reference translation. A feature vector is extracted from each {source, reference, MT} tuple. The training data consists of sets of translations that have been annotated with relative ranks. During training, all ranked sets are converted to sets of feature vectors, where the label for each feature vector is the rank. The ranking model is a linear SVM that predicts a relative score for each feature vector, and is implemented by SVM-rank (Joachims, 2006). W"
W11-2111,2005.eamt-1.15,0,0.151528,"r it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations and derive the regression features from them. Gamon et al. (2005) build a classifier to distinguish machine-generated translations from human 108 Proceedings of the 6th Workshop on Statistical Machine Translation, pages 108–115, c Edinburgh, Scotland, UK, July 30–31, 2011. 2011 Association for Computational Linguistics ones using fluency-based features and show that by combining the scores of this classifier with LM perplexities, they obtain an MT metric that has good correlation with human judgments but not better than the baseline BLEU metric. The fundamental questions that inspired our proposed metrics are as follows: • Can an operational English-profici"
W11-2111,N07-2020,0,0.0667457,"Missing"
W11-2111,2004.tmi-1.8,0,0.0272496,"of such methods is task-based in that the methods determine the quality of a translation in terms of how well it serves the need of an extrinsic task. These tasks can either be downstream NLP Besides extrinsic evaluation, there is another set of methods that attempt to “learn” what makes a good translation and then predict the quality of new translations without comparing to reference translations. Corston-Oliver et al. (2001) proposed the idea of building a decision tree classifier to simply distinguish between machine and human translations using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study"
W11-2111,P02-1040,0,0.0878901,"se the MTeRater-Plus meta-metric that uses e-rater features plus all of the hybrid features described below. Both metrics were trained on the same data using the same machine learning model, and differ only in their feature sets. 4.1 E-rater Features Each sentence is associated with an e-rater sentencelevel vector and a document-level vector as previously described and each column in these vectors was used a feature. 4.2 Features for Hybrid Models We used existing automatic MT metrics as baselines in our evaluation, and also as features in our hybrid metric. The metrics we used were: 1. BLEU (Papineni et al., 2002): Case-insensitive and case-sensitive BLEU scores were produced using mteval-v13a.pl, which calculates smoothed sentence-level scores. 2. TERp (Snover et al., 2009): Translation Edit Rate plus (TERp) scores were produced using terp v1. The scores were case-insensitive and edit costs from Snover et al. (2009) were used to produce scores tuned for fluency and adequacy. 3. METEOR (Lavie and Denkowski, 2009): Meteor scores were produced using Meteor-next v1.2. All types of matches were allowed (exact, stem, synonym and paraphrase) and scores tuned specifically to rank, HTER and adequacy were produ"
W11-2111,quirk-2004-training,0,0.0527501,"ct the quality of new translations without comparing to reference translations. Corston-Oliver et al. (2001) proposed the idea of building a decision tree classifier to simply distinguish between machine and human translations using language model (LM) and syntactic features. Kulesza and Shieber (2004) attempt the same task using an support vector machine (SVM) classifier and features derived from reference-based MT metrics such as WER, PER, BLEU and NIST. They also claim that the confidence score for the classifier being used, if available, may be taken as an estimate of translation quality. Quirk (2004) took a different approach and examined whether it is possible to explicitly compute a confidence measure for each translated sentence by using features derived from both the source and target language sides. Albrecht and Hwa (2007a) expanded on this idea and conducted a larger scale study to show the viability of regression as a sentence-level metric of MT quality. They used features derived from several other reference-driven MT metrics. In other work (Albrecht and Hwa, 2007b), they showed that one could substitute translations from other MT systems for human-authored reference translations"
W11-2111,W09-0441,1,0.84083,"machine learning model, and differ only in their feature sets. 4.1 E-rater Features Each sentence is associated with an e-rater sentencelevel vector and a document-level vector as previously described and each column in these vectors was used a feature. 4.2 Features for Hybrid Models We used existing automatic MT metrics as baselines in our evaluation, and also as features in our hybrid metric. The metrics we used were: 1. BLEU (Papineni et al., 2002): Case-insensitive and case-sensitive BLEU scores were produced using mteval-v13a.pl, which calculates smoothed sentence-level scores. 2. TERp (Snover et al., 2009): Translation Edit Rate plus (TERp) scores were produced using terp v1. The scores were case-insensitive and edit costs from Snover et al. (2009) were used to produce scores tuned for fluency and adequacy. 3. METEOR (Lavie and Denkowski, 2009): Meteor scores were produced using Meteor-next v1.2. All types of matches were allowed (exact, stem, synonym and paraphrase) and scores tuned specifically to rank, HTER and adequacy were produced using the “-t” flag in the tool. We also implemented features closely related to or inspired by other MT metrics. The set of these auxiliary features is referre"
W11-2111,C08-1109,1,0.871697,"stem has been used operationally for over 10 years in highstakes exams such as the GRE and TOEFL given its speed, reliability and high agreement with human raters. E-rater combines 8 main features using linear regression to produce a numerical score for an essay. These features are grammar, usage, mechanics, style, organization, development, lexical complexity and vocabulary usage. The grammar feature covers errors such as sentence fragments, verb form errors and pronoun errors (Chodorow and Leacock, 2000). The usage feature detects errors related to articles (Han et al., 2006), prepositions (Tetreault and Chodorow, 2008) and collocations (Futagi et al., 2008). The mechanics feature checks for spelling, punctuation and capitalization errors. The style feature checks for passive constructions and word repetition, among others. Organization and development tabulate the presence or absence of discourse elements and the length of each element. Finally, the lexical complexity feature details how complex the writer’s words are based on frequency indices and writing scales, and the vocabulary feature evaluates how appropriate the words are for the given topic). Since many of the features are essay-specific, there is"
W11-2111,vilar-etal-2006-error,0,0.0565763,"Missing"
W11-2111,P09-1048,1,\N,Missing
W12-2005,N03-1003,0,0.0574673,"; Rosti et al., 2007; Feng et al., 2009). However, our combination approach is different in that all the round-trip translations are produced by a single system but via different pivot languages. Finally, the idea of combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases. However, they do not combine the candidate paraphrases in any way. A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). 3 Methodology The basic"
W12-2005,P06-1032,0,0.470893,"Missing"
W12-2005,D09-1115,0,0.0341623,"Missing"
W12-2005,W09-2110,0,0.28295,"Missing"
W12-2005,J10-3003,1,0.826535,"ould simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases. However, they do not combine the candidate paraphrases in any way. A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). 3 Methodology The basic idea underlying our error correction technique is quite simple: if we can automatically generate alternative surface renderings of the meaning expressed in the original sentence and then pick the one that is most fluent, we are likely to have picked a version of the sentence in which the original grammatical errors have been fixed. In this paper, we propose generating such alternative formulations using statistical machine translation. For example, we take the original sentence E and translate it to Chinese using the Google TransOriginal Swedish Italian Russian French"
W12-2005,N03-1024,0,0.0396708,"r detection, our combination approach is directly related to the research on machine translation system combination wherein translation hypotheses produced by different SMT systems are combined to allow the extraction of a better, combined hypothesis (Bangalore et al., 2001; Rosti et al., 2007; Feng et al., 2009). However, our combination approach is different in that all the round-trip translations are produced by a single system but via different pivot languages. Finally, the idea of combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf"
W12-2005,P11-1094,0,0.0311841,"Missing"
W12-2005,N07-1029,0,0.0345598,"f different round-trip translations and explore a whole new set of corrections that go beyond the translations themselves. Finally, we do not restrict our analysis to any single type of 45 error. In fact, our test sentences contain several different types of grammatical errors. Outside of the literature on grammatical error detection, our combination approach is directly related to the research on machine translation system combination wherein translation hypotheses produced by different SMT systems are combined to allow the extraction of a better, combined hypothesis (Bangalore et al., 2001; Rosti et al., 2007; Feng et al., 2009). However, our combination approach is different in that all the round-trip translations are produced by a single system but via different pivot languages. Finally, the idea of combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (20"
W12-2005,W09-0441,1,0.823581,"ree itself. Therefore, the task is more complex than simply selecting the right round-trip translation. We posit that a better approach will be to combine the evidence of correction produced by each independent translation model and increase the likelihood of producing a final whole-sentence correction. Additionally, by engineering such a combination, we increase the likelihood that the final correction will preserve the meaning of the original sentence. In order to combine the round-trip translations, we developed a heuristic alignment algorithm that uses the TERp machine translation metric (Snover et al., 2009). The TERp metric takes a pair of sentences and computes the least number of edit operations that can be employed to turn one sentence into the other.2 As a by-product of computing the edit sequence, TERp produces an alignment between the two sentences where each alignment link is defined by an edit operation. Figure 2 shows an example of the alignment produced by TERp between the original sentence from Figure 1 and its Russian roundtrip translation. Note that TERp also allows shifting words and phrases in the second sentence in order to obtain a smaller edit cost (as indicated by the asterisk"
W12-2005,C10-1149,0,0.0207563,"combining multiple surface renderings with the same meaning has also been explored in paraphrase generation. Pang et al. (2003) propose an algorithm to align sets of parallel sentences driven entirely by the syntactic representations of the sentences. The alignment algorithm outputs a merged lattice from which lexical, phrasal, and sentential paraphrases could simply be read off. Barzilay and Lee (2003) cluster topically related sentences into slotted word lattices by using multiple sequence alignment for the purpose of downstream paraphrase generation from comparable corpora. More recently, Zhao et al. (2010) perform round-trip translation of English sentences via different pivot languages and different off-the-shelf SMT systems to generate candidate paraphrases. However, they do not combine the candidate paraphrases in any way. A detailed survey of paraphrase generation techniques can be found in (Androutsopoulos and Malakasiotis, 2010) and (Madnani and Dorr, 2010). 3 Methodology The basic idea underlying our error correction technique is quite simple: if we can automatically generate alternative surface renderings of the meaning expressed in the original sentence and then pick the one that is mo"
W12-2027,A00-2019,0,0.308125,"the HOO 2012 format did not account for this, which may have decreased recognition performance slightly. 234 Base System As our base system, we repurpose a complex system designed to automatically score student essays (both native and non-native and across a wide range of competency levels). The system is also used to give feedback to essay writers, so precision is favored over recall. There are three main modules in the essay-scoring system whose purpose it is to detect preposition and determiner errors (as they are defined in that system). Many of the details have been reported previously (Chodorow and Leacock, 2000; Han et al., 2004; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008), so here we will only give brief summaries of these modules. It is important to note that this system was run without modification. That is, no training of new models or tuning was carried out specifically for the shared task. In addition, for the two statistical modules, we only had access to the final, boolean decisions about whether an error is present or not at a particular location in text. That is, we did not have access to confidence scores, and so task-specific tuning for F-score was not an optio"
W12-2027,W07-1604,1,0.823641,"creased recognition performance slightly. 234 Base System As our base system, we repurpose a complex system designed to automatically score student essays (both native and non-native and across a wide range of competency levels). The system is also used to give feedback to essay writers, so precision is favored over recall. There are three main modules in the essay-scoring system whose purpose it is to detect preposition and determiner errors (as they are defined in that system). Many of the details have been reported previously (Chodorow and Leacock, 2000; Han et al., 2004; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008), so here we will only give brief summaries of these modules. It is important to note that this system was run without modification. That is, no training of new models or tuning was carried out specifically for the shared task. In addition, for the two statistical modules, we only had access to the final, boolean decisions about whether an error is present or not at a particular location in text. That is, we did not have access to confidence scores, and so task-specific tuning for F-score was not an option. 3.1 Preposition Error Detection The base system detects"
W12-2027,W12-2006,0,0.04109,"tric used in the HOO 2012 Shared Task: balanced F-score, or F1 (§6). We find that the tuned hybrid system improves upon the recall and F-score of the base system. Also, in the HOO 2012 Shared Task, the hybrid system achieved results that were competitive with other submitted grammatical error detection systems (§7). 233 The 7th Workshop on the Innovative Use of NLP for Building Educational Applications, pages 233–241, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics 2 Task Definition 3 In this section, we provide a brief overview of the HOO 2012 Shared Task (Dale et al., 2012). The task focuses on prepositions and determiners only, distinguishing the following error types: preposition selection errors (coded “RT” in the data), extraneous prepositions (“UT”), missing prepositions (“MT”), determiner selection errors (“RD”), extraneous determiners (“UD”), and missing determiners (“MD”). For training and testing data, the shared task uses short essays from an examination for speakers of English as a foreign language. The data includes gold standard human annotations identifying preposition and determiner errors. These errors are represented as edits that transform an u"
W12-2027,N10-1019,0,0.107838,"different types. In this section, we describe how we filter edits using their scores and how we combine them with edits from the base system (§3). As described above, for an alternative v to be considered as a candidate edit, the value of r(w, i, v) in Eq. 2 must be greater than a threshold of 1, indicating that the alternative scores higher than the original word. However, we observed low precision during development when including all candidate edits and decided to penalize the ratios. Bergsma et al. (2009) discuss raising the threshold, which has 7 The heuristics are based on those used in Gamon (2010) (personal communication). 237 a similar effect. Preliminary experiments indicated that different edits (e.g., extraneous preposition edits and preposition selection edits) should have different penalties, and we also want to avoid edits with overlapping spans. Thus, for each location with one or more candidate edits, we select the best according to Equation 3 and filter out the rest. v ∗ = arg max v r(w, i, v) − penalty(wi , v) (3) penalty(wi , v) is a function that takes the current word wi and the alternative v and returns one of 6 values: qRT for preposition selection, qU T for extraneous"
W12-2027,han-etal-2004-detecting,0,0.20828,"account for this, which may have decreased recognition performance slightly. 234 Base System As our base system, we repurpose a complex system designed to automatically score student essays (both native and non-native and across a wide range of competency levels). The system is also used to give feedback to essay writers, so precision is favored over recall. There are three main modules in the essay-scoring system whose purpose it is to detect preposition and determiner errors (as they are defined in that system). Many of the details have been reported previously (Chodorow and Leacock, 2000; Han et al., 2004; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008), so here we will only give brief summaries of these modules. It is important to note that this system was run without modification. That is, no training of new models or tuning was carried out specifically for the shared task. In addition, for the two statistical modules, we only had access to the final, boolean decisions about whether an error is present or not at a particular location in text. That is, we did not have access to confidence scores, and so task-specific tuning for F-score was not an option. 3.1 Preposition"
W12-2027,C08-1109,1,0.913083,"formance slightly. 234 Base System As our base system, we repurpose a complex system designed to automatically score student essays (both native and non-native and across a wide range of competency levels). The system is also used to give feedback to essay writers, so precision is favored over recall. There are three main modules in the essay-scoring system whose purpose it is to detect preposition and determiner errors (as they are defined in that system). Many of the details have been reported previously (Chodorow and Leacock, 2000; Han et al., 2004; Han et al., 2006; Chodorow et al., 2007; Tetreault and Chodorow, 2008), so here we will only give brief summaries of these modules. It is important to note that this system was run without modification. That is, no training of new models or tuning was carried out specifically for the shared task. In addition, for the two statistical modules, we only had access to the final, boolean decisions about whether an error is present or not at a particular location in text. That is, we did not have access to confidence scores, and so task-specific tuning for F-score was not an option. 3.1 Preposition Error Detection The base system detects incorrect and extraneous prepos"
W13-1706,C12-1025,0,0.625905,"e and we expect that the data will become available through the Linguistic Data Consortium in 2013. For future editions of the NLI shared task, we think it would be interesting to expand the scope of NLI from identifying the L1 of student essays to be able to identify the L1 of any piece of writing. The ICLE and TOEFL 11 corpora are both collections of academic writing and thus it may be the case that certain features or methodologies generalize better to other writing genres and domains. For those interested in robust NLI approaches, please refer to the TOR team shared task report as well as Brooke and Hirst (2012). In addition, since the TOEFL 11 data contains proficiency level one could include an evaluation by proficiency level as language learners make different types of errors and may even have stylistic differences in their writing as their proficiency progresses. Finally, while this may be in the periphery of the scope of an NLI shared task, one interesting evaluation is to see how well human raters can fare on this task. This would of course involve knowledgeable language instructors who have years of experience in teaching students from different L1s. Our thinking is that NLI might be one task"
W13-1706,P12-2038,0,0.551993,") have figured prominently in prior work. Not only are they easy to compute, but they can be quite predictive. However, there are many variations on the features. Past reseach efforts have explored different n-gram windows (though most tend to focus on unigrams and bigrams), different thresholds for how many ngrams to include as well as whether to encode the feature as binary (presence or absence of the particular n-gram) or as a normalized count. The inclusion of syntactic features has been a focus in recent work. Wong and Dras (2011) explored the use of production rules from two parsers and Swanson and Charniak (2012) explored the use of Tree Substitution Grammars (TSGs). Tetreault et al. (2012) also investigated the use of TSGs as well as dependency features extracted from the Stanford parser. Other approaches to NLI have included the use of Latent Dirichlet Analysis to cluster features (Wong et al., 2011), adaptor grammars (Wong et al., 2012), and language models (Tetreault et al., 2012). Additionally, there has been research into the effects of training and testing on different corpora (Brooke and Hirst, 2011). Much of the aforementioned work takes the perspective of optimizing for the task of Native La"
W13-1706,C12-1158,1,0.829245,"red essays written by college-level English language learners. However, this corpus is quite small for training and testing statistical systems which makes it difficult to tell whether the systems that are developed can scale well to larger data sets or to different domains. Since the ICLE corpus was not designed with the task of NLI in mind, the usability of the corpus for this task is further compromised by idiosyncrasies in the data such as topic bias (as shown by Brooke and Hirst (2011)) and the occurrence of characters which only appear in essays written by speakers of certain languages (Tetreault et al., 2012). As a result, it is hard to draw conclusions about which features 48 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 48–57, c Atlanta, Georgia, June 13 2013. 2013 Association for Computational Linguistics actually perform best. The second issue is that there has been little consistency in the field in the use of cross-validation, the number of L1s, and which L1s are used. As a result, comparing one approach to another has been extremely difficult. The first Shared Task in Native Language Identification is intended to better unify this c"
W13-1706,D11-1148,0,0.502312,"f the ICLE corpus consisting of 5 L1s. N-gram features (word, character and POS) have figured prominently in prior work. Not only are they easy to compute, but they can be quite predictive. However, there are many variations on the features. Past reseach efforts have explored different n-gram windows (though most tend to focus on unigrams and bigrams), different thresholds for how many ngrams to include as well as whether to encode the feature as binary (presence or absence of the particular n-gram) or as a normalized count. The inclusion of syntactic features has been a focus in recent work. Wong and Dras (2011) explored the use of production rules from two parsers and Swanson and Charniak (2012) explored the use of Tree Substitution Grammars (TSGs). Tetreault et al. (2012) also investigated the use of TSGs as well as dependency features extracted from the Stanford parser. Other approaches to NLI have included the use of Latent Dirichlet Analysis to cluster features (Wong et al., 2011), adaptor grammars (Wong et al., 2012), and language models (Tetreault et al., 2012). Additionally, there has been research into the effects of training and testing on different corpora (Brooke and Hirst, 2011). Much of"
W13-1706,U11-1015,0,0.044661,"many ngrams to include as well as whether to encode the feature as binary (presence or absence of the particular n-gram) or as a normalized count. The inclusion of syntactic features has been a focus in recent work. Wong and Dras (2011) explored the use of production rules from two parsers and Swanson and Charniak (2012) explored the use of Tree Substitution Grammars (TSGs). Tetreault et al. (2012) also investigated the use of TSGs as well as dependency features extracted from the Stanford parser. Other approaches to NLI have included the use of Latent Dirichlet Analysis to cluster features (Wong et al., 2011), adaptor grammars (Wong et al., 2012), and language models (Tetreault et al., 2012). Additionally, there has been research into the effects of training and testing on different corpora (Brooke and Hirst, 2011). Much of the aforementioned work takes the perspective of optimizing for the task of Native Language Identification, that is, what is the best way of modeling the problem to get the highest system accuracy? The problem of Native Language Identifica1 http://nlisharedtask2013.org/bibliography-of-relatedwork-in-nli tion is also of interest to researchers in Second Language Acquisition wher"
W13-1706,D12-1064,0,0.294674,"ther to encode the feature as binary (presence or absence of the particular n-gram) or as a normalized count. The inclusion of syntactic features has been a focus in recent work. Wong and Dras (2011) explored the use of production rules from two parsers and Swanson and Charniak (2012) explored the use of Tree Substitution Grammars (TSGs). Tetreault et al. (2012) also investigated the use of TSGs as well as dependency features extracted from the Stanford parser. Other approaches to NLI have included the use of Latent Dirichlet Analysis to cluster features (Wong et al., 2011), adaptor grammars (Wong et al., 2012), and language models (Tetreault et al., 2012). Additionally, there has been research into the effects of training and testing on different corpora (Brooke and Hirst, 2011). Much of the aforementioned work takes the perspective of optimizing for the task of Native Language Identification, that is, what is the best way of modeling the problem to get the highest system accuracy? The problem of Native Language Identifica1 http://nlisharedtask2013.org/bibliography-of-relatedwork-in-nli tion is also of interest to researchers in Second Language Acquisition where they seek to explain syntactic trans"
W13-1706,P11-1019,0,0.0387459,"uebingen Ualberta UKP Unibuc UNT UTD VTEX Abbreviation BOB CHO HAI CN COR CUN CYW DAR EUR HAU ITA JAR KYL LIM HYD MIC CAR MQ NAI NRC OSL TOR TUE UAB UKP BUC UNT UTD VTX missions. Table 5 shows the results for the third subtask “Open-2”. Four teams competed in this task for a total of 15 submissions. The challenge for those competing in the Open tasks was finding enough non-TOEFL 11 data for each L1 to train a classifier. External corpora commonly used in the competition included the: • ICLE: which covered all L1s except for Arabic, Hindi and Telugu; • FCE: First Certificate in English Corpus (Yannakoudakis et al., 2011): a collection of essay written for an English assessment exam, which covered all L1s except for Arabic, Hindi and Telugu • ICNALE: International Corpus Network of Asian Learners of English (Ishikawa, 2011): a collection of essays written by Chinese, Japanese and Korean learners of English along with 7 other L1s with Asian backgrounds. Table 2: Participating Teams and Team Abbreviations top submission for each team and its performance by overall accuracy and by L1.2 Table 3 shows results for the Closed sub-task where teams developed systems that were trained solely on TOEFL 11- TRAIN and TOEFL"
W13-3601,de-marneffe-etal-2006-generating,0,0.067319,"Missing"
W13-3601,N10-1019,0,0.180097,"r determiners, prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb fo"
W13-3601,P03-1054,0,0.00544378,"h purposes since June 20111 . All instances of grammatical errors are annotated in NUCLE, and the errors are classified into 27 error types (Dahlmeier et al., 2013). To help participating teams in their preparation for the shared task, we also performed automatic preprocessing of the NUCLE corpus and released the preprocessed form of NUCLE. The preprocessing operations performed on the NUCLE essays include sentence segmentation and word tokenization using the NLTK toolkit (Bird et al., 2009), and part-of-speech (POS) tagging, constituency and dependency tree parsing using the Stanford parser (Klein and Manning, 2003; de Marneffe et al., 2006). The error annotations, which are originally at the character level, are then mapped to error annotations at the word token level. Error annotations at the word token 1 level also facilitate scoring, as we will see in Section 4, since our scorer operates by matching tokens. Note that although we released our own preprocessed version of NUCLE, the participating teams were however free to perform their own preprocessing if they so preferred. 3.1.1 Revised version of NUCLE NUCLE release version 2.3 was used in the CoNLL-2013 shared task. In this version, 17 essays were"
W13-3601,D11-1010,1,0.122995,"agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreement, besides articles/determiners and prepositio"
W13-3601,P11-1092,1,0.860095,"agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreement, besides articles/determiners and prepositio"
W13-3601,D12-1052,1,0.806821,"admit some bad effect which is brought by the new technology, still the advantages of the new technologies cannot be simply discarded. The noun number error effect needs to be corrected (effect → effects). This necessitates the correction of a subject-verb agreement error (is → are). A pipeline system in which corrections for subjectverb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of (Dahlmeier and Ng, 2012a), for example, is designed to deal with multiple, interacting errors. Note that the essays in the training data and the test essays naturally contain grammatical errors of all types, beyond the five error types focused in our shared task. In the automatically corrected essays returned by a participating system, only corrections necessary to correct errors of the five types From past to the present, many important innovations have surfaced. There is an article/determiner error (past → the past) in this sentence. The error annotation, also called correction or edit, in SGML format is shown in"
W13-3601,D10-1094,0,0.289959,", prepositions, noun form, verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreem"
W13-3601,W13-3602,0,0.378187,"ion approach ANPSV AN ANPSV ANPSV ANPSV STEL SZEG TILB TOR UAB STAN SJT1 NTHU SAAR KOR NARA ANPSV R M R M T M L L M R M SV AN S ANP AP N SV ANPV A S ANPSV IITB M ANP HIT Approach T Error ANPSV Team CAMB and Manning, 2003). We also make use of the POS tags assigned in the preprocessed form of the test essays. We then assign an error type to a system edit based on the automatically determined POS tags, as follows: evaluated with alternative answers. Not surprisingly, the teams which submitted alternative answers tend to show the greatest improvements in their F1 measure. Overall, the UIUC team (Rozovskaya et al., 2013) achieves the best F1 measure, with a clear lead over the other teams in the shared task, under both evaluation settings (without and with alternative answers). For future research which uses the test data of the CoNLL-2013 shared task, we recommend that evaluation be carried out in the setting that does not use alternative answers, to ensure a fairer evaluation. This is because the scores of the teams which submitted alternative answers tend to be higher in a biased way when evaluated with alternative answers. Rank 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 Team UIUC NTHU UMC NARA HIT STEL CAM"
W13-3601,N12-1067,1,0.697277,"admit some bad effect which is brought by the new technology, still the advantages of the new technologies cannot be simply discarded. The noun number error effect needs to be corrected (effect → effects). This necessitates the correction of a subject-verb agreement error (is → are). A pipeline system in which corrections for subjectverb agreement errors occur strictly before corrections for noun number errors would not be able to arrive at a fully corrected sentence for this example. The ability to correct multiple, interacting errors is thus necessary in our shared task. The recent work of (Dahlmeier and Ng, 2012a), for example, is designed to deal with multiple, interacting errors. Note that the essays in the training data and the test essays naturally contain grammatical errors of all types, beyond the five error types focused in our shared task. In the automatically corrected essays returned by a participating system, only corrections necessary to correct errors of the five types From past to the present, many important innovations have surfaced. There is an article/determiner error (past → the past) in this sentence. The error annotation, also called correction or edit, in SGML format is shown in"
W13-3601,P10-2065,1,0.493158,"verb form, subject-verb agreement, pronouns, word choice, sentence structure, punctuation, capitalization, etc. Of all the error types, determiners and prepositions are among 1 Proceedings of the Seventeenth Conference on Computational Natural Language Learning: Shared Task, pages 1–12, c Sofia, Bulgaria, August 8-9 2013. 2013 Association for Computational Linguistics the most frequent errors made by learners of English. Not surprisingly, much published research on grammatical error correction focuses on article and preposition errors (Han et al., 2006; Gamon, 2010; Rozovskaya and Roth, 2010; Tetreault et al., 2010; Dahlmeier and Ng, 2011b), with relatively less work on correcting word choice errors (Dahlmeier and Ng, 2011a). Article and preposition errors were also the only error types featured in the HOO 2012 shared task. Likewise, although all error types were included in the HOO 2011 shared task, almost all participating teams dealt with article and preposition errors only (besides spelling and punctuation errors). In the CoNLL-2013 shared task, it was felt that the community is now ready to deal with more error types, including noun number, verb form, and subject-verb agreement, besides articles/de"
W13-3601,W13-1703,1,0.717953,"ince there are five error types in our shared task compared to two in HOO 2012, there is a greater chance of encountering multiple, interacting errors in a sentence in our shared task. This increases the complexity of our shared task relative to that of HOO 2012. To illustrate, consider the following sentence: are made. The other errors are to be left uncorrected. 3 Data This section describes the training and test data released to each participating team in our shared task. 3.1 Training Data The training data provided in our shared task is the NUCLE corpus, the NUS Corpus of Learner English (Dahlmeier et al., 2013). As noted by (Leacock et al., 2010), the lack of a manually annotated and corrected corpus of English learner texts has been an impediment to progress in grammatical error correction, since it prevents comparative evaluations on a common benchmark test data set. NUCLE was created precisely to fill this void. It is a collection of 1,414 essays written by students at the National University of Singapore (NUS) who are non-native speakers of English. The essays were written in response to some prompts, and they cover a wide range of topics, such as environmental pollution, health care, etc. The g"
W13-3601,P11-1019,0,0.412215,"Missing"
W13-3601,W11-2838,0,0.650871,"the various approaches adopted by the participating teams, and present the evaluation results. 1 Introduction Grammatical error correction is the shared task of the Seventeenth Conference on Computational Natural Language Learning in 2013 (CoNLL2013). In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors present in the essay, and return the corrected essay. This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) 2011 and 2012 organized in the past two years (Dale and Kilgarriff, 2011; Dale et al., 2012). In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application. This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed. Also, tackling this task has far-reaching impact, since it is estimated that hundreds of millions of people worldwi"
W13-3601,W12-2006,0,0.751371,"ted by the participating teams, and present the evaluation results. 1 Introduction Grammatical error correction is the shared task of the Seventeenth Conference on Computational Natural Language Learning in 2013 (CoNLL2013). In this task, given an English essay written by a learner of English as a second language, the goal is to detect and correct the grammatical errors present in the essay, and return the corrected essay. This task has attracted much recent research interest, with two shared tasks Helping Our Own (HOO) 2011 and 2012 organized in the past two years (Dale and Kilgarriff, 2011; Dale et al., 2012). In contrast to previous CoNLL shared tasks which focused on particular subtasks of natural language processing, such as named entity recognition, semantic role labeling, dependency parsing, or coreference resolution, grammatical error correction aims at building a complete end-to-end application. This task is challenging since for many error types, current grammatical error correction systems do not achieve high performance and much research is still needed. Also, tackling this task has far-reaching impact, since it is estimated that hundreds of millions of people worldwide are learning Engl"
W13-3601,W13-3604,0,\N,Missing
W15-0620,C14-1185,0,0.408647,"Missing"
W15-0620,W13-1726,0,0.0970702,"Missing"
W15-0620,P05-1022,0,0.225282,"Missing"
W15-0620,W13-1727,0,0.253176,"Missing"
W15-0620,W13-1712,0,0.0938667,"Missing"
W15-0620,D14-1142,0,0.246652,"Missing"
W15-0620,W13-1714,0,0.336048,"Missing"
W15-0620,W15-0606,1,0.830691,"Missing"
W15-0620,D14-1144,1,0.807671,"Missing"
W15-0620,W13-1716,1,0.797299,"Missing"
W15-0620,D08-1027,0,0.0773695,"Missing"
W15-0620,C12-1158,1,0.820422,"Missing"
W15-0620,W13-1706,1,0.864163,"Missing"
W16-3638,P12-2059,0,0.0167208,"pressions and blacklists, to catch bad language and consequently remove a post. Essentially, users learn over time not to use common lexical items and words to convey certain language. Thus, characters often play an important role in the comment language. Characters, in combination with words, can act as basic phonetic, morpho-lexical and semantic units in comments such as “ki11 yrslef a$$hole”. Character n-grams have been proven useful for other NLP tasks such as authorship identification (Sapkota et al., 2015), native language identification (Tetreault et al., 2013) and machine translation (Nakov and Tiedemann, 2012), but surprisingly have not been the focus in prior work for abusive language. In this paper, we investigate the role that character n-grams play in this task by exploring their use in two different algorithms. We compare their results to two state-of-the-art approaches by evaluating on a corpus of nearly 1M comments. Briefly, our contributions are summarized as follows: 1) character n-grams outperform word n-grams in both algorithms, and 2) the models proposed in this work outperform the previous state-of-the-art for this dataset. Although word and character n-grams have been used as features"
W16-3638,N15-1010,0,0.0147501,"ards and guidelines imposed by media companies that users must adhere to, in conjunction with regular expressions and blacklists, to catch bad language and consequently remove a post. Essentially, users learn over time not to use common lexical items and words to convey certain language. Thus, characters often play an important role in the comment language. Characters, in combination with words, can act as basic phonetic, morpho-lexical and semantic units in comments such as “ki11 yrslef a$$hole”. Character n-grams have been proven useful for other NLP tasks such as authorship identification (Sapkota et al., 2015), native language identification (Tetreault et al., 2013) and machine translation (Nakov and Tiedemann, 2012), but surprisingly have not been the focus in prior work for abusive language. In this paper, we investigate the role that character n-grams play in this task by exploring their use in two different algorithms. We compare their results to two state-of-the-art approaches by evaluating on a corpus of nearly 1M comments. Briefly, our contributions are summarized as follows: 1) character n-grams outperform word n-grams in both algorithms, and 2) the models proposed in this work outperform t"
W16-3638,W13-1706,1,0.738086,"rs must adhere to, in conjunction with regular expressions and blacklists, to catch bad language and consequently remove a post. Essentially, users learn over time not to use common lexical items and words to convey certain language. Thus, characters often play an important role in the comment language. Characters, in combination with words, can act as basic phonetic, morpho-lexical and semantic units in comments such as “ki11 yrslef a$$hole”. Character n-grams have been proven useful for other NLP tasks such as authorship identification (Sapkota et al., 2015), native language identification (Tetreault et al., 2013) and machine translation (Nakov and Tiedemann, 2012), but surprisingly have not been the focus in prior work for abusive language. In this paper, we investigate the role that character n-grams play in this task by exploring their use in two different algorithms. We compare their results to two state-of-the-art approaches by evaluating on a corpus of nearly 1M comments. Briefly, our contributions are summarized as follows: 1) character n-grams outperform word n-grams in both algorithms, and 2) the models proposed in this work outperform the previous state-of-the-art for this dataset. Although w"
W16-3638,P12-2018,0,0.0152188,"classes, using our SVM classifier (”Combination”). In terms of overall performance, all methods improved on or tied the Djuric et al. (2015b), C2V and token n-gram baselines. The top performing baseline and current state-of-the-art, Nobata et al. (2016), which consists of a comprehensive combination of a range of different features, is bested by NBSVM using solely character n-grams (77 FSupport Vector Machine with Naive Bayes Features (NBSVM) Naive Bayes (NB) and Support Vector Machines (SVM) have been proven to be effective approaches for NLP applications such as sentiment and text analysis. Wang and Manning (2012) showed the power of combining these two generative and discriminative classifiers where an SVM 301 Method Djuric et al. Nobata et al. Token n-grams C2V d300w10∗ C2V d300w5 C2V d100w10 C2V d100w5 NBSVM (word)∗ NBSVM (char)∗ RNNLM (word)∗ RNNLM (char1 )∗ RNNLM (char2 ) Combination Rec. 79 76 58 57 56 56 60 72 72 78 68 75 Prec. 77 70 77 76 75 76 84 83 59 60 68 84 F-1 78 73 66 66 65 64 70 77 65 68 68 79 AUC 80 91 84 85 84 82 82 89 92 82 85 85 93 2015b). As one would expect, decreasing the dimensionality of the embedding and the context window results in a loss of performance of as much as 18 F-1"
W16-3638,W12-2103,0,0.38382,"stealing like a lil maggot. Hang thm all.” In that example, there are tokenization and normalization issues, as well as a conscious bastardization of words in an effort to evade blacklists or to add color to the post. While previous work for detecting abusive language has been dominated by lexical-based approaches, we claim that morphological features play a more significant role in this task. This 2 Related Work Prior work in abusive language has been rather diffuse as researchers have focused on different aspects ranging from profanity detection (Sood et al., 2012) to hate speech detection (Warner and Hirschberg, 2012) to cyberbullying (Dadvar et al., 2013) and to abusive language in general (Chen et al., 2012; Djuric et al., 2015b). The overwhelming majority of this work has 299 Proceedings of the SIGDIAL 2016 Conference, pages 299–303, c Los Angeles, USA, 13-15 September 2016. 2016 Association for Computational Linguistics two, RNNLM and NBSVM, we use as methodologies for which to explore the impact of characterbased vs. token-based features. focused on using supervised classification with canonical NLP features. Token n-grams are one of the most popular features across many works (Yin et al., 2009; Chen"
W17-0802,L16-1704,0,0.362644,"at are frequently seen in the comments section. Many of these labels are for characteristics of online conversations not captured by traditional argumentation or dialogue features. Some of the labels we collect have been annotated in previous work (§2), but this is the first time they are aggregated in a single corpus at the dialogue level. In this paper, we present the Yahoo News Annotated Comments Corpus (YNACC), which contains 2.4k threads and 10k comments from the comments sections of Yahoo News articles. We additionally collect annotations on 1k threads from the Internet Argument Corpus (Abbott et al., 2016), representing another domain of online debates. We contrast annotations of Yahoo and IAC threads, explore ways in which threads perceived to be ERICs differ in this two venues, and identify some unanticipated characteristics of ERICs. This is the first exploration of how characteristics of individual comments contribute to the dialogue-level classification of an exchange. YNACC will facilitate research to understand ERICS and other aspects of dialogue. The corpus and annotations will be available at https: //github.com/cnap/ynacc. This work presents a dataset and annotation scheme for the new"
W17-0802,D16-1066,0,0.0218999,"ews articles are annotated in the SENSEI corpus, which contains human-authored summaries of 1.8k comments posted on Guardian articles (Barker et al., 2016). Participants described Recent work has focused on the analysis of usergenerated text in various online venues, including labeling certain qualities of individual comments, comment pairs, or the roles of individual commenters. The largest and most extensively annotated corpus predating this work is the Internet Argument Corpus (IAC), which contains approximately 480k comments in 16.5k threads from on14 threads, “dogmatism” of reddit users (Fast and Horvitz, 2016), and argumentation units in discussions related to technology (Ghosh et al., 2014). each comment with short, free-form text labels and then wrote a 150–250-word comment summary with these labels. Barker et al. (2016) recognized that comments have diverse qualities, many of which are coded in this work (§3), but did not explicitly collect labels of them. 3 Annotation scheme This section outlines our coding scheme for identifying ERICs, with labels for comment threads and each comment contained therein. Starting with the annotation categories from the IAC and the curation criteria of Diakopoulo"
W17-0802,W14-2106,0,0.0191122,"s of 1.8k comments posted on Guardian articles (Barker et al., 2016). Participants described Recent work has focused on the analysis of usergenerated text in various online venues, including labeling certain qualities of individual comments, comment pairs, or the roles of individual commenters. The largest and most extensively annotated corpus predating this work is the Internet Argument Corpus (IAC), which contains approximately 480k comments in 16.5k threads from on14 threads, “dogmatism” of reddit users (Fast and Horvitz, 2016), and argumentation units in discussions related to technology (Ghosh et al., 2014). each comment with short, free-form text labels and then wrote a 150–250-word comment summary with these labels. Barker et al. (2016) recognized that comments have diverse qualities, many of which are coded in this work (§3), but did not explicitly collect labels of them. 3 Annotation scheme This section outlines our coding scheme for identifying ERICs, with labels for comment threads and each comment contained therein. Starting with the annotation categories from the IAC and the curation criteria of Diakopoulos (2015), we have adapted these schemes and identified new characteristics that hav"
W17-0802,andreas-etal-2012-annotating,0,0.0154127,"rpora from online sources related to this work include the following: Concepts related to persuasiveness have been studied, including annotations for “convincing-ness” in debate forums (Habernal and Gurevych, 2016), influencers in discussions from blogs and Wikipedia (Biran et al., 2012), and user relations as a proxy of persuasion in reddit (Tan et al., 2016; Wei et al., 2016). Politeness was labeled and identified in Stack Exchange and Wikipedia discussions (Danescu-NiculescuMizil et al., 2013). Some previous work focused on detecting agreement has considered blog and Wikipedia discussions (Andreas et al., 2012) and debate forums (Skeppstedt et al., 2016). Sarcasm has been identified in a corpus of microblogs identified with the hashtag #sarcasm on Twitter (Gonz´alez-Ib´anez et al., 2011; Davidov et al., 2010) and in online forums (Oraby et al., 2016). Sentiment has been studied widely, often in the context of reviews (Pang and Lee, 2005), and in the context of user-generated exchanges, positive and negative attitudes have been identified in Usenet discussions (Hassan et al., 2010). Other qualities of user-generated text that are not covered in this work but have been investigated before include meta"
W17-0802,P11-2102,0,0.136874,"Missing"
W17-0802,W16-3605,0,0.0211581,"you say is valid. F that's your opinion. but it's not valid. my factual statement is. ' $ "" ( ) ' $ "" ( ) Figure 2: A non-ERIC that is labeled argumentative and off-topic with continual disagreement. 2 Related work line forums in which users debate contentious issues. The IAC has been coded for for topic (3k threads), stance (2k authors), and agreement, sarcasm, and hostility (10k comment pairs) (Abbott et al., 2016; Walker et al., 2012). Comments from online news articles are annotated in the SENSEI corpus, which contains human-authored summaries of 1.8k comments posted on Guardian articles (Barker et al., 2016). Participants described Recent work has focused on the analysis of usergenerated text in various online venues, including labeling certain qualities of individual comments, comment pairs, or the roles of individual commenters. The largest and most extensively annotated corpus predating this work is the Internet Argument Corpus (IAC), which contains approximately 480k comments in 16.5k threads from on14 threads, “dogmatism” of reddit users (Fast and Horvitz, 2016), and argumentation units in discussions related to technology (Ghosh et al., 2014). each comment with short, free-form text labels"
W17-0802,P16-1150,0,0.0186777,"sh for the labels to reflect the nature of all comments posted on online articles instead of just the qualities sought in editorially curated comments. ERICs can take many forms and may not reflect the formal tone or intent that editors in traditional news outlets seek. Our coding scheme intersects with attributes examined in several different areas of research. Some of the most recent and relevant discourse corpora from online sources related to this work include the following: Concepts related to persuasiveness have been studied, including annotations for “convincing-ness” in debate forums (Habernal and Gurevych, 2016), influencers in discussions from blogs and Wikipedia (Biran et al., 2012), and user relations as a proxy of persuasion in reddit (Tan et al., 2016; Wei et al., 2016). Politeness was labeled and identified in Stack Exchange and Wikipedia discussions (Danescu-NiculescuMizil et al., 2013). Some previous work focused on detecting agreement has considered blog and Wikipedia discussions (Andreas et al., 2012) and debate forums (Skeppstedt et al., 2016). Sarcasm has been identified in a corpus of microblogs identified with the hashtag #sarcasm on Twitter (Gonz´alez-Ib´anez et al., 2011; Davidov et a"
W17-0802,W12-2105,0,0.0176162,"nstead of just the qualities sought in editorially curated comments. ERICs can take many forms and may not reflect the formal tone or intent that editors in traditional news outlets seek. Our coding scheme intersects with attributes examined in several different areas of research. Some of the most recent and relevant discourse corpora from online sources related to this work include the following: Concepts related to persuasiveness have been studied, including annotations for “convincing-ness” in debate forums (Habernal and Gurevych, 2016), influencers in discussions from blogs and Wikipedia (Biran et al., 2012), and user relations as a proxy of persuasion in reddit (Tan et al., 2016; Wei et al., 2016). Politeness was labeled and identified in Stack Exchange and Wikipedia discussions (Danescu-NiculescuMizil et al., 2013). Some previous work focused on detecting agreement has considered blog and Wikipedia discussions (Andreas et al., 2012) and debate forums (Skeppstedt et al., 2016). Sarcasm has been identified in a corpus of microblogs identified with the hashtag #sarcasm on Twitter (Gonz´alez-Ib´anez et al., 2011; Davidov et al., 2010) and in online forums (Oraby et al., 2016). Sentiment has been st"
W17-0802,D10-1121,0,0.0295508,"cuMizil et al., 2013). Some previous work focused on detecting agreement has considered blog and Wikipedia discussions (Andreas et al., 2012) and debate forums (Skeppstedt et al., 2016). Sarcasm has been identified in a corpus of microblogs identified with the hashtag #sarcasm on Twitter (Gonz´alez-Ib´anez et al., 2011; Davidov et al., 2010) and in online forums (Oraby et al., 2016). Sentiment has been studied widely, often in the context of reviews (Pang and Lee, 2005), and in the context of user-generated exchanges, positive and negative attitudes have been identified in Usenet discussions (Hassan et al., 2010). Other qualities of user-generated text that are not covered in this work but have been investigated before include metaphor (Jang et al., 2014) and tolerance (Mukherjee et al., 2013) in online discussion 3.1 Thread labels Agreement The overall agreement present in a thread. • Agreement throughout • Continual disagreement • Agreement → disagreement: Begins with agreement which turns into disagreement. • Disagreement → agreement: Starts with disagreement that converges into agreement. Constructiveness A binary label indicating when a conversation is an ERIC, or has a clear exchange of ideas, o"
W17-0802,P13-1025,0,0.0665009,"Missing"
W17-0802,W14-2301,0,0.0286296,"ebate forums (Skeppstedt et al., 2016). Sarcasm has been identified in a corpus of microblogs identified with the hashtag #sarcasm on Twitter (Gonz´alez-Ib´anez et al., 2011; Davidov et al., 2010) and in online forums (Oraby et al., 2016). Sentiment has been studied widely, often in the context of reviews (Pang and Lee, 2005), and in the context of user-generated exchanges, positive and negative attitudes have been identified in Usenet discussions (Hassan et al., 2010). Other qualities of user-generated text that are not covered in this work but have been investigated before include metaphor (Jang et al., 2014) and tolerance (Mukherjee et al., 2013) in online discussion 3.1 Thread labels Agreement The overall agreement present in a thread. • Agreement throughout • Continual disagreement • Agreement → disagreement: Begins with agreement which turns into disagreement. • Disagreement → agreement: Starts with disagreement that converges into agreement. Constructiveness A binary label indicating when a conversation is an ERIC, or has a clear exchange of ideas, opinions, and/or information done so somewhat respectfully.1 • Constructive • Not constructive Type The overall type or tone of the conversation,"
W17-0802,W10-2914,0,0.0149767,"evych, 2016), influencers in discussions from blogs and Wikipedia (Biran et al., 2012), and user relations as a proxy of persuasion in reddit (Tan et al., 2016; Wei et al., 2016). Politeness was labeled and identified in Stack Exchange and Wikipedia discussions (Danescu-NiculescuMizil et al., 2013). Some previous work focused on detecting agreement has considered blog and Wikipedia discussions (Andreas et al., 2012) and debate forums (Skeppstedt et al., 2016). Sarcasm has been identified in a corpus of microblogs identified with the hashtag #sarcasm on Twitter (Gonz´alez-Ib´anez et al., 2011; Davidov et al., 2010) and in online forums (Oraby et al., 2016). Sentiment has been studied widely, often in the context of reviews (Pang and Lee, 2005), and in the context of user-generated exchanges, positive and negative attitudes have been identified in Usenet discussions (Hassan et al., 2010). Other qualities of user-generated text that are not covered in this work but have been investigated before include metaphor (Jang et al., 2014) and tolerance (Mukherjee et al., 2013) in online discussion 3.1 Thread labels Agreement The overall agreement present in a thread. • Agreement throughout • Continual disagreemen"
W17-0802,P12-3005,0,0.0603517,"Missing"
W17-0802,P13-1165,0,0.0209042,"016). Sarcasm has been identified in a corpus of microblogs identified with the hashtag #sarcasm on Twitter (Gonz´alez-Ib´anez et al., 2011; Davidov et al., 2010) and in online forums (Oraby et al., 2016). Sentiment has been studied widely, often in the context of reviews (Pang and Lee, 2005), and in the context of user-generated exchanges, positive and negative attitudes have been identified in Usenet discussions (Hassan et al., 2010). Other qualities of user-generated text that are not covered in this work but have been investigated before include metaphor (Jang et al., 2014) and tolerance (Mukherjee et al., 2013) in online discussion 3.1 Thread labels Agreement The overall agreement present in a thread. • Agreement throughout • Continual disagreement • Agreement → disagreement: Begins with agreement which turns into disagreement. • Disagreement → agreement: Starts with disagreement that converges into agreement. Constructiveness A binary label indicating when a conversation is an ERIC, or has a clear exchange of ideas, opinions, and/or information done so somewhat respectfully.1 • Constructive • Not constructive Type The overall type or tone of the conversation, describing the majority of comments. Tw"
W17-0802,P16-2032,0,0.00977986,"and may not reflect the formal tone or intent that editors in traditional news outlets seek. Our coding scheme intersects with attributes examined in several different areas of research. Some of the most recent and relevant discourse corpora from online sources related to this work include the following: Concepts related to persuasiveness have been studied, including annotations for “convincing-ness” in debate forums (Habernal and Gurevych, 2016), influencers in discussions from blogs and Wikipedia (Biran et al., 2012), and user relations as a proxy of persuasion in reddit (Tan et al., 2016; Wei et al., 2016). Politeness was labeled and identified in Stack Exchange and Wikipedia discussions (Danescu-NiculescuMizil et al., 2013). Some previous work focused on detecting agreement has considered blog and Wikipedia discussions (Andreas et al., 2012) and debate forums (Skeppstedt et al., 2016). Sarcasm has been identified in a corpus of microblogs identified with the hashtag #sarcasm on Twitter (Gonz´alez-Ib´anez et al., 2011; Davidov et al., 2010) and in online forums (Oraby et al., 2016). Sentiment has been studied widely, often in the context of reviews (Pang and Lee, 2005), and in the context of us"
W17-0802,N16-1070,0,0.0393992,"conversation is an ERIC, or has a clear exchange of ideas, opinions, and/or information done so somewhat respectfully.1 • Constructive • Not constructive Type The overall type or tone of the conversation, describing the majority of comments. Two labels can be chosen if conversations exhibit more than one dominant feature. • Argumentative: Contains a lot of “back and forth” between participants that does not necessarily reach a conclusion. • Flamewar: Contains insults, users “yelling” at each other, and no information exchanged. 1 Note that this definition of constructive differs from that of Niculae and Danescu-Niculescu-Mizil (2016), who use the term to denote discrete progress made towards identifying a point on a map. Our definition draws from the more traditional meaning when used in the context of conversations as “intended to be useful or helpful” (Macmillan, 2017). 15 • Informative: Contributes new information to the discussion. • Mean: The purpose of the comment is to be rude, mean, or hateful. • Sarcastic: Uses sarcasm with either intent to humor (overlaps with Funny) or offend. • Sympathetic: A warm, friendly comment that expresses positive emotion or sympathy. Topic The topic addressed in a comment, and more th"
W17-0802,W16-3604,0,0.0238041,"m blogs and Wikipedia (Biran et al., 2012), and user relations as a proxy of persuasion in reddit (Tan et al., 2016; Wei et al., 2016). Politeness was labeled and identified in Stack Exchange and Wikipedia discussions (Danescu-NiculescuMizil et al., 2013). Some previous work focused on detecting agreement has considered blog and Wikipedia discussions (Andreas et al., 2012) and debate forums (Skeppstedt et al., 2016). Sarcasm has been identified in a corpus of microblogs identified with the hashtag #sarcasm on Twitter (Gonz´alez-Ib´anez et al., 2011; Davidov et al., 2010) and in online forums (Oraby et al., 2016). Sentiment has been studied widely, often in the context of reviews (Pang and Lee, 2005), and in the context of user-generated exchanges, positive and negative attitudes have been identified in Usenet discussions (Hassan et al., 2010). Other qualities of user-generated text that are not covered in this work but have been investigated before include metaphor (Jang et al., 2014) and tolerance (Mukherjee et al., 2013) in online discussion 3.1 Thread labels Agreement The overall agreement present in a thread. • Agreement throughout • Continual disagreement • Agreement → disagreement: Begins with"
W17-0802,P05-1015,0,0.0204434,"n reddit (Tan et al., 2016; Wei et al., 2016). Politeness was labeled and identified in Stack Exchange and Wikipedia discussions (Danescu-NiculescuMizil et al., 2013). Some previous work focused on detecting agreement has considered blog and Wikipedia discussions (Andreas et al., 2012) and debate forums (Skeppstedt et al., 2016). Sarcasm has been identified in a corpus of microblogs identified with the hashtag #sarcasm on Twitter (Gonz´alez-Ib´anez et al., 2011; Davidov et al., 2010) and in online forums (Oraby et al., 2016). Sentiment has been studied widely, often in the context of reviews (Pang and Lee, 2005), and in the context of user-generated exchanges, positive and negative attitudes have been identified in Usenet discussions (Hassan et al., 2010). Other qualities of user-generated text that are not covered in this work but have been investigated before include metaphor (Jang et al., 2014) and tolerance (Mukherjee et al., 2013) in online discussion 3.1 Thread labels Agreement The overall agreement present in a thread. • Agreement throughout • Continual disagreement • Agreement → disagreement: Begins with agreement which turns into disagreement. • Disagreement → agreement: Starts with disagree"
W17-0802,walker-etal-2012-corpus,0,\N,Missing
W17-0802,W16-2818,0,\N,Missing
W17-5007,C14-1185,0,0.251184,"Missing"
W17-5007,W17-5023,0,0.0297696,"iple kernel learning. WLZ (Li and Zou, 2017) build an ensemble of single-feature SVMs fed into a multi-layer perceptron (MLP), which is a meta-classifier trained on the outputs of the base SVM classifiers. The single features are based on lexical and syntactic information and the best submission includes character, word, stem, and function word n-grams as well as syntactic dependencies. Essay-only Track The best essay-only submission for each team, along with rankings and other details, are listed in Table 4.1. Each team’s best system is briefly described below, ordered by rankings. Uvic-NLP (Chan et al., 2017) trained a single SVM model on word n-grams (1–3) and character n-grams (4-5). They also conducted several postevaluation experiments, improving their results to 0.8730 using an LDA meta-classifier trained on individual SVM classifiers. ItaliaNLP Lab (Cimino and Dell’Orletta, 2017) utilize a novel classifier stacking approach based on a sentence-level classifier whose predictions are used by a second document-level classifier. The sentence classifier is based on a Logistic Regression model trained on standard lexical, stylistic, and syntactic NLI features. The documentclassifier is an SVM, tra"
W17-5007,W17-5049,0,0.19025,"Missing"
W17-5007,W17-5041,0,0.0444426,"Missing"
W17-5007,W17-5024,0,0.0496381,"trained on word bigrams and character 7-grams. They tried a variety of n-gram combinations and found this to work best on the development data. A total of 19 teams participated in the task, 17 of which submitted system description papers. Participation across the three tracks varied, with 17 participants in the essay-only track, 9 in the speech-only track, and 10 in the fusion track. The results for each track are described in the following sections. For every track we briefly outline each team’s best system. Interested readers can refer to the team’s paper for more details. 4.1 UnibucKernel (Ionescu and Popescu, 2017) use different types of character-level string kernels which are combined with multiple kernel learning. WLZ (Li and Zou, 2017) build an ensemble of single-feature SVMs fed into a multi-layer perceptron (MLP), which is a meta-classifier trained on the outputs of the base SVM classifiers. The single features are based on lexical and syntactic information and the best submission includes character, word, stem, and function word n-grams as well as syntactic dependencies. Essay-only Track The best essay-only submission for each team, along with rankings and other details, are listed in Table 4.1."
W17-5007,W17-5025,0,0.0281986,"Missing"
W17-5007,D14-1142,1,0.743654,"Missing"
W17-5007,W17-5021,0,0.0649744,"Missing"
W17-5007,W13-1714,0,0.641856,"Missing"
W17-5007,P17-1134,1,0.867612,"NLI works by identifying language use patterns that are common to certain groups of speakers that share the same native language. This process is underpinned by the presupposition that an author’s linguistic background will dispose them towards particular language production patterns in their learned languages, as influenced by their mother tongue. Predicting the native language of a writer has applications in different fields. It can be used for authorship identification (Estival et al., 2007), forensic analysis (Gibbons, 2003), tracing linguistic influence in potentially multi-author texts (Malmasi et al., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and"
W17-5007,W15-0620,1,0.926713,"Missing"
W17-5007,W17-5048,0,0.0366318,"aseline: essay/transcript/i-vector 0.7901 0.7909 SVM trained on word unigrams (essay/transcript) + i-vectors Baseline: Essay + Transcript 0.7786 0.7791 Linear SVM trained on word unigrams (essays + transcripts) 4 ut.dsp 0.7748 0.7764 n-gram language models over chars/words (essay+transcript) 5 ltl 0.7346 0.7345 No paper submitted. 0.0910 0.0910 Randomly select an L1 Random Baseline Table 3: Official results in the fusion track. The official metric is the macro-averaged F1-score. Accuracy (Acc.) is also reported. Team rankings are determined by statistical significance testing (see §3.1). L2F (Kepler et al., 2017) designed a system that combined three types of text-based classifiers (an RNN with a bidirectional GRU layer, a Naive Bayes classifier with byte n-grams, and a Naive Bayes classifier with n-grams based on representations of the words using Byte Pair Encoding) with versions of the i-vector features that were postprocessed using centering and whitening in an attempt to reduce channel variability. These classifiers were combined together in a Neural Network fusion approach and the authors demonstrated that the i-vector features were the main driver of performance. ETRI-SLP (Oh et al., 2017) subm"
W17-5007,W16-4801,1,0.85993,"Missing"
W17-5007,W17-5043,0,0.116863,"Missing"
W17-5007,W17-5042,0,0.105,"labels. Their experiments indicate that inclusion of the sentence prediction features provides a small increase in performance. ETRI-SLP (Oh et al., 2017) designed a system that was based on word n-gram features (with n ranging from 1 to 3) and character n-gram features (with n ranging from 4 to 6). The normalized count vectors based on these features were used to extract LSA features, which were then reduced using LDA. The count and LSA-LDA features were used to train SVM and DNN classifiers whose outputs were subsequently combined via late fusion in a DNN-based ensemble classifier. CIC-FBK (Markov et al., 2017) build an SVM with multiple lexical and syntactic features. They introduce two new feature types – typed character n-grams and syntactic n-grams – and combine them with word, lemma, and POS n-grams, function words, and spelling error character n-grams. Features are weighted using log-entropy. CEMI (Ircing et al., 2017) use a Logistic Regression meta-classifier to achieve their best essay-only results. The meta-classifier is trained on the outputs of several base classifiers, which are trained on TF-IDF weighted word unigrams, word bigrams, character n-grams and POS n-grams. Groningen (Kulmizev"
W17-5007,W17-5044,0,0.0282337,"t data. A total of 19 teams participated in the task, 17 of which submitted system description papers. Participation across the three tracks varied, with 17 participants in the essay-only track, 9 in the speech-only track, and 10 in the fusion track. The results for each track are described in the following sections. For every track we briefly outline each team’s best system. Interested readers can refer to the team’s paper for more details. 4.1 UnibucKernel (Ionescu and Popescu, 2017) use different types of character-level string kernels which are combined with multiple kernel learning. WLZ (Li and Zou, 2017) build an ensemble of single-feature SVMs fed into a multi-layer perceptron (MLP), which is a meta-classifier trained on the outputs of the base SVM classifiers. The single features are based on lexical and syntactic information and the best submission includes character, word, stem, and function word n-grams as well as syntactic dependencies. Essay-only Track The best essay-only submission for each team, along with rankings and other details, are listed in Table 4.1. Each team’s best system is briefly described below, ordered by rankings. Uvic-NLP (Chan et al., 2017) trained a single SVM mode"
W17-5007,W17-5022,0,0.0430648,"). GadjahMada (Sari et al., 2017) apply a character embedding model with a feed-forward neural network classifier in the essay track. This is based on the relatively high performance of character ngrams in previous research. An embedding size of 25 was used with n-grams of length 2–5. NLI-ISU (Vajjala and Banerjee, 2017) explored the use of n-grams and embeddings in their submissions. Their best run was a Logistic Regression model trained on word 1-3 grams. They also report that spell checking features, as well as word and document embeddings did not work well on the development data. ut.dsp (Mohammadi et al., 2017) utilize n-gram language models over words and characters. For each L1, a language model over character 3- and 4-grams as well as word unigrams and bigrams is calculated and smoothing is applied. For each text in the test set, the probably of the whole text for all language models in each class is calculated and the class with the maximum probability is chosen as the predicted label. This approach does not involve any supervised learning. ¨ IUCL (Smiley and Kubler, 2017) investigated the use of phonetic features for the essay classification task based on the hypothesis that speakers from diffe"
W17-5007,W15-0606,1,0.926679,"vidence from various participants suggests that high-order character n-grams (as high as n = 10) are extremely useful for this task. This is likely because when extracted across word boundaries, these features capture not only sub-word (e.g. morphological) information, but also dependencies between words. However, it should also be noted that the top systems in all tracks made use of syntactic features which can give them a slight performance boost. This is not surprising as it has been shown that lexical and syntactic features each capture diverse types of information that are complementary (Malmasi and Cahill, 2015). Average performance is much higher than 2013. Although much of the training data remains the same, the submissions were much more competitive than the first NLI shared tasks. This is likely due to NLI being a much more established task, as well as the aforementioned prevalence of more sophisticated models such as metaclassifiers. A number of open questions remain. For example, it is not clear if any one approach is dominant across all tracks as most of the top-ranked teams in the essay track did not participate in the other tracks. It is hard to say how well their systems would have done in"
W17-5007,W17-5047,0,0.0363074,"ed on individual SVM classifiers. ItaliaNLP Lab (Cimino and Dell’Orletta, 2017) utilize a novel classifier stacking approach based on a sentence-level classifier whose predictions are used by a second document-level classifier. The sentence classifier is based on a Logistic Regression model trained on standard lexical, stylistic, and syntactic NLI features. The documentclassifier is an SVM, trained using the same features, as well as the sentence prediction labels. Their experiments indicate that inclusion of the sentence prediction features provides a small increase in performance. ETRI-SLP (Oh et al., 2017) designed a system that was based on word n-gram features (with n ranging from 1 to 3) and character n-gram features (with n ranging from 4 to 6). The normalized count vectors based on these features were used to extract LSA features, which were then reduced using LDA. The count and LSA-LDA features were used to train SVM and DNN classifiers whose outputs were subsequently combined via late fusion in a DNN-based ensemble classifier. CIC-FBK (Markov et al., 2017) build an SVM with multiple lexical and syntactic features. They introduce two new feature types – typed character n-grams and syntact"
W17-5007,D14-1144,1,0.925479,"Missing"
W17-5007,N01-1031,0,0.692118,"lly or via Automatic Speech Recognition) and audio features for dialect identification (Malmasi et al., 2016), a task that involves identifying specific dialects of pluricentric languages, such as Spanish or Arabic.1 The combination of transcripts and acoustic features has also provided good results for dialect identification (Zampieri et al., 2017b), demonstrating that it is possible to improve performance by combining this information. While there has been growing interest in using such features, the use of speech transcripts for NLI is not entirely new. In fact, the very first NLI study by Tomokiyo and Jones (2001) was based on applying a Naive Bayes classifier to transcriptions of speech from native and non-native speakers, albeit using limited data. However, this strand of NLI research has not received much attention, most likely due to the costly and laborious nature of collecting and transcribing non-native speech. Following this trend, the 2016 Computational Paralinguistics Challenge (Schuller et al., 2016) also included an NLI task based on the spoken response using the raw audio. The NLI Shared Task 2017 attempts to combine these approaches by including a written response (essay) and a spoken res"
W17-5007,W17-5028,0,0.0402701,"Missing"
W17-5007,W07-0602,0,0.514904,"author texts (Malmasi et al., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of L1s is known a priori. Two previous shared tasks on"
W17-5007,W17-5026,0,0.110995,"N classifier 0.7104 0.7109 Linear SVM trained on word unigrams 0.0910 0.0910 Randomly select an L1 Table 1: Official results in the essay-only track. The official metric is the macro-averaged F1-score. Accuracy (Acc.) is also reported. Rankings are determined by statistical significance testing (see §3.1). GadjahMada (Sari et al., 2017) apply a character embedding model with a feed-forward neural network classifier in the essay track. This is based on the relatively high performance of character ngrams in previous research. An embedding size of 25 was used with n-grams of length 2–5. NLI-ISU (Vajjala and Banerjee, 2017) explored the use of n-grams and embeddings in their submissions. Their best run was a Logistic Regression model trained on word 1-3 grams. They also report that spell checking features, as well as word and document embeddings did not work well on the development data. ut.dsp (Mohammadi et al., 2017) utilize n-gram language models over words and characters. For each L1, a language model over character 3- and 4-grams as well as word unigrams and bigrams is calculated and smoothing is applied. For each text in the test set, the probably of the whole text for all language models in each class is"
W17-5007,P11-1093,0,0.0862778,"s, as influenced by their mother tongue. Predicting the native language of a writer has applications in different fields. It can be used for authorship identification (Estival et al., 2007), forensic analysis (Gibbons, 2003), tracing linguistic influence in potentially multi-author texts (Malmasi et al., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Iden"
W17-5007,U09-1008,0,0.367747,"l., 2017), and naturally to support Second Language Acquisition research (Malmasi and Dras, 2014). It can also be used in educational applications such as developing grammatical error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of L1s is known a priori. Two previous shared tasks on NLI have been organiz"
W17-5007,W17-5027,0,0.03595,"els over characters (3-4) and words (1-2) Word Unigram Baseline Random Baseline 0.8318 0.8264 0.8264 0.8110 Ensemble of resnets, LSTM and document embeddings Logistic Regression model with word n-grams (1-3) Phonetic features combined in an SVM Char embeddings w/ a feed-forward NN classifier 0.7104 0.7109 Linear SVM trained on word unigrams 0.0910 0.0910 Randomly select an L1 Table 1: Official results in the essay-only track. The official metric is the macro-averaged F1-score. Accuracy (Acc.) is also reported. Rankings are determined by statistical significance testing (see §3.1). GadjahMada (Sari et al., 2017) apply a character embedding model with a feed-forward neural network classifier in the essay track. This is based on the relatively high performance of character ngrams in previous research. An embedding size of 25 was used with n-grams of length 2–5. NLI-ISU (Vajjala and Banerjee, 2017) explored the use of n-grams and embeddings in their submissions. Their best run was a Logistic Regression model trained on word 1-3 grams. They also report that spell checking features, as well as word and document embeddings did not work well on the development data. ut.dsp (Mohammadi et al., 2017) utilize n"
W17-5007,W17-5045,0,0.0520433,"nd the transcriptions, and a second SVM combining the unigrams from the essays and the transcriptions with the i-vectors. The test period for each track lasted 3 days, and teams could submit up to 12 systems per track. The essay-only and speech-only test phases ran concurrently. The IDs for the essay data and transcription data were generated by separate random processes for this test period. For the fusion test period, an updated package providing linked IDs between the essay and spoken transcription data was released. 2 3 http://kaldi-asr.org 65 For more details see §7.3 of Malmasi and Dras (2017) 4 Results tubasfs (Rama and C ¸ o¨ ltekin, 2017) used a single SVM classifier trained on word bigrams and character 7-grams. They tried a variety of n-gram combinations and found this to work best on the development data. A total of 19 teams participated in the task, 17 of which submitted system description papers. Participation across the three tracks varied, with 17 participants in the essay-only track, 9 in the speech-only track, and 10 in the fusion track. The results for each track are described in the following sections. For every track we briefly outline each team’s best system. Intere"
W17-5007,W17-1201,1,0.784529,"Missing"
W17-5007,W17-5046,0,0.0229943,"Missing"
W17-5007,W13-1706,1,0.61284,"cal error correction systems which can personalize their feedback and model performance to the native language of the user (Rozovskaya and Roth, 2011). Most work in NLI focused on predicting the native language of an ESL (English as a Second Language) writer based on a sample essay, although NLI has also been shown to work on other languages (Malmasi and Dras, 2015). Work by Koppel et al. (2005), Tsur and Rappoport (2007) Wong and Dras (2009), and Tetreault et al. (2012) set the stage for much of the recent research efforts. However, it was the 2013 Native Language Identification Shared Task (Tetreault et al., 2013) that led to an explosion of interest in this area by making public a large dataset developed specifically Native Language Identification (NLI) is the task of automatically identifying the native language (L1) of an individual based on their language production in a learned language. It is typically framed as a classification task where the set of L1s is known a priori. Two previous shared tasks on NLI have been organized where the aim was to identify the L1 of learners of English based on essays (2013) and spoken responses (2016) they provided during a standardized assessment of academic Engl"
W17-5007,C12-1158,1,0.924147,"Missing"
W17-5019,N12-1067,0,0.0441724,"progress even further (§5). We look forward to discussing this proposal with the community and to refine a shared task for 2018. Introduction 2 In the field of grammatical error correction (GEC), the Helping Our Own shared tasks in 2011 (Dale and Kilgarriff, 2011) and 2012 (Dale et al., 2012), and then the CoNLL shared tasks of 2013 (Ng et al., 2013) and 2014 (Ng et al., 2014) marked a sea change. For the first time there were public datasets, most notably the NUS Corpus of Learner English (NUCLE; Dahlmeier et al., 2013), and evaluation metrics, of which the most commonly used to date is M2 (Dahlmeier and Ng, 2012). This has allowed researchers from other fields, such as machine translation, to enter GEC more easily. It has also enabled new developments, with many papers published on metrics, new algorithms (most recently neural methods), and occasionally new datasets. Even with the accelerated progress in GEC, problems yet remain in the field. The use of specific datasets may be GEC’s worst enemy, as system and even evaluation metric development rely too heavily on the NUCLE test set. While probably one of the most important contributions to the field’s development to date, the lack of publicly availab"
W17-5019,W13-1703,0,0.20863,"We finish with a recommendation for a new community-driven shared task that will help the field progress even further (§5). We look forward to discussing this proposal with the community and to refine a shared task for 2018. Introduction 2 In the field of grammatical error correction (GEC), the Helping Our Own shared tasks in 2011 (Dale and Kilgarriff, 2011) and 2012 (Dale et al., 2012), and then the CoNLL shared tasks of 2013 (Ng et al., 2013) and 2014 (Ng et al., 2014) marked a sea change. For the first time there were public datasets, most notably the NUS Corpus of Learner English (NUCLE; Dahlmeier et al., 2013), and evaluation metrics, of which the most commonly used to date is M2 (Dahlmeier and Ng, 2012). This has allowed researchers from other fields, such as machine translation, to enter GEC more easily. It has also enabled new developments, with many papers published on metrics, new algorithms (most recently neural methods), and occasionally new datasets. Even with the accelerated progress in GEC, problems yet remain in the field. The use of specific datasets may be GEC’s worst enemy, as system and even evaluation metric development rely too heavily on the NUCLE test set. While probably one of t"
W17-5019,P17-1074,0,0.224972,"o metric directly measures meaning preservation. This means that a system could produce a more fluent version of the original but accidentally change one word, and that could change the meaning of the whole sentence. For example, if a system accidentally corrected documentary to document in “The documentary gave a nice summary of global warming.” By current metrics, that error would have the same penalty as a minor spelling mistake. Finally, the most commonly used GEC metric, M2 , has a serious weakness, which has been noted in earlier papers (Felice and Briscoe, 2015; Sakaguchi et al., 2016; Bryant et al., 2017). The phrasal alignments under-penalize a sequence of incorrect tokens, and to illustrate how troubling this is, we tested a series of dummy systems, where each system produces the same sentence regardless of input (the sentences produced by each system are a, a a, and a a a). Table 2 shows their scores on the CoNLL 2014 test set evaluated on the official NUCLE references (without alternatives), compared to the top 3 systems in the shared task, CAMB14 (Felice et al., 2014), CUUI14 (Rozovskaya et al., 2014), and AMU14 (JunczysDowmunt and Grundkiewicz, 2014). The reader will notice that GLEU and"
W17-5019,W12-2006,0,0.119584,"Missing"
W17-5019,P15-1068,0,0.340127,"f millions or even tens of millions of sentence pairs. The largest GEC datasets barely approach that figure, with 2.5 million sentences at a maximum, a number which includes sentences that were not corrected. Table 1 summarizes the strengths and weaknesses of the most commonly used GEC corpora across different properties ranging from size to diversity in native language (L1). The most notable weakness across corpora is the lack of multiple reference corrections. NUCLE contains two corrections per sentence and JFLEG 4. M2 and GLEU scores increase with more references but at a diminishing rate (Bryant and Ng, 2015; Sakaguchi et al., 2016). Further investigation is warranted to determine what an ideal number of references is, given the trade off between cost and reliability. Some corpora contain little diversity in proficiency, topic, and/or native language of the writers (namely NUCLE and AESW), however AESW is the only corpus to contain sentences by native English speakers. 3.2 A third issue is that no metric directly measures meaning preservation. This means that a system could produce a more fluent version of the original but accidentally change one word, and that could change the meaning of the who"
W17-5019,W11-2838,0,0.0726649,"ir impact on downstream users, except, e.g., Nagata and Nakatani (2010); Chodorow et al. (2010). In this short paper, we take stock of the current state of GEC (§2) and its limitations (§3), and outline where we believe the field should be five years from now (§4). We finish with a recommendation for a new community-driven shared task that will help the field progress even further (§5). We look forward to discussing this proposal with the community and to refine a shared task for 2018. Introduction 2 In the field of grammatical error correction (GEC), the Helping Our Own shared tasks in 2011 (Dale and Kilgarriff, 2011) and 2012 (Dale et al., 2012), and then the CoNLL shared tasks of 2013 (Ng et al., 2013) and 2014 (Ng et al., 2014) marked a sea change. For the first time there were public datasets, most notably the NUS Corpus of Learner English (NUCLE; Dahlmeier et al., 2013), and evaluation metrics, of which the most commonly used to date is M2 (Dahlmeier and Ng, 2012). This has allowed researchers from other fields, such as machine translation, to enter GEC more easily. It has also enabled new developments, with many papers published on metrics, new algorithms (most recently neural methods), and occasiona"
W17-5019,N13-1055,1,0.857554,"pus (JFLEG) is a small dataset for tuning and evaluating GEC systems. 1.5k sentences are taken from the GUG corpus (Heilman et al., 2014), which labels sentences with an ordinal grammaticality score. In JFLEG, each sentence is corrected four times for grammaticality and fluency (Sakaguchi et al., 2016). 2.2 3 3.1 Limitations Problems with Datasets As we saw in the previous section, the majority of the commonly used datasets are limited to students, specifically college-level ESL writers. To date, the overwhelmingly majority of publications benchmark on NUCLE, save for a few exceptions such as Cahill et al. (2013) and Rei and Yannakoudakis (2016) which means that research efforts are becoming over-optimized for one set. This lack of diversity means that it is not clear how systems perform on other genres under different training conditions. We should look to the parsing community as a warning sign. For well over a decade, the field was heavily focused on improving parsing accuracy on the Penn Treebank (Marcus et al., 1993), but robustness was greatly improved with the advent of Ontonotes (Hovy et al., 2006) and the Google Web Treebank (Petrov and Evaluation Precision, recall, and F-score have been used"
W17-5019,W16-0506,1,0.917472,"Missing"
W17-5019,N15-1060,0,0.0657203,"tive English speakers. 3.2 A third issue is that no metric directly measures meaning preservation. This means that a system could produce a more fluent version of the original but accidentally change one word, and that could change the meaning of the whole sentence. For example, if a system accidentally corrected documentary to document in “The documentary gave a nice summary of global warming.” By current metrics, that error would have the same penalty as a minor spelling mistake. Finally, the most commonly used GEC metric, M2 , has a serious weakness, which has been noted in earlier papers (Felice and Briscoe, 2015; Sakaguchi et al., 2016; Bryant et al., 2017). The phrasal alignments under-penalize a sequence of incorrect tokens, and to illustrate how troubling this is, we tested a series of dummy systems, where each system produces the same sentence regardless of input (the sentences produced by each system are a, a a, and a a a). Table 2 shows their scores on the CoNLL 2014 test set evaluated on the official NUCLE references (without alternatives), compared to the top 3 systems in the shared task, CAMB14 (Felice et al., 2014), CUUI14 (Rozovskaya et al., 2014), and AMU14 (JunczysDowmunt and Grundkiewic"
W17-5019,C12-1038,1,0.89541,"e whole effort more community-driven and takes the pressure off one group from having to supply all annotations. 4.3 Consensus on Goals and Applications As a corollary to data and metrics, the end-goal of GEC also needs to be refined within the community. Initial approaches to GEC seemed to focus on providing feedback to English language learners where specific error types would be targeted and feedback would be given in terms of detection or possible corrections. The work was also motivated by concurrent work in using NLP for automatic essay scoring (for example, Attali and Burstein (2006)). Chodorow et al. (2012) noted several other applications for GEC: improving overall writing quality for both native and nonnative writers, assistive language learning, and applications within NLP (such as post-editing in MT). More recently the field has drifted to “whole sentence GEC” using statistical or neural MT approaches. In this situation, the writer simply gets a complete rewrite of their sentence, which may be useful as an instructional tool in some circumstances, but not all. There is no consensus on what the focus application(s) should be. Which application determines which methods and which evaluation met"
W17-5019,W14-1702,0,0.0266291,"Missing"
W17-5019,D16-1195,0,0.138279,"his is through a GEC Turing Test, where system outputs are blindly judged alongside human corrections of the same sentences. If human adjudicators think the system outputs are indistinguishable in quality from the human corrections (for example, given a set of criteria such as being good corrections, meaning preserving and native-sounding) then that is a very strong signal that GEC has attained human-level performance. To illustrate the shortcomings of current metrics, Table 3 contains a JFLEG sentence corrected by current leading systems (AMU16 (JunczysDowmunt and Grundkiewicz, 2016); NUS16 (Chollampatt et al., 2016); CAMB16 (Yuan and Briscoe, 2016)) and the automatic metric scores.2 Notice that the CAMB16 sentence, which changes tooth pastes → tooth problems, is ranked the highest system output by GLEU and the second highest by IM and M2 . All metrics score it higher than the unchanged Source sentence. Another issues evidenced in the table is that IM and M2 score the imperfect correction (CAMB14) as better than Reference; and according to M2 , the Dummy output is better than Source. We believe that the GEC field should take In this section we outline our recommendations for how the field should develop."
W17-5019,W16-0511,0,0.0537485,"Missing"
W17-5019,E17-2037,1,0.752535,"greatly improved with the advent of Ontonotes (Hovy et al., 2006) and the Google Web Treebank (Petrov and Evaluation Precision, recall, and F-score have been used to evaluate GEC systems that correct targeted error types. Three additional evaluation metrics 1 Because of noise and implementation differences in sentence extraction, the size varies from 2–2.5 million sentences. 181 System “a” “a a” “a a a” Source CAMB14 CUUI14 AMU14 Src>Game Src<Sys GLEU [0,100] 0.2 0.6 1.6 57.4 64.3 64.6 64.6 3 3 IM [-100, 100] 0.0 0.0 0.0 0.0 -5.3 -2.2 -2.5 7 7 M2 P 28.4 28.7 28.7 100.0 39.7 41.8 41.6 3 7 ric (Napoles et al., 2017). However, it is important to take these results with a grain of salt—all benchmarking of the metrics was done with the CoNLL 2014 systems and data, and it remains to be seen if this ranking would hold on other, larger datasets. [0, 100] R F0.5 31.3 28.9 31.8 29.3 32.0 29.4 0.0 0.0 30.1 37.3 24.9 36.8 21.4 35.0 7 7 3 3 Another issue with the metrics is the number of references available for comparison. As in machine translation, the more references (humangenerated gold-standard corrections) one has, the better one can evaluate a system. The CoNLL 2014 test set has 18 references annotated, but"
W17-5019,P14-2029,1,0.891984,"Missing"
W17-5019,W14-1701,0,0.294607,"stock of the current state of GEC (§2) and its limitations (§3), and outline where we believe the field should be five years from now (§4). We finish with a recommendation for a new community-driven shared task that will help the field progress even further (§5). We look forward to discussing this proposal with the community and to refine a shared task for 2018. Introduction 2 In the field of grammatical error correction (GEC), the Helping Our Own shared tasks in 2011 (Dale and Kilgarriff, 2011) and 2012 (Dale et al., 2012), and then the CoNLL shared tasks of 2013 (Ng et al., 2013) and 2014 (Ng et al., 2014) marked a sea change. For the first time there were public datasets, most notably the NUS Corpus of Learner English (NUCLE; Dahlmeier et al., 2013), and evaluation metrics, of which the most commonly used to date is M2 (Dahlmeier and Ng, 2012). This has allowed researchers from other fields, such as machine translation, to enter GEC more easily. It has also enabled new developments, with many papers published on metrics, new algorithms (most recently neural methods), and occasionally new datasets. Even with the accelerated progress in GEC, problems yet remain in the field. The use of specific"
W17-5019,N06-2015,0,0.0640555,"overwhelmingly majority of publications benchmark on NUCLE, save for a few exceptions such as Cahill et al. (2013) and Rei and Yannakoudakis (2016) which means that research efforts are becoming over-optimized for one set. This lack of diversity means that it is not clear how systems perform on other genres under different training conditions. We should look to the parsing community as a warning sign. For well over a decade, the field was heavily focused on improving parsing accuracy on the Penn Treebank (Marcus et al., 1993), but robustness was greatly improved with the advent of Ontonotes (Hovy et al., 2006) and the Google Web Treebank (Petrov and Evaluation Precision, recall, and F-score have been used to evaluate GEC systems that correct targeted error types. Three additional evaluation metrics 1 Because of noise and implementation differences in sentence extraction, the size varies from 2–2.5 million sentences. 181 System “a” “a a” “a a a” Source CAMB14 CUUI14 AMU14 Src>Game Src<Sys GLEU [0,100] 0.2 0.6 1.6 57.4 64.3 64.6 64.6 3 3 IM [-100, 100] 0.0 0.0 0.0 0.0 -5.3 -2.2 -2.5 7 7 M2 P 28.4 28.7 28.7 100.0 39.7 41.8 41.6 3 7 ric (Napoles et al., 2017). However, it is important to take these res"
W17-5019,W14-1703,0,0.0347887,"Missing"
W17-5019,W13-3601,1,0.949916,"n this short paper, we take stock of the current state of GEC (§2) and its limitations (§3), and outline where we believe the field should be five years from now (§4). We finish with a recommendation for a new community-driven shared task that will help the field progress even further (§5). We look forward to discussing this proposal with the community and to refine a shared task for 2018. Introduction 2 In the field of grammatical error correction (GEC), the Helping Our Own shared tasks in 2011 (Dale and Kilgarriff, 2011) and 2012 (Dale et al., 2012), and then the CoNLL shared tasks of 2013 (Ng et al., 2013) and 2014 (Ng et al., 2014) marked a sea change. For the first time there were public datasets, most notably the NUS Corpus of Learner English (NUCLE; Dahlmeier et al., 2013), and evaluation metrics, of which the most commonly used to date is M2 (Dahlmeier and Ng, 2012). This has allowed researchers from other fields, such as machine translation, to enter GEC more easily. It has also enabled new developments, with many papers published on metrics, new algorithms (most recently neural methods), and occasionally new datasets. Even with the accelerated progress in GEC, problems yet remain in the"
W17-5019,P02-1040,0,0.0988565,"Missing"
W17-5019,D16-1161,0,0.0221286,"Missing"
W17-5019,J93-2004,0,0.0628557,"Missing"
W17-5019,W14-1704,0,0.029893,"Missing"
W17-5019,Q16-1013,1,0.926664,"ns of millions of sentence pairs. The largest GEC datasets barely approach that figure, with 2.5 million sentences at a maximum, a number which includes sentences that were not corrected. Table 1 summarizes the strengths and weaknesses of the most commonly used GEC corpora across different properties ranging from size to diversity in native language (L1). The most notable weakness across corpora is the lack of multiple reference corrections. NUCLE contains two corrections per sentence and JFLEG 4. M2 and GLEU scores increase with more references but at a diminishing rate (Bryant and Ng, 2015; Sakaguchi et al., 2016). Further investigation is warranted to determine what an ideal number of references is, given the trade off between cost and reliability. Some corpora contain little diversity in proficiency, topic, and/or native language of the writers (namely NUCLE and AESW), however AESW is the only corpus to contain sentences by native English speakers. 3.2 A third issue is that no metric directly measures meaning preservation. This means that a system could produce a more fluent version of the original but accidentally change one word, and that could change the meaning of the whole sentence. For example,"
W17-5019,P15-2097,1,0.885606,"Missing"
W17-5019,P12-2039,0,0.0369813,"Missing"
W17-5019,P11-1019,0,0.0777998,"Missing"
W17-5019,N16-1042,0,0.0123477,"here system outputs are blindly judged alongside human corrections of the same sentences. If human adjudicators think the system outputs are indistinguishable in quality from the human corrections (for example, given a set of criteria such as being good corrections, meaning preserving and native-sounding) then that is a very strong signal that GEC has attained human-level performance. To illustrate the shortcomings of current metrics, Table 3 contains a JFLEG sentence corrected by current leading systems (AMU16 (JunczysDowmunt and Grundkiewicz, 2016); NUS16 (Chollampatt et al., 2016); CAMB16 (Yuan and Briscoe, 2016)) and the automatic metric scores.2 Notice that the CAMB16 sentence, which changes tooth pastes → tooth problems, is ranked the highest system output by GLEU and the second highest by IM and M2 . All metrics score it higher than the unchanged Source sentence. Another issues evidenced in the table is that IM and M2 score the imperfect correction (CAMB14) as better than Reference; and according to M2 , the Dummy output is better than Source. We believe that the GEC field should take In this section we outline our recommendations for how the field should develop. 4.1 Data As the world’s communica"
W18-5023,J95-2003,0,0.964535,"Missing"
W18-5023,J08-1001,0,0.946471,"ining URLs (as they often quote writing from other sources) and texts with too many line breaks (usually lists). 2.4 Annotation We collected coherence judgments both from expert raters with prior linguistic annotation experience, as in Burstein et al. (2010) and from untrained raters via Amazon Mechanical Turk. This allows us to assess the efficacy of using untrained raters for this task. We asked the raters to rate the coherence of each text on a 3-point scale from 1 (low coherence) to 3 (high coherence) given the following instructions, which are based on prior coherence annotation efforts (Barzilay and Lapata, 2008; Burstein et al., 2013): A text with high coherence is easy to understand, well-organized, and contains only details that support the main point of the text. A text with low coherence is difficult to understand, not well organized, or contains unnecessary details. Try to ignore the effects of grammar or spelling errors when assigning a coherence rating. 2.5 Grammarly Corpus of Discourse Coherence The resulting four domains each contain 1200 texts (1000 for training, 200 for testing). Each text has been scored as {low, medium, high} coherence by 5 MTurk raters and 3 expert raters. There is one"
W18-5023,N10-1099,1,0.958252,"ta, 2005), uses human coherence judgments, but include machine-generated texts. Coherence models are only required to identify which of a pair of texts is more coherent (presumably identifying human-written texts). The line of work most closely related to our approach is the application of coherence modeling to automated essay scoring. Essays are written by test-takers, not professional writers, so they are not assumed to be coherent. Manual annotation is required to assign the essay an overall quality score (Feng et al., 2014) or to rate the coherence of the essay (Somasundaran et al., 2014; Burstein et al., 2010, 2013). While this line of work goes beyond sentence ordering to examine the qualities of a low-coherence text, it has only been applied to test-taker essays. In contrast to previous datasets, we collect writ2.2 Domains For a robust evaluation, we selected domains that reflect what an average person writes on a regular basis: forum posts, emails, and product reviews. For online forum posts, we sampled responses from the Yahoo Answers L6 corpus2 for the Yahoo domain. For emails, we used the State Department’s release of emails from Hillary Clinton’s office3 and emails from the Enron Corpus4 to"
W18-5023,P03-1069,0,0.12585,"ly. We present a corpus that contains texts from four domains, covering a range of coherence, each annotated with a document-level coherence score. In Sections 2.22.6, we describe our data collection process and the characteristics of the resulting corpus. herent, and any reordering of the same sentences is less coherent. Presented with a pair of texts – the original and a random permutation of the same sentences – a coherence model should be able to identify the original text. More challenging versions of this task (sentence insertion (Elsner and Charniak, 2011) and paragraph reconstruction (Lapata, 2003; Li and Jurafsky, 2017)) all assume that the original text is perfectly coherent. Datasets for the sentence ordering task tend to use texts that have been professionally written and extensively edited. These have included the Accidents and Earthquakes datasets (Barzilay and Lapata, 2005), the Wall Street Journal (Elsner and Charniak, 2008, 2011; Lin et al., 2011; Feng et al., 2014; Tien Nguyen and Joty, 2017), and Wikipedia (Li and Jurafsky, 2017). Another task, summary evaluation (Barzilay and Lapata, 2005), uses human coherence judgments, but include machine-generated texts. Coherence model"
W18-5023,P15-1136,0,0.0619819,"Missing"
W18-5023,D14-1218,0,0.517147,"Missing"
W18-5023,P08-2011,0,0.73949,"taset, Evaluation and Methods Alice Lai University of Illinois at Urbana-Champaign∗ aylai2@illinois.edu Abstract 1991; Somasundaran et al., 2014), discourse relations (Lin et al., 2011; Feng et al., 2014), and syntactic features (Louis and Nenkova, 2012). Neural networks have also been successfully applied to coherence (Li and Hovy, 2014; Tien Nguyen and Joty, 2017; Li and Jurafsky, 2017). However, until now, these approaches have not been benchmarked on a common dataset. Past work has focused on the discourse coherence of well-formed texts in domains like newswire (Barzilay and Lapata, 2005; Elsner and Charniak, 2008) via tasks like sentence ordering that use artificially constructed data. It was unknown how well the best methods would fare on real-world data that most people generate. In this work, we seek to address the above deficiencies via four main contributions. First, we present a new corpus, the Grammarly Corpus of Discourse Coherence (GCDC), for real-world discourse coherence. The corpus contains texts the average person might write, e.g. emails and online reviews, each with a coherence rating from expert annotators (see examples in Table 1 and supplementary material). Second, we introduce two si"
W18-5023,D17-1019,0,0.708504,"a corpus that contains texts from four domains, covering a range of coherence, each annotated with a document-level coherence score. In Sections 2.22.6, we describe our data collection process and the characteristics of the resulting corpus. herent, and any reordering of the same sentences is less coherent. Presented with a pair of texts – the original and a random permutation of the same sentences – a coherence model should be able to identify the original text. More challenging versions of this task (sentence insertion (Elsner and Charniak, 2011) and paragraph reconstruction (Lapata, 2003; Li and Jurafsky, 2017)) all assume that the original text is perfectly coherent. Datasets for the sentence ordering task tend to use texts that have been professionally written and extensively edited. These have included the Accidents and Earthquakes datasets (Barzilay and Lapata, 2005), the Wall Street Journal (Elsner and Charniak, 2008, 2011; Lin et al., 2011; Feng et al., 2014; Tien Nguyen and Joty, 2017), and Wikipedia (Li and Jurafsky, 2017). Another task, summary evaluation (Barzilay and Lapata, 2005), uses human coherence judgments, but include machine-generated texts. Coherence models are only required to i"
W18-5023,P11-2022,0,0.901084,"e the ways in which people try but fail to write coherently. We present a corpus that contains texts from four domains, covering a range of coherence, each annotated with a document-level coherence score. In Sections 2.22.6, we describe our data collection process and the characteristics of the resulting corpus. herent, and any reordering of the same sentences is less coherent. Presented with a pair of texts – the original and a random permutation of the same sentences – a coherence model should be able to identify the original text. More challenging versions of this task (sentence insertion (Elsner and Charniak, 2011) and paragraph reconstruction (Lapata, 2003; Li and Jurafsky, 2017)) all assume that the original text is perfectly coherent. Datasets for the sentence ordering task tend to use texts that have been professionally written and extensively edited. These have included the Accidents and Earthquakes datasets (Barzilay and Lapata, 2005), the Wall Street Journal (Elsner and Charniak, 2008, 2011; Lin et al., 2011; Feng et al., 2014; Tien Nguyen and Joty, 2017), and Wikipedia (Li and Jurafsky, 2017). Another task, summary evaluation (Barzilay and Lapata, 2005), uses human coherence judgments, but inclu"
W18-5023,P11-1100,0,0.247791,"original and a random permutation of the same sentences – a coherence model should be able to identify the original text. More challenging versions of this task (sentence insertion (Elsner and Charniak, 2011) and paragraph reconstruction (Lapata, 2003; Li and Jurafsky, 2017)) all assume that the original text is perfectly coherent. Datasets for the sentence ordering task tend to use texts that have been professionally written and extensively edited. These have included the Accidents and Earthquakes datasets (Barzilay and Lapata, 2005), the Wall Street Journal (Elsner and Charniak, 2008, 2011; Lin et al., 2011; Feng et al., 2014; Tien Nguyen and Joty, 2017), and Wikipedia (Li and Jurafsky, 2017). Another task, summary evaluation (Barzilay and Lapata, 2005), uses human coherence judgments, but include machine-generated texts. Coherence models are only required to identify which of a pair of texts is more coherent (presumably identifying human-written texts). The line of work most closely related to our approach is the application of coherence modeling to automated essay scoring. Essays are written by test-takers, not professional writers, so they are not assumed to be coherent. Manual annotation is"
W18-5023,C14-1089,0,0.656198,"g is useful to a range of applications. An automated coherence scoring model could provide writing feedback, e.g. identifying a missing transition between topics or highlighting a poorly organized paragraph. Such a model could also improve the quality of natural language generation systems. One approach to modeling coherence is to model the distribution of entities over sentences. The entity grid (Barzilay and Lapata, 2005), based on Centering Theory (Grosz et al., 1995), was the first of these models. Extensions to the entity grid include additional features (Elsner and Charniak, 2008, 2011; Feng et al., 2014), a graph representation (Guinaudeau and Strube, 2013; Mesgar and Strube, 2015), and neural convolutions (Tien Nguyen and Joty, 2017). Other approaches have used lexical cohesion (Morris and Hirst, ∗ Joel Tetreault Grammarly joel.tetreault@grammarly.com 2 A Corpus for Discourse Coherence 2.1 Related Work Most previous work in discourse coherence has been evaluated on a sentence ordering task that assumes each text is well-formed and perfectly co1 Research performed while at Grammarly. https://github.com/aylai/GCDC-corpus 214 Proceedings of the SIGDIAL 2018 Conference, pages 214–223, c Melbourn"
W18-5023,C14-1090,0,0.681471,"Missing"
W18-5023,D12-1106,0,0.786661,"Missing"
W18-5023,S15-1036,0,0.291717,"del could provide writing feedback, e.g. identifying a missing transition between topics or highlighting a poorly organized paragraph. Such a model could also improve the quality of natural language generation systems. One approach to modeling coherence is to model the distribution of entities over sentences. The entity grid (Barzilay and Lapata, 2005), based on Centering Theory (Grosz et al., 1995), was the first of these models. Extensions to the entity grid include additional features (Elsner and Charniak, 2008, 2011; Feng et al., 2014), a graph representation (Guinaudeau and Strube, 2013; Mesgar and Strube, 2015), and neural convolutions (Tien Nguyen and Joty, 2017). Other approaches have used lexical cohesion (Morris and Hirst, ∗ Joel Tetreault Grammarly joel.tetreault@grammarly.com 2 A Corpus for Discourse Coherence 2.1 Related Work Most previous work in discourse coherence has been evaluated on a sentence ordering task that assumes each text is well-formed and perfectly co1 Research performed while at Grammarly. https://github.com/aylai/GCDC-corpus 214 Proceedings of the SIGDIAL 2018 Conference, pages 214–223, c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics S"
W18-5023,P17-1121,0,0.788376,"issing transition between topics or highlighting a poorly organized paragraph. Such a model could also improve the quality of natural language generation systems. One approach to modeling coherence is to model the distribution of entities over sentences. The entity grid (Barzilay and Lapata, 2005), based on Centering Theory (Grosz et al., 1995), was the first of these models. Extensions to the entity grid include additional features (Elsner and Charniak, 2008, 2011; Feng et al., 2014), a graph representation (Guinaudeau and Strube, 2013; Mesgar and Strube, 2015), and neural convolutions (Tien Nguyen and Joty, 2017). Other approaches have used lexical cohesion (Morris and Hirst, ∗ Joel Tetreault Grammarly joel.tetreault@grammarly.com 2 A Corpus for Discourse Coherence 2.1 Related Work Most previous work in discourse coherence has been evaluated on a sentence ordering task that assumes each text is well-formed and perfectly co1 Research performed while at Grammarly. https://github.com/aylai/GCDC-corpus 214 Proceedings of the SIGDIAL 2018 Conference, pages 214–223, c Melbourne, Australia, 12-14 July 2018. 2018 Association for Computational Linguistics Score Text Low Should I be flattered? Even a little bit"
W18-5023,N16-1167,0,0.39038,"for the words in that sentence. The document vector is the average over all sentence vectors in that document, and is passed through a hidden layer and a softmax to produce a distribution over coherence labels. degree of its graph, so for classification we identify the thresholds that maximize accuracy on the training data. Entity grid with convolutions (EG RID C ONV) Tien Nguyen and Joty (2017) applied a convolutional neural network to the entity grid to capture long-range transitions. We use the authors’ implementation.6 3.3 Lexical Coherence Graph (L EX G RAPH) The lexical coherence graph (Mesgar and Strube, 2016) represents sentences as nodes of a graph, connecting nodes with an edge if the two sentences contain a pair of similar words (i.e. the cosine similarity of their pre-trained word vectors is greater than a threshold). From the graph, we can extract a feature vector that expresses the frequency of all k-node subgraphs. We use the authors’ implementation7 and train a random forest classifier over the feature vectors. 3.4 Paragraph sequence (PAR S EQ) The role of paragraph breaks has not been explicitly discussed in previous work. Models like EG RID assume that entity transitions have the same we"
W18-5023,J91-1002,0,0.206406,"Missing"
W18-5023,W14-1701,0,0.0338481,".238 0.294 0.278 0.279 0.161 0.117 0.169 L EX G RAPH 0.342 0.094 0.357 0.000 C LIQUE S ENTAVG PAR S EQ 0.055 0.481† 0.447 0.000 0.332 0.296 0.077 0.393† 0.373 0.146 0.199 0.112 Yahoo Train in-domain Train all data Clinton 16.6 Enron 18.4 Yelp 14.8 We relabel a text as low coherence if at least two expert annotators judged the text to be low coherence, and relabel as not low coherence otherwise. We report the F0.5 score of the low coherence class in Table 8, where precision is emphasized twice as much as recall.8 This is in line with evaluation standards in other writing feedback applications (Ng et al., 2014). Again, the neural models perform best in most domains. However, the results of this experiment in particular show that there is still a large gap between the performance of these models and what might be required for high-precision real-world applications. 4.5 Cross-Domain Classification Up to this point, we assumed that the four domains are different enough from one another that we should train separate models for each. To test 8 54.9 58.5 Test accuracy Clinton Enron 60.2 61.0 53.2 53.9 Yelp 54.4 56.5 this assumption, we train PAR S EQ, one of the top performing neural models, in one domain"
W18-5023,Q16-1005,1,0.83641,"rating a batch of generally low coherence forum data, business emails may appear to be more coherent. However, our goal is to discover the characteristics of a low coherence business email or a low coherence forum 39.2 17.4 38.6 20.6 44.2 19.4 43.4 21.8 25.3 37.0 24.7 51.1 20.9 50.7 36.7 51.1 Table 2: Distribution of coherence classes as a percentage of the training data. # types # tokens Yahoo 13,235 189,444 Clinton 15,564 220,115 Enron 13,694 223,347 Yelp 12,201 213,852 Table 3: Type and token counts in each domain. 2.6 Annotation Agreement To quantify agreement among annotators, we follow Pavlick and Tetreault (2016)’s approach to 216 Number of texts 1000 Number of paragraphs 800 Number of sentences Number of words 600 400 200 0 1 2 3 4 5+ Yahoo 1−5 Clinton 6−10 11−20 Enron 21+ 100-150 151-200 201-250 251-300 Yelp Figure 1: Number of paragraphs, sentences, and words per document. Domain Raters Yahoo untrained expert untrained expert untrained expert untrained expert Clinton Enron Yelp ICC Weighted κ 0.113 ± 0.024 0.557 ± 0.010 0.270 ± 0.020 0.398 ± 0.015 0.141 ± 0.021 0.428 ± 0.014 0.120 ± 0.026 0.304 ± 0.015 0.060 ± 0.013 0.386 ± 0.009 0.156 ± 0.013 0.250 ± 0.011 0.077 ± 0.012 0.273 ± 0.011 0.069 ± 0.014"
W18-5023,D17-1035,0,0.0163934,"o 3-class classification by labeling each clique with the document class label (low, medium, high). To predict the text label, the model averages the predicted coherence class distributions over all cliques. 4 Evaluation We evaluate the models on multiple coherence prediction tasks. The best model parameters, reported in the supplementary material, are the result of 10-fold cross-validation over the training data. For all neural models (EG RID C ONV, EG RID C ONV +coref, C LIQUE, S ENTAVG, and PAR S EQ), the reported results are the mean of 10 runs with different random seeds, as suggested by Reimers and Gurevych (2017). We indicate (†) when the best neural model result is significantly better (p < 0.05) than the best non-neural result. We use the one-sample Wilcoxon signed rank test and adjusted the pvalues to account for the false discovery rate. 4.1 Classification 6 https://github.com/datienguyen/cnn_ coherence 7 https://github.com/MMesgar/lcg For this task, each text has a consensus label expressing how coherent it is: {low, medium, high}. 218 System Yahoo Accuracy Clinton Enron Yelp System Yahoo Spearman ρ Clinton Enron Yelp Majority class Baseline 41.0 43.5 55.5 56.0 44.0 52.5 54.0 55.0 Baseline 0.089"
W18-5023,P13-1010,0,\N,Missing
W18-6105,D16-1161,0,0.0132108,"h, 2011; Dahlmeier and Ng, 2011), but did not consider run-on sentences. The closest work to our own is Israel et al. (2012), who used Conditional Random Fields (CRFs) for correcting comma errors (excluding comma splices, a type of run-on sentence). Lee et al. (2014) used a similar system based on CRFs but focused on comma splice correction. Recently, the field has focused on the task of whole-sentence correction, targeting all errors in a sentence in one pass. Whole-sentence correction methods borrow from advances in statistical machine translation (Madnani et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016) and, more recently, neural machine translation (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018; Xie et al., 2018; Junczys-Dowmunt et al., 2018). To date, GEC systems have been evaluated on corpora of non-native student writing such as NUCLE (Dahlmeier et al., 2013) and the Cambridge Learner Corpus First Certificate of English (Yannakoudakis et al., 2011). The 2013 and 2014 CoNLL Shared Tasks in GEC used NUCLE as their train and test sets (Ng et al., 2013, 2014). There are few instances of run-on sentences annotated in both test sets, making it hard to assess system performance on that error"
W18-6105,L16-1103,0,0.0138682,"et al., 2013, 2014). There are few instances of run-on sentences annotated in both test sets, making it hard to assess system performance on that error type. A closely related task to run-on error correction is that of punctuation restoration in the automatic speech recognition (ASR) field. Here, a system takes as input a speech transcription and is tasked with inserting any type of punctuation where appropriate. Most work utilizes textual features with n-gram models (Gravano et al., 2009), CRFs (Lu and Ng, 2010), convolutional neural networks or recurrent neural networks (Peitz et al., 2011; Che et al., 2016). The Punctuator (Tilk and Alum¨ae, 2016) is a leading punctuation restoration system based on a sequence-to-sequence model (Seq2Seq) trained on long slices of text which can span multiple sentences. 3 Table 2: NUCLE sentence labeled to indicate what follows each token: a space (S) or period (P). ply two sequence models to this task, conditional random fields (roCRF) and Seq2Seq (roS2S). 3.1 Conditional Random Fields Our CRF model, roCRF, represents a sentence as a sequence of spaces between tokens, labeled to indicate whether a period should be inserted in that space. Each space is represente"
W18-6105,N18-1055,0,0.0354002,"elds (CRFs) for correcting comma errors (excluding comma splices, a type of run-on sentence). Lee et al. (2014) used a similar system based on CRFs but focused on comma splice correction. Recently, the field has focused on the task of whole-sentence correction, targeting all errors in a sentence in one pass. Whole-sentence correction methods borrow from advances in statistical machine translation (Madnani et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016) and, more recently, neural machine translation (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018; Xie et al., 2018; Junczys-Dowmunt et al., 2018). To date, GEC systems have been evaluated on corpora of non-native student writing such as NUCLE (Dahlmeier et al., 2013) and the Cambridge Learner Corpus First Certificate of English (Yannakoudakis et al., 2011). The 2013 and 2014 CoNLL Shared Tasks in GEC used NUCLE as their train and test sets (Ng et al., 2013, 2014). There are few instances of run-on sentences annotated in both test sets, making it hard to assess system performance on that error type. A closely related task to run-on error correction is that of punctuation restoration in the automatic speech recognition (ASR) field. Here,"
W18-6105,P18-1059,0,0.0236245,"cial data is an area of future work. The results of this study are inconclusive in terms of how much harder the task is on clean versus noisy text. However, our findings suggest that artificial run-ons are similar to naturally occurring run-ons in ungrammatical text because models trained on artificial data do just as well predicting real run-ons as artificial ones. In this work, we found that a leading GEC model (Chollampatt and Ng, 2018) does not correct any run-on sentences, even though there was an overlap between the test and training data for that model. This supports the recent work of Choshen and Abend (2018), who found that GEC systems tend to ignore less frequent errors due to reference bias. Based on our work with run-on sentences, a common error type that is infrequent in annotated data, we strongly encourage future GEC work to address low-coverage errors. Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 632– 642. Association for Computational Linguistics. Daniel Dahlmeier and Hwee Tou Ng. 2011. Grammatical error correction with alternating structure optimization. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:"
W18-6105,2005.mtsummit-papers.11,0,0.100302,"recision, recall, and the F0.5 score. In GEC, precision is more important than recall, and therefore the standard metric for evaluation is F0.5 , which weights precision twice as much as recall. Baselines: We report results on a balanced random baseline and state-of-the-art models from whole-sentence GEC (NUS18) and punctuation restoration (the Punctuator). NUS18 is the released GEC model of Chollampatt and Ng (2018), trained on two GEC corpora, NUCLE and Lang8 (Mizumoto et al., 2011). We test two versions of the Punctuator: Punctuator-EU is the released model, trained on English Europarl v7 (Koehn, 2005), and Punctuator-RO, which we trained on artificial clean data (FakeGiga-Train) using the authors’ code.1 roCRF: We train our model with `1regularization and c = 10 using the CRF++ toolkit.2 Only features that occur at least 5 times in the training set were included. Spaces are labeled to contain missing punctuation when the marginal probability is less than 0.70. Parameters are tuned to F0.5 on 25k held-out sentences. 1 2 35 github.com/ottokart/punctuator2 Version 0.59, github.com/taku910/crfpp/ Random Punctuator-EU Punctuator-RO roCRF roS2S Clean v. Noisy - Artificial Data FakeGiga FakeESL P"
W18-6105,2011.iwslt-papers.7,0,0.0226882,"n and test sets (Ng et al., 2013, 2014). There are few instances of run-on sentences annotated in both test sets, making it hard to assess system performance on that error type. A closely related task to run-on error correction is that of punctuation restoration in the automatic speech recognition (ASR) field. Here, a system takes as input a speech transcription and is tasked with inserting any type of punctuation where appropriate. Most work utilizes textual features with n-gram models (Gravano et al., 2009), CRFs (Lu and Ng, 2010), convolutional neural networks or recurrent neural networks (Peitz et al., 2011; Che et al., 2016). The Punctuator (Tilk and Alum¨ae, 2016) is a leading punctuation restoration system based on a sequence-to-sequence model (Seq2Seq) trained on long slices of text which can span multiple sentences. 3 Table 2: NUCLE sentence labeled to indicate what follows each token: a space (S) or period (P). ply two sequence models to this task, conditional random fields (roCRF) and Seq2Seq (roS2S). 3.1 Conditional Random Fields Our CRF model, roCRF, represents a sentence as a sequence of spaces between tokens, labeled to indicate whether a period should be inserted in that space. Each"
W18-6105,D14-1162,0,0.081611,"Missing"
W18-6105,Y14-1063,0,0.0153762,"8. 2018 Association for Computational Linguistics 2 Related Work This/S shows/S the/S rising/S of/S life/S expectancies/P it/S is/S an/S achievement/S and/S it/S is/S also/S a/S challenge/S ./S Early work in the field of GEC focused on correcting specific error types such as preposition and article errors (Tetreault et al., 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011), but did not consider run-on sentences. The closest work to our own is Israel et al. (2012), who used Conditional Random Fields (CRFs) for correcting comma errors (excluding comma splices, a type of run-on sentence). Lee et al. (2014) used a similar system based on CRFs but focused on comma splice correction. Recently, the field has focused on the task of whole-sentence correction, targeting all errors in a sentence in one pass. Whole-sentence correction methods borrow from advances in statistical machine translation (Madnani et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016) and, more recently, neural machine translation (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018; Xie et al., 2018; Junczys-Dowmunt et al., 2018). To date, GEC systems have been evaluated on corpora of non-native student writin"
W18-6105,P11-1093,0,0.0255844,"ith longdistance dependencies, whereas most other grammatical errors are local and only need a small window for decent accuracy. 33 Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 33–38 c Brussels, Belgium, Nov 1, 2018. 2018 Association for Computational Linguistics 2 Related Work This/S shows/S the/S rising/S of/S life/S expectancies/P it/S is/S an/S achievement/S and/S it/S is/S also/S a/S challenge/S ./S Early work in the field of GEC focused on correcting specific error types such as preposition and article errors (Tetreault et al., 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011), but did not consider run-on sentences. The closest work to our own is Israel et al. (2012), who used Conditional Random Fields (CRFs) for correcting comma errors (excluding comma splices, a type of run-on sentence). Lee et al. (2014) used a similar system based on CRFs but focused on comma splice correction. Recently, the field has focused on the task of whole-sentence correction, targeting all errors in a sentence in one pass. Whole-sentence correction methods borrow from advances in statistical machine translation (Madnani et al., 2012; Felice et al., 2014; Junczys"
W18-6105,D10-1018,0,0.0133952,"., 2011). The 2013 and 2014 CoNLL Shared Tasks in GEC used NUCLE as their train and test sets (Ng et al., 2013, 2014). There are few instances of run-on sentences annotated in both test sets, making it hard to assess system performance on that error type. A closely related task to run-on error correction is that of punctuation restoration in the automatic speech recognition (ASR) field. Here, a system takes as input a speech transcription and is tasked with inserting any type of punctuation where appropriate. Most work utilizes textual features with n-gram models (Gravano et al., 2009), CRFs (Lu and Ng, 2010), convolutional neural networks or recurrent neural networks (Peitz et al., 2011; Che et al., 2016). The Punctuator (Tilk and Alum¨ae, 2016) is a leading punctuation restoration system based on a sequence-to-sequence model (Seq2Seq) trained on long slices of text which can span multiple sentences. 3 Table 2: NUCLE sentence labeled to indicate what follows each token: a space (S) or period (P). ply two sequence models to this task, conditional random fields (roCRF) and Seq2Seq (roS2S). 3.1 Conditional Random Fields Our CRF model, roCRF, represents a sentence as a sequence of spaces between toke"
W18-6105,P10-2065,1,0.771801,"entence-level mistakes with longdistance dependencies, whereas most other grammatical errors are local and only need a small window for decent accuracy. 33 Proceedings of the 2018 EMNLP Workshop W-NUT: The 4th Workshop on Noisy User-generated Text, pages 33–38 c Brussels, Belgium, Nov 1, 2018. 2018 Association for Computational Linguistics 2 Related Work This/S shows/S the/S rising/S of/S life/S expectancies/P it/S is/S an/S achievement/S and/S it/S is/S also/S a/S challenge/S ./S Early work in the field of GEC focused on correcting specific error types such as preposition and article errors (Tetreault et al., 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011), but did not consider run-on sentences. The closest work to our own is Israel et al. (2012), who used Conditional Random Fields (CRFs) for correcting comma errors (excluding comma splices, a type of run-on sentence). Lee et al. (2014) used a similar system based on CRFs but focused on comma splice correction. Recently, the field has focused on the task of whole-sentence correction, targeting all errors in a sentence in one pass. Whole-sentence correction methods borrow from advances in statistical machine translation (Madnani et al., 2012; F"
W18-6105,W12-2005,1,0.827336,"(Tetreault et al., 2010; Rozovskaya and Roth, 2011; Dahlmeier and Ng, 2011), but did not consider run-on sentences. The closest work to our own is Israel et al. (2012), who used Conditional Random Fields (CRFs) for correcting comma errors (excluding comma splices, a type of run-on sentence). Lee et al. (2014) used a similar system based on CRFs but focused on comma splice correction. Recently, the field has focused on the task of whole-sentence correction, targeting all errors in a sentence in one pass. Whole-sentence correction methods borrow from advances in statistical machine translation (Madnani et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016) and, more recently, neural machine translation (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018; Xie et al., 2018; Junczys-Dowmunt et al., 2018). To date, GEC systems have been evaluated on corpora of non-native student writing such as NUCLE (Dahlmeier et al., 2013) and the Cambridge Learner Corpus First Certificate of English (Yannakoudakis et al., 2011). The 2013 and 2014 CoNLL Shared Tasks in GEC used NUCLE as their train and test sets (Ng et al., 2013, 2014). There are few instances of run-on sentences annotated in both test s"
W18-6105,I11-1017,0,0.0796587,"n independent clause in the corrected text.) In total, we find fewer than 500 run-on sentences. Run5 Experiments Metrics: We report precision, recall, and the F0.5 score. In GEC, precision is more important than recall, and therefore the standard metric for evaluation is F0.5 , which weights precision twice as much as recall. Baselines: We report results on a balanced random baseline and state-of-the-art models from whole-sentence GEC (NUS18) and punctuation restoration (the Punctuator). NUS18 is the released GEC model of Chollampatt and Ng (2018), trained on two GEC corpora, NUCLE and Lang8 (Mizumoto et al., 2011). We test two versions of the Punctuator: Punctuator-EU is the released model, trained on English Europarl v7 (Koehn, 2005), and Punctuator-RO, which we trained on artificial clean data (FakeGiga-Train) using the authors’ code.1 roCRF: We train our model with `1regularization and c = 10 using the CRF++ toolkit.2 Only features that occur at least 5 times in the training set were included. Spaces are labeled to contain missing punctuation when the marginal probability is less than 0.70. Parameters are tuned to F0.5 on 25k held-out sentences. 1 2 35 github.com/ottokart/punctuator2 Version 0.59, g"
W18-6105,N18-1057,0,0.0400095,"ditional Random Fields (CRFs) for correcting comma errors (excluding comma splices, a type of run-on sentence). Lee et al. (2014) used a similar system based on CRFs but focused on comma splice correction. Recently, the field has focused on the task of whole-sentence correction, targeting all errors in a sentence in one pass. Whole-sentence correction methods borrow from advances in statistical machine translation (Madnani et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016) and, more recently, neural machine translation (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018; Xie et al., 2018; Junczys-Dowmunt et al., 2018). To date, GEC systems have been evaluated on corpora of non-native student writing such as NUCLE (Dahlmeier et al., 2013) and the Cambridge Learner Corpus First Certificate of English (Yannakoudakis et al., 2011). The 2013 and 2014 CoNLL Shared Tasks in GEC used NUCLE as their train and test sets (Ng et al., 2013, 2014). There are few instances of run-on sentences annotated in both test sets, making it hard to assess system performance on that error type. A closely related task to run-on error correction is that of punctuation restoration in the automatic speech"
W18-6105,W12-3018,1,0.842553,"Missing"
W18-6105,P11-1019,0,0.0511877,"used on the task of whole-sentence correction, targeting all errors in a sentence in one pass. Whole-sentence correction methods borrow from advances in statistical machine translation (Madnani et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016) and, more recently, neural machine translation (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018; Xie et al., 2018; Junczys-Dowmunt et al., 2018). To date, GEC systems have been evaluated on corpora of non-native student writing such as NUCLE (Dahlmeier et al., 2013) and the Cambridge Learner Corpus First Certificate of English (Yannakoudakis et al., 2011). The 2013 and 2014 CoNLL Shared Tasks in GEC used NUCLE as their train and test sets (Ng et al., 2013, 2014). There are few instances of run-on sentences annotated in both test sets, making it hard to assess system performance on that error type. A closely related task to run-on error correction is that of punctuation restoration in the automatic speech recognition (ASR) field. Here, a system takes as input a speech transcription and is tasked with inserting any type of punctuation where appropriate. Most work utilizes textual features with n-gram models (Gravano et al., 2009), CRFs (Lu and N"
W18-6105,W14-1701,0,0.121878,"Missing"
W18-6105,N16-1042,0,0.015623,"k to our own is Israel et al. (2012), who used Conditional Random Fields (CRFs) for correcting comma errors (excluding comma splices, a type of run-on sentence). Lee et al. (2014) used a similar system based on CRFs but focused on comma splice correction. Recently, the field has focused on the task of whole-sentence correction, targeting all errors in a sentence in one pass. Whole-sentence correction methods borrow from advances in statistical machine translation (Madnani et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016) and, more recently, neural machine translation (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018; Xie et al., 2018; Junczys-Dowmunt et al., 2018). To date, GEC systems have been evaluated on corpora of non-native student writing such as NUCLE (Dahlmeier et al., 2013) and the Cambridge Learner Corpus First Certificate of English (Yannakoudakis et al., 2011). The 2013 and 2014 CoNLL Shared Tasks in GEC used NUCLE as their train and test sets (Ng et al., 2013, 2014). There are few instances of run-on sentences annotated in both test sets, making it hard to assess system performance on that error type. A closely related task to run-on error correction is that of pun"
W18-6105,W13-3601,1,0.841248,"ection methods borrow from advances in statistical machine translation (Madnani et al., 2012; Felice et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016) and, more recently, neural machine translation (Yuan and Briscoe, 2016; Chollampatt and Ng, 2018; Xie et al., 2018; Junczys-Dowmunt et al., 2018). To date, GEC systems have been evaluated on corpora of non-native student writing such as NUCLE (Dahlmeier et al., 2013) and the Cambridge Learner Corpus First Certificate of English (Yannakoudakis et al., 2011). The 2013 and 2014 CoNLL Shared Tasks in GEC used NUCLE as their train and test sets (Ng et al., 2013, 2014). There are few instances of run-on sentences annotated in both test sets, making it hard to assess system performance on that error type. A closely related task to run-on error correction is that of punctuation restoration in the automatic speech recognition (ASR) field. Here, a system takes as input a speech transcription and is tasked with inserting any type of punctuation where appropriate. Most work utilizes textual features with n-gram models (Gravano et al., 2009), CRFs (Lu and Ng, 2010), convolutional neural networks or recurrent neural networks (Peitz et al., 2011; Che et al.,"
W19-4449,P06-1032,0,0.036625,"tated data in a cost-effective way (Yuan and Briscoe, 2016; Xie et al., 2016; Chollampatt and Ng, 2018). Recent work in artificial error generation (AEG) is inspired by the back-translation approach of machine translation systems (Sennrich et al., 2016; Poncelas et al., 2018). In this framework, an intermediate model is trained to translate correct sentences into errorful sentences. A new parallel cor2 Related Work Before the adoption of neural models, early approaches to AEG involved identifying error statistics and patterns in the corpus and applying them to grammatically correct sentences (Brockett et al., 2006; Rozovskaya and Roth, 2010). Inspired by the back-translation approach, recent AEG approaches inject errors into grammatically correct input sentences by adopting methods from neural machine translation (Felice and Yuan, 2014; Kasewa et al., 2018). Xie et al. (2018) propose an approach that adds noise to the beam-search phase of an back-translation based AEG model to generate more diverse errors. They use the synthesized parallel data generated by this method to train a multi-layer convolutional GEC model and achieve a 5 point F0.5 improvement on the CoNLL-2014 test data (Ng et al., 2014). ∗"
W19-4449,W18-0529,0,0.314551,"d using the largely available grammatically correct sentences and the corresponding synthetic data generated by this intermediate model. The newly created corpus with the artificial errors is then used to train a GEC model (Rei et al., 2017; Xie et al., 2018; Ge et al., 2018). To date, there is no work that compares how different base model architectures perform in the AEG task. In this paper, we investigate how effective are different model architectures in generating artificial, parallel data to improve a GEC model. Specifically, we train four recent neural models (and one rule-based model (Bryant and Briscoe, 2018)), including two new syntax-based models, for generating as well as correcting errors. We analyze which models are effective in the AEG and correction conditions as well as by data size. Essentially, we seek to understand how effective are recent sequence-to-sequence (seq2seq) neural model as AEG mechanisms “out of the box.” In recent years, sequence-to-sequence models have been very effective for end-to-end grammatical error correction (GEC). As creating human-annotated parallel corpus for GEC is expensive and time-consuming, there has been work on artificial corpus generation with the aim of"
W19-4449,W17-5032,0,0.269954,"Missing"
W19-4449,N12-1067,0,0.196099,"then train the GEC models using these parallel datasets. The three other experiments are variants of the first. In Exp2 we train all correction models on artificial errors generated by the top neural AEG systems and a rule-based system for comparison. In Exp3, we train the GEC models on NUCLE to analyze models built on real data. Finally, in Exp4, we train all GEC models on artificial data to determine how well correction models can perform without any real data. All our experiments are tested on the CoNLL2014 test set and we use the sentence-level F 0.5 score from the MaxMatch (M 2 ) scorer (Dahlmeier and Ng, 2012) for evaluation. All models are implemented using the Fairseq framework.3 4.3 Figure 1: (Exp1) Models trained on the artificial data generated by the corresponding AEG model. The Xaxis represents the amount of artificial data added to NUCLE-CLC during training. Exp2: Since the performance of MLCONV and Transformer GEC models improve with the addition of artificial data generated by corresponding AEG models, we hypothesize that the artificial error generated by these models are useful. To test this hypothesis, we train all the GEC models with various amount of artificial error generated by MLCO"
W19-4449,N10-1018,0,0.0344837,"fective way (Yuan and Briscoe, 2016; Xie et al., 2016; Chollampatt and Ng, 2018). Recent work in artificial error generation (AEG) is inspired by the back-translation approach of machine translation systems (Sennrich et al., 2016; Poncelas et al., 2018). In this framework, an intermediate model is trained to translate correct sentences into errorful sentences. A new parallel cor2 Related Work Before the adoption of neural models, early approaches to AEG involved identifying error statistics and patterns in the corpus and applying them to grammatically correct sentences (Brockett et al., 2006; Rozovskaya and Roth, 2010). Inspired by the back-translation approach, recent AEG approaches inject errors into grammatically correct input sentences by adopting methods from neural machine translation (Felice and Yuan, 2014; Kasewa et al., 2018). Xie et al. (2018) propose an approach that adds noise to the beam-search phase of an back-translation based AEG model to generate more diverse errors. They use the synthesized parallel data generated by this method to train a multi-layer convolutional GEC model and achieve a 5 point F0.5 improvement on the CoNLL-2014 test data (Ng et al., 2014). ∗ Work done during internship"
W19-4449,W13-1703,0,0.51077,"Missing"
W19-4449,N18-1057,0,0.547982,"2018). In this framework, an intermediate model is trained to translate correct sentences into errorful sentences. A new parallel cor2 Related Work Before the adoption of neural models, early approaches to AEG involved identifying error statistics and patterns in the corpus and applying them to grammatically correct sentences (Brockett et al., 2006; Rozovskaya and Roth, 2010). Inspired by the back-translation approach, recent AEG approaches inject errors into grammatically correct input sentences by adopting methods from neural machine translation (Felice and Yuan, 2014; Kasewa et al., 2018). Xie et al. (2018) propose an approach that adds noise to the beam-search phase of an back-translation based AEG model to generate more diverse errors. They use the synthesized parallel data generated by this method to train a multi-layer convolutional GEC model and achieve a 5 point F0.5 improvement on the CoNLL-2014 test data (Ng et al., 2014). ∗ Work done during internship at Grammarly 478 Proceedings of the Fourteenth Workshop on Innovative Use of NLP for Building Educational Applications, pages 478–483 c Florence, Italy, August 2, 2019. 2019 Association for Computational Linguistics Ge et al. (2018) propos"
W19-4449,N16-1042,0,0.0462853,"written text. Recent work treats GEC as a translation task that use sequenceto-sequence models (Sutskever et al., 2014; Bahdanau et al., 2015) to rewrite sentences with grammatical errors to grammatically correct sentences. As with machine translation models, GEC models benefit largely from the amount of parallel training data. Since it is expensive and time-consuming to create annotated parallel corpus for training, there is research into generating sentences with artificial errors from grammatically correct sentences with the goal of simulating human-annotated data in a cost-effective way (Yuan and Briscoe, 2016; Xie et al., 2016; Chollampatt and Ng, 2018). Recent work in artificial error generation (AEG) is inspired by the back-translation approach of machine translation systems (Sennrich et al., 2016; Poncelas et al., 2018). In this framework, an intermediate model is trained to translate correct sentences into errorful sentences. A new parallel cor2 Related Work Before the adoption of neural models, early approaches to AEG involved identifying error statistics and patterns in the corpus and applying them to grammatically correct sentences (Brockett et al., 2006; Rozovskaya and Roth, 2010). Inspire"
W19-4449,N03-1031,0,\N,Missing
W19-4449,W14-1701,0,\N,Missing
W19-4449,E14-3013,0,\N,Missing
W19-4449,P16-1009,0,\N,Missing
