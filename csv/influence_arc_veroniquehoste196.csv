2020.computerm-1.12,W18-4909,0,0.0308939,"Missing"
2020.computerm-1.12,N18-2052,0,0.0520181,"Missing"
2020.computerm-1.12,2020.computerm-1.13,0,0.0350263,"tries, if they are present and annotated in the corpus). We do not discount the importance of ATE systems that handle term variation, but a choice was made to focus on the core task for the first edition of the task. gold standard with both terms and Named Entities. These double scores did not influence the final ranking based on f1-scores. The dataset has been used for more detailed evaluations as well (see section 4.3) and participants were encouraged to report scores on the training domains in their own papers as well. 4.2. Participants Five teams participated in the shared task: TALNLS2N (Hazem et al., 2020), RACAI (Pais and Ion, 2020), e-Terminology (Oliver and V`azquez, 2020), NLPLab UQAM (no system description paper), and NYU (no system description paper but based on previous work in Meyers et al. (2018)). NYU and RACAI participated only in the English track, TALN-LS2N participated in both the English and French tracks, and e-Terminology and NLPLab UQAM participated in all tracks. We refer to their own system description papers for more details, but will There are three different tracks (one per language) and participants could enter in one or multiple tracks. When participants submitted their"
2020.computerm-1.12,C14-1029,0,0.0567379,"Missing"
2020.computerm-1.12,W16-4702,0,0.237528,"Missing"
2020.computerm-1.12,loukachevitch-2012-automatic,0,0.405981,"an ambiguous There are many more ways in which ATE systems can vary. Some can already be deduced from the ways in which the datasets are annotated, such as support for nested terms. Another very fundamental difference is the frequency cutoff: many ATE systems only extract terms which appear above a certain frequency threshold in the corpora. This threshold is extremely variable, with some systems that do not have any threshold, others that only extract candidate terms which appear 15 times or more (Pollak et al., 2019), and still others where only the top-n most frequent terms are extracted (Loukachevitch, 2012). Term length is similarly variable, with systems that don’t place any restrictions, others that extract only single-word terms, only multi-word terms, or those that extract all terms between 1 and n tokens (with n ranging from 2 to 15), where n is sometimes determined by the restrictions of a system, sometimes experimentally set to an optimal value, and at other times directly determined by the maximum term length in a gold standard. There are many other possible differences, such as POS patterns, which will not be discussed in any detail here. More information regarding both datasets for ATE"
2020.computerm-1.12,R15-1062,0,0.343163,"Missing"
2020.computerm-1.12,2020.computerm-1.15,0,0.0606562,"Missing"
2020.computerm-1.12,2020.computerm-1.14,0,0.0372712,"nd annotated in the corpus). We do not discount the importance of ATE systems that handle term variation, but a choice was made to focus on the core task for the first edition of the task. gold standard with both terms and Named Entities. These double scores did not influence the final ranking based on f1-scores. The dataset has been used for more detailed evaluations as well (see section 4.3) and participants were encouraged to report scores on the training domains in their own papers as well. 4.2. Participants Five teams participated in the shared task: TALNLS2N (Hazem et al., 2020), RACAI (Pais and Ion, 2020), e-Terminology (Oliver and V`azquez, 2020), NLPLab UQAM (no system description paper), and NYU (no system description paper but based on previous work in Meyers et al. (2018)). NYU and RACAI participated only in the English track, TALN-LS2N participated in both the English and French tracks, and e-Terminology and NLPLab UQAM participated in all tracks. We refer to their own system description papers for more details, but will There are three different tracks (one per language) and participants could enter in one or multiple tracks. When participants submitted their final results on the test d"
2020.computerm-1.12,W14-6001,0,0.150785,"unsupervised systems, as well as the distinction between sequence labelling approaches and systems that start from a limited list of unique term candidates. Splitting systems by their features is perhaps even more difficult, since research has moved far beyond using simple linguistic and statistical features. Some examples include ˇ the use of topic modelling (Sajatovi´ c et al., 2019; Bolshakova et al., 2013), queries on search engines, Wikipedia, or other external resources (Kessler et al., 2019; Vivaldi and Rodr´ıguez, 2001), and word embeddings (Amjadian et al., 2016; Kucza et al., 2018; Qasemizadeh and Handschuh, 2014; Pollak et al., 2019). Some methods are even called “featureless” (Gao and Yuan, 2019; Wang et al., 2016). ACTER Annotated Corpora for Term Extraction Research ACTER is a collection of domain-specific corpora in which terms have been manually annotated. It covers three languages (English, French, and Dutch) and four domains (corruption, dressage (equitation), heart failure, and wind energy). It has been created in light of some of the perceived difficulties that have been mentioned. A previous version (which did not yet bear the ACTER acronym) has already been elaborately described (Rigouts T"
2020.computerm-1.12,L16-1294,0,0.0310143,"the manual annotation of ATE results, rather than manual annotations in the unprocessed text. A final remark is that some corpora have been annotated with multiple term labels or have even been annotated according to large taxonomies, while others don’t make any distinctions beyond terms. As will be discussed in more detail in section 3, the ACTER dataset has been specifically designed to deal with some of the issues addressed here. Related Research Manually Annotated Gold Standards for ATE Two of the most commonly used annotated datasets are GENIA (Kim et al., 2003), and the ACL RD-TEC 2.0 (Qasemizadeh and Schumann, 2016), both of which are in English. GENIA is a collection of 2000 abstracts from the MEDLINE database in the domain of bio-medicine, specifically “transcription factors in human blood cells”. Over 400k tokens were annotated by two domain experts to obtain 93,293 term annotations. The ACL-RD-TEC 2.0 contains 300 annotated abstracts from the ACL Anthology Reference Corpus. Again, two experts performed the annotation of 33k tokens, which resulted in 6818 term annotations. They claim three main advantages over GENIA: first, the domain (computational linguistics) means that ATE researchers will have a"
2020.computerm-1.12,C10-3015,0,0.0970024,"Missing"
2020.computerm-1.12,R19-1117,1,0.72686,"er two (Macken et al., 2013). These methods typically select candidate terms based on their POS-pattern and rank these candidate terms using the statistical metrics, thus combining the advantages of both techniques. A particular difficulty is defining the cut-off threshold for the term candidates, which can be defined as the top-n terms, the top-n percentage of terms, or all terms above a certain threshold score. Manually predicting the ideal cut-off point is extremely difficult and can result in a skew towards either precision or recall, which can be detrimental to the final f1score (Rigouts Terryn et al., 2019a). While this typology of linguistic, statistical, and hybrid systems is sometimes still used today, in recent years, the advance of machine learning techniques has made such a simple classification of ATE methodologies more complicated (Gao and Yuan, 2019). Methodologies have become so diverse that they are no longer easily captured in such a limited number of clearly delineated categories. For instance, apart from the distinction between statistical and linguistic systems, one could also distinguish between rulebased methods and machine learning methods. However, rather than a simple binary"
2020.computerm-1.12,W11-1816,0,0.0914129,"Missing"
2020.computerm-1.12,U16-1011,0,0.0306596,"e statistical score (Drouin, 2003; Kosa et al., 2020), systems that combine a limited number of features with a voting algorithm (Fedorenko et al., 2013; Vivaldi and Rodr´ıguez, 2001), an evolutionary algorithm that optimises the ROC-curve (Az´e et al., 2005), rule-induction (Foo and Merkel, 2010), supportvector models (Ramisch et al., 2010), logistic regression (Bolshakova et al., 2013; Judea et al., 2014), basic neural networks (H¨atty and Schulte im Walde, 2018a), recursive neural networks (Kucza et al., 2018), siamese neural networks (Shah et al., 2019), and convolutional neural networks (Wang et al., 2016). Within the machine learn86 3. ing systems, there are vast differences between supervised, semi-supervised, and unsupervised systems, as well as the distinction between sequence labelling approaches and systems that start from a limited list of unique term candidates. Splitting systems by their features is perhaps even more difficult, since research has moved far beyond using simple linguistic and statistical features. Some examples include ˇ the use of topic modelling (Sajatovi´ c et al., 2019; Bolshakova et al., 2013), queries on search engines, Wikipedia, or other external resources (Kessl"
2020.computerm-1.12,W19-5118,0,0.036623,"Missing"
2020.crac-1.2,J16-3004,1,0.893236,"Missing"
2020.crac-1.2,R11-1026,1,0.822878,"Missing"
2020.crac-1.2,C10-1031,0,0.040323,"Missing"
2020.crac-1.2,P97-1023,0,0.841558,"Missing"
2020.crac-1.2,W06-0301,0,0.0965056,"Missing"
2020.crac-1.2,J13-4004,0,0.028231,"an anaphor – pronoun – and an antecedent constituting an aspect term in itself should enable us to derive additional semantic information. 15 For the research presented here, we explored the added value of incorporating coreference information by including it as a separate processing step before the feature extraction. Crucial for this step is that the coreference resolution is highly accurate, since an anaphor–antecedent mismatch can also lead to a semantic information mismatch. To this purpose, we relied on existing systems in both languages: the deterministic Stanford Coreference Resolver (Lee et al., 2013) for English and the COREA system for Dutch (De Clercq et al., 2011). The Stanford system is a rule-based system that includes a total of ten rules (or “sieves”) for entity coreference, such as exact string match and pronominal resolution. The sieves are applied from highest to lowest precision, each rule adding coreference links. The COREA system is a mention-pair system (Hoste, 2016) that recasts the coreference resolution problem as a classification task: a classifier is trained to decide whether a pair of noun phrases or mentions is coreferential of not. In other words, resolving anaphor m"
2020.crac-1.2,P10-1139,0,0.0456442,"Missing"
2020.crac-1.2,W02-1011,0,0.0490022,"Missing"
2020.crac-1.2,S14-2004,0,0.039758,"held-out test set and does so regardless of whether coreference information was included. 1 Introduction In the last two decades, the field of sentiment analysis (SA) has yielded a lot of attention in both academia and commerce (see Liu (2015), Mohammad (2016) or Zhang et al. (2018) for overviews). The attention in SA research has shifted from the coarse-grained detection of the polarity of a given piece of text to the more fine-grained detection of not only polarity, but also the target of the expressed sentiment, as exemplified by the SemEval shared tasks on aspect-based sentiment analysis (Pontiki et al., (2014; 2015; 2016)). In reviews, many references to different aspects of a given product, experience, etc. are made and in a large number of cases, these references are even implicit. Regarding these implicit references, there are two options: either the referent is truly implicit meaning that the aspect can only be inferred from the implied meaning of the sentence, or the referent is an anaphor referring back to an antecedent that was or was not previously mentioned in the review. While it has been claimed that anaphora or coreference resolution plays an important role in opinion mining to resolve"
2020.crac-1.2,S15-2082,0,0.0639219,"Missing"
2020.crac-1.2,S15-2130,0,0.0391594,"Missing"
2020.crac-1.2,W06-1640,0,0.169235,"Missing"
2020.crac-1.2,S15-2083,0,0.118374,"sis can be decomposed into three subtasks: aspect term extraction, aspect term aggregation or classification and aspect term polarity estimation. The focus of the research presented here is on the second one. The idea is to predict several predefined and domain-specific categories, i.e. a multiclass classification task. The two systems achieving the best results on this individual subtask in SemEval 2015 Task 12 both used classification to this purpose, respectively individual binary classifiers trained on each possible category which are afterwards entered in a sigmoidal feedforward network (Toh and Su, 2015) and a single Maximum Entropy classifier (Saias, 2015). When it comes to feature engineering, especially lexical features in the form of bag-of-words such as word unigrams and bigrams (Toh and Su, 2015) or word and lemma unigrams (Saias, 2015) and lexical-semantic features in the form of clusters learned from a large corpus of reference data (Saias, 2015) were used. Since then, these benchmark SemEval datasets have been used many times to train and test neural models yielding state-of-the-art results on both this second subtask and end-to-end aspect-based sentiment analysis (Do et al., 2019) H"
2020.crac-1.2,S16-1045,0,0.0155743,"210 1280 392 Table 2: the overall number of targets and the number of implicit and explicit targets in the datasets. 4.1 Information sources As a baseline, we derived bag-of-words token unigram features of the sentence in which a target occurs in order to represent some of the lexical information present in each of the categories. In bag-of-words representations, each feature corresponds to a single word found in the training corpus. Besides these lexical features, features in the form of clusters derived from a large domain-specific reference corpus have also proven useful (Toh and Su, 2015; Toh and Su, 2016). Given the lack of such reference corpora for Dutch, we decided to link mentions of concepts and instances to either semantic lexicons like WordNet (Fellbaum, 1998)(English) or Cornetto (Vossen et al., 2013) (Dutch) , and to a Wikipedia-based knowledge base (Hovy et al., 2013) such as DBpedia (Lehmann et al., 2013). This led to the creation of a set of lexico-semantic features. Six WordNet features were derived, each representing a value indicating the number of (unique) terms annotated as aspect terms from that category that (1) co-occur in the synset of the candidate term or (2) which are a"
2020.crac-1.2,J09-3003,0,0.143112,"Missing"
2020.fnp-1.36,D19-5105,0,0.241134,"in text. Event extraction consists of identifying event triggers, i.e. the tokens that express an event of a predetermined type, and identifying participant arguments, i.e. the tokens that express prototypical participant roles. Event extraction is typically an upstream step in pipelines for financial applications: it has been used for news summarization of single (Lee et al., 2003; Marujo et al., 2017) or multiple documents (Liu et ˇ al., 2007; Glavaˇs and Snajder, 2014), forecasting and market analysis (Nassirtoussi et al., 2014; Bholat et al., 2015; Nardo et al., 2016; Zhang et al., 2018; Chen et al., 2019), risk analysis (Hogenboom et al., 2015; Wei et al., 2019), policy assessment (Tobback et al., 2018; Karami et al., 2018), and marketing (Rambocas and Pacheco, 2018). This work aims to enable these information extraction tasks in the financial domain by making available a dataset and fine-grained event extraction model for companyspecific news by classifying economic event triggers and participant arguments in text. Our SENTiVENT dataset of company-specific events was conceived to be compatible with finegrained event representations of the ACE benchmark corpora as to enable direct application"
2020.fnp-1.36,doddington-etal-2004-automatic,0,0.0935933,"to the task of financial event extraction. 2 Related Research Our work on economic event extraction is accommodated within the rich history of automatic event detection. The ACE (Automatic Content Extraction) annotation scheme and programme was highly influential in event processing. Periodically, “Event Detection and Recognition” evaluation competitions were organized where event extraction corpora were released (Consortium, 2005; Walker et al., 2006) to enable automatic inference of entities mentioned in text, the relations among entities, and the events in which these entities participate (Doddington et al., 2004). Some years later, the ERE (Entities, Relations, Events) standard was conceived as a continuation of ACE with the goal of improving annotation consistency and quality. Our annotation scheme is inspired by the Rich ERE Event annotation schemes This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 235 Proceedings of the 1st Joint Workshop on Financial Narrative Processing and MultiLing Financial Summarisation, pages 235–245 Barcelona, Spain (Online), December 12, 2020. (Linguistic Data Consortium, 201"
2020.fnp-1.36,D19-5102,0,0.169779,"h has been successful in formulating trading strategies (Ben Ami and Feldman, 2017) and in assessing the impact of events on the stock market (Boudoukh et al., 2016). Along the same line, Hogenboom et al. (2013) rely on a hand-crafted financial event ontology for pattern-based event detection in the economic domain and incorporates lexicons, gazetteers, PoS-tagging and morphological analysis. Several semi- or distantly supervision approaches exist in which seed sets are manually labeled or rule-sets are used to generate or enhance training data (Qian et al., 2019; R¨onnqvist and Sarlin, 2017; Ein-Dor et al., 2019). For Chinese, Yang et al. (2018) and Chen et al. (2019) rely on a knowledge-base of rules for extracting ACE-like events in stock market prediction. Han et al. (2018) describe a hybrid approach to ACE-like event extraction by labeling triggers, argument types, and event types for Chinese news articles with 8 economic event types using an automatically expanded trigger dictionary and used a pattern-matching approach for argument extraction. Few strictly supervised approaches exist due to the lack of human-annotated ground-truth data. Malik et al. (2011) annotated Dividend and Profit figure slo"
2020.fnp-1.36,W18-2501,0,0.0388393,"Missing"
2020.fnp-1.36,W18-3101,1,0.688954,"Missing"
2020.fnp-1.36,2020.fnp-1.36,1,0.0541379,"ime”, “Place”, and “Capital” can belong to any event. The typology was iteratively developed on a sample corpus in which news events were evaluated by financial domain experts. Starting with the set of event types in previous literature (Feldman et al., 2011b; Boudoukh et al., 2019; Hogenboom et al., 2013; Van De Kauter et al., 2015; Du et al., 2016), types were removed and added based on frequency and cohesion of categories. Relevant argument roles were also added in this process of iterative refinement. For a full description of the types and subtypes, we refer to the annotation guidelines (Jacobs, 2020b). Here is a list of main Types → unique Arguments: • CSR/B RAND → Company • D EAL → Goal, Partner • D IVIDEND → Amount, Company, HistoricalYieldRatio, YieldRatio • E MPLOYMENT → Amount, Employee, Employer, Replacer, Replacing, Title • E XPENSE → Amount, Company, HistoricalAmount • FACILITY → Company, Facility • F INANCIAL R EPORT → Reportee, Result • F INANCING → Amount, Financee, Financer • I NVESTMENT → CapitalInvested, Investee, Investor • L EGAL → Allegation, Adjudicator, Complainant, Defendant, Sentence • M ACROECONOMICS → AffectedCompany, Sector • M ERGER /ACQUISITION → Acquirer, Cost,"
2020.fnp-1.36,P18-1007,0,0.0622557,"Missing"
2020.fnp-1.36,L16-1051,1,0.409916,"Missing"
2020.fnp-1.36,N16-1034,0,0.079324,"s identifying event triggers and their event types, event arguments and event roles. The input documents are represented as a sequence of tokens D from which the model constructs the set of all possible within-sentence spans S = {s1 , ..., sT } in the document (limited by a threshold-length). Each token di is assigned an event type label ti . Then, for each trigger di , event arguments are assigned by predicting and argument role ai j for all spans sj in the same sentence as di . Evaluation For ACE05, we follow the evaluation splits (cf. Table 1) and method commonly used in previous research (Nguyen et al., 2016; Sha et al., 2018; Zhang et al., 2019). All reported precision (P), recall (R), and F1 -scores (F1 ) are micro-averaged. Our experiments involve the following four subtasks: Trigger identification (Trig-ID) is the subtask of identifying if a token position matches a ground-truth reference trigger. Trigger classification (Trig-C) determines the type of the identified trigger. A trigger is correct when it is correctly identified and its type label matches the reference. Argument identification (Arg-ID) is the subtask of identifying if a text span belongs to a certain event type. An argument is"
2020.fnp-1.36,D19-1585,0,0.388912,"acheco, 2018). This work aims to enable these information extraction tasks in the financial domain by making available a dataset and fine-grained event extraction model for companyspecific news by classifying economic event triggers and participant arguments in text. Our SENTiVENT dataset of company-specific events was conceived to be compatible with finegrained event representations of the ACE benchmark corpora as to enable direct application of advances in the field. In our pilot study, we investigate the portability of an existing state-of-the-art model for event extraction, named DYGIE++ (Wadden et al., 2019b), to the task of financial event extraction. 2 Related Research Our work on economic event extraction is accommodated within the rich history of automatic event detection. The ACE (Automatic Content Extraction) annotation scheme and programme was highly influential in event processing. Periodically, “Event Detection and Recognition” evaluation competitions were organized where event extraction corpora were released (Consortium, 2005; Walker et al., 2006) to enable automatic inference of entities mentioned in text, the relations among entities, and the events in which these entities participa"
2020.fnp-1.36,P18-4009,0,0.0901784,"trading strategies (Ben Ami and Feldman, 2017) and in assessing the impact of events on the stock market (Boudoukh et al., 2016). Along the same line, Hogenboom et al. (2013) rely on a hand-crafted financial event ontology for pattern-based event detection in the economic domain and incorporates lexicons, gazetteers, PoS-tagging and morphological analysis. Several semi- or distantly supervision approaches exist in which seed sets are manually labeled or rule-sets are used to generate or enhance training data (Qian et al., 2019; R¨onnqvist and Sarlin, 2017; Ein-Dor et al., 2019). For Chinese, Yang et al. (2018) and Chen et al. (2019) rely on a knowledge-base of rules for extracting ACE-like events in stock market prediction. Han et al. (2018) describe a hybrid approach to ACE-like event extraction by labeling triggers, argument types, and event types for Chinese news articles with 8 economic event types using an automatically expanded trigger dictionary and used a pattern-matching approach for argument extraction. Few strictly supervised approaches exist due to the lack of human-annotated ground-truth data. Malik et al. (2011) annotated Dividend and Profit figure slots and detected these types with"
2020.lrec-1.204,E17-2092,0,0.352738,"r emotion framework are unclear (De Bruyne et al., 2019). 2.3. Dimensional frameworks in emotion corpora Although dimensional models are used to a lesser extent in emotion detection, some researchers recently emphasized the potential of and even need for a dimensional approach (Buechel and Hahn, 2016; Wood et al., 2018). Buechel and Hahn (2016) consider a VAD-approach superior to a categorical one due to the lack of consensus on the basic emotions set, but also because basic emotions are not equally distributed along the valence and arousal dimensions. This considering, EmoBank was created by Buechel and Hahn (2017) in a bi-representational format: 10k sentences were annotated with VAD-scores, of which a subset also has annotations for Ekman’s six. The dimensional annotations were obtained using the 5-point self-assessment manikin or SAM-scale (see Figure 1d), a pictorial scale depicting the VAD-dimensions (Bradley and Lang, 1994). Some studies have used a dimensional approach to categorical annotation, meaning that category ratings are used instead of discrete classes. Mohammad and Bravo-Marquez (2017) obtained ratings for the intensity of anger, fear, joy and sadness by using Best-Worst scaling. In thi"
2020.lrec-1.204,S19-2005,0,0.0281177,"ited to annotate texts coming from various domains and topics, but that the connotation of the labels strongly depends on the origin of the texts. Moreover, it seems that information is lost when an emotional state is forcedly classified in a limited set of categories, indicating that a bi-representational format is desirable when creating an emotion corpus. Keywords: NLP, emotion detection, emotion annotation 1. Introduction When dealing with the task of automatically detecting emotions in texts – a well-studied topic in the field of natural language processing or NLP (Mohammad et al., 2018; Chatterjee et al., 2019) – the first bottleneck is data acquisition. Not only is there the need to collect a considerable amount of data, one also needs to decide on an appropriate framework to annotate these data instances in order to build an emotion corpus. Seeing the plethora of existing emotion frameworks, this decision is not trivial. On the one hand, emotions can be represented as categories, typically by using a set of basic emotions. The frameworks of Ekman (1992) (with the basic emotions anger, disgust, fear, joy, sadness and surprise) and Plutchik (1980) (in which trust and anticipation are added) are the"
2020.lrec-1.204,W17-5205,0,0.0178722,"s are not equally distributed along the valence and arousal dimensions. This considering, EmoBank was created by Buechel and Hahn (2017) in a bi-representational format: 10k sentences were annotated with VAD-scores, of which a subset also has annotations for Ekman’s six. The dimensional annotations were obtained using the 5-point self-assessment manikin or SAM-scale (see Figure 1d), a pictorial scale depicting the VAD-dimensions (Bradley and Lang, 1994). Some studies have used a dimensional approach to categorical annotation, meaning that category ratings are used instead of discrete classes. Mohammad and Bravo-Marquez (2017) obtained ratings for the intensity of anger, fear, joy and sadness by using Best-Worst scaling. In this approach – which they claim to give more reliable scores than rating scales – annotators are given four items, of which they have to indicate which one is most representative for a certain emotion category (or highest on the emotional axis) and which one is not at all (lowest on the axis). This information is then converted into real-valued scores. 3. Experiments The creation of a new Dutch emotion corpus is a necessity, as also claimed by Vaassen (2014). In order to build a corpus that is"
2020.lrec-1.204,S18-1001,0,0.123338,"ss and fear are well-suited to annotate texts coming from various domains and topics, but that the connotation of the labels strongly depends on the origin of the texts. Moreover, it seems that information is lost when an emotional state is forcedly classified in a limited set of categories, indicating that a bi-representational format is desirable when creating an emotion corpus. Keywords: NLP, emotion detection, emotion annotation 1. Introduction When dealing with the task of automatically detecting emotions in texts – a well-studied topic in the field of natural language processing or NLP (Mohammad et al., 2018; Chatterjee et al., 2019) – the first bottleneck is data acquisition. Not only is there the need to collect a considerable amount of data, one also needs to decide on an appropriate framework to annotate these data instances in order to build an emotion corpus. Seeing the plethora of existing emotion frameworks, this decision is not trivial. On the one hand, emotions can be represented as categories, typically by using a set of basic emotions. The frameworks of Ekman (1992) (with the basic emotions anger, disgust, fear, joy, sadness and surprise) and Plutchik (1980) (in which trust and antici"
2020.lrec-1.204,L18-1192,0,0.0124853,"d a shared task on emotion detection in suicide notes and employed a set of 15 emotions which might be indicative of suicidal behavior. However, tailoring the framework to the specific task or domain of the data only happens in rare cases, and in most studies, the motives for choosing a particular emotion framework are unclear (De Bruyne et al., 2019). 2.3. Dimensional frameworks in emotion corpora Although dimensional models are used to a lesser extent in emotion detection, some researchers recently emphasized the potential of and even need for a dimensional approach (Buechel and Hahn, 2016; Wood et al., 2018). Buechel and Hahn (2016) consider a VAD-approach superior to a categorical one due to the lack of consensus on the basic emotions set, but also because basic emotions are not equally distributed along the valence and arousal dimensions. This considering, EmoBank was created by Buechel and Hahn (2017) in a bi-representational format: 10k sentences were annotated with VAD-scores, of which a subset also has annotations for Ekman’s six. The dimensional annotations were obtained using the 5-point self-assessment manikin or SAM-scale (see Figure 1d), a pictorial scale depicting the VAD-dimensions ("
2020.lrec-1.204,W16-0404,0,0.0458947,"Missing"
2020.lrec-1.204,W17-5203,0,0.046932,"Missing"
2020.lrec-1.204,H05-1073,0,0.309686,"Missing"
2020.semeval-1.135,Q17-1010,0,0.0426542,"est Common Subsequence Ratio (LCSR), Normalized Levenshtein Distance (NLD) and Jaro-Winkler Similarity. As for the semantic features, Hossain et al. (2019) noticed that the editors frequently opted for replacements that are semantically distant from their replaced counterparts. To capture this humor generation technique, we calculated the cosine similarity between the vector of the original token and its replacement, and between the original headline and its replacement. For this purpose, after removing numbers and punctuation marks and lower-casing, word vectors were extracted with fastText (Bojanowski et al., 2017). The vector for a given headline was calculated as the mean of its word vectors. Formula 1 illustrates how we modified cosine similarity to calculate the distance between two vectors A and B as a feature with a positive value to match the other features: 1+ cosine similarity(A, B) = A·B ||A||×||B|| 2 (1) In our modification of cosine similarity, two vectors (word or headline representations) can be exactly the opposite (0), exactly the same (1), or any value in-between. Our final group of features deals with perplexity. Intuitively, an utterance might be considered funny when there is a high"
2020.semeval-1.135,C18-1157,0,0.0172852,"uare error (RMSE) of 0.5253. 1 Introduction Research in the field of computational humor can be divided into three distinct tasks: detecting, assessing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval shared task on Assessing the Funninness of Edited News Headlines (Hossain et al., 2020), Hossain et al. (2019) introduce a novel dataset, named “Humicroedit”, that can be used for"
2020.semeval-1.135,N18-2018,0,0.0171477,"pre-trained language model RoBERTa to learn latent features in the news headlines that are useful to predict the funniness of each headline. The latter system was also our final submission to the competition and is ranked seventh among the 49 participating teams, with a root-mean-square error (RMSE) of 0.5253. 1 Introduction Research in the field of computational humor can be divided into three distinct tasks: detecting, assessing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI"
2020.semeval-1.135,W18-1111,0,0.0360497,"eadline or about the relationship between the original and edited headline. By creating features that model different types of humor generation techniques used by the editors, we want to learn how successful each technique is. In this section, we describe the different types of features that were created and motivate why these features would be useful for the task of humor detection according to existing humor theories. Secondly, we discuss the experimental setup and the machine learning model that was used to perform regression. 3.1.1 Feature engineering Some of our features were inspired by Chhaya et al. (2018), who introduced a supervised learning method to model frustration. Their feature set covers a wide range of features for emotion detection, which intuitively could be useful for our task as well. We implemented three groups of their features, leaving out 1034 those features that are either not relevant for this task or underrepresented in the dataset. The following features were used for the original headline: lexical features (number of uppercase words, number of non-alphanumeric characters, number of punctuation marks, average word length), NER-based features (presence of NER tags ‘person’,"
2020.semeval-1.135,N19-1423,0,0.10084,"e attention mechanism and, building on that, the transformer (Vaswani et al., 2017). Language models are typically trained with unsupervised training objectives which allows them to learn general language patterns from a given dataset. These patterns are encoded in the millions or even billions of parameters of the language model. Transfer learning allows us to transfer these parameters to other down-stream tasks, which is exactly what we did for this task. In our experiments we used RoBERTa (Liu et al., 2019), which is an improvement of the most wellknown currently used language model, BERT (Devlin et al., 2019). RoBERTa was trained on a dataset that is much larger than BERT. Whereas BERT was originally trained on 16 GB of data, RoBERTa utilises 160 GB of corpora. Most notable for our purposes is that a large part of that dataset (76 GB) is their newly created CC-news corpus, containing 63 million English news articles. Even though the current task involves news headlines, we would assume that during pretraining, RoBERTa at least learnt some hidden features that could relate to news-specific text. In our best performing model, we finetuned RoBERTa-base (125M parameters) using a custom head configurat"
2020.semeval-1.135,N19-1172,0,0.0203563,"sing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval shared task on Assessing the Funninness of Edited News Headlines (Hossain et al., 2020), Hossain et al. (2019) introduce a novel dataset, named “Humicroedit”, that can be used for all three tasks described above (see Section 2). The shared task is split into two sub-tasks, namely (i) a regression task to predict the mean"
2020.semeval-1.135,N19-1012,0,0.390217,"each headline. The latter system was also our final submission to the competition and is ranked seventh among the 49 participating teams, with a root-mean-square error (RMSE) of 0.5253. 1 Introduction Research in the field of computational humor can be divided into three distinct tasks: detecting, assessing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval shared task on Assessing"
2020.semeval-1.135,2020.semeval-1.98,0,0.087035,"insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval shared task on Assessing the Funninness of Edited News Headlines (Hossain et al., 2020), Hossain et al. (2019) introduce a novel dataset, named “Humicroedit”, that can be used for all three tasks described above (see Section 2). The shared task is split into two sub-tasks, namely (i) a regression task to predict the mean grade of all annotators for a given edited sentence, and (ii) a binary classification task to select the funnier of two edited headlines. We only participated in the regression sub-task. The remainder of this paper is structured as follows. In Section 2, we provide an overview of the dataset that was used for the competition. Section 3 introduces our two machine"
2020.semeval-1.135,L18-1102,0,0.0548846,"Missing"
2020.semeval-1.135,S17-2005,0,0.145151,"Missing"
2020.semeval-1.135,S17-2004,0,0.084207,"ict the funniness of each headline. The latter system was also our final submission to the competition and is ranked seventh among the 49 participating teams, with a root-mean-square error (RMSE) of 0.5253. 1 Introduction Research in the field of computational humor can be divided into three distinct tasks: detecting, assessing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval s"
2020.semeval-1.135,P18-2124,0,0.0238974,"eplacement Standard perplexity feature, pruned perplexity feature RMSE 0.576 0.579 0.578 0.578 0.577 0.575 0.575 0.576 Table 1: An overview of the different groups of features and their performance in the NuSVR algorithm (without hyperparameter tuning) when trained on train + dev data, and evaluated on test data. 3.2 Transformer-Based System Humor often expects world knowledge and awareness of the context to drive its intent home. Language models are capable of some language understanding (Wang et al., 2018, GLUE, and its successors), and on the popular SQUAD 2.0 question-answering benchmark (Rajpurkar et al., 2018) single (non-ensemble) language models even outperform human participants (Yang et al., 2019; Lan et al., 2019; Clark et al., 2020). The question is, however, if language models can use this information to figure out whether some given input is not only (un)true but also whether it is funny. Even though language models are nothing new, the last few years have seen great improvements in their architecture, most notably the attention mechanism and, building on that, the transformer (Vaswani et al., 2017). Language models are typically trained with unsupervised training objectives which allows th"
2020.semeval-1.135,W18-5446,0,0.0158529,"inkler Similarity Cosine sim. original token and its replacement, cosine sim. original headline and its replacement Standard perplexity feature, pruned perplexity feature RMSE 0.576 0.579 0.578 0.578 0.577 0.575 0.575 0.576 Table 1: An overview of the different groups of features and their performance in the NuSVR algorithm (without hyperparameter tuning) when trained on train + dev data, and evaluated on test data. 3.2 Transformer-Based System Humor often expects world knowledge and awareness of the context to drive its intent home. Language models are capable of some language understanding (Wang et al., 2018, GLUE, and its successors), and on the popular SQUAD 2.0 question-answering benchmark (Rajpurkar et al., 2018) single (non-ensemble) language models even outperform human participants (Yang et al., 2019; Lan et al., 2019; Clark et al., 2020). The question is, however, if language models can use this information to figure out whether some given input is not only (un)true but also whether it is funny. Even though language models are nothing new, the last few years have seen great improvements in their architecture, most notably the attention mechanism and, building on that, the transformer (Vas"
2020.semeval-1.135,D15-1284,0,0.0300313,"with a root-mean-square error (RMSE) of 0.5253. 1 Introduction Research in the field of computational humor can be divided into three distinct tasks: detecting, assessing and generating humor. Humor detection is performed by either distinguishing humorous items such as one-liners, jokes or texts from their non-humorous counterparts (Chen and Soo, 2018; Fan et al., 2020), or by ranking these items according to their respective funniness (Potash et al., 2017; Hossain et al., 2019). Moreover, researchers have tried to gain insights in the techniques and mechanisms that underlie humor production (Yang et al., 2015; Cattle and Ma, 2018; Hossain et al., 2019; West and Horvitz, 2019). Such insights can be used for automatic humor generation (Shahaf et al., 2015; He et al., 2019; Winters et al., 2019). Due to the subjective nature of humor and the deep conceptual common-sense knowledge that it requires, tasks involving humor remain challenging AI problems. Part of the challenge also lies in the scarcity of publicly available datasets. For the SemEval shared task on Assessing the Funninness of Edited News Headlines (Hossain et al., 2020), Hossain et al. (2019) introduce a novel dataset, named “Humicroedit”,"
2021.naacl-main.21,2020.acl-main.747,0,0.0982635,"Missing"
2021.naacl-main.21,P17-4017,0,0.028886,"ce, especially in non-English settings.1 1 Introduction Online platforms and social media are increasingly important as communication channels in various companies’ customer relationship management (CRM). To ensure effective, qualitative and timely customer service, Natural Language Processing (NLP) can assist by providing insights to optimize customer interactions, but also in real-time tasks: (i) detect emotions (Gupta et al., 2010), (ii) categorize or prioritize customer tickets (Molino et al., 2018), (iii) aid in virtual assistants through natural language understanding and/or generation (Cui et al., 2017), etc. Despite this NLP progress for CRM, often small and medium-sized companies (SMEs) struggle with applying such recent technology due to the limited size, noise and imbalance in their datasets. General solutions to such challenges are transfer learning strategies (Ruder, 2019): feature extraction uses frozen model parameters after pretraining on an external (larger) training corpus, while finetuning continues training on the smaller in-domain corpus. In the large body of work adopting such strategies (e.g., Pan and Yang 2009), little effort has been put into addressing specific CRM use cas"
2021.naacl-main.21,N19-1423,0,0.183977,"f the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 220–225 June 6–11, 2021. ©2021 Association for Computational Linguistics customer service database for decision support and machine fault diagnosis. Gupta (2011) extracted a set of sentiment and syntactic features from tweets for customer problem identification tasks. Molino et al. (2018) introduced the Customer Obsession Ticket Assistant for ticket resolution, using feature engineering techniques and encoder-decoder models. Highly popular pretrained language models, such as BERT (Devlin et al., 2019), have also been explored for different customer service tasks: Hardalov et al. (2019) considered re-ranking candidate answers in chatbots, while Deng et al. (2020) proposed BERT-based topic prediction for incoming customer requests. Although the performance gains obtained by pretraining language models are well-established, they need further exploration in terms of multilinguality. Some studies (Pires et al., 2019; Karthikeyan et al., 2019; Wu et al., 2019) have investigated the transferability of multilingual models on different tasks, but they do not consider the effect of progressive pretr"
2021.naacl-main.21,W10-0202,0,0.0282088,"Missing"
2021.naacl-main.21,2020.acl-main.740,0,0.0298772,"Missing"
2021.naacl-main.21,2021.ccl-1.108,0,0.0579054,"Missing"
2021.naacl-main.21,2020.emnlp-demos.2,0,0.0181433,"they need further exploration in terms of multilinguality. Some studies (Pires et al., 2019; Karthikeyan et al., 2019; Wu et al., 2019) have investigated the transferability of multilingual models on different tasks, but they do not consider the effect of progressive pretraining on a smaller and less diverse multilingual corpus, as we will do. 3 3.1 Methodology Architecture We selected some of the most popular publicly available pretrained language models to explore transfer learning properties for CRM classification tasks: RoBERTa (Liu et al., 2019), XLM (Conneau et al., 2020), and BERTweet (Nguyen et al., 2020). These models are pretrained on the English Wikipedia and BookCorpus (Zhu et al., 2015), CommonCrawl in 100 languages, and 850M English tweets, respectively. The XLM and BERTweet pretraining procedure is based on RoBERTa, which itself is a transformer-based Masked Language Model (MLM; Devlin et al., 2019). All of these models require a different classifier ‘head’ for each target task to estimate the probability of a class label. 3.2 Transfer Strategies The following notations are used throughout the rest of this paper to describe pretraining stages: • π – further pretraining the original MLM"
2021.naacl-main.21,P19-1493,0,0.0226528,"ustomer Obsession Ticket Assistant for ticket resolution, using feature engineering techniques and encoder-decoder models. Highly popular pretrained language models, such as BERT (Devlin et al., 2019), have also been explored for different customer service tasks: Hardalov et al. (2019) considered re-ranking candidate answers in chatbots, while Deng et al. (2020) proposed BERT-based topic prediction for incoming customer requests. Although the performance gains obtained by pretraining language models are well-established, they need further exploration in terms of multilinguality. Some studies (Pires et al., 2019; Karthikeyan et al., 2019; Wu et al., 2019) have investigated the transferability of multilingual models on different tasks, but they do not consider the effect of progressive pretraining on a smaller and less diverse multilingual corpus, as we will do. 3 3.1 Methodology Architecture We selected some of the most popular publicly available pretrained language models to explore transfer learning properties for CRM classification tasks: RoBERTa (Liu et al., 2019), XLM (Conneau et al., 2020), and BERTweet (Nguyen et al., 2020). These models are pretrained on the English Wikipedia and BookCorpus ("
2021.naacl-main.21,P19-1495,0,0.0463894,"Missing"
2021.naacl-main.21,N19-5004,0,0.0273483,"ocessing (NLP) can assist by providing insights to optimize customer interactions, but also in real-time tasks: (i) detect emotions (Gupta et al., 2010), (ii) categorize or prioritize customer tickets (Molino et al., 2018), (iii) aid in virtual assistants through natural language understanding and/or generation (Cui et al., 2017), etc. Despite this NLP progress for CRM, often small and medium-sized companies (SMEs) struggle with applying such recent technology due to the limited size, noise and imbalance in their datasets. General solutions to such challenges are transfer learning strategies (Ruder, 2019): feature extraction uses frozen model parameters after pretraining on an external (larger) training corpus, while finetuning continues training on the smaller in-domain corpus. In the large body of work adopting such strategies (e.g., Pan and Yang 2009), little effort has been put into addressing specific CRM use cases that need to rely on social media data that is noisy, possibly multilingual, and domain-specific for a given company. In this paper, we analyze the possibilities and limitations of transfer learning for a number of CRM tasks, following up on the findings of Gururangan et al. (2"
2021.wassa-1.15,baccianella-etal-2010-sentiwordnet,0,0.0518732,"earch areas such as electoral politics (e.g. Bansal and Srivastava, 2018; Chiu and Hsu, 2018), political viewpoints and argumentation mining (e.g. Chen et al., 2010) and stock market predictions (e.g. 2.1 Implicit sentiment analysis from the author’s perspective Balahur et al. (2010) performed sentiment analysis on quotations in English newswire text. They defined the sentiment of named entities in quotations by applying sentiment lexicons to varying context windows inside the quotes. Jiang et al. (2017) combined a clustering algorithm with lexicon-based sentiment analysis using SentiWordNet (Baccianella et al., 2010) at the sentence level to distinguish between positive and negative attitudes from UK news sources towards climate change-related topics. A similar methodology was applied by Burscher et al. (2016) to analyze the framing of the topic of nuclear power in English news articles. They found that within the frame of nuclear accidents or waste, articles were much more negative compared to articles that focused on the effects of nuclear power on climate change, or its economic aspects. Nozza et al. (2017) presented a multi-view corpus enriched with different variations of sentiment annotations; inclu"
2021.wassa-1.15,balahur-etal-2010-sentiment,0,0.0949285,"Missing"
2021.wassa-1.15,P16-2004,0,0.0272978,"of sentiment annotations; including objective versus subjective labels, implicit versus explicit sentiment, emotion categories, irony annotations, and so on. While the study presents clear definitions of the categories, the accompanying corpus examples are rather confusing (e.g. with “Tonight @CinemaX #SuicideSquad!! Come to see #HarleyQuinn :)” as 139 an example of an objective text and “I went out the cinema after 15 minutes #suicidesquad” as an example of an implied opinion). Low inter-rater agreement scores also confirm the difficulty to distinguish between implicit and explicit opinions. Chen and Chen (2016) explored implicit aspectbased sentiment analysis in Chinese hotel reviews following the premise that implicit opinion expressions are located nearby explicit opinions. Fang et al. (2020) proposed an aspect-based approach to implicit opinion analysis of Chinese car reviews. They applied similarity metrics and clustering algorithms to extract and categorize feature expressions and aggregated their implicit sentiment based on pointwise mutual information (PMI). 2.2 Implicit sentiment analysis from the readers’ perspective cal elections with a classification accuracy of 0.56 F1 -score. To the bes"
2021.wassa-1.15,de-smedt-daelemans-2012-vreselijk,0,0.0401133,"Missing"
2021.wassa-1.15,2020.findings-emnlp.292,0,0.0147552,"ion, however, means that some of the semantics may be lost, which may have affected the results of this approach. 4.2.2 Machine Learning Approach to Event Polarity Detection Using machine learning, we investigated a featurebased and end-to-end architecture. For the featurebased approach, we applied Support Vector Machines using the LibSVM library (Chang and Lin, 2011). For the latter approach, we applied two state-of-the-art transformer-based architectures for Dutch, i.e. BERTje (Vries et al., 2019) and Rob6 7 https://github.com/SenticNet/concept-parser. Translations done on 22/09/2020. BERT (Delobelle et al., 2020). While both models are based on the BERT architecture originally released for English (Devlin et al., 2019), they were each pre-trained on different corpora. BERTje is pre-trained on a 12 GB Dutch corpus composed of different genres, including books, social media data, Wikipedia and -especially relevant for our task- newswire text. By contrast, RobBERT is based on the Dutch section of the OSCAR corpus (Ortiz Su´arez et al., 2019), a 39 GB large subcorpus of the Common Crawl corpus8 , the largest web crawl corpus available. Although the latter is pre-trained on much more data, we expect BERTje"
2021.wassa-1.15,N19-1423,0,0.00859235,"h. 4.2.2 Machine Learning Approach to Event Polarity Detection Using machine learning, we investigated a featurebased and end-to-end architecture. For the featurebased approach, we applied Support Vector Machines using the LibSVM library (Chang and Lin, 2011). For the latter approach, we applied two state-of-the-art transformer-based architectures for Dutch, i.e. BERTje (Vries et al., 2019) and Rob6 7 https://github.com/SenticNet/concept-parser. Translations done on 22/09/2020. BERT (Delobelle et al., 2020). While both models are based on the BERT architecture originally released for English (Devlin et al., 2019), they were each pre-trained on different corpora. BERTje is pre-trained on a 12 GB Dutch corpus composed of different genres, including books, social media data, Wikipedia and -especially relevant for our task- newswire text. By contrast, RobBERT is based on the Dutch section of the OSCAR corpus (Ortiz Su´arez et al., 2019), a 39 GB large subcorpus of the Common Crawl corpus8 , the largest web crawl corpus available. Although the latter is pre-trained on much more data, we expect BERTje to be better suited for the current task. SVM parameter settings for the classifier and feature extraction"
2021.wassa-1.15,W17-4205,0,0.0206603,"ons), as well as from the reader’s (i.e. implicit sentiment). Research on implied opinions is prevalent in research areas such as electoral politics (e.g. Bansal and Srivastava, 2018; Chiu and Hsu, 2018), political viewpoints and argumentation mining (e.g. Chen et al., 2010) and stock market predictions (e.g. 2.1 Implicit sentiment analysis from the author’s perspective Balahur et al. (2010) performed sentiment analysis on quotations in English newswire text. They defined the sentiment of named entities in quotations by applying sentiment lexicons to varying context windows inside the quotes. Jiang et al. (2017) combined a clustering algorithm with lexicon-based sentiment analysis using SentiWordNet (Baccianella et al., 2010) at the sentence level to distinguish between positive and negative attitudes from UK news sources towards climate change-related topics. A similar methodology was applied by Burscher et al. (2016) to analyze the framing of the topic of nuclear power in English news articles. They found that within the frame of nuclear accidents or waste, articles were much more negative compared to articles that focused on the effects of nuclear power on climate change, or its economic aspects."
2021.wassa-1.15,E09-1046,0,0.0533553,"a training partition of 6,683 events and a test set of 742 events. The label distributions in both sets remained the same as in Table 4. Lexicon-based Approach to Event Sentiment Detection We first explored the effectiveness of two lexiconbased approaches to automatically determine implicit sentiment in news events. For the first approach, we relied on four sentiment lexicons for Dutch, including the Pattern lexicon (De Smedt and Daelemans, 2012) composed of 3,223 qualitative adjectives, an in-house sentiment lexicon with size n= 434 composed of manual review annotations, the Duoman lexicon (Jijkoun and Hofmann, 2009) composed of 8,757 wordforms and the NRC Hashtag Lexicon (Mohammad and Turney, 2013) including 13,683 entries5 . All lexicons were manually checked to filter irrelevant entries. The order in which these lexicons were consulted was determined by preliminary experiments (i.e. when a word had no match in the Pattern lexicon, the next step was to consult the in-house lexicon, next Duoman and finally NRC). For the second approach, we used SenticNet (Cambria and Hussain, 2015), an automatically constructed semantic knowledge resource based on common sense knowledge from the Open Mind Common Sense in"
2021.wassa-1.15,E17-1026,0,0.028321,"combined a clustering algorithm with lexicon-based sentiment analysis using SentiWordNet (Baccianella et al., 2010) at the sentence level to distinguish between positive and negative attitudes from UK news sources towards climate change-related topics. A similar methodology was applied by Burscher et al. (2016) to analyze the framing of the topic of nuclear power in English news articles. They found that within the frame of nuclear accidents or waste, articles were much more negative compared to articles that focused on the effects of nuclear power on climate change, or its economic aspects. Nozza et al. (2017) presented a multi-view corpus enriched with different variations of sentiment annotations; including objective versus subjective labels, implicit versus explicit sentiment, emotion categories, irony annotations, and so on. While the study presents clear definitions of the categories, the accompanying corpus examples are rather confusing (e.g. with “Tonight @CinemaX #SuicideSquad!! Come to see #HarleyQuinn :)” as 139 an example of an objective text and “I went out the cinema after 15 minutes #suicidesquad” as an example of an implied opinion). Low inter-rater agreement scores also confirm the"
2021.wassa-1.15,S15-2077,0,0.0135681,"e readers’ viewpoints regarding the framing of events. It was shown that, for instance, more homophobic newspapers reported on violence against gay people more vaguely compared to violence against straight people, as a result of which the former incidents were perceived less harmful. Conversely, more neutral newspapers were found to report on all types of violence in the same manner. In 2007, a shared task was set up by Strapparava and Mihalcea (2007) focusing on valence and emotion classification of English newspaper headlines. The SemEval-2015 task on implicit sentiment detection of events (Russo et al., 2015) focused on predicting whether structured events (i.e. newspaper sentences containing the pattern “I—we + [verbal/nominal keyword]”) are considered pleasant or unpleasant. While most work has been done on English data, similar approaches to detect sentiment and emotions in news from the readers’ perspective have been applied to Czech (Burget et al., 2011), Chinese (Lin et al., 2008) and Dutch (Atteveldt et al., 2008). Related research has also focused on sentiment analysis of named entities in news (Godbole et al., 2007) and sentiment analysis for fake news detection (Kula et al., 2020; Bhutan"
2021.wassa-1.15,S07-1013,0,0.138495,"ng effects on violence perception in news reporting of homophobic attacks. Apart from investigating author’s perceptions, they performed a manual content analysis to investigate readers’ viewpoints regarding the framing of events. It was shown that, for instance, more homophobic newspapers reported on violence against gay people more vaguely compared to violence against straight people, as a result of which the former incidents were perceived less harmful. Conversely, more neutral newspapers were found to report on all types of violence in the same manner. In 2007, a shared task was set up by Strapparava and Mihalcea (2007) focusing on valence and emotion classification of English newspaper headlines. The SemEval-2015 task on implicit sentiment detection of events (Russo et al., 2015) focused on predicting whether structured events (i.e. newspaper sentences containing the pattern “I—we + [verbal/nominal keyword]”) are considered pleasant or unpleasant. While most work has been done on English data, similar approaches to detect sentiment and emotions in news from the readers’ perspective have been applied to Czech (Burget et al., 2011), Chinese (Lin et al., 2008) and Dutch (Atteveldt et al., 2008). Related resear"
2021.wassa-1.15,strapparava-valitutti-2004-wordnet,0,0.504816,"Missing"
2021.wassa-1.15,P10-1059,0,0.0211303,"e mere impact of news events on their audiences without having readers’ reactions at hand, the focus of this research lies on detecting implicit sentiment rather than implied opinions. Irrespective of potential framing, when consuming news, readers may infer a positive or negative impression of an event or topic based on world knowledge, cultural background, historical context or even personal experiences. Such text spans are known as “statements or phrases that describe positive or negative factual information about something without conveying a private state” (Wilson, 2008, p. 2741). Later, Toprak et al. (2010) coined the term ‘polar facts’ to refer to such statements. In what follows, we discuss some seminal studies on sentiment analysis in factual text from both the author’s and readers’ perspectives. - Can we automatically detect the implicit sentiment evoked by fine-grained news events? 2 Related Research While sentiment and emotion analysis have a long history in review analysis and recommendation applications using user-generated content, one of the first studies on subjectivity analysis focused on newswire text (Bruce and Wiebe, 1999). This work, among others, has inspired researchers to appl"
2021.wassa-1.15,J18-4010,1,0.853285,"Missing"
2021.wassa-1.15,wilson-2008-annotating,0,0.0686398,"ng newswire text. Looking at the mere impact of news events on their audiences without having readers’ reactions at hand, the focus of this research lies on detecting implicit sentiment rather than implied opinions. Irrespective of potential framing, when consuming news, readers may infer a positive or negative impression of an event or topic based on world knowledge, cultural background, historical context or even personal experiences. Such text spans are known as “statements or phrases that describe positive or negative factual information about something without conveying a private state” (Wilson, 2008, p. 2741). Later, Toprak et al. (2010) coined the term ‘polar facts’ to refer to such statements. In what follows, we discuss some seminal studies on sentiment analysis in factual text from both the author’s and readers’ perspectives. - Can we automatically detect the implicit sentiment evoked by fine-grained news events? 2 Related Research While sentiment and emotion analysis have a long history in review analysis and recommendation applications using user-generated content, one of the first studies on subjectivity analysis focused on newswire text (Bruce and Wiebe, 1999). This work, among o"
2021.wassa-1.22,2020.findings-emnlp.148,0,0.0208776,"and Gurevych (2019). It uses so-called siamese and triplet network structures, or a “twin network”, that processes two sentences in the same way simultaneously. SBERT provides embeddings at a sentence level with the same size as the original BERT. • Pre-trained Word2Vec from the Gensim package6 . This model includes 300-dimension word vectors for a vocabulary with 3 million words and phrases trained on a Google News dataset. It is included here because of its popularity in NLP tasks. • Twitter-roBERTa-based model for Emotion Recognition, one of the seven fine-tuned roBERTa models presented by Barbieri et al. (2020). Each described model was trained for a specific task and provided an embedding at the token level similar to BERT. The model that we consider was trained for the emotion detection task (E-c) using a different collection of tweets from the same authors of SemEval 2018 Task 1 (Mohammad et al., 2018), in which the emotions anger, joy, sadness, and optimism are used. • DeepMoji7 is a state-of-the-art sentiment embedding model, pre-trained on millions of tweets with emojis to recognize emotions and sarcasm. We used its implementation on PyTorch by Huggingface8 , which provides for each sentence a"
2021.wassa-1.22,D18-2029,0,0.0278508,"Missing"
2021.wassa-1.22,2020.aacl-main.46,0,0.152357,"se, given that the effectiveness of a solution is evaluated only using the Pearson Correlation Coefficient (see formula (3) in Section 3.5). In general, machine learning models in the Natural Language Processing (NLP) field rarely explain their predicted labels. This inspires the need for explainable models, which concentrate on interpreting outputs and the connection of inputs with outputs. For example, Liu et al. (2019) present an explainable classification approach that solves NLP tasks with comparable accuracy to neural networks and also generates explanations for its solutions. Recently, Danilevsky et al. (2020) presented an overview of explainable methods for NLP tasks. Apart from focusing on explanations of model predictions, they also discuss the most important techniques to generate and visualize explanations. The paper also discusses evaluation techniques to measure the quality of the obtained explanations, which could be useful in future work. In this paper, we consider one of the simplest explainable models: the kNN method. In the context of NLP, kNN has recently been applied by (Fatema Rajani et al., 2020) as a backoff method for classifiers based on BERT and RoBERTa (see Section 3.2). In par"
2021.wassa-1.22,N19-1423,0,0.00917586,"ence Encoder (USE) (Cer et al., 2018) is a sentence-level embedding approach developed by the TensorFlow team9 . It provides a 512-dimensional vector for a sentence or even a whole paragraph that can be used for different tasks such as text classification, sentence similarity, etc. USE was trained with a deep averaging network (DAN) encoder on several data sources. The model is available in two options: trained with a DAN and with a Transformer encoder. After basic experiments, we chose the second one for further experiments. • Bidirectional Encoder Representations from Transformers (BERT) by Devlin et al. (2019). Sentence-level embeddings are applied to each tweet as a whole, while for word (or token) level embeddings, we represent a tweet vector as the mean of its words’ (tokens’) vectors. 3.3 Emotional lexicon vocabularies As an additional source of information to complement tweet embeddings, we also consider lexicon scores. Emotional lexicons are vocabularies that provide scores of different emotion intensity for a word. In our experiments, we use the following English lexicons: 5 • Valence Arousal Dominance (NRC VAD) lexicon (20,007 words) (Mohammad, 2018a) – each word has a score (float number b"
2021.wassa-1.22,S18-1002,0,0.0199629,"e competition results. In Section 6, we examine some of the test samples with correct and wrong predictions to see how we can use our model’s interpretability to explain the obtained results. Finally, in Section 7, we discuss our results and consider possible ways to improve them. 203 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 203–212 April 19, 2021. ©2021 Association for Computational Linguistics 2 Related work 3 First, we briefly recall the most successful proposals2 to the SemEval-2018 task. The winning approach (Duppada et al., 2018) uses tweet embedding vectors in ensembles of XGBoost and Random Forest classification models. The runners-up (Gee and Wang, 2018) perform transfer learning with Long Short Term Memory (LSTM) neural networks. The third-place contestants (Rozental and Fleischer, 2018) train an ensemble of a complex model consisting of Gated-Recurrent-Units (GRU) using a convolution neural network (CNN) as an attention mechanism. It is clear that the leaderboard is dominated by solutions that are neither simple nor interpretable. This comes as no surprise, given that the effectiveness of a solution is evaluated"
2021.wassa-1.22,S18-1056,0,0.0204541,"r model’s interpretability to explain the obtained results. Finally, in Section 7, we discuss our results and consider possible ways to improve them. 203 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 203–212 April 19, 2021. ©2021 Association for Computational Linguistics 2 Related work 3 First, we briefly recall the most successful proposals2 to the SemEval-2018 task. The winning approach (Duppada et al., 2018) uses tweet embedding vectors in ensembles of XGBoost and Random Forest classification models. The runners-up (Gee and Wang, 2018) perform transfer learning with Long Short Term Memory (LSTM) neural networks. The third-place contestants (Rozental and Fleischer, 2018) train an ensemble of a complex model consisting of Gated-Recurrent-Units (GRU) using a convolution neural network (CNN) as an attention mechanism. It is clear that the leaderboard is dominated by solutions that are neither simple nor interpretable. This comes as no surprise, given that the effectiveness of a solution is evaluated only using the Pearson Correlation Coefficient (see formula (3) in Section 3.5). In general, machine learning models in the Natura"
2021.wassa-1.22,P19-1560,0,0.0254696,"sing a convolution neural network (CNN) as an attention mechanism. It is clear that the leaderboard is dominated by solutions that are neither simple nor interpretable. This comes as no surprise, given that the effectiveness of a solution is evaluated only using the Pearson Correlation Coefficient (see formula (3) in Section 3.5). In general, machine learning models in the Natural Language Processing (NLP) field rarely explain their predicted labels. This inspires the need for explainable models, which concentrate on interpreting outputs and the connection of inputs with outputs. For example, Liu et al. (2019) present an explainable classification approach that solves NLP tasks with comparable accuracy to neural networks and also generates explanations for its solutions. Recently, Danilevsky et al. (2020) presented an overview of explainable methods for NLP tasks. Apart from focusing on explanations of model predictions, they also discuss the most important techniques to generate and visualize explanations. The paper also discusses evaluation techniques to measure the quality of the obtained explanations, which could be useful in future work. In this paper, we consider one of the simplest explainab"
2021.wassa-1.22,P18-1017,0,0.0126342,"tations from Transformers (BERT) by Devlin et al. (2019). Sentence-level embeddings are applied to each tweet as a whole, while for word (or token) level embeddings, we represent a tweet vector as the mean of its words’ (tokens’) vectors. 3.3 Emotional lexicon vocabularies As an additional source of information to complement tweet embeddings, we also consider lexicon scores. Emotional lexicons are vocabularies that provide scores of different emotion intensity for a word. In our experiments, we use the following English lexicons: 5 • Valence Arousal Dominance (NRC VAD) lexicon (20,007 words) (Mohammad, 2018a) – each word has a score (float number between 0 and 1) for Valence, Arousal, and Dominance. http://www.nltk.org/nltk_data/ https://radimrehurek.com/gensim/ models/word2vec.html 7 https://deepmoji.mit.edu/ 8 https://github.com/huggingface/ torchMoji 9 https://www.tensorflow.org/hub/ tutorials/semantic_similarity_with_tf_ hub_universal_encoder 6 10 https://github.com/dnanhkhoa/ pytorch-pretrained-BERT/blob/master/ examples/extract_features.py 205 • Emotional Lexicon (EMOLEX) (14,182 words) lexicon (Mohammad and Turney, 2013) – each word has ten scores (0 or 1), one per emotion: anger, anticip"
2021.wassa-1.22,S18-1001,0,0.304879,"ities can help to enhance results’ reliability and guide error analysis. In particular, we apply the weighted kNN model to the shared emotion detection task in tweets from SemEval-2018. Tweets are represented using different text embedding methods and emotion lexicon vocabulary scores, and classification is done by an ensemble of weighted kNN models. Our best approaches obtain results competitive with state-of-the-art solutions and open up a promising alternative path to neural network methods. 1 Introduction In this paper, we consider SemEval-2018 Task 1 EI-oc: Affect in Tweets for English1 (Mohammad et al., 2018). This is a classification problem in which data instances are raw tweets, labeled with scores expressing how much each of four considered emotions (anger, sadness, joy, and fear) are present. Our target is to implement the weighted k Nearest Neighbor (wkNN) algorithm to detect emotions in tweets. In doing so, we consider different ways 1 https://competitions.codalab.org/ competitions/17751 of tweet embeddings and combine them with various emotional lexicons, which provide an emotional score for each word. The motivation for using wkNN is to show the potential of a simple, interpretable machin"
2021.wassa-1.22,L18-1027,0,0.0221292,"tations from Transformers (BERT) by Devlin et al. (2019). Sentence-level embeddings are applied to each tweet as a whole, while for word (or token) level embeddings, we represent a tweet vector as the mean of its words’ (tokens’) vectors. 3.3 Emotional lexicon vocabularies As an additional source of information to complement tweet embeddings, we also consider lexicon scores. Emotional lexicons are vocabularies that provide scores of different emotion intensity for a word. In our experiments, we use the following English lexicons: 5 • Valence Arousal Dominance (NRC VAD) lexicon (20,007 words) (Mohammad, 2018a) – each word has a score (float number between 0 and 1) for Valence, Arousal, and Dominance. http://www.nltk.org/nltk_data/ https://radimrehurek.com/gensim/ models/word2vec.html 7 https://deepmoji.mit.edu/ 8 https://github.com/huggingface/ torchMoji 9 https://www.tensorflow.org/hub/ tutorials/semantic_similarity_with_tf_ hub_universal_encoder 6 10 https://github.com/dnanhkhoa/ pytorch-pretrained-BERT/blob/master/ examples/extract_features.py 205 • Emotional Lexicon (EMOLEX) (14,182 words) lexicon (Mohammad and Turney, 2013) – each word has ten scores (0 or 1), one per emotion: anger, anticip"
2021.wassa-1.22,D19-1410,0,0.0133354,"it into tokens (for example, the word ”tokens” will be resented as ”tok”, ”##en”, ”##s”), and for each token, a vector was created. • Stop-word removal: for this process, the list of stop-words from the NLTK package5 is used. We do not apply preprocessing or stop-word removal a priori, but rather examine whether they improve the classification during the experimental stage. 3.2 Tweet embedding To perform classification, each tweet is represented by a vector or set of vectors, using the following word embedding techniques: • Sentence-BERT (SBERT) is a modified and tuned BERT model presented in Reimers and Gurevych (2019). It uses so-called siamese and triplet network structures, or a “twin network”, that processes two sentences in the same way simultaneously. SBERT provides embeddings at a sentence level with the same size as the original BERT. • Pre-trained Word2Vec from the Gensim package6 . This model includes 300-dimension word vectors for a vocabulary with 3 million words and phrases trained on a Google News dataset. It is included here because of its popularity in NLP tasks. • Twitter-roBERTa-based model for Emotion Recognition, one of the seven fine-tuned roBERTa models presented by Barbieri et al. (20"
2021.wassa-1.22,S18-1033,0,0.0203522,"ways to improve them. 203 Proceedings of the 11th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis, pages 203–212 April 19, 2021. ©2021 Association for Computational Linguistics 2 Related work 3 First, we briefly recall the most successful proposals2 to the SemEval-2018 task. The winning approach (Duppada et al., 2018) uses tweet embedding vectors in ensembles of XGBoost and Random Forest classification models. The runners-up (Gee and Wang, 2018) perform transfer learning with Long Short Term Memory (LSTM) neural networks. The third-place contestants (Rozental and Fleischer, 2018) train an ensemble of a complex model consisting of Gated-Recurrent-Units (GRU) using a convolution neural network (CNN) as an attention mechanism. It is clear that the leaderboard is dominated by solutions that are neither simple nor interpretable. This comes as no surprise, given that the effectiveness of a solution is evaluated only using the Pearson Correlation Coefficient (see formula (3) in Section 3.5). In general, machine learning models in the Natural Language Processing (NLP) field rarely explain their predicted labels. This inspires the need for explainable models, which concentrate"
2021.wassa-1.27,N18-1173,0,0.0570743,"Missing"
2021.wassa-1.27,2020.acl-main.112,0,0.02651,"Missing"
2021.wassa-1.27,W17-5218,1,0.889054,"Missing"
2021.wassa-1.27,de-smedt-daelemans-2012-vreselijk,0,0.0313221,"Missing"
2021.wassa-1.27,2020.findings-emnlp.292,0,0.0891451,"RTje, but that directly adding lexicon information to transformers does not improve performance. In the meta-learning approach, lexicon information does have a positive effect on BERTje, but not on RobBERT. This suggests that more emotional information is already contained within this latter language model. 1 Introduction Computational analysis of affect in Dutch texts is mostly restricted to polarity analysis (negative/positive/neutral), for which we know a tradition of lexicon-based approaches. Recently, a BERT-based model, BERTje (de Vries et al., 2019), and a RoBERTa-based model, RobBERT (Delobelle et al., 2020), have been created for Dutch, and they have achieved promising results on the task of sentiment analysis. For emotion detection, however, these models have not yet been evaluated. In a first step towards improving emotion detection for Dutch, we will evaluate BERTje and RobBERT on the task of emotion detection. Instead of casting aside the many efforts that have been made in the creation of Dutch sentiment and emotion lexica, we will investigate whether transformers and affect lexica can complement each other. Attempts of combining BERT models with additional features have already been succes"
2021.wassa-1.27,N19-1423,0,0.020198,"n classification and emotion regression with the dimensions valence, arousal and dominance (VAD), and this in both domains. This injection occurs just before the prediction layer by concatenating the [CLS] token, which is normally used on its own as input for the classification, with the lexicon vector. This concatenated vector goes through a pre-classifier (linear layer with 2,048 nodes) and then to the prediction layer with Sigmoid activation function. The model architecture is shown in Figure 1. Two Dutch transformer models are investigated: BERTje (de Vries et al., 2019), based on BERT by Devlin et al. (2019) and RobBERT (Delobelle et al., 2020), the Dutch version of the robustly optimized RoBERTa (Liu et al., 2019). RobBERT is trained on 39GB of common crawl data (Su´arez et al., 2019), while BERTje is trained on only 12GB (including multiple genres). Both models are implemented with HuggingFace’s Transformers library (Wolf et al., 2019). We use AdamW optimizer (Loshchilov and Hutter, 2017) and the ReduceLROnPlateau learning rate scheduler with lr = 5e − 5. The loss function is Binary Cross Entropy for the classification task and Mean Squared Error loss for regression. The maximum sequence length"
2021.wassa-1.27,E09-1046,0,0.0567268,"100 for testing (same splits for all models/tasks). 3.2 Lexicon information We investigate whether lexicon information and transformer models can be complementary either by injecting lexicon information directly into the transformer architecture or by using a metalearning approach in which predictions from transformer models are combined with lexicon features. Both models require the creation of a lexicon vector per target sentence. For the creation of this vector, we combine seven existing open-source Dutch sentiment and emotion lexica, namely Pattern (De Smedt and Daelemans, 2012), Duoman (Jijkoun and Hofmann, 2009), LIWC (Boot et al., 2017), NRC Emotion (Mohammad and Turney, 2013), NRC VAD (Mohammad, 2018), Memolon (Buechel et al., 2020) and the VAD norms by Moors et al. (2013). For each word in the target sentence, lexicon values are obtained through a lookup in each affect lexicon. These values are then averaged over the words in the target sentence. The vector is 33-dimensional, as all lexica include values for multiple emotion categories or dimensions which add up to 33 in total. For lexica that do not have entries for any of the words in the sentence, the respective value in the lexicon vector is 0"
2021.wassa-1.27,2020.alw-1.5,0,0.0271979,"ing results on the task of sentiment analysis. For emotion detection, however, these models have not yet been evaluated. In a first step towards improving emotion detection for Dutch, we will evaluate BERTje and RobBERT on the task of emotion detection. Instead of casting aside the many efforts that have been made in the creation of Dutch sentiment and emotion lexica, we will investigate whether transformers and affect lexica can complement each other. Attempts of combining BERT models with additional features have already been successful for tasks like abusive language and sarcasm detection (Koufakou et al., 2020; Kumar et al., 2021). We consider two architectures. In the first one, we inject lexicon information in the transformer model before the prediction layer. We do this by concatenating the [CLS] token of the target sentence (which BERT and RoBERTa models use as input for prediction) with a lexicon vector obtained from seven Dutch affect lexica. In the second approach, we employ a meta-learning architecture and use a support vector machine (SVM) that learns from the transformer model’s output. The predictions from the transformer are concatenated with the lexicon vector and used as input for the"
2021.wassa-1.27,2021.ccl-1.108,0,0.0521578,"Missing"
2021.wassa-1.27,S18-1001,0,0.0324143,"Missing"
2021.wassa-1.27,L18-1030,0,0.0135217,"btained from seven Dutch affect lexica. In the second approach, we employ a meta-learning architecture and use a support vector machine (SVM) that learns from the transformer model’s output. The predictions from the transformer are concatenated with the lexicon vector and used as input for the SVM. We evaluate our models on 1,000 Dutch Tweets and 1,000 transcribed utterances from Flemish TVshows. As multiple researchers have emphasized the need of studying emotions not only in terms of basic emotions, but based on dimensions like valence, arousal and dominance as well (Buechel and Hahn, 2016; Mohammad and Kiritchenko, 2018), the data has been annotated in a bi-representational design: both with categorical annotations for anger, joy, fear, love, sadness or neutral, and scores for the dimensions valence, arousal and dominance. First we will discuss related work on Dutch emotion detection and other attempts on combining transformer models with additional features in Section 2. In Section 3, we will describe the methodology of our experiments and in Section 4, we report the results. We end with a conclusion in Section 5. 2 Related Work Although most emotion detection research deals with the English language, recent"
2021.wassa-1.27,P18-1017,0,0.0116648,"con information and transformer models can be complementary either by injecting lexicon information directly into the transformer architecture or by using a metalearning approach in which predictions from transformer models are combined with lexicon features. Both models require the creation of a lexicon vector per target sentence. For the creation of this vector, we combine seven existing open-source Dutch sentiment and emotion lexica, namely Pattern (De Smedt and Daelemans, 2012), Duoman (Jijkoun and Hofmann, 2009), LIWC (Boot et al., 2017), NRC Emotion (Mohammad and Turney, 2013), NRC VAD (Mohammad, 2018), Memolon (Buechel et al., 2020) and the VAD norms by Moors et al. (2013). For each word in the target sentence, lexicon values are obtained through a lookup in each affect lexicon. These values are then averaged over the words in the target sentence. The vector is 33-dimensional, as all lexica include values for multiple emotion categories or dimensions which add up to 33 in total. For lexica that do not have entries for any of the words in the sentence, the respective value in the lexicon vector is 0. Transformer model In this architecture, we inject the lexicon information into the transfor"
2021.wassa-1.27,W18-6205,0,0.0464366,"Missing"
2021.wassa-1.27,2020.coling-main.535,0,0.0409978,"Missing"
2021.wassa-1.27,W11-1713,0,0.056152,"Missing"
C00-1048,J95-4004,0,0.884311,"This paper describes the use of rule induction techniques for the automatic extraction of phonemic knowledge and rules from pairs of pronunciation lexica. This extracted knowledge allows the adaptation of speech processing systems to regional variants of a language. As a case study, we apply the approach to Northern Dutch and Flemish (the variant of Dutch spoken in Flanders, a part of Belgium), based on Celex and Fonilex, pronunciation lexica for Northern Dutch and Flemish, respectively. In our study, we compare two rule induction techniques, TransformationBased Error-Driven Learning (TBEDL) (Brill, 1995) and C5.0 (Quinlan, 1993), and evaluate the extracted knowledge quantitatively (accuracy) and qualitatively (linguistic relevance of the rules). We conclude that, whereas classi cation-based rule induction with C5.0 is more accurate, the transformation rules learned with TBEDL can be more easily interpreted. 1 Introduction A central component of speech processing systems is a pronunciation lexicon de ning the relationship between the spelling and pronunciation of words. Regional variants of a language may di er considerably in their pronunciation. Once a speaker from a particular region is det"
C00-1048,J95-2004,0,0.132364,"techniques, viz. Transformation-Based Error Driven Learning (TBEDL) (Brill, 1995) and C5.0 (Quinlan, 1993). In the process of Transformation-Based Error-Driven Learning, transformation rules are learned by comparing a corpus that is annotated by an initial state annotator to a correctly annotated corpus, which is called the 	ruth&quot;. During that comparison, an ordered list of transformation rules is learned. This ordering implies that the application of an earlier rule sometimes makes it possible for a later rule to apply (so-called feeding&quot;). In other cases, as also described in the work of Roche and Schabes (1995), a given structure fails to undergo a rule as a consequence of some earlier rule (leeding&quot;). These rules are applied to the output of the initial state annotator in order to bring that output closer to the 	ruth&quot;. A rule consists of two parts: a transformation and a 	riggering environment&quot;. For each iteration in the learning process, it is investigated for each possible rule how many mistakes can be corrected through application of that rule. The rule which causes the greatest error reduction is retained. Figure 1 shows the TBEDL learning process applied to the comparison of the Celex rep"
C08-1067,P98-1074,0,0.0375799,"ic terminology extraction for all languages taking French as the pivot language 2. improved consistency of the database entries, e.g. through the automatic replacement of synonyms by the preferred term (decided in (1)) c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. This paper presents a novel terminology extraction method applied to the French-English part of the database. There is a long tradition of research into bilingual terminology extraction (Kupiec, 1993), (Gaussier, 1998). In most systems, candidate terms are first identified in the source language based on predefined PoS patterns – for French, N N, N Prep N, and N Adj are typical patterns. In a second step, the translation candidates are extracted from the bilingual corpus based on word alignments. In recent work, Itagaki et al. (2007) use the phrase table derived from the GIZA++ alignments to identify the translations. We use a different and more flexible approach. We developed a sub-sentential alignment system that links linguistically motivated phrases in parallel texts based on lexical correspondences and"
C08-1067,P02-1050,0,0.0843761,"Missing"
C08-1067,P07-2045,0,0.00495923,"ength sentences (8-19 words) and long sentences (&gt; 19 words). Each test corpus contains approximately 4,500 words. We also compiled a development corpus containing sentences of varying sentence length to debug the system and to determine the value of the thresholds used in the system. The formal characteristics of the test corpora and the training corpus are given in Table 2. 3 Sub-sentential alignment Sub-sentential alignments – and the underlying word alignments – are used in the context of Machine Translation to create phrase tables for phrase-based statistical machine translation systems (Koehn et al., 2007). A stand-alone subsentential alignment module however, is also useful for human translators if incorporated in CATtools, e.g. sophisticated bilingual concordance systems, or in sub-sentential translation memory systems (Gotti et al., 2005). A quite obvious application of a sub-sentential alignment system is the creation of bilingual dictionaries and terminology extraction from bilingual corpora (Melamed, 2000), (Itagaki et al., 2007). In the context of statistical machine translation, GIZA++ is one of the most widely used word alignment toolkits. GIZA++ implements the IBM models and is used i"
C08-1067,P93-1003,0,0.234813,"ined: 1. automatic terminology extraction for all languages taking French as the pivot language 2. improved consistency of the database entries, e.g. through the automatic replacement of synonyms by the preferred term (decided in (1)) c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. This paper presents a novel terminology extraction method applied to the French-English part of the database. There is a long tradition of research into bilingual terminology extraction (Kupiec, 1993), (Gaussier, 1998). In most systems, candidate terms are first identified in the source language based on predefined PoS patterns – for French, N N, N Prep N, and N Adj are typical patterns. In a second step, the translation candidates are extracted from the bilingual corpus based on word alignments. In recent work, Itagaki et al. (2007) use the phrase table derived from the GIZA++ alignments to identify the translations. We use a different and more flexible approach. We developed a sub-sentential alignment system that links linguistically motivated phrases in parallel texts based on lexical c"
C08-1067,J00-2004,0,0.0130313,"l alignments – and the underlying word alignments – are used in the context of Machine Translation to create phrase tables for phrase-based statistical machine translation systems (Koehn et al., 2007). A stand-alone subsentential alignment module however, is also useful for human translators if incorporated in CATtools, e.g. sophisticated bilingual concordance systems, or in sub-sentential translation memory systems (Gotti et al., 2005). A quite obvious application of a sub-sentential alignment system is the creation of bilingual dictionaries and terminology extraction from bilingual corpora (Melamed, 2000), (Itagaki et al., 2007). In the context of statistical machine translation, GIZA++ is one of the most widely used word alignment toolkits. GIZA++ implements the IBM models and is used in Moses (Koehn et al., 2007) to generate the initial source-to-target and targetto-source word alignments after which some symmetrization heuristics combine the alignments of both translation directions. We present an alternative – linguistically-based – approach, that starts from a lexical probabilistic bilingual dictionary generated by IBM Model One. 530 3.1 Architecture The basic idea behind our approach is"
C08-1067,moore-2002-fast,0,0.0839103,"kes as input sentence-aligned texts, together with additional linguistic annotations (part-of-speech codes and lemmas) for the source and the target text. In the first step of the process, the source and target sentences are divided into chunks based on PoS information, and lexical correspondences are retrieved from a bilingual dictionary. During anchor chunk alignment, the sub-sentential aligner links chunks based on lexical correspondences and chunk similarity. 3.2 Bilingual Dictionary We used the Perl implementation of IBM Model One that is part of the Microsoft Bilingual Sentence Aligner (Moore, 2002) to derive a bilingual dictionary from a parallel corpus. IBM Model One 1 The more syntax-aware SMT systems assume that to a certain extent syntactic relationships in one language directly map to syntactic relationships in the other, which Hwa (2002) calls the Direct Correspondence Assumption. is a purely lexical model: it only takes into account word frequencies of source and target sentences2 . The IBM models allow only 1:n word mappings, and are therefore asymmetric. To overcome this problem, we ran the model in two directions: from French to English and from English to French. To get high-"
C08-1067,J03-1002,0,0.004563,"English PoS tagger often tags a past participle erroneously as a past tense. 532 We adapted the annotation guidelines of Macken (2007) to the French-English language pair, and used three different types of links: regular links for straightforward correspondences, fuzzy links for translation-specific shifts of various kinds, and null links for words for which no correspondence could be indicated. Figure 2 shows an example. Figure 2: Manual reference: regular links are indicated by x’s, fuzzy links and null links by 0’s To evaluate the system’s performance, we used the evaluation methodology of Och and Ney (2003). Och and Ney distinguished sure alignments (S) and possible alignments (P) and introduced the following redefined precision and recall measures: precision = |A ∩ P | |A ∩ S| , recall = |A| |S| (1) (2) We consider all regular links of the manual reference as sure alignments and all fuzzy and null links as possible alignments to compare the output of our system with the manual reference. We trained statistical translation models using Moses. Moses uses the GIZA++ toolkit (IBM Model 1-4) in both translation directions (source to target, target to source) and allows for different symmetrization h"
C08-1067,W00-0901,0,0.0775748,"is an accurate measure for finding the most surprisingly frequent words in a corpus. Low LL values on the other hand allow to retrieve common vocabulary with high frequencies in both corpora. We have created a frequency list for both corpora and calculated the Log-Likelihood values for each word in this frequency list. In the formula below, N corresponds to the number of words in the corpus, whereas the observed values O correspond to the real frequencies of a word in the corpus. The formula for calculating both the expected values (E) and the Log-Likelihood have been described in detail by (Rayson and Garside, 2000). Log-Likelihood Measure In order to detect single word terms that are distinctive enough to be kept in our bilingual lexicon, we have applied the Log-Likelihood measure (LL). This metric considers frequencies of words weighted over two different corpora (in our case a technical automotive corpus and the more general purpose corpus ”Le Monde”), in order to assign high LL-values to words having much higher or lower frequencies than expected. Daille (1995) Oi ) Ei (4) Manual inspection of the Log-Likelihood figures confirmed our hypothesis that more domainspecific terms in our corpus got high LL"
C08-1067,W00-0726,0,0.271103,"Missing"
C08-1067,C98-1071,0,\N,Missing
C16-1257,W10-2914,0,0.329908,"Missing"
C16-1257,W16-0425,0,0.175134,"Missing"
C16-1257,P11-2008,0,0.172382,"Missing"
C16-1257,S13-2093,0,0.0433394,"Missing"
C16-1257,maynard-greenwood-2014-cares,0,0.12366,"Missing"
C16-1257,D13-1066,0,0.181057,"Missing"
C16-1257,D11-1141,0,0.0670702,"ies (Table 1), as well as the extra non-ironic instances that were added (Section 3.2) are equally distributed among the train and test sets. 4.1 Preprocessing After constructing the corpus, all emoji were replaced by their name or a description using the Python Emoji module2 to facilitate annotation and processing of the data. Furthermore, we normalised hyperlinks and @-replies or mentions to http://someurl and @someuser, respectively. Other preprocessing steps involve tokenisation and PoS-tagging (Gimpel et al., 2011), lemmatisation (Van de Kauter et al., 2013) and named entity recognition (Ritter et al., 2011). 1 Due to a refinement of the annotations, the corpus statistics are slightly different from a first version of the annotated corpus described in Van Hee et al. (2016a). 2 https://github.com/carpedm20/emoji/. 2733 4.2 Information Sources For the automatic irony detection system, we implemented a variety of features that represent every instance within a (sparse) feature vector. • As lexical features, we included bags-of-words (BoW) features that represent a tweet as a ‘bag’ of its words or characters. We incorporated token unigrams and bigrams and character trigrams and fourgrams. Furthermore"
C16-1257,E12-2021,0,0.104504,"Missing"
C16-1257,H05-1044,0,0.0804356,"of interjections was added as a feature. Furthermore, we included a binary feature indicating a ‘clash’ between verb tenses in the tweet (see Reyes et al. (2013)). Finally, we integrated four features indicating the presence of named entities in a tweet: one binary feature and three numeric features, indicating (i) the number of named entities in the text, (ii) the number and (iii) frequency of tokens that are part of a named entity. • Six sentiment lexicon features were implemented based on existing sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (GI) (Stone et al., 1966), MPQA (Wilson et al., 2005), the NRC Emotion Lexicon (Mohammad and Turney, 2013), Liu’s opinion lexicon (Hu and Liu, 2004), and Bounce (K¨okciyan et al., 2013). For each lexicon, five numeric and one binary feature were derived: – the number of positive, negative and neutral lexicon words averaged over text length; – the overall tweet polarity (i.e., the sum of the values of the identified sentiment words); – the difference between the highest positive and lowest negative sentiment values; – a binary feature indicating whether there is a polarity contrast (i.e., at least one positive and one negative sentiment word from"
D19-5536,L18-1216,0,0.061556,"Missing"
D19-5536,de-clercq-etal-2014-towards,1,0.892761,"Missing"
D19-5536,R13-1024,1,0.848713,"Missing"
D19-5536,P06-2005,0,0.0523601,"se of homophonous graphemic variants of a word, abbreviations, spelling mistakes or letter transpositions are also used regularly (Eisenstein et al., 2014). Since NLP tools have originally been developed for and trained on standard language, these nonstandard forms adversely affect their performance. One of the computational approaches which has been suggested to overcome this problem is text normalization (Sproat et al., 2001). This approach envisages transforming the lexical variants to their canonical forms. In this way, standard NLP tools can be applied in a next step after normalization (Aw et al., 2006). Please note that for some NLP applications, e.g. sentiment analysis, it might be beneficial to keep some ’noise’ in the data. For example, the use of flooding or capital letters could be a good indicator of the emotion present in the text (Van Hee et al., 2017). However, for applications aiming at information extraction from text, normalization is needed to help to improve the performance of downstream NLP tasks (Schulz et al., 2016). Kobus et al. (2008) introduced three metaphors to refer to these normalization approaches: the spell checking, automatic speech recognition and machine transla"
D19-5536,P17-2090,0,0.0403068,"first at the word level and then at the character level. This approach revealed good results across various genres of UGC. However, a high number of phonetic alternations remained unresolved. Although the encoder-decoder model has shown its effectiveness in large datasets, it is much less effective when only a small number of sentence pairs is available (Sennrich et al., 2016b; Zoph et al., 2016). Automatic data augmentation is commonly used in vision and speech and can help train more robust models, particularly when using smaller datasets (Chatfield et al., 2014; Taylor and Nitschke, 2017). Fadaee et al. (2017) present Translation Data Augmentation (TDA), a method to improve the translation quality for low resource pairs (English to German and German to English). Their approach generates new sentence pairs containing rare words in new, synthetically created contexts. Recently, neural networks have proven to outperform many state-of-the-art systems in several NLP tasks (Young et al., 2018). The encoder-decoder model for recurrent neural networks (RNN) was developed in order to address the sequence-to-sequence nature of machine translation and obtains good results for this task (Sutskever et al., 2014"
D19-5536,P11-1038,0,0.0327866,"uages using different techniques ranging from hand-crafted rules (Chua et al., 2018) to deep learning approaches (Ikeda et al., 2016; Sproat and Jaitly, 2016; Lusetti et al., 2018). Three different metaphors were introduced by Kobus et al. (2008) to refer to these normalization approaches. That is the automatic speech recognition (ASR), spell checking, and translation metaphors. The ASR approach exploits the similarity between social media text and spoken language. Several works have followed this methodology, mostly combining it with the others (Beaufort and Roekhaut, 2010; Xue et al., 2011; Han and Baldwin, 2011). In the spell checking approach, corrections from noisy to standard words occurs at the word level. Some approaches have treated the problem by using dictionaries containing standard and non-standard words (Clark and Araki, 2011). However, the success of this kind of systems highly depends on the coverage of the dictionary. Since social media language is highly productive and new terms constantly appear, it is very challenging and expensive to continuously keep such a dictionary up to date. In this work, we consider the normalization task as a Machine Translation problem and treat noisy UGC t"
D19-5536,P10-1079,0,0.0333093,"ormalization has been performed on diverse languages using different techniques ranging from hand-crafted rules (Chua et al., 2018) to deep learning approaches (Ikeda et al., 2016; Sproat and Jaitly, 2016; Lusetti et al., 2018). Three different metaphors were introduced by Kobus et al. (2008) to refer to these normalization approaches. That is the automatic speech recognition (ASR), spell checking, and translation metaphors. The ASR approach exploits the similarity between social media text and spoken language. Several works have followed this methodology, mostly combining it with the others (Beaufort and Roekhaut, 2010; Xue et al., 2011; Han and Baldwin, 2011). In the spell checking approach, corrections from noisy to standard words occurs at the word level. Some approaches have treated the problem by using dictionaries containing standard and non-standard words (Clark and Araki, 2011). However, the success of this kind of systems highly depends on the coverage of the dictionary. Since social media language is highly productive and new terms constantly appear, it is very challenging and expensive to continuously keep such a dictionary up to date. In this work, we consider the normalization task as a Machine"
D19-5536,W16-3918,0,0.335438,"outperform many state-of-the-art systems in several NLP tasks (Young et al., 2018). Especially the encoderdecoder model with an attention mechanism (Bahdanau et al., 2014) for recurrent neural networks (RNN) has lead to a new paradigm in machine translation, i.e., Neural MT (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Sennrich et al., 2016a). 2 Related Work Previous research on UGC text normalization has been performed on diverse languages using different techniques ranging from hand-crafted rules (Chua et al., 2018) to deep learning approaches (Ikeda et al., 2016; Sproat and Jaitly, 2016; Lusetti et al., 2018). Three different metaphors were introduced by Kobus et al. (2008) to refer to these normalization approaches. That is the automatic speech recognition (ASR), spell checking, and translation metaphors. The ASR approach exploits the similarity between social media text and spoken language. Several works have followed this methodology, mostly combining it with the others (Beaufort and Roekhaut, 2010; Xue et al., 2011; Han and Baldwin, 2011). In the spell checking approach, corrections from noisy to standard words occurs at the word level. Some appr"
D19-5536,D16-1161,0,0.0188826,"nt in the text (Van Hee et al., 2017). However, for applications aiming at information extraction from text, normalization is needed to help to improve the performance of downstream NLP tasks (Schulz et al., 2016). Kobus et al. (2008) introduced three metaphors to refer to these normalization approaches: the spell checking, automatic speech recognition and machine translation metaphors. In this paper, the focus will be on the machine translation metaphor. One of the most conventional approaches is to use Statistical Machine Translation (SMT) techniques (Kaufmann, 2010; De Clercq et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016), in particular using One of the most persistent characteristics of written user-generated content (UGC) is the use of non-standard words. This characteristic contributes to an increased difficulty to automatically process and analyze UGC. Text normalization is the task of transforming lexical variants to their canonical forms and is often used as a pre-processing step for conventional NLP tasks in order to overcome the performance drop that NLP systems experience when applied to UGC. In this work, we follow a Neural Machine Translation approach to text normalization. To train such an encoderd"
D19-5536,I17-2044,0,0.213687,"lization of English tweets. First, they pre-processed the tweets to remove as much noise as possible, and then they used Moses1 to convert them into standard English. Moses is a statistical machine translation package which can produce high quality translations from one lanMany works have adopted and applied these techniques to the normalization task (Ikeda et al., 2016; Mandal and Nanmaran, 2018; Lusetti et al., 2018) some of them outperforming the SMT approach. However, it is well-known that these neural systems require a huge amount of data in order to perform properly (Ikeda et al., 2016; Saito et al., 2017). When it comes to translation these data even have to be parallel and should thus consist of aligned source and target sentences. Unfortunately, when it comes to UGC text normalization there is a lack of parallel corpora in which UGC is considered the source language and its standardized form the target language. Furthermore, the problem even exacerbates when working with low-resourced languages. In this work, we follow an NMT approach to tackle text normalization of Dutch UGC and explore how to overcome this parallel data bottleneck for Dutch, a low-resource language. We start off with a pub"
D19-5536,P17-4012,0,0.0139431,"though their work focuses on the Text to Speech (TTS) use case of text normalization, they compared prior work of text normalization for TTS (Rao et al., 2015; William Chan, 2016) and also discuss the problems that arise when using neural networks for text normalization. They made clear that although RNNs were often capable to produce surprisingly good results and learn some complex mappings, they are prone to make errors like read277 We relied on OpenNMT2 to train our encoderdecoder model. OpenNMT is an open source (MIT) initiative for neural machine translation and neural sequence modeling (Klein et al., 2017). The main system is implemented in the Lua/Torch mathematical framework, and can easily be extended using Torch’s internal standard neural network components. We used the version of the system with the basic architecture which consists of an encoder using a simple LSTM recurrent neural network. The decoder applies attention over the source sequence and implements input feeding (Luong et al., 2015). character and morphological level. Instead, they proposed a two-level data augmentation model that converted standard sentences to dialect sentences by using extracted morphological conversion patt"
D19-5536,C08-1056,0,0.212055,"transforming the lexical variants to their canonical forms. In this way, standard NLP tools can be applied in a next step after normalization (Aw et al., 2006). Please note that for some NLP applications, e.g. sentiment analysis, it might be beneficial to keep some ’noise’ in the data. For example, the use of flooding or capital letters could be a good indicator of the emotion present in the text (Van Hee et al., 2017). However, for applications aiming at information extraction from text, normalization is needed to help to improve the performance of downstream NLP tasks (Schulz et al., 2016). Kobus et al. (2008) introduced three metaphors to refer to these normalization approaches: the spell checking, automatic speech recognition and machine translation metaphors. In this paper, the focus will be on the machine translation metaphor. One of the most conventional approaches is to use Statistical Machine Translation (SMT) techniques (Kaufmann, 2010; De Clercq et al., 2014; Junczys-Dowmunt and Grundkiewicz, 2016), in particular using One of the most persistent characteristics of written user-generated content (UGC) is the use of non-standard words. This characteristic contributes to an increased difficul"
D19-5536,P07-2045,0,0.0100804,"onsidered important language resources for several NLP tasks (Van Hee et al., 2017; Pinto et al., 2016; Zhu et al., 2014). However, one of their most persistent characteristics is the use non-standard words. Social media texts are considered a type of written usergenerated content (UGC) in which several lan275 Proceedings of the 2019 EMNLP Workshop W-NUT: The 5th Workshop on Noisy User-generated Text, pages 275–285 c Hong Kong, Nov 4, 2019. 2019 Association for Computational Linguistics and results, whereas section 5 concludes this work and offers prospects for future work. the Moses toolkit (Koehn et al., 2007). However, neural networks have proven to outperform many state-of-the-art systems in several NLP tasks (Young et al., 2018). Especially the encoderdecoder model with an attention mechanism (Bahdanau et al., 2014) for recurrent neural networks (RNN) has lead to a new paradigm in machine translation, i.e., Neural MT (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Sennrich et al., 2016a). 2 Related Work Previous research on UGC text normalization has been performed on diverse languages using different techniques ranging from hand-crafted rules (Chua et"
D19-5536,P16-1009,0,0.249915,"ng, Nov 4, 2019. 2019 Association for Computational Linguistics and results, whereas section 5 concludes this work and offers prospects for future work. the Moses toolkit (Koehn et al., 2007). However, neural networks have proven to outperform many state-of-the-art systems in several NLP tasks (Young et al., 2018). Especially the encoderdecoder model with an attention mechanism (Bahdanau et al., 2014) for recurrent neural networks (RNN) has lead to a new paradigm in machine translation, i.e., Neural MT (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Sennrich et al., 2016a). 2 Related Work Previous research on UGC text normalization has been performed on diverse languages using different techniques ranging from hand-crafted rules (Chua et al., 2018) to deep learning approaches (Ikeda et al., 2016; Sproat and Jaitly, 2016; Lusetti et al., 2018). Three different metaphors were introduced by Kobus et al. (2008) to refer to these normalization approaches. That is the automatic speech recognition (ASR), spell checking, and translation metaphors. The ASR approach exploits the similarity between social media text and spoken language. Several works have followed this"
D19-5536,P16-1162,0,0.263074,"ng, Nov 4, 2019. 2019 Association for Computational Linguistics and results, whereas section 5 concludes this work and offers prospects for future work. the Moses toolkit (Koehn et al., 2007). However, neural networks have proven to outperform many state-of-the-art systems in several NLP tasks (Young et al., 2018). Especially the encoderdecoder model with an attention mechanism (Bahdanau et al., 2014) for recurrent neural networks (RNN) has lead to a new paradigm in machine translation, i.e., Neural MT (NMT) (Sutskever et al., 2014; Cho et al., 2014; Bahdanau et al., 2014; Luong et al., 2015; Sennrich et al., 2016a). 2 Related Work Previous research on UGC text normalization has been performed on diverse languages using different techniques ranging from hand-crafted rules (Chua et al., 2018) to deep learning approaches (Ikeda et al., 2016; Sproat and Jaitly, 2016; Lusetti et al., 2018). Three different metaphors were introduced by Kobus et al. (2008) to refer to these normalization approaches. That is the automatic speech recognition (ASR), spell checking, and translation metaphors. The ASR approach exploits the similarity between social media text and spoken language. Several works have followed this"
D19-5536,D15-1166,0,0.129967,"Missing"
D19-5536,I11-1109,0,0.0230987,"pear, it is very challenging and expensive to continuously keep such a dictionary up to date. In this work, we consider the normalization task as a Machine Translation problem and treat noisy UGC text as the source language and its normalized form as the target language. In the past, several works have also used this approach and there are two leading paradigms: Statistical and Neural Machine Translation. Statistical Machine Translation (SMT) models, especially those trained at the character-level, have proven highly effective for the task because they capture well intra-word transformations (Pennell and Liu, 2011). Besides, they have the advantage of being effective when small training data is provided, thanks to their small vocabulary size. Kaufmann (2010), for example, followed a two step approach for the normalization of English tweets. First, they pre-processed the tweets to remove as much noise as possible, and then they used Moses1 to convert them into standard English. Moses is a statistical machine translation package which can produce high quality translations from one lanMany works have adopted and applied these techniques to the normalization task (Ikeda et al., 2016; Mandal and Nanmaran, 20"
D19-5536,D16-1163,0,0.0258981,"De Clercq et al. (2013) proposed a phrase-based method to normalize Dutch UGC comprising various genres. In a preprocessing step they handled emoticons, hyperlinks, hashtags and so forth. Then they worked in two steps: first at the word level and then at the character level. This approach revealed good results across various genres of UGC. However, a high number of phonetic alternations remained unresolved. Although the encoder-decoder model has shown its effectiveness in large datasets, it is much less effective when only a small number of sentence pairs is available (Sennrich et al., 2016b; Zoph et al., 2016). Automatic data augmentation is commonly used in vision and speech and can help train more robust models, particularly when using smaller datasets (Chatfield et al., 2014; Taylor and Nitschke, 2017). Fadaee et al. (2017) present Translation Data Augmentation (TDA), a method to improve the translation quality for low resource pairs (English to German and German to English). Their approach generates new sentence pairs containing rare words in new, synthetically created contexts. Recently, neural networks have proven to outperform many state-of-the-art systems in several NLP tasks (Young et al.,"
D19-5903,P17-1009,0,0.0429762,"Missing"
D19-5903,D15-1247,0,0.0565449,"Missing"
D19-5903,L16-1699,0,0.0549852,"Missing"
D19-5903,C18-1075,0,0.0272688,"Missing"
D19-5903,W15-0809,0,0.101089,"hey are open to interpretation and may be worded in idiosyncratic ways 15 Proceedings of the First Workshop on Aggregating and Analysing Crowdsourced Annotations for NLP, pages 15–23 c Hong Kong, China, November 3, 2019. 2019 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 2 Nuggets, an event description that allows tagging multiple tokens as the event trigger. Multi-token triggers allow annotators to more easily navigate cases in which multiple words could be chosen as a trigger, e.g. “hold a meeting”, “serve a sentence”, and they can be continuous or discontinuous (Mitamura et al., 2015). Related work Event recognition was introducted as a task in the ACE program in 2004. Event mention recall was not evaluated directly; rather, scoring happened on the level of events rather than event mentions (such that one event is a bucket of multiple mentions referring to the same event). A mapping between gold mentions and system output mentions was a prerequisite for scoring (Doddington et al., 2004). The same is true for the Event Mention Detection (VMD) task in ACE 2005 (National Institute of Standards and Technology, 2005). The ACE 2005 corpus thereafter became widely used in event d"
D19-5903,cybulska-vossen-2014-using,0,0.0612401,"Missing"
D19-5903,doddington-etal-2004-automatic,0,0.0320283,"rs allow annotators to more easily navigate cases in which multiple words could be chosen as a trigger, e.g. “hold a meeting”, “serve a sentence”, and they can be continuous or discontinuous (Mitamura et al., 2015). Related work Event recognition was introducted as a task in the ACE program in 2004. Event mention recall was not evaluated directly; rather, scoring happened on the level of events rather than event mentions (such that one event is a bucket of multiple mentions referring to the same event). A mapping between gold mentions and system output mentions was a prerequisite for scoring (Doddington et al., 2004). The same is true for the Event Mention Detection (VMD) task in ACE 2005 (National Institute of Standards and Technology, 2005). The ACE 2005 corpus thereafter became widely used in event detection studies. Because triggers consist of singleword tokens, testing recall is straightforward. Li et al. (2013) and Li et al. (2014), for instance, treat matches as correct if their offsets (span) and event subtype match exactly. F1 is used to score performance overall. Event nugget detection, and with it event triggers that consist of more than one word, were introduced as a task in the TAC KBP track"
D19-5903,2006.jeptalnrecital-invite.2,0,0.0743614,"Missing"
D19-5903,grishman-2010-impact,0,0.024852,"Missing"
D19-5903,W16-5706,0,0.0337693,"Missing"
D19-5903,K15-1002,0,0.0630794,"Missing"
D19-5903,D14-1198,0,0.0234408,"ated directly; rather, scoring happened on the level of events rather than event mentions (such that one event is a bucket of multiple mentions referring to the same event). A mapping between gold mentions and system output mentions was a prerequisite for scoring (Doddington et al., 2004). The same is true for the Event Mention Detection (VMD) task in ACE 2005 (National Institute of Standards and Technology, 2005). The ACE 2005 corpus thereafter became widely used in event detection studies. Because triggers consist of singleword tokens, testing recall is straightforward. Li et al. (2013) and Li et al. (2014), for instance, treat matches as correct if their offsets (span) and event subtype match exactly. F1 is used to score performance overall. Event nugget detection, and with it event triggers that consist of more than one word, were introduced as a task in the TAC KBP track in 2014. Liu et al. (2015) proposes a method to evaluate nugget recall which enables fuzzy matching for annotations with non-perfectly-overlapping spans. In this work the Dice coefficient is used to measure the set similarity between the tokens covered by each annotation, which turns out to be the same as F1 score. System men"
D19-5903,P13-1008,0,0.030863,"recall was not evaluated directly; rather, scoring happened on the level of events rather than event mentions (such that one event is a bucket of multiple mentions referring to the same event). A mapping between gold mentions and system output mentions was a prerequisite for scoring (Doddington et al., 2004). The same is true for the Event Mention Detection (VMD) task in ACE 2005 (National Institute of Standards and Technology, 2005). The ACE 2005 corpus thereafter became widely used in event detection studies. Because triggers consist of singleword tokens, testing recall is straightforward. Li et al. (2013) and Li et al. (2014), for instance, treat matches as correct if their offsets (span) and event subtype match exactly. F1 is used to score performance overall. Event nugget detection, and with it event triggers that consist of more than one word, were introduced as a task in the TAC KBP track in 2014. Liu et al. (2015) proposes a method to evaluate nugget recall which enables fuzzy matching for annotations with non-perfectly-overlapping spans. In this work the Dice coefficient is used to measure the set similarity between the tokens covered by each annotation, which turns out to be the same as"
D19-5903,D16-1038,0,0.035376,"Missing"
D19-5903,W15-0807,0,0.380511,"e is true for the Event Mention Detection (VMD) task in ACE 2005 (National Institute of Standards and Technology, 2005). The ACE 2005 corpus thereafter became widely used in event detection studies. Because triggers consist of singleword tokens, testing recall is straightforward. Li et al. (2013) and Li et al. (2014), for instance, treat matches as correct if their offsets (span) and event subtype match exactly. F1 is used to score performance overall. Event nugget detection, and with it event triggers that consist of more than one word, were introduced as a task in the TAC KBP track in 2014. Liu et al. (2015) proposes a method to evaluate nugget recall which enables fuzzy matching for annotations with non-perfectly-overlapping spans. In this work the Dice coefficient is used to measure the set similarity between the tokens covered by each annotation, which turns out to be the same as F1 score. System mentions are mapped to gold standard mentions by selecting the gold-system pair with the highest Dice coefficient score. An overall matching score is produced by considering other features of the event annotation. Mitamura et al. (2015) uses this method to assess the consistency of annotation in the 2"
D19-5903,P14-5016,0,0.0583996,"Missing"
daelemans-hoste-2002-evaluation,W96-0208,0,\N,Missing
daelemans-hoste-2002-evaluation,W00-0720,1,\N,Missing
daelemans-hoste-2002-evaluation,P01-1005,0,\N,Missing
de-clercq-etal-2012-evaluating,reynaert-etal-2010-balancing,1,\N,Missing
de-clercq-etal-2012-evaluating,W07-1513,1,\N,Missing
de-clercq-etal-2012-evaluating,J08-2005,0,\N,Missing
de-clercq-etal-2012-evaluating,N07-1070,0,\N,Missing
de-clercq-etal-2012-evaluating,P10-1025,0,\N,Missing
de-clercq-etal-2012-evaluating,W10-1814,0,\N,Missing
de-clercq-etal-2012-evaluating,J02-3001,0,\N,Missing
de-clercq-etal-2012-evaluating,C08-1050,0,\N,Missing
de-clercq-etal-2012-evaluating,schuurman-etal-2010-interacting,1,\N,Missing
de-clercq-etal-2014-towards,W10-0513,0,\N,Missing
de-clercq-etal-2014-towards,C08-1056,0,\N,Missing
de-clercq-etal-2014-towards,P11-2013,0,\N,Missing
de-clercq-etal-2014-towards,P02-1040,0,\N,Missing
de-clercq-etal-2014-towards,P11-1037,0,\N,Missing
de-clercq-etal-2014-towards,P07-2045,0,\N,Missing
de-clercq-etal-2014-towards,D11-1141,0,\N,Missing
de-clercq-etal-2014-towards,P10-1079,0,\N,Missing
de-clercq-etal-2014-towards,kestemont-etal-2012-netlog,0,\N,Missing
de-clercq-etal-2014-towards,R13-1024,1,\N,Missing
de-clercq-etal-2014-towards,P07-1033,0,\N,Missing
de-clercq-etal-2014-towards,W09-2010,0,\N,Missing
de-clercq-etal-2014-towards,P13-2121,0,\N,Missing
desmet-hoste-2010-towards,D07-1073,0,\N,Missing
desmet-hoste-2010-towards,markert-nissim-2002-towards,0,\N,Missing
desmet-hoste-2010-towards,W02-2004,0,\N,Missing
desmet-hoste-2010-towards,S07-1007,0,\N,Missing
desmet-hoste-2010-towards,E09-1070,0,\N,Missing
desmet-hoste-2010-towards,W02-2025,0,\N,Missing
desmet-hoste-2010-towards,P02-1060,0,\N,Missing
desmet-hoste-2010-towards,oostdijk-etal-2008-coi,0,\N,Missing
desmet-hoste-2010-towards,W03-0419,0,\N,Missing
desmet-hoste-2010-towards,J96-2004,0,\N,Missing
desmet-hoste-2010-towards,P03-1004,0,\N,Missing
desmet-hoste-2010-towards,W02-2024,0,\N,Missing
desmet-hoste-2010-towards,D07-1074,0,\N,Missing
desmet-hoste-2014-recognising,E12-2021,0,\N,Missing
E09-1057,moore-2002-fast,0,0.0299683,"second, tools to generate additional linguistic information (PoS tagger, lemmatizer and a chunker). The sub-sentential alignment system takes as input sentence-aligned texts, together with the additional linguistic annotations for the source and the target texts. The source and target sentences are divided into chunks based on PoS information, and lexical correspondences are retrieved from a bilingual dictionary. In order to extract bilingual dictionaries from the three parallel corpora, we used the Perl implementation of IBM Model One that is part of the Microsoft Bilingual Sentence Aligner (Moore, 2002). In order to link chunks based on lexical clues and chunk similarity, the following steps are taken for each sentence pair: # words 6,408,693 7,305,151 7,100,585 Preprocessing We PoS-tagged and lemmatized the French, English and Italian corpora with the freely available TreeTagger tool (Schmid, 1994) and we used TadPole (Van den Bosch et al., 2007) to annotate the Dutch corpus. In a next step, chunk information was added by a rule-based language-independent chunker (Macken et al., 2008) that contains distituency rules, which implies that chunk boundaries are added between two PoS codes that c"
E09-1057,J03-1002,0,0.0118533,"f corresponding PoS codes and ’R’ for words linked by language-dependent rules. As the contextual clues (the left and right neigbours of the additional candidate chunks are anchor chunks) provide some extra indication that the chunks can be linked, the similarity test for the final candidates was somewhat relaxed: the percentage of words that have to be linked was lowered to 0.80 and a more relaxed PoS matching function was used. 3.4 Evaluation To test our alignment module, we manually indicated all translational correspondences in the three test corpora. We used the evaluation methodology of Och and Ney (2003) to evaluate the system’s performance. They distinguished sure alignments (S) and possible alignments (P) and introduced the following redefined precision and recall measures (where A refers to the set of alignments): Linking Remaining Chunks In a second step, chunks consisting of one function word – mostly punctuation marks and conjunctions – are linked based on corresponding part-ofspeech codes if their left or right neighbour on the diagonal is an anchor chunk. Corresponding final punctuation marks are also linked. In a final step, additional candidates are constructed by selecting non-anch"
E09-1057,C94-2167,0,0.309216,"t module is particularly well suited for the extraction of complex multiword terms. 1 This article reports on the term extraction experiments for 3 language pairs, i.e. French-Dutch, French-English and French-Italian. The focus was on the extraction of automative lexicons. Introduction Automatic Term Recognition (ATR) systems are usually categorized into two main families. On the one hand, the linguistically-based or rule-based approaches use linguistic information such as PoS tags, chunk information, etc. to filter out stop words and restrict candidate terms to predefined syntactic patterns (Ananiadou, 1994), (Dagan and Church, 1994). On the other hand, the statistical corpus-based approaches select n-gram sequences as candidate terms that are filtered by means of The remainder of this paper is organized as follows: Section 2 describes the corpus. In Section 3 we present our linguistically-based sub-sentential alignment system and in Section 4 we describe how we generate and filter our list of candidate terms. We compare the performance of our system with both bilingual and monolingual state-ofthe-art terminology extraction systems. Section 5 concludes this paper. Proceedings of the 12th Conferen"
E09-1057,W00-0901,0,0.0127159,"e climatisation tableau de commande de climatisation Log-Likelihood Measure The Log-Likehood measure (LL) should allow us to detect single word terms that are distinctive enough to be kept in our bilingual lexicon (Daille, 1995). This metric considers word frequencies weighted over two different corpora (in our case a technical automotive corpus and the more general purpose corpus “Le Monde”1 ), in order to assign high LL-values to words having much higher or lower frequencies than expected. We implemented the formula for both the expected values and the Log-Likelihood values as described by (Rayson and Garside, 2000). Manual inspection of the Log-Likelihood figures confirmed our hypothesis that more domainspecific terms in our corpus were assigned high LL-values. We experimentally defined the threshold for Log-Likelihood values corresponding to distinctive terms on our development corpus. Example (4) shows some translation pairs which are filtered out by applying the LL threshold. air conditioning control (4) air conditioning control panel Fr: cependant – En: however – It: tuttavia – Du: echter Fr: choix – En: choice – It: scelta – Du: keuze 4.1 Fr: continuer – En: continue – It: continuare – Du: Filterin"
E09-1057,A94-1006,0,0.0874688,"larly well suited for the extraction of complex multiword terms. 1 This article reports on the term extraction experiments for 3 language pairs, i.e. French-Dutch, French-English and French-Italian. The focus was on the extraction of automative lexicons. Introduction Automatic Term Recognition (ATR) systems are usually categorized into two main families. On the one hand, the linguistically-based or rule-based approaches use linguistic information such as PoS tags, chunk information, etc. to filter out stop words and restrict candidate terms to predefined syntactic patterns (Ananiadou, 1994), (Dagan and Church, 1994). On the other hand, the statistical corpus-based approaches select n-gram sequences as candidate terms that are filtered by means of The remainder of this paper is organized as follows: Section 2 describes the corpus. In Section 3 we present our linguistically-based sub-sentential alignment system and in Section 4 we describe how we generate and filter our list of candidate terms. We compare the performance of our system with both bilingual and monolingual state-ofthe-art terminology extraction systems. Section 5 concludes this paper. Proceedings of the 12th Conference of the European Chapter"
E09-1057,zhang-etal-2008-comparative,0,0.0537254,"Missing"
E09-1057,P93-1003,0,0.333339,"Missing"
E09-1057,C08-1067,1,0.873831,"Missing"
hendrickx-etal-2008-coreference,J98-2001,0,\N,Missing
hendrickx-etal-2008-coreference,W96-0102,1,\N,Missing
hendrickx-etal-2008-coreference,M95-1005,0,\N,Missing
hendrickx-etal-2008-coreference,N01-1008,0,\N,Missing
hendrickx-etal-2008-coreference,A88-1003,0,\N,Missing
hendrickx-etal-2008-coreference,W02-1008,0,\N,Missing
hendrickx-etal-2008-coreference,W05-0303,0,\N,Missing
hendrickx-etal-2008-coreference,C02-1139,0,\N,Missing
hendrickx-etal-2008-coreference,N06-1025,0,\N,Missing
hendrickx-etal-2008-coreference,J01-4004,0,\N,Missing
hendrickx-etal-2008-coreference,P98-2143,0,\N,Missing
hendrickx-etal-2008-coreference,C98-2138,0,\N,Missing
hendrickx-etal-2008-coreference,hoste-de-pauw-2006-knack,1,\N,Missing
hendrickx-etal-2008-coreference,M95-1006,0,\N,Missing
hendrickx-etal-2008-coreference,M95-1011,0,\N,Missing
hendrickx-etal-2008-coreference,P03-1023,0,\N,Missing
hoste-de-pauw-2006-knack,W96-0102,0,\N,Missing
hoste-de-pauw-2006-knack,van-eynde-etal-2000-part,0,\N,Missing
hoste-de-pauw-2006-knack,W04-0108,1,\N,Missing
hoste-de-pauw-2006-knack,J00-4005,0,\N,Missing
hoste-de-pauw-2006-knack,P03-1062,1,\N,Missing
hoste-de-pauw-2006-knack,A00-1031,0,\N,Missing
hoste-de-pauw-2006-knack,kibble-van-deemter-2000-coreference,0,\N,Missing
hoste-de-pauw-2006-knack,W03-2603,0,\N,Missing
hoste-de-pauw-2006-knack,oostdijk-2000-spoken,0,\N,Missing
hoste-etal-2008-learning,J90-1003,0,\N,Missing
hoste-etal-2008-learning,J93-2004,0,\N,Missing
hoste-etal-2008-learning,C94-2167,0,\N,Missing
hoste-etal-2008-learning,W00-0901,0,\N,Missing
hoste-etal-2008-learning,A94-1006,0,\N,Missing
hoste-etal-2008-learning,H90-1021,0,\N,Missing
hoste-etal-2008-learning,C00-1030,0,\N,Missing
hoste-etal-2008-learning,W02-0301,0,\N,Missing
J16-3004,P05-1018,0,0.0582202,"Missing"
J16-3004,J08-1001,0,0.0203663,"as important readability predictors. POS-based features, which are less difficult to compute, have also been used and have proven to be effective, too (Heilman et al. 2007), especially features based on noun and preposition word class information (Feng et al. 2010) or features representing the amount of function words present in a text (Leroy et al. 2008). Overall, Schwarm and Ostendorf’s parse tree features have been reproduced frequently and were found effective when combined with n-gram modeling (Heilman et al. 2007; Petersen and Ostendorf 2009; Nenkova et al. 2010) and discourse features (Barzilay and Lapata 2008). This brings us to a final set of features, namely, those relating to semantics, which has been a popular focus in modern readability research (Pitler and Nenkova 2008; Feng et al. 2010; François 2011). Whereas the added value of the lexical and syntactic features has been corroborated repeatedly in the computational approaches to readability prediction that have surfaced in the last decade, it has proven much more difficult to unequivocally determine the added value of semantic features. Capturing semantics can be done from two different angles. The first angle relates to features that are u"
J16-3004,W09-1206,0,0.0468308,"Missing"
J16-3004,N04-1025,0,0.0121119,", the underlying assumption of a regression between readability and the modeled text characteristics (Heilman, Collins-Thompson, and Eskenazi 2008), and so forth. Furthermore, there seems to be a remarkably strong correspondence between the readability formulas themselves, even across different languages (van Oosten, Tanghe, and Hoste 2010). These objections have led to new quantitative approaches of doing readability prediction that adopt a machine learning perspective to the task. Advancements in these fields have introduced more intricate prediction methods such as naive Bayes classifiers (Collins-Thompson and Callan 2004), logistic regression (François 2009) and 1 See the results of the CoNLL-2011 Shared Task at http://conll.cemantix.org/2011/. 459 Computational Linguistics Volume 42, Number 3 support vector machines (Schwarm and Ostendorf 2005; Feng et al. 2010; Tanaka-Ishii, Tezuka, and Terada 2010), and especially more complex features ranging from lexical features over syntactic to semantic and discourse features. The vocabulary used in a text largely determines its readability (Alderson 1984; Pitler and Nenkova 2008). Until the millennium, lexical features were mainly studied by counting words, measuring"
J16-3004,R13-1024,1,0.896831,"Missing"
J16-3004,R11-1026,1,0.888264,"Missing"
J16-3004,de-clercq-etal-2012-evaluating,1,0.897315,"Missing"
J16-3004,de-marneffe-etal-2006-generating,0,0.0527767,"Missing"
J16-3004,W14-1213,0,0.0162771,"Techniques such as forward selection, backward elimination (John, Kohavi, and Pfleger 1994), and bidirectional hillclimbing (Caruana and Freitag 1994) differ in the point where they start their search, but all share the potential problem of convergence to a local optimum. In the case of genetic algorithms (GAs) search does not start from a local search point, but from a population of individuals, thus exploring different areas of the search space in parallel (and it also allows multiple optima). Genetic algorithms for feature selection in readability prediction have, for example, been used by Falkenjack and Jonsson (2014) to determine the added value of syntax features for Swedish readability prediction. Because, besides feature selection, changing the hyperparameters of an algorithm can also have a dramatic effect on classifier performance (Hoste 2005; Desmet 2014) and should be determined experimentally, we chose to use GAs as a computationally feasible way to tackle this optimization problem, which involves searching the space of all possible feature subsets and parameter settings to identify the combination that is optimal or near-optimal. Genetic algorithms (see Goldberg [1989] and Mitchell [1996] for mor"
J16-3004,E09-1027,0,0.0880167,"Missing"
J16-3004,C10-2032,0,0.155911,"Missing"
J16-3004,P05-1045,0,0.0410899,"Missing"
J16-3004,W12-2207,0,0.0145149,"adability Prediction Table 3 Overview of all features that were implemented for the readability prediction tasks divided into various subgroups. Traditional tradlen tradlex 4 2 Lexical lexlm lexterm 2 2 Syntactic shallowsynt deepsynt Semantic shallowsem ner coref srl 12 7 5 20 27 6 distinct features, which were all computed on the document level using state-of-the-art text processing tools. A schematic overview can be found in Table 3. – Traditional features: We included four length-related features (tradlen) that have proven successful in previous work (Feng et al. 2010; Nenkova et al. 2010; François and Miltsakaki 2012): the average word and sentence length, the ratio of long words in a text (i.e., words containing more than three syllables), and the percentage of polysyllable words. We also incorporated two traditional lexical features (tradlex): the percentage of words that can be found in the Chall and Dale list (1995) for the English texts or in the CLIB list (Staphorsius 1994) for the Dutch texts.4 We also calculated the type token ratio to measure the level of lexical complexity within a text. All these features were obtained after processing the text with a state-of-the-art English (LeTs; Van de Kaute"
J16-3004,W08-0909,0,0.015063,"Missing"
J16-3004,N07-1058,0,0.0305787,"ing from lexical features over syntactic to semantic and discourse features. The vocabulary used in a text largely determines its readability (Alderson 1984; Pitler and Nenkova 2008). Until the millennium, lexical features were mainly studied by counting words, measuring lexical diversity using the type token ratio, or by calculating frequency statistics based on lists (Flesch 1948; Kincaid et al. 1975; Chall and Dale 1995). In later work, a generalization over this list look-up was made by training unigram language models on grade levels (Si and Callan 2001; Collins-Thompson and Callan 2005; Heilman et al. 2007). Subsequent work by Schwarm and Ostendorf (2005) compared higher-ordered n-gram models trained on part-of-speech sequences with those using information gain and found that the latter gave the best results. To this purpose they used two paired corpora (one complex and one simplified version) to train their language models. Using the same corpora, these findings were corroborated by Feng et al. (2010) when they investigated readability targeted to people with intellectual disabilities. These results were thus achieved when training and testing different language models that are built on various"
J16-3004,C10-1062,0,0.0997439,"or people with intellectual disabilities (Feng et al. 2010), and on the readability of texts in specific domains, such as the medical one (Leroy and Endicott 2011). The investigation of the readability of a wide variety of texts without targeting a specific audience has not received much attention (Benjamin 2012). Moreover, when it comes to current state-of-the art systems, it can be observed that even though more complex features trained on various levels of complexity have proven quite successful when implemented in a readability prediction system (Pitler and Nenkova 2008; Feng et al. 2010; Kate et al. 2010), there is still no consensus on which features are actually the best predictors of readability. As a consequence, when institutions, companies, or other research disciplines wish to use readability prediction techniques, they still rely on the more outdated superficial characteristics and formulas (see, for example, the recent work by van Boom [2014] on the readability of mortgage terms). In this article, we investigate the creation of a fully automatic readability assessment system that can assess generic text material in two languages, English and Dutch. We use a supervised machine learning"
J16-3004,J13-4004,0,0.0369923,") and shallow entities (based on PoS-tags [ner]). For English, we used the Stanford NER (Finkel, Grenager, and Manning 2005) and for Dutch the NERD system (Desmet and Hoste 2013). Coreferential relations, then, might indicate how structured and thus how coherent a particular text is. We represented as features the number of coreferential chains present in a text, the average length of a chain, the average number of coreferring expressions and unique mentions, and we also count how many chains span more than half of the text (coref ). To this purpose, we used the Stanford Coreference Resolver (Lee et al. 2013) for English and COREA (De Clercq, Hendrickx, and Hoste 2011) for Dutch. In order to determine how many agents or modifiers a particular text contains, we also calculated the average number of arguments and modifiers and the average occurrence of every possible PropBank label (Palmer, Gildea, and Kingsbury 2005) (srl). For the construction of these features, we used the English semantic role labeler (SRL) as part of the Mate-Tools (Björkelund, Hafdell, and Nugues 2009) and for Dutch the SoNaR SRL (De Clercq, Monachesi, and Hoste 2012). Both the entity and coreference features were tested befor"
J16-3004,J05-1004,0,0.0361791,"Missing"
J16-3004,D08-1020,0,0.0568323,"Missing"
J16-3004,prasad-etal-2008-penn,0,0.0395568,"that salient entities prefer prominent over non-prominent syntactic positions within a clause and are more likely to be introduced in a main clause than in a subordinate clause. Though originally devised for other research purposes, they found that the proportion of transitions in this entity grid model results in predicting the readability of a text in combination with the syntactic features as introduced by Schwarm and Ostendorf (2005). Subsequent work by Pitler and Nenkova (2008) compared this entity grid model with the added value of discourse relations as annotated in the Penn Treebank (Prasad et al. 2008). They treat each text as a bag of relations rather than a bag of words and compute the log likelihood of a text based on its discourse relations and text length compared to the overall treebank. They found that these discourse relations are indeed good in distinguishing texts, especially when combined with the 461 Computational Linguistics Volume 42, Number 3 entity grid model. Because these discourse relations were only based on gold standard information whereas, in the end, a readability prediction system should be able to function automatically, Feng et al. (2010) proposed an alternative t"
J16-3004,W00-0901,0,0.0280065,"Missing"
J16-3004,P05-1065,0,0.0816594,"research and the automatic prediction of readability has a very long and rich tradition (see surveys by Klare 1976; DuBay 2004; Benjamin 2012; and CollinsThompson 2014). Whereas superficial text characteristics leading to on-the-spot readability formulas were popular until the last decade of the previous century (Flesch 1948; Gunning 1952; Kincaid et al. 1975), recent advances in the field of computer science and natural language processing have triggered the inclusion of more intricate characteristics in present-day readability research (Si and Callan 2001; Collins-Thompson and Callan 2005; Schwarm and Ostendorf 2005; Heilman, Collins-Thompson, and Eskenazi 2008; Feng et al. 2010). The bulk of these studies, however, have focused on readability as perceived by specific groups of people, such as children (Schwarm and Ostendorf 2005), second language learners (François 2009), or people with intellectual disabilities (Feng et al. 2010), and on the readability of texts in specific domains, such as the medical one (Leroy and Endicott 2011). The investigation of the readability of a wide variety of texts without targeting a specific audience has not received much attention (Benjamin 2012). Moreover, when it com"
J16-3004,J10-2002,0,0.0799976,"Missing"
J16-3004,W10-1001,0,0.0183782,"Missing"
J16-3004,van-oosten-etal-2010-towards,1,0.838585,"Missing"
J16-3004,E09-3003,0,\N,Missing
J18-4010,L16-1185,0,0.0641843,"Missing"
J18-4010,L16-1256,0,0.0239491,"glish tweets by searching Twitter with the hashtags #irony, #sarcasm, and #not. For this purpose, we made use of Tweepy,4 a Python library to access the official Twitter API. The tweets were collected between 1 December 2014 and 4 January 2015, represent 2,676 unique Twitter users, and have an average length of 15 tokens. An example tweet is presented in Figure 1. To minimize data noise (see earlier), all tweets were manually labeled using a newly developed fine-grained annotation scheme. Although a number of annotation schemes for irony have been developed recently (e.g., Riloff et al. 2013; Bosco et al. 2016; Stranisci et al. 2016), most of them describe a binary distinction (i.e., ironic vs. not-ironic) or flag irony as part of sentiment annotations. By contrast, Karoui et al. (2017) defined explicit and implicit irony activations based on incongruity in (French, English, and Italian) ironic tweets and they defined eight fine-grained categories of pragmatic devices that realize such an incongruity, including analogy, hyperbole, rhetorical question, oxymoron, and so forth. The typology provides valuable insights into the linguistic realization of irony that could improve 4 https://github.com/twee"
J18-4010,W14-2608,0,0.0302769,"Missing"
J18-4010,C16-1251,0,0.371839,"ledge (i.e., the feeling a concept generally invokes for a person or a group of people) is also referred to as prototypical sentiment (Hoste et al. 2016). Like in Example (2), prototypical sentiment expressions are devoid of subjective words and rely on common sense shared by the speaker and receiver in an interaction. To be able to grasp such implied sentiment, sentiment analysis systems require additional knowledge that provides insight into the world we live in and affective information associated with natural language concepts. Although modeling implicit sentiment is still in its infancy (Cambria et al. 2016), such linking of concepts or situations to implicit sentiment will open new perspectives in natural language processing (NLP) applications, not only for sentiment analysis tasks, but also for any type of tasks that involves semantic text processing, such as automatic irony detection and the detection of cyberharassment (Dinakar et al. 2012; Van Hee et al. 2015). Although automatic systems perform well in deriving particular information from a given text (i.e., the meaning of a word in context, emotions expressed by the author of a text), they often struggle to perform tasks where extratextual"
J18-4010,J96-2004,0,0.16426,"ditional clarifications were recommended prior to annotation of the entire corpus. After adding some refinements to the scheme (Van Hee (2017), a second agreement study was carried out by three Master’s students, who each annotated the same subset (i.e., 100 randomly selected instances) of the corpus. In both rounds, inter-annotator agreement was calculated at different steps in the annotation process. As the metric, we used Fleiss’ Kappa (Fleiss 1971), a widespread statistical measure in the field of computational linguistics for assessing agreement between annotators on categorical ratings (Carletta 1996). The results of the interannotator agreement study are presented in Table 1. With the exception of harshness, which proves to be difficult to judge on, Kappa scores show a moderate to substantial agreement between annotators for the different annotation steps.5 Overall, we see that similar or better agreement was obtained after the refinement of the annotation scheme, which had the largest effect on the irony annotation. An exception, however, is the annotation of a polarity contrast between targets and evaluations, where the agreement drops from 0.66 to 0.55 between the first and second roun"
J18-4010,P15-1038,0,0.0361092,"Missing"
J18-4010,W10-2914,0,0.120007,"Missing"
J18-4010,E14-1040,0,0.0211231,"sentence, causing “a soldier veered his jeep into a crowded market and killed three civilians” to be perceived as more negative toward the soldier than “a soldier’s jeep veered into a crowded market, causing three civilian deaths.” Wilson annotated objective polar utterances or statements that describe positive or negative factual information about something (e.g., “The camera broke the first time I used it”) in meeting report content. A similar study was conducted by Toprak, Jakob, and Gurevych (2010), who annotated objective polar utterances in consumer reviews, but named them polar facts. Deng and Wiebe (2014) detected implicit sentiment by inferencing over explicit sentiment expressions, namely, through implicature rules (i.e., “goodFor” and “badFor”), describing events that have either a positive or negative effect on objects or entities. Ebrahimi 795 Computational Linguistics Volume 44, Number 4 (2013) tackled this problem in the medical domain and exploited disease symptoms as negative implicit sentiment features to predict side effects in drug reviews. Implicit or prototypical sentiment is part of common sense, meaning that the information is not explicitly mentioned, but implicitly shared by"
J18-4010,S16-1173,0,0.061526,"Missing"
J18-4010,P13-1174,0,0.0696005,"was expanded by extracting connotative information from Twitter (i.e., bootstrapping by using seed words like “failure” and “disease”), and using ConceptNet (Speer and Havasi 2013). The development of EmotiNet is grounded in Appraisal Theory (Scherer 1999) and aims to store emotional reactions to real-world contexts (e.g., “I’m going to a family party because my mother obliges me to” → disgust). The knowledge base was used to learn intrinsic properties of entities and situations in journalistic text that trigger certain emotions like “refugees” and “being sentenced” (Balahur and Tanev 2016). Feng et al. (2013) created a connotation lexicon that determines the underlying sentiment of seemingly objective words (e.g., cooperation(+) , overcharge(-) ) and the general connotation of named entities (e.g., Einstein(+) , Osama(-) ). Zhang and Liu (2011) hypothesized that resource phrases (e.g., “this washer uses a lot of electricity”) are important carriers of implicit sentiment. They automatically extracted resource terms like “water” and “money” with resource 1 Game With a Purpose: A computer game which integrates human intervention in a computational process in an entertaining way. 2 International Surve"
J18-4010,W16-0425,0,0.0410396,"ed sequential minimal optimization and logistic regression. Reyes, Rosso, and Veale (2013) defined features based on conceptual descriptions in irony literature, being signatures, unexpectedness, style, and emotional scenarios and experimented with na¨ıve Bayes and decision trees. Kunneman et al. (2015) pioneered irony detection in Dutch tweets using word n-gram features and a Balanced Winnow classifier. Van Hee, Lefever, and Hoste (2016b) combined lexical with sentiment, syntactic, and semantic Word2Vec cluster features for irony detection using a support vector machine (SVM). Recent work by Ghosh and Veale (2016), Poria et al. (2016), and Zhang, Zhang, and Fu (2016) has approached irony detection using (deep) neural networks, which make use of continuous automatic features instead of manually defined ones. Besides text-based features, irony research has explored extratextual features related to the author or context of a tweet, such as previous tweets or topics, author profile information, typical sentiment expressed by an author, and so on (Bamman and Smith 2015; Wang et al. 2015). Riloff et al. (2013), Khattri et al. (2015), and Van Hee (2017) exploited implicit or prototypical sentiment information"
J18-4010,P11-2008,0,0.0419782,"Missing"
J18-4010,P11-2102,0,0.389902,"Missing"
J18-4010,N09-1057,0,0.0588481,"Missing"
J18-4010,P13-2121,0,0.0237069,"rds/hashtag words/interjections, hashtag-to-word ratio, emoticon frequency, and tweet length. The first three are binary features, and the others are numeric and present normalized floats (i.e., divided by the tweet length in tokens), except the tweet length feature. A third set of lexical features include conditional n-gram probabilities based on language model probabilities. Although often exploited in machine translation research (e.g., Bojar et al. 2016), language model information incorporated as features is, to our knowledge, novel in irony detection. The models were created with KENLM (Heafield et al. 2013) and are trained on an ironic and a non-ironic background corpus.8 As 6 http://www.chatslang.com/terms/abbreviations. 7 n-grams based on raw tokens were preferred over lemma forms, as preliminary experiments revealed that better results were obtained with the former. 8 The data (1,126,128 tweets) were collected between April 2016 and January 2017 by crawling Twitter at regular intervals using the Twitter Search API. There is no overlap with the training corpus. 803 Computational Linguistics Volume 44, Number 4 features we extracted log probabilities indicating how probable a tweet is likely to"
J18-4010,P15-2124,0,0.0658214,"Missing"
J18-4010,D16-1104,0,0.0269753,"Missing"
J18-4010,P15-2106,0,0.0334952,"Missing"
J18-4010,E17-1025,0,0.0114016,"e tweets were collected between 1 December 2014 and 4 January 2015, represent 2,676 unique Twitter users, and have an average length of 15 tokens. An example tweet is presented in Figure 1. To minimize data noise (see earlier), all tweets were manually labeled using a newly developed fine-grained annotation scheme. Although a number of annotation schemes for irony have been developed recently (e.g., Riloff et al. 2013; Bosco et al. 2016; Stranisci et al. 2016), most of them describe a binary distinction (i.e., ironic vs. not-ironic) or flag irony as part of sentiment annotations. By contrast, Karoui et al. (2017) defined explicit and implicit irony activations based on incongruity in (French, English, and Italian) ironic tweets and they defined eight fine-grained categories of pragmatic devices that realize such an incongruity, including analogy, hyperbole, rhetorical question, oxymoron, and so forth. The typology provides valuable insights into the linguistic realization of irony that could improve 4 https://github.com/tweepy/tweepy. 799 Computational Linguistics Volume 44, Number 4 Figure 1 Corpus example. its automatic detection (e.g., the correlation between irony markers and irony activation type"
J18-4010,N01-1025,0,0.0255343,"Missing"
J18-4010,W15-2905,0,0.0960545,"es the presentation of a manually annotated irony corpus and details on how we developed our irony detection pipeline and experimented with different information sources for the task. 3.1 State of the Art in Irony Detection Research in NLP has recently seen various attempts to tackle irony detection. As described by Joshi, Bhattacharyya, and Carman (2017), irony modeling approaches can roughly be classified into rule-based and machine learning methods. Whereas rulebased approaches mostly rely on lexical information and require no training (e.g., Veale and Hao 2010; Maynard and Greenwood 2014; Khattri et al. 2015), machine learning does utilize training data and exploits various information sources (or features), including bags of words, syntactic patterns, sentiment information, and semantic relatedness (Davidov, Tsur, and Rappoport 2010; Liebrecht, Kunneman, and van den Bosch 2013; Reyes, Rosso, and Veale 2013). Twitter has been a popular data genre for the task, as 3 When discussing related research, we refer to irony using the terminology utilized by the corresponding researchers (i.e., “sarcasm,” “irony,” or “verbal irony”). 798 Van Hee, Lefever, and Hoste Using Common Sense to Detect Irony on Twi"
J18-4010,W13-1605,0,0.0833337,"Missing"
J18-4010,W06-2915,0,0.0360499,"n 3, we present a state-of-the-art irony detection system and in Section 4 we investigate the feasibility to model implicit sentiment in an automatic way. Exploring whether implicit sentiment information benefits irony detection is the focus of Section 5. Section 6 concludes and suggests some directions for future research. 2. Modeling Implicit Sentiment Modeling implicit sentiment is not a new challenge. Efforts to tackle this problem have been undertaken in different research areas, among others, sentiment analysis, content analysis in journalism, and irony detection. Although early work by Lin et al. (2006) investigated how to identify the perspective from which a document is written automatically, it is Greene (2007) and Wilson (2008) who have pioneered implicit sentiment research. Greene introduced the concept of what he later called syntactic packaging (Greene and Resnik 2009) and demonstrated the influence of syntactic choices on the perceived implicit sentiment of news headlines. He showed, for instance, that the active voice tends to attribute a greater sense of responsibility to the agent of a sentence, causing “a soldier veered his jeep into a crowded market and killed three civilians” t"
J18-4010,maynard-greenwood-2014-cares,0,0.0297326,"proach is given. This includes the presentation of a manually annotated irony corpus and details on how we developed our irony detection pipeline and experimented with different information sources for the task. 3.1 State of the Art in Irony Detection Research in NLP has recently seen various attempts to tackle irony detection. As described by Joshi, Bhattacharyya, and Carman (2017), irony modeling approaches can roughly be classified into rule-based and machine learning methods. Whereas rulebased approaches mostly rely on lexical information and require no training (e.g., Veale and Hao 2010; Maynard and Greenwood 2014; Khattri et al. 2015), machine learning does utilize training data and exploits various information sources (or features), including bags of words, syntactic patterns, sentiment information, and semantic relatedness (Davidov, Tsur, and Rappoport 2010; Liebrecht, Kunneman, and van den Bosch 2013; Reyes, Rosso, and Veale 2013). Twitter has been a popular data genre for the task, as 3 When discussing related research, we refer to irony using the terminology utilized by the corresponding researchers (i.e., “sarcasm,” “irony,” or “verbal irony”). 798 Van Hee, Lefever, and Hoste Using Common Sense"
J18-4010,C16-1151,0,0.0137293,"timization and logistic regression. Reyes, Rosso, and Veale (2013) defined features based on conceptual descriptions in irony literature, being signatures, unexpectedness, style, and emotional scenarios and experimented with na¨ıve Bayes and decision trees. Kunneman et al. (2015) pioneered irony detection in Dutch tweets using word n-gram features and a Balanced Winnow classifier. Van Hee, Lefever, and Hoste (2016b) combined lexical with sentiment, syntactic, and semantic Word2Vec cluster features for irony detection using a support vector machine (SVM). Recent work by Ghosh and Veale (2016), Poria et al. (2016), and Zhang, Zhang, and Fu (2016) has approached irony detection using (deep) neural networks, which make use of continuous automatic features instead of manually defined ones. Besides text-based features, irony research has explored extratextual features related to the author or context of a tweet, such as previous tweets or topics, author profile information, typical sentiment expressed by an author, and so on (Bamman and Smith 2015; Wang et al. 2015). Riloff et al. (2013), Khattri et al. (2015), and Van Hee (2017) exploited implicit or prototypical sentiment information to model a polarity"
J18-4010,D13-1066,0,0.261893,"Missing"
J18-4010,D11-1141,0,0.00761371,"), Table 2 Experimental corpus statistics: Number of instances per annotation category plus non-ironic tweets from a background corpus. ironic by clash 1,728 total 802 other type of irony situational irony other verbal irony 401 2,396 267 not ironic (hashtag corpus) not ironic (backgr. corpus) 604 1,792 2,396 Van Hee, Lefever, and Hoste Using Common Sense to Detect Irony on Twitter which is trained on user-generated content. For lack of a reliable Twitter-specific lemmatizer, we made use of the LeTs Preprocess toolkit (Van de Kauter et al. 2013). We used the Twitter named entity recognizer by Ritter et al. (2011) for named entity recognition. Additionally, all tweets were cleaned (e.g., replacement of HTML-escaped characters) and a number of (shallow) normalization steps were introduced to decrease feature sparseness. In concrete terms, all hyperlinks and @-replies in the tweets were normalized to “http://someurl” and “@someuser,” respectively, and abbreviations were replaced by their full form, based on an English abbreviation dictionary6 (e.g., “w/e” → “whatever”). Furthermore, variations in suspension dots were normalized to three dots (e.g., “.....” → “. . . ”), multiple white spaces were reduced"
J18-4010,L16-1462,0,0.0502238,"Missing"
J18-4010,P10-1059,0,0.143273,"Missing"
J18-4010,S14-2070,1,0.910067,"Missing"
J18-4010,C16-1257,1,0.889789,"Missing"
J18-4010,L16-1283,1,0.901638,"Missing"
J18-4010,R15-1086,1,0.875843,"Missing"
J18-4010,wilson-2008-annotating,0,0.055604,"n an automatic way. Exploring whether implicit sentiment information benefits irony detection is the focus of Section 5. Section 6 concludes and suggests some directions for future research. 2. Modeling Implicit Sentiment Modeling implicit sentiment is not a new challenge. Efforts to tackle this problem have been undertaken in different research areas, among others, sentiment analysis, content analysis in journalism, and irony detection. Although early work by Lin et al. (2006) investigated how to identify the perspective from which a document is written automatically, it is Greene (2007) and Wilson (2008) who have pioneered implicit sentiment research. Greene introduced the concept of what he later called syntactic packaging (Greene and Resnik 2009) and demonstrated the influence of syntactic choices on the perceived implicit sentiment of news headlines. He showed, for instance, that the active voice tends to attribute a greater sense of responsibility to the agent of a sentence, causing “a soldier veered his jeep into a crowded market and killed three civilians” to be perceived as more negative toward the soldier than “a soldier’s jeep veered into a crowded market, causing three civilian deat"
J18-4010,H05-1044,0,0.1139,"Missing"
J18-4010,I11-1131,0,0.0219135,"eory (Scherer 1999) and aims to store emotional reactions to real-world contexts (e.g., “I’m going to a family party because my mother obliges me to” → disgust). The knowledge base was used to learn intrinsic properties of entities and situations in journalistic text that trigger certain emotions like “refugees” and “being sentenced” (Balahur and Tanev 2016). Feng et al. (2013) created a connotation lexicon that determines the underlying sentiment of seemingly objective words (e.g., cooperation(+) , overcharge(-) ) and the general connotation of named entities (e.g., Einstein(+) , Osama(-) ). Zhang and Liu (2011) hypothesized that resource phrases (e.g., “this washer uses a lot of electricity”) are important carriers of implicit sentiment. They automatically extracted resource terms like “water” and “money” with resource 1 Game With a Purpose: A computer game which integrates human intervention in a computational process in an entertaining way. 2 International Survey on Emotion Antecedents and Reactions corpus. 796 Van Hee, Lefever, and Hoste Using Common Sense to Detect Irony on Twitter usage verbs such as “use” and “spend” and found that, when occurring together, they often imply a positive or negat"
J18-4010,C16-1231,0,0.0238262,"Missing"
J18-4010,S14-2009,0,\N,Missing
J18-4010,W16-2301,0,\N,Missing
L16-1051,W06-0901,0,0.107341,". Furthermore, defining a set of strict rules often results in low recall scores, since these lexico-semantic rules usually cover only a portion of the many various ways in which certain information can be lexicalized. Finally, rule-based systems are not easily portable to other languages and domains (or, in the case of event detection, to other types of events). In this paper, we tackle the task of economic event detection by means of a supervised machine learning approach, which we expect will be able to detect a wider variety of lexicalizations of economic events. Whereas many researchers (Ahn, 2006; Hardy et al., 2006; Ji and Grishman, 2008) have successfully applied machine learning techniques for event extraction (and coreference) tasks, we are not aware of studies focusing on economic events that employ machine learning methods without making use of event extraction rules. For this paper, we investigated the viability of a classification-based approach to economic event de330 Figure 1: Annotation of economic events in brat. tection based on an annotated corpus of Dutch financial news articles. We aimed at the detection of 10 types of company-specific events on the sentence level. The"
L16-1051,M93-1019,0,0.553062,"mpted the use of text mining techniques for the automatic detection of economic events in news text. Identifying news published about certain events in an automatic way enables researchers in the field of event studies to process more data in less time, and can consequently lead to new insights into the correlation between events and stock market movements. Furthermore, automatic event detection can be of use for various financial applications such as algorithmic trading (Hogenboom, 2012). Many of the existing approaches to the detection of economic events are pattern-based (i.e. rule-based). Appelt et al. (1993), for instance, apply a system which makes use of a domain pattern recognizer for the detection of joint venture events in English and Japanese text. Drury and Almeida (2011) also make use of phrase extraction patterns for the identification of business event phrases in news stories. Other pattern-based methodologies for the detection of certain types of economic events have been adopted by Arendarenko and Kakkonen (2012) and Hogenboom et al. (2013), who developed resp. the BEECON and SPEED systems. Both systems make use of domain ontologies and manually defined lexicon-semantic rules (e.g. “C"
L16-1051,P08-1030,0,0.0228484,"f strict rules often results in low recall scores, since these lexico-semantic rules usually cover only a portion of the many various ways in which certain information can be lexicalized. Finally, rule-based systems are not easily portable to other languages and domains (or, in the case of event detection, to other types of events). In this paper, we tackle the task of economic event detection by means of a supervised machine learning approach, which we expect will be able to detect a wider variety of lexicalizations of economic events. Whereas many researchers (Ahn, 2006; Hardy et al., 2006; Ji and Grishman, 2008) have successfully applied machine learning techniques for event extraction (and coreference) tasks, we are not aware of studies focusing on economic events that employ machine learning methods without making use of event extraction rules. For this paper, we investigated the viability of a classification-based approach to economic event de330 Figure 1: Annotation of economic events in brat. tection based on an annotated corpus of Dutch financial news articles. We aimed at the detection of 10 types of company-specific events on the sentence level. The use of several lexical, syntactic and seman"
L16-1051,P09-2079,0,0.0374138,"Missing"
L16-1051,lefever-etal-2014-evaluation,1,0.89873,"Missing"
L16-1051,E12-2021,0,0.183269,"Missing"
L16-1051,vossen-etal-2008-integrating,0,0.0390921,"Missing"
L16-1283,barbieri-saggion-2014-modelling-irony,0,0.14584,"Missing"
L16-1283,maynard-greenwood-2014-cares,0,0.146354,"verbal irony in social media texts. Furthermore, we present some statistics on the annotated corpora, from which we can conclude that the detection of contrasting evaluations might be a good indicator for recognizing irony. Keywords: social media, figurative language processing, verbal irony 1. Introduction With the arrival of Web 2.0, technologies like social media have become accessible to a vast amount of people. As a result, they have become valuable sources of information about the public’s opinion. What characterizes social media content is that it is often rich in figurative language (Maynard and Greenwood, 2014; Reyes et al., 2013). Handling figurative language represents, however, one of the most challenging tasks in natural language processing. It is often characterized by linguistic devices such as humor, metaphor and irony, whose meaning goes beyond the literal meaning and is therefore often hard to capture, even for humans. Effectively, understanding figurative language often requires world knowledge and familiarity with the conversational context and the cultural background of the conversation’s participants; information that is difficult to access by machines. Verbal irony is a particular gen"
L16-1283,D13-1066,0,0.0815295,"he results of an inter-annotator agreement study to assess the annotation guidelines. Section 4. elaborates on the annotated corpus; a number of statistics are presented to provide insight into the data. Finally, Section 5. concludes the paper with some prospects for future research. (1) It was so nice of my dad to come to my graduation party. #not 2. Regular sentiment analysis systems will probably classify this tweet as positive, whereas the intended emotion is undeniably a negative one. The hashtag #not indicates the presence of irony in this example. By contrast, in example 2. (taken from Riloff et al. (2013)), there is no explicit indication of irony present. Nevertheless, the irony is noticeable because given our world knowledge, we know that the act of going to the dentist (for a root canal) is typically 1 http://alt.qcri.org/semeval2015/task11 Related Research There are many different theoretical approaches to verbal irony. Traditionally, a distinction is made between situational and verbal irony. Situational irony is often referred to as situations that fail to meet some expectations (Lucariello, 1994; Shelley, 2001). Shelley (2001) illustrates this with firefighters who have a fire in their"
L16-1283,C96-2162,0,0.861352,"Missing"
L16-1465,de-clercq-etal-2012-evaluating,1,0.898454,"Missing"
L16-1465,S15-2122,1,0.864852,"Missing"
L16-1465,de-smedt-daelemans-2012-vreselijk,0,0.511896,"Missing"
L16-1465,E09-1046,0,0.265055,"y preprocessed using the LeTs Preprocess toolkit (Van de Kauter et al., 2013). Based on the resulting tokens and lemmas the lookup was performed. We only proceeded to the next steps when subjectivity was found. Aspect Term Extraction The first part of an ABSA system requires that candidate terms are automatically extracted, these terms are typically nouns, noun phrases or multiword expressions and they should be related to the restaurants domain. Moreover, terms can only be extracted when they are part of a subjective statement. In order to determine this subjectivity, we performed a lexicon (Jijkoun and Hofmann, 2009) lookup on both the surface forms and lemmas. To this purpose, In this sentence the TExSIS system will indicate good appetizer, our mother and pizza margherita as candidate terms. After subjectivity filtering, the positive word good will be stripped, leaving the term appetizer. The semantic filtering using Cornetto and DBpedia will lead to the conclusion that pizza margherita has more semantic links with the restaurant domain than the term our mother, which means that in the end only appetizer and pizza margherita will and should be extracted as aspect terms by our system. 2913 3.1.2. Experime"
L16-1465,W14-1306,0,0.0305379,"tures for category classification Lexical • Bag-of-words Semantic • Lexical-semantic > Cornetto > DBpedia • Semantic roles ober featvect CAT gebakjes featvect CAT 3.3 POLARITY CLASSIFICATION Review Onbeschofte [ober], maar overheerlijke [gebakjes]! SERVICE_general = negative FOOD_quality = positive Features for polarity classification Lexical • Token and character n-grams • Sentiment lexicons • Word-shape ober featvect POL gebakjes featvect POL Figure 6: Architecture of our Dutch ABSA pipeline illustrated with an example. EN: Rude waiter, but mouthwatering pastries! 3. ABSA pipeline Following Pavlopoulos and Androutsopoulos (2014) and the SemEval ABSA subtask classification (Pontiki et al., 2014; Pontiki et al., 2015), we discern three individual subtasks when it comes to performing aspect-based sentiment analysis automatically: aspect term extraction (Section 3.1), aspect term aggregation (Section 3.2) and aspect term polarity estimation (Section 3.3). Following similar experiments we performed on English data (De Clercq et al., 2015), we present a pipeline for Dutch which tackles these three subtasks incrementally. Figure 6 visualizes the architecture that was developed in order to perform the task of aspect-based se"
L16-1465,S14-2004,0,0.145618,"Missing"
L16-1465,S15-2082,0,0.273362,"4 concludes this paper and offers prospects for future work. 2. Dutch ABSA corpora Aspect-based sentiment analysis has proven important for mining and summarizing opinions from online reviews (Gamon et al., 2005; Titov and McDonald, 2008; Pontiki et al., 2014). Several benchmark datasets have been made publicly available, such as the product reviews dataset of Hu and Liu (2004) or the restaurant reviews dataset of Ganu et al. (2009). More recently, parts of these two datasets were extracted and re-annotated for two SemEval shared tasks on aspect-based sentiment analysis (Pontiki et al., 2014; Pontiki et al., 2015). For Dutch, to our knowledge, no such benchmark datasets exist. 2.1. Data collection We created two domain-specific corpora: one comprising restaurant (REST) and another comprising smartphone reviews (SMART). We are pleased to inform that both datasets have been made available for research purposes in the framework of SemEval 2016 task 5, the focus of which is multilingual ABSA (Pontiki et al., 2016). All reviews were crawled from online user platforms. For the restaurant reviews we relied on TripAdvisor1 and for the smartphone reviews on the online store Bol.com2 . On both platforms reviews"
L16-1465,S16-1002,1,0.867373,"Missing"
L16-1465,E12-2021,0,0.0385026,"detailed description and an overview of which main– attribute combinations are possible we refer to the previously mentioned guidelines. In the third and final step of the annotation process, the polarity of the sentiment expressed towards every annotated aspect expression/category is indicated. Three main polarities are distinguished: positive, neutral and negative. The neutral label applies to mildly positive or negative sentiment or when two opposing sentiments towards one feature expression occur within one sentence. Annotations were performed using BRAT4 , the brat rapid annotation tool (Stenetorp et al., 2012). It takes UTF8encoded text files as input, and stores the annotations in a proprietary standoff format. An example is presented below: in the first step of the annotation process the aspect expression wachttijd has been indicated, next it was assigned to the category Service–General, and finally the polarity of the sentiment expressed towards this aspect has been labeled as negative. (1) Dit is het vuilste, slechtste restaurant ooit! EN: This is the dirtiest, worst restaurant ever! Figure 1: ABSA annotation using BRAT. EN: Long waiting time. (2) Gaan eten bij Mama Mia deze namiddag. EN: Went"
L16-1465,P08-1036,0,0.0595104,"e been manually annotated using newly developed guidelines that comply to standard practices in the field. In Section 3 we present our pipeline which consists of three incremental steps: aspect term extraction, aspect category classification and aspect polarity classification. For each step we report results of first experiments that were performed on our restaurants dataset. Section 4 concludes this paper and offers prospects for future work. 2. Dutch ABSA corpora Aspect-based sentiment analysis has proven important for mining and summarizing opinions from online reviews (Gamon et al., 2005; Titov and McDonald, 2008; Pontiki et al., 2014). Several benchmark datasets have been made publicly available, such as the product reviews dataset of Hu and Liu (2004) or the restaurant reviews dataset of Ganu et al. (2009). More recently, parts of these two datasets were extracted and re-annotated for two SemEval shared tasks on aspect-based sentiment analysis (Pontiki et al., 2014; Pontiki et al., 2015). For Dutch, to our knowledge, no such benchmark datasets exist. 2.1. Data collection We created two domain-specific corpora: one comprising restaurant (REST) and another comprising smartphone reviews (SMART). We are"
L16-1465,S15-2083,0,0.097984,"cation task requires a system that is able to grasp subtle differences between the various categories (e.g. Food–General versus Food–Prices versus Food–Quality versus Food–Style&Options). To create such a system we first of all extracted typical lexical bagof-words features based on the sentence in which an aspect term occurs. 5 An analysis of the top-performing system of the SemEval2015 Task 12, however, revealed that besides lexical features, features in the form of clusters derived from a large reference corpus of restaurant reviews and thus capturing semantic information, are very useful (Toh and Su, 2015). For Dutch, we did not have such a large reference corpus available, but we did derive other semantic features. Based on the two semantic information sources that we also used for the semantic filtering of our TExSIS system, i.e. Cornetto and DBpedia, we derived various lexicalsemantic features. To be more precise, for Cornetto this translated to six features, each representing a value indicating the number of (unique) terms annotated as aspect terms from that category that (1) co-occur in the synset of the candidate term or (2) which are a hyponym/hypernym of a candidate term in the synset."
L16-1465,S14-2070,1,0.898558,"Missing"
L16-1465,J09-3003,0,0.0411759,"so has the interest in new text mining techniques to handle and analyze this growing amount of subjective text. One of the main research topics is sentiment analysis, also known as opinion mining. The objective of sentiment analysis is the extraction of subjective information from text, rather than factual information. Originally, it focused on the task of automatically classifying an entire document as positive, negative or neutral (Liu, 2012). More recently, the focus has shifted from coarse-grained to fine grained sentiment analysis, where sentiment has to be assigned at the clause level (Wilson et al., 2009). Often, users are not only interested in people’s general sentiments about a certain product, but also in their opinions about specific features, i.e. parts or attributes of that product. The task of automatically detecting all sentiment expressions within a given document and the concepts and aspects (or features) to which they refer is known as aspectbased or feature-based sentiment analysis, i.e. ABSA (Pontiki et al., 2014). Such systems do not only try to distinguish the positive from the negative utterances, but also strive to detect the target of the opinion, which comes down to a very"
L18-1284,C10-1070,0,0.0910224,"Missing"
L18-1284,hadi-etal-2004-evalda,0,0.148585,"Missing"
lefever-etal-2012-discovering,E06-1002,0,\N,Missing
lefever-etal-2012-discovering,P11-2055,1,\N,Missing
lefever-etal-2012-discovering,P10-1023,0,\N,Missing
lefever-etal-2012-discovering,D08-1080,0,\N,Missing
lefever-etal-2012-discovering,2005.mtsummit-papers.11,0,\N,Missing
lefever-etal-2014-evaluation,bosma-vossen-2010-bootstrapping,0,\N,Missing
lefever-etal-2014-evaluation,J90-1003,0,\N,Missing
lefever-etal-2014-evaluation,W04-1807,0,\N,Missing
lefever-etal-2014-evaluation,C92-2082,0,\N,Missing
lefever-etal-2014-evaluation,P99-1016,0,\N,Missing
lefever-etal-2014-evaluation,J98-1004,0,\N,Missing
lefever-etal-2014-evaluation,P06-1015,0,\N,Missing
lefever-etal-2014-evaluation,W12-3206,0,\N,Missing
lefever-etal-2014-evaluation,P10-1134,0,\N,Missing
lefever-etal-2014-evaluation,R13-1078,1,\N,Missing
lefever-etal-2014-evaluation,N04-1041,0,\N,Missing
lefever-etal-2014-evaluation,E12-2021,0,\N,Missing
lefever-etal-2014-evaluation,S12-1012,0,\N,Missing
lefever-hoste-2010-construction,J93-1004,0,\N,Missing
lefever-hoste-2010-construction,E09-1010,0,\N,Missing
lefever-hoste-2010-construction,C04-1192,0,\N,Missing
lefever-hoste-2010-construction,W09-2413,1,\N,Missing
lefever-hoste-2010-construction,W02-0808,0,\N,Missing
lefever-hoste-2010-construction,W09-2412,0,\N,Missing
lefever-hoste-2010-construction,P03-1058,0,\N,Missing
lefever-hoste-2010-construction,J03-1002,0,\N,Missing
lefever-hoste-2010-construction,2005.mtsummit-papers.11,0,\N,Missing
P03-1062,daelemans-hoste-2002-evaluation,1,0.825919,"onstrated by (Daelemans et al., 1999), exceptions do typically reoccur in language data. Hence, machine learning algorithms that retain a memory trace of individual instances, like memory-based learning algorithms based on the k-nearest neighbour classifier, outperform decision tree or rule inducers precisely for this reason. Comparing the performance of machine learning algorithms is not straightforward, and deserves careful methodological consideration. For a fair comparison, both algorithms should be objectively and automatically optimized for the task to be learned. This point is made by (Daelemans and Hoste, 2002), who show that, for tasks such as word-sense disambiguation and part-of-speech tagging, tuning algorithms in terms of feature selection and classifier parameters gives rise to significant improvements in performance. In this paper, therefore, we optimize both CART and MBL individually and per task, using a heuristic optimization method called iterative deepening. The second issue, that of task combination, stems from the intuition that the two tasks have a lot in common. For instance, (Hirschberg, 1993) reports that knowledge of the location of breaks facilitates accent placement. Although pi"
P03-1062,W96-0102,1,0.89273,". An excerpt of the annotated data with all generated symbolic and numeric1 features is presented in Table 1. Word forms (Wrd) – The word form tokens form the central unit to which other features are added. Pre- and post-punctuation – All punctuation marks in the data are transferred to two separate features: a pre-punctuation feature (PreP) for punctuation marks such as quotation marks appearing before the token, and a post-punctuation feature (PostP) for punctuation marks such as periods, commas, and question marks following the token. Part-of-speech (POS) tagging – We used MBT version 1.0 (Daelemans et al., 1996) to develop a memory-based POS tagger trained on the Eindhoven corpus of written Dutch, which does not overlap with our base data. We split up the full POS tags into two features, the first (PosC) containing the main POS category, the second (PosF) the POS subfeatures. Diacritical accent – Some tokens bear an orthographical diacritical accent put there by the author to particularly emphasize the token in question. These accents were stripped off the accented letter, and transferred to a binary feature (DiA). NP and VP chunking (NpC & VpC) – An approximation of the syntactic structure is provid"
P03-1062,P00-1030,0,0.0146539,"1997). Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary placement for Dutch, using an"
P03-1062,W99-0619,0,0.0486747,"erance (Cutler et al., 1997). Predicting prosody is known to be a hard problem that is thought to require information on syntactic boundaries, syntactic and semantic relations between constituents, discourse-level knowledge, and phonological well-formedness constraints (Hirschberg, 1993). However, producing all this information – using full parsing, including establishing semanto-syntactic relations, and full discourse analysis – is currently infeasible for a realtime system. Resolving this dilemma has been the topic of several studies in pitch accent placement (Hirschberg, 1993; Black, 1995; Pan and McKeown, 1999; Pan and Hirschberg, 2000; Marsi et al., 2002) and in prosodic boundary placement (Wang and Hirschberg, 1997; Taylor and Black, 1998). The commonly adopted solution is to use shallow information sources that approximate full syntactic, semantic and discourse information, such as the words of the text themselves, their part-of-speech tags, or their information content (in general, or in the text at hand), since words with a high (semantic) information content or load tend to receive pitch accents (Ladd, 1996). Within this research paradigm, we investigate pitch accent and prosodic boundary pla"
P11-2055,D07-1007,0,0.0634039,"than English), we decided to take a multilingual approach to WSD, that builds up the sense inventory on the basis of the Europarl parallel corpus (Koehn, 2005). Using 317 translations from a parallel corpus implicitly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the target translations. It also facilitates the integration of WSD in multilingual applications such as multilingual Information Retrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence"
P11-2055,P07-1005,0,0.0312932,"to take a multilingual approach to WSD, that builds up the sense inventory on the basis of the Europarl parallel corpus (Koehn, 2005). Using 317 translations from a parallel corpus implicitly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the target translations. It also facilitates the integration of WSD in multilingual applications such as multilingual Information Retrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004)."
P11-2055,daelemans-hoste-2002-evaluation,1,0.745982,"GIZA++ (Och and Ney, 2003) with its default settings for all focus words. This word alignment output was then considered to be the label for the training instances for the corresponding classifier (e.g. the Dutch translation is the label that is used to train the Dutch classifier). By considering this word alignment output as oracle information, we redefined the CLWSD task as a classification task. To train our five classifiers (English as input language and French, German, Dutch, Italian and Spanish as focus languages), we used the memory-based learning (MBL) algorithm implemented in TIMBL (Daelemans and Hoste, 2002), which has successfully been deployed in previous WSD classification tasks (Hoste et al., 2002). We performed heuristic experiments to define the parameter settings for the classifier, leading to the selection of the Jeffrey Divergence distance metric, Gain Ratio feature weighting and k = 7 as number of nearest neighbours. In future work, we plan to use an optimized word-expert approach in which a genetic algorithm performs joint feature selection and parameter optimization per ambiguous word (Daelemans et al., 2003). For our feature vector creation, we combined a set of English local context"
P11-2055,J94-4003,0,0.332534,"Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). The research described in this paper is novel as it presents a truly multilingual classification-based approach to WSD that directly incorporates evidence from four other languages. To this end, we build further on two well-known research ideas: (1) the possibility to use parallel corpora to extract translation labels and features in an automated way and (2) the assumption that incorporating evidence from multiple languages into the feature vector will be more informative than a"
P11-2055,P02-1033,0,0.319522,"etrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). The research described in this paper is novel as it presents a truly multilingual classification-based approach to WSD that directly incorporates evidence from four other languages. To this end, we build further on two well-known research ideas: (1) the possibility to use parallel corpora to extract translation labels and features in an automated way and (2) the assumption that incorporating evidence from multiple languages into the featu"
P11-2055,J93-1004,0,0.0491088,"tions such as multilingual Information Retrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). The research described in this paper is novel as it presents a truly multilingual classification-based approach to WSD that directly incorporates evidence from four other languages. To this end, we build further on two well-known research ideas: (1) the possibility to use parallel corpora to extract translation labels and features in an automated way and (2) the assumption that incorporating evidenc"
P11-2055,S10-1026,0,0.111609,"participated for Dutch and Spanish) and T3-COLEUR. The UvT-WSD system (van Gompel, 2010), that also uses a k-nearest neighbor classifier and a variety of local and global context features, obtained the best scores for Spanish and Dutch in the SemEval CLWSD competition. Although we also use a memory-based learner, our method is different from this system in the way the feature vectors are constructed. Next to the incorporation of similar local context features, we also include evidence from multiple languages in our feature vector. For French, Italian and German however, the T3-COLEUR system (Guo and Diab, 2010) outperformed the other systems in the SemEval competition. This system adopts a different approach: during the training phase a monolingual WSD system processes the English input sentence and a word alignment module is used to extract the aligned translation. The English senses together with their aligned translations (and probability scores) are then stored in a word sense translation table, in which look-ups are performed during the testing phase. This system also differs from the Uvt-WSD and ParaSense systems in the sense that the word senses are derived from WordNet, whereas the other sys"
P11-2055,W02-0808,0,0.258787,"Missing"
P11-2055,2005.mtsummit-papers.11,0,0.0206818,"nguages. 1 Introduction Word Sense Disambiguation (WSD) is the NLP task that consists in selecting the correct sense of a polysemous word in a given context. Most stateof-the-art WSD systems are supervised classifiers that are trained on manually sense-tagged corpora, which are very time-consuming and expensive to build (Agirre and Edmonds, 2006) . In order to overcome this acquisition bottleneck (sense-tagged corpora are scarce for languages other than English), we decided to take a multilingual approach to WSD, that builds up the sense inventory on the basis of the Europarl parallel corpus (Koehn, 2005). Using 317 translations from a parallel corpus implicitly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the target translations. It also facilitates the integration of WSD in multilingual applications such as multilingual Information Retrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translatio"
P11-2055,lefever-hoste-2010-construction,1,0.806978,"icted set of monolingual or bilingual features. Furthermore, our WSD system does not use any information from external lexical resources such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998). Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 317–322, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Experimental Setup Starting point of the experiments was the six-lingual sentence-aligned Europarl corpus that was used in the SemEval-2010 “Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b). The task is a lexical sample task for twenty English ambiguous nouns that consists in assigning a correct translation in the five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect the relevant translations for each of the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings for all focus words. This word alignment output was then considered to be the label for the training instances for the corresponding classifier (e.g. the Dutch translation is the label that"
P11-2055,S10-1003,1,0.941097,"icted set of monolingual or bilingual features. Furthermore, our WSD system does not use any information from external lexical resources such as WordNet (Fellbaum, 1998) or EuroWordNet (Vossen, 1998). Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 317–322, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics 2 Experimental Setup Starting point of the experiments was the six-lingual sentence-aligned Europarl corpus that was used in the SemEval-2010 “Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b). The task is a lexical sample task for twenty English ambiguous nouns that consists in assigning a correct translation in the five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect the relevant translations for each of the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings for all focus words. This word alignment output was then considered to be the label for the training instances for the corresponding classifier (e.g. the Dutch translation is the label that"
P11-2055,S07-1009,0,0.0100329,"rward accuracy measure. The SemEval metric takes into account the frequency weights of the gold standard translations: translations that were picked by different annotators get a higher weight. For the BEST evaluation, systems 1 http://code.google.com/apis/language/ http://wt.jrc.it/lt/Acquis/ 3 http://www.natcorp.ox.ac.uk/ 2 319 can propose as many guesses as the system believes are correct, but the resulting score is divided by the number of guesses. In this way, systems that output a lot of guesses are not favoured. For a more detailed description of the SemEval scoring scheme, we refer to McCarthy and Navigli (2007). Following variables are used for the SemEval precision formula. Let H be the set of annotators, T the set of test items and hi the set of responses for an item i ∈ T for annotator h ∈ H. Let A be the set of items from T where the system provides at least one answer and ai : i ∈ A the set of guesses from the system for item i. For each i, we calculate the multiset union (Hi ) for all hi for all h ∈ H and for each unique type (res) in Hi that has an associated frequency (f reqres ). P P P rec = ai :i∈A res∈ai f reqres |ai | |Hi | |A| (1) The second metric we use is a straightforward accuracy m"
P11-2055,P03-1058,0,0.0688095,"ual Information Retrieval (IR) or Machine Translation (MT). Significant improvements in terms of general MT quality were for the first time reported by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Several studies have already shown the validity of using parallel corpora for sense discrimination (e.g. (Ide et al., 2002)), for bilingual WSD modules (e.g. (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002; Chan and Ng, 2005; Dagan and Itai, 1994)) and for WSD systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). The research described in this paper is novel as it presents a truly multilingual classification-based approach to WSD that directly incorporates evidence from four other languages. To this end, we build further on two well-known research ideas: (1) the possibility to use parallel corpora to extract translation labels and features in an automated way and (2) the assumption that incorporating evidence from multiple l"
P11-2055,J03-1002,0,0.0020911,"l Linguistics 2 Experimental Setup Starting point of the experiments was the six-lingual sentence-aligned Europarl corpus that was used in the SemEval-2010 “Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b). The task is a lexical sample task for twenty English ambiguous nouns that consists in assigning a correct translation in the five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect the relevant translations for each of the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings for all focus words. This word alignment output was then considered to be the label for the training instances for the corresponding classifier (e.g. the Dutch translation is the label that is used to train the Dutch classifier). By considering this word alignment output as oracle information, we redefined the CLWSD task as a classification task. To train our five classifiers (English as input language and French, German, Dutch, Italian and Spanish as focus languages), we used the memory-based learning (MBL) algorithm implemented in TIMBL (Daelemans and Hoste, 2002),"
P11-2055,C04-1192,0,0.146204,"Missing"
P11-2055,S10-1053,0,0.399631,"Missing"
P11-2055,W09-2413,1,\N,Missing
R11-1026,doddington-etal-2004-automatic,0,0.0216555,"Missing"
R11-1026,C08-1033,0,0.0154312,"aria, 12-14 September 2011. they use genre-specific features such as average length of the coreferential chain and average distance separating several mentions of the same referent. An exception to this observation of small datasets is the new OntoNotes 4.0 corpus that is used for the CoNLL 2011 Shared Task on unrestricted coreference resolution, as the corpus contains approximately 1 million words from 5 different text genres.3 We do see a growing interest in one specific different text genre, namely biomedical text in many NLP tasks, including coreference resolution (e.g Yang et al. (2004), Gasperin and Briscoe (2008), Ngan Nguyen and Tsujii (2008)). The data for the experiments come from three Dutch corpus projects in which coreference was annotated: COREA (Hendrickx et al., 2008a), DuOMAn (Hendrickx and Hoste, 2009) and SoNaR (Schuurman et al., 2010)4 . Combining these three resources allows us to work with diverse data spread over different text genres. Another advantage is that all data was annotated following the same approach: first all NPs were pre-tagged based on syntactic dependency structures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et al., 2007) were reused in each"
R11-1026,hendrickx-etal-2008-coreference,1,0.926636,"Missing"
R11-1026,W07-1503,0,0.036824,"in many NLP tasks, including coreference resolution (e.g Yang et al. (2004), Gasperin and Briscoe (2008), Ngan Nguyen and Tsujii (2008)). The data for the experiments come from three Dutch corpus projects in which coreference was annotated: COREA (Hendrickx et al., 2008a), DuOMAn (Hendrickx and Hoste, 2009) and SoNaR (Schuurman et al., 2010)4 . Combining these three resources allows us to work with diverse data spread over different text genres. Another advantage is that all data was annotated following the same approach: first all NPs were pre-tagged based on syntactic dependency structures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et al., 2007) were reused in each project. Though the emphasis in this study is on edited text, we also include unedited text, viz. blogs and news comments (Hendrickx and Hoste, 2009). With this crossdomain portability study, we aim to see which genres perform better or worse and whether it is possible to determine a priori which training data to add to our resolver so as to obtain better results. The results are presented using four of the more frequently used evaluation metrics for coreference research, namely MUC (Vilain et al., 1995), Bcubed (Bagga"
R11-1026,W09-2411,0,0.0520498,"Missing"
R11-1026,H05-1083,0,0.158255,"delines (Bouma et al., 2007) were reused in each project. Though the emphasis in this study is on edited text, we also include unedited text, viz. blogs and news comments (Hendrickx and Hoste, 2009). With this crossdomain portability study, we aim to see which genres perform better or worse and whether it is possible to determine a priori which training data to add to our resolver so as to obtain better results. The results are presented using four of the more frequently used evaluation metrics for coreference research, namely MUC (Vilain et al., 1995), Bcubed (Bagga and Baldwin, 1998), CEAF (Luo and Zitouni, 2005) and BLANC (Recasens and Hovy, 2011). We show that adding more data to training proves mostly beneficial, especially when genrespecific information is included. Moreover, training a resolver on each genre separately allows us to classify each genre as having good or bad generalization power when applied to other genres. This led us to conduct experiments in which we train on all genres while progressively leaving out the worst-performing cross-domain genres as an attempt to boost overall performance. Although the 3 4 results are sometimes better, performance does not rise nor drop dramatically"
R11-1026,rodriguez-etal-2010-anaphoric,0,0.0411628,"Missing"
R11-1026,magnini-etal-2006-cab,0,0.0302547,"Dutch Coreference Resolution Orph´ee De Clercq, V´eronique Hoste LT3, Language and Translation Technology Team University College Ghent Groot-Brittannielaan 45 B - 9000 Gent, Belgium orphee.declercq@hogent.be veronique.hoste@hogent.be Abstract have been put in annotating corpora with coreferential relations. Not only a widespread language such as English (e.g. ACE-2 (Doddington et al., 2004), ARRAU (Poesio and Artstein, 2008), OntoNotes 3.0 (Weischedel et al., 2009)), but also Czech (PDT 2.0 (Kuˇcov´a and Hajiˇcov´a, 2004)), Catalan (AnCora-Ca (Recasens and Mart´ı, 2010)) and Italian (I-CAB (Magnini et al., 2006))2 can now rely on substantial resources for coreference research. One of the challenges in many current NLP tasks is to test their portability across different domains and languages. This portability to other languages was the main objective of the SemEval 2010 Task on Coreference Resolution in Multiple Languages (Recasens et al., 2010). The issue of domain portability was the focus of the ACL 2010 Workshop on Domain Adaptation for NLP (Daum´e III et al., 2010). In this paper we investigate the performance of an existing mention-pair coreference resolver for Dutch (Hoste, 2005; Hendrickx et a"
R11-1026,schuurman-etal-2010-interacting,1,0.831589,"OntoNotes 4.0 corpus that is used for the CoNLL 2011 Shared Task on unrestricted coreference resolution, as the corpus contains approximately 1 million words from 5 different text genres.3 We do see a growing interest in one specific different text genre, namely biomedical text in many NLP tasks, including coreference resolution (e.g Yang et al. (2004), Gasperin and Briscoe (2008), Ngan Nguyen and Tsujii (2008)). The data for the experiments come from three Dutch corpus projects in which coreference was annotated: COREA (Hendrickx et al., 2008a), DuOMAn (Hendrickx and Hoste, 2009) and SoNaR (Schuurman et al., 2010)4 . Combining these three resources allows us to work with diverse data spread over different text genres. Another advantage is that all data was annotated following the same approach: first all NPs were pre-tagged based on syntactic dependency structures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et al., 2007) were reused in each project. Though the emphasis in this study is on edited text, we also include unedited text, viz. blogs and news comments (Hendrickx and Hoste, 2009). With this crossdomain portability study, we aim to see which genres perform better or w"
R11-1026,J01-4004,0,0.0499172,"nce shifts, datasets of equal size (about 30K) were randomly selected. The focus of the current experiments was on resolving identity and predicative relations. Table 1 gives some statistics about each dataset, such as the average sentence length and the number of coreferring NPs. For all experiments we used an existing coreference resolver for Dutch, developed by Hoste (2005) and Hendrickx et al. (2008b). The system Website from CoNLL 2011: http://conll.bbn.com SoNaR is currently still under development. 5 187 http://mmax2.net follows a machine learning approach6 based on the seminal work of Soon et al. (2001) and represents a mention-pair model. First, a classifier is trained to decide whether a pair of NPs is coreferential or not, after which coreference chains are built for the pairs of NPs that were classified as coreferential. #docs ADM AUTO EXT INST MED JOUR WIKI DUO 21 15 29 18 213 52 15 56 #tokens 30,215 30,058 29,940 29,994 30,001 30,002 30,340 29,740 avg. senl 18.1 14.6 15.9 17.5 14.4 18.2 18.9 19.7 6.3 (Daelemans et al., 2010) with default parameter settings. Our experimental results are evaluated using the four scoring metrics as implemented in the scoring script from the coreference re"
R11-1026,W02-2024,0,0.0521019,"recall by counting how many elements are in the true coreferential cluster and how many in the predicted coreferential cluster. All datasets were preprocessed in the same way. Tokenisation, lemmatisation, Part-of-Speech tagging and grammatical relations were based on the manually verified output of the Alpino parser (Bouma et al., 2001), i.e. gold standard dependency structures. For the DuOMAn data, however, no gold standard dependency trees were available. Named entity recognition was performed using MBT (Daelemans et al., 2003), trained on the 2002 CoNNL shared task Dutch dataset (Tjong Kim Sang, 2002) and an additional gazetteer lookup. As features we employ string matching, distance between sentences and NPs, grammatical role and named entity overlap, synonym/hypernym lookup using Cornetto (a Dutch database combining Dutch Wordnet (Vossen, 1998) and the Referentie Bestand Nederlands (Martin and Ploeger, 1999)) and local context. All instances were built between NP pairs going 20 sentences back in context. NPs that are not part of a coreferential chain (singletons) are included as negative examples. For more information we refer to Hoste (2005) and Hendrickx et al. (2008a). Since the focus"
R11-1026,P10-1142,0,0.0157525,"oped to overcome problems with the other scoring methods. This measure is a variant of the Rand Index (Rand, 1971) adapted for coreference resolution and it averages over a score for correctly detecting singletons, and a score for detecting the correct cluster for coreferential elements. An important remark to make here is that our system does not take into account chains of only one element. As a consequence, contrary to the SemEval-2010 competition, when we compute 6 For an extensive overview of the different machine learning approaches for coreference resolution, we refer to the surveys of Ng (2010) and Poesio et al. (forthcoming) 7 Hoste (2005) built a separate learning module for each of these NP types based on the motivation that the impact of different information sources varies per NP type. 188 1. 2. 3. TRAIN one genre all genres but one all genres one genre all LOO outliers TEST that genre left out genre one genre other genres one genre 3 Results The results of the first round of experiments are presented in Figure 1. The dots marked as individual present the experiments in which each classifier was trained and tested on the same material. The scores for All-individual present expe"
R11-1026,M95-1005,0,0.416969,"ures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et al., 2007) were reused in each project. Though the emphasis in this study is on edited text, we also include unedited text, viz. blogs and news comments (Hendrickx and Hoste, 2009). With this crossdomain portability study, we aim to see which genres perform better or worse and whether it is possible to determine a priori which training data to add to our resolver so as to obtain better results. The results are presented using four of the more frequently used evaluation metrics for coreference research, namely MUC (Vilain et al., 1995), Bcubed (Bagga and Baldwin, 1998), CEAF (Luo and Zitouni, 2005) and BLANC (Recasens and Hovy, 2011). We show that adding more data to training proves mostly beneficial, especially when genrespecific information is included. Moreover, training a resolver on each genre separately allows us to classify each genre as having good or bad generalization power when applied to other genres. This led us to conduct experiments in which we train on all genres while progressively leaving out the worst-performing cross-domain genres as an attempt to boost overall performance. Although the 3 4 results are s"
R11-1026,nguyen-etal-2008-challenges,0,0.0237694,"se genre-specific features such as average length of the coreferential chain and average distance separating several mentions of the same referent. An exception to this observation of small datasets is the new OntoNotes 4.0 corpus that is used for the CoNLL 2011 Shared Task on unrestricted coreference resolution, as the corpus contains approximately 1 million words from 5 different text genres.3 We do see a growing interest in one specific different text genre, namely biomedical text in many NLP tasks, including coreference resolution (e.g Yang et al. (2004), Gasperin and Briscoe (2008), Ngan Nguyen and Tsujii (2008)). The data for the experiments come from three Dutch corpus projects in which coreference was annotated: COREA (Hendrickx et al., 2008a), DuOMAn (Hendrickx and Hoste, 2009) and SoNaR (Schuurman et al., 2010)4 . Combining these three resources allows us to work with diverse data spread over different text genres. Another advantage is that all data was annotated following the same approach: first all NPs were pre-tagged based on syntactic dependency structures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et al., 2007) were reused in each project. Though the emphasis i"
R11-1026,poesio-artstein-2008-anaphoric,0,0.0234744,"the performance of an existing mention-pair coreference resolver for Dutch (Hoste, 2005; Hendrickx et al., 2008b) across various text genres. More specifically we want to know whether training on out-of-domain data can be done without performance loss. The above-mentioned corpora designed for coreference resolution consist almost exclusively of text from the same genre, i.e. newspaper texts, and as a consequence resulting coreference resolvers are mostly trained on this particular genre. Moreover, when other genres are included, the acquired data are rather scarce: 25K of dialogues in ARRAU (Poesio and Artstein, 2008), 23K manuals in AnATar (Hammami et al., 2009) or 50K of annotated blogs in LiveMemories (Rodr´ıguez et al., 2010). Another related study is the work of Longo and Todirascu (2010). They analyzed a French corpus (50K) consisting of 5 different text genres to develop genre-specific features; in their study This article explores the portability of a coreference resolver across a variety of eight text genres. Besides newspaper text, we also include administrative texts, autocues, texts used for external communication, instructive texts, wikipedia texts, medical texts and unedited new media texts."
R11-1026,C04-1033,0,0.0119811,"86–193, Hissar, Bulgaria, 12-14 September 2011. they use genre-specific features such as average length of the coreferential chain and average distance separating several mentions of the same referent. An exception to this observation of small datasets is the new OntoNotes 4.0 corpus that is used for the CoNLL 2011 Shared Task on unrestricted coreference resolution, as the corpus contains approximately 1 million words from 5 different text genres.3 We do see a growing interest in one specific different text genre, namely biomedical text in many NLP tasks, including coreference resolution (e.g Yang et al. (2004), Gasperin and Briscoe (2008), Ngan Nguyen and Tsujii (2008)). The data for the experiments come from three Dutch corpus projects in which coreference was annotated: COREA (Hendrickx et al., 2008a), DuOMAn (Hendrickx and Hoste, 2009) and SoNaR (Schuurman et al., 2010)4 . Combining these three resources allows us to work with diverse data spread over different text genres. Another advantage is that all data was annotated following the same approach: first all NPs were pre-tagged based on syntactic dependency structures (Bouma and Kloostermans, 2007) and secondly the COREA guidelines (Bouma et a"
R11-1026,S10-1001,1,\N,Missing
R11-1026,C69-7001,0,\N,Missing
R11-1026,C69-6902,0,\N,Missing
R11-1026,W10-2600,0,\N,Missing
R13-1024,P06-2005,0,0.107264,"proposed a supervised noisy channel model using Hidden Markov Models to calculate the probability of less frequent words. Extensions to this approach were made by studying word processes (Cook and Stevenson, 2009), adapting weighted finite-state machines and rewrite rules (Beaufort et al., 2010) or by adding other elements such as orthographic, phonetic and contextual factors (Xue et al., 2011). Another approach is using statistical machine translation (SMT) techniques for text normalization. Previous work in this field has mostly focused on phrase-based machine translation at the word level. Aw et al. (2006) were the first to compare dictionary substitution using frequencies with phrase-based machine translation. They revealed that SMT improves BLEU scores for English SMS translation. Also working on English text, Raghunathan et al. (2009) confirmed that using an SMT system outperforms a dictionary look-up, most no3 Three Genres of UGC In order to normalize using a machine translation system, and to evaluate the performance, it is essential to build a gold standard data set that can serve as training and test material. As far as we know, no such data set is currently available for Dutch. 3.1 Corp"
R13-1024,P10-1079,0,0.557858,"red to as the spell-checking, machine translation and speech recognition metaphors (Kobus et al., 2008). The most intuitive way of normalizing text would be to approach the problem as a spellchecking one where noisy text has to be transformed to standard text using noisy channel models. Choudhury et al. (2007), for example, proposed a supervised noisy channel model using Hidden Markov Models to calculate the probability of less frequent words. Extensions to this approach were made by studying word processes (Cook and Stevenson, 2009), adapting weighted finite-state machines and rewrite rules (Beaufort et al., 2010) or by adding other elements such as orthographic, phonetic and contextual factors (Xue et al., 2011). Another approach is using statistical machine translation (SMT) techniques for text normalization. Previous work in this field has mostly focused on phrase-based machine translation at the word level. Aw et al. (2006) were the first to compare dictionary substitution using frequencies with phrase-based machine translation. They revealed that SMT improves BLEU scores for English SMS translation. Also working on English text, Raghunathan et al. (2009) confirmed that using an SMT system outperfo"
R13-1024,W09-2010,0,0.0454891,"define three dominant approaches to transfer noisy into standard text. These are referred to as the spell-checking, machine translation and speech recognition metaphors (Kobus et al., 2008). The most intuitive way of normalizing text would be to approach the problem as a spellchecking one where noisy text has to be transformed to standard text using noisy channel models. Choudhury et al. (2007), for example, proposed a supervised noisy channel model using Hidden Markov Models to calculate the probability of less frequent words. Extensions to this approach were made by studying word processes (Cook and Stevenson, 2009), adapting weighted finite-state machines and rewrite rules (Beaufort et al., 2010) or by adding other elements such as orthographic, phonetic and contextual factors (Xue et al., 2011). Another approach is using statistical machine translation (SMT) techniques for text normalization. Previous work in this field has mostly focused on phrase-based machine translation at the word level. Aw et al. (2006) were the first to compare dictionary substitution using frequencies with phrase-based machine translation. They revealed that SMT improves BLEU scores for English SMS translation. Also working on"
R13-1024,P07-1013,0,0.0751251,"Missing"
R13-1024,I11-1109,0,0.171077,"e work by Liu et al. (2011a; 2012). They propose a cognition-driven text normalization system using an unsupervised approach. By observing and simulating human techniques for the normalization task, they avoid dependence on human annotations. They construct a broad-coverage system to enable better word-coverage, using three key components: enhanced letter transformation, visual priming and string/phonetic similarity. If we consider normalization, the task intuitively has a lot in common with transliteration tasks for which character-based SMT systems have proven adequate (Vilar et al., 2007). Pennell and Liu (2011) were the first to study characterbased normalization. They, however, limited their approach by only focusing on abbreviations. In this paper, we propose a cascaded model that follows a machine translation approach and tries to tackle the full range of normalization problems. media genres used, their characteristics and how these have been normalized in Section 3. The setup and experiments are presented in Section 4. We examine the results in Section 5, perform a qualitative error analysis in Section 6 to end with some conclusions and prospects for future work in Section 7. 2 Related Work Trad"
R13-1024,kestemont-etal-2012-netlog,0,0.171929,"Missing"
R13-1024,D11-1141,0,0.0568722,"Missing"
R13-1024,C08-1056,0,0.504823,"Missing"
R13-1024,P07-2045,0,0.00585982,"Missing"
R13-1024,2008.amta-srw.5,0,0.0245086,"which we test this approach on the other genres to see whether it is possible to create a robust system that can process all three UGC genres. To evaluate our approach, both the Word Error Rate (WER) and BLEU scores were calculated. WER, an evaluation metric that is based on edit distance at the word level, is very well suited for the evaluation of NLP tasks where the input and output strings are closely related. As a consequence, the metric is used for the evaluation of optical character recognition (Kolak et al., 2003), grapheme-to-phoneme conversion (Demberg et al., 2007), diacritization (Schlippe et al., 2008) and vocalization of Arabic (K¨ubler and Mohamed, 2008). The BLEU metric, which has been specifically designed for measuring machine translation quality, measures the n-gram overlap between the translation being evaluated and a set of target translations. We therefore believe that BLEU is less appropriate for evaluation in the current set-up, but we include it for comparison’s sake (as other systems mention it such as Aw et al. (2006), Kobus et al. (2008), etc.). 5.1 Results on SMS 10 WER 15 20 25 This is, to our knowledge, the first study on Dutch text normalization, so there is no basis for"
R13-1024,N03-1018,0,0.0399779,"tested on the other two genres, 125 SNS posts and 125 tweets (Section 5.2). SMS genre, after which we test this approach on the other genres to see whether it is possible to create a robust system that can process all three UGC genres. To evaluate our approach, both the Word Error Rate (WER) and BLEU scores were calculated. WER, an evaluation metric that is based on edit distance at the word level, is very well suited for the evaluation of NLP tasks where the input and output strings are closely related. As a consequence, the metric is used for the evaluation of optical character recognition (Kolak et al., 2003), grapheme-to-phoneme conversion (Demberg et al., 2007), diacritization (Schlippe et al., 2008) and vocalization of Arabic (K¨ubler and Mohamed, 2008). The BLEU metric, which has been specifically designed for measuring machine translation quality, measures the n-gram overlap between the translation being evaluated and a set of target translations. We therefore believe that BLEU is less appropriate for evaluation in the current set-up, but we include it for comparison’s sake (as other systems mention it such as Aw et al. (2006), Kobus et al. (2008), etc.). 5.1 Results on SMS 10 WER 15 20 25 Th"
R13-1024,P11-2013,0,0.0954099,"framework of the STEVIN programme, see Spijns and Odijk (2013) for an overview. 179 Proceedings of Recent Advances in Natural Language Processing, pages 179–188, Hissar, Bulgaria, 7-13 September 2013. tably when used on an out-of-domain test set. Kobus et al. (2008) followed the same approach but combined the machine translation features with a speech recognition approach using HMMs on a French corpus. They concluded that the two systems perform better on different aspects of the task and that combining these two modules works best. A different way of approaching normalization is the work by Liu et al. (2011a; 2012). They propose a cognition-driven text normalization system using an unsupervised approach. By observing and simulating human techniques for the normalization task, they avoid dependence on human annotations. They construct a broad-coverage system to enable better word-coverage, using three key components: enhanced letter transformation, visual priming and string/phonetic similarity. If we consider normalization, the task intuitively has a lot in common with transliteration tasks for which character-based SMT systems have proven adequate (Vilar et al., 2007). Pennell and Liu (2011) wer"
R13-1024,P11-1037,0,0.0217483,"framework of the STEVIN programme, see Spijns and Odijk (2013) for an overview. 179 Proceedings of Recent Advances in Natural Language Processing, pages 179–188, Hissar, Bulgaria, 7-13 September 2013. tably when used on an out-of-domain test set. Kobus et al. (2008) followed the same approach but combined the machine translation features with a speech recognition approach using HMMs on a French corpus. They concluded that the two systems perform better on different aspects of the task and that combining these two modules works best. A different way of approaching normalization is the work by Liu et al. (2011a; 2012). They propose a cognition-driven text normalization system using an unsupervised approach. By observing and simulating human techniques for the normalization task, they avoid dependence on human annotations. They construct a broad-coverage system to enable better word-coverage, using three key components: enhanced letter transformation, visual priming and string/phonetic similarity. If we consider normalization, the task intuitively has a lot in common with transliteration tasks for which character-based SMT systems have proven adequate (Vilar et al., 2007). Pennell and Liu (2011) wer"
R13-1024,E12-1015,0,0.0287471,"t into characters and a translation at the character level takes place. This intuitively makes sense, because transformations at the character level are more likely to be reproduced than a combination of possible transformations at the word level. Trying to generalize such character transformations at the word level would probably fail due to data sparseness. We worked with both character unigram and bigram translation models. Bigrams supposedly have the advantage that one character of context across phrase boundaries is used in the selection of translation alternatives from the phrase table (Tiedemann, 2012). This means that more precise translations will be suggested. For our experiments we first focus on the individual performance we can achieve within the annotators were asked to indicate the end of a thought (to account for missing punctuation), regional words, foreign words and named entities. They could also flag words that are ungrammatical, stressed, part of a compound, used as interjections or words that require consecutive normalization operations. To check the reliability of our annotation guidelines, the two annotators each normalized the 1,000 text messages. We estimated the interann"
R13-1024,P12-1109,0,0.0600586,"Missing"
R13-1024,treurniet-etal-2012-collection,1,0.893276,"Missing"
R13-1024,W07-0705,0,0.0499513,"g normalization is the work by Liu et al. (2011a; 2012). They propose a cognition-driven text normalization system using an unsupervised approach. By observing and simulating human techniques for the normalization task, they avoid dependence on human annotations. They construct a broad-coverage system to enable better word-coverage, using three key components: enhanced letter transformation, visual priming and string/phonetic similarity. If we consider normalization, the task intuitively has a lot in common with transliteration tasks for which character-based SMT systems have proven adequate (Vilar et al., 2007). Pennell and Liu (2011) were the first to study characterbased normalization. They, however, limited their approach by only focusing on abbreviations. In this paper, we propose a cascaded model that follows a machine translation approach and tries to tackle the full range of normalization problems. media genres used, their characteristics and how these have been normalized in Section 3. The setup and experiments are presented in Section 4. We examine the results in Section 5, perform a qualitative error analysis in Section 6 to end with some conclusions and prospects for future work in Sectio"
R13-1078,S12-1012,0,0.119364,"a label to each cluster, it is then also possible to extract is − a relations between each cluster member and the cluster label. Caraballo (1999) uses syntactic dependency features (such as conjunction and apposition) to automatically build noun clusters. Pantel and Ravichandran (2004) extended his 594 3.1.1 work by including all syntactic dependency relations for each considered noun. More recent distributional approaches rely on the Distributional Inclusion Hypothesis, according to which semantically narrower terms include a significant number of distributional features of their hypernyms (Lenci and Benotto, 2012). The main advantage of the distributional approaches is that they allow to find semantically related terms, even when they do not explicitly occur in predefined patterns in text. The main disadvantage, however, is that these clustering approaches have difficulties to determine the exact semantic relationship (synonymy, antonymy, hyponymy) between the semantically related concepts. The corpus used in the experiments is a onemillion subcorpus of the 500-million word balanced reference corpus for contemporary (1954present) Dutch texts: SoNaR (Oostdijk et al., 2012). It consists of 38 text types"
R13-1078,P99-1016,0,0.284059,"target word, both cooccurrence and syntactic information can be extracted from the surrounding words. Unsupervised learning methods like clustering to obtain taxonomies, definitions and semantically similar words have been applied by (Widdows, 2003; Pereira et al., 1993; Van de Cruys, 2010). Clustering has also shown to be a valid approach to automatically detect hypernym relations between terms. By clustering words according to their contexts in text and assigning a label to each cluster, it is then also possible to extract is − a relations between each cluster member and the cluster label. Caraballo (1999) uses syntactic dependency features (such as conjunction and apposition) to automatically build noun clusters. Pantel and Ravichandran (2004) extended his 594 3.1.1 work by including all syntactic dependency relations for each considered noun. More recent distributional approaches rely on the Distributional Inclusion Hypothesis, according to which semantically narrower terms include a significant number of distributional features of their hypernyms (Lenci and Benotto, 2012). The main advantage of the distributional approaches is that they allow to find semantically related terms, even when the"
R13-1078,J96-2004,0,0.0118713,"Missing"
R13-1078,P98-2127,0,0.340679,"hyponymy relations in English text. Subsequently, various researchers continued working with this pattern-based approach for English (Cederberg and Widdows, 2003; Pantel and Ravichandran, 2004; Riloff and Shepherd, 1997; Roark and Charniak, 1998) as well as for other languages such as French (Malais´e et al., 2004) or Romanian (Mititelu, 2008). The patterns were further extended through translation and manually searching through texts (Kozareva et al., 2008), or by using more sophisticated methods of clustering related terms, starting from known hypernym pairs and features (Snow et al., 2006; Lin, 1998) or lists of seed words known to have the desired relationship (Roark and Charniak, 1998; Riloff and Shepherd, 1997; Widdows and Borow, 2002). Pantel and Pennacchiotti (2006) used generic patterns (broad coverage noisy patterns) to extract semantic relations and subsequently apply refining techniques to deal with the wide variety of such relations. Similar approaches that combine pattern extraction with post-processing techniques to enrich the system and improve the results have been investigated, for example, with Support Vector Machines and Hidden Markov Models (Ritter et al., 2009). A diffe"
R13-1078,W03-0415,0,0.163526,", Language and Translation Technology Team University College Ghent Gent, Belgium gwendolijn@gmail.com Abstract researchers have started to investigate how semantic resources such as ontologies can be learned from text instead of being created manually. For an overview we refer to (Biemann, 2005). In this paper, we focus on the detection of hypernym relations between nouns and noun phrases. Automatic extraction of nouns or noun phrases which are semantically related has been successfully achieved in prior research, for example using coordination and co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al.,"
R13-1078,W04-1807,0,0.641168,"Missing"
R13-1078,N03-1011,0,0.0519658,"Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al., 2003), which is the part-whole relationship, and synonymy (Lin et al., 2003), which expresses equality. Automatic hypernym detection has been explored in multiple ways. A clear distinction can be made between the pattern-based approaches and the statistical approaches. The aim of the present research is to present a hybrid approach in which distributional information acts as a filter on the pattern-based output. Although our current focus is on hypernym detection of noun-noun pairs, the final goal of this research is to use the automatic hypernym detection system to obtain a hierarchically structur"
R13-1078,C92-2082,0,0.748159,"co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al., 2003), which is the part-whole relationship, and synonymy (Lin et al., 2003), which expresses equality. Automatic hypernym detection has been explored in multiple ways. A clear distinction can be made between the pattern-based approaches and the statistical approaches. The aim of the present research is to present a hybrid approach in which distributional information acts as a filter on the pattern-based output. Although our current focus is on hypernym detection of noun-noun pairs, the final goal of this research is to use the automati"
R13-1078,P10-1134,0,0.146011,"Missing"
R13-1078,P09-1049,0,0.0285254,"olijn Schropp LT3, Language and Translation Technology Team University College Ghent Gent, Belgium gwendolijn@gmail.com Abstract researchers have started to investigate how semantic resources such as ontologies can be learned from text instead of being created manually. For an overview we refer to (Biemann, 2005). In this paper, we focus on the detection of hypernym relations between nouns and noun phrases. Automatic extraction of nouns or noun phrases which are semantically related has been successfully achieved in prior research, for example using coordination and co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference"
R13-1078,P08-1119,0,0.0239374,"attern-based approaches were inspired by the seminal work of Hearst (1992) in which she identified a set of lexico-syntactic patterns for the identification of hyponymy relations in English text. Subsequently, various researchers continued working with this pattern-based approach for English (Cederberg and Widdows, 2003; Pantel and Ravichandran, 2004; Riloff and Shepherd, 1997; Roark and Charniak, 1998) as well as for other languages such as French (Malais´e et al., 2004) or Romanian (Mititelu, 2008). The patterns were further extended through translation and manually searching through texts (Kozareva et al., 2008), or by using more sophisticated methods of clustering related terms, starting from known hypernym pairs and features (Snow et al., 2006; Lin, 1998) or lists of seed words known to have the desired relationship (Roark and Charniak, 1998; Riloff and Shepherd, 1997; Widdows and Borow, 2002). Pantel and Pennacchiotti (2006) used generic patterns (broad coverage noisy patterns) to extract semantic relations and subsequently apply refining techniques to deal with the wide variety of such relations. Similar approaches that combine pattern extraction with post-processing techniques to enrich the syst"
R13-1078,P06-1015,0,0.375589,"ws, 2003; Pantel and Ravichandran, 2004; Riloff and Shepherd, 1997; Roark and Charniak, 1998) as well as for other languages such as French (Malais´e et al., 2004) or Romanian (Mititelu, 2008). The patterns were further extended through translation and manually searching through texts (Kozareva et al., 2008), or by using more sophisticated methods of clustering related terms, starting from known hypernym pairs and features (Snow et al., 2006; Lin, 1998) or lists of seed words known to have the desired relationship (Roark and Charniak, 1998; Riloff and Shepherd, 1997; Widdows and Borow, 2002). Pantel and Pennacchiotti (2006) used generic patterns (broad coverage noisy patterns) to extract semantic relations and subsequently apply refining techniques to deal with the wide variety of such relations. Similar approaches that combine pattern extraction with post-processing techniques to enrich the system and improve the results have been investigated, for example, with Support Vector Machines and Hidden Markov Models (Ritter et al., 2009). A different approach has been used by Navigli et al. (2010), that use word class lattices, or directed acyclic graphs, to Other researchers have applied a distributional approach to"
R13-1078,N04-1041,0,0.546234,"present our results in Section 4. Section 5 concludes the paper with some prospects for future research. 2 Related Research Two main approaches are used to learn hypernym relations from text: pattern-based (or rule-based) approaches and distributional approaches. Most of the pattern-based approaches were inspired by the seminal work of Hearst (1992) in which she identified a set of lexico-syntactic patterns for the identification of hyponymy relations in English text. Subsequently, various researchers continued working with this pattern-based approach for English (Cederberg and Widdows, 2003; Pantel and Ravichandran, 2004; Riloff and Shepherd, 1997; Roark and Charniak, 1998) as well as for other languages such as French (Malais´e et al., 2004) or Romanian (Mititelu, 2008). The patterns were further extended through translation and manually searching through texts (Kozareva et al., 2008), or by using more sophisticated methods of clustering related terms, starting from known hypernym pairs and features (Snow et al., 2006; Lin, 1998) or lists of seed words known to have the desired relationship (Roark and Charniak, 1998; Riloff and Shepherd, 1997; Widdows and Borow, 2002). Pantel and Pennacchiotti (2006) used ge"
R13-1078,N04-3008,0,0.0320259,"sed cooccurring words. In a second step, we applied Pointwise Mutual Information (Church and Hanks, 1990) as a weighting function to discover informative semantic similarity relations between words. As we only want to consider contexts with a high semantic discrimination value, we smoothened the matrix by removing stop words and low frequent words (occurring less than 3 times in the corpus) from the context features. Finally, the cooccurrence matrix was converted into a vector of context features per target word. The matrix and vector construction was performed with the SenseClusters Package (Pedersen and Purandare, 2004). We used the CLUTO clustering toolkit (Karypis, 2002) to group semantically related words into clusters. Similarity between the context vectors was computed by taking their cosine, the cosine of the angle between two vectors being the inner product of the vectors. We used a K-means clustering algorithm and ran experiments with a varying number of output clusters. The impact of the desired number of output clusters is discussed in section 4. 4 Experimental results 4.1 Experimental set-up To evaluate the performance of both the patternbased and combined approach, we extracted a test set from th"
R13-1078,P93-1024,0,0.555766,"tices, or directed acyclic graphs, to Other researchers have applied a distributional approach to automatically extract hypernym pairs from text. The latter approaches start from the distributional hypothesis, stating that words that occur in similar contexts tend to be semantically similar (Harris, 1968). In order to define the context of a given target word, both cooccurrence and syntactic information can be extracted from the surrounding words. Unsupervised learning methods like clustering to obtain taxonomies, definitions and semantically similar words have been applied by (Widdows, 2003; Pereira et al., 1993; Van de Cruys, 2010). Clustering has also shown to be a valid approach to automatically detect hypernym relations between terms. By clustering words according to their contexts in text and assigning a label to each cluster, it is then also possible to extract is − a relations between each cluster member and the cluster label. Caraballo (1999) uses syntactic dependency features (such as conjunction and apposition) to automatically build noun clusters. Pantel and Ravichandran (2004) extended his 594 3.1.1 work by including all syntactic dependency relations for each considered noun. More recent"
R13-1078,C02-1114,0,0.238169,"ollege Ghent Gent, Belgium gwendolijn@gmail.com Abstract researchers have started to investigate how semantic resources such as ontologies can be learned from text instead of being created manually. For an overview we refer to (Biemann, 2005). In this paper, we focus on the detection of hypernym relations between nouns and noun phrases. Automatic extraction of nouns or noun phrases which are semantically related has been successfully achieved in prior research, for example using coordination and co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al., 2003), which is the part-whole relationship, and syn"
R13-1078,W97-0313,0,0.0669935,"4. Section 5 concludes the paper with some prospects for future research. 2 Related Research Two main approaches are used to learn hypernym relations from text: pattern-based (or rule-based) approaches and distributional approaches. Most of the pattern-based approaches were inspired by the seminal work of Hearst (1992) in which she identified a set of lexico-syntactic patterns for the identification of hyponymy relations in English text. Subsequently, various researchers continued working with this pattern-based approach for English (Cederberg and Widdows, 2003; Pantel and Ravichandran, 2004; Riloff and Shepherd, 1997; Roark and Charniak, 1998) as well as for other languages such as French (Malais´e et al., 2004) or Romanian (Mititelu, 2008). The patterns were further extended through translation and manually searching through texts (Kozareva et al., 2008), or by using more sophisticated methods of clustering related terms, starting from known hypernym pairs and features (Snow et al., 2006; Lin, 1998) or lists of seed words known to have the desired relationship (Roark and Charniak, 1998; Riloff and Shepherd, 1997; Widdows and Borow, 2002). Pantel and Pennacchiotti (2006) used generic patterns (broad cover"
R13-1078,N03-1036,0,0.367894,"Translation Technology Team University College Ghent Gent, Belgium gwendolijn@gmail.com Abstract researchers have started to investigate how semantic resources such as ontologies can be learned from text instead of being created manually. For an overview we refer to (Biemann, 2005). In this paper, we focus on the detection of hypernym relations between nouns and noun phrases. Automatic extraction of nouns or noun phrases which are semantically related has been successfully achieved in prior research, for example using coordination and co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al.,"
R13-1078,P98-2182,0,0.0687135,"chnology Team University College Ghent Gent, Belgium gwendolijn@gmail.com Abstract researchers have started to investigate how semantic resources such as ontologies can be learned from text instead of being created manually. For an overview we refer to (Biemann, 2005). In this paper, we focus on the detection of hypernym relations between nouns and noun phrases. Automatic extraction of nouns or noun phrases which are semantically related has been successfully achieved in prior research, for example using coordination and co-occurrence information (Oh et al., 2009; Cederberg and Widdows, 2003; Roark and Charniak, 1998; Widdows and Borow, 2002). However, automatically distinguishing exactly which semantic relationship exists between them is not that straightforward. One of these semantic relationships is the hypernym relation which can be seen as a set-subset relation. In the literature the following description is adopted the most: a(n) NP0 is a (kind of) NP1; where NP1 is the hypernym of NP0 (which is in turn the hyponym) and the relationship is reflexive and transitive but not symmetric (Miller et al., 1990; Hearst, 1992). Note the subtle difference with meronymy (Girju et al., 2003), which is the part-w"
R13-1078,P06-1101,0,0.150274,"ion can be made between the pattern-based approaches and the statistical approaches. The aim of the present research is to present a hybrid approach in which distributional information acts as a filter on the pattern-based output. Although our current focus is on hypernym detection of noun-noun pairs, the final goal of this research is to use the automatic hypernym detection system to obtain a hierarchically structured term list for any kind of input text. Prior research in hypernym detection suggested the extracted hypernym-hyponym pairs could be used to extend general thesauri like WordNet (Snow et al., 2006; Roark and Charniak, This paper proposes a two-step approach to find hypernym relations between pairs of noun phrases in Dutch text. We first apply a pattern-based approach that combines lexical and shallow syntactic information to extract a list of candidate hypernym pairs from the input text. In a second step, distributional similarity information is used to filter the obtained list of candidate pairs. Evaluation of the system shows encouraging results and reveals that the distributional information particularly helps to improve the precision for context dependent hypernym pairs. The propos"
R13-1078,W09-1122,0,0.041,"Missing"
R13-1078,bosma-vossen-2010-bootstrapping,0,\N,Missing
R13-1078,J90-1003,0,\N,Missing
R13-1078,C98-2177,0,\N,Missing
R13-1078,C98-2122,0,\N,Missing
R15-1086,E09-1046,0,0.0109468,"s. # Posts 3085 181 1671 129 546 23 49 1 • Character n-gram bag-of-words: binary features indicating the presence of character trigrams (without crossing word boundaries), to provide some abstraction from the word level. • Sentiment lexicon features: four numeric features representing the number of positive, negative, and neutral lexicon words (averaged over text length) and the overall post polarity (i.e. the sum of the values of identified sentiment words averaged over text length)4 . The features were calculated based on existing sentiment lexicons for Dutch (De Smedt and Daelemans, 2012b; Jijkoun and Hofmann, 2009). Table 3: Data distribution for the different author roles in cyberbullying events. 4 Experiments This section describes the experiments that were conducted to gain insight into the detection and fine-grained classification of cyberbullying events. 4.1 Features Experimental Setup 5 Two sets of experiments were conducted. Firstly, we explored the detection of cyberbullying posts regardless of the harmfulness score (i.e. we considered posts that were given a score of 1 or 2) and the author’s role. The second set of experiments focuses on a more complex task, the identification of fine-grained t"
R15-1086,de-smedt-daelemans-2012-vreselijk,1,0.415387,"Missing"
R15-1086,desmet-hoste-2014-recognising,1,0.851197,"f cyberbullying posts regardless of the harmfulness score (i.e. we considered posts that were given a score of 1 or 2) and the author’s role. The second set of experiments focuses on a more complex task, the identification of fine-grained text categories related to cyberbullying (see Section 3.2). To this end, a binary classifier was built for each category. Evaluation was done using 10-fold cross-validation. We used Support Vector Machines (SVM) as the classification algorithm since they have proven to work well for high-skew text classification tasks similar to the ones under investigation (Desmet and Hoste, 2014). We used linear kernels and experimentally determined the optimal cost value c to be 1. All experiments were carried out using Pattern (De Smedt and Daelemans, 2012a), a Python Results We implemented different experimental set-ups with various feature groups and hence determined the informativeness of each feature group for the current classification tasks. We explored the contributiveness of the following feature groups in isolation: word unigram bag-of-words (which can be considered as the baseline approach), word bigram bag-of-words, character trigram bag-ofwords, and sentiment lexicon fea"
R15-1086,E12-2021,0,0.0383805,"Missing"
R19-1086,P06-2005,0,0.0654992,"ly, it is very difficult and expensive to maintain a high coverage lexicon. Other works have approached the problem using a noisy chanThe machine translation metaphor treats social media text as the source language and its normalized form as the target language. Several works have tackled the problem of text normalization using this approach. Statistical Machine Translation (SMT) models, especially those trained at the character-level, have proven highly effective for the task because they capture well intra-word transformations. One of the first works following this approach was presented by Aw et al. (2006). They adapted phrase-based SMT to the task of normalizing English SMS producing messages that collated well with manually normalized ones. Besides, they studied the impact of the normalization on the task of SMS translation, showing that SMS normalization, as a preprocessing step of MT, can boost the translation performance. Kaufmann (2010) used a two-step approach for 741 Source sentence iz da muzieksgool vnavnd ? kwt da niemr . wa is je msn k en e nieuwe msn omda k er nie meer op graal . xxx @renskedemaessc dm me je gsmnummer eens ;-) Target sentence is dat muziekschool vanavond ? ik weet d"
R19-1086,P10-1079,0,0.19869,"/10.26615/978-954-452-056-4_086 nel model. In this model, the goal is to find the intended word w given a word x where the letters have been changed in some way. Correct words in the text remain untouched. This model is probably the most popular and successful approach to spelling correction (Dutta et al., 2015; Goot, 2015). Although spelling correction is mostly performed on languages which are morphologically simple and with a fairly strict word order, like English, there has been some progress for normalization applied to other languages as well, such as Russian (Sorokin, 2017) and French (Beaufort and Roekhaut, 2010). statistical machine translation (SMT) and neural machine translation (NMT), on English and Dutch parallel corpora with data coming from three genres (text messages, message board posts and tweets). For SMT we explore the added value of varying background corpora for training the language model. For NMT we have a look at data augmentation since the parallel datasets we are working with are limited in size. Our results reveal that when relying on SMT to perform the normalization it is beneficial to use a background corpus that is close to the genre to be normalized. Regarding NMT, we find that"
R19-1086,D16-1120,0,0.0313693,"Text Normalization Claudia Matos Veliz, Orph´ee De Clercq and V´eronique Hoste LT 3 , Language and Translation Technology Team - Ghent Univeristy Groot-Brittanni¨elaan 45, 9000, Ghent, Belgium Firstname.Lastname@UGent.be Abstract verse range of ways to communicate and particular forms of language variations (Ke et al., 2008). Social variables such as gender, age and race can also influence communication style (Schwartz et al., 2013; Blodgett et al., 2016). Location is also an important variable, since it can lead to the use of dialect and non-standard words. (Vandekerckhove and Nobels, 2010; Blodgett et al., 2016). Very typical for this User-Generated Content (UGC) is the expression of emotions by the use of symbols or lexical variations of the words (Van Hee et al., 2017). This can be done in the form of flooding or the repetition of characters, capitalization, and the productive use of emoticons. In addition, the use of homophonous graphemic variants of a word, abbreviations, spelling mistakes or letter transpositions are very typical, since people tend to write as they speak and/or write as fast as possible. One can imagine that all those characteristics contribute to an increased difficulty of auto"
R19-1086,P11-1038,0,0.020826,"ice, which is converted to a word lattice using a phoneme-grapheme dictionary. Finally, the word lattice is decoded by applying a language model to it and using a best-path algorithm to recover the most likely original word sequence. This metaphor has mainly been merged with the machine translation (infra) and spell checking (supra) metaphors to improve the quality of the normalization. Kobus et al. (2008), for example, incorporated ideas from speech recognition to text message normalization and combined it with a machine translation system. Beaufort and Roekhaut (2010); Xue et al. (2011) and Han and Baldwin (2011) also combined the automatic speech recognition approach with spell checking and machine translation techniques. Related Works Previous research on UGC text normalization has been performed on diverse languages using different techniques ranging from hand-crafted rules (Chua et al., 2018) to deep learning approaches (Ikeda et al., 2016; Sproat and Jaitly, 2016; Lusetti et al., 2018). Kobus et al. (2008) introduced three metaphors to refer to these normalization approaches: the spell checking, automatic speech recognition and translation metaphors. In the spell checking metaphor, corrections fr"
R19-1086,W16-3918,0,0.0199633,"ors to improve the quality of the normalization. Kobus et al. (2008), for example, incorporated ideas from speech recognition to text message normalization and combined it with a machine translation system. Beaufort and Roekhaut (2010); Xue et al. (2011) and Han and Baldwin (2011) also combined the automatic speech recognition approach with spell checking and machine translation techniques. Related Works Previous research on UGC text normalization has been performed on diverse languages using different techniques ranging from hand-crafted rules (Chua et al., 2018) to deep learning approaches (Ikeda et al., 2016; Sproat and Jaitly, 2016; Lusetti et al., 2018). Kobus et al. (2008) introduced three metaphors to refer to these normalization approaches: the spell checking, automatic speech recognition and translation metaphors. In the spell checking metaphor, corrections from noisy to standard words occur at the word level. As in conventional spelling correction one has to deal with both non-word and real-word errors (Clark and Araki, 2011). The disadvantage of this approach is that all non-standard words (NSWs) have to be represented in the dictionary in order to obtain the corresponding normalization."
R19-1086,L18-1216,0,0.0660292,"Missing"
R19-1086,P17-4012,0,0.023352,"lves the data scarcity problem (Saito et al., 2017) or to annotate more data (step B). We tested this on the Dutch corpus and one particular genre, namely text messages (SMS). For step A, we augmented the parallel data by duplicating monolingual subtitles data on both the source and target side. For step B, we sampled and manually annotated ten thousand extra tokens from the Flemish part of the SoNaR corpus (Treurniet et al., 2012)2 . We relied on OpenNMT3 to train our encoderdecoder model. OpenNMT is an open source (MIT) initiative for neural machine translation and neural sequence modeling (Klein et al., 2017). The main system is implemented in the Lua/Torch mathematical framework, and can easily be extended using Torch’s internal standard neural network components. We used the version of the system with the basic architecture which consists of an encoder using a simple LSTM recurrent neural network. The decoder applies attention over the source sequence and implements input feeding (Luong et al., 2015). taken for Dutch, for which we used an in-house subtitles dataset, Europarl, and the combination of both. Corpus English OPUS Europarl Combined Dutch Subtitles Europarl Combined Sentences 22,512,649"
R19-1086,C08-1056,0,0.24328,"l those characteristics contribute to an increased difficulty of automatically processing and analyzing UGC. Since Natural Language Processing (NLP) tools have originally been developed for and trained on standard language, these non-standard forms adversely affect language analysis using these tools. One of the computational approaches which has been suggested to tackle this problem is text normalization (Sproat et al., 2001). This approach envisages transforming the lexical variants to their canonical forms. In this way, standard NLP tools can be applied in a next step, after normalization. Kobus et al. (2008) introduced three metaphors to refer to these normalization approaches: the spell checking, automatic speech recognition and machine translation metaphors. In section 2 these approaches are discussed in more depth. In this work, we follow the third metaphor and tackle text normalization as a Machine Translation (MT) task. We test the two leading paradigms, One of the main characteristics of social media data is the use of non-standard language. Since NLP tools have been trained on traditional text material, their performance drops when applied to social media data. One way to overcome this is"
R19-1086,de-clercq-etal-2014-towards,1,0.901967,"Missing"
R19-1086,R13-1024,1,0.856711,"Missing"
R19-1086,D15-1166,0,0.086554,"Missing"
R19-1086,N13-1037,0,0.112249,"parallel datasets we are working with are limited in size. Our results reveal that when relying on SMT to perform the normalization, it is beneficial to use a background corpus that is close to the genre to be normalized. Regarding NMT, we find that the translations - or normalizations - coming out of this model are far from perfect and that for a lowresource language like Dutch adding additional training data works better than artificially augmenting the data. 1 Introduction Probably one of the most persistent characteristics of social media texts is that they are full of non-standard words (Eisenstein, 2013). Several sources of noise can influence the way people write. For example, the different kind of social media platforms available nowadays provide a di740 Proceedings of Recent Advances in Natural Language Processing, pages 740–749, Varna, Bulgaria, Sep 2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_086 nel model. In this model, the goal is to find the intended word w given a word x where the letters have been changed in some way. Correct words in the text remain untouched. This model is probably the most popular and successful approach to spelling correction (Dutta et al., 2015; Goot,"
R19-1086,E12-1015,0,0.151582,"social media text corpus, we needed to find a resource that somewhat resembles this specific domain. We chose to work with existing corpora comprising two flavors of transcribed speech, namely subtitles and transcriptions of parliamentary debates, because we believe that these can better represent, to some extent, the usergenerated content that we can find in social media texts. Table 3 represents the different corpora we used for our experiments. For the English experiments we relied on three different background corpora for constructing our language models: the OpenSubtitles corpus (OPUS) (Tiedemann, 2012b) which is a collection of documents from http://www.opensubtitles.org/; the Europarl corpus (Koehn et al., 2006), extracted from the proceedings of the European Parliament; and the combination of both. A similar approach was SMT Approach The core idea behind SMT relies on the noisy channel model. In this task, two basic components are integrated: argmax P (y|x) = argmax P (x|y)P (y) y∈W y∈W The translation model P (y|x) is responsible for the correctness of the translation from the source x = x1 , x2 , ..., xm to the target sentence y = 1 All data was normalized following the procedure descr"
R19-1086,tiedemann-2012-parallel,0,0.0541111,"social media text corpus, we needed to find a resource that somewhat resembles this specific domain. We chose to work with existing corpora comprising two flavors of transcribed speech, namely subtitles and transcriptions of parliamentary debates, because we believe that these can better represent, to some extent, the usergenerated content that we can find in social media texts. Table 3 represents the different corpora we used for our experiments. For the English experiments we relied on three different background corpora for constructing our language models: the OpenSubtitles corpus (OPUS) (Tiedemann, 2012b) which is a collection of documents from http://www.opensubtitles.org/; the Europarl corpus (Koehn et al., 2006), extracted from the proceedings of the European Parliament; and the combination of both. A similar approach was SMT Approach The core idea behind SMT relies on the noisy channel model. In this task, two basic components are integrated: argmax P (y|x) = argmax P (x|y)P (y) y∈W y∈W The translation model P (y|x) is responsible for the correctness of the translation from the source x = x1 , x2 , ..., xm to the target sentence y = 1 All data was normalized following the procedure descr"
R19-1086,I17-2044,0,0.0772296,"2 , ..., xm to the target sentence y = 1 All data was normalized following the procedure described in Schulz et al. (2016) 743 Neural systems, however, require huge amounts of data in order to perform properly. The training data we have available for text normalization amounts to only a few hundred sentences, as can be derived from Table 2. Moreover, manually annotating more training is highly time-consuming. Under these conditions, we decided to experimentally verify whether it is more beneficial to use a data augmentation technique (step A) which possibly resolves the data scarcity problem (Saito et al., 2017) or to annotate more data (step B). We tested this on the Dutch corpus and one particular genre, namely text messages (SMS). For step A, we augmented the parallel data by duplicating monolingual subtitles data on both the source and target side. For step B, we sampled and manually annotated ten thousand extra tokens from the Flemish part of the SoNaR corpus (Treurniet et al., 2012)2 . We relied on OpenNMT3 to train our encoderdecoder model. OpenNMT is an open source (MIT) initiative for neural machine translation and neural sequence modeling (Klein et al., 2017). The main system is implemented"
R19-1086,W17-1408,0,0.0233323,"2–4, 2019. https://doi.org/10.26615/978-954-452-056-4_086 nel model. In this model, the goal is to find the intended word w given a word x where the letters have been changed in some way. Correct words in the text remain untouched. This model is probably the most popular and successful approach to spelling correction (Dutta et al., 2015; Goot, 2015). Although spelling correction is mostly performed on languages which are morphologically simple and with a fairly strict word order, like English, there has been some progress for normalization applied to other languages as well, such as Russian (Sorokin, 2017) and French (Beaufort and Roekhaut, 2010). statistical machine translation (SMT) and neural machine translation (NMT), on English and Dutch parallel corpora with data coming from three genres (text messages, message board posts and tweets). For SMT we explore the added value of varying background corpora for training the language model. For NMT we have a look at data augmentation since the parallel datasets we are working with are limited in size. Our results reveal that when relying on SMT to perform the normalization it is beneficial to use a background corpus that is close to the genre to b"
R19-1117,W16-4702,0,0.285304,"Missing"
R19-1117,W94-0104,0,0.722234,"preliminary list of cts is produced based on partof-speech (POS) patterns. Next, statistical metrics are applied to measure termhood (to what degree a term is related to the domain) and unithood for multi-word terms (whether the individual tokens combine to form a lexical unit) (Kageura and Umino, 1996). These metrics are used to sort the cts based on their likelihood to be actual terms. To filter the list, one can either determine a cut-off value or select the top n or top n percent of terms. As a final step, manual validation is required. This has been a standard methodology for some time (Daille, 1994) and is still used by state-ofthe-art systems such as TermoStat (Drouin, 2003) and TExSIS (Macken et al., 2013). However, the problem with these methodologies is determining the cut-off point (Lopes and Vieira, 2015) and combining multiple features (e.g., separate measures for termhood and unithood). It has become clear that multiple evidence (i.e. combining multiple features) is highly beneficial for ATE (Dobrov and Loukachevitch, 2011; Loukachevitch, 2012). Supervised machine learning (ML) methodologies are now being used in answer to these problems. By automatically learning an optimal comb"
R19-1117,R11-1103,0,0.168259,"her determine a cut-off value or select the top n or top n percent of terms. As a final step, manual validation is required. This has been a standard methodology for some time (Daille, 1994) and is still used by state-ofthe-art systems such as TermoStat (Drouin, 2003) and TExSIS (Macken et al., 2013). However, the problem with these methodologies is determining the cut-off point (Lopes and Vieira, 2015) and combining multiple features (e.g., separate measures for termhood and unithood). It has become clear that multiple evidence (i.e. combining multiple features) is highly beneficial for ATE (Dobrov and Loukachevitch, 2011; Loukachevitch, 2012). Supervised machine learning (ML) methodologies are now being used in answer to these problems. By automatically learning an optimal combination of features and cut-off points, many features can be efficiently combined. One of the biggest hurdles for the progress of ATE technologies has been the data acquisition bottleneck, both for evaluation and now also as training data. Manually annotating terms is a slow and arduous task, with notoriously low interannotator agreement due to the ambiguous nature of terms. This lack of agreement on the basic characteristics of terms i"
R19-1117,loukachevitch-2012-automatic,0,0.668372,"select the top n or top n percent of terms. As a final step, manual validation is required. This has been a standard methodology for some time (Daille, 1994) and is still used by state-ofthe-art systems such as TermoStat (Drouin, 2003) and TExSIS (Macken et al., 2013). However, the problem with these methodologies is determining the cut-off point (Lopes and Vieira, 2015) and combining multiple features (e.g., separate measures for termhood and unithood). It has become clear that multiple evidence (i.e. combining multiple features) is highly beneficial for ATE (Dobrov and Loukachevitch, 2011; Loukachevitch, 2012). Supervised machine learning (ML) methodologies are now being used in answer to these problems. By automatically learning an optimal combination of features and cut-off points, many features can be efficiently combined. One of the biggest hurdles for the progress of ATE technologies has been the data acquisition bottleneck, both for evaluation and now also as training data. Manually annotating terms is a slow and arduous task, with notoriously low interannotator agreement due to the ambiguous nature of terms. This lack of agreement on the basic characteristics of terms is also reflected in th"
R19-1117,W18-4909,0,0.243061,"Missing"
R19-1117,C14-1029,0,0.415532,"Missing"
R19-1117,L16-1294,0,0.131607,"Missing"
R19-1117,2011.eamt-1.31,0,0.0317846,"d and elaborate analysis and comparison of a traditional, state-ofthe-art system (TermoStat) and a new, supervised ML approach (HAMLET), using the results obtained for the same, manually annotated, Dutch corpus about dressage. 1 Introduction Automatic term extraction (ATE), also known as automatic term recognition (ATR), has long been an established task within the field of natural language processing. It can be used both in its own right, to automatically obtain a list of candidate terms (cts) from a specialised corpus, or as a preprocessing step for other tasks, such as machine translation (Wolf et al., 2011). The traditional method for ATE is a hybrid approach, combining both linguistic and statistical information. In a first step, linguistic preprocessing is performed and a preliminary list of cts is produced based on partof-speech (POS) patterns. Next, statistical metrics are applied to measure termhood (to what degree a term is related to the domain) and unithood for multi-word terms (whether the individual tokens combine to form a lexical unit) (Kageura and Umino, 1996). These metrics are used to sort the cts based on their likelihood to be actual terms. To filter the list, one can either det"
reynaert-etal-2012-beyond,W07-1513,0,\N,Missing
reynaert-etal-2012-beyond,W02-2021,0,\N,Missing
reynaert-etal-2012-beyond,W03-0425,0,\N,Missing
reynaert-etal-2012-beyond,treurniet-etal-2012-collection,1,\N,Missing
reynaert-etal-2012-beyond,schuurman-etal-2004-linguistic,1,\N,Missing
reynaert-etal-2012-beyond,de-clercq-etal-2012-evaluating,1,\N,Missing
reynaert-etal-2012-beyond,R11-1026,1,\N,Missing
reynaert-etal-2012-beyond,schuurman-vandeghinste-2010-cultural,1,\N,Missing
reynaert-etal-2012-beyond,W03-2414,1,\N,Missing
S01-1020,W96-0102,1,0.895448,"Missing"
S01-1020,kilgarriff-rosenzweig-2000-english,0,0.0438141,"even with limited training data. 1 Introdu tion We report on the use of ma hine learning, espe ially memory-based learning and lassi er ombination, for word sense disambiguation (WSD) in the English all words task of SENSEVAL2. WSD an be des ribed as the problem of assigning the appropriate sense to a given word in a given ontext. Ma hine learning te hniques show state-of-the-art a ura y on WSD, e.g. memory-based learning (Ng and Lee, 1996; Veenstra et al., 2000), de ision lists (Yarowsky, 2000), and ombination methods (Es udero et al., 2000). Results of the rst SENSEVAL exer ise for English (Killgarri and Rosenzweig, 2000), in whi h only a restri ted set of words had to be disambiguated, showed that supervised learning systems outperform unsupervised ones, even when little orpus training material was available. In our submission to SENSEVAL2, we investigated whether the supervised learning approa h an be s aled to the all-words task. As a ba k-o for word-tag pairs for whi h no or not enough training data was available, we used the most frequent sense in the WordNet1.7 sense lexi on (Fellbaum, 1998) as default lassi er in the disambiguation pro ess. Sense disambiguation was mainly performed by a memory-based lea"
S01-1020,P96-1006,0,0.0972931,"le word expert was determined. Results show that espe ially memory-based learning in a word-expert approa h is a feasible method for unrestri ted word-sense disambiguation, even with limited training data. 1 Introdu tion We report on the use of ma hine learning, espe ially memory-based learning and lassi er ombination, for word sense disambiguation (WSD) in the English all words task of SENSEVAL2. WSD an be des ribed as the problem of assigning the appropriate sense to a given word in a given ontext. Ma hine learning te hniques show state-of-the-art a ura y on WSD, e.g. memory-based learning (Ng and Lee, 1996; Veenstra et al., 2000), de ision lists (Yarowsky, 2000), and ombination methods (Es udero et al., 2000). Results of the rst SENSEVAL exer ise for English (Killgarri and Rosenzweig, 2000), in whi h only a restri ted set of words had to be disambiguated, showed that supervised learning systems outperform unsupervised ones, even when little orpus training material was available. In our submission to SENSEVAL2, we investigated whether the supervised learning approa h an be s aled to the all-words task. As a ba k-o for word-tag pairs for whi h no or not enough training data was available, we used"
S07-1019,S07-1012,0,0.326078,"rices. The classification and clustering experiments, and the final combination of the different outputs are discussed in Section 3. Section 4 gives an overview of the results on the test data and Section 5 summarizes the main findings of the paper. 105 Proceedings of the 4th International Workshop on Semantic Evaluations (SemEval-2007), pages 105–108, c Prague, June 2007. 2007 Association for Computational Linguistics 2 Data sets and feature construction The data we have used for training our system were made available in the framework of the SemEval (task 13: Web People Search) competition (Artiles et al., 2007). As preliminary training corpus (referred to as “trial data” in our article), we used the WePS corpus (Web People Search corpus), available at http://nlp.uned.es/weps. For the real training set, this trial set was expanded in order to cover different degrees of ambiguity (very common names, uncommon names and celebrity names which tend to monopolize search results). The training corpus is composed of 40 sets of 100 web pages, each set corresponding to the first 100 results for a person name query. The documents were manually clustered. Documents that couldn’t be clustered properly have been p"
S07-1019,P98-1012,0,0.35216,"n Finding information about people on the World Wide Web is one of the most popular activities of Internet users. Given the high ambiguity of person names and the increasing amount of information on the web, it becomes very important to organize this large amount of information into meaningful clusters referring each to one single individual. The problem of resolving name ambiguity on the Internet has been approached from different angles. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) Timur Fayruzov Computational Web Intelligence Ghent University Association Krijgslaan 281, 9000 Gent Timur.Fayruzov@UGent.be have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. Documents get clustered using the standard vector space model. Other researchers have taken this search for distinctive keywords one step further and tried to come up with “concepts” describing the documents. Fleischman and Hovy (2004) introduce the “maximum entropy model”: a binary classifier"
S07-1019,W04-0701,0,0.0273924,"ace combining biographic facts and associated names, whereas Bagga and Baldwin (1998) Timur Fayruzov Computational Web Intelligence Ghent University Association Krijgslaan 281, 9000 Gent Timur.Fayruzov@UGent.be have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. Documents get clustered using the standard vector space model. Other researchers have taken this search for distinctive keywords one step further and tried to come up with “concepts” describing the documents. Fleischman and Hovy (2004) introduce the “maximum entropy model”: a binary classifier determines whether two concept-instance pairs refer to the same individual. Pedersen (2006) presented an unsupervised approach using bigrams in the contexts to be clustered, thus aiming at a concept level semantic space instead of a word level feature space. For the semeval contest, we approached the task from a double supervised and unsupervised perspective. For the supervised classification, the task was redefined in the form of feature vectors containing disambiguating information on pairs of documents. In addition to this, differe"
S07-1019,W03-0405,0,0.11896,"t a combined classification and clustering approach doesn’t always compare favorably to those obtained by the different algorithms separately. 1 Introduction Finding information about people on the World Wide Web is one of the most popular activities of Internet users. Given the high ambiguity of person names and the increasing amount of information on the web, it becomes very important to organize this large amount of information into meaningful clusters referring each to one single individual. The problem of resolving name ambiguity on the Internet has been approached from different angles. Mann and Yarowsky (2003) have proposed a Web based clustering technique relying on a feature space combining biographic facts and associated names, whereas Bagga and Baldwin (1998) Timur Fayruzov Computational Web Intelligence Ghent University Association Krijgslaan 281, 9000 Gent Timur.Fayruzov@UGent.be have looked for coreference chains within each document, take the context of these chains for creating summaries about each entity and convert these summaries into a bag of words. Documents get clustered using the standard vector space model. Other researchers have taken this search for distinctive keywords one step"
S07-1019,C98-1012,0,\N,Missing
S10-1001,W99-0212,0,0.0429338,"cation of the expressions in a text that refer to the same discourse entity (1), has attracted considerable attention within the NLP community. (1) Major League Baseball sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: 2 Linguistic Resources In this section, we first present the sources of the data used in the task. We then describe the automatic tools that predicted input annotations for the coreference resolution systems. 1 http://stel.ub.edu/semeval2010-coref 1 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8, c Uppsala, Sw"
S10-1001,S10-1022,0,0.0889833,"Missing"
S10-1001,orasan-etal-2008-anaphora,0,0.0341483,"o to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: 2 Linguistic Resources In this section, we first present the sources of the data used in the task. We then describe the automatic tools that predicted input annotations for the coreference resolution systems. 1 http://stel.ub.edu/semeval2010-coref 1 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Catalan Dutch English German Italian Spanish #docs Training #sents #tokens #docs 829 145 229 900 80 875 8,709 2,544 3,648 19,233 2,951 9,022 2"
S10-1001,S10-1021,1,0.673549,"Missing"
S10-1001,W99-0707,0,0.0438335,"6 English The OntoNotes Release 2.0 corpus (Pradhan et al., 2007) covers newswire and broadcast news data: 300k words from The Wall Street Journal, and 200k words from the TDT-4 collection, respectively. OntoNotes builds on the Penn Treebank for syntactic annotation and on the Penn PropBank for predicate argument structures. Semantic annotations include NEs, words senses (linked to an ontology), and coreference information. The OntoNotes corpus is distributed by the Linguistic Data Consortium.2 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). German The T¨uBa-D/Z corpus (Hinrichs et al., 2005) is a newspaper treebank based on data taken from the daily issues of “die tageszeitung” (taz). It currently comprises 794k words manually annotated with semantic and coreference information. Due to licensing restrictions of the original texts, a taz-DVD must be purchased to obtain a license."
S10-1001,doddington-etal-2004-automatic,0,0.0804398,"sent its head of security to Chicago to review the second incident of an on-field fan attack in the last seven months. The league is reviewing security at all ballparks to crack down on spectator violence. Using coreference information has been shown to be beneficial in a number of NLP applications including Information Extraction (McCarthy and Lehnert, 1995), Text Summarization (Steinberger et al., 2007), Question Answering (Morton, 1999), and Machine Translation. There have been a few evaluation campaigns on coreference resolution in the past, namely MUC (Hirschman and Chinchor, 1997), ACE (Doddington et al., 2004), and ARE (Orasan et al., 2008), yet many questions remain open: 2 Linguistic Resources In this section, we first present the sources of the data used in the task. We then describe the automatic tools that predicted input annotations for the coreference resolution systems. 1 http://stel.ub.edu/semeval2010-coref 1 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 1–8, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Catalan Dutch English German Italian Spanish #docs Training #sents #tokens #docs 829 145 229 900 80 875 8,709 2"
S10-1001,rodriguez-etal-2010-anaphoric,1,0.142463,"Missing"
S10-1001,S10-1017,1,0.853056,"Missing"
S10-1001,W08-1007,0,0.0168985,"annotation and on the Penn PropBank for predicate argument structures. Semantic annotations include NEs, words senses (linked to an ontology), and coreference information. The OntoNotes corpus is distributed by the Linguistic Data Consortium.2 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). German The T¨uBa-D/Z corpus (Hinrichs et al., 2005) is a newspaper treebank based on data taken from the daily issues of “die tageszeitung” (taz). It currently comprises 794k words manually annotated with semantic and coreference information. Due to licensing restrictions of the original texts, a taz-DVD must be purchased to obtain a license.2 Italian Lemmas and PoS were provided by TextPro,7 and dependency information by MaltParser.8 3 The German and Dutch training datasets were not completely stable during the competition period due to a few errors. Revised versions were released on March"
S10-1001,C08-1098,0,0.00615627,"respectively. OntoNotes builds on the Penn Treebank for syntactic annotation and on the Penn PropBank for predicate argument structures. Semantic annotations include NEs, words senses (linked to an ontology), and coreference information. The OntoNotes corpus is distributed by the Linguistic Data Consortium.2 Dutch Lemmas, PoS and NEs were automatically provided by the memory-based shallow parser for Dutch (Daelemans et al., 1999), and dependency information by the Alpino parser (van Noord et al., 2006). German Lemmas were predicted by TreeTagger (Schmid, 1995), PoS and morphology by RFTagger (Schmid and Laws, 2008), and dependency information by MaltParser (Hall and Nivre, 2008). German The T¨uBa-D/Z corpus (Hinrichs et al., 2005) is a newspaper treebank based on data taken from the daily issues of “die tageszeitung” (taz). It currently comprises 794k words manually annotated with semantic and coreference information. Due to licensing restrictions of the original texts, a taz-DVD must be purchased to obtain a license.2 Italian Lemmas and PoS were provided by TextPro,7 and dependency information by MaltParser.8 3 The German and Dutch training datasets were not completely stable during the competition per"
S10-1001,W05-0303,0,0.0585819,"Missing"
S10-1001,hoste-de-pauw-2006-knack,1,0.856981,"Missing"
S10-1001,S10-1020,0,0.0536547,"Missing"
S10-1001,van-noord-etal-2006-syntactic,0,0.0179959,"Missing"
S10-1001,S10-1018,0,0.147949,"Missing"
S10-1001,M95-1005,0,0.744599,"d). Results are presented sequentially by language and setting, and participating systems are ordered alphabetically. The participation of systems across languages and settings is rather irregular,11 thus making it difficult to draw firm concluEvaluation Metrics Since there is no agreement at present on a standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures. The task scorer provides results in the two mentionbased metrics B3 (Bagga and Baldwin, 1998) and CEAF-φ3 (Luo, 2005), and the two link-based metrics MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, in prep). The first three measures have been widely used, while BLANC is a proposal of a new measure interesting to test. The mention detection subtask is measured with recall, precision, and F1 . Mentions are rewarded with 1 point if their boundaries coincide with those 11 4 Only 45 entries in Table 5 from 192 potential cases. BART (Broscheit et al., 2010) Corry (Uryupina, 2010) RelaxCor (Sapena et al., 2010) SUCRE (Kobdani and Sch¨utze, 2010) TANL-1 (Attardi et al., 2010) UBIU (Zhekova and K¨ubler, 2010) System Architecture ML Methods External Resources Closest"
S10-1001,H05-1004,0,0.516482,"st scores in each setting are highlighted in bold). Results are presented sequentially by language and setting, and participating systems are ordered alphabetically. The participation of systems across languages and settings is rather irregular,11 thus making it difficult to draw firm concluEvaluation Metrics Since there is no agreement at present on a standard measure for coreference resolution evaluation, one of our goals was to compare the rankings produced by four different measures. The task scorer provides results in the two mentionbased metrics B3 (Bagga and Baldwin, 1998) and CEAF-φ3 (Luo, 2005), and the two link-based metrics MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy, in prep). The first three measures have been widely used, while BLANC is a proposal of a new measure interesting to test. The mention detection subtask is measured with recall, precision, and F1 . Mentions are rewarded with 1 point if their boundaries coincide with those 11 4 Only 45 entries in Table 5 from 192 potential cases. BART (Broscheit et al., 2010) Corry (Uryupina, 2010) RelaxCor (Sapena et al., 2010) SUCRE (Kobdani and Sch¨utze, 2010) TANL-1 (Attardi et al., 2010) UBIU (Zhekova and K¨ubler, 2010)"
S10-1001,S10-1019,0,0.0843106,"Missing"
S10-1001,C10-2125,1,\N,Missing
S10-1001,M98-1029,0,\N,Missing
S10-1003,W09-2413,1,0.714521,"-lingual WSD task: (a) a parallel corpus on the basis of which the gold standard sense inventory was created and (b) a collection of English sentences containing the lexical sample words annotated with their contextually appropriate translations in five languages. 1 http://www.statmt.org/europarl/ 15 Proceedings of the 5th International Workshop on Semantic Evaluation, ACL 2010, pages 15–20, c Uppsala, Sweden, 15-16 July 2010. 2010 Association for Computational Linguistics Below, we provide a short summary of the complete data construction process. For a more detailed description, we refer to Lefever and Hoste (2009; 2010). across languages on the basis of unique sentence IDs. After the selection of all unique translation combinations, the translations were grouped into clusters. The clusters were organized in two levels, in which the top level reflects the main sense categories (e.g. for the word coach we have (1) (sports) manager, (2) bus, (3) carriage and (4) part of a train), and the subclusters represent the finer sense distinctions. Translations that correspond to English multiword units were identified and in case of non-apparent compounds, i.e. compounds which are not marked with a “-”, the diffe"
S10-1003,lefever-hoste-2010-construction,1,0.801637,"Missing"
S10-1003,S07-1009,0,0.0454523,"h.n.de 12 :: Coach 1; Fußbaltrainer 1; P Nationaltrainer 2; Trainer 3; P coach.n.it 12 :: allenatore 3; P rec = coach.n.es 12 :: entrenador 3; 3 3.1 a i :i∈A |A| P P Evaluation Rec = Scoring res∈a i f req res |ai | |H i | (1) res∈a i f req res |ai | a i :i∈T |T | |H i | (2) Out-of-five (Oof ) evaluation For the more relaxed evaluation, systems can propose up to five guesses. For this evaluation, the resulting score is not divided by the number of guesses. To score the participating systems, we use an evaluation scheme which is inspired by the English lexical substitution task in SemEval 2007 (McCarthy and Navigli, 2007). We perform both a best result evaluation and a more relaxed evaluation for the top five results. The evaluation is performed using precision and recall (P rec and Rec in the equations below), and Mode precision (M P ) and Mode recall (M R ), where we calculate precision and recall against the translation that is preferred by the majority of annotators, provided that one translation is more frequent than the others. For the precision and recall formula we use the following variables. Let H be the set of annotators, T the set of test items and hi the set of responses for an item i ∈ T for anno"
S10-1003,S07-1001,0,0.0656999,"Missing"
S10-1003,E09-1010,0,0.0316399,"necks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993; Ide et al., 2002; Ng et al., 2003; Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted 2 Task setup 2.1 Data sets Two types of data sets were used in the Cross-lingual WSD task: (a) a parallel corpus on the basis of which the gold standard sense inventory was created and (b) a collection of English sentences containing the lexical sample words annotated with their contextually appropriate translations in five languages. 1 http://www.statmt.org/europarl/ 15 Proceedings of the 5th International Workshop on Se"
S10-1003,P03-1058,0,0.219148,"ses on two bottlenecks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993; Ide et al., 2002; Ng et al., 2003; Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted 2 Task setup 2.1 Data sets Two types of data sets were used in the Cross-lingual WSD task: (a) a parallel corpus on the basis of which the gold standard sense inventory was created and (b) a collection of English sentences containing the lexical sample words annotated with their contextually appropriate translations in five languages. 1 http://www.statmt.org/europarl/ 15 Proceedings of the 5th Internati"
S10-1003,P91-1023,0,0.279579,"compounds which are not marked with a “-”, the different compound parts were separated by §§ in the clustering file (e.g. the German Post§§kutsche). All clustered translations were also manually lemmatized. The gold standard sense inventory was derived from the Europarl parallel corpus2 , which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus, viz. English (our target language), Dutch, French, German, Italian and Spanish. All data were already sentence-aligned using a tool based on the Gale and Church (1991) algorithm, which was part of the Europarl corpus. We only considered the 1-1 sentence alignments between English and the five other languages. These sentence alignments were made available to the task participants for the five trial words. The sense inventory extracted from the parallel data set (Section 2.2) was used to annotate the sentences in the trial set and the test set, which were extracted from the JRC-ACQUIS Multilingual Parallel Corpus3 and BNC4 . 2.2 2.3 Sense annotation of the test data The resulting sense inventory was used to annotate the sentences in the trial set (20 sentence"
S10-1003,J03-1002,0,0.0101799,"were also allowed to provide fewer. These potentially different translations were used to assign frequency weights (shown in example (2)) to the gold standard translations per sentence. The example (1) below shows the annotation result in both German and Dutch for an English source sentence containing coach. Creation of the sense inventory Two steps were taken to obtain a multilingual sense inventory: (1) word alignment on the sentences to find the set of possible translations for the set of ambiguous nouns and (2) clustering by meaning (per target word) of the resulting translations. GIZA++ (Och and Ney, 2003) was used to generate the initial word alignments, which were manually verified by certified translators in all six involved languages. The human annotators were asked to assign a “NULL” link to words for which no valid translation could be identified. Furthermore, they were also asked to provide extra information on compound translations (e.g. the Dutch word Investeringsbank as a translation of the English multiword Investment Bank ), fuzzy links, or target words with a different PoS (e.g. the verb to bank ). The manually verified translations were clustered by meaning by one annotator. In or"
S10-1003,W09-2412,0,0.123758,"Missing"
S10-1003,W02-0808,0,0.177975,"iguation task focuses on two bottlenecks in current WSD research, namely the scarcity of sense inventories and sense-tagged corpora (especially for languages other than English) and the growing tendency to evaluate the performance of WSD systems in a real application such as machine translation and cross-language information retrieval (see for example Agirre et al. (2007)). The Cross-lingual WSD task aims at the development of a multilingual data set to test the feasibility of multilingual WSD. Many studies have already shown the validity of this crosslingual evidence idea (Gale et al., 1993; Ide et al., 2002; Ng et al., 2003; Apidianaki, 2009), but until now no benchmark data sets have been available. For the SemEval-2010 competition we developed (i) a sense inventory in which the sense distinctions were extracted 2 Task setup 2.1 Data sets Two types of data sets were used in the Cross-lingual WSD task: (a) a parallel corpus on the basis of which the gold standard sense inventory was created and (b) a collection of English sentences containing the lexical sample words annotated with their contextually appropriate translations in five languages. 1 http://www.statmt.org/europarl/ 15 Proceedings of"
S10-1003,2005.mtsummit-papers.11,0,0.0996708,"(sports) manager, (2) bus, (3) carriage and (4) part of a train), and the subclusters represent the finer sense distinctions. Translations that correspond to English multiword units were identified and in case of non-apparent compounds, i.e. compounds which are not marked with a “-”, the different compound parts were separated by §§ in the clustering file (e.g. the German Post§§kutsche). All clustered translations were also manually lemmatized. The gold standard sense inventory was derived from the Europarl parallel corpus2 , which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus, viz. English (our target language), Dutch, French, German, Italian and Spanish. All data were already sentence-aligned using a tool based on the Gale and Church (1991) algorithm, which was part of the Europarl corpus. We only considered the 1-1 sentence alignments between English and the five other languages. These sentence alignments were made available to the task participants for the five trial words. The sense inventory extracted from the parallel data set (Section 2.2) was used to annotate the sentences in"
S13-2029,E09-1010,0,0.106415,"y, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task. The remainder of this paper is organized as follows. Section 2 focuses on the task description and briefly recapitalizes the construction of the sense inventory and the annotation procedure of the test sentences. Section 3 presents the participating sy"
S13-2029,S13-2032,0,0.0479872,"cond and best performing - flavor of the system (SnT run) calculates the cosine similarity between the context words of the test and training sentences. The output of the system then contains the translation that results from running word alignment on the focus word in the training corpus. As a fallback, WordNet is again used. The WN senses are sorted by frequency in the SemCor corpus and the corresponding translation is selected from the aligned WordNet in the target language. The third run of the system (merged) combines the output from the other two flavors of the system. The LIMSI system (Apidianaki, 2013) applies an unsupervised CLWSD method that was proposed in (Apidianaki, 2009) for three target languages, viz. Spanish, Italian and French. First, word alignment is applied on the parallel corpus and three bilingual lexicons are built, containing for each focus word the translations in the three target languages. In a next step, a vector is built for each translation of the English focus word, using the cooccurrences of the word in the sentences in which it gets this particular translation. A clustering algorithm then groups the feature vectors using the Weighted Jaccard measure. New instances"
S13-2029,P91-1034,0,0.696471,"owards a specific target domain or application and to train a dedicated CLWSD system using these particular sense inventories. Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task. The remainder of this paper is organized as follows. Section 2 focuses on the task description and briefly recapitalizes"
S13-2029,D07-1007,0,0.0924441,"for a particular word that should fit all possible domains and applications. In addition, the use of domain-specific corpora allows to derive sense inventories that are tailored towards a specific target domain or application and to train a dedicated CLWSD system using these particular sense inventories. Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed co"
S13-2029,S13-2034,0,0.0397537,"uses local context features for a window of three words containing the focus word. Parameters were optimized on the trial data. The second flavor of the system (c1lN) uses the same configuration of the system, but without parameter optimization. The third configuration of the system (var) is heavily optimized on the trial data, selecting the winning configuration per trial word and evaluation metric. In addition to the local context features, also global bag-of-word context features are considered for this version of the system. A completely different approach is taken by the NRC-SMT system (Carpuat, 2013), that uses a statistical machine translation approach to tackle the CLWSD task. The baseline version of the system (SMTbasic) represents a standard phrase-based SMT baseline, that is trained only on the intersected Europarl corpus. Translations for the test instances are extracted from the top hypothesis (for the best evaluation) or from the 100-best list (for the Out-of-five evaluation). The optimized version of the system (SMTadapt2) is trained on the Europarl corpus and additional news data, and uses mixture models that are developed for domain adaptation in SMT. In addition to the five sy"
S13-2029,J93-1004,0,0.584128,"rget domain or application and to train a dedicated CLWSD system using these particular sense inventories. Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task. The remainder of this paper is organized as follows. Section 2 focuses on the task description and briefly recapitalizes the construction of th"
S13-2029,lefever-hoste-2010-construction,1,0.906382,"ate.google.com 3 http://be.bing.com/translator/ 2 language technology today. It requires the construction of an artificial text understanding as the system should detect the correct word sense based on the context of the word. Different methodologies have been investigated to solve the problem; see for instance Agirre and Edmonds (2006) and Navigli (2009) for a detailed overview of WSD algorithms and evaluation. This paper reports on the second edition of the “Cross-Lingual Word Sense Disambiguation” (CLWSD) task, that builds further on the insights we gained from the SemEval-2010 evaluation (Lefever and Hoste, 2010b) and for which new test data were annotated. The task is an unsupervised Word Sense Disambiguation task for English nouns, the sense label of which is composed of translations in different target languages (viz. French, Italian, Spanish, Dutch and German). The sense inventory is built up on the basis of the Europarl parallel corpus; all translations of a polysemous word were manually grouped into clusters, which constitute different senses of that given word. For the test data, native speakers assigned a translation cluster(s) to each test sentence and gave their top three translations from"
S13-2029,S10-1003,1,0.559637,"ate.google.com 3 http://be.bing.com/translator/ 2 language technology today. It requires the construction of an artificial text understanding as the system should detect the correct word sense based on the context of the word. Different methodologies have been investigated to solve the problem; see for instance Agirre and Edmonds (2006) and Navigli (2009) for a detailed overview of WSD algorithms and evaluation. This paper reports on the second edition of the “Cross-Lingual Word Sense Disambiguation” (CLWSD) task, that builds further on the insights we gained from the SemEval-2010 evaluation (Lefever and Hoste, 2010b) and for which new test data were annotated. The task is an unsupervised Word Sense Disambiguation task for English nouns, the sense label of which is composed of translations in different target languages (viz. French, Italian, Spanish, Dutch and German). The sense inventory is built up on the basis of the Europarl parallel corpus; all translations of a polysemous word were manually grouped into clusters, which constitute different senses of that given word. For the test data, native speakers assigned a translation cluster(s) to each test sentence and gave their top three translations from"
S13-2029,S07-1009,0,0.0335916,"osch, 2005). As most classifiers can be initialized with a wide range of parameters, we used a genetic algorithm to optimize the parameter settings for our classification task. 4 Results 4.1 Experimental set up Test set The lexical sample contains 50 English sentences per ambiguous focus word. All instances were manually annotated per language, which resulted in a set of gold standard translation labels per instance. For the construction of the test dataset, we refer to Section 2. 7 http://code.google.com/apis/language/ Evaluation metric The BEST precision and recall metric was introduced by (McCarthy and Navigli, 2007) in the framework of the SemEval-2007 competition. The metric takes into account the frequency weights of the gold standard translations: translations that were picked by different annotators received a higher associated frequency which is incorporated in the formulas for calculating precision and recall. For the BEST precision and recall evaluation, the system can propose as many guesses as the system believes are correct, but the resulting score is divided by the number of guesses. In this way, systems that output many guesses are not favored and systems can maximize their score by guessing"
S13-2029,P03-1058,0,0.075708,"ion and to train a dedicated CLWSD system using these particular sense inventories. Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task. The remainder of this paper is organized as follows. Section 2 focuses on the task description and briefly recapitalizes the construction of the sense inventory"
S13-2029,S13-2031,0,0.0989072,"Missing"
S13-2029,P07-1006,0,0.0305196,"e inventories. Thirdly, working immediately with translations instead of more abstract sense labels allows to bypass the need to map abstract sense labels to corresponding translations. This makes it easier to integrate a dedicated WSD module into real multilingual applications such as machine translation (Carpuat and Wu, 2007) or information retrieval (Clough and Stevenson, 2004). Many studies have already shown the validity of a cross-lingual approach to Word Sense Disambiguation (Brown et al., 1991; Gale and Church, 1993; Ng et al., 2003; Diab, 2004; Tufis¸ et al., 2004; Chan and Ng, 2005; Specia et al., 2007; Apidianaki, 2009). The Cross-lingual WSD task contributes to this research domain by the construction of a dedicated benchmark data set where the ambiguous words were annotated with the senses from a multilingual sense inventory extracted from a parallel corpus. This benchmark data sets allows a detailed comparison between different approaches to the CLWSD task. The remainder of this paper is organized as follows. Section 2 focuses on the task description and briefly recapitalizes the construction of the sense inventory and the annotation procedure of the test sentences. Section 3 presents t"
S13-2029,S13-2030,0,0.025878,"uggestions. Both evaluation tasks are explained in more detail in Section 4.1. 3 Systems 3.1 Systems participating to the official CLWSD evaluation campaign Five different research teams participated to the CLWSD task and submitted up to three different runs of their system, resulting in 12 different submissions for the task. All systems took part in both the best and the Out-of-five evaluation tasks. These systems took very different approaches to solve the task, ranging from statistical machine translation, classification and sense clustering to topic model based approaches. The XLING team (Tan and Bond, 2013) submitted three runs of their system for all five target languages. The first version of the system presents a 6 http://www.americannationalcorpus.org/ 160 topic matching and translation approach to CLWSD (TnT run), where LDA is applied on the Europarl sentences containing the ambiguous focus word in order to train topic models. Each sentence in the training corpus is assigned a topic that contains a list of associated words with the topic. The topic of the test sentence is then inferred and compared to the matching training sentences by means of the cosine similarity between the training and"
S13-2029,C04-1192,0,0.0826561,"Missing"
S13-2029,S13-2033,0,0.0269961,"Missing"
S14-2005,S13-2029,1,0.709155,"y, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.” Desired output: “I return home because I am tired.” • Input (L1=Dutch, L2=English): “Workers are facing a massive aanval op their employment and social rights.” Desired output: “Workers are facing a massive attack on their employment and social rights.” The task can be related to two tasks that were offered in previous years of SemEval: Lexical Substitution (Mihalcea et al., 2010) and most notably Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2013). When comparing our task to the Cross-Lingual Word-Sense Disambiguation task, one notable difference is the fact that our task concerns not just words, but also phrases. Another essential difference is the nature of the context; our context is in L2 instead of L1. Unlike the Cross-Lingual Word Sense Disambiguation task, we do not constrain the L1 words or phrases that may be used for translation, except for a maximum length which we set to 6 tokens, whereas Lefever and Hoste (2013) only tested a select number of nouns. Our task emphasizes a correct meaning-preserving choice 37 site1 . We crea"
S14-2005,S10-1002,0,0.0325185,"k to L1 in an L2 context. The full L2 sentence acts as reference sentence. Finally, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.” Desired output: “I return home because I am tired.” • Input (L1=Dutch, L2=English): “Workers are facing a massive aanval op their employment and social rights.” Desired output: “Workers are facing a massive attack on their employment and social rights.” The task can be related to two tasks that were offered in previous years of SemEval: Lexical Substitution (Mihalcea et al., 2010) and most notably Cross-lingual Word Sense Disambiguation (Lefever and Hoste, 2013). When comparing our task to the Cross-Lingual Word-Sense Disambiguation task, one notable difference is the fact that our task concerns not just words, but also phrases. Another essential difference is the nature of the context; our context is in L2 instead of L1. Unlike the Cross-Lingual Word Sense Disambiguation task, we do not constrain the L1 words or phrases that may be used for translation, except for a maximum length which we set to 6 tokens, whereas Lefever and Hoste (2013) only tested a select number o"
S14-2005,S14-2030,0,0.0733812,"nd Nanyang Technological University (Singapore) – all language pairs 6. TeamZ - Anubhav Gupta - Universit´e de Franche-Comt´e (France) – English-Spanish, English-German Participants implemented distinct methodologies and implementations. One obvious avenue of tackling the problem is through standard Statistical Machine Translation (SMT). The CNRC team takes a pure SMT approach with few modifications. They employ their own Portage decoder and directly send an L1 fragment in an L2 context, corresponding to a partial translation hypothesis with only one fragment left to decode, to their decoder (Goutte et al., 2014). The UEdin team applies a similar method using the Moses decoder, marking the L2 context so that the decoder leaves this context as is. In addition they add a context similarity feature for every phrase pair in the phrase translation table, which expresses topical similarity with the test context. In order to properly decode, the phrase table is filtered per test sentence (Hasler, 2014). The IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-l"
S14-2005,S14-2110,0,0.0209095,"uesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. CNRC - Cyril Goutte, Michel Simard, Marine Carpuat - National Research Council Canada – All language pairs 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 higher. These scores can be viewed on the result website at http://github.com/proycon/ semeval2014task5/. The result website also holds the system output and evaluation scripts with which all graphs and tables can be reproduced. We observe that the best scoring team in the task (UEdin), as well as the CNRC team, both employ standard Statistical Machine Translation and achieve high results. From this we can conclude that standard SMT techniques are suitable for this task. Teams IUCL and UNAL achieve similarly good results, building on word and phrase alignment data as does SMT, yet not usin"
S14-2005,S14-2060,0,0.037367,"Missing"
S14-2005,S14-2123,0,0.0228464,"tions. They employ their own Portage decoder and directly send an L1 fragment in an L2 context, corresponding to a partial translation hypothesis with only one fragment left to decode, to their decoder (Goutte et al., 2014). The UEdin team applies a similar method using the Moses decoder, marking the L2 context so that the decoder leaves this context as is. In addition they add a context similarity feature for every phrase pair in the phrase translation table, which expresses topical similarity with the test context. In order to properly decode, the phrase table is filtered per test sentence (Hasler, 2014). The IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-linear model: phrase table, L2 Language Model, Multilingual Dictionary, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve t"
S14-2005,S14-2132,0,0.0206542,"IUCL and UNAL teams do make use of the information from word alignments or phrase translation tables, but do not use a standard SMT decoder. The IUCL system combines various information sources in a log-linear model: phrase table, L2 Language Model, Multilingual Dictionary, and a dependency-based collocation model, although this latter source was not finished in time for the system submission (Rudnick et al., 2014). The UNAL system extracts syntactic features as a means to relate L1 fragments with L2 context to their L2 fragment translations, and uses memorybased classifiers to achieve this (Silva-Schlenker et al., 2014). The two systems on the lower end of the result spectrum use different techniques altogether. The Sensible team approaches the problem • Best - The system may only output one, its best, translation; • Out of Five - The system may output up to five alternatives, effectively allowing 5 guesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. C"
S14-2005,P07-2045,0,0.0150004,"Missing"
S14-2005,S14-2094,0,0.0289288,"- The system may output up to five alternatives, effectively allowing 5 guesses. Only the best match is counted. This metric does not count how many of the five are valid. Participants could submit up to three runs per language pair and evaluation type. 5 Participants Six teams submitted systems, three of which participated for all language pairs. In alphabetic order, these are: 1. CNRC - Cyril Goutte, Michel Simard, Marine Carpuat - National Research Council Canada – All language pairs 39 by attempting to emulate the manual post-editing process human translators employ to correct MT output (Tan et al., 2014), whereas TeamZ relies on Wiktionary as the sole source (Gupta, 2014). 6 higher. These scores can be viewed on the result website at http://github.com/proycon/ semeval2014task5/. The result website also holds the system output and evaluation scripts with which all graphs and tables can be reproduced. We observe that the best scoring team in the task (UEdin), as well as the CNRC team, both employ standard Statistical Machine Translation and achieve high results. From this we can conclude that standard SMT techniques are suitable for this task. Teams IUCL and UNAL achieve similarly good results,"
S14-2005,P14-1082,1,0.682767,"Missing"
S14-2005,2005.mtsummit-papers.11,0,0.00919859,"d in a simple XML format that explicitly marks the fragments. System output of participants adheres to the same format. The trial set, released early on in the task, was used by participants to develop and tune their systems on. The test set corresponds to the final data released for the evaluation period; the final evaluation was conducted on this data. The trial data was constructed in an automated fashion in the way described in our pilot study (van Gompel and van den Bosch, 2014). First a phrase-translation table is constructed from a parallel corpus. We used the Europarl parallel corpus (Koehn, 2005) and the Moses tools (Koehn et al., 2007), which in turn makes use of GIZA++ (Och and Ney, 2000). Only strong phrase pairs (exceeding a set threshold) were retained and weaker ones were pruned. This phrase-translation table was then used to create input sentences in which the L2 fragments are swapped for their L1 counterparts, effectively mimicking a fall-back to L1 in an L2 context. The full L2 sentence acts as reference sentence. Finally, to ensure all fragments are correct and sensible, a manual selection from this • Input (L1=French,L2=English): “I rentre a` la maison because I am tired.”"
S14-2070,S13-2053,0,0.0275992,"9 0.8502 (+ 0.0013) 0.8582 (+ 0.0080) 0.8654 (+ 0.0072) 0.8650 (- 0.0004) 0.8660 (+ 0.0010) 0.8660 (=) 0.8654 (- 0.0006) Table 2: F-scores obtained after adding other features for the Twitter and SMS development data (test data SemEval-2013) – subtask A. PMI features: PMI (pointwise mutual information) values indicating the association of a word with positive and negative sentiment. The higher the PMI value, the stronger the word-sentiment association. For each unigram and bigram in the training data, PMI values were extracted from the word-sentiment association lexicon created by NRC Canada (Mohammad et al., 2013). A second PMI feature was considered for each unigram based on the word-sentiment associations found in the SemEval-2014 training dataset. PMI values were calculated as follows: Features lexicons n-grams n-grams + lexicons + normalization n-grams + Part-of-Speech + negation + word shape + named entity + dependency + PMI Dev Twitter 0.5342 0.5896 0.6442 0.6414 (- 0.0028) 0.6466 (+ 0.0052) 0.6542 (+ 0.0076) 0.6581 (+ 0.0039) 0.6559 (- 0.0022) 0.6467 (- 0.0092) 0.6525 (+ 0.0058) Dev SMS 0.5119 0.5628 0.6040 0.6084 (+ 0.0044) 0.6333 (+ 0.0249) 0.6384 (+ 0.0051) 0.6394 (+ 0.0010) 0.6399 (+ 0.0005)"
S14-2070,de-marneffe-etal-2006-generating,0,0.0572289,"Missing"
S14-2070,S13-2052,0,0.0604941,"Missing"
S14-2070,P11-2008,0,0.25856,"Missing"
S14-2070,D11-1141,0,0.0392706,"to Helsinki tomorrow or on the day after tomorrow, yay!). Linguistic Preprocessing First, we performed manual cleaning on the datasets to replace non-UTF-8 characters, and we tokenized all messages using the Carnegie Mellon University Twitter Part-of-Speech Tagger (Gimpel et al., 2011). Subsequently, we Part-of-Speech tagged all instances using the CMU Twitter Partof-Speech Tagger (Gimpel et al., 2011), and performed dependency parsing using a caseless parsing model of the Stanford parser (de Marneffe et al., 2006). Besides that, we also tagged all named entities using the Twitter NLP tools (Ritter et al., 2011) for Named Entity Recognition. As a final preprocessing step, we decided to combine the labels neutral, objective and neutral-OR-objective, thus recasting the task as a three-way classification task. 2.2 (within word tokens) found in the training data. • The number of capitalized words (e.g. SO EXCITED). • The number of hashtags (e.g. #win). Lexicon features: As sentiment lexicons we consulted existing resources: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), MPQA (Wilson et al., 2005), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), Bing Liu (Hu and Liu, 2004)"
S14-2070,P09-2079,0,0.067791,"Missing"
S14-2070,W10-0204,0,0.0630392,"also tagged all named entities using the Twitter NLP tools (Ritter et al., 2011) for Named Entity Recognition. As a final preprocessing step, we decided to combine the labels neutral, objective and neutral-OR-objective, thus recasting the task as a three-way classification task. 2.2 (within word tokens) found in the training data. • The number of capitalized words (e.g. SO EXCITED). • The number of hashtags (e.g. #win). Lexicon features: As sentiment lexicons we consulted existing resources: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), MPQA (Wilson et al., 2005), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), Bing Liu (Hu and Liu, 2004), and Bounce (K¨okciyan et al., 2013) – the latter three are Twitter-specific. Additionally, we created a list of emoticons extracted from the SemEval-2014 training data. Based on these resources, the following features were extracted: Feature Extraction • The number of positive, negative, and neutral lexicon words averaged over text length We implemented a number of lexical and syntactic features that represent every phrase (subtask A) or message (subtask B) within a feature vector: • The overall polarity, which is the sum of the values o"
S14-2070,W11-1709,0,0.312809,"services, social networking sites, and short messaging services have considerably increased the amount of user-generated content produced online. Millions of people rely on these services to send messages, share their views or gather information about others. Simultaneously, companies, marketeers and politicians are anxious to detect sentiment in UGC since these messages might contain valuable information about the public opinion. This explains why sentiment analysis has been a research area of great interest in the last few years (Wiebe et al., 2005; Wilson et al., 2005; Pang and Lee, 2008; Mohammad and Yang, 2011). Though first studies focussed more on product or movie reviews, we see that analyzing sentiment in UGC is currently becoming increasingly popular. The main difference between these two sources of information is that the former is rather long and contains quite formal language whereas the latter one is generally very brief and noisy and thus represents some different challenges (Maynard et al., 2012). In this paper, we describe our contribution to the SemEval-2014 Task 9: Sentiment Analysis in The datasets for training, development and testing were provided by the task organizers. The trainin"
S14-2070,H05-1044,0,\N,Missing
S14-2070,S14-2009,0,\N,Missing
S14-2070,S13-2093,0,\N,Missing
S15-2115,de-marneffe-etal-2006-generating,0,0.00967202,"Missing"
S15-2115,S15-2080,0,0.042618,"Missing"
S15-2115,P11-2008,0,0.04267,"Missing"
S15-2115,maynard-greenwood-2014-cares,0,0.0752971,"Missing"
S15-2115,D11-1141,0,0.055131,"raction. 1 As some tweets were made inaccessible by their creators, we were able to download only 914 of them 685 2.1 Linguistic Preprocessing All tweets were tokenized and PoS-tagged using the Carnegie Mellon University Twitter Part-of-SpeechTagger (Gimpel et al., 2011). Lemmatization was done using the LT3 LeTs Preprocess Toolkit (Van de Kauter et al., 2013). We used a caseless parsing model of the Stanford parser (de Marneffe et al., 2006) for a dependency representation of the messages. As a final step, we tagged all named entities using the Twitter NLP tools for Named Entity Recognition (Ritter et al., 2011). 2.2 Features As a first step, we implemented a set of features that have shown to perform well for sentiment classification in previous research (Van Hee et al., 2014). These include word-based features (e.g. bagof-words), lexical features (e.g. character flooding), sentiment features (e.g. an overall sentiment score per tweet, based on existing sentiment lexicons), and syntactic features (e.g. dependency relation features)2 . To provide some abstraction, we also added PoS n-gram features to the set of bag-of-words features. Nevertheless, as a substantial part of the data we are confronted w"
S15-2115,P14-1024,0,0.0387703,"Missing"
S15-2115,D11-1063,0,0.0543312,"Missing"
S15-2115,S14-2070,1,0.840414,"ntiment (i.e. at least one positive and one negative sentiment word) is contained by the instance. Interjection Count – Numeric feature indicating how many interjections are contained by an instance. This value is normalized by dividing it by the number of tokens in the instance. As stated by (Carvalho et al., 2009), interjections may be potential clues for irony detection. Sarcasm Hashtag – Binary feature indicating whether an instance contains a hashtag that may indicate the presence of sarcasm. To this end, a list of 2 For a detailed description of these features we refer to Van Hee et al. (2014). 3 A number of these features (i.e. contradiction, sudden change, and temporal imbalance) are inspired by Reyes et al. (2013). ≈ 100 sarcasm-related hashtags was extracted from the training data. Punctuation Mark Count – Normalized numeric feature indicating the number of punctuation marks that are contained by an instance. Emoticon count – Normalized numeric feature indicating the number of emoticons that are contained by an instance. Contradiction – Binary feature that indicates whether an instance contains a linguistic contradiction marker (i.e. words like nonetheless, yet, however). Sudde"
S15-2122,J92-4003,0,0.164496,"atures consist of: 1. WordNet features: for each main category, a value is derived indicating the number of (unique) terms annotated as aspect terms from that category in the training data that (1) co-occur in the synset of the candidate term or (2) which are a hyponym/hypernym of a term in the synset. In case the candidate term is a multi-word term whose full term is not found, this value is calculated for all nouns in the multi-word term and the resulting sum is divided by the number of nouns. 2. Cluster features: using the implementation of the Brown hierarchical word clustering algorithm (Brown et al., 1992) by Liang (2005), we derived clusters from the Yelp dataset1 . Then, we derived for each main category a value indicating the number of (unique) terms annotated as aspect terms from that category in the training data that co-occur with the candidate term in the same cluster. Since clusters can only contain single words, we calculate this value for all the nouns in a multi-word term and take the mean of the resulting sum. 3. Linked Open Data (LOD) features: using DBpedia (Lehmann et al., 2013), we included binary values indicating whether a candidate term occurs in one of the following DBpedia"
S15-2122,W11-1902,0,0.0604374,"Missing"
S15-2122,P14-5010,0,0.00564095,"ood of the candidate terms (Kageura and Umino, 1996). To this purpose, we employed Log-likelihood (Rayson and Garside, 2000), C-value (Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including c"
S15-2122,W10-0204,0,0.0225577,"tical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system 720 (Lee et al., 2011). We should also point"
S15-2122,W11-1709,0,0.0123764,"ed using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system 720 (Lee et al., 2011). We should also point out that we only allowed"
S15-2122,W14-1306,0,0.120383,"e annotated with automatically identified <target, aspect category&gt; tuples, the laptop sentences only with the identified aspect categories. In the second phase (Phase B), the gold annotations for the above two datasets, as well as for a hidden domain, were given and the participants had to return the corresponding polarities (positive, negative, neutral). For more information we refer to Pontiki et al. (2015). We tackled the problem by dividing the ABSA task into three incremental subtasks: (i) aspect term extraction, (ii) aspect term classification and (iii) aspect term polarity estimation (Pavlopoulos and Androutsopoulos, 2014). The first two are at the basis of Phase A, whereas the final one constitutes Phase B. For the first step, viz. extracting terms (or targets), we wanted to test our in-house hybrid terminology extraction system (Section 2). Next, we performed a multiclass classification task relying on a feature space containing both lexical and semantic information to aggregate the previously identified terms into the domain-specific and predefined aspects (or aspect categories) (Section 3). Finally, we performed polarity classification by deriving both general and domain-specific lexical features from the r"
S15-2122,S14-2004,0,0.0374204,"t analysis of user-generated content. Until recently, the main research focus has been on discovering the overall polarity of a certain text or phrase. A noticeable shift has occurred to consider a more fine-grained approach, known as aspect-based sentiment analysis (ABSA). For this task the goal is to automatically identify the aspects of given target entities and the sentiment expressed towards each of them. In this paper, we present the LT3 system that participated in this year’s SemEval 2015 ABSA task. Though the focus was on the same domains (restaurants and laptops) as last year’s task (Pontiki et al., 2014), it differed in two ways. This time, entire reviews were to be annotated and for one subtask the systems were confronted with an out-of-domain test set, unknown to the participants. The task ran in two phases. In the first phase (Phase A), the participants were given two test sets (one for the laptops and one for the restaurants domain). The restaurant sentences were to be annotated with automatically identified <target, aspect category&gt; tuples, the laptop sentences only with the identified aspect categories. In the second phase (Phase B), the gold annotations for the above two datasets, as w"
S15-2122,S15-2082,0,0.0932815,"Missing"
S15-2122,W00-0901,0,0.0266193,"tic analysis, TExSIS relies on tokenized, Part-of-Speech tagged, lemmatized and chunked data using the LeTs Preprocess toolkit (Van de Kauter et al., 2013), which is incorporated in the architecture. Subsequently, all words and chunks matching certain Part-of-Speech patterns (i.e. nouns and noun phrases) were considered as candidate terms. In order to determine the specificity of and cohesion between these candidate terms, we combine several statistical filters to represent the termhood and unithood of the candidate terms (Kageura and Umino, 1996). To this purpose, we employed Log-likelihood (Rayson and Garside, 2000), C-value (Frantzi et al., 2000) and termhood (Vintar, 2010). All these statistical filters were calculated using the Web 1T 5-gram corpus (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicon"
S15-2122,S14-2009,0,0.0176511,"41.73 obtained by the winning team). This leads us to conclude that there’s quite some room for improvement for the aggregation phase. Normally, the similarity between terms is first computed after which some sort of clustering is performed 4 Phase B In recent years, sentiment analysis has been a popular research strand. An example is last year’s SemEval task 9 Sentiment Analysis in Twitter, which drew over 45 participants. The competition revealed that the best systems use supervised machine learning techniques and rely much on lexical features in the form of n-grams and sentiment lexicons (Rosenthal et al., 2014). For Phase B, in which we had all gold standard terms and aspect categories available, we decided to extend our LT3 system with another classification round where we classify every aspect as positive, negative or neutral. All features are derived from the sentence in which the terms were found and we participated in all three domains. 4.1 Domain Restaurants Laptops Hotels Feature Extraction We implemented a number of lexical features. First of all, we derived bag-of-words token unigram features. Then, we also generated features using two of the more well-known sentiment lexicons: General Inqu"
S15-2122,S14-2070,1,0.799655,"Missing"
S15-2122,H05-1044,0,0.00818665,"s (Brants and Franz, 2006) as a reference corpus. After a manual inspection of the first output for the training data, we formulated some filtering heuristics. We filter out terms consisting of more than six words, terms that refer to location names or that contain sentiment words. Locations are found using the Stanford CoreNLP toolkit (Manning et al., 2014) and for the sentiment words, we filter those terms occurring in one of the following sentiment lexicons: AFINN (Nielsen, 2011), General Inquirer (Stone et al., 1966), NRC Emotion (Mohammad and Turney, 2010; Mohammad and Yang, 2011), MPQA (Wilson et al., 2005) and Bing Liu (Hu and Liu, 2004). The terms that resulted from this filtered TExSIS output, supplemented with those terms that were annotated in the training data but not recognized by our terminology extraction system, were all considered as candidate terms. Finally, this list of candidate targets was further extended by also including coreferential links as null terms. Coreference resolution of each individual review was performed with the Stanford multi-pass sieve coreference resolution system 720 (Lee et al., 2011). We should also point out that we only allowed terms to be identified in th"
S16-1002,L16-1465,1,0.768602,"Missing"
S16-1002,S15-2080,0,0.0503377,"Missing"
S16-1002,klinger-cimiano-2014-usage,0,0.0621363,"Missing"
S16-1002,P15-2128,0,0.0333833,"Missing"
S16-1002,S16-1003,0,0.0786167,"Missing"
S16-1002,S13-2052,0,0.0105895,"Missing"
S16-1002,piperidis-2012-meta,0,0.0160887,"Missing"
S16-1002,S14-2004,1,0.673256,"Missing"
S16-1002,S15-2082,1,0.813624,"Missing"
S16-1002,S14-2009,0,0.0111835,"Missing"
S16-1002,S15-2078,0,0.0105712,"Missing"
S16-1002,D13-1170,0,0.0173861,"Missing"
S16-1002,E12-2021,0,0.0937892,"Missing"
S18-1005,K16-1017,0,0.0646404,"eficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features are and have been used extensively over the past years, deep learning techniques have recently gained increasing popularity for this task. Such systems often rely on semantic relatedness (i.e. through word and character embeddings (e.g. Amir et al., 2016; Ghosh and Veale, 2016)) deduced by the network and reduce feature engineering efforts. Regardless of the methodology and algorithm used, irony detection often involves binary classification where irony is defined as instances that express the opposite of what is meant (e.g. Riloff et al., 2013; Joshi et al., 2017). Twitter has been a popular data genre for this task, as it is easily accessible and provides a rapid and convenient method to find (potentially) ironic messages by looking for hashtags like #irony, #not and #sarcasm. As a consequence, irony detection research often relies on autom"
S18-1005,D17-1169,0,0.0605263,"nent models. They combined (through averaging) pretrained word and emoji embeddings with handcrafted features, including sentiment contrasts between elements in a tweet (i.e. left vs. right sections, hashtags vs. text, emoji vs. text), sentiment intensity and word-based features like flooding and capitalisation). For Task B, they used a slightly altered (i.e. ensemble LR models and concatenated word embeddings instead of averaged) model. NLPRL-IITBHU (Rangwani et al., 2018) ranked fourth and used an XGBoost Classifier to tackle Task A. They combined pre-trained CNN activations using DeepMoji (Felbo et al., 2017) with ten types of handcrafted features. These were based on polarity contrast information, readability metrics, context incongruity, character flooding, punctuation counts, discourse markers/intensifiers/interjections/swear words counts, general token counts, WordNet similarity, polarity scores and URL counts. The fifth best system for Task A was built by NIHRIO (Vu et al., 2018) and consists of a neural-networks-based architecture (i.e. Multilayer Perceptron). The system exploited lexical (word- and character-level unigrams, bigrams and trigrams), syntactic (PoS-tags with tfidf values), sema"
S18-1005,E14-3007,0,0.0779629,".679 precision 0.583 recall 0.666 F1 0.622 0.651 0.649 0.598 0.603 0.556 0.571 0.584 0.643 0.546 0.544 0.496 0.500 0.450 0.455 0.427 0.897 0.714 0.714 0.781 0.556 0.540 0.408 0.142 0.113 0.618 0.618 0.607 0.527 0.491 0.431 0.213 0.200 Table 5: Best unconstrained systems for Task A. In the top five unconstrained (i.e. using additional training data) systems for Task A are #NonDicevoSulSerio, INAOE-UPV, RM@IT, ValenTO and UTMN, with F1 scores ranging between 0.622 and 0.527. #NonDicevoSulserio extended the training corpus with 3,500 tweets from existing irony corpora (e.g. Riloff et al. (2013); Barbieri and Saggion (2014); Pt´acˇ ek et al. (2014) and built an SVM classifier exploiting structural features (e.g. hashtag count, text length), sentiment- (e.g. contrast between text and emoji sentiment), and emotion-based (i.e. emotion lexicon scores) features. INAOE-UPV combined pretrained word embeddings from the Google News corpus with word-based features (e.g. n-grams). They also extended the official training data with benchmark corpora previously used in irony research and trained their system with a total of 165,000 instances. RM@IT approached the task using an ensemble classifier based on attentionbased recu"
S18-1005,S18-1093,0,0.0266587,"HPCC Random BL LDR ECNU NEUROSENTPDI INAOE-UPV Systems and Results for Task B While 43 teams competed in Task A, 31 teams submitted a system for Task B on multiclass irony classification. Table 6 presents the official ranking with each team’s performance in terms of accuracy, precision, recall and F1 score. Similar to Task A, we discuss the top five systems in the overall ranking (Table 6) and then zoom in on the best performing constrained and unconstrained systems (Tables 7 and 8). For Task B, the top five is nearly similar to the top five for Task A and includes the following teams: UCDCC (Ghosh, 2018), NTUASLP (Baziotis et al., 2018), THU NGN (Wu et al., 2018), NLPRL-IITBHU (Rangwani et al., 2018) and NIHRIO (Vu et al., 2018). All of the teams tackled multiclass irony classification by applying (mostly) the same architecture as for Task A (see earlier). Inspired by siamese networks (Bromley et al., 1993) used in image classification, the UCDCC team developed a siamese architecture for irony detection in both subtasks. The neural network architecture makes use of Glove word acc 0.732 0.652 0.605 0.603 precision 0.577 0.496 0.486 0.466 recall 0.504 0.512 0.541 0.506 F1 0.507 0.496 0.495 0.47"
S18-1005,S15-2080,0,0.541466,"corpus. For both tasks, a training corpus of 3,834 tweets was provided, as well as a test set containing 784 tweets. Our shared tasks received submissions from 43 teams for the binary classification Task A and from 31 teams for the multiclass Task B. The highest classification scores obtained for both subtasks are respectively F1 = 0.71 and F1 = 0.51 and demonstrate that fine-grained irony classification is much more challenging than binary irony detection. 1 Introduction The development of the social web has stimulated the use of figurative and creative language, including irony, in public (Ghosh et al., 2015). From a philosophical/psychological perspective, discerning the mechanisms that underlie ironic speech improves our understanding of human reasoning and communication, and more and more, this interest in understanding irony also emerges in the machine learning community (Wallace, 2015). Although an unanimous definition of irony is still lacking in the literature, it is often identified as a trope whose actual meaning differs from what is literally enunciated. Due to its nature, irony has important implications for natural language processing (NLP) tasks, which aim to understand and produce hu"
S18-1005,W16-0425,0,0.473198,"xt of SemEval (Rosenthal et al., 2017)). However, the SemEval-2014 shared task Sentiment Analysis in Twitter (Rosenthal et al., 2014) demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets. The results revealed that, while sentiment classification performance on regular tweets reached up to F1 = 0.71, scores on the ironic tweets varied between F1 = 0.29 and F1 = 0.57. In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g. Liu, 2012; Maynard and Greenwood, 2014; Ghosh and Veale, 2016). Like other types of figurative language, ironic text should not be interpreted in its literal sense; it requires a more complex understanding based on associations with the context or world knowledge. Examples 1 and 2 are sentences that regular sentiment analysis systems would probably classify as positive, whereas the intended sentiment is undeniably negative. This paper presents the first shared task on irony detection: given a tweet, automatic natural language processing systems should determine whether the tweet is ironic (Task A) and which type of irony (if any) is expressed (Task B). T"
S18-1005,J92-4003,0,0.285384,", context incongruity, character flooding, punctuation counts, discourse markers/intensifiers/interjections/swear words counts, general token counts, WordNet similarity, polarity scores and URL counts. The fifth best system for Task A was built by NIHRIO (Vu et al., 2018) and consists of a neural-networks-based architecture (i.e. Multilayer Perceptron). The system exploited lexical (word- and character-level unigrams, bigrams and trigrams), syntactic (PoS-tags with tfidf values), semantic features (word embeddings using GloVe (Pennington et al., 2014), LSI features and Brown cluster features (Brown et al., 1992)) and polarity features derived from the Hu and Liu Opinion Lexicon (Hu and Liu, 2004). Systems and results for Task A In total, 43 teams competed in Task A on binary irony classification. Table 3 presents each team’s performance in terms of accuracy, precision, recall and F1 score. In all tables, the systems are ranked by the official F1 score (shown in the fifth column). Scores from teams that are marked with an asterisk should be interpreted carefully, as the number of predictions they submitted does not correspond to the number of test instances. As can be observed from the table, the SVM"
S18-1005,P11-2102,0,0.247968,"important to note that by a tweet, we understand the actual text it contains, without metadata (e.g. user id, time stamp, location). Although such metadata could help to recognise irony, the objective of this task is to learn, at message level, how irony is linguistically realised. Previous work on irony detection mostly applied supervised machine learning mainly exploiting lexical features. Other features often include punctuation mark/interjection counts (e.g Davidov et al., 2010), sentiment lexicon scores (e.g. Bouazizi and Ohtsuki, 2016; Far´ıas et al., 2016), emoji (e.g. Gonz´alez-Ib´an˜ ez et al., 2011), writing style, emotional scenarios, part of speechpatterns (e.g. Reyes et al., 2013), and so on. Also beneficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features are and have been used extensively over the past years, deep learning techniques have recently gained increasing popularity for th"
S18-1005,C16-1251,0,0.0155516,"eptron, and an optimised (i.e. using feature selection) voting classifier combining Support Vector Machines with Logistic Regression. Neural networkbased systems exploiting word embeddings derived from the training dataset or generated from Wikipedia corpora perform less well for the task. Similar to Task A, the unconstrained systems do not seem to benefit from additional data, as they do not outperform the constrained submissions for the task. 46 lexical features (e.g. n-grams, punctuation and hashtag counts, emoji presence) and sentimentor emotion- lexicon features (e.g. based on SenticNet (Cambria et al., 2016), VADER (Hutto and Gilbert, 2014), aFinn (Nielsen, 2011)). Also important but to a lesser extent were syntactic (e.g. PoS-patterns) and semantic features, based on word, character and emoji embeddings or semantic clusters. The best systems for Task A and Task B obtained an F1 score of respectively 0.705 and 0.507 and clearly outperformed the baselines provided for this task. When looking at the scores per class label in Task B, we observe that high scores were obtained for the non-ironic and ironic by clash classes, and that other irony appears to be the most challenging irony type. Among all"
S18-1005,S17-2103,0,0.025055,"ne learning mainly exploiting lexical features. Other features often include punctuation mark/interjection counts (e.g Davidov et al., 2010), sentiment lexicon scores (e.g. Bouazizi and Ohtsuki, 2016; Far´ıas et al., 2016), emoji (e.g. Gonz´alez-Ib´an˜ ez et al., 2011), writing style, emotional scenarios, part of speechpatterns (e.g. Reyes et al., 2013), and so on. Also beneficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features are and have been used extensively over the past years, deep learning techniques have recently gained increasing popularity for this task. Such systems often rely on semantic relatedness (i.e. through word and character embeddings (e.g. Amir et al., 2016; Ghosh and Veale, 2016)) deduced by the network and reduce feature engineering efforts. Regardless of the methodology and algorithm used, irony detection often involves binary classification where irony is defined as instances"
S18-1005,J96-2004,0,0.681654,"Missing"
S18-1005,W10-2914,0,0.209203,"challenged to automatically determine whether irony is used and which type of irony is expressed. We thus defined two subtasks: It is important to note that by a tweet, we understand the actual text it contains, without metadata (e.g. user id, time stamp, location). Although such metadata could help to recognise irony, the objective of this task is to learn, at message level, how irony is linguistically realised. Previous work on irony detection mostly applied supervised machine learning mainly exploiting lexical features. Other features often include punctuation mark/interjection counts (e.g Davidov et al., 2010), sentiment lexicon scores (e.g. Bouazizi and Ohtsuki, 2016; Far´ıas et al., 2016), emoji (e.g. Gonz´alez-Ib´an˜ ez et al., 2011), writing style, emotional scenarios, part of speechpatterns (e.g. Reyes et al., 2013), and so on. Also beneficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features a"
S18-1005,E17-2068,0,0.034751,"motion lexicon scores) features. INAOE-UPV combined pretrained word embeddings from the Google News corpus with word-based features (e.g. n-grams). They also extended the official training data with benchmark corpora previously used in irony research and trained their system with a total of 165,000 instances. RM@IT approached the task using an ensemble classifier based on attentionbased recurrent neural networks and the FastTable 3: Official (CodaLab) results for Task A, ranked by F1 score. The highest scores in each column are shown in bold and the baselines are indicated in purple. 44 Text (Joulin et al., 2017) library for learning word representations. They enriched the provided training corpus with, on the one hand, the data sets provided for SemEval-2015 Task 11 (Ghosh et al., 2015) and, on the other hand, the sarcasm corpus composed by Pt´acˇ ek et al. (2014). Altogether, this generated a training corpus of approximately 110,000 tweets. ValenTO took advantage of irony corpora previously used in irony detection that were manually annotated or through crowdsourcing (e.g. Riloff et al., 2013; Pt´acˇ ek et al., 2014). In addition, they extended their corpus with an unspecified number of self-collect"
S18-1005,D14-1162,0,0.0811876,"se were based on polarity contrast information, readability metrics, context incongruity, character flooding, punctuation counts, discourse markers/intensifiers/interjections/swear words counts, general token counts, WordNet similarity, polarity scores and URL counts. The fifth best system for Task A was built by NIHRIO (Vu et al., 2018) and consists of a neural-networks-based architecture (i.e. Multilayer Perceptron). The system exploited lexical (word- and character-level unigrams, bigrams and trigrams), syntactic (PoS-tags with tfidf values), semantic features (word embeddings using GloVe (Pennington et al., 2014), LSI features and Brown cluster features (Brown et al., 1992)) and polarity features derived from the Hu and Liu Opinion Lexicon (Hu and Liu, 2004). Systems and results for Task A In total, 43 teams competed in Task A on binary irony classification. Table 3 presents each team’s performance in terms of accuracy, precision, recall and F1 score. In all tables, the systems are ranked by the official F1 score (shown in the fifth column). Scores from teams that are marked with an asterisk should be interpreted carefully, as the number of predictions they submitted does not correspond to the number"
S18-1005,P15-2106,0,0.177051,"lied supervised machine learning mainly exploiting lexical features. Other features often include punctuation mark/interjection counts (e.g Davidov et al., 2010), sentiment lexicon scores (e.g. Bouazizi and Ohtsuki, 2016; Far´ıas et al., 2016), emoji (e.g. Gonz´alez-Ib´an˜ ez et al., 2011), writing style, emotional scenarios, part of speechpatterns (e.g. Reyes et al., 2013), and so on. Also beneficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features are and have been used extensively over the past years, deep learning techniques have recently gained increasing popularity for this task. Such systems often rely on semantic relatedness (i.e. through word and character embeddings (e.g. Amir et al., 2016; Ghosh and Veale, 2016)) deduced by the network and reduce feature engineering efforts. Regardless of the methodology and algorithm used, irony detection often involves binary classification where irony i"
S18-1005,S17-2004,0,0.0651054,"Missing"
S18-1005,S18-1104,0,0.0944065,"s. WLV (Rohanian et al., 2018) developed an ensemble voting classifier with logistic regression (LR) and a support vector machine (SVM) as component models. They combined (through averaging) pretrained word and emoji embeddings with handcrafted features, including sentiment contrasts between elements in a tweet (i.e. left vs. right sections, hashtags vs. text, emoji vs. text), sentiment intensity and word-based features like flooding and capitalisation). For Task B, they used a slightly altered (i.e. ensemble LR models and concatenated word embeddings instead of averaged) model. NLPRL-IITBHU (Rangwani et al., 2018) ranked fourth and used an XGBoost Classifier to tackle Task A. They combined pre-trained CNN activations using DeepMoji (Felbo et al., 2017) with ten types of handcrafted features. These were based on polarity contrast information, readability metrics, context incongruity, character flooding, punctuation counts, discourse markers/intensifiers/interjections/swear words counts, general token counts, WordNet similarity, polarity scores and URL counts. The fifth best system for Task A was built by NIHRIO (Vu et al., 2018) and consists of a neural-networks-based architecture (i.e. Multilayer Perce"
S18-1005,maynard-greenwood-2014-cares,0,0.123957,"the field (e.g. in the context of SemEval (Rosenthal et al., 2017)). However, the SemEval-2014 shared task Sentiment Analysis in Twitter (Rosenthal et al., 2014) demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets. The results revealed that, while sentiment classification performance on regular tweets reached up to F1 = 0.71, scores on the ironic tweets varied between F1 = 0.29 and F1 = 0.57. In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g. Liu, 2012; Maynard and Greenwood, 2014; Ghosh and Veale, 2016). Like other types of figurative language, ironic text should not be interpreted in its literal sense; it requires a more complex understanding based on associations with the context or world knowledge. Examples 1 and 2 are sentences that regular sentiment analysis systems would probably classify as positive, whereas the intended sentiment is undeniably negative. This paper presents the first shared task on irony detection: given a tweet, automatic natural language processing systems should determine whether the tweet is ironic (Task A) and which type of irony (if any)"
S18-1005,D13-1066,0,0.510167,"Missing"
S18-1005,P16-1104,0,0.0317065,"on mark/interjection counts (e.g Davidov et al., 2010), sentiment lexicon scores (e.g. Bouazizi and Ohtsuki, 2016; Far´ıas et al., 2016), emoji (e.g. Gonz´alez-Ib´an˜ ez et al., 2011), writing style, emotional scenarios, part of speechpatterns (e.g. Reyes et al., 2013), and so on. Also beneficial for this task are combinations of different feature types (e.g. Van Hee et al., 2016b), author information (e.g. Bamman and Smith, 2015), features based on (semantic or factual) oppositions (e.g Karoui et al., 2015; Gupta and Yang, 2017; Van Hee, 2017) and even eye-movement patterns of human readers (Mishra et al., 2016). While a wide range of features are and have been used extensively over the past years, deep learning techniques have recently gained increasing popularity for this task. Such systems often rely on semantic relatedness (i.e. through word and character embeddings (e.g. Amir et al., 2016; Ghosh and Veale, 2016)) deduced by the network and reduce feature engineering efforts. Regardless of the methodology and algorithm used, irony detection often involves binary classification where irony is defined as instances that express the opposite of what is meant (e.g. Riloff et al., 2013; Joshi et al., 2"
S18-1005,S17-1007,0,0.136597,"t al., 2015; Van Hee, 2017). 2 3 • Task A describes a binary irony classification task to define, for a given tweet, whether irony is expressed. • Task B describes a multiclass irony classification task to define whether it contains a specific type of irony (verbal irony by means of a polarity clash, situational irony, or another type of verbal irony, see further) or is not ironic. Concretely, participants should define which one out of four categories a tweet contains: ironic by clash, situational irony, other verbal irony or not ironic. Automatic Irony Detection As described by Joshi et al. (2017), recent approaches to irony can roughly be classified as either rule-based or (supervised and unsupervised) machine learning-based. While rule-based approaches mostly rely upon lexical information and require no training, machine learning invariably makes use of training data and exploits different types of information sources (or features), such as bags of words, syntactic patterns, sentiment information or semantic relatedness. Task Description We propose two subtasks A and B for the automatic detection of irony on Twitter, for which we provide more details below. 3.1 Task A: Binary Irony C"
S18-1005,S18-1090,0,0.0147965,"qual weight in the final score. 2 According to magnitude guidelines by Landis and Koch (1977). 42 For both subtasks, two baselines were provided against which to compare the systems’ performance. The first baseline randomly assigns irony labels and the second one is a linear SVM classifier with standard hyperparameter settings exploiting tf-idf word unigram features (implemented with scikit-learn (Pedregosa et al., 2011)). The second baseline system is made available to the task participants via GitHub3 . 6 ble classifier applied majority voting to combine the outcomes of the two models. WLV (Rohanian et al., 2018) developed an ensemble voting classifier with logistic regression (LR) and a support vector machine (SVM) as component models. They combined (through averaging) pretrained word and emoji embeddings with handcrafted features, including sentiment contrasts between elements in a tweet (i.e. left vs. right sections, hashtags vs. text, emoji vs. text), sentiment intensity and word-based features like flooding and capitalisation). For Task B, they used a slightly altered (i.e. ensemble LR models and concatenated word embeddings instead of averaged) model. NLPRL-IITBHU (Rangwani et al., 2018) ranked"
S18-1005,S16-1001,0,0.0419889,"ly 110,000 tweets. ValenTO took advantage of irony corpora previously used in irony detection that were manually annotated or through crowdsourcing (e.g. Riloff et al., 2013; Pt´acˇ ek et al., 2014). In addition, they extended their corpus with an unspecified number of self-collected irony tweets using the hashtags #irony and #sarcasm. Finally, UTMN developed an SVM classifier exploiting binary bag-of-words features. They enriched the training set with 1,000 humorous tweets from SemEval-2017 Task 6 (Potash et al., 2017) and another 1,000 tweets with positive polarity from SemEval-2016 Task 4 (Nakov et al., 2016), resulting in a training corpus of 5,834 tweets. Interestingly, when comparing the best constrained with the best unconstrained system for Task A, we see a difference of 10 points in favour of the constrained system, which indicates that adding more training data does not necessarily improve the classification performance. 7 embeddings as features and creates two identical subnetworks that are each fed with different parts of a tweet. Under the premise that ironic statements are often characterised by a form of opposition or contrast, the architecture captures this incongruity between two par"
S18-1005,S17-2088,0,0.140726,"ts Cynthia Van Hee, Els Lefever and V´eronique Hoste LT3 Language and Translation Technology Team Ghent University Groot-Brittanni¨elaan 45, 9000 Ghent firstname.lastname@ugent.be Abstract detection has a large potential for various applications in the domain of text mining, especially those that require semantic analysis, such as author profiling, detecting online harassment, and, maybe the most well-known example, sentiment analysis. Due to its importance in industry, sentiment analysis research is abundant and significant progress has been made in the field (e.g. in the context of SemEval (Rosenthal et al., 2017)). However, the SemEval-2014 shared task Sentiment Analysis in Twitter (Rosenthal et al., 2014) demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets. The results revealed that, while sentiment classification performance on regular tweets reached up to F1 = 0.71, scores on the ironic tweets varied between F1 = 0.29 and F1 = 0.57. In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g. Liu, 2012; Maynard and Greenwood, 2014; Ghosh and Veale, 2016). Like other typ"
S18-1005,S14-2009,0,0.020285,"am Ghent University Groot-Brittanni¨elaan 45, 9000 Ghent firstname.lastname@ugent.be Abstract detection has a large potential for various applications in the domain of text mining, especially those that require semantic analysis, such as author profiling, detecting online harassment, and, maybe the most well-known example, sentiment analysis. Due to its importance in industry, sentiment analysis research is abundant and significant progress has been made in the field (e.g. in the context of SemEval (Rosenthal et al., 2017)). However, the SemEval-2014 shared task Sentiment Analysis in Twitter (Rosenthal et al., 2014) demonstrated the impact of irony on automatic sentiment classification by including a test set of ironic tweets. The results revealed that, while sentiment classification performance on regular tweets reached up to F1 = 0.71, scores on the ironic tweets varied between F1 = 0.29 and F1 = 0.57. In fact, it has been demonstrated that several applications struggle to maintain high performance when applied to ironic text (e.g. Liu, 2012; Maynard and Greenwood, 2014; Ghosh and Veale, 2016). Like other types of figurative language, ironic text should not be interpreted in its literal sense; it requi"
S18-1005,E12-2021,0,0.114923,"Missing"
S18-1005,C16-1257,1,0.924617,"Missing"
S18-1005,S18-1085,0,0.0496312,"Missing"
S18-1005,S18-1006,0,0.159345,"Missing"
S18-1016,H05-1073,0,0.207504,"Missing"
S18-1016,W10-0513,0,0.0182955,"and semantic features (see Table 4). Regarding the latter category, both features from traditional and distributional semantics were integrated. We first took the synset depth (distance to root) of all content words (calculated with WordNet (Miller, 1995)) and averaged the scores to get a mean synset depth for the tweet. Furthermore, we included two types of features from distributional semantics, namely word embeddings and word clusters. The word embeddings were extracted with Weka Affective Tweets, using pre-trained embeddings from 10 million tweets taken from the Edinburgh Twitter Corpus (Petrovic et al., 2010). For the word clusters, we downloaded a subset of around 1.5M tweets from the SemEval 2018 AIT DISC corpus (Mohammad et al., 2018). We first created word embeddings with word2vec using both skipgram and continuous bow and afterwards applied k-means clustering on the resulting word vectors. We experimented with various cluster sizes (800 of size 100, 1000 of size 100 and 800 of size 300). These clusters were implemented as binary features. 4 4.1 Type polarity polarity polarity polarity polarity polarity + Plutchik emotions polarity + Plutchik emotions Plutchik emotions polarity polarity polari"
S18-1016,S07-1013,0,0.173012,"Missing"
S18-1016,S17-1007,0,0.0227125,"Missing"
S18-1016,S12-1033,0,0.0609257,"Missing"
S18-1016,W17-5205,0,0.0368894,"Missing"
S18-1016,S18-1001,0,0.03485,"Missing"
S18-1016,L18-1030,0,0.064694,"2007), chat messages (e.g. Holzman and Pottenger, 2003; Brooks et al., 2013), and tweets (e.g. Mohammad, 2012; Wang et al., 2012). The big advantage of using tweet datasets is the relative ease with which twitter data can be collected and the possibility of using hashtags as emotion labels (distant supervision approach). For this paper, we used the data that was collected for the SemEval shared task on Affect in Tweets (Mohammad et al., 2018), a collection of tweets annotated for eleven emotions: anger, anticipation, disgust, fear, joy, love, optimism, pessimism, sadness, surprise, and trust (Mohammad and Kiritchenko, 2018). We participated in Subtask 5: Detecting Emotions (English emotion classification). The remainder of this paper is structured as follows: in Section 2 we describe how we first analyzed the data in order to get more insight in the task. Section 3 discusses how the data was preprocessed and which information sources were extracted. Next, in Section 4 the actual experimental setup and results are discussed and we end this paper with a conclusion in Section 5. This paper presents an emotion classification system for English tweets, submitted for the SemEval shared task on Affect in Tweets, subtas"
S19-2077,S19-2007,0,0.039027,"ual Detection of Hate Speech Against Immigrants and Women in Twitter (hatEval) Nina Bauwelinck, Gilles Jacobs, V´eronique Hoste and Els Lefever LT3, Language and Translation Technology Team Department of Translation, Interpreting and Communication – Ghent University Groot-Brittanni¨elaan 45, 9000 Ghent, Belgium firstname.lastname@ugent.be, gillesm.jacobs@ugent.be Abstract standard machine learning approach or deep learning methods (Pitsilis et al., 2018). This was also the approach we took for our hate speech detection system. We participated for both subtasks proposed for English for Task 5 (Basile et al., 2019), being TASK A, which was defined as a binary classification task where systems have to predict whether a tweet with a given target (women or immigrants) is hateful or not hateful, and TASK B, where systems are asked first to classify hateful tweets as aggressive or not aggressive, and second to identify the target harassed as individual or generic (i.e. single human or group). This paper describes our contribution to the SemEval-2019 Task 5 on the detection of hate speech against immigrants and women in Twitter (hatEval). We considered a supervised classification-based approach to detect hate"
S19-2077,H05-1044,0,0.0633562,"and non-linguistic features included. This featurization pipeline is based on work in cyberbullying detection and analysis (Van Hee et al., 2018). The whole set of features listed below was used to build all three classifiers. We did not apply any feature selection. Bag-of-words features: We included binary token unigrams, bigrams and trigrams, along with character trigrams and fourgrams. The latter provide robustness to the spelling variation typically found in social media. Lexicon features: We computed positive and negative opinion word ratio and overall post sentiment using both the MPQA (Wilson et al., 2005) and Hu and Liu’s (Hu and Liu, 2004) opinion lexicons. We added positive, negative and neutral emoji counts based on the BOUNCE emoji sentiment lexicon (K¨okciyan et al., 2013). We also included the relative frequency of all 64 psychometric categories in the Linguistic Inquiry and Word Count (LIWC) dictionary (Pennebaker et al., 2007). Furthermore, we included diminisher, intensifier, negation, and “allness” lexicons which relate to a negative mindset in the context of suicidality research (Osgood and Walker, 1959; Gottschalk and Gleser, 1960; Shapero, 2011) as well as a proper name gazetteer."
schuurman-etal-2010-interacting,desmet-hoste-2010-towards,1,\N,Missing
schuurman-etal-2010-interacting,schuurman-2008-spatiotemporal,1,\N,Missing
schuurman-etal-2010-interacting,markert-nissim-2002-towards,0,\N,Missing
schuurman-etal-2010-interacting,reynaert-etal-2010-balancing,0,\N,Missing
schuurman-etal-2010-interacting,van-noord-etal-2006-syntactic,1,\N,Missing
schuurman-etal-2010-interacting,M95-1005,0,\N,Missing
schuurman-etal-2010-interacting,W07-1513,1,\N,Missing
schuurman-etal-2010-interacting,W02-2025,0,\N,Missing
schuurman-etal-2010-interacting,J96-2004,0,\N,Missing
schuurman-etal-2010-interacting,R09-1041,0,\N,Missing
schuurman-etal-2010-interacting,doddington-etal-2004-automatic,0,\N,Missing
schuurman-etal-2010-interacting,schuurman-vandeghinste-2010-cultural,1,\N,Missing
van-oosten-etal-2010-towards,J93-2004,0,\N,Missing
vanopstal-etal-2010-towards,W01-0516,0,\N,Missing
vanopstal-etal-2010-towards,W02-0312,0,\N,Missing
vanopstal-etal-2010-towards,J96-2004,0,\N,Missing
vanopstal-etal-2010-towards,P03-1004,0,\N,Missing
W02-0809,S01-1003,1,\N,Missing
W02-0809,J01-3001,0,\N,Missing
W02-0809,P97-1056,1,\N,Missing
W02-0809,W02-0814,1,\N,Missing
W02-0809,P96-1006,0,\N,Missing
W02-0814,P99-1037,1,0.884605,"Missing"
W02-0814,P01-1005,0,0.0641157,"Missing"
W02-0814,W96-0102,1,0.832954,"Missing"
W02-0814,S01-1020,1,0.844583,"Missing"
W02-0814,J98-1001,0,0.0345256,"Missing"
W02-0814,P96-1006,0,0.0730626,"polysemy and sense distributions. Iris Hendrickx and Antal van den Bosch ILK Computational Linguistics Tilburg University, The Netherlands I.H.E.Hendrickx,antalb @kub.nl  1 Introduction The task of word sense disambiguation (WSD) is to assign a sense label to a word in context. Both knowledge-based and statistical methods have been applied to the problem. See (Ide and V´eronis, 1998) for an introduction to the area. Recently (both S ENSEVAL competitions), various machine learning (ML) approaches have been demonstrated to produce relatively successful WSD systems, e.g. memory-based learning (Ng and Lee, 1996; Veenstra et al., 2000), decision lists (Yarowsky, 2000), boosting (Escudero et al., 2000). In this paper, we evaluate the results of a memorybased learning approach to WSD. We ask ourselves whether we can learn lessons from the errors made in the S ENSEVAL -2 competition. More particularly, we are interested whether there are words or categories of words which are more difficult to predict than other words. If so, do these words have certain characteristic features? We furthermore investigate the interaction between the use of different information sources and the part-of-speech categories o"
W02-0814,J01-3001,0,0.0788235,"Missing"
W02-0814,1995.iwpt-1.8,0,\N,Missing
W02-0814,J95-4002,0,\N,Missing
W02-0814,C00-2098,0,\N,Missing
W02-0814,H94-1020,0,\N,Missing
W02-0814,P00-1058,0,\N,Missing
W02-0814,P90-1035,0,\N,Missing
W02-0814,P83-1017,0,\N,Missing
W02-0814,P98-2156,0,\N,Missing
W02-0814,C98-2151,0,\N,Missing
W04-0827,C02-1039,0,0.0184143,"and value weighting methods, different neighborhood size and weighting parameters, etc., that should be optimized for each word expert independently. See (Daelemans et al., 2003b) for more information. It has been claimed, e.g. in (Daelemans et al., 1999), that lazy learning has the right bias for learning natural language processing tasks as it makes possible learning from atypical and low-frequency events that are usually discarded by eager learning methods. Architecture. Previous work on memory-based WSD includes work from Ng and Lee (1996), Veenstra et al. (2000), Hoste et al. (2002) and Mihalcea (2002). The current design of our WSD system is largely based on Hoste et al. (2002). Figure 1 gives an overview of the design of our WSD system: the training text is first linguistically analyzed. For each word-lemma–POS-tag combination, we check if it (i) is in our sense lexicon, (ii) has more than one sense and (iii) has a frequency in the training text above a certain threshold. For all combinations matching these three conditions, we train a word expert module. To all combinations with only one sense, or with more senses and a frequency below the threshold, we assign the default sense, which is"
W04-0827,P96-1006,0,0.452921,"ice between different statistical and information-theoretic feature and value weighting methods, different neighborhood size and weighting parameters, etc., that should be optimized for each word expert independently. See (Daelemans et al., 2003b) for more information. It has been claimed, e.g. in (Daelemans et al., 1999), that lazy learning has the right bias for learning natural language processing tasks as it makes possible learning from atypical and low-frequency events that are usually discarded by eager learning methods. Architecture. Previous work on memory-based WSD includes work from Ng and Lee (1996), Veenstra et al. (2000), Hoste et al. (2002) and Mihalcea (2002). The current design of our WSD system is largely based on Hoste et al. (2002). Figure 1 gives an overview of the design of our WSD system: the training text is first linguistically analyzed. For each word-lemma–POS-tag combination, we check if it (i) is in our sense lexicon, (ii) has more than one sense and (iii) has a frequency in the training text above a certain threshold. For all combinations matching these three conditions, we train a word expert module. To all combinations with only one sense, or with more senses and a fre"
W09-2413,D07-1007,0,0.180514,"d. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. Although WSD has long time been studied as a stand-alone NLP task, there is a growing feeling in the WSD community that WSD should preferably be integrated in real applications such as Machine Translation or multilingual information retrieval (Agirre and Edmonds, 2006). Several studies have demonstrated that for instance Statistical Machine Translation (SMT) benefits from incorporating a dedicated WSD module (Chan et al., 2007; Carpuat and Wu, 2007). Using translations from a corpus instead of human-defined sense labels is one way of facilitating the integration of WSD in multilingual applications. It also implic1 http://www.senseval.org/ Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 82–87, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics itly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the translations. Furthermore, this type of corpus-based approach is languageindependent"
W09-2413,P07-1005,0,0.0576053,"d expensive to build. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. Although WSD has long time been studied as a stand-alone NLP task, there is a growing feeling in the WSD community that WSD should preferably be integrated in real applications such as Machine Translation or multilingual information retrieval (Agirre and Edmonds, 2006). Several studies have demonstrated that for instance Statistical Machine Translation (SMT) benefits from incorporating a dedicated WSD module (Chan et al., 2007; Carpuat and Wu, 2007). Using translations from a corpus instead of human-defined sense labels is one way of facilitating the integration of WSD in multilingual applications. It also implic1 http://www.senseval.org/ Proceedings of the NAACL HLT Workshop on Semantic Evaluations: Recent Achievements and Future Directions, pages 82–87, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics itly deals with the granularity problem as finer sense distinctions are only relevant as far as they are lexicalized in the translations. Furthermore, this type of corpus-based approach"
W09-2413,P02-1033,0,0.0423092,"e can infer that the English noun “bill” has at most four different senses. These different senses in turn can be grouped in case of synonymy. In the Dutch-French Europarl, for example, both “rekening” and “kosten”, are translated by the French “frais”, which might indicate that both Dutch words are synonymous. Several WSD studies are based on the idea of cross-lingual evidence. Gale et al. (1993) use a bilingual parallel corpus for the automatic creation of a sense-tagged data set, where target words in the source language are tagged with their translation of the word in the target language. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. Ide et al. (2002) use a multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al., 2003). The 83 propo"
W09-2413,P91-1023,0,0.129323,"e´ tablissement de cr´edit, Bank/ Kreditinstitut, banca, banco Table 2: Example of multi-lingual sense labels for the English noun bank 2.1 Corpus and word selection The document collection which serves as the basis for the gold standard construction and system evaluation is the Europarl parallel corpus2 , which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus: English (our target language), Dutch, French, German, Italian and Spanish. All sentences are aligned using a tool based on the Gale and Church (1991) algorithm. We only consider the 1-1 sentence alignments between English and the five other languages (see also Tufis et al. (2004) for a similar strategy). These 1-1 alignments will be made available to all task participants. Participants are free to use other training corpora, but additional translations which are not present in Europarl will not be included in the sense inventory that is used for evaluation. language. The manual construction of the sense inventory will be discussed in Section 2.2. The test data contains 50 instances for 20 nouns from the test data as used in the Cross-Lingu"
W09-2413,P92-1032,0,0.142822,"Senseval1 and its successor Semeval revealed that supervised approaches to WSD usually achieve better results than unsupervised methods (M`arquez et al., 2006). The former use machine learning techniques to induce a classifier from manually sense-tagged data, where each occurrence of a polysemous word gets assigned a sense label from a predefined sense inventory such as WordNet (Fellbaum, 1998). These supervised methods, however, heavily rely on large sensetagged corpora which are very time consuming and expensive to build. This phenomenon, well known as the knowledge acquisition bottleneck (Gale et al., 1992), explains the modest use and success of supervised WSD in real applications. Although WSD has long time been studied as a stand-alone NLP task, there is a growing feeling in the WSD community that WSD should preferably be integrated in real applications such as Machine Translation or multilingual information retrieval (Agirre and Edmonds, 2006). Several studies have demonstrated that for instance Statistical Machine Translation (SMT) benefits from incorporating a dedicated WSD module (Chan et al., 2007; Carpuat and Wu, 2007). Using translations from a corpus instead of human-defined sense lab"
W09-2413,W02-0808,0,0.440967,"ated by the French “frais”, which might indicate that both Dutch words are synonymous. Several WSD studies are based on the idea of cross-lingual evidence. Gale et al. (1993) use a bilingual parallel corpus for the automatic creation of a sense-tagged data set, where target words in the source language are tagged with their translation of the word in the target language. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. Ide et al. (2002) use a multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al., 2003). The 83 proposed Cross-lingual Word Sense Disambiguation task differs from earlier work (e.g. Ide et al. (2002)) through its independence from an externally defined sense set. The remainder of this paper is organized as follows. In Se"
W09-2413,2005.mtsummit-papers.11,0,0.0404393,"o people living on the [bank] of the river Language NL,F,D,I,ES Sense label oever/dijk, rives/rivage/bord/bords, Ufer, riva, orilla The [bank] of Scotland ... Language NL,F,D,I,ES Sense label bank/kredietinstelling, banque/ e´ tablissement de cr´edit, Bank/ Kreditinstitut, banca, banco Table 2: Example of multi-lingual sense labels for the English noun bank 2.1 Corpus and word selection The document collection which serves as the basis for the gold standard construction and system evaluation is the Europarl parallel corpus2 , which is extracted from the proceedings of the European Parliament (Koehn, 2005). We selected 6 languages from the 11 European languages represented in the corpus: English (our target language), Dutch, French, German, Italian and Spanish. All sentences are aligned using a tool based on the Gale and Church (1991) algorithm. We only consider the 1-1 sentence alignments between English and the five other languages (see also Tufis et al. (2004) for a similar strategy). These 1-1 alignments will be made available to all task participants. Participants are free to use other training corpora, but additional translations which are not present in Europarl will not be included in t"
W09-2413,S07-1009,0,0.0616228,"in a trilingual setting. 3 System evaluation As stated before, systems can participate in two tasks, i.e. systems can either participate in one or more bilingual evaluation tasks or they can participate in the multilingual evaluation task incorporating the five supported languages. The evaluation of the multilingual evaluation task is simply the average of the system scores on the five bilingual evaluation tasks. 3.1 Evaluation strategies For the evaluation of the participating systems we will use an evaluation scheme which is inspired by the English lexical substitution task in SemEval 2007 (McCarthy and Navigli, 2007). The evaluation will be performed using precision and recall (P and R in the equations that follow). We perform both a best result evaluation and a more relaxed evaluation for the top five results. Let H be the set of annotators, T be the set of test items and hi be the set of responses for an item i ∈ T for annotator h ∈ H. Let A be the set of items from T where the system provides at least one answer and ai : i ∈ A be the set of guesses from the system for item i. For each i, we calculate the multiset union (Hi ) for all hi for all h ∈ H and for each unique type (res) in Hi that has an asso"
W09-2413,P03-1058,0,0.23805,"nguage. Diab and Resnik (2002) present an unsupervised approach to WSD that exploits translational correspondences in parallel corpora that were artificially created by applying commercial MT systems on a sense-tagged English corpus. Ide et al. (2002) use a multilingual parallel corpus (containing seven languages from four language families) and show that sense distinctions derived from translation equivalents are at least as reliable as those made by human annotators. Moreover, some studies present multilingual WSD systems that attain state-of-the-art performance in all-words disambiguation (Ng et al., 2003). The 83 proposed Cross-lingual Word Sense Disambiguation task differs from earlier work (e.g. Ide et al. (2002)) through its independence from an externally defined sense set. The remainder of this paper is organized as follows. In Section 2, we present a detailed description of the cross-lingual WSD task. It introduces the parallel corpus we used, informs on the development and test data and discusses the annotation procedure. Section 3 gives an overview of the different scoring strategies that will be applied. Section 4 concludes this paper. 2 Task set up The cross-lingual Word Sense Disamb"
W09-2413,J03-1002,0,0.00241474,"lish target word. They are not bound to a predefined parallel corpus, but can freely choose the translations from any available resource. Selecting the target words from the set of nouns thats will be used for the Lexical Substitution Task should make it easier for systems to participate in both tasks. 2.2 The sense inventory for the 5 target nouns in the development data and the 20 nouns in the test data is manually built up in three steps. 1. In the first annotation step, the 5 translations of the English word are identified per sentence ID. In order to speed up this identification, GIZA++ (Och and Ney, 2003) is used to generate the initial word alignments for the 5 languages. All word alignments are manually verified. In this step, we might come across multiword translations, especially in Dutch and German which tend to glue parts of compounds together in one orthographic unit. We decided to keep these translations as such, even if they do not correspond exactly to the English target word. In following sentence, the Dutch translation witboek corresponds in fact to the English compound white paper, and not to the English target word paper: English: the European Commission presented its white paper"
W09-2413,C04-1192,0,0.30313,"Missing"
W09-2413,J93-1004,0,\N,Missing
W09-2413,lefever-hoste-2010-construction,1,\N,Missing
W09-2413,S07-1001,0,\N,Missing
W09-2413,E09-1010,0,\N,Missing
W09-2413,W09-2412,0,\N,Missing
W09-2413,P91-1034,0,\N,Missing
W09-2413,P07-1006,0,\N,Missing
W11-1006,D07-1090,0,0.0109685,"T, Google (that disposes of large computing clusters and a network of data centers for Web search) has very valuable assets at its disposal for this task. We can only speculate about the amount of resources that Google uses to train its translation engine. Part of the training data comes from transcripts of United Nations meetings (in six official languages) and those of the European Parliament (Europarl corpus). Google research papers report on a distributed infrastructure that is used to train on up to two trillion tokens, which result in language models containing up to 300 billion ngrams (Brants et al., 2007). 3 ParaSense This section describes the ParaSense WSD system: a multilingual classification-based approach to Word Sense Disambiguation. Instead of using a predefined monolingual sense-inventory such as WordNet, we use a language-independent framework where the word senses are derived automatically from word alignments on a parallel corpus. We used the sentence-aligned Europarl corpus (Koehn, 2005) for the construction of our WSD module. The following six languages were selected: English (our focus language), Dutch, French, German, Italian and Spanish. We only considered the 1-1 sentence alig"
W11-1006,D07-1007,0,0.0253052,"uning problems of this dedicated WSD feature. Specia (2006) used an inductive logic programming-based WSD system which was tested on seven ambiguous verbs in English-Portuguese translation. The latter systems already present promising results for the use of WSD in MT, but really significant improvements in terms of general machine translation qualProceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 52–60, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics ity were for the first time obtained by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Stroppa et al. (2007) take a completely different approach to perform some sort of implicit Word Sense Disambiguation in MT. They introduce context-information features that exploit source similarity, in addition to target similarity that is modeled by the language model, in an SMT framework. For the estimation of these features that are very similar to"
W11-1006,P07-1005,0,0.0271417,"icated WSD feature. Specia (2006) used an inductive logic programming-based WSD system which was tested on seven ambiguous verbs in English-Portuguese translation. The latter systems already present promising results for the use of WSD in MT, but really significant improvements in terms of general machine translation qualProceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 52–60, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics ity were for the first time obtained by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Stroppa et al. (2007) take a completely different approach to perform some sort of implicit Word Sense Disambiguation in MT. They introduce context-information features that exploit source similarity, in addition to target similarity that is modeled by the language model, in an SMT framework. For the estimation of these features that are very similar to the typical WSD local"
W11-1006,P02-1033,0,0.0357851,"subtle change compared to the reference sentence, but it often drastically affects the global understanding of the sentence. Secondly, we explore the potential benefits of a real multilingual approach to WSD. The idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of a polysemous word are often lexicalized across languages (Resnik and Yarowsky, 2000). Many WSD studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual WSD classifiers (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002) or 53 systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). Our WSD system is different in the sense that it is independent from a predefined sense-inventory (it only uses the parallel corpus at hand) and that it is truly multilingual as it incorporates information from four other languages (French, Dutch, Spanish, Italian and German depending on the target language of the classifier). Although our classifiers are still very preliminary in terms of the feature set and parameters that are used, we obtain interesting results on our test sample of"
W11-1006,J93-1004,0,0.0833292,"slation of an ambiguous word might be a subtle change compared to the reference sentence, but it often drastically affects the global understanding of the sentence. Secondly, we explore the potential benefits of a real multilingual approach to WSD. The idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of a polysemous word are often lexicalized across languages (Resnik and Yarowsky, 2000). Many WSD studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual WSD classifiers (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002) or 53 systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). Our WSD system is different in the sense that it is independent from a predefined sense-inventory (it only uses the parallel corpus at hand) and that it is truly multilingual as it incorporates information from four other languages (French, Dutch, Spanish, Italian and German depending on the target language of the classifier). Although our classifiers are still very preliminary in terms of the feature set and parameters that are used, we obtain i"
W11-1006,P05-1050,0,0.012163,"• local context features related to a window of three words preceding and following the focus word containing for each of these words their full form, lemma, Part-of-Speech and chunk information These local context features are to be considered as a basic feature set. The Senseval evaluation exercises have shown that feeding additional information sources to the classifier results in better system performance (Agirre and Martinez, 2004). In future experiments we plan to integrate a.o. lemma information on the surrounding content words and semantic analysis (e.g. Singular Value Decomposition (Gliozzo et al., 2005)) in order to detect latent correlations between terms. 3.2.2 Translation Features In addition to the commonly deployed local context features, we also extracted a set of binary bagof-words features from the aligned translations that are not the target language of the classifier (e.g. for the French classifier, we extract bag-of-words features from the Italian, Spanish, Dutch and German aligned translations). We preprocessed all aligned translations by means of the Treetagger tool (Schmid, 1994) that outputs Part-of-Speech and lemma information. Per ambiguous focus word, a list of all content"
W11-1006,N03-1017,0,0.00895131,"stical Machine Translation Systems For our experiments, we analyzed the behavior of two phrase-based statistical machine translation (SMT) systems on the translation of ambiguous nouns. SMT generates translations on the basis of statistical models whose parameters are derived from the analysis of sentence-aligned parallel text corpora. Phrase-based SMT is considered as the dominant paradigm in MT research today. It combines a phrase translation model (which is based on the noisy channel model) and a phrase-based decoder in order to find the most probable translation e of a foreign sentence f (Koehn et al., 2003). Usually the Bayes rule is used to reformulate this translation probability: argmaxe p(e|f ) = argmaxe p(f |e)p(e) This allows for a language model p(e) that guarantees the fluency and grammatical correctness of the translation, and a separate translation model p(f |e) that focusses on the quality of the translation. Training of both the language model (on monolingual data) as well as the translation model (on bilingual text corpora) requires large amounts of text data. Research has pointed out that adding more training data, both for the translation as for the language models, results in bet"
W11-1006,P07-2045,0,0.00596872,"(Callison-Burch et al., 2009). Therefore it is important to notice that our comparison of the two SMT systems is somewhat unfair, as we compared the Moses research system (that was trained on the Europarl corpus) with the Google commercial system that is trained on a much larger data set. It remains an interesting exercise though, as we consider the commercial system as the upper bound of how far current SMT can get in case it has unlimited access to text corpora and computational resources. 2.1 Moses The first statistical machine translation system we used is the off-the-shelf Moses toolkit (Koehn et al., 2007). As the Moses system is open-source, well documented, supported by a very lively users forum and reaches state-of-the-art performance, it has quickly been adopted by the community and highly stimulated development in the SMT field. It also features factored translation models, which enable the integration of linguistic and other information at the word level. This makes Moses a good candidate to experiment with for example a dedicated WSD module, that requires more enhanced linguistic information (such as lemmas and Part-of-Speech tags). We trained Moses for English–French and English– Dutch"
W11-1006,2005.mtsummit-papers.11,0,0.0320149,"rpus). Google research papers report on a distributed infrastructure that is used to train on up to two trillion tokens, which result in language models containing up to 300 billion ngrams (Brants et al., 2007). 3 ParaSense This section describes the ParaSense WSD system: a multilingual classification-based approach to Word Sense Disambiguation. Instead of using a predefined monolingual sense-inventory such as WordNet, we use a language-independent framework where the word senses are derived automatically from word alignments on a parallel corpus. We used the sentence-aligned Europarl corpus (Koehn, 2005) for the construction of our WSD module. The following six languages were selected: English (our focus language), Dutch, French, German, Italian and Spanish. We only considered the 1-1 sentence alignments between English and the five other languages. This way we obtained a six-lingual sentence-aligned subcorpus of Europarl, that contains 884.603 sentences per language. For our experiments we used the lexical sample of twenty ambiguous nouns that was also used in the SemEval-2010 ”Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b), which consists in assigning a cor"
W11-1006,lefever-hoste-2010-construction,1,0.866505,". We used the sentence-aligned Europarl corpus (Koehn, 2005) for the construction of our WSD module. The following six languages were selected: English (our focus language), Dutch, French, German, Italian and Spanish. We only considered the 1-1 sentence alignments between English and the five other languages. This way we obtained a six-lingual sentence-aligned subcorpus of Europarl, that contains 884.603 sentences per language. For our experiments we used the lexical sample of twenty ambiguous nouns that was also used in the SemEval-2010 ”Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b), which consists in assigning a correct translation in five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect all relevant translations for the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings on our parallel corpus. The obtained word alignment output was then considered to be the classification label for the training instances for a given classifier (e.g. the French translation resulting from the word alignment is the label that is used to train the French c"
W11-1006,S10-1003,1,0.866454,". We used the sentence-aligned Europarl corpus (Koehn, 2005) for the construction of our WSD module. The following six languages were selected: English (our focus language), Dutch, French, German, Italian and Spanish. We only considered the 1-1 sentence alignments between English and the five other languages. This way we obtained a six-lingual sentence-aligned subcorpus of Europarl, that contains 884.603 sentences per language. For our experiments we used the lexical sample of twenty ambiguous nouns that was also used in the SemEval-2010 ”Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b), which consists in assigning a correct translation in five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect all relevant translations for the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings on our parallel corpus. The obtained word alignment output was then considered to be the classification label for the training instances for a given classifier (e.g. the French translation resulting from the word alignment is the label that is used to train the French c"
W11-1006,P10-1023,0,0.0130162,"ere we only have the English test sentences at our disposal), we had to adopt a different strategy. We decided to use the Google Translate API to automatically generate translations for all English test instances in the five target languages. This automatic translation process can be done using whatever machine translation tool, but we chose the Google API because of its easy integration. Online machine translation tools have already been used before to create artificial parallel corpora that were used for NLP tasks such as for instance Named Entity Recognition (Shah et al., 2010). Similarly, Navigli and Ponzetto (2010) used the Google Translate API to enrich BabelNet, a wide-coverage multilingual semantic network, with lexical information for all languages. Once the automatic aligned translations were generated, we preprocessed them in the same way as we did for the aligned training translations. In a next step, we again selected all content words from these translations and constructed the binary bag-of-words features. 4 Evaluation To evaluate the two machine translation systems as well as the ParaSense system on their performance on the lexical sample of twenty ambiguous words, we used the sense inventory"
W11-1006,P03-1058,0,0.0346299,"word might be a subtle change compared to the reference sentence, but it often drastically affects the global understanding of the sentence. Secondly, we explore the potential benefits of a real multilingual approach to WSD. The idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of a polysemous word are often lexicalized across languages (Resnik and Yarowsky, 2000). Many WSD studies have incorporated this cross-lingual evidence idea and have successfully applied bilingual WSD classifiers (Gale and Church, 1993; Ng et al., 2003; Diab and Resnik, 2002) or 53 systems that use a combination of existing WordNets with multilingual evidence (Tufis¸ et al., 2004). Our WSD system is different in the sense that it is independent from a predefined sense-inventory (it only uses the parallel corpus at hand) and that it is truly multilingual as it incorporates information from four other languages (French, Dutch, Spanish, Italian and German depending on the target language of the classifier). Although our classifiers are still very preliminary in terms of the feature set and parameters that are used, we obtain interesting result"
W11-1006,J03-1002,0,0.00266907,"s. This way we obtained a six-lingual sentence-aligned subcorpus of Europarl, that contains 884.603 sentences per language. For our experiments we used the lexical sample of twenty ambiguous nouns that was also used in the SemEval-2010 ”Cross-Lingual Word Sense Disambiguation” (CLWSD) task (Lefever and Hoste, 2010b), which consists in assigning a correct translation in five supported target languages (viz. French, Italian, Spanish, German and Dutch) for an ambiguous focus word in a given context. In order to detect all relevant translations for the twenty ambiguous focus words, we ran GIZA++ (Och and Ney, 2003) with its default settings on our parallel corpus. The obtained word alignment output was then considered to be the classification label for the training instances for a given classifier (e.g. the French translation resulting from the word alignment is the label that is used to train the French classifier). This way we obtained all class labels (or oracle translations) for all training instances for our five classifiers (English as an input language and French, German, Dutch, Italian and Spanish as target languages). For the experiments described in this paper, we focused on the English– Frenc"
W11-1006,P02-1040,0,0.0827205,"nt from previous research in two aspects. Firstly, we evaluate the performance of two state-of-the-art SMT systems and a dedicated WSD system on the translation of ambiguous words. The comparison is done against a manually constructed gold-standard for two language pairs, viz. English–French and English–Dutch. Although it is crucial to measure the general translation quality after integrating a dedicated WSD module in the SMT system, we think it is equally interesting to conduct a dedicated evaluation of the translation quality on ambiguous nouns. Standard SMT evaluation metrics such as BLEU (Papineni et al., 2002) or edit-distance metrics (e.g. Word Error Rate) measure the global overlap of the translation with a reference, and are thus not very sensitive to WSD errors. The mistranslation of an ambiguous word might be a subtle change compared to the reference sentence, but it often drastically affects the global understanding of the sentence. Secondly, we explore the potential benefits of a real multilingual approach to WSD. The idea to use translations from parallel corpora to distinguish between word senses is based on the hypothesis that different meanings of a polysemous word are often lexicalized"
W11-1006,P06-3010,0,0.0238316,"f ambiguous words are gathered from a parallel corpus by means of word alignment. The authors reported improvements on two simplified translation tasks: word translation and blank filling. The evaluation was done on an English-French parallel corpus but is confronted with the important limitation of having only one valid translation (the aligned translation in the parallel corpus) as a gold standard translation. Cabezas and Resnik (2005) tried to improve an SMT system by adding additional translations to the phrase table, but were confronted with tuning problems of this dedicated WSD feature. Specia (2006) used an inductive logic programming-based WSD system which was tested on seven ambiguous verbs in English-Portuguese translation. The latter systems already present promising results for the use of WSD in MT, but really significant improvements in terms of general machine translation qualProceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 52–60, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics ity were for the first time obtained by Carpuat and Wu (2007) and Chan et al. (2007). Both papers d"
W11-1006,2007.tmi-papers.28,0,0.0196963,"of this dedicated WSD feature. Specia (2006) used an inductive logic programming-based WSD system which was tested on seven ambiguous verbs in English-Portuguese translation. The latter systems already present promising results for the use of WSD in MT, but really significant improvements in terms of general machine translation qualProceedings of SSST-5, Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation, pages 52–60, c ACL HLT 2011, Portland, Oregon, USA, June 2011. 2011 Association for Computational Linguistics ity were for the first time obtained by Carpuat and Wu (2007) and Chan et al. (2007). Both papers describe the integration of a dedicated WSD module in a Chinese-English statistical machine translation framework and report statistically significant improvements in terms of standard MT evaluation metrics. Stroppa et al. (2007) take a completely different approach to perform some sort of implicit Word Sense Disambiguation in MT. They introduce context-information features that exploit source similarity, in addition to target similarity that is modeled by the language model, in an SMT framework. For the estimation of these features that are very similar to"
W11-1006,C04-1192,0,0.0804087,"Missing"
W11-1006,H05-1097,0,0.0185866,"nnotators), whereas very detailed sense distinctions are often irrelevant for practical applications. In addition to this, there is a growing feeling in the community that WSD should be used and evaluated in real application such as Machine Translation (MT) or Information Retrieval (IR) (Agirre and Edmonds, 2006). An important line of research consists in the development of dedicated WSD modules for MT. Instead of assigning a sense label from a monolingual sense-inventory to the ambiguous words, the WSD system has to predict a correct translation for the ambiguous word in a given context. In (Vickrey et al., 2005), the problem was defined as a word translation task. The translation choices of ambiguous words are gathered from a parallel corpus by means of word alignment. The authors reported improvements on two simplified translation tasks: word translation and blank filling. The evaluation was done on an English-French parallel corpus but is confronted with the important limitation of having only one valid translation (the aligned translation in the parallel corpus) as a gold standard translation. Cabezas and Resnik (2005) tried to improve an SMT system by adding additional translations to the phrase"
W11-1006,W09-2413,1,\N,Missing
W11-1006,W09-0401,0,\N,Missing
W11-1415,J96-2004,0,0.261427,"Missing"
W11-1415,N04-1025,0,0.0372647,"e show that the data sets resulting from both annotation strategies are very similar. We conclude that crowdsourcing is a viable alternative to the opinions of language experts for readability prediction. 1 Introduction The task of automatically determining the readability of texts has a long and rich tradition. This has not only resulted in a large number of readability formulas (Flesch, 1948; Brouwer, 1963; Dale and Chall, 1948; Gunning, 1952; McLaughlin, 1969), but also to the more recent tendency of using insights from NLP for automatic readability prediction (Schwarm and Ostendorf, 2005; Collins-Thompson and Callan, 2004; Pitler and Nenkova, 2008). Potential applications include the selection of reading material for language learners, automatic essay scoring, the selection of online text material for automatic summarization, etc. One of the well-known bottlenecks in data-driven NLP research is the lack of sufficiently large data sets for which annotators provided labels with sufficient agreement. Also readability research is faced with the crucial obstacle that very few corpora of generic texts exist of which reliable readability information is available (Tanaka-Ishii et al., 2010). When constructing such a c"
W11-1415,E09-3003,0,0.28543,"Missing"
W11-1415,W08-0909,0,0.0974276,"the data from language experts and how those data can be converted to relative assessments. Section 3 outlines a simpler crowsourcing application and its correspondences with the experts. Finally, in section 4, we draw conclusions and give a short summary of future work. 2 Readability assessment by the expert reader Since readability prediction was initially primarily designed to identify reading material suited to the reading competence of a given individual, most of the existing data sets are drawn from textbooks and other sources intended for different compentence levels (Franc¸ois, 2009; Heilman et al., 2008). For Dutch, for example, the only large-scale experimental readability research (Staphorsius and Krom, 1985; Staphorsius, 1994) is limited to texts for elementary school children.1 For English, the situation is similar as for Dutch, viz. a predominant focus on educational corpora. Recently, an evaluation was designed by LDC in the framework of the DARPA Machine Reading Program (Kate et al., 2010). For this purpose a more general corpus was assembled which was not tailored to a specific audience, genre or domain. Unfortunately, the data are not available for further use. Our research focus is"
W11-1415,C10-1062,0,0.249863,"ground knowledge of the subject at hand (McNamara et al., 1993). The construction of a corpus, which can serve as a gold standard against which new scoring or ranking systems can be tested, thus requires a multifaceted approach taking into account both the properties of the text under evaluation and those of the readers. In recent years, a tendency seems to have arisen to also explicitly address this subjective aspect of readability. Pitler and Nenkova (2008), for example, base their readability prediction method exclusively on the extent to which readers found a text to be “well-written” and Kate et al. (2010) take the assessments supplied by a number of experts as their gold standard, and test their readability prediction method as well as assessments by novices against these expert opinions. In this paper, we report on two methodologies to construct a corpus of readability assessments, which can serve as a gold standard against which new scoring or ranking systems can be tested. Both methodologies were used for collecting readability assessments of Dutch and English texts. Since these data collection experiments for English only recently started, the focus in this paper will be on 120 Proceedings"
W11-1415,D08-1020,0,0.431147,"from both annotation strategies are very similar. We conclude that crowdsourcing is a viable alternative to the opinions of language experts for readability prediction. 1 Introduction The task of automatically determining the readability of texts has a long and rich tradition. This has not only resulted in a large number of readability formulas (Flesch, 1948; Brouwer, 1963; Dale and Chall, 1948; Gunning, 1952; McLaughlin, 1969), but also to the more recent tendency of using insights from NLP for automatic readability prediction (Schwarm and Ostendorf, 2005; Collins-Thompson and Callan, 2004; Pitler and Nenkova, 2008). Potential applications include the selection of reading material for language learners, automatic essay scoring, the selection of online text material for automatic summarization, etc. One of the well-known bottlenecks in data-driven NLP research is the lack of sufficiently large data sets for which annotators provided labels with sufficient agreement. Also readability research is faced with the crucial obstacle that very few corpora of generic texts exist of which reliable readability information is available (Tanaka-Ishii et al., 2010). When constructing such a corpus, the inherent subject"
W11-1415,P05-1065,0,0.131571,"one having internet access. We show that the data sets resulting from both annotation strategies are very similar. We conclude that crowdsourcing is a viable alternative to the opinions of language experts for readability prediction. 1 Introduction The task of automatically determining the readability of texts has a long and rich tradition. This has not only resulted in a large number of readability formulas (Flesch, 1948; Brouwer, 1963; Dale and Chall, 1948; Gunning, 1952; McLaughlin, 1969), but also to the more recent tendency of using insights from NLP for automatic readability prediction (Schwarm and Ostendorf, 2005; Collins-Thompson and Callan, 2004; Pitler and Nenkova, 2008). Potential applications include the selection of reading material for language learners, automatic essay scoring, the selection of online text material for automatic summarization, etc. One of the well-known bottlenecks in data-driven NLP research is the lack of sufficiently large data sets for which annotators provided labels with sufficient agreement. Also readability research is faced with the crucial obstacle that very few corpora of generic texts exist of which reliable readability information is available (Tanaka-Ishii et al."
W11-1415,D08-1027,0,0.0376771,"Missing"
W11-1415,J10-2002,0,0.207696,"and Ostendorf, 2005; Collins-Thompson and Callan, 2004; Pitler and Nenkova, 2008). Potential applications include the selection of reading material for language learners, automatic essay scoring, the selection of online text material for automatic summarization, etc. One of the well-known bottlenecks in data-driven NLP research is the lack of sufficiently large data sets for which annotators provided labels with sufficient agreement. Also readability research is faced with the crucial obstacle that very few corpora of generic texts exist of which reliable readability information is available (Tanaka-Ishii et al., 2010). When constructing such a corpus, the inherent subjectivity of the concept of readability cannot be ignored. The ease with which a given reader can correctly identify the message conveyed in a text is, among other things, inextricably related to the reader’s background knowledge of the subject at hand (McNamara et al., 1993). The construction of a corpus, which can serve as a gold standard against which new scoring or ranking systems can be tested, thus requires a multifaceted approach taking into account both the properties of the text under evaluation and those of the readers. In recent yea"
W11-1415,van-oosten-etal-2010-towards,1,0.874275,"Missing"
W12-0301,van-eynde-etal-2000-part,0,0.0279447,"Missing"
W15-3043,P11-1022,0,0.0514624,"y to capture either accuracy or fluency errors, where accuracy is concerned with how much of the meaning expressed in the source is also expressed in the target text, and fluency is concerned with to what extent the translation is wellformed, regardless of sentence meaning. This distinction is well known in quality assessment schemes for MT (White, 1995; Secară, 2005; Lommel et al., 2014). Some of the additional features are based on ideas that were explored in previous work on QE, such as; context features for the target word and of POS tags, (Xiong et al., 2010), alignment context features (Bach et al., 2011) and adequacy and fluency indicators (Specia et al., 2013). 353 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 353–360, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. The rest of this paper is organized as follows. Section 2 and Section 3 give an overview of the shared task on word-level QE and sentence-level QE respectively and describe also the features we extracted, the learning methods and the additional language resources we used and the experiments we conducted. Finally, in Section 4, we discuss the results we obtain"
W15-3043,padro-stanilovsky-2012-freeling,0,0.143606,"Missing"
W15-3043,W11-2123,0,0.0552678,"resources. As monolingual data resource, we used a corpus of more than 13 million Spanish sentences collected from the News Crawl Corpus1 (years 2007-2013) to build two types of language models: one based on surface forms and one based on PoS codes. The following preprocessing steps have been applied on the data before building the language models: normalizing punctuation and numbers, tokenization, named entity recognition using the Stanford NER tool (Finkel et al., 2005), lowercasing, and PoStagging using FreeLing (Padró and Stanilovsky, 2012). The surface form LM has been built using KenLM (Heafield 2011). For the PoS LM, we used IRSTLM with Witten-Bell smoothing (Federico et al., 2008) as the modified KneserNey smoothing, which is used by KENLM, is not well defined when there are no singletons (Chen and Goodman 1999), which leads to modeling issues in the PoS corpus. As bilingual data, we selected 6 million sentence pairs from OPUS (Tiedemann 2012) from various domains and used the Moses toolkit (Koehn et al. 2006) to obtain word and phrase alignments. Even though there are more bilingual sentences available, to avoid a bias to one specific domain, a similar number of sentences of different d"
W15-3043,2006.amta-papers.25,0,0.0501284,"ybrid 2B Accuracy 0,74 0,79 0,81 0,73 F1 “BAD” 0.317 0.292 0.161 0.375 Table 2: Classification performance of the best TiMBL system, in comparison with the ensemble systems on the development set. Based on all the results, we selected the following systems for the submission of this year’s shared task on word-level QE: • SCATE-HYBRID: Hybrid 2B • SCATE-MBL: TiMBL-m These two systems obtained comparable scores (F1 “BAD”) on the test set of 0.367 and 0.305 respectively. 3 Sentence-level Quality Estimation The sentence-level QE task aims at predicting Human mediated Translation Edit Rate (HTER) (Snover et al., 2006) between the raw MT output and its manually post-edited version. In addition to scoring the sentences for quality, a ranking variant of this task is defined as ranking all MT sentences, for all source sentences, from best to worst. 3.1 Features and Language Resources In our experiments, in addition to 17 baseline features that were provided together with the data sets, we designed 17 additional features. In this section, we briefly list out the additional features we used in WMT 2015 sentence-level QE task. We used the same additional language resources as in the word-level QE task to extract"
W15-3043,W14-3340,0,0.134002,"describe the UGENT-LT3 SCATE submissions to task 1 (sentence-level QE) and task 2 (word-level QE). Sentence-level and word-level QE are related tasks. Sentence-level QE assigns a global score to an automatically translated sentence whereas word-level QE is more fine-grained and tries to detect the problematic word sequences. Therefore we first developed a word-level QE system and incorporate the word-level predictions as additional features in the sentence-level QE system. The usefulness of including word-level predictions in sentence-level QE has already been demonstrated by de Souza et al. (2014) For both tasks, we extracted additional features and combine these with the baseline feature set to estimate quality. The new features try to capture either accuracy or fluency errors, where accuracy is concerned with how much of the meaning expressed in the source is also expressed in the target text, and fluency is concerned with to what extent the translation is wellformed, regardless of sentence meaning. This distinction is well known in quality assessment schemes for MT (White, 1995; Secară, 2005; Lommel et al., 2014). Some of the additional features are based on ideas that were explored"
W15-3043,P13-4014,0,0.0621042,"Missing"
W15-3043,1995.mtsummit-1.37,0,0.931006,"lness of including word-level predictions in sentence-level QE has already been demonstrated by de Souza et al. (2014) For both tasks, we extracted additional features and combine these with the baseline feature set to estimate quality. The new features try to capture either accuracy or fluency errors, where accuracy is concerned with how much of the meaning expressed in the source is also expressed in the target text, and fluency is concerned with to what extent the translation is wellformed, regardless of sentence meaning. This distinction is well known in quality assessment schemes for MT (White, 1995; Secară, 2005; Lommel et al., 2014). Some of the additional features are based on ideas that were explored in previous work on QE, such as; context features for the target word and of POS tags, (Xiong et al., 2010), alignment context features (Bach et al., 2011) and adequacy and fluency indicators (Specia et al., 2013). 353 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 353–360, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. The rest of this paper is organized as follows. Section 2 and Section 3 give an overview of the sh"
W15-3043,P10-1062,0,0.0204779,"ture set to estimate quality. The new features try to capture either accuracy or fluency errors, where accuracy is concerned with how much of the meaning expressed in the source is also expressed in the target text, and fluency is concerned with to what extent the translation is wellformed, regardless of sentence meaning. This distinction is well known in quality assessment schemes for MT (White, 1995; Secară, 2005; Lommel et al., 2014). Some of the additional features are based on ideas that were explored in previous work on QE, such as; context features for the target word and of POS tags, (Xiong et al., 2010), alignment context features (Bach et al., 2011) and adequacy and fluency indicators (Specia et al., 2013). 353 Proceedings of the Tenth Workshop on Statistical Machine Translation, pages 353–360, c Lisboa, Portugal, 17-18 September 2015. 2015 Association for Computational Linguistics. The rest of this paper is organized as follows. Section 2 and Section 3 give an overview of the shared task on word-level QE and sentence-level QE respectively and describe also the features we extracted, the learning methods and the additional language resources we used and the experiments we conducted. Finally"
W15-3043,P05-1045,0,\N,Missing
W15-3043,J07-1003,0,\N,Missing
W15-3043,P07-2045,0,\N,Missing
W15-3043,C04-1046,0,\N,Missing
W15-3043,N07-1029,0,\N,Missing
W15-3043,2009.eamt-1.5,0,\N,Missing
W15-3043,W14-3302,0,\N,Missing
W15-3043,W13-2201,0,\N,Missing
W15-3043,2014.eamt-1.38,0,\N,Missing
W15-3043,tiedemann-2012-parallel,0,\N,Missing
W16-0317,desmet-hoste-2014-recognising,1,0.801202,"6 ACL Workshop on Computational Linguistics and Clinical Psychology included a shared task focusing on triage classification in forum posts from ReachOut.com, an online service for youth mental health issues. The aim is to automatically classify an unseen post as one of four categories indicating the severity of mental distress. ReachOut staff has annotated a corpus of posts with crisis/red/amber/green semaphore labels that indicate how urgently a post needs moderator attention. The system described in this paper is based on a suicidality classification system intended for Dutch social media (Desmet and Hoste, 2014). Therefore, we approach the current mental distress triage task from a suicide detection standpoint. 2 Related Work Machine learning and natural language processing have already shown potential in modelling and deOne line of research focuses on detecting suicidality in individuals relying on their post history: Huang et al. (2007) aim to identify Myspace.com bloggers at risk of suicide by means of a keywordbased approach using a manually collected dictionary of weighted suicide-related terms. Users were ranked by pattern-matching keywords on their posts. This approach suffered from low precis"
W16-0317,S13-2093,0,0.0668621,"Missing"
W16-0317,I13-1079,0,0.0516021,"Missing"
W16-0317,W15-1207,0,0.0673449,"Missing"
W16-0317,H05-1044,0,0.0658112,"Missing"
W16-2393,W03-0413,0,0.0625533,"resulting term list to the well-performing features of the word-level QE task of last year. For sentence-level QE, we analyzed the importance of the features and based on those insights extended the feature set of last year. We also experimented with different learning methods and ensembles. We present our observations from the different experiments we conducted and our submissions for both tasks. 1 Introduction Machine Translation (MT) Quality Estimation (QE) is the task of providing a quality indicator for unseen automatically translated sentences without relying on reference translations (Gandrabur and Foster, 2003; Blatz et al., 2004). The WMT16 QE shared task proposes three evaluation tasks: (1) scoring and ranking sentences according to predicted post-editing effort given a source sentence and its translation; predicting the individual (2a) words and (2b) phrases (segmented by the Statistical Machine Translation (SMT) decoder) that require post-editing; and (3) predicting the quality at document level. In this paper, we describe the UGENT-LT3 SCATE submissions to task 1 (sentence-level QE) and task 2a (word-level QE). By conceiving the QE as a supervised Machine Learning (ML) problem for both tasks,"
W16-2393,W11-2123,0,0.0312022,"al language resources that were used to extract features and the number of segments in each data set. We used the Moses Toolkit (Koehn et al., 2007) to obtain phrase alignments from the collected data. The phrase alignments were pruned to exclude entries with a direct alignment probability P (t|s) &lt; 0.01. We built the LM and PoS LM on the target side of the collected bilingual data. The following preprocessing steps have been applied on the data prior to building the LM and the phrase table: normalization of digits, tokenization and lowercasing. The surface form LM has been built using KenLM (Heafield, 2011). For building the PoS LM, we used TreeTagger (Schmid, 1995) to obtain the PoS tags on the target (DE) data and IRSTLM (Federico et al., 2008) for building the LM. As smoothing technique we used Witten-Bell as the modified Kneser-Ney smoothing, which is used by KenLM, is not well defined when there are no singletons (Chen and Goodman, 1996) and leads to modeling issues on the PoS data. The resulting LMs and phrase table were stored in databases and indexed to speed up lookup operations. 2.2 BAD b OK MLT BAD b+s OK MLT BAD b+s OK +t MLT LR 0.33 0.87 0.29 0.41 0.83 0.34 0.45 0.83 0.37 PE 0.37 0."
W16-2393,P07-2045,0,0.00406531,"ed hyper-parameter optimization for the ML algorithms (when applicable) using 10-fold cross validation on the training set and tested the classification performance on the development set. All the features were scaled to the [0, 1] range prior to training. The classification performance of different algorithms and feature sets, with respect to F1 scores for the BAD class, the OK class and the multiplication of the two (MLT), are provided in Table 4. Table 3: Additional language resources that were used to extract features and the number of segments in each data set. We used the Moses Toolkit (Koehn et al., 2007) to obtain phrase alignments from the collected data. The phrase alignments were pruned to exclude entries with a direct alignment probability P (t|s) &lt; 0.01. We built the LM and PoS LM on the target side of the collected bilingual data. The following preprocessing steps have been applied on the data prior to building the LM and the phrase table: normalization of digits, tokenization and lowercasing. The surface form LM has been built using KenLM (Heafield, 2011). For building the PoS LM, we used TreeTagger (Schmid, 1995) to obtain the PoS tags on the target (DE) data and IRSTLM (Federico et a"
W16-2393,C04-1046,0,0.0616293,"well-performing features of the word-level QE task of last year. For sentence-level QE, we analyzed the importance of the features and based on those insights extended the feature set of last year. We also experimented with different learning methods and ensembles. We present our observations from the different experiments we conducted and our submissions for both tasks. 1 Introduction Machine Translation (MT) Quality Estimation (QE) is the task of providing a quality indicator for unseen automatically translated sentences without relying on reference translations (Gandrabur and Foster, 2003; Blatz et al., 2004). The WMT16 QE shared task proposes three evaluation tasks: (1) scoring and ranking sentences according to predicted post-editing effort given a source sentence and its translation; predicting the individual (2a) words and (2b) phrases (segmented by the Statistical Machine Translation (SMT) decoder) that require post-editing; and (3) predicting the quality at document level. In this paper, we describe the UGENT-LT3 SCATE submissions to task 1 (sentence-level QE) and task 2a (word-level QE). By conceiving the QE as a supervised Machine Learning (ML) problem for both tasks, we ex2 Word-level Qua"
W16-2393,2014.eamt-1.38,0,0.0293532,"Abstract tended the features that we extracted for our last year’s submission (Tezcan et al., 2015), which try to capture the accuracy and fluency errors in MT output. While accuracy is concerned with how much of the meaning expressed in the source is also expressed in the target text, fluency is concerned with to what extent the translation is wellformed. This distinction between accuracy and fluency was suggested to break down human translation quality judgments into separate and smaller units (White, 1995) and is well known in quality assessment schemes for MT (White, 1995; Secar˘a, 2005; Lommel et al., 2014). Similarly, we use the same distinction to break down the QE task into separate units. In addition to the features that try to capture accuracy and fluency errors, given the specialized domain of this year’s data set (IT), for word-level QE, we extracted features that try to capture terminological problems. For both tasks, we experimented with different learning methods. For word-level QE we also built ensemble systems that are based on majority voting and bagging (random forests), in which multiple decision trees are constructed using bootstrapped training sets and the predictions of these t"
W16-2393,P96-1041,0,0.187026,"on the target side of the collected bilingual data. The following preprocessing steps have been applied on the data prior to building the LM and the phrase table: normalization of digits, tokenization and lowercasing. The surface form LM has been built using KenLM (Heafield, 2011). For building the PoS LM, we used TreeTagger (Schmid, 1995) to obtain the PoS tags on the target (DE) data and IRSTLM (Federico et al., 2008) for building the LM. As smoothing technique we used Witten-Bell as the modified Kneser-Ney smoothing, which is used by KenLM, is not well defined when there are no singletons (Chen and Goodman, 1996) and leads to modeling issues on the PoS data. The resulting LMs and phrase table were stored in databases and indexed to speed up lookup operations. 2.2 BAD b OK MLT BAD b+s OK MLT BAD b+s OK +t MLT LR 0.33 0.87 0.29 0.41 0.83 0.34 0.45 0.83 0.37 PE 0.37 0.81 0.30 0.40 0.83 0.33 0.37 0.85 0.31 RF 0.23 0.90 0.20 0.45 0.85 0.38 0.45 0.86 0.39 SVC 0.24 0.88 0.21 0.42 0.83 0.35 0.43 0.82 0.35 CRF 0.30 0.89 0.26 0.38 0.80 0.30 0.44 0.82 0.36 MBL 0.29 0.88 0.25 0.38 0.80 0.30 0.39 0.81 0.32 Table 4: The performance of different ML algorithms and feature sets on the development set. The plus sign ‘+"
W16-2393,W15-4912,0,0.0395397,"Missing"
W16-2393,W14-3340,0,0.101345,"Missing"
W16-2393,2006.amta-papers.25,0,0.208243,"Missing"
W16-2393,W15-3043,1,0.86009,"e extracted terms are provided in Table 2. Length 17.57 19.48 Table 1: Number of words, distribution of the binary labels and the average sentence length, on the training and development set. 2.1 Source Term (EN) dialog box SWF file pop-up note export exported image cross-references Features and Language Resources To characterize each target word of the MT output, in addition to the provided baseline features, which were described in the WMT15 QE shared task (Bojar et al., 2015), we extracted the features1 we used for our last year’s submission, for which detailed descriptions can be found in Tezcan et al. (2015). Technical texts, like in the IT domain, express concepts in a concise and consistent form and leave little room for data redundancy. This is often achieved with the use a specialized terminology (Rinaldi et al., 2004). As a result, in professional translation services, correct and consistent handling of terminology becomes an important inTarget Term (DE) Dialogfeld SWF-Datei Popup-Notiz exportieren exportierten Bilds Querverweise Table 2: Examples of bilingual terminology automatically extracted by TExSIS. Based on this bilingual term list, we marked all entries, starting with the longest te"
W16-2393,tiedemann-2012-parallel,0,0.0341459,"ces for the other additional features we extracted (see Tezcan et al. 2015 for more details). These features are based on a surface Language Model (LM) and a Partof-speech (PoS) LM of the target language, and a 1 All features that are described in Tezcan et al. (2015) except the features based on named entities and simplified Part-of-Speech (PoS) tags. 844 Phrase Table consisting of phrase alignments and translation probabilities between the source and target languages. As bilingual data, we used the provided training set, the Autodesk Post-Editing Data2 and a collection of corpora from OPUS (Tiedemann, 2012) in the IT domain. The number of sentence pairs collected from each corpus is presented in Table 3. Corpus WMT16 Autodesk Gnome KDE4 PHP Ubuntu TOTAL # Segments 12000 124486 28439 224035 39707 12992 441659 different learning methods and ensembles. In our experiments we used 6 different learning methods: Logistic Regression (LR), Perceptron (PE), Random Forest (RF) and Linear Support Vector Classification (SVC) using the Scikit-learn module in Python (Pedregosa et al., 2011), Conditional Random Fields (CRFs) using the CRF++ Toolkit (Kudo, 2005) and Memory-Based Learning (MBL) using TiMBL (Daele"
W16-2393,1995.mtsummit-1.37,0,0.58621,"root-Brittannielaan 45, 9000 Ghent, Belgium {arda.tezcan, veronique.hoste, lieve.macken}@ugent.be Abstract tended the features that we extracted for our last year’s submission (Tezcan et al., 2015), which try to capture the accuracy and fluency errors in MT output. While accuracy is concerned with how much of the meaning expressed in the source is also expressed in the target text, fluency is concerned with to what extent the translation is wellformed. This distinction between accuracy and fluency was suggested to break down human translation quality judgments into separate and smaller units (White, 1995) and is well known in quality assessment schemes for MT (White, 1995; Secar˘a, 2005; Lommel et al., 2014). Similarly, we use the same distinction to break down the QE task into separate units. In addition to the features that try to capture accuracy and fluency errors, given the specialized domain of this year’s data set (IT), for word-level QE, we extracted features that try to capture terminological problems. For both tasks, we experimented with different learning methods. For word-level QE we also built ensemble systems that are based on majority voting and bagging (random forests), in whic"
W16-2393,2015.eamt-1.13,0,\N,Missing
W16-2393,W15-3001,0,\N,Missing
W16-3409,augustinus-etal-2012-example,0,0.0721079,"Missing"
W16-3409,P11-1022,0,0.136778,"translation (CAT) workflows (Guerberof, 2009; Depraetere et al., 2014), to produce high-quality translations, humans still need to intervene in the translation process and do this usually by post-editing (correcting) the MT output. Post-editing MT output requires post-editors to detect translation errors prior to correcting them. Hence, automatic quality estimation (QE) systems not only aim to estimate the post-editing effort at segment level to filter low quality translations (Specia et al, 2009), but also to detect the location and the nature of errors at word level (Ueffing and Ney, 2007; Bach et al., 2011). MT errors can be analysed as adequacy and fluency errors. While adequacy is concerned with how much of the source content and meaning is also expressed in the target text, fluency is concerned with to what extent the translation is well formed and adheres to the norms of the target language. The distinction between adequacy and 204 Tezkan et al. fluency has been used in different translation error taxonomies (Lommel et al., 2014; Daems, Macken and Vandepitte, 2014). Besides the difficulties of transferring source content and meaning to a target sentence, the task of producing grammatically c"
W16-3409,C04-1046,0,0.0691686,"and in Section 4, we describe the data sets we used. In Section 5, we give the results of our experiments. Finally, in Section 6, we conclude by discussing the results and future work. 2. Related work QE is the task of providing a quality indicator for machine-translated text without relying on reference translations (Gandrabur and Foster, 2003). Most work on QE has focused on segment level, which aims to provide a binary or continuous quality score for the whole machine-translated sentence that reflects the post-editing time or the number of edits that are required to correct the MT output (Blatz et al., 2004; Specia et al., 2009; Hardmeier et al., 2011). QE on word or sub-segment level, on the other hand, has received less attention. Estimating the quality of MT output on word or sub-segment level has a number of advantages compared to sentence-level QE. First of all, word-level QE systems can highlight problematic text fragments in machine-translated text to guide the post-editors. Furthermore, since the overall quality of an MT system depends on the individual errors it makes, word-level QE systems can easily be extended to estimate segment-level quality (de Souza et al., 2014; Tezcan et al., 2"
W16-3409,W14-3302,0,0.0225931,"Missing"
W16-3409,2013.mtsummit-wptp.8,1,0.890648,"Missing"
W16-3409,2015.mtsummit-wptp.3,1,0.84223,"the target text, fluency is concerned with to what extent the translation is well formed and adheres to the norms of the target language. The distinction between adequacy and 204 Tezkan et al. fluency has been used in different translation error taxonomies (Lommel et al., 2014; Daems, Macken and Vandepitte, 2014). Besides the difficulties of transferring source content and meaning to a target sentence, the task of producing grammatically correct sentences remains to be challenging for MT systems, independent of the domain of text to be translated and the type of MT system (Costa et al., 2015; Daems et al., 2015). This motivates us to examine the use of dependency structures, which represent the abstract grammatical relations that hold between constituents, for detecting grammatical errors in machine-translated text. A dependency tree is a rooted, directed acyclic graph, which represents all words in a sentence as nodes and grammatical relations between the words as edges. A labelled dependency tree, additionally, incorporates the nature of the grammatical relationships between the words as annotations of relation names on the edges of the tree. Dependency trees are interesting for the QE task due to"
W16-3409,de-marneffe-etal-2006-generating,0,0.0390025,"Missing"
W16-3409,W14-3340,0,0.0223896,"Missing"
W16-3409,2011.eamt-1.32,0,0.042691,"Missing"
W16-3409,2011.mtsummit-papers.62,0,0.0492226,"eatures together with source-side and alignment content features to train a classifier for predicting word-level quality. Hardmeier et al. (2011) used tree kernels over constituency and dependency parses of MT input and output in conjunction with Support Vector Machine (SVM) classification for QE. A number of studies focused on detecting grammatical errors. Stymne and Ahrenberg (2010) used a mainly rule-based Swedish grammar checker not only to assess the grammaticality of their English-Swedish SMT system, but also for post-processing the MT output by applying the grammar checker suggestions. Ma and McKeown (2011), on the other hand, used feature-based lexicalized tree adjoining grammars (FB-LTAG) to detect and filter ungrammatical translations generated by their MT system. A Tree Adjoining Grammar (TAG) consists of a number of elementary trees, which can be combined with substitution and adjunction operations and while the derivation trees in TAG resemble dependency structures, the derived trees are phrase-structure trees (Joshi and Rambow, 2013). The approaches we propose differ from previous work in several ways. First of all, we use only dependency tree information of the target language to detect"
W16-3409,J11-4002,0,0.273012,"Missing"
W16-3409,W03-2414,0,0.137781,"Missing"
W16-3409,2009.eamt-1.5,0,0.0805653,"m sentence to sentence. Even though it has been shown that using MT leads to productivity gains in computer-assisted translation (CAT) workflows (Guerberof, 2009; Depraetere et al., 2014), to produce high-quality translations, humans still need to intervene in the translation process and do this usually by post-editing (correcting) the MT output. Post-editing MT output requires post-editors to detect translation errors prior to correcting them. Hence, automatic quality estimation (QE) systems not only aim to estimate the post-editing effort at segment level to filter low quality translations (Specia et al, 2009), but also to detect the location and the nature of errors at word level (Ueffing and Ney, 2007; Bach et al., 2011). MT errors can be analysed as adequacy and fluency errors. While adequacy is concerned with how much of the source content and meaning is also expressed in the target text, fluency is concerned with to what extent the translation is well formed and adheres to the norms of the target language. The distinction between adequacy and 204 Tezkan et al. fluency has been used in different translation error taxonomies (Lommel et al., 2014; Daems, Macken and Vandepitte, 2014). Besides the"
W16-3409,P15-4020,0,0.014466,", 2011) or by combining correct text fragments from different MT systems (Ueffing and Ney, 2007). In one of the early works on word-level QE, Blatz et al. (2004) used a collection of features to build a binary classifier that provides confidence scores for each word in machine-translated text. Besides using features that capture the relationships between source and target words, such as the word posterior probabilities and semantic similarities, they used additional target-language features that were based on basic syntax checking and word frequency. A recent work on word-level QE is QuEst++ (Specia et al., 2015), which is an open-source toolkit for QE on word, sentence and document level. QuEst++ consists of two main modules: a feature extraction and a Machine Learning (ML) module. For word-level QE, besides using features that explore the word alignments and POS-similarities between source and target texts, QuEst++ uses target-language features that are derived from n-gram language models. As n-gram language models rely primarily on local context, they can capture short-distance dependencies (e.g. article-noun agreements), but they fail to capture long-distance dependencies such as non-adjacent subj"
W16-3409,stymne-ahrenberg-2010-using,0,0.0199014,"al. (2011) compared the dependency relations of Arabic source sentences and English MT translations and incorporated childfather and children correspondences as features in their ML system. They used source and target dependency structure features together with source-side and alignment content features to train a classifier for predicting word-level quality. Hardmeier et al. (2011) used tree kernels over constituency and dependency parses of MT input and output in conjunction with Support Vector Machine (SVM) classification for QE. A number of studies focused on detecting grammatical errors. Stymne and Ahrenberg (2010) used a mainly rule-based Swedish grammar checker not only to assess the grammaticality of their English-Swedish SMT system, but also for post-processing the MT output by applying the grammar checker suggestions. Ma and McKeown (2011), on the other hand, used feature-based lexicalized tree adjoining grammars (FB-LTAG) to detect and filter ungrammatical translations generated by their MT system. A Tree Adjoining Grammar (TAG) consists of a number of elementary trees, which can be combined with substitution and adjunction operations and while the derivation trees in TAG resemble dependency struc"
W16-3409,W15-3043,1,0.812861,"atz et al., 2004; Specia et al., 2009; Hardmeier et al., 2011). QE on word or sub-segment level, on the other hand, has received less attention. Estimating the quality of MT output on word or sub-segment level has a number of advantages compared to sentence-level QE. First of all, word-level QE systems can highlight problematic text fragments in machine-translated text to guide the post-editors. Furthermore, since the overall quality of an MT system depends on the individual errors it makes, word-level QE systems can easily be extended to estimate segment-level quality (de Souza et al., 2014; Tezcan et al., 2015). Word-level QE systems can additionally be used for improving MT quality by providing valuable information about Detecting Grammatical Errors in Machine Translation 205 the location, the frequency and the type of errors MT systems make (Popovic and Ney, 2011) or by combining correct text fragments from different MT systems (Ueffing and Ney, 2007). In one of the early works on word-level QE, Blatz et al. (2004) used a collection of features to build a binary classifier that provides confidence scores for each word in machine-translated text. Besides using features that capture the relationship"
W16-3409,J07-1003,0,0.057548,"ns in computer-assisted translation (CAT) workflows (Guerberof, 2009; Depraetere et al., 2014), to produce high-quality translations, humans still need to intervene in the translation process and do this usually by post-editing (correcting) the MT output. Post-editing MT output requires post-editors to detect translation errors prior to correcting them. Hence, automatic quality estimation (QE) systems not only aim to estimate the post-editing effort at segment level to filter low quality translations (Specia et al, 2009), but also to detect the location and the nature of errors at word level (Ueffing and Ney, 2007; Bach et al., 2011). MT errors can be analysed as adequacy and fluency errors. While adequacy is concerned with how much of the source content and meaning is also expressed in the target text, fluency is concerned with to what extent the translation is well formed and adheres to the norms of the target language. The distinction between adequacy and 204 Tezkan et al. fluency has been used in different translation error taxonomies (Lommel et al., 2014; Daems, Macken and Vandepitte, 2014). Besides the difficulties of transferring source content and meaning to a target sentence, the task of produ"
W17-5218,N10-1122,0,0.0293077,"2 . For banking there are 22 such possible combinations, for retail 24 and for HR 23. Table 2 gives an overview of the three largest main categories per domain. In a next step, sentiment bearing words were selected, assigned a polarity: positive, negative or neutral (OpinEx), and linked to the appropriate aspect term (is about arrow). All annotations were carried out with the BRAT rapid annotation tool (Stenetorp et al., 2012). Datasets and Annotations In the past, ABSA datasets have been annotated comprising movie reviews (Thet et al., 2010), reviews for electronic products(Hu and Liu, 2004; Brody and Elhadad, 2010), and restaurant reviews (Brody and Elhadad, 2010; Ganu et al., 2009). As mentioned above, in the framework of three SemEval shared tasks (Pontiki et al., 2014, 2015, 2016), several benchmark review datasets coming from various domains (electronics, hotels, restaurants, and telecom) and languages (English, Dutch, French, Arabic, Chinese, Spanish, Turkish and Russian) have been made publicly available. For the work presented here, direct customer feedback data written in Dutch was collected in three domains: banking, retail and human resources (HR). The data provider for the first domain, banki"
W17-5218,S16-1044,0,0.0265928,"Missing"
W17-5218,L16-1465,1,0.851826,"Missing"
W17-5218,de-smedt-daelemans-2012-vreselijk,0,0.533722,"Missing"
W17-5218,S15-2082,0,0.0794152,"Missing"
W17-5218,S14-2004,0,0.548154,"Belgium 2 Hello Customer, Belgium {orphee.declercq, els.lefever, gillesm.jacobs, veronique.hoste}@ugent.be tijl@hellocustomer.com Abstract tems do not only try to distinguish the positive from the negative utterances, but also strive to detect the target of the opinion, which comes down to a very fine-grained sentiment analysis task and “almost all real-life sentiment analysis systems in industry should be based on this level of analysis” (Liu, 2015, p10). This fine-grained sentiment analysis task received special attention in the framework of three SemEval shared tasks: SemEval 2014 Task 4 (Pontiki et al., 2014) and SemEval 2015 Task 12 (Pontiki et al., 2015), which focussed on English customer reviews, and SemEval 2016 Task 5 (Pontiki et al., 2016) where seven other languages were also included. Each time the idea was to perform three subtasks: (i) extract all aspect expressions of the entities, (ii) categorize these aspect expressions into predefined categories and (iii) determine whether an opinion on an aspect is positive, negative or neutral. In this paper, we discuss a fine-grained sentiment analysis pipeline to deal with qualitative Dutch feedback data coming from three different domains: bank"
W17-5218,E09-1046,0,0.257749,".2 95.4 Retail F-1 94.9 93.2 93.4 95.2 Prec 89.6 95.6 91.0 95.8 Rec 90.9 95.5 91.4 95.8 HR F-1 89.2 95.6 89.7 95.8 Prec 95.2 94.9 96.5 95.9 Rec 95.4 95.1 96.8 96.2 F-1 95.0 94.4 96.4 95.9 Table 4: Precision, recall, and F-1 scores for aspect term extraction on held-out test sets. tem. As information sources, we implemented the following features: (1) bag-of-words: binary token unigram features, (2) lexicon lookup features based on domain-specific lexicons extracted from the training data, as well as existing sentiment lexicons for Dutch, i.e. Pattern (De Smedt and Daelemans, 2012) and Duoman (Jijkoun and Hofmann, 2009), (3) negator: flips the value of negated lexicon matches and (4) the predicted category of the aspect term. For these experiments, we also envisaged the three different setups: in-domain, crossdomain, and all domain. It is important to mention that for sentiment prediction, the entire sentence is considered for the construction of the features. As a result, conflicting sentiments will be ruled out. In future work, we intend to limit the context window of the detected aspect term. As the polarity detection takes into account the output of the previous two steps, this task was also evaluated by"
W17-5218,S15-2130,0,0.0186697,"ree characters as an approximate suffix; (2) lemma, (3) CGN part-ofspeech (PoS) tag, (4) syntactic chunk, and (5) Named Entity label as provided by the LeTs preprocessing toolkit (Van de Kauter et al., 2013). Both full labels and coarse super-category for PoS, chunk, and NE labels were included as features. 3.2 Aspect Category Classification The aspect category classification subtask requires a system able to label a large variety of classes, in our case 22, 24 and 23 categories. The two systems achieving the best results for SemEval 2015 both used a classification approach (Toh and Su, 2015; Saias, 2015). Furthermore, especially lexical features in the form of bag-of-words have proven successful. The best system (Toh and Su, 2015) also incorporated lexical-semantic features in the form of clusters learned from a large corpus of reference review data, whereas the secondbest (Saias, 2015) applied filtering heuristics on the classification output and thus solely relied on lexical information for the classification. For SemEval 2016 Toh and Su (2016) discovered that when the probability output of a Deep Convolutional Neural Network (Severyn and Moschitti, 2015) was added as additional features, t"
W17-5218,S15-2127,0,0.177309,"Missing"
W17-5218,S15-2079,0,0.0289324,"Missing"
W17-5218,E12-2021,0,0.0209746,"aspect term and assigned it to a predefined aspect category (CatEx). These aspect categories are domain-dependent and consist of a main category (e.g. Personnel) and subcategory (e.g. quality)2 . For banking there are 22 such possible combinations, for retail 24 and for HR 23. Table 2 gives an overview of the three largest main categories per domain. In a next step, sentiment bearing words were selected, assigned a polarity: positive, negative or neutral (OpinEx), and linked to the appropriate aspect term (is about arrow). All annotations were carried out with the BRAT rapid annotation tool (Stenetorp et al., 2012). Datasets and Annotations In the past, ABSA datasets have been annotated comprising movie reviews (Thet et al., 2010), reviews for electronic products(Hu and Liu, 2004; Brody and Elhadad, 2010), and restaurant reviews (Brody and Elhadad, 2010; Ganu et al., 2009). As mentioned above, in the framework of three SemEval shared tasks (Pontiki et al., 2014, 2015, 2016), several benchmark review datasets coming from various domains (electronics, hotels, restaurants, and telecom) and languages (English, Dutch, French, Arabic, Chinese, Spanish, Turkish and Russian) have been made publicly available. F"
W17-5218,S15-2083,0,0.0174104,"e final two and three characters as an approximate suffix; (2) lemma, (3) CGN part-ofspeech (PoS) tag, (4) syntactic chunk, and (5) Named Entity label as provided by the LeTs preprocessing toolkit (Van de Kauter et al., 2013). Both full labels and coarse super-category for PoS, chunk, and NE labels were included as features. 3.2 Aspect Category Classification The aspect category classification subtask requires a system able to label a large variety of classes, in our case 22, 24 and 23 categories. The two systems achieving the best results for SemEval 2015 both used a classification approach (Toh and Su, 2015; Saias, 2015). Furthermore, especially lexical features in the form of bag-of-words have proven successful. The best system (Toh and Su, 2015) also incorporated lexical-semantic features in the form of clusters learned from a large corpus of reference review data, whereas the secondbest (Saias, 2015) applied filtering heuristics on the classification output and thus solely relied on lexical information for the classification. For SemEval 2016 Toh and Su (2016) discovered that when the probability output of a Deep Convolutional Neural Network (Severyn and Moschitti, 2015) was added as addition"
W17-5218,S16-1045,0,0.0283281,"Missing"
W18-3101,W14-2907,0,0.0571157,"Missing"
W18-3101,W06-0901,0,0.140996,"calized. Thus, the need for flexible data-driven approaches, which do not require predefined ontological resources, arises. R¨onnqvist and Sarlin (2017) provide an example of successful datadriven, weakly-supervised distress event detection based on bank entity mentions. Here, bank distress events are conceptualized as mentions of bank entities in a time-window and no typology classification is assigned. We are not aware of any published data-driven, supervised event detection approaches for the economic domain. However, in general domain event extraction, as embodied by projects such as ACE (Ahn, 2006) and ERE/TACKBP (Mitamura et al., 2016), supervised methods for extraction of event structures are predominant because of their promise of improved performance. 2 As discussed in Sprugnoli and Tonelli (2017), the definition of events in the field of information extraction differs widely. In this work, we employ a conceptualization of economic event detection as ‘retrieving textually reported real-world occurrences, actions, relations, and situations involving companies and firms’. Unlike other supervised data-driven ‘event extraction’ tasks such as in the ACE/ERE programs (Aguilar et al., 2014"
W18-3101,L16-1051,1,0.876711,"014), we do not conceptualize events as structured schemata/frames, but more limited as textual mentions of real-world occurrences. The task presented here is often also referred to as event ‘mention’, ‘nugget’, or ‘trigger’ detection. The classification experiments described here are currently at the sentence-level, but our event annotation scheme is token-level. Data Description In this section, we describe the SentiFM economic event dataset collection and annotation. The annotated dataset consists of an English and Dutch news corpus. While in this paper the focus is on English, we refer to Lefever and Hoste (2016) for a pilot study on Dutch event detection and a description of the Dutch event data. A reference to where to download the SentiFM dataset can be found in Section 7. The goal of the SentiFM dataset is to enable supervised data-driven event detection in companyspecific economic news. For English, we downloaded articles from the newspaper The Financial Times using the ProQuest Newsstand by means of keyword-search. The keywords were manually determined based on a subsample of random articles as being indicative to one of the event types. All articles were published between November 2004 and Nove"
W18-3101,D14-1162,0,0.0875982,"ng layer which feeds into an LSTM block. The LSTM block is connected to an output layer with a sigmoid activation function. Bi-directionality of the LSTM-layer is tested in hyper-parameter optimization. We use the Adam optimization algorithm with binary cross-entropy loss function. The embedding layer turns positive integers, in our case hold-in set token indexes, in dense vectors with fixed dimensionality. An existing word embedding matrix can be used in the input-layer which tunes pre-trained word vectors. Three embedded inputs were tested with the multi-label set-up: 200 dimensional GloVe (Pennington et al., 2014) word vectors trained on the hold-in set, 300 dimensional GloVe vectors trained on a 6 billion token corpus of Wikipedia (2014) + Gigawords5B1 (henceforth, 6B corpus), and no pre-trained embeddings. The latter means our classifier trains embedded word-representations (with a fixed dimensionality of 200) itself based on the token sequences of the hold-in set. We evaluated our own GloVe models on an analogy quality assessment task provided with the word2vec source code2 . We picked the highest dimensional word vector model from the top ten ranking on the analogy task. We excluded lower dimension"
W18-3101,E12-2021,0,0.122022,"Missing"
W18-3101,P13-1086,0,0.332235,"enboom et al., 2013; Du et al., 2016). These use rule-sets or ontology knowledge-bases which are largely or fully created by hand. The Stock Sonar project (Feldman et al., 2011) notably uses domain experts to formulate event rules for rule-based stock sentiment analysis. This technology has been successfully used in assessing the impact of events on the stock market (Boudoukh et al., 2016) and in formulating trading strategies (Ben Ami and Feldman, 2017). Other approaches conceptualize economic event detection as the extraction of event tuples (Ding et al., 2015) or as semantic frame parsing (Xie et al., 2013). A drawback of knowledge-based information extraction methods is that creating rules and ontologies is a difficult, time-consuming process. Furthermore, defining a set of strict rules often reThis paper presents a dataset and supervised classification approach for economic event detection in English news articles. Currently, the economic domain is lacking resources and methods for data-driven supervised event detection. The detection task is conceived as a sentence-level classification task for 10 different economic event types. Two different machine learning approaches were tested: a rich fe"
