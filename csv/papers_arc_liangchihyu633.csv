2021.rocling-1.51,{ROCLING}-2021 Shared Task: Dimensional Sentiment Analysis for Educational Texts,2021,-1,-1,1,1,2441,liangchih yu,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"This paper presents the ROCLING 2021 shared task on dimensional sentiment analysis for educational texts which seeks to identify a real-value sentiment score of self-evaluation comments written by Chinese students in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm. Of the 7 teams registered for this shared task for two-dimensional sentiment analysis, 6 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques for the educational domain. All data sets with gold standards and scoring script are made publicly available to researchers."
2021.findings-acl.206,{MA}-{BERT}: Learning Representation by Incorporating Multi-Attribute Knowledge in Transformers,2021,-1,-1,3,0,8009,you zhang,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2020.rocling-1.22,An Adaptive Method for Building a {C}hinese Dimensional Sentiment Lexicon,2020,-1,-1,2,0,15607,yinglung lin,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.rocling-1.26,Sentiment Analysis for Investment Atmosphere Scoring,2020,-1,-1,2,0,15613,chihhsiang peng,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.rocling-1.32,Scientific Writing Evaluation Using Ensemble Multi-channel Neural Networks,2020,-1,-1,4,0,2403,yuhshyang wang,Proceedings of the 32nd Conference on Computational Linguistics and Speech Processing (ROCLING 2020),0,None
2020.aacl-main.4,Graph Attention Network with Memory Fusion for Aspect-level Sentiment Analysis,2020,-1,-1,3,0,15153,li yuan,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Aspect-level sentiment analysis(ASC) predicts each specific aspect term{'}s sentiment polarity in a given text or review. Recent studies used attention-based methods that can effectively improve the performance of aspect-level sentiment analysis. These methods ignored the syntactic relationship between the aspect and its corresponding context words, leading the model to focus on syntactically unrelated words mistakenly. One proposed solution, the graph convolutional network (GCN), cannot completely avoid the problem. While it does incorporate useful information about syntax, it assigns equal weight to all the edges between connected words. It may still incorrectly associate unrelated words to the target aspect through the iterations of graph convolutional propagation. In this study, a graph attention network with memory fusion is proposed to extend GCN{'}s idea by assigning different weights to edges. Syntactic constraints can be imposed to block the graph convolutional propagation of unrelated words. A convolutional layer and a memory fusion were applied to learn and exploit multiword relations and draw different weights of words to improve performance further. Experimental results on five datasets show that the proposed method yields better performance than existing methods."
D19-1343,Investigating Dynamic Routing in Tree-Structured {LSTM} for Sentiment Analysis,2019,0,3,2,0.205267,1794,jin wang,Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP),0,"Deep neural network models such as long short-term memory (LSTM) and tree-LSTM have been proven to be effective for sentiment analysis. However, sequential LSTM is a bias model wherein the words in the tail of a sentence are more heavily emphasized than those in the header for building sentence representations. Even tree-LSTM, with useful structural information, could not avoid the bias problem because the root node will be dominant and the nodes in the bottom of the parse tree will be less emphasized even though they may contain salient information. To overcome the bias problem, this study proposes a capsule tree-LSTM model, introducing a dynamic routing algorithm as an aggregation layer to build sentence representation by assigning different weights to nodes according to their contributions to prediction. Experiments on Stanford Sentiment Treebank (SST) for sentiment classification and EmoBank for regression show that the proposed method improved the performance of tree-LSTM and other neural network models. In addition, the deeper the tree structure, the bigger the improvement."
W17-5233,{YZU}-{NLP} at {E}mo{I}nt-2017: Determining Emotion Intensity Using a Bi-directional {LSTM}-{CNN} Model,2017,0,2,2,0,31557,yuanye he,"Proceedings of the 8th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"The EmoInt-2017 task aims to determine a continuous numerical value representing the intensity to which an emotion is expressed in a tweet. Compared to classification tasks that identify 1 among n emotions for a tweet, the present task can provide more fine-grained (real-valued) sentiment analysis. This paper presents a system that uses a bi-directional LSTM-CNN model to complete the competition task. Combining bi-directional LSTM and CNN, the prediction process considers both global information in a tweet and local important information. The proposed method ranked sixth among twenty-one teams in terms of Pearson Correlation Coefficient."
O17-1022,æç¨è©åéæ¼èªè¨æ¨£å¼æ¢åä¹ç ç©¶ (Mining Language Patterns Using Word Embeddings) [In {C}hinese],2017,0,0,3,0,32713,xiang xiao,Proceedings of the 29th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2017),0,None
I17-4002,{IJCNLP}-2017 Task 2: Dimensional Sentiment Analysis for {C}hinese Phrases,2017,0,0,1,1,2441,liangchih yu,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"This paper presents the IJCNLP 2017 shared task on Dimensional Sentiment Analysis for Chinese Phrases (DSAP) which seeks to identify a real-value sentiment score of Chinese single words and multi-word phrases in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm. Of the 19 teams registered for this shared task for two-dimensional sentiment analysis, 13 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques, especially for Chinese affective computing. All data sets with gold standards and scoring script are made publicly available to researchers."
I17-4025,{S}enti{NLP} at {IJCNLP}-2017 Task 4: Customer Feedback Analysis Using a {B}i-{LSTM}-{CNN} Model,2017,0,2,3,0,32818,shuying lin,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"The analysis of customer feedback is useful to provide good customer service. There are a lot of online customer feedback are produced. Manual classification is impractical because the high volume of data. Therefore, the automatic classification of the customer feedback is of importance for the analysis system to identify meanings or intentions that the customer express. The aim of shared Task 4 of IJCNLP 2017 is to classify the customer feedback into six tags categorization. In this paper, we present a system that uses word embeddings to express the feature of the sentence in the corpus and the neural network as the classifier to complete the shared task. And then the ensemble method is used to get final predictive result. The proposed method get ranked first among twelve teams in terms of micro-averaged F1 and second for accura-cy metric."
D17-1056,Refining Word Embeddings for Sentiment Analysis,2017,3,41,1,1,2441,liangchih yu,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Word embeddings that can capture semantic and syntactic information from contexts have been extensively used for various natural language processing tasks. However, existing methods for learning context-based word embeddings typically fail to capture sufficient sentiment information. This may result in words with similar vector representations having an opposite sentiment polarity (e.g., good and bad), thus degrading sentiment analysis performance. Therefore, this study proposes a word vector refinement model that can be applied to any pre-trained word vectors (e.g., Word2vec and GloVe). The refinement model is based on adjusting the vector representations of words such that they can be closer to both semantically and sentimentally similar words and further away from sentimentally dissimilar words. Experimental results show that the proposed method can improve conventional word embeddings and outperform previously proposed sentiment embeddings for both binary and fine-grained classification on Stanford Sentiment Treebank (SST)."
W16-4906,Overview of {NLP}-{TEA} 2016 Shared Task for {C}hinese Grammatical Error Diagnosis,2016,0,3,3,1,1205,lunghao lee,Proceedings of the 3rd Workshop on Natural Language Processing Techniques for Educational Applications ({NLPTEA}2016),0,"This paper presents the NLP-TEA 2016 shared task for Chinese grammatical error diagnosis which seeks to identify grammatical error types and their range of occurrence within sentences written by learners of Chinese as foreign language. We describe the task definition, data preparation, performance metrics, and evaluation results. Of the 15 teams registered for this shared task, 9 teams developed the system and submitted a total of 36 runs. We expected this evaluation campaign could lead to the development of more advanced NLP techniques for educational applications, especially for Chinese error detection. All data sets with gold standards and scoring scripts are made publicly available to researchers."
W16-0513,The {NTNU}-{YZU} System in the {AESW} Shared Task: Automated Evaluation of Scientific Writing Using a Convolutional Neural Network,2016,26,1,3,1,1205,lunghao lee,Proceedings of the 11th Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This study describes the design of the NTNU-YZU system for the automated evaluation of scientific writing shared task. We employ a convolutional neural network with the Word2Vec/GloVe embedding representation to predict whether a sentence needs language editing. For the Boolean prediction track, our best F-score of 0.6108 ranked second among the ten submissions. Our system also achieved an F-score of 0.7419 for the probabilistic estimation track, ranking fourth among the nine submissions."
S16-1039,{YZU}-{NLP} Team at {S}em{E}val-2016 Task 4: Ordinal Sentiment Classification Using a Recurrent Convolutional Network,2016,13,3,2,0,34235,yunchao he,Proceedings of the 10th International Workshop on Semantic Evaluation ({S}em{E}val-2016),0,None
P16-2037,Dimensional Sentiment Analysis Using a Regional {CNN}-{LSTM} Model,2016,26,96,2,0.205267,1794,jin wang,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Dimensional sentiment analysis aims to recognize continuous numerical values in multiple dimensions such as the valencearousal (VA) space. Compared to the categorical approach that focuses on sentiment classification such as binary classification (i.e., positive and negative), the dimensional approach can provide more fine-grained sentiment analysis. This study proposes a regional CNN-LSTM model consisting of two parts: regional CNN and LSTM to predict the VA ratings of texts. Unlike a conventional CNN which considers a whole text as input, the proposed regional CNN uses an individual sentence as a region, dividing an input text into several regions such that the useful affective information in each region can be extracted and weighted according to their contribution to the VA prediction. Such regional information is sequentially integrated across regions using LSTM for VA prediction. By combining the regional CNN and LSTM, both local (regional) information within sentences and long-distance dependency across sentences can be considered in the prediction process. Experimental results show that the proposed method outperforms lexicon-based, regression-based, and NN-based methods proposed in previous studies."
N16-1066,Building {C}hinese Affective Resources in Valence-Arousal Dimensions,2016,17,48,1,1,2441,liangchih yu,Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,None
W15-4401,Overview of the {NLP}-{TEA} 2015 Shared Task for {C}hinese Grammatical Error Diagnosis,2015,1,10,2,1,1205,lunghao lee,Proceedings of the 2nd Workshop on Natural Language Processing Techniques for Educational Applications,0,"This paper introduces the NLP-TEA 2015 shared task for Chinese grammatical error diagnosis. We describe the task, data preparation, performance metrics, and evaluation results. The hope is that such an evaluation campaign may produce more advanced Chinese grammatical error diagnosis techniques. All data sets with gold standards and evaluation tools are publicly available for research purposes."
P15-2129,Predicting Valence-Arousal Ratings of Words Using a Weighted Graph Method,2015,26,18,1,1,2441,liangchih yu,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Compared to the categorical approach that represents affective states as several discrete classes (e.g., positive and negative), the dimensional approach represents affective states as continuous numerical values on multiple dimensions, such as the valence-arousal (VA) space, thus allowing for more fine-grained sentiment analysis. In building dimensional sentiment applications, affective lexicons with valence-arousal ratings are useful resources but are still very rare. Therefore, this study proposes a weighted graph model that considers both the relations of multiple nodes and their similarities as weights to automatically determine the VA ratings of affective words. Experiments on both English and Chinese affective lexicons show that the proposed method yielded a smaller error rate on VA prediction than the linear regression, kernel method, and pagerank algorithm used in previous studies."
O15-2001,Guest Editoral: Special Issue on {C}hinese as a Foreign Language,2015,10,1,2,1,1205,lunghao lee,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 20, Number 1, June 2015-Special Issue on {C}hinese as a Foreign Language",0,"This introduction paper describes the research trends of Chinese as a second/foreign language along with related studies. We also overview the research papers included in this special issue. Finally, we conclude the findings and offer the suggestions."
W14-6820,Overview of {SIGHAN} 2014 Bake-off for {C}hinese Spelling Check,2014,4,16,1,1,2441,liangchih yu,Proceedings of The Third {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper introduces a Chinese Spelling Check campaign organized for the SIGHAN 2014 bake-off, including task description, data preparation, performance metrics, and evaluation results based on essays written by Chinese as a foreign language learners. The hope is that such evaluations can produce more advanced Chinese spelling check techniques."
C14-2015,A Sentence Judgment System for Grammatical Error Detection,2014,7,11,2,1,1205,lunghao lee,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: System Demonstrations",0,"This study develops a sentence judgment system using both rule-based and n-gram statistical methods to detect grammatical errors in Chinese sentences. The rule-based method provides 142 rules developed by linguistic experts to identify potential rule violations in input sentences. The n-gram statistical method relies on the n-gram scores of both correct and incorrect training sentences to determine the correctness of the input sentences, providing learners with improved understanding of linguistic rules and n-gram frequencies."
C14-1080,Identifying Emotion Labels from Psychiatric Social Texts Using Independent Component Analysis,2014,27,6,1,1,2441,liangchih yu,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Accessing the web has been an efficient and effective means to acquire self-help knowledge when suf- fering from depressive problems. Many mental health websites have developed community-based ser- vices such as web forums and blogs for Internet users to share their depressive problems with other us- ers and health professionals. Other users or health professionals can then make recommendations in re- sponse to these problems. Such communications produce a large number of documents called psychiat- ric social texts containing rich emotion labels representing different depressive problems. Automatically identify such emotion labels can make online psychiatric services more effective. This study proposes a framework combining latent semantic analysis (LSA) and independent component analysis (ICA) to ex- tract concept-level features for emotion label identification. LSA is used to discover latent concepts that do not frequently occur in psychiatric social texts, and ICA is used to extract independent components by minimizing the term dependence among the concepts. By combining LSA and ICA, more useful la- tent concepts can be discovered for different emotion labels, and the dependence between them can also be minimized. The discriminant power of classifiers can thus be improved by training them on the inde- pendent components with minimized term overlap. Experimental results show that the use of concept- level features yielded better performance than the use of word-level features. Additionally, combining LSA and ICA improved the performance of using each LSA and ICA alone."
W13-4420,Candidate Scoring Using Web-Based Measure for {C}hinese Spelling Error Correction,2013,13,6,1,1,2441,liangchih yu,Proceedings of the Seventh {SIGHAN} Workshop on {C}hinese Language Processing,0,"Chinese character correction involves two major steps: 1) Providing candidate corrections for all or partially identified characters in a sentence, and 2) Scoring all altered sentences and identifying which is the best corrected sentence. In this paper a web-based measure is used to score candidate sentences, in which there exists one continuous error character in a sentence in almost all sentences in the Bakeoff corpora. The approach of using a web-based measure can be applied directly to sentences with multiple error characters, either consecutive or not, and is not optimized for one-character error correction of Chinese sentences. The results show that the approach achieved a fair precision score whereas the recall is low compared to results reported in this Bakeoff."
W12-6303,A Language Modeling Approach to Identifying Code-Switched Sentences and Words,2012,19,3,1,1,2441,liangchih yu,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"Globalization and multilingualism contribute to code-switching xe2x80x93 the phenomenon in which speakers produce utterances containing words or expressions from a second language. Processing code-switched sentences is a significant challenge for multilingual intelligent systems. This study proposes a language modeling approach to the problem of codeswitching language processing, dividing the problem into two subtasks: the detection of code-switched sentences and the identification of code-switched words in sentences. A codeswitched sentence is detected on the basis of whether it contains words or phrases from another language. Once the code-switched sentences are identified, the positions of the code-switched words in the sentences are then identified. Experimental results on MandarinTaiwanese code-switching sentences show that the language modeling approach achieved a 79.52% F-measure and an accuracy of 80.23% for detecting code-switched sentences, and a 51.20% F-measure for the identification of code-switched words."
W12-6335,Traditional {C}hinese Parsing Evaluation at {SIGHAN} Bake-offs 2012,2012,8,4,3,0,2391,yuenhsien tseng,Proceedings of the Second {CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"This paper presents the overview of traditional Chinese parsing task at SIGHAN Bake-offs 2012. On behalf of task organizers, we describe all aspects of the task for traditional Chinese parsing, i.e., task description, data preparation, performance metrics, and evaluation results. We summarize the performance results of all participant teams in this evaluation, in the hope to encourage more future studies on traditional Chinese parsing"
O12-1017,æç¨è·³è«èªè¨æ¨¡åæ¼åç¾©è©åä»£ä¹ç ç©¶ (Skip N-gram Modeling for Near-Synonym Choice) [In {C}hinese],2012,-1,-1,4,0,42767,shihting chen,Proceedings of the 24th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2012),0,None
C12-3064,Developing and Evaluating a Computer-Assisted Near-Synonym Learning System,2012,16,1,1,1,2441,liangchih yu,Proceedings of {COLING} 2012: Demonstration Papers,0,"Despite their similar meanings, near-synonyms may have different usages in different contexts. For second language learners, such differences are not easily grasped in practical use. In this paper, we develop a computer-assisted near-synonym learning system for Chinese English-as-aSecond-Language (ESL) learners using two automatic near-synonym choice techniques: pointwise mutual information (PMI) and n-grams. The two techniques can provide useful contextual information for learners, making it easier for them to understand different usages of various English near-synonyms in a range of contexts. The system is evaluated using a vocabulary test with near-synonyms as candidate choices. Participants are required to select the best near-synonym for each question both with and without use of the system. Experimental results show that both techniques can improve participantsxe2x80x99 ability to discriminate among nearsynonyms. In addition, participants are found to prefer to use the PMI in the test, despite n-grams providing more precise information."
O11-2013,å¤èªèªç¢¼è½æä¹æªç¥è©æ·å (Unknown Word Extraction from Multilingual Code-Switching Sentences) [In {C}hinese],2011,0,0,5,0,44721,yilun wu,{ROCLING} 2011 Poster Papers,0,None
I11-1155,A Baseline System for {C}hinese Near-Synonym Choice,2011,20,5,1,1,2441,liangchih yu,Proceedings of 5th International Joint Conference on Natural Language Processing,0,"Near-synonym sets represent groups of words with similar meaning, which are useful knowledge resources for many natural language applications such as query expansion for information retrieval (IR) and computer-assisted language learning. However, near-synonyms are not necessarily interchangeable in contexts due to their specific usage and syntactic constraints. Previous studies have developed various methods for near-synonym choice in English sentences. To our best knowledge, there is no such evaluation on Chinese sentences. Therefore, this paper implements two baseline systems: pointwise mutual information (PMI) and a 5-gram language model that are widely used in previous work for Chinese nearsynonym choice evaluation. Experimental results show that the 5-gram language model achieves higher accuracy than PMI."
O10-5002,Word Sense Disambiguation Using Multiple Contextual Features,2010,33,0,1,1,2441,liangchih yu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 15, Number 3-4, September/{D}ecember 2010",0,"Word sense disambiguation (WSD) is a technique used to identify the correct sense of polysemous words, and it is useful for many applications, such as machine translation (MT), lexical substitution, information retrieval (IR), and biomedical applications. In this paper, we propose the use of multiple contextual features, including the predicate-argument structure and named entities, to train two commonly used classifiers, Naive Bayes (NB) and Maximum Entropy (ME), for word sense disambiguation. Experiments are conducted to evaluate the classifiers' performance on the OntoNotes corpus and are compared with classifiers trained using a set of baseline features, such as the bag-of-words, n-grams, and part-of-speech (POS) tags. Experimental results show that incorporating both predicate-argument structure and named entities yields higher classification accuracy for both classifiers than does the use of the baseline features, resulting in accuracy as high as 81.6% and 87.4%, respectively, for NB and ME."
C10-1141,Discriminative Training for Near-Synonym Substitution,2010,20,8,1,1,2441,liangchih yu,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Near-synonyms are useful knowledge resources for many natural language applications such as query expansion for information retrieval (IR) and paraphrasing for text generation. However, near-synonyms are not necessarily interchangeable in contexts due to their specific usage and syntactic constraints. Accordingly, it is worth to develop algorithms to verify whether near-synonyms do match the given contexts. In this paper, we consider the near-synonym substitution task as a classification task, where a classifier is trained for each near-synonym set to classify test examples into one of the near-synonyms in the set. We also propose the use of discriminative training to improve classifiers by distinguishing positive and negative features for each near-synonym. Experimental results show that the proposed method achieves higher accuracy than both pointwise mutual information (PMI) and n-gram-based methods that have been used in previous studies."
P09-2051,Mining Association Language Patterns for Negative Life Event Classification,2009,11,9,1,1,2441,liangchih yu,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Negative life events, such as death of a family member, argument with a spouse and loss of a job, play an important role in triggering depressive episodes. Therefore, it is worth to develop psychiatric services that can automatically identify such events. In this paper, we propose the use of association language patterns, i.e., meaningful combinations of words (e.g., ), as features to classify sentences with negative life events into predefined categories (e.g., Family, Love, Work). The language patterns are discovered using a data mining algorithm, called association pattern mining, by incrementally associating frequently co-occurred words in the sentences annotated with negative life events. The discovered patterns are then combined with single words to train classifiers. Experimental results show that association language patterns are significant features, thus yielding better performance than the baseline system using single words alone."
O08-6002,Corpus Cleanup of Mistaken Agreement Using Word Sense Disambiguation,2008,30,0,1,1,2441,liangchih yu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 13, Number 4, {D}ecember 2008",0,"Word sense annotated corpora are useful resources for many text mining applications. Such corpora are only useful if their annotations are consistent. Most large-scale annotation efforts take special measures to reconcile inter-annotator disagreement. To date, however, nobody has investigated how to automatically determine exemplars in which the annotators agree but are wrong. In this paper, we use OntoNotes, a large-scale corpus of semantic annotations, including word senses, predicate-argument structure, ontology linking, and coreference. To determine the mistaken agreements in word sense annotation, we employ word sense disambiguation (WSD) to select a set of suspicious candidates for human evaluation. Experiments are conducted from three aspects (precision, cost-effectiveness ratio, and entropy) to examine the performance of WSD. The experimental results show that WSD is most effective in identifying erroneous annotations for highly-ambiguous words, while a baseline is better for other cases. The two methods can be combined to improve the cleanup process. This procedure allows us to find approximately 2% of the remaining erroneous agreements in the OntoNotes corpus. A similar procedure can be easily defined to check other annotated corpora."
C08-1133,{O}nto{N}otes: Corpus Cleanup of Mistaken Agreement Using Word Sense Disambiguation,2008,20,5,1,1,2441,liangchih yu,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Annotated corpora are only useful if their annotations are consistent. Most large-scale annotation efforts take special measures to reconcile inter-annotator disagreement. To date, however, no-one has investigated how to automatically determine exemplars in which the annotators agree but are wrong. In this paper, we use OntoNotes, a large-scale corpus of semantic annotations, including word senses, predicate-argument structure, ontology linking, and coreference. To determine the mistaken agreements in word sense annotation, we employ word sense disambiguation (WSD) to select a set of suspicious candidates for human evaluation. Experiments are conducted from three aspects (precision, cost-effectiveness ratio, and entropy) to examine the performance of WSD. The experimental results show that WSD is most effective on identifying erroneous annotations for highly-ambiguous words, while a baseline is better for other cases. The two methods can be combined to improve the cleanup process. This procedure allows us to find approximately 2% remaining erroneous agreements in the OntoNotes corpus. A similar procedure can be easily defined to check other annotated corpora."
P07-1129,Topic Analysis for Psychiatric Document Retrieval,2007,14,3,1,1,2441,liangchih yu,Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics,1,"Psychiatric document retrieval attempts to help people to efficiently and effectively locate the consultation documents relevant to their depressive problems. Individuals can understand how to alleviate their symptoms according to recommendations in the relevant documents. This work proposes the use of high-level topic information extracted from consultation documents to improve the precision of retrieval results. The topic information adopted herein includes negative life events, depressive symptoms and semantic relations between symptoms, which are beneficial for better understanding of users' queries. Experimental results show that the proposed approach achieves higher precision than the word-based retrieval models, namely the vector space model (VSM) and Okapi model, adopting word-level information alone."
P06-2121,{HAL}-Based Cascaded Model for Variable-Length Semantic Pattern Induction from Psychiatry Web Resources,2006,16,1,1,1,2441,liangchih yu,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Negative life events play an important role in triggering depressive episodes. Developing psychiatric services that can automatically identify such events is beneficial for mental health care and prevention. Before these services can be provided, some meaningful semantic patterns, such as , have to be extracted. In this work, we present a text mining framework capable of inducing variable-length semantic patterns from unannotated psychiatry web resources. This framework integrates a cognitive motivated model, Hyperspace Analog to Language (HAL), to represent words as well as combinations of words. Then, a cascaded induction process (CIP) bootstraps with a small set of seed patterns and incorporates relevance feedback to iteratively induce more relevant patterns. The experimental results show that by combining the HAL model and relevance feedback, the CIP can induce semantic patterns from the unannotated web corpora so as to reduce the reliance on annotated corpora."
O05-2003,Automated Alignment and Extraction of a Bilingual Ontology for Cross-Language Domain-Specific Applications,2005,0,3,4,0,31442,juifeng yeh,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 1, March 2005",0,None
W04-1110,Automated Alignment and Extraction of Bilingual Domain Ontology for Medical Domain Web Search,2004,15,2,4,0,31442,juifeng yeh,Proceedings of the Third {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper proposes an approach to automated ontology alignment and domain ontology extraction from two knowledge bases. First, WordNet and HowNet knowledge bases are aligned to construct a bilingual universal ontology based on the co-occurrence of the words in a parallel corpus. The bilingual universal ontology has the merit that it contains more structural and semantic information coverage from two complementary knowledge bases, WordNet and HowNet. For domain-specific applications, a medical domain ontology is further extracted from the universal ontology using the islanddriven algorithm and a medical domain corpus. Finally, the domain-dependent terms and some axioms between medical terms based on a medical encyclopaedia are added into the ontology. For ontology evaluation, experiments on web search were conducted using the constructed ontology. The experimental results show that the proposed approach can automatically align and extract the domain-specific ontology. In addition, the extracted ontology also shows its promising ability for medical web search."
C04-1164,Automated Alignment and Extraction of Bilingual Domain Ontology for Cross-Language Domain-Specific Applications,2004,14,15,4,0,31442,juifeng yeh,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"In this paper we propose a novel approach for ontology alignment and domain ontology extraction from the existing knowledge bases, WordNet and HowNet. These two knowledge bases are aligned to construct a bilingual ontology based on the cooccurrence of the words in the sentence pairs of a parallel corpus. The bilingual ontology has the merit that it contains more structural and semantic information coverage from these two complementary knowledge bases. For domainspecific applications, the domain specific ontology is further extracted from the bilingual ontology by the island-driven algorithm and the domain-specific corpus. Finally, the domain-dependent terminologies and some axioms between domain terminologies are integrated into the ontology. For ontology evaluation, experiments were conducted by comparing the benchmark constructed by the ontology engineers or experts. The experimental results show that the proposed approach can extract an aligned bilingual domain-specific ontology."
