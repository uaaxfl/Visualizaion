2007.mtsummit-papers.9,J05-4005,1,0.874874,"Missing"
2007.mtsummit-papers.9,H05-1061,0,0.220125,"ic terms. It has been reported This paper describes a system that automatically mines EnglishChinese translation pairs from large amount of monolingual to achieve high precision but low coverage since anchor Chinese web pages. Our approach is motivated by the texts own only a small percentage in web pages. observation that many Chinese terms (e.g., named entities that Other methods leveraged the quick response of search are not stored in a conventional dictionary) are accompanied by engines (Zhang and Vines, 2004; Cheng et al., 2004; their English translations in the Chinese web pages. In our Huang et al., 2005; Kuo et al., 2006). In these approaches, approach, candidate translations are extracted using pre-defined one side of a translation pair (say English terms) is given, templates. Transliterations and translation pairs are then and search engines are used to find the other side of the identified using statistical learning methods. We compare translation pair (say Chinese terms) from the Web. several approaches to aligning transliterations and mining Though interesting, these approaches are not feasible to translations on more than 300GB Chinese web pages. In our build very large bilingual dicti"
2007.mtsummit-papers.9,W02-1001,0,0.219164,"itional bilingual dictionaries. To our knowledge, it is the first attempt to mining bilingual dictionary from such a large quantity of Web pages. The remaining of the paper is organized as follows: in section 2, we will describe the architecture of the system. We give a brief introduction to the three major modules: data pre-processing, transliteration alignment and translation selection. The next three sections will provide the details of the three modules respectively. Section 3 presents the algorithms for pre-processing; section 4 describes a binary classifier based on averaged perceptron (Collins, 2002) to determine transliterations. The translation selection module is described in section 5. Section 6 presents the experiments using the mined dictionary. Section 7 concludes the paper. 2. Architecture of the Dictionary Mining System The system of mining bilingual dictionary consists of three components: pre-processing, transliteration alignment, and translation selection. The pre-processing module bridges the monolingual Chinese web pages and the translation selection module. The pre-processing module filters the HTML tags, normalizes character coding,, extracts translation segments based on"
2007.mtsummit-papers.9,P98-2220,0,0.0408706,"advantages: 1) it reduces the number of possible pairs; 2) it can improve the precision of translation selection. Hereafter, we call each of the above pair an instance (for our learning process). The correct pair is a positive instance; otherwise, a negative instance. As many of the translation candidates concern named entities, it is important to handle transliteration. In the previous studies, various types of information, such as alignment probability and usage of special Chinese character for transliteration, have been considered. However, the information was combined in a heuristic way (Wan and Verspoor, 1998; Gao et al., 2004). In our work, we formulate the transliteration alignment as a binary classification problem. A more principled discriminative training framework is proposed to combine different types of information in a systemic way. Our method is more scalable than previous approaches mentioned earlier and can be applied on larger data sets. As a matter of fact, our approach has been applied to a large set of Chinese Web pages, which amounts to more than 300GB. We expect to extract far more translation pairs than those in previous work. Another advantage of our method is that it does not"
2020.acl-demos.16,N19-1423,0,0.603845,"To enable efficient production deployment, MT-DNN supports multitask knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pretrained models will be publicly available at https://github.com/namisan/mt-dnn. 1 Introduction NLP model development has observed a paradigm shift in recent years, due to the success in using pretrained language models to improve a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019). Unlike the traditional pipeline approach that conducts annotation in stages using primarily supervised learning, the new paradigm features a universal pretraining stage that trains a large neural language model via self-supervision on a large unlabeled text corpus, followed by a fine-tuning step that starts from the pretrained contextual representations and conducts supervised learning for ∗ Equal Contribution. The complete name of our toolkit is M T 2 -DNN (The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding), but we use MT-DNN for sake of simplicity."
2020.acl-demos.16,I05-5002,0,0.0420279,"ummary of the four benchmarks: GLUE, SNLI, SciTail and ANLI. Model MNLI RTE QNLI SST MRPC Acc Acc Acc Acc F1 BERT 84.5 63.5 91.1 92.9 89.0 BERT + MTL 85.3 79.1 91.5 93.6 89.2 BERT + AdvTrain 85.6 71.2 91.6 93.0 91.3 Table 2: Comparison among single task, multi-Task and adversarial training on MNLI, RTE, QNLI, SST and MPRC in GLUE. Model BERTLARGE (Nie et al., 2019) RoBERTaLARGE (Nie et al., 2019) RoBERTa-LARGE + AdvTrain Dev 49.3 53.7 57.1 Test 44.2 49.7 57.1 Table 3: Results in terms of accuracy on the ANLI. sis (Socher et al., 2013), text similarity (Cer et al., 2017), paraphrase detection (Dolan and Brockett, 2005), and natural language inference (NLI) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009; Levesque et al., 2012; Williams et al., 2018). The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models. • SNLI. The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated (Bowman et al., 2015). This is the most widely used entailment dataset for NLI. • S"
2020.acl-demos.16,W18-2501,0,0.0440435,"Missing"
2020.acl-demos.16,W07-1401,0,0.0447164,"BERT 84.5 63.5 91.1 92.9 89.0 BERT + MTL 85.3 79.1 91.5 93.6 89.2 BERT + AdvTrain 85.6 71.2 91.6 93.0 91.3 Table 2: Comparison among single task, multi-Task and adversarial training on MNLI, RTE, QNLI, SST and MPRC in GLUE. Model BERTLARGE (Nie et al., 2019) RoBERTaLARGE (Nie et al., 2019) RoBERTa-LARGE + AdvTrain Dev 49.3 53.7 57.1 Test 44.2 49.7 57.1 Table 3: Results in terms of accuracy on the ANLI. sis (Socher et al., 2013), text similarity (Cer et al., 2017), paraphrase detection (Dolan and Brockett, 2005), and natural language inference (NLI) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009; Levesque et al., 2012; Williams et al., 2018). The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models. • SNLI. The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated (Bowman et al., 2015). This is the most widely used entailment dataset for NLI. • SciTail This is a textual entailment dataset derived from a science question answering (SciQ) dataset (Khot"
2020.acl-demos.16,P84-1044,0,0.129197,"Missing"
2020.acl-demos.16,N15-1092,1,0.751,"l., 2019; Liu et al., 2019b). Fine-tuning Once the text encoder is trained in the pre-training stage, an additional task-specific layer is usually added for fine-tuning based on the downstream task. Besides the existing typical single-task fine-tuning, MT-DNN facilitates a joint fine-tuning with a configurable list of related tasks in a MTL fashion. By encoding task-relatedness and sharing underlying text representations, MTL is a powerful training paradigm that promotes the model generalization ability and results in improved performance (Caruana, 1997; Liu et al., 2019b; Luong et al., 2015; Liu et al., 2015; Ruder, 2017; Collobert et al., 2011). Additionally, a two-step fine-tuning stage is also supported to utilize datasets from related tasks, i.e. a single-task fine-tuning following a multi-task fine-tuning. It also supports two popular sampling strategies in MTL training: 1) sampling tasks uniformly (Caruana, 1997; Liu et al., 2015); 2) sampling tasks based on the size of the dataset (Liu et al., 2019b). This makes it easy to explore various ways to feed training data to MTL training. Finally, to further improve the model robustness, MT-DNN also offers a recipe to apply adversarial training ("
2020.acl-demos.16,P19-1441,1,0.929166,"an effectively model textual variations and distributional similarity. Therefore, they can make subsequent task-specific training more sample efficient and often significantly boost performance in downstream tasks. However, these models are quite large and pose significant challenges to production deployment that has stringent memory or speed requirements. As a result, knowledge distillation has become another key feature in this new learning paradigm. An effective distillation step can often substantially compress a large model for efficient deployment (Clark et al., 2019; Tang et al., 2019; Liu et al., 2019a). In the NLP community, there are several well designed frameworks for research and commercial purposes, including toolkits for providing conventional layered linguistic annotations (Manning et al., 2014), platforms for developing novel neural models (Gardner et al., 2018) and systems for neural machine translation (Ott et al., 2019). However, it is hard to find an existing tool that supports all features in the new paradigm and can be easily customized for new tasks. For example, (Wolf et al., 2019) provides a number of popular Transformerbased (Vaswani et al., 2017) text encoders in a nice"
2020.acl-demos.16,P18-1157,1,0.845195,"customize their own encoders. For example, one can design an encoder with few Transformer layers (e.g. 3 layers) to distill knowledge from the BERT large model (24 layers), so that they can deploy this small mode online to meet the latency restriction as shown in Figure 2. Task-Specific Output Layers: We can incorporate arbitrary natural language tasks, each with its task-specific output layer. For example, we implement the output layers as a neural decoder for a neural ranker for relevance ranking, a logistic regression for text classification, and so on. A multistep reasoning decoder, SAN (Liu et al., 2018a,b) is also provided. Customers can choose from existing task-specific output layer or implement new one by themselves. 3 Application In this section, we present a comprehensive set of examples to illustrate how to customize MTDNN for new tasks. We use popular benchmarks from general and biomedical domains, including GLUE (Wang et al., 2018), SNLI (Bowman et al., 2015), SciTail (Khot et al., 2018), SQuAD (Rajpurkar et al., 2016), ANLI (Nie et al., 2019), and biomedical named entity recognition (NER), relation extraction (RE) and question answering (QA) (Lee et al., 2019). To make the experime"
2020.acl-demos.16,2021.ccl-1.108,0,0.195475,"Missing"
2020.acl-demos.16,P14-5010,0,0.00631447,"stream tasks. However, these models are quite large and pose significant challenges to production deployment that has stringent memory or speed requirements. As a result, knowledge distillation has become another key feature in this new learning paradigm. An effective distillation step can often substantially compress a large model for efficient deployment (Clark et al., 2019; Tang et al., 2019; Liu et al., 2019a). In the NLP community, there are several well designed frameworks for research and commercial purposes, including toolkits for providing conventional layered linguistic annotations (Manning et al., 2014), platforms for developing novel neural models (Gardner et al., 2018) and systems for neural machine translation (Ott et al., 2019). However, it is hard to find an existing tool that supports all features in the new paradigm and can be easily customized for new tasks. For example, (Wolf et al., 2019) provides a number of popular Transformerbased (Vaswani et al., 2017) text encoders in a nice unified interface, but does not offer multitask learning or adversarial training, state-of-the-art techniques that have been shown to significantly improve performance. Additionally, most public frameworks"
2020.acl-demos.16,N19-4009,0,0.0311237,"or speed requirements. As a result, knowledge distillation has become another key feature in this new learning paradigm. An effective distillation step can often substantially compress a large model for efficient deployment (Clark et al., 2019; Tang et al., 2019; Liu et al., 2019a). In the NLP community, there are several well designed frameworks for research and commercial purposes, including toolkits for providing conventional layered linguistic annotations (Manning et al., 2014), platforms for developing novel neural models (Gardner et al., 2018) and systems for neural machine translation (Ott et al., 2019). However, it is hard to find an existing tool that supports all features in the new paradigm and can be easily customized for new tasks. For example, (Wolf et al., 2019) provides a number of popular Transformerbased (Vaswani et al., 2017) text encoders in a nice unified interface, but does not offer multitask learning or adversarial training, state-of-the-art techniques that have been shown to significantly improve performance. Additionally, most public frameworks do not offer knowledge distillation. A notable exception is DistillBERT (Sanh et al., 2019), but it provides a standalone compress"
2020.acl-demos.16,N18-1202,0,0.408136,"k learning paradigm. To enable efficient production deployment, MT-DNN supports multitask knowledge distillation, which can substantially compress a deep neural model without significant performance drop. We demonstrate the effectiveness of MT-DNN on a wide range of NLU applications across general and biomedical domains. The software and pretrained models will be publicly available at https://github.com/namisan/mt-dnn. 1 Introduction NLP model development has observed a paradigm shift in recent years, due to the success in using pretrained language models to improve a wide range of NLP tasks (Peters et al., 2018; Devlin et al., 2019). Unlike the traditional pipeline approach that conducts annotation in stages using primarily supervised learning, the new paradigm features a universal pretraining stage that trains a large neural language model via self-supervision on a large unlabeled text corpus, followed by a fine-tuning step that starts from the pretrained contextual representations and conducts supervised learning for ∗ Equal Contribution. The complete name of our toolkit is M T 2 -DNN (The Microsoft Toolkit of Multi-Task Deep Neural Networks for Natural Language Understanding), but we use MT-DNN f"
2020.acl-demos.16,D16-1264,0,0.347892,"he output layers as a neural decoder for a neural ranker for relevance ranking, a logistic regression for text classification, and so on. A multistep reasoning decoder, SAN (Liu et al., 2018a,b) is also provided. Customers can choose from existing task-specific output layer or implement new one by themselves. 3 Application In this section, we present a comprehensive set of examples to illustrate how to customize MTDNN for new tasks. We use popular benchmarks from general and biomedical domains, including GLUE (Wang et al., 2018), SNLI (Bowman et al., 2015), SciTail (Khot et al., 2018), SQuAD (Rajpurkar et al., 2016), ANLI (Nie et al., 2019), and biomedical named entity recognition (NER), relation extraction (RE) and question answering (QA) (Lee et al., 2019). To make the experiments reproducible, we make all the configuration files publicly available. We also provide a quick guide for customizing a new task in Jupyter notebooks. 3.1 General Domain Natural Language Understanding Benchmarks • GLUE. The General Language Understanding Evaluation (GLUE) benchmark is a collection of nine natural language understanding (NLU) tasks. As shown in Table 1, it includes question answering (Rajpurkar et al., 2016), li"
2020.acl-demos.16,D13-1170,0,0.0317297,"ication NLI Classification NLI Classification MRC Span Classification Table 1: Summary of the four benchmarks: GLUE, SNLI, SciTail and ANLI. Model MNLI RTE QNLI SST MRPC Acc Acc Acc Acc F1 BERT 84.5 63.5 91.1 92.9 89.0 BERT + MTL 85.3 79.1 91.5 93.6 89.2 BERT + AdvTrain 85.6 71.2 91.6 93.0 91.3 Table 2: Comparison among single task, multi-Task and adversarial training on MNLI, RTE, QNLI, SST and MPRC in GLUE. Model BERTLARGE (Nie et al., 2019) RoBERTaLARGE (Nie et al., 2019) RoBERTa-LARGE + AdvTrain Dev 49.3 53.7 57.1 Test 44.2 49.7 57.1 Table 3: Results in terms of accuracy on the ANLI. sis (Socher et al., 2013), text similarity (Cer et al., 2017), paraphrase detection (Dolan and Brockett, 2005), and natural language inference (NLI) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009; Levesque et al., 2012; Williams et al., 2018). The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models. • SNLI. The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotate"
2020.acl-demos.16,N18-1074,0,0.0361753,"Missing"
2020.acl-demos.16,W18-5446,0,0.125971,"Missing"
2020.acl-demos.16,N18-1101,0,0.306618,"AdvTrain 85.6 71.2 91.6 93.0 91.3 Table 2: Comparison among single task, multi-Task and adversarial training on MNLI, RTE, QNLI, SST and MPRC in GLUE. Model BERTLARGE (Nie et al., 2019) RoBERTaLARGE (Nie et al., 2019) RoBERTa-LARGE + AdvTrain Dev 49.3 53.7 57.1 Test 44.2 49.7 57.1 Table 3: Results in terms of accuracy on the ANLI. sis (Socher et al., 2013), text similarity (Cer et al., 2017), paraphrase detection (Dolan and Brockett, 2005), and natural language inference (NLI) (Dagan et al., 2006; Bar-Haim et al., 2006; Giampiccolo et al., 2007; Bentivogli et al., 2009; Levesque et al., 2012; Williams et al., 2018). The diversity of the tasks makes GLUE very suitable for evaluating the generalization and robustness of NLU models. • SNLI. The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated (Bowman et al., 2015). This is the most widely used entailment dataset for NLI. • SciTail This is a textual entailment dataset derived from a science question answering (SciQ) dataset (Khot et al., 2018). In contrast to other entailment datasets mentioned previo"
2020.acl-demos.16,D15-1075,0,\N,Missing
2020.acl-demos.16,S17-2001,0,\N,Missing
2020.acl-demos.19,D18-1547,0,0.234895,"Missing"
2020.acl-demos.19,P19-1360,0,0.06182,"b-2 provides a template-based method and SC-LSTM (Wen et al., 2015). 2.3.1 CamRest676 CamRest676 (Wen et al., 2017) is a Wizard-of-Oz dataset, consisting of 676 dialogues in a restaurant domain. ConvLab-2 offers an agenda-based user simulator and a complete set of models for building a traditional pipeline dialogue system on the CamRest676 dataset. 2.2.6 Word-level Policy Word-level policy directly generates a natural language response (rather than dialogue acts) according to the dialogue history and the belief state. ConvLab-2 integrates three models: MDRG (Budzianowski et al., 2018a), HDSA (Chen et al., 2019), and LaRL (Zhao et al., 2019). MDRG is the baseline model proposed by Budzianowski et al. (2018b) on MultiWOZ, while HDSA and LaRL achieve much stronger performance on this dataset. 2.2.7 User Policy User policy is the core of a user simulator. It takes a pre-set user goal and system dialogue acts as input and outputs user dialogue acts. ConvLab-2 provides an agenda-based (Schatzmann et al., 2007) model and neural network-based models including HUS and its variational variants (G¨ur et al., 2018). To perform end-to-end simulation, researchers can equip the user policy with NLU and NLG compone"
2020.acl-demos.19,N19-1423,0,0.0163653,"ntegrated models in ConvLab-2 are marked in bold. Researchers can easily add their models by implementing the interface of the corresponding component. We will keep adding state-of-the-art models to reflect the latest progress in task-oriented dialogue. 2.2.1 Natural Language Understanding The natural language understanding (NLU) component, which is used to parse the other agent’s intent, takes an utterance as input and outputs the corresponding dialogue acts. ConvLab-2 provides three models: Semantic Tuple Classifier (STC) (Mairesse et al., 2009), MILU (Lee et al., 2019b), and BERTNLU. BERT (Devlin et al., 2019) has shown strong performance in many NLP tasks. Thus, ConvLab-2 proposes a new BERTNLU model. BERTNLU adds two MLPs on top of BERT for intent classification and slot tagging, respectively, and fine-tunes all parameters on the specified tasks. BERTNLU achieves the best performance on MultiWOZ in comparison with other models. 2.2.2 Dialogue State Tracking The dialogue state tracking (DST) component updates the belief state, which contains the constraints and requirements of the other agent (such as a user). ConvLab-2 provides a rule-based tracker that takes dialogue acts parsed by the NLU as in"
2020.acl-demos.19,W14-4337,0,0.245946,"Missing"
2020.acl-demos.19,P19-1546,0,0.189107,"l Intelligence, † State Key Lab of Intelligent Technology and Systems, † Beijing National Research Center for Information Science and Technology, Tsinghua University, Beijing, China ‡ Microsoft Research, Redmond, USA † ‡ {zhu-q18,z-zhang15,fangy17,gxly19}@mails.tsinghua.edu.cn {jincli,bapeng,jfgao}@microsoft.com † {zxy-dcs,aihuang}@tsinghua.edu.cn Abstract We present ConvLab-2, an open-source toolkit that enables researchers to build task-oriented dialogue systems with state-of-the-art models, perform an end-to-end evaluation, and diagnose the weakness of systems. As the successor of ConvLab (Lee et al., 2019b), ConvLab2 inherits ConvLab’s framework but integrates more powerful dialogue models and supports more datasets. Besides, we have developed an analysis tool and an interactive tool to assist researchers in diagnosing dialogue systems. The analysis tool presents rich statistics and summarizes common mistakes from simulated dialogues, which facilitates error analysis and system improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: F"
2020.acl-demos.19,P19-3011,1,0.903958,"Missing"
2020.acl-demos.19,P18-1133,0,0.311994,"improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: Framework of ConvLab-2. The top block shows different approaches to build a dialogue system. Introduction Task-oriented dialogue systems are gaining increasing attention in recent years, resulting in a number of datasets (Henderson et al., 2014; Wen et al., 2017; Budzianowski et al., 2018b; Rastogi et al., 2019) and a wide variety of models (Wen et al., 2015; Peng et al., 2017; Lei et al., 2018; Wu et al., 2019; Gao et al., 2019). However, very few opensource toolkits provide full support to assembling an end-to-end dialogue system with state-of-the-art models, evaluating the performance in an end-toend fashion, and analyzing the bottleneck both qualitatively and quantitatively. To fill the gap, we have developed ConvLab-2 based on our previous dialogue system platform ConvLab (Lee et al., 2019b). ConvLab-2 inherits its predecessor’s framework and extend it by integrating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author. two powerful tools"
2020.acl-demos.19,D17-1259,0,0.47465,"20. 2020 Association for Computational Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows researchers to quickly assemble a complete dialogue system (using a set of recipes). ConvLab-2 inherits the flexible framework of ConvLab and imports recently proposed models that achieve state-of-the-art performance. In addition, ConvLab-2 supports several large-scale dialogue datasets including CamRest676 (Wen et al., 2017), MultiWOZ (Budzianowski et al., 2018b), DealOrNoDeal (Lewis et al., 2017), and CrossWOZ (Zhu et al., 2020). To support end-to-end evaluation, ConvLab-2 provides user simulators for automatic evaluation and integrates Amazon Mechanical Turk for human evaluation, similar to ConvLab. Moreover, it provides an analysis tool and a human-machine interactive tool for diagnosing a dialogue system. Researchers can perform quantitative analysis using the analysis tool. It presents useful statistics extracted from the conversations between the user simulator and the dialogue system. This information helps reveal the weakness of the system and signifies the direction for furthe"
2020.acl-demos.19,D17-2014,0,0.0314309,"rating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author. two powerful tools, namely the analysis tool and the interactive tool, are provided for in-depth error analysis. ConvLab-2 will be the development platform for Multi-domain Task-oriented Dialog Challenge II track in the 9th Dialog System Technology Challenge (DSTC9)1 . As shown in Figure 1, there are many approaches to building a task-oriented dialogue system, ranging from pipeline methods with multiple components to fully end-to-end models. Previous toolkits focus on either end-to-end models (Miller et al., 2017) or one specific component such as dialogue policy (POL) (Ultes et al., 2017), while the others toolkits that are designed for developers (Bocklisch et al., 2017; Papangelis et al., 2020) do not 1 https://sites.google.com/dstc. community/dstc9/home 142 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 142–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows rese"
2020.acl-demos.19,D17-1237,1,0.845901,"analysis and system improvement. The interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: Framework of ConvLab-2. The top block shows different approaches to build a dialogue system. Introduction Task-oriented dialogue systems are gaining increasing attention in recent years, resulting in a number of datasets (Henderson et al., 2014; Wen et al., 2017; Budzianowski et al., 2018b; Rastogi et al., 2019) and a wide variety of models (Wen et al., 2015; Peng et al., 2017; Lei et al., 2018; Wu et al., 2019; Gao et al., 2019). However, very few opensource toolkits provide full support to assembling an end-to-end dialogue system with state-of-the-art models, evaluating the performance in an end-toend fashion, and analyzing the bottleneck both qualitatively and quantitatively. To fill the gap, we have developed ConvLab-2 based on our previous dialogue system platform ConvLab (Lee et al., 2019b). ConvLab-2 inherits its predecessor’s framework and extend it by integrating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author."
2020.acl-demos.19,P18-2069,0,0.077924,"Missing"
2020.acl-demos.19,N07-2038,0,0.404849,"rectly generates a natural language response (rather than dialogue acts) according to the dialogue history and the belief state. ConvLab-2 integrates three models: MDRG (Budzianowski et al., 2018a), HDSA (Chen et al., 2019), and LaRL (Zhao et al., 2019). MDRG is the baseline model proposed by Budzianowski et al. (2018b) on MultiWOZ, while HDSA and LaRL achieve much stronger performance on this dataset. 2.2.7 User Policy User policy is the core of a user simulator. It takes a pre-set user goal and system dialogue acts as input and outputs user dialogue acts. ConvLab-2 provides an agenda-based (Schatzmann et al., 2007) model and neural network-based models including HUS and its variational variants (G¨ur et al., 2018). To perform end-to-end simulation, researchers can equip the user policy with NLU and NLG components to assemble a complete user simulator. 2.2.8 End-to-end Model A fully end-to-end dialogue model receives the dialogue history and generates a response in natural language directly. ConvLab-2 extends Sequicity (Lei et al., 2018) to multi-domain scenarios: when the model senses that the current domain has switched, it resets the belief span, which records information of the current domain. ConvLa"
2020.acl-demos.19,D19-1010,1,0.86197,", 2019a), and TRADE (Wu et al., 2019). TRADE generates the belief state 143 from utterances using a copy mechanism and achieves state-of-the-art performance on MultiWOZ. the DealOrNoDeal dataset, we provide the ROLLOUTS RL policy proposed by Lewis et al. (2017). 2.2.4 Dialogue Policy Dialogue policy receives the belief state and outputs system dialogue acts. ConvLab-2 provides a rule-based policy, a simple neural policy that learns directly from the corpus using imitation learning, and reinforcement learning policies including REINFORCE (Williams, 1992), PPO (Schulman et al., 2017), and GDPL (Takanobu et al., 2019). GDPL achieves state-of-the-art performance on MultiWOZ. Compared with ConvLab, ConvLab-2 can integrate a new dataset more conveniently. For each dataset, ConvLab-2 provides a unified data loader that can be used by all the models, thus separating data processing from the model definition. Currently, ConvLab-2 supports four task-oriented dialogue datasets, including CamRest676 (Wen et al., 2017), MultiWOZ (Eric et al., 2019), DealOrNoDeal (Lewis et al., 2017), and CrossWOZ (Zhu et al., 2020). 2.2.5 Natural Language Generation The natural language generation (NLG) component transforms dialogue"
2020.acl-demos.19,P17-4013,0,0.0308827,"Corresponding author. two powerful tools, namely the analysis tool and the interactive tool, are provided for in-depth error analysis. ConvLab-2 will be the development platform for Multi-domain Task-oriented Dialog Challenge II track in the 9th Dialog System Technology Challenge (DSTC9)1 . As shown in Figure 1, there are many approaches to building a task-oriented dialogue system, ranging from pipeline methods with multiple components to fully end-to-end models. Previous toolkits focus on either end-to-end models (Miller et al., 2017) or one specific component such as dialogue policy (POL) (Ultes et al., 2017), while the others toolkits that are designed for developers (Bocklisch et al., 2017; Papangelis et al., 2020) do not 1 https://sites.google.com/dstc. community/dstc9/home 142 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 142–149 c July 5 - July 10, 2020. 2020 Association for Computational Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows researchers to quickly assemble a complete dialogue system (using a set of recipe"
2020.acl-demos.19,D15-1199,0,0.102409,"Missing"
2020.acl-demos.19,E17-1042,0,0.288565,"Missing"
2020.acl-demos.19,P19-1078,0,0.112045,"interactive tool provides a user interface that allows developers to diagnose an assembled dialogue system by interacting with the system and modifying the output of each system component. 1 Figure 1: Framework of ConvLab-2. The top block shows different approaches to build a dialogue system. Introduction Task-oriented dialogue systems are gaining increasing attention in recent years, resulting in a number of datasets (Henderson et al., 2014; Wen et al., 2017; Budzianowski et al., 2018b; Rastogi et al., 2019) and a wide variety of models (Wen et al., 2015; Peng et al., 2017; Lei et al., 2018; Wu et al., 2019; Gao et al., 2019). However, very few opensource toolkits provide full support to assembling an end-to-end dialogue system with state-of-the-art models, evaluating the performance in an end-toend fashion, and analyzing the bottleneck both qualitatively and quantitatively. To fill the gap, we have developed ConvLab-2 based on our previous dialogue system platform ConvLab (Lee et al., 2019b). ConvLab-2 inherits its predecessor’s framework and extend it by integrating many recently proposed state-of-the-art dialogue models. In addition, ∗ Corresponding author. two powerful tools, namely the anal"
2020.acl-demos.19,N19-1123,0,0.131839,"method and SC-LSTM (Wen et al., 2015). 2.3.1 CamRest676 CamRest676 (Wen et al., 2017) is a Wizard-of-Oz dataset, consisting of 676 dialogues in a restaurant domain. ConvLab-2 offers an agenda-based user simulator and a complete set of models for building a traditional pipeline dialogue system on the CamRest676 dataset. 2.2.6 Word-level Policy Word-level policy directly generates a natural language response (rather than dialogue acts) according to the dialogue history and the belief state. ConvLab-2 integrates three models: MDRG (Budzianowski et al., 2018a), HDSA (Chen et al., 2019), and LaRL (Zhao et al., 2019). MDRG is the baseline model proposed by Budzianowski et al. (2018b) on MultiWOZ, while HDSA and LaRL achieve much stronger performance on this dataset. 2.2.7 User Policy User policy is the core of a user simulator. It takes a pre-set user goal and system dialogue acts as input and outputs user dialogue acts. ConvLab-2 provides an agenda-based (Schatzmann et al., 2007) model and neural network-based models including HUS and its variational variants (G¨ur et al., 2018). To perform end-to-end simulation, researchers can equip the user policy with NLU and NLG components to assemble a complete use"
2020.acl-demos.19,2020.tacl-1.19,1,0.84554,"nal Linguistics have state-of-the-art models integrated. ConvLab (Lee et al., 2019b) is the first toolkit that provides various powerful models for all dialogue components and allows researchers to quickly assemble a complete dialogue system (using a set of recipes). ConvLab-2 inherits the flexible framework of ConvLab and imports recently proposed models that achieve state-of-the-art performance. In addition, ConvLab-2 supports several large-scale dialogue datasets including CamRest676 (Wen et al., 2017), MultiWOZ (Budzianowski et al., 2018b), DealOrNoDeal (Lewis et al., 2017), and CrossWOZ (Zhu et al., 2020). To support end-to-end evaluation, ConvLab-2 provides user simulators for automatic evaluation and integrates Amazon Mechanical Turk for human evaluation, similar to ConvLab. Moreover, it provides an analysis tool and a human-machine interactive tool for diagnosing a dialogue system. Researchers can perform quantitative analysis using the analysis tool. It presents useful statistics extracted from the conversations between the user simulator and the dialogue system. This information helps reveal the weakness of the system and signifies the direction for further improvement. With the interacti"
2020.acl-demos.30,D17-2014,0,0.0333427,"transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification. Icecaps (Shiv et al., 2019) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi-task training. The ConvAI2 challenge (Dinan et al., 2019) has a focus on personalized conversations. ParlAI (Miller et al., 2017) is another library for developing task-oriented dialogue systems. It contains pre-trained models for knowledge-grounded chatbot trained with crowdsourced data. The Text-to-Text Transformer (Raffel et al., 2019) unifies multiple text modeling tasks, and achieves the state-of-the-art results in various natural language generation and understanding benchmarks. User Bot Table 5: An interactive example of multi-turn dialogue Role Response User Bot what is the meaning of life ? The meaning is to be with your family and friends . I’m going to guess : It means that your parents and friends have loved"
2020.acl-demos.30,P02-1040,0,0.106794,"of 50,257 entries, and was trained on 16 Nvidia V100 machines with 4 https://github.com/mgalley/ DSTC7-End-to-End-Conversation-Modeling/ tree/master/evaluation 3 https://github.com/huggingface/ pytorch-transformers 272 other filtering criteria such as turn length, this yields a 5-reference test set of size 2208. (For each instance, one of the 6 human responses is set aside to assess human performance on this task.) Note that our training data is collected from a different time span from the test set. We performed automatic evaluation using standard machine translation metrics, including BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and NIST (Doddington, 2002). NIST is a variant of BLEU that weights n-gram matches by their information gain, i.e., it indirectly penalizes uninformative n-grams. We also use Entropy (Zhang et al., 2018) and Dist-n (Li et al., 2016a) to evaluate lexical diversity. More details are provided in Galley et al. (2019). We compared D IALO GPT with our in-house competitive sequence-to-sequence model P ER SONALITY C HAT based on (Li et al., 2016a) and trained on Twitter data, which has been used in production as a Cognitive Service for Microsoft Azure.5 Table 2 summ"
2020.acl-demos.30,N18-1202,0,0.0181606,"rning repository (Wolf et al., 2019) contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model, which achieves the state-of-the-art performance on ConvAI-2 dialogue competition. DLGnet (Olabiyi and Mueller, 2019) is a large transformer model trained on dialogue dataset and achieves good performance in multi-turn dialogue generation. AllenNLP (Gardner et al., 2018) is developed as a toolkit for many natural language processing tasks, including the large-scale pre-trained bi-LSTM sentence representation learning framework ELMo (Peters et al., 2018). Texar (Hu et al., 2018) focuses on text generation including style transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification. Icecaps (Shiv et al., 2019) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi-task training. The ConvAI2 challenge (Din"
2020.acl-demos.30,P19-1539,1,0.811156,"t thus poses a greater one-to-many problem than is typical in other text generation tasks such as neural machine translation, text summarization and paraphrasing. Human conversations are also generally more informal, noisy, and, when in the form of textual chat, often contain informal abbreviations or syntactic/lexical errors. Most open-domain neural response generation systems suffer from content or style inconsistency (Li et al., 2016b; Zhang et al., 2019; Gao et al., 2019c), lack of long-term contextual information (Serban et al., 2017), and blandness (Li et al., 2016a; Zhang et al., 2018; Qin et al., 2019). While these issues can be alleviated by modelling strategies specifically designed to boost information content, a transformer-based architecture like GPT-2 (Radford et al., 2018), which uses a multi-layer self-attentive mechanism to allow fully-connected cross-attention to the full context in a computationally efficient manner, seems like a natural choice for exploring a more general solution. Transformer models, for example, allow long-term dependency information to be better be preserved across time (Radford et al., 2018), thereby improving content consistency. They also have higher model"
2020.acl-demos.30,N19-1125,1,0.926516,"set aside to assess human performance on this task.) Note that our training data is collected from a different time span from the test set. We performed automatic evaluation using standard machine translation metrics, including BLEU (Papineni et al., 2002), METEOR (Lavie and Agarwal, 2007), and NIST (Doddington, 2002). NIST is a variant of BLEU that weights n-gram matches by their information gain, i.e., it indirectly penalizes uninformative n-grams. We also use Entropy (Zhang et al., 2018) and Dist-n (Li et al., 2016a) to evaluate lexical diversity. More details are provided in Galley et al. (2019). We compared D IALO GPT with our in-house competitive sequence-to-sequence model P ER SONALITY C HAT based on (Li et al., 2016a) and trained on Twitter data, which has been used in production as a Cognitive Service for Microsoft Azure.5 Table 2 summarizes the automatic evaluation results. D IALO GPT with 345M parameters and beam search achieved the highest automatic score across most metrics. Scores for D IALO GPT with 345M parameters are better across the board than with 117M parameters. Beam search (with beam width 10) dramatically improves BLEU and DIST scores, and marginally improves NIST"
2020.acl-demos.30,D19-1190,1,0.865771,"an Microsoft Corporation, Redmond, WA, USA ∗ {yizzhang,siqi.sun,mgalley,yenchen,chrisbkt,xiag,jfgao,jingjl,billdol}@microsoft.com Abstract tion. Neural response generation is a subcategory of text-generation that shares the objective of generating natural-looking text (distinct from any training instance) that is relevant to the prompt. Modelling conversations, however, presents distinct challenges in that human dialogue, which encapsulates the possibly competing goals of two participants, is intrinsically more diverse in the range of potential responses (Li et al., 2016a; Zhang et al., 2018; Gao et al., 2019a,b). It thus poses a greater one-to-many problem than is typical in other text generation tasks such as neural machine translation, text summarization and paraphrasing. Human conversations are also generally more informal, noisy, and, when in the form of textual chat, often contain informal abbreviations or syntactic/lexical errors. Most open-domain neural response generation systems suffer from content or style inconsistency (Li et al., 2016b; Zhang et al., 2019; Gao et al., 2019c), lack of long-term contextual information (Serban et al., 2017), and blandness (Li et al., 2016a; Zhang et al.,"
2020.acl-demos.30,W18-2501,0,0.0119024,"lionaire and happy . There is a reason the rich have a lot of money There are several open-sourced toolkits for largescale pre-trained transformer models. Huggingface Conv-AI transfer learning repository (Wolf et al., 2019) contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model, which achieves the state-of-the-art performance on ConvAI-2 dialogue competition. DLGnet (Olabiyi and Mueller, 2019) is a large transformer model trained on dialogue dataset and achieves good performance in multi-turn dialogue generation. AllenNLP (Gardner et al., 2018) is developed as a toolkit for many natural language processing tasks, including the large-scale pre-trained bi-LSTM sentence representation learning framework ELMo (Peters et al., 2018). Texar (Hu et al., 2018) focuses on text generation including style transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification."
2020.acl-demos.30,P16-1162,0,0.0541096,"of masked multi-head selfattention layers to train on massive web-text data. The text generated either from scratch or based on a user-specific prompt is realistic-looking. The success of GPT-2 demonstrates that a transformer language model is able to characterize human language data distributions at a fine-grained level, presumably due to large large model capacity and superior efficiency. Our model inherits from GPT-2 (Radford et al., 2018), a 12-to-48 layer transformer with layer normalization, a initialization scheme that accounts for model depth that we modified, and byte pair encodings (Sennrich et al., 2016) for the tokenizer. We follow the OpenAI GPT-2 to model a multiturn dialogue session as a long text and frame the generation task as language modeling. We first concatenate all dialog turns within a dialogue session into a long text x1 , · · · , xN (N is the sequence length), ended by the end-of-text token. We denote the source sentence (dialogue history) as S = x1 , · · · , xm and target sentence (ground truth response) as T = xm+1 , · · · , xN , the conditional probability of P (T |S) can be written as the product of a series of conditional probabilities: Dataset The dataset is extracted fro"
2020.acl-demos.30,W19-5944,0,0.0533119,"the “ground truth” references that will be tested on, while R4 is the “heldout” human response that serves to compute a “human” score. In semantic space, a generated response Rg from a well-trained model will presumably tend to lie in the vicinity the geometric center R3: I will s end some one right away R4: I . s the perp e trato Rg: W r still hen w R1: W insid as th e? as an is bre ythin ak-in g sto ? len? R2: I s any one hurt or in jured ? Source: I would like to report a break-in. Figure 1: A generated response can surpass a human response in automatic metrics. Example responses are from Gupta et al. (2019) of all possible responses, because the training objective seeks to generate the most likely response. This may be close to the geometric mean of all training instances, thus “averaging out” these instances. Consequently, a generated response Rg might have a lower “semantic distance” (manifested in higher automatic scores like BLEU) from R1-R3 than the targeted human response R4. 4.3 A New Reddit Multi-reference Dataset We further evaluate D IALO GPT on a multireference test set with 6K examples. The results are shown in Table 3. We test our method on two settings: training from scratch and fi"
2020.acl-demos.30,W18-2503,0,0.0199459,", 2019) contains the code for training conversational AI systems with transfer learning based on the GPT-2 transformer language model, which achieves the state-of-the-art performance on ConvAI-2 dialogue competition. DLGnet (Olabiyi and Mueller, 2019) is a large transformer model trained on dialogue dataset and achieves good performance in multi-turn dialogue generation. AllenNLP (Gardner et al., 2018) is developed as a toolkit for many natural language processing tasks, including the large-scale pre-trained bi-LSTM sentence representation learning framework ELMo (Peters et al., 2018). Texar (Hu et al., 2018) focuses on text generation including style transferring and controllable generation. It includes reinforcement learning capabilities along with its sequence modelling tools. DeepPavlov (Burtsev et al., 2018) is a popular framework focusing on task-oriented dialogue. This public repository contains several demos and pre-trained models for question answering and sentiment classification. Icecaps (Shiv et al., 2019) is a response generation toolkit with techniques such as grounding on personalities or external knowledge and multi-task training. The ConvAI2 challenge (Dinan et al., 2019) has a fo"
2020.acl-demos.39,P18-1133,0,0.0165971,"eraged to trigger different rule-based dialog flows, e.g. asking appropriate questions based on missing slots from the dialog state. However, a rule-based DM suffers from two major problems. First, these systems can have difficulty handling complex dialogs. Second, updating a rule-based DM to handle unexpected user responses and off-track conversations is often difficult due to the rigid structure of the dialog flow, the long-tail (sparseness) of user-system dialogs, and the complexity in jumping to unrelated parts of the flow. In end-to-end approaches proposed recently (Madotto et al., 2018; Lei et al., 2018), the DM is implemented as a neural network model that is trained directly on text transcripts of dialogs. Gao et al. (2019) presents a survey of recent approaches. One benefit provided by using a neural network model is that the network infers a latent representation of dialog state, eliminating the need for explicitly specifying dialog states. Neural-based DMs has been an area of active development for the research community as well as in industry; PyDial (Ultes et al., 2017), ParlAI (Miller et al., 2017), Plato (Papangelis et al., 2020), Rasa (Bocklisch et al., 2017), DeepPavlov (Burtsev et"
2020.acl-demos.39,P18-1136,0,0.0158725,"ction, that can be leveraged to trigger different rule-based dialog flows, e.g. asking appropriate questions based on missing slots from the dialog state. However, a rule-based DM suffers from two major problems. First, these systems can have difficulty handling complex dialogs. Second, updating a rule-based DM to handle unexpected user responses and off-track conversations is often difficult due to the rigid structure of the dialog flow, the long-tail (sparseness) of user-system dialogs, and the complexity in jumping to unrelated parts of the flow. In end-to-end approaches proposed recently (Madotto et al., 2018; Lei et al., 2018), the DM is implemented as a neural network model that is trained directly on text transcripts of dialogs. Gao et al. (2019) presents a survey of recent approaches. One benefit provided by using a neural network model is that the network infers a latent representation of dialog state, eliminating the need for explicitly specifying dialog states. Neural-based DMs has been an area of active development for the research community as well as in industry; PyDial (Ultes et al., 2017), ParlAI (Miller et al., 2017), Plato (Papangelis et al., 2020), Rasa (Bocklisch et al., 2017), Dee"
2020.acl-demos.39,D17-2014,0,0.0309002,"lated parts of the flow. In end-to-end approaches proposed recently (Madotto et al., 2018; Lei et al., 2018), the DM is implemented as a neural network model that is trained directly on text transcripts of dialogs. Gao et al. (2019) presents a survey of recent approaches. One benefit provided by using a neural network model is that the network infers a latent representation of dialog state, eliminating the need for explicitly specifying dialog states. Neural-based DMs has been an area of active development for the research community as well as in industry; PyDial (Ultes et al., 2017), ParlAI (Miller et al., 2017), Plato (Papangelis et al., 2020), Rasa (Bocklisch et al., 2017), DeepPavlov (Burtsev et al., 2018), and ConvLab (Lee et al., 2019) are a few examples. However, these machine-learned neural DMs are often viewed as black boxes from which dialog authors have difficulty interpreting why individual use cases succeed or fail. Further, these approaches often lack a general mechanism for accepting taskspecific knowledge and constraints, thus requiring a large number of validated dialog transcripts for training. Collection and curation of this type of corpus is often infeasible. This paper presents Co"
2020.acl-demos.39,P17-1062,0,0.197421,"viewed as black boxes from which dialog authors have difficulty interpreting why individual use cases succeed or fail. Further, these approaches often lack a general mechanism for accepting taskspecific knowledge and constraints, thus requiring a large number of validated dialog transcripts for training. Collection and curation of this type of corpus is often infeasible. This paper presents Conversation Learner, a machine teaching tool for building DMs, which combines the strengths of both rule-based and machinelearned approaches. Conversation Learner is based on Hybrid Code Networks (HCNs) (Williams et al., 2017) and the machine teaching discipline (Simard et al., 2017). Conversation Learner allows dialog authors to (1) import a dialog flow developed using popular dialog composers, (2) convert the dialog flow to an HCN-based DM, (3) continuously improve the HCN-based DM by reviewing usersystem dialog logs and providing updates via a machine teaching UI, and (4) convert the (revised) HCN-based DM back into a dialog flow for further editing and verification. Section 2 describes the architecture and main components of Conversation Learner. Section 3 demonstrates Conversation Learner features. Section 4 p"
2020.acl-demos.39,P17-4013,0,\N,Missing
2020.acl-main.197,D15-1075,0,0.0783238,"DNNBASE 95.8 94.1 SMARTBERT-BASE 94.8 93.2 MT-DNN-SMARTBASEv0 96.0 94.0 MT-DNN-SMARTBASE 96.1 94.2 BERTLARGE 95.7 94.4 MT-DNNLARGE 96.3 95.0 SMARTBERT-LARGE 96.2 94.7 MT-DNN-SMARTLARGEv0 96.6 95.2 6 Table 7: Results on the SNLI and SciTail dataset. test our model on an adversarial natural language inference (ANLI) dataset (Nie et al., 2019). We evaluate the performance of SMART on each subset (i.e., R1,R2,R3) of ANLI dev and test set. The results are presented in Table 6. Table 6 shows the results of training on combined NLI data (ANLI (Nie et al., 2019) + MNLI (Williams et al., 2018) + SNLI (Bowman et al., 2015) + FEVER (Thorne et al., 2018)) and training on only ANLI data. In the combined data setting, we obverse that SMARTRoBERTa-LARGE obtains the best Conclusion We propose a robust and efficient computation framework, SMART, for fine-tuning large scale pre-trained natural language models in a principled manner. The framework effectively alleviates the overfitting and aggressive updating issues in the fine-tuning stage. SMART includes two important ingredients: 1) smooth-inducing adversarial regularization; 2) Bregman proximal point optimization. Our empirical results suggest that SMART improves th"
2020.acl-main.197,P18-1031,0,0.162455,"8th Annual Meeting of the Association for Computational Linguistics, pages 2177–2190 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2019; Lan et al., 2019; Dong et al., 2019; Raffel et al., 2019). Due to the limited data from the target task/domain and the extremely high complexity of the pre-trained model, aggressive fine-tuning often makes the adapted model overfit the training data of the target task/domain and therefore does not generalize well to unseen data. To mitigate this issue, the fine-tuning methods often rely on hyper-parameter tuning heuristics. For example, Howard and Ruder (2018) use a heuristic learning rate schedule and gradually unfreeze the layers of the language model to improve the fine-tune performance; Peters et al. (2019) give a different suggestion that they only adapt certain layers and freeze the others; (Houlsby et al., 2019; Stickland and Murray, 2019) propose to add additional layers to the pre-trained model and fine-tune both of them or only the additional layers. However, these methods require significant tuning efforts. To fully harness the power of fine-tuning in a more principled manner, we propose a new learning framework for robust and efficient"
2020.acl-main.197,2020.tacl-1.5,0,0.0152249,"ated parameters of pk and qk , respectively. 2 Background The transformer models were originally proposed in Vaswani et al. (2017) for neural machine translation. Their superior performance motivated Devlin et al. (2019) to propose a bidirectional transformer-based language model named BERT. Specifically, Devlin et al. (2019) pre-trained the BERT model using a large corpus without any human annotation through unsupervised learning tasks. BERT motivated many follow-up works to further improve the pre-training by introducing new unsupervised learning tasks (Yang et al., 2019; Dong et al., 2019; Joshi et al., 2020), enlarging model size (Lan et al., 2019; Raffel et al., 2019), enlarging training corpora (Liu et al., 2019c; Yang et al., 2019; Raffel et al., 2019) and multi-tasking (Liu et al., 2019a,b). The pre-trained language model is then adapted to downstream tasks and further fine-tuned. Specifically, the top layer of the language model can be replaced by a task-specific layer and then continue to train on downstream tasks. To prevent overfitting, existing heuristics include choosing a 2178 small learning rate or a triangular learning rate schedule, and a small number of iterations, and other fine-t"
2020.acl-main.197,P19-1478,0,0.0427603,"Missing"
2020.acl-main.197,N15-1092,1,0.853476,"nt), SMARTRoBERTa obtains an even larger improvement showing its robustness to ambiguity. 5.3 SST MRPC Acc Acc 92.9 89.0 93.0 91.3 92.8 90.8 92.9 91.2 Table 3: Ablation study of SMART on 5 GLUE tasks. Note that all models used the BERTBASE model as their encoder. The results are reported in Table 3. It is expected that the removal of either component (smooth regularization or proximal point method) in SMART would result in a performance drop. For example, on MNLI, removing smooth reguError Analysis SMART with Multi-task Learning It has been shown that multi-task learning (MTL, Caruana (1997); Liu et al. (2015, 2019b)) has a regularization effect via alleviating overfitting to a specific task. One question is whether MTL helps SMART as well. In this section, we are going to answer this question. Following Liu et al. (2019b), we first “pre-trained” shared embeddings using MTL with SMART, denoted as MT-DNNSMART 8 , and then adapted the training data on each task on top of the shared embeddings. We also include a baseline which fine-tuned each task 8 Due to limitation of computational resources, we only trained jointly using MTL on MNLI, RTE, QNLI, SST and MRPC, while MT-DNN was trained on the whole G"
2020.acl-main.197,P19-1441,1,0.828565,"by far the largest language model, T5, has an enormous size of about 11 billion parameters (Raffel et al., 2019). For the second fine-tuning stage, researchers adapt the pre-trained language model to the target task/domain. They usually replace the top layer of the language model by a task/domainspecific sub-network, and then continue to train the new model with the limited data of the target task/domain. Such a fine-tuning approach accounts for the low-resource issue in the target task/domain, and has achieved state-of-the-art performance in many popular NLP benchmarks (Devlin et al., 2019; Liu et al., 2019c; Yang et al., 2177 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2177–2190 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2019; Lan et al., 2019; Dong et al., 2019; Raffel et al., 2019). Due to the limited data from the target task/domain and the extremely high complexity of the pre-trained model, aggressive fine-tuning often makes the adapted model overfit the training data of the target task/domain and therefore does not generalize well to unseen data. To mitigate this issue, the fine-tuning methods often rely on hyper-p"
2020.acl-main.197,2020.acl-demos.16,1,0.818871,"Missing"
2020.acl-main.197,2021.ccl-1.108,0,0.368231,"Missing"
2020.acl-main.197,N18-1202,0,0.0604348,"of-domain data). The goal is to transfer the knowledge from the high-resource domains to the low-resource target domain. Here we are particularly interested in the popular twostage transfer learning framework (Pan and Yang, 2009). The first stage is pre-training, where a high-capacity model is trained for the out-ofdomain high-resource relevant tasks. The second stage is fine-tuning, where the high-capacity model is adapted to the low-resource task in the target domain. For many applications in NLP, most popular transfer learning methods choose to pre-train a large language model, e.g., ELMo (Peters et al., 2018), GPT (Radford et al., 2019) and BERT (Devlin et al., 2019). Such a language model can capture general semantic and syntactic information that can be further used in downstream NLP tasks. The language model is particularly attractive, because it can be trained in a completely unsupervised manner with huge amount of unlabeled data, which are extremely cheap to fetch from internet nowadays. The resulting extremely large multidomain text corpus allows us to train huge language models. To the best of our knowledge, by far the largest language model, T5, has an enormous size of about 11 billion par"
2020.acl-main.197,W19-4302,0,0.0669215,"; Lan et al., 2019; Dong et al., 2019; Raffel et al., 2019). Due to the limited data from the target task/domain and the extremely high complexity of the pre-trained model, aggressive fine-tuning often makes the adapted model overfit the training data of the target task/domain and therefore does not generalize well to unseen data. To mitigate this issue, the fine-tuning methods often rely on hyper-parameter tuning heuristics. For example, Howard and Ruder (2018) use a heuristic learning rate schedule and gradually unfreeze the layers of the language model to improve the fine-tune performance; Peters et al. (2019) give a different suggestion that they only adapt certain layers and freeze the others; (Houlsby et al., 2019; Stickland and Murray, 2019) propose to add additional layers to the pre-trained model and fine-tune both of them or only the additional layers. However, these methods require significant tuning efforts. To fully harness the power of fine-tuning in a more principled manner, we propose a new learning framework for robust and efficient fine-tuning on the pre-trained language models through regularized optimization techniques. Specifically, our framework consists of two important ingredie"
2020.acl-main.197,D16-1264,0,0.0571223,"Missing"
2020.acl-main.197,D13-1170,0,0.0243242,"Missing"
2020.acl-main.197,N18-1074,0,0.0240072,"Missing"
2020.acl-main.197,W18-5446,0,0.133739,"g of the Association for Computational Linguistics, pages 2177–2190 c July 5 - 10, 2020. 2020 Association for Computational Linguistics 2019; Lan et al., 2019; Dong et al., 2019; Raffel et al., 2019). Due to the limited data from the target task/domain and the extremely high complexity of the pre-trained model, aggressive fine-tuning often makes the adapted model overfit the training data of the target task/domain and therefore does not generalize well to unseen data. To mitigate this issue, the fine-tuning methods often rely on hyper-parameter tuning heuristics. For example, Howard and Ruder (2018) use a heuristic learning rate schedule and gradually unfreeze the layers of the language model to improve the fine-tune performance; Peters et al. (2019) give a different suggestion that they only adapt certain layers and freeze the others; (Houlsby et al., 2019; Stickland and Murray, 2019) propose to add additional layers to the pre-trained model and fine-tune both of them or only the additional layers. However, these methods require significant tuning efforts. To fully harness the power of fine-tuning in a more principled manner, we propose a new learning framework for robust and efficient"
2020.acl-main.197,Q19-1040,0,0.0595184,"Missing"
2020.acl-main.197,N18-1101,0,0.0468458,") - 88.3 BERTBASE 94.3 92.0 MT-DNNBASE 95.8 94.1 SMARTBERT-BASE 94.8 93.2 MT-DNN-SMARTBASEv0 96.0 94.0 MT-DNN-SMARTBASE 96.1 94.2 BERTLARGE 95.7 94.4 MT-DNNLARGE 96.3 95.0 SMARTBERT-LARGE 96.2 94.7 MT-DNN-SMARTLARGEv0 96.6 95.2 6 Table 7: Results on the SNLI and SciTail dataset. test our model on an adversarial natural language inference (ANLI) dataset (Nie et al., 2019). We evaluate the performance of SMART on each subset (i.e., R1,R2,R3) of ANLI dev and test set. The results are presented in Table 6. Table 6 shows the results of training on combined NLI data (ANLI (Nie et al., 2019) + MNLI (Williams et al., 2018) + SNLI (Bowman et al., 2015) + FEVER (Thorne et al., 2018)) and training on only ANLI data. In the combined data setting, we obverse that SMARTRoBERTa-LARGE obtains the best Conclusion We propose a robust and efficient computation framework, SMART, for fine-tuning large scale pre-trained natural language models in a principled manner. The framework effectively alleviates the overfitting and aggressive updating issues in the fine-tuning stage. SMART includes two important ingredients: 1) smooth-inducing adversarial regularization; 2) Bregman proximal point optimization. Our empirical results s"
2020.acl-main.331,P19-1033,1,0.648302,"//msnews.github.io. 1 Introduction Online news services such as Google News and Microsoft News have become important platforms for a large population of users to obtain news information (Das et al., 2007; Wu et al., 2019a). Massive news articles are generated and posted online every day, making it difficult for users to find interested news quickly (Okura et al., 2017). Personalized news recommendation can help users alleviate information overload and improve news reading experience (Wu et al., 2019b). Thus, it is widely used in many online news platforms (Li et al., 2011; Okura et al., 2017; An et al., 2019). In traditional recommender systems, users and items are usually represented using IDs, and their interactions such as rating scores are used to learn ID representations via methods like collaborative filtering (Koren, 2008). However, news recommendation has some special challenges. First, news articles on news websites update very quickly. New news articles are posted continuously, and existing news articles will expire in short time (Das et al., 2007). Thus, the cold-start problem is very severe in news recommendation. Second, news articles contain rich textual information such as title and"
2020.acl-main.331,N19-1423,0,0.0292461,"ove the performance of different neural text representation methods such as CNN and LSTM for news recommendation. It shows that selecting important words in news texts using attention can help learn more informative news representations. Another interesting finding is that the combination of LSTM and attention can achieve the best performance. However, to our best knowledge, it is not used in existing news recommendation methods. 5.3.2 Pre-trained Language Models Next, we explore whether the quality of news representation can be further improved by the pretrained language models such as BERT (Devlin et al., 2019), which have achieved huge success in different NLP tasks. We applied BERT to the news representation module of three state-of-theart news recommendation methods, i.e., NAML, LSTUR and NRMS. The results are summarized in Fig. 3. We find that by replacing the original word embedding module with the pre-trained BERT model, the performance of different news recommendation methods can be improved. It shows the BERT model pre-trained on large-scale corpus like Wikipedia can provide useful semantic information for news representation. We also find that fine-tuning the pre-trained BERT model with the"
2020.acl-main.331,D14-1162,0,0.0942926,"ify and compare the methods introduced in Section 4 on the MIND dataset. Since most of these news recommendation methods are based on news titles, for fair comparison, we only used news titles in experiments unless otherwise mentioned. We will explore the usefulness of different news texts such as body in Section 5.3.3. In order to simulate the practical news recommendation scenario where we always have unseen users not included in training data, we randomly sampled half of the users for training, and used all the users for test. For those methods that need word embeddings, we used the Glove (Pennington et al., 2014) as initialization. Adam was used as the optimizer. Since the non-clicked news are usually much more than the clicked news in each impression log, following (Wu et al., 2019b) we applied negative sampling technique to model training. All hyper-parameters were selected according to the results on the validation set. The metrics used in our experiments are AUC, MRR, nDCG@5 and nDCG@10, which are standard metrics for recommendation result evaluation. Each experiment was repeated 10 times. 3601 Overall LibFM DSSM Wide&Deep DeepFM DFM GRU DKN NPA NAML LSTUR NRMS Overlap Users Unseen Users AUC MRR n"
2020.acl-main.331,D16-1264,0,0.0417148,"and body. It is not appropriate to simply representing them using IDs, and it is important to understand their content from their texts (Kompan and Bielikov´a, 2010). Third, there is no explicit rating of news articles posted by users on news platforms. Thus, in news recommendation users’ interest in news is usually inferred from their click behaviors in an implicit way (Ilievski and Roy, 2013). A large-scale and high-quality dataset can significantly facilitate the research in an area, such as ImageNet for image classification (Deng et al., 2009) and SQuAD for machine reading comprehension (Rajpurkar et al., 2016). There are several public datasets for traditional recommendation tasks, such as Amazon dataset1 for product recommendation and MovieLens dataset2 for movie recommendation. Based on these datasets, many well-known recommendation methods have been developed. However, existing studies on news recommendation are much fewer, and many of them are conducted on proprietary datasets (Okura et al., 2017; Wang et al., 2018; Wu et al., 2019a). Although there are a few public datasets for news recommendation, they are usually in small size and most of them are not in English. Thus, a public 1 2 http://jm"
2020.acl-main.331,D19-1671,1,0.849233,"Missing"
2020.acl-main.331,N16-1174,0,0.052786,"dy (AMV) + Body + Cat. (AMV) + Body + Cat. + Ent. (AMV) AUC MRR nDCG@5 nDCG@10 66.22 64.17 66.32 67.07 67.09 67.23 67.38 67.50 67.60 31.92 30.49 31.88 32.34 32.40 32.41 32.37 32.43 32.51 34.53 32.81 34.42 34.98 35.03 35.04 35.12 35.21 35.24 40.23 38.57 40.22 40.74 40.80 40.83 40.79 40.96 41.03 Table 5: News representation with different news information. “Abs.”, “Cat.” and “Ent.” mean abstract, category and entity, respectively. Figure 3: BERT for news representation. ding (Avg-Emb), CNN, LSTM and multi-head selfattention (Self-Att). Since attention mechanism is an important technique in NLP (Yang et al., 2016), we also apply it to the aforementioned neural text representation methods. The results are in Table 4. We have several findings from the results. First, neural text representation methods such as CNN, Self-Att and LSTM can outperform traditional text representation methods like TF-IDF and LDA. This is because the neural text representation models can be learned with the news recommendation task, and they can capture the contexts of texts to generate better news representations. Second, Self-Att and LSTM outperform CNN in news representation. This is because multi-head self-attention and LSTM"
2020.cl-1.2,P07-1111,0,0.0102926,"5. The authors claimed that the learned metric correlates better with human evaluation than BLEU and ROUGE. Similarly, Cuayáhuitl et al. (2018) proposed to learn reward functions using human conversations (with a focus on lengthy conversation histories) for training and evaluating chatbots. Misu et al. (2012) asked annotators to annotate the quality of system responses and then applied regression to learn a reward function for system evaluation. However, as argued by Gao, Galley, and Li (2019), machine-learned metrics lead to potential problems such as overfitting and ”gaming of the metric” (Albrecht and Hwa 2007). For example, Sai et al. (2019) showed that ADEM can be easily fooled with a variation as simple as reversing the word order in the text. Their experiments on several such adversarial scenarios draw out counter intuitive scores on the dialogue responses. All prior work suggests that automatic evaluation of open-domain dialog systems is by no means a solved problem. In our opinion, developing a successful automatic evaluation metric has two prerequisites. First, there should be a fairly large, representative conversational data set. This data set should have a good coverage of daily life topic"
2020.cl-1.2,W05-0909,0,0.159757,"ent pairs with ratings of 1 and 2 are extracted from the database used for the retrieval-based candidate generator. These ratings are determined automatically based on how many times users follow the comments, computed from the XiaoIce logs. The image–comment pairs with rating 0 are randomly sampled. Table 4 presents the result of a pilot study (Huang et al. 2019) showing that the XiaoIce Image Commenting skill outperforms several state-of-the-art image captioning systems on a test set consisting of 5K image–comment pairs whose ratings are 2, in terms of BLEU-4 (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005), CIDEr (Vedantam, Lawrence, Zitnick, and Parikh 2015), ROUGE-L (Lin 2004), and SPICE (Anderson et al. 2016). Figure 14 shows a few example comments generated by the competing systems in Table 4. It can be observed that the XiaoIce-produced comments are emotional, subjective, imaginative, and are very likely to inspire meaningful human–machine interactions, while the comments generated by the other image captioning models are reasonable in content but boring in the context of social chats, and thus less likely to improve user engagement. In the A/B test we observe that Image Commenting doubles"
2020.cl-1.2,W14-4012,0,0.0144948,"Missing"
2020.cl-1.2,W18-0802,0,0.0189031,"are strategy that can drive the conversation when needed (right). For example, XiaoIce determines to “drive” the conversation to a new topic when the conversation has stalled in Turn 3, and to be actively listening when the user herself is engaged in the conversation in Turns 4 and 7. 56 Zhou et al. The Design and Implementation of XiaoIce, an Empathetic Social Chatbot only align with the primary design goal of XiaoIce as an AI companion with which users form long-term, emotional connections, but also take into account culture differences and many sensitive ethical questions as exemplified in Curry and Rieser (2018), Schmidt and Wiegand (2017), and Brahnam (2005). Thus, for different platforms deployed in different regions, we design different personas guided by large-scale analysis on human conversations. Take the XiaoIce persona designed for WeChat deployed in China as an example. We have collected human conversations of millions of users, and labeled each user as having a “desired” persona or not depending on whether his or her conversations contain inappropriate requests or responses that contain swearing, bullying, and so forth. Our finding is that the majority of the “desired” users are young, fema"
2020.cl-1.2,N18-5020,0,0.0623964,"Missing"
2020.cl-1.2,P15-2073,1,0.785038,"esponses using either retrieval-based methods or generationbased methods, or hybrid methods. Retrieval-based methods are often evaluated using traditional information retrieval metrics (Manning, Raghavan, and Schütze 2008) such as Precision@K, Mean Average Precision, and normalized Discounted Cumulative Gain. Generation-based methods are evaluated using those metrics borrowed from text generation tasks like machine translation and text summarization, using string and n-gram matching metrics such as BLEU (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005), and ROUGE (Lin 2004). deltaBLEU (Galley et al. 2015) is an extension of BLEU that exploits numerical ratings associated with conversational responses. There has been significant debate as to whether these automatic metrics are appropriate for evaluating conversational response generation systems. Liu et al. (2016) argued that they are not by showing that most of these metrics (e.g., BLEU) correlate poorly with human judgments. But, as pointed out in Gao, Galley, and Li (2019), the correlation analysis by Liu et al. (2016) is performed at the sentence level whereas BLEU is designed from the outset to be used as a corpus-level metric. Galley et a"
2020.cl-1.2,J05-4005,1,0.463613,"llowing the same topic (decided by Topic Manager), responding in a consistent and positive way as specified (by the values of intent, sentiment, opinion, etc.) in eR , which are computed based on those in eQ using a set of heuristics. The response must also fit XiaoIce’s persona, whose key-value pairs, such as age, gender, and interests, are extracted from the pre-compiled XiaoIce profile. We will describe how eQ and eR are used for response generation in Section 4.3. Evaluation. The empathetic computing module consists of a set of classifiers. We use an off-the-shelf named entity recognizer (Gao et al. 2005) for identifying 13 types of named entities without retraining for CQU, and train a group of classifiers for user understanding (i.e., co-reference resolution, topic detection, intent detection, opinion detection, and sentiment analysis) using 10K manually labeled dialogue sessions. The effectiveness of the empathetic computing module is verified in the A/B test on Weibo users. Although we do not observe any significant change in CPS, NAU is increased from 0.5 million to 5.1 million in 3 months. The module was released in July 2018, and became 64 Zhou et al. The Design and Implementation of Xi"
2020.cl-1.2,D14-1002,1,0.77109,"Missing"
2020.cl-1.2,N16-1014,1,0.751878,"average number of conversation-turns between the chatbot and the user in a conversational session. The larger the CPS is, the better engaged the social chatbot is. It is worth noting that we optimize XiaoIce for expected CPS that corresponds to long-term, rather than short-term, engagement. In our evaluation, the expected CPS is approximated by averaging the CPS of human–XiaoIce conversations collected from millions of active users over a long period of time (typically 1–6 months). The evaluation methodology eliminates many possibilities of gaming the metric. For example, some recent studies (Li et al. 2016c; Fang et al. 2017) show that encompassing bland but interactive responses such as “I don’t understand, what do you mean?” can sometimes increase the CPS of the ongoing human–machine conversation. But this hurts the CPS and NAU in the long run because few users are willing to talk (again) to a bot that always gives bland responses no matter how interactive these responses are, not to mention establishing long-term, emotional connections. In contrast, incorporating many task-completion skills often reduces the CPS in the short term because these skills help users accomplish tasks more efficien"
2020.cl-1.2,P16-1094,1,0.518474,"average number of conversation-turns between the chatbot and the user in a conversational session. The larger the CPS is, the better engaged the social chatbot is. It is worth noting that we optimize XiaoIce for expected CPS that corresponds to long-term, rather than short-term, engagement. In our evaluation, the expected CPS is approximated by averaging the CPS of human–XiaoIce conversations collected from millions of active users over a long period of time (typically 1–6 months). The evaluation methodology eliminates many possibilities of gaming the metric. For example, some recent studies (Li et al. 2016c; Fang et al. 2017) show that encompassing bland but interactive responses such as “I don’t understand, what do you mean?” can sometimes increase the CPS of the ongoing human–machine conversation. But this hurts the CPS and NAU in the long run because few users are willing to talk (again) to a bot that always gives bland responses no matter how interactive these responses are, not to mention establishing long-term, emotional connections. In contrast, incorporating many task-completion skills often reduces the CPS in the short term because these skills help users accomplish tasks more efficien"
2020.cl-1.2,D16-1127,1,0.596479,"average number of conversation-turns between the chatbot and the user in a conversational session. The larger the CPS is, the better engaged the social chatbot is. It is worth noting that we optimize XiaoIce for expected CPS that corresponds to long-term, rather than short-term, engagement. In our evaluation, the expected CPS is approximated by averaging the CPS of human–XiaoIce conversations collected from millions of active users over a long period of time (typically 1–6 months). The evaluation methodology eliminates many possibilities of gaming the metric. For example, some recent studies (Li et al. 2016c; Fang et al. 2017) show that encompassing bland but interactive responses such as “I don’t understand, what do you mean?” can sometimes increase the CPS of the ongoing human–machine conversation. But this hurts the CPS and NAU in the long run because few users are willing to talk (again) to a bot that always gives bland responses no matter how interactive these responses are, not to mention establishing long-term, emotional connections. In contrast, incorporating many task-completion skills often reduces the CPS in the short term because these skills help users accomplish tasks more efficien"
2020.cl-1.2,W04-1013,0,0.214601,"based candidate generator. These ratings are determined automatically based on how many times users follow the comments, computed from the XiaoIce logs. The image–comment pairs with rating 0 are randomly sampled. Table 4 presents the result of a pilot study (Huang et al. 2019) showing that the XiaoIce Image Commenting skill outperforms several state-of-the-art image captioning systems on a test set consisting of 5K image–comment pairs whose ratings are 2, in terms of BLEU-4 (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005), CIDEr (Vedantam, Lawrence, Zitnick, and Parikh 2015), ROUGE-L (Lin 2004), and SPICE (Anderson et al. 2016). Figure 14 shows a few example comments generated by the competing systems in Table 4. It can be observed that the XiaoIce-produced comments are emotional, subjective, imaginative, and are very likely to inspire meaningful human–machine interactions, while the comments generated by the other image captioning models are reasonable in content but boring in the context of social chats, and thus less likely to improve user engagement. In the A/B test we observe that Image Commenting doubles the expected CPS across all dialogues that contain images. 75 Computation"
2020.cl-1.2,D16-1230,0,0.0343905,"n, and normalized Discounted Cumulative Gain. Generation-based methods are evaluated using those metrics borrowed from text generation tasks like machine translation and text summarization, using string and n-gram matching metrics such as BLEU (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005), and ROUGE (Lin 2004). deltaBLEU (Galley et al. 2015) is an extension of BLEU that exploits numerical ratings associated with conversational responses. There has been significant debate as to whether these automatic metrics are appropriate for evaluating conversational response generation systems. Liu et al. (2016) argued that they are not by showing that most of these metrics (e.g., BLEU) correlate poorly with human judgments. But, as pointed out in Gao, Galley, and Li (2019), the correlation analysis by Liu et al. (2016) is performed at the sentence level whereas BLEU is designed from the outset to be used as a corpus-level metric. Galley et al. (2015) showed that the correlation of string-based metrics (e.g., BLEU and deltaBLEU) significantly increases with the units of measurement longer than a sentence. Nevertheless, in open-domain dialog systems, the same input may have many plausible responses th"
2020.cl-1.2,P17-1103,0,0.0171456,"tset to be used as a corpus-level metric. Galley et al. (2015) showed that the correlation of string-based metrics (e.g., BLEU and deltaBLEU) significantly increases with the units of measurement longer than a sentence. Nevertheless, in open-domain dialog systems, the same input may have many plausible responses that differ in topics or contents significantly. Therefore, low BLEU (or other metrics) scores do not necessarily indicate low quality, as the number of reference responses is always limited in the test set. Recently, several machine-learned metrics for dialog evaluation are proposed. Lowe et al. (2017) proposed the ADEM metric, which uses a variant of the pre-trained VHRED model (Serban et al. 2017) for evaluation. The model takes dialogue context, user input, and gold and system responses as input, and produces a qualitative score between 1 and 5. The authors claimed that the learned metric correlates better with human evaluation than BLEU and ROUGE. Similarly, Cuayáhuitl et al. (2018) proposed to learn reward functions using human conversations (with a focus on lengthy conversation histories) for training and evaluating chatbots. Misu et al. (2012) asked annotators to annotate the quality"
2020.cl-1.2,W12-1611,0,0.0350377,"rics for dialog evaluation are proposed. Lowe et al. (2017) proposed the ADEM metric, which uses a variant of the pre-trained VHRED model (Serban et al. 2017) for evaluation. The model takes dialogue context, user input, and gold and system responses as input, and produces a qualitative score between 1 and 5. The authors claimed that the learned metric correlates better with human evaluation than BLEU and ROUGE. Similarly, Cuayáhuitl et al. (2018) proposed to learn reward functions using human conversations (with a focus on lengthy conversation histories) for training and evaluating chatbots. Misu et al. (2012) asked annotators to annotate the quality of system responses and then applied regression to learn a reward function for system evaluation. However, as argued by Gao, Galley, and Li (2019), machine-learned metrics lead to potential problems such as overfitting and ”gaming of the metric” (Albrecht and Hwa 2007). For example, Sai et al. (2019) showed that ADEM can be easily fooled with a variation as simple as reversing the word order in the text. Their experiments on several such adversarial scenarios draw out counter intuitive scores on the dialogue responses. All prior work suggests that auto"
2020.cl-1.2,I17-1047,1,0.87724,"Missing"
2020.cl-1.2,P02-1040,0,0.108777,"wn in Figure 13. The image-comment pairs with ratings of 1 and 2 are extracted from the database used for the retrieval-based candidate generator. These ratings are determined automatically based on how many times users follow the comments, computed from the XiaoIce logs. The image–comment pairs with rating 0 are randomly sampled. Table 4 presents the result of a pilot study (Huang et al. 2019) showing that the XiaoIce Image Commenting skill outperforms several state-of-the-art image captioning systems on a test set consisting of 5K image–comment pairs whose ratings are 2, in terms of BLEU-4 (Papineni et al. 2002), METEOR (Banerjee and Lavie 2005), CIDEr (Vedantam, Lawrence, Zitnick, and Parikh 2015), ROUGE-L (Lin 2004), and SPICE (Anderson et al. 2016). Figure 14 shows a few example comments generated by the competing systems in Table 4. It can be observed that the XiaoIce-produced comments are emotional, subjective, imaginative, and are very likely to inspire meaningful human–machine interactions, while the comments generated by the other image captioning models are reasonable in content but boring in the context of social chats, and thus less likely to improve user engagement. In the A/B test we obs"
2020.cl-1.2,D17-1237,1,0.843562,"own in Figure 17(b); and a special user command such as “XiaoIce, what is the weather today” can trigger the Weather skill as shown in Figure 19(a). If multiple skills are triggered simultaneously, we select the one to activate based on their triggering confidence scores, pre-defined priorities, and the session context. To ensure a smooth conversation, we avoid switching among different skills too often. We prefer keeping the running skill activated until it terminates to activating a new skill. This is similar to the way sub-tasks (i.e., skills) are managed in composite-task completion bots (Peng et al. 2017). We will discuss the low-level policies in the later sections where the individual dialogue skills are described. 61 Computational Linguistics Volume 46, Number 1 4.1.3 Topic Manager. Topic Manager simulates human behavior of changing topics during a conversation. It consists of a classifier for deciding at each dialogue turn whether or not to switch topics, and a topic recommendation engine for suggesting a new topic. Topic switching is triggered if XiaoIce does not have sufficient knowledge about the topic to engage in a meaningful conversation, or the user is getting bored. The classifier"
2020.cl-1.2,W17-1101,0,0.0128849,"ve the conversation when needed (right). For example, XiaoIce determines to “drive” the conversation to a new topic when the conversation has stalled in Turn 3, and to be actively listening when the user herself is engaged in the conversation in Turns 4 and 7. 56 Zhou et al. The Design and Implementation of XiaoIce, an Empathetic Social Chatbot only align with the primary design goal of XiaoIce as an AI companion with which users form long-term, emotional connections, but also take into account culture differences and many sensitive ethical questions as exemplified in Curry and Rieser (2018), Schmidt and Wiegand (2017), and Brahnam (2005). Thus, for different platforms deployed in different regions, we design different personas guided by large-scale analysis on human conversations. Take the XiaoIce persona designed for WeChat deployed in China as an example. We have collected human conversations of millions of users, and labeled each user as having a “desired” persona or not depending on whether his or her conversations contain inappropriate requests or responses that contain swearing, bullying, and so forth. Our finding is that the majority of the “desired” users are young, female users. Therefore, we desi"
2020.cl-1.2,P15-1152,0,0.0863375,"Missing"
2020.cl-1.2,W07-0313,0,0.146061,"gotistical and only demonstrates her wit and creativity when appropriate. As shown in Figure 1, XiaoIce responds sensibly to some sensitive questions (e.g., Session 20), and then skillfully shifts to new topics that are more comfortable for both parties. As we are making XiaoIce an open social chatbot development platform for third-parties, the XiaoIce persona will be configurable based on specific user scenarios and cultures. 2.2 Social Chatbot Metric: CPS Unlike task-oriented bots, whose performance is measured by task success rate, measuring the performance of social chatbots is difficult (Shawar and Atwell 2007). In the past, the Turing Test has been used to evaluate chitchat performance. But it is not sufficient to measure the success of long-term, emotional engagement with users. In addition to the Number of Active Users (NAU), we propose to use expected Conversation-turns Per Session (CPS) as the success metric for social chatbots. It is the average number of conversation-turns between the chatbot and the user in a conversational session. The larger the CPS is, the better engaged the social chatbot is. It is worth noting that we optimize XiaoIce for expected CPS that corresponds to long-term, rath"
2020.cl-1.2,N15-1020,1,0.825696,"tary: The neural-model-based generator offers robustness and high coverage, whereas the retrieval-based provides high-quality responses for popular topics. Neural response generation is a very active research topic in the conversational AI community (Gao, Galley, and Li 2019). Its role in developing social chatbots is becoming increasingly important as its performance keeps improving. The neural response generator in XiaoIce follows the sequence-to-sequence (seq2seq) framework (Cho et al. 2014; Sutskever, Vinyals, and Le 2014) used for conversation response generation (Shang, Lu, and Li 2015; Sordoni et al. 2015; Vinyals and Le 2015; Li et al. 2016a, 2016b; Serban et al. 2016; Xing et al. 2017). The generator is based on a GRU-RNN model, similar to the Speaker-Addressee model (Li et al. 2016b). Given input (Qc , eQ , eR ), we wish to predict how XiaoIce (the addressee) modeled by eR would respond to query Qc produced by the user (speaker) modeled by eQ . As illustrated in Figure 6, we first obtain an interactive representation v ∈ Rd by linearly combining query and response empathy vectors eQ and eR in an attempt to model the interactive style of XiaoIce toward the user, > v = σ(W> Q eQ + WR eR ) Fig"
2020.cl-1.2,C16-1063,0,0.0457181,"Missing"
2020.emnlp-main.349,K19-1079,0,0.115309,"Missing"
2020.emnlp-main.349,P19-1200,1,0.889615,"GE scores were obtained by producing text that is more repetitive and generic.10 In contrast, P LOT M ACHINES generally achieves good performance on both ROUGE and diversity scores, with self-BLEU scores that are lower than most other models. Notably, they generally have more similar self-BLEU scores to the actual gold stories, indicating that the language diversity is more similar to what humans write. Automatic Metrics In this section, we evaluate performance using different automatic metrics. We compute ROUGE scores (Lin, 2004) and self-BLEU (Zhu et al., 2018) following from previous work (Shen et al., 2019; Zhu et al., 2018) showing that a large ROUGE score together with a low self-BLEU score can demonstrate a model’s ability to generate realisticlooking as well as diverse generations. Coverage We compute ROUGE scores (Lin, 2004) with respect to the gold stories (Table 2). Results show that the full P LOT M ACHINES achieves comparable or higher ROUGE on all three datasets. Both P LOT M ACHINES variants (using GPT or GPT-2 as a base) achieve improvements over G ROVER, even though G ROVER includes significantly more parameters than the model using GPT. Ablations In the bottom block of Table 2, we"
2020.emnlp-main.349,D16-1032,1,\N,Missing
2020.emnlp-main.349,P18-1082,0,\N,Missing
2020.emnlp-main.349,N18-1156,0,\N,Missing
2020.emnlp-main.349,D18-1462,0,\N,Missing
2020.emnlp-main.349,W13-4069,0,\N,Missing
2020.emnlp-main.349,W18-1505,0,\N,Missing
2020.emnlp-main.378,P17-1061,0,0.0600197,"t variable language modeling. Language VAEs have inspired new applications in NLP, via exploiting many interesting properties of the model’s latent space (Bowman et al., 2016; Kim et al., 2018b). Its modeling capacity and empirical performance is somewhat limited, partially due to the KL vanishing issue described in Section 4.3. Several attempts have been made to alleviate this issue, including different KL annealing/thresholding schemes (Bowman et al., 2016; Fu et al., 2019; Higgins et al., 2017; Li et al., 2019), decoder architectures (Yang et al., 2017; Dieng et al., 2018), auxiliary loss (Zhao et al., 2017), semi-amortized inference (Kim et al., 2018a), aggressive encoder training schedule (He et al., 2019), batch normalized inference (Zhu et al., 2020) and flexible posterior (Fang et al., 2019). Subramanian et al. (2018) have shown some promise that general encoder can benefit language generation. Transformers (Vaswani et al., 2017) are recently considered in VAEs for classification (Gururangan et al., 2019) and storytelling (Wang and Wan, 2019). Pre-training VAEs has been recently considered in conditional text generation to amortize the training of decoders and to allow easy adaptation in new"
2020.emnlp-main.378,2020.acl-main.235,0,0.0152864,"ace (Bowman et al., 2016; Kim et al., 2018b). Its modeling capacity and empirical performance is somewhat limited, partially due to the KL vanishing issue described in Section 4.3. Several attempts have been made to alleviate this issue, including different KL annealing/thresholding schemes (Bowman et al., 2016; Fu et al., 2019; Higgins et al., 2017; Li et al., 2019), decoder architectures (Yang et al., 2017; Dieng et al., 2018), auxiliary loss (Zhao et al., 2017), semi-amortized inference (Kim et al., 2018a), aggressive encoder training schedule (He et al., 2019), batch normalized inference (Zhu et al., 2020) and flexible posterior (Fang et al., 2019). Subramanian et al. (2018) have shown some promise that general encoder can benefit language generation. Transformers (Vaswani et al., 2017) are recently considered in VAEs for classification (Gururangan et al., 2019) and storytelling (Wang and Wan, 2019). Pre-training VAEs has been recently considered in conditional text generation to amortize the training of decoders and to allow easy adaptation in new generation tasks (Duan et al., 2019). All these efforts utilize simple LSTM (Hochreiter and Schmidhuber, 1997) and shallow Transformer (Vaswani et a"
2020.emnlp-main.378,J93-2004,0,\N,Missing
2020.emnlp-main.378,P15-1107,0,\N,Missing
2020.emnlp-main.378,I17-1099,0,\N,Missing
2020.emnlp-main.378,N18-1202,0,\N,Missing
2020.emnlp-main.378,K16-1002,0,\N,Missing
2020.emnlp-main.378,N19-1423,0,\N,Missing
2020.emnlp-main.378,D19-1370,0,\N,Missing
2020.emnlp-main.378,D19-1407,1,\N,Missing
2020.emnlp-main.378,D19-1190,1,\N,Missing
2020.emnlp-main.378,D18-3004,0,\N,Missing
2020.emnlp-main.378,N19-1125,1,\N,Missing
2020.emnlp-main.378,N19-1021,1,\N,Missing
2020.emnlp-main.463,N19-4009,0,0.0767189,"Missing"
2020.emnlp-main.463,P19-1558,0,0.130843,"Missing"
2020.findings-emnlp.157,2020.emnlp-main.703,1,0.827082,"MM agent recursively models conversations with instances of itself to choose the right questions to ask (and answers to give) to reach the goal. Introduction A key challenge for embodied language is moving beyond instruction following to instruction generation, which can require understanding the listener. The turn-based dialogue paradigm raises a myriad of new research questions, from grounded versions of traditional problems like co-reference resolution (Das et al., 2017a) to explicitly modeling theory of mind in order to consider the listener’s ability to understand generated instructions (Bisk et al., 2020). In this paper, we develop end-to-end dialogue agents to navigate photorealistic, indoor scenes to reach goal rooms. We train agents using the human-human Collaborative Vision-andDialogue Navigation (CVDN) (Thomason et al., 2019) dataset. CVDN dialogues are turn-based, with a navigator following guide instructions and asking questions when needed. Modeling turn-based dialogues involves four core challenges: C1 A navigator deciding when to ask a question. C2 Generating navigator questions. C3 Generating guide question answers. C4 Generating navigator actions. Prior work has addressed individua"
2020.findings-emnlp.157,P82-1020,0,0.80111,"Missing"
2020.findings-emnlp.157,2020.acl-main.232,0,0.312223,"or generating an answer to estimate their effects on navigation. Viewed as a single system, the agents cooperatively search through the space of dialogues to efficiently perform embodied navigation. 2 Related Work and Background We build on research in multimodal navigation and the wider literature involving goal oriented dialogue. Table 1 summarizes how our work differs from existing work in vision-and-language navigation and task-oriented dialogue modelling. Instruction Following tasks an embodied agent with interpreting natural language instructions and visual observations to reach a goal (Jayannavar et al., 2020; Wang et al., 2019; Ma et al., 2019; Anderson et al., 2018; Chen and Mooney, 2011). These instructions describe step-by-step actions the agent needs to take, and can involve the creation of speaker models for data augmentation that provide additional instructions (Fried et al., 2018). This paradigm has been extended to longer trajectories and outdoor environments (Chen et al., 2019), as well as to agents in the real world (Chai et al., 2018; Tellex et al., 2014). In this work, we focus on the the simulated, photorealistic indoor environments of the MatterPort dataset (Chang et al., 2017), and"
2020.findings-emnlp.157,P17-1163,0,0.0602341,"Missing"
2020.findings-emnlp.157,P19-1537,0,0.0224005,"d et al., 2018). This paradigm has been extended to longer trajectories and outdoor environments (Chen et al., 2019), as well as to agents in the real world (Chai et al., 2018; Tellex et al., 2014). In this work, we focus on the the simulated, photorealistic indoor environments of the MatterPort dataset (Chang et al., 2017), and go beyond instruction following alone to a twoagent dialogue setting. Navigation Dialogues task a navigator and a guide to cooperate to find a destination. Previous work includes substantial information asymmetry between the navigator and guide (de Vries et al., 2018; Narayan-Chen et al., 2019). Information asymmetry can take the form of the navigator seeing a bird’s eye, abstract semantic map while the guide sees egocentric simulation frames (de Vries et al., 2018), affecting the kind of dialog possible when low-level visual cues cannot be grounded by the navigator. Other work only investigates the navigation portion of the dialogue without considering text question generation and answering (Thomason et al., 2019). Going beyond models that perform navigation from dialogue history alone (Wang et al., 2020; Zhu et al., 2020; Hao et al., 2020), or decide when to ask navigator question"
2020.findings-emnlp.157,D19-1063,0,0.0523249,"Missing"
2020.findings-emnlp.157,P02-1040,0,0.107036,"tion with respect to the goal location. Dialogue navigation proceeds by iterating through the three roles until either the N avigator loc = p0 ; hist = t0 ; ~a ∼ N (hist); loc, hist = update(~a, loc, hist); while ~a 6= STOP and len(hist) &lt; 20 do q ∼ Q(hist, loc) ; // Question ~s = path(loc, goal, horizon = 5) ; o ∼ O(hist, loc, q, ~s) ; // Answer hist ← hist + (q, o); for a ∈ N (hist) do loc ← loc + a ; // Move hist ← hist + a; end end return (goal − t0 ) − (loc − t0 ) chooses to stop or a maximum number of turns are played (Algorithm 1). In addition to “Goal Progress”, we report BLEU scores (Papineni et al., 2002) for evaluating the generation of questions and answers by comparing against human questions and answers. Note, in our dialogue setting, Goal Progress also implicitly measures the utility of generated language and is therefore complementary to BLEU when evaluating utility versus fluency. 4 Models We introduce the Recursive Mental Model (RMM) as an initial approach to our new full dialogue CVDN task formulation. Key to this approach is allowing component models (N avigator, Questioner, and Guide) to learn from each other and roll out possible dialogues and trajectories. We compare our model to"
2020.findings-emnlp.157,D17-1237,1,0.854087,", 2017; Chai et al., 2018). Modeling goal-oriented dialogue requires skills that go beyond language modeling, such as asking questions to clearly define a user request, querying knowledge bases, and interpreting results from queries as options to complete a transaction. Many recent task oriented systems are data-driven and trained end-to-end using semisupervised or transfer learning methods (Ham et al., 2020; Mrksic et al., 2017). However, these datadriven approaches may lack grounding between the text and the environment state. Reinforcement learning-based dialogue modeling (Su et al., 2016; Peng et al., 2017; Liu et al., 2017) can improve completion rate and user experience by helping ground conversational data to environments. 1733 3 Task and Data Algorithm 1: Dialogue Navigation Our work creates a two-agent dialogue task, building on the CVDN dataset (Thomason et al., 2019) of human-human dialogues. In that dataset, a human N avigator and Guide collaborate to find a goal room containing a target object. The N avigator moves through the environment, and the Guide views this navigation until the N avigator asks a question in natural language (C1, C2). Then, the Guide can see the next few steps a"
2020.findings-emnlp.157,P16-1230,0,0.0323414,"Bordes and Weston, 2017; Chai et al., 2018). Modeling goal-oriented dialogue requires skills that go beyond language modeling, such as asking questions to clearly define a user request, querying knowledge bases, and interpreting results from queries as options to complete a transaction. Many recent task oriented systems are data-driven and trained end-to-end using semisupervised or transfer learning methods (Ham et al., 2020; Mrksic et al., 2017). However, these datadriven approaches may lack grounding between the text and the environment state. Reinforcement learning-based dialogue modeling (Su et al., 2016; Peng et al., 2017; Liu et al., 2017) can improve completion rate and user experience by helping ground conversational data to environments. 1733 3 Task and Data Algorithm 1: Dialogue Navigation Our work creates a two-agent dialogue task, building on the CVDN dataset (Thomason et al., 2019) of human-human dialogues. In that dataset, a human N avigator and Guide collaborate to find a goal room containing a target object. The N avigator moves through the environment, and the Guide views this navigation until the N avigator asks a question in natural language (C1, C2). Then, the Guide can see th"
2020.findings-emnlp.17,D18-1547,0,0.140555,"Missing"
2020.findings-emnlp.17,P19-1360,0,0.175832,"ew domains. In summary, our key contributions are three-fold: • A new benchmark F EW S HOT WOZ is introduced to simulate the few-shot adaptation setting where only a handful of training data from each domain is available. • We propose a new model SC-GPT. To our best knowledge, this work is the first study of exploiting state-of-the-art pre-trained language models for NLG in task-oriented dialog systems. • On the MultiWOZ dataset, SC-GPT creates a new SOTA, outperforming previous models by 4 points in BLEU. On F EW S HOTWOZ, SC-GPT outperforms several strong baselines such as SC-LSTM and HDSA (Chen et al., 2019), showing that SC-GPT adapts to new domain much more effectively, requiring much smaller amounts of in-domain labels. 2 Background A typical task-oriented spoken dialog system uses a pipeline architecture, as shown in Figure 1 (a), where each dialog turn is processed using a fourstep procedure. (i) Transcriptions of user’s input are first passed to the natural language understanding (NLU) module, where the user’s intention and other key information are extracted. (ii) This information is then formatted as the input to dialog state tracking (DST), which maintains the current state of the dialog"
2020.findings-emnlp.17,N19-1423,0,0.0541816,"dialog act of an utterance may exist in multiple domains. We choose to keep utterances whose dialog act appears only in one domain. Similar delexicalising processing is applied to ensure that each dialog act has only one target utterance. Finally, to simulate the few-shot learning in practice, we randomly sample 50 training examples for each domain, except the Taxi domain, which has 40 examples. 5 Related Work Pre-trained Models. Pre-trained language models (PLMs) have substantially advanced the stateof-the-art across a variety of natural language processing (NLP) tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Keskar et al., 2019; Raffel et al., 2019; Peng et al., 2020). PLMs are often trained to predict words based on their context on massive text data, and the learned models can be fine-tuned to adapt to various downstream tasks. The closest line of research to ours are GPT-2 (Radford et al.), CTRL (Keskar et al., 2019) and Grover (Zellers et al., 2019). GPT2 first investigated missive Transformer-based autoregressive language models with large-scale text data for pre-training. After fine-tuning, GPT-2 achieves drastic improvements on several generation tasks"
2020.findings-emnlp.17,P16-2008,0,0.23625,"Missing"
2020.findings-emnlp.17,W17-5525,0,0.22897,"Missing"
2020.findings-emnlp.17,N18-1202,0,0.0184667,"s-domain dataset, the dialog act of an utterance may exist in multiple domains. We choose to keep utterances whose dialog act appears only in one domain. Similar delexicalising processing is applied to ensure that each dialog act has only one target utterance. Finally, to simulate the few-shot learning in practice, we randomly sample 50 training examples for each domain, except the Taxi domain, which has 40 examples. 5 Related Work Pre-trained Models. Pre-trained language models (PLMs) have substantially advanced the stateof-the-art across a variety of natural language processing (NLP) tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Keskar et al., 2019; Raffel et al., 2019; Peng et al., 2020). PLMs are often trained to predict words based on their context on massive text data, and the learned models can be fine-tuned to adapt to various downstream tasks. The closest line of research to ours are GPT-2 (Radford et al.), CTRL (Keskar et al., 2019) and Grover (Zellers et al., 2019). GPT2 first investigated missive Transformer-based autoregressive language models with large-scale text data for pre-training. After fine-tuning, GPT-2 achieves drastic improvements on sev"
2020.findings-emnlp.17,P98-1116,0,0.476705,"emantic form selected by a dialog policy) into a final response in natural language. Hence, the response should be adequate to represent semantic dialog actions, and fluent to engage users’ attention. As the ultimate interface to interacts with users, NLG plays a significant impact on the users’ experience. Existing methods for NLG can be broadly summarized into two major categories. (i) Template1 Semantically-Conditioned Generative Pre-Training based methods require domain experts to handcraft templates for each domain, and the system fills in slot-values afterward (Cheyer and Guzzoni, 2014; Langkilde and Knight, 1998). Thus, the produced responses are often adequate to contain the required semantic information, but not always fluent and nature, hurting users’ experiences. (ii) Statistical language models such as neural networks (Gao et al., 2019) learn to generate fluent responses via training from labelled corpus. One canonical model is semantically conditioned LSTM (SC-LSTM) (Wen et al., 2015b), which encodes dialog acts with onehot representations and uses it as an extra feature to inform the sentence generation process. Despite its good performance on simple domains, it requires large amounts of domain"
2020.findings-emnlp.17,K17-1044,0,0.0210435,"uch as Rasa4 , Microsoft Bot Framework5 , and Conversational Learner6 , and chit-chat systems such as XiaoIce (Zhou et al.), DialoGPT (Zhang et al., 2019), Meena (Adiwardana et al., 2020). In this paper, we focus on task-oriented systems, particularly the NLG module. With the blooming of deep learning, neural sequential models have shown powerful capability and flexibility in NLG. Extensive efforts have been made, including new architecture choices such as RNNs (Wen et al., 2015a), attention RNNs (Duˇsek and Jurˇc´ıcˇ ek, 2016), SC-LSTM (Wen et al., 2015b) and its variants (Tran et al., 2017; Tran and Nguyen, 2017), as well as learning objectives (Zhu et al., 2019; Zhu, 2020; Mi et al., 2019). However, they all require large amounts of annotated data to reach satisfactory performance. A more realistic scenario is to require much less labeling and improve the sample efficiency of models, This is especially important when deploying the models to new domains, where dialog acts need to be labelled from scratch. Our paper aims to formally set up such a research scenario by proposing a new dataset F EW S HOTWOZ, and a new model SC-GPT. 176 4 https://rasa.com/ https://dev.botframework.com/ 6 https://www.micros"
2020.findings-emnlp.17,N16-1014,1,0.900078,"Missing"
2020.findings-emnlp.17,W17-5528,0,0.0156309,"ed dialog systems such as Rasa4 , Microsoft Bot Framework5 , and Conversational Learner6 , and chit-chat systems such as XiaoIce (Zhou et al.), DialoGPT (Zhang et al., 2019), Meena (Adiwardana et al., 2020). In this paper, we focus on task-oriented systems, particularly the NLG module. With the blooming of deep learning, neural sequential models have shown powerful capability and flexibility in NLG. Extensive efforts have been made, including new architecture choices such as RNNs (Wen et al., 2015a), attention RNNs (Duˇsek and Jurˇc´ıcˇ ek, 2016), SC-LSTM (Wen et al., 2015b) and its variants (Tran et al., 2017; Tran and Nguyen, 2017), as well as learning objectives (Zhu et al., 2019; Zhu, 2020; Mi et al., 2019). However, they all require large amounts of annotated data to reach satisfactory performance. A more realistic scenario is to require much less labeling and improve the sample efficiency of models, This is especially important when deploying the models to new domains, where dialog acts need to be labelled from scratch. Our paper aims to formally set up such a research scenario by proposing a new dataset F EW S HOTWOZ, and a new model SC-GPT. 176 4 https://rasa.com/ https://dev.botframework.c"
2020.findings-emnlp.17,2021.ccl-1.108,0,0.150947,"Missing"
2020.findings-emnlp.17,P10-1157,0,0.151859,"Missing"
2020.findings-emnlp.17,W15-4639,0,0.0703327,"Missing"
2020.findings-emnlp.17,N16-1015,0,0.0172994,"hen the number of possible combinations of dialog acts grows exponentially with the number of slots in more complex domains. We revisit the current research benchmarks for NLG, and notice that each dialog domain is extensively labelled to favor model training. However, this is in contrast to the real-world application scenarios, where only very limited amounts of labelled data are available for new domains. To simulate such a few-shot learning setting, we have developed a new benchmark dataset, called F EW S HOTWOZ, based on the MultiWOZ (Budzianowski et al., 2018) and Cambridge NLG datasets (Wen et al., 2016a). F EW S HOT WOZ consists of dialog utterances from 7 domains. For each domain, we provide less than 50 labeled utterances for finetuning. We believe that F EW S HOT WOZ can better inspire research to address the challenge of learning data-hungry statistical models with very limited amounts of labelled data in real-world scenarios. To deal with the challenge of few-shot learning, we develop the SC-GPT model. SC-GPT is a multi172 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 172–182 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Task Or"
2020.findings-emnlp.17,D15-1199,0,0.172566,"Missing"
2020.findings-emnlp.17,2020.sigdial-1.33,1,0.82731,"d chit-chat systems such as XiaoIce (Zhou et al.), DialoGPT (Zhang et al., 2019), Meena (Adiwardana et al., 2020). In this paper, we focus on task-oriented systems, particularly the NLG module. With the blooming of deep learning, neural sequential models have shown powerful capability and flexibility in NLG. Extensive efforts have been made, including new architecture choices such as RNNs (Wen et al., 2015a), attention RNNs (Duˇsek and Jurˇc´ıcˇ ek, 2016), SC-LSTM (Wen et al., 2015b) and its variants (Tran et al., 2017; Tran and Nguyen, 2017), as well as learning objectives (Zhu et al., 2019; Zhu, 2020; Mi et al., 2019). However, they all require large amounts of annotated data to reach satisfactory performance. A more realistic scenario is to require much less labeling and improve the sample efficiency of models, This is especially important when deploying the models to new domains, where dialog acts need to be labelled from scratch. Our paper aims to formally set up such a research scenario by proposing a new dataset F EW S HOTWOZ, and a new model SC-GPT. 176 4 https://rasa.com/ https://dev.botframework.com/ 6 https://www.microsoft.com/enus/research/project/conversation-learner/ 5 Model S"
2020.findings-emnlp.17,D19-1123,1,0.839483,"onal Learner6 , and chit-chat systems such as XiaoIce (Zhou et al.), DialoGPT (Zhang et al., 2019), Meena (Adiwardana et al., 2020). In this paper, we focus on task-oriented systems, particularly the NLG module. With the blooming of deep learning, neural sequential models have shown powerful capability and flexibility in NLG. Extensive efforts have been made, including new architecture choices such as RNNs (Wen et al., 2015a), attention RNNs (Duˇsek and Jurˇc´ıcˇ ek, 2016), SC-LSTM (Wen et al., 2015b) and its variants (Tran et al., 2017; Tran and Nguyen, 2017), as well as learning objectives (Zhu et al., 2019; Zhu, 2020; Mi et al., 2019). However, they all require large amounts of annotated data to reach satisfactory performance. A more realistic scenario is to require much less labeling and improve the sample efficiency of models, This is especially important when deploying the models to new domains, where dialog acts need to be labelled from scratch. Our paper aims to formally set up such a research scenario by proposing a new dataset F EW S HOTWOZ, and a new model SC-GPT. 176 4 https://rasa.com/ https://dev.botframework.com/ 6 https://www.microsoft.com/enus/research/project/conversation-learner"
2020.findings-emnlp.17,C98-1112,0,\N,Missing
2020.findings-emnlp.17,P16-1162,0,\N,Missing
2020.findings-emnlp.209,D18-1547,0,0.0413183,"he correct classification. Given an equal mixture of real data samples and generated samples from the generator Genθ , the loss function for the discriminator Dφ is: LD (φ) = E((s,a)sim )∼Genθ (log(1 − Dφ ((s, a)sim ))) − E(s,a)∼data (Dφ (Encω (s), a)real )). (7) After the adversarial training is finished, we will keep the discriminator Dφ as the reward function for future dialog agent training while the generator Genθ will be discarded. Next, we discuss a suitable experimental environment for validating the presented method. 4 4.1 Experiemntal Setup Dataset and Training Environment MultiWOZ (Budzianowski et al., 2018) is a multi-domain dialogue dataset spanning 7 distinct domains1 , and 10, 438 dialogues. The main scenario in this dataset is that a dialogue agent is trying to satisfy the demand from tourists such as booking a restaurant or recommending a hotel with specific requirements. The average number of turns is 8.93 and 15.39 for single and multidomain dialogues, respectively. ConvLab (Lee et al., 2019) is an open-source multi-domain end-to-end dialogue system platform offering the annotated MultiWOZ dataset and associated pre-trained reference models. We reuse the rule-based dialogue state tracker"
2020.findings-emnlp.209,P17-1045,1,0.928098,"Missing"
2020.findings-emnlp.209,W18-5041,0,0.501919,"eward will be a large negative value; if the dialogue is still ongoing, a small negative value will be returned to encourage shorter sessions (Peng et al., 2018b). However, the rule-based solution is inflexible as it assigns the same negative reward to all the system actions before the dialogue ends. The sparse reward makes the qualities of different actions indistinguishable. Additionally, the rule-based approaches only return a meaningful reward when dialogue finishes, which can delay the penalty for low-quality actions and a high reward for high-quality ones during the conversation itself. Liu and Lane (2018) address the difficulties listed above by employing adversarial training for policy learning by jointly training two systems: (1) a policy model that decides which action to take at each turn, and (2) a discriminator that marks if a dialogue was successful or not. Feedback from the discriminator is used as a reward to push the policy model to complete a task indistinguishably from humans. Improving upon this solution, Takanobu et al. (2019) propose to replace the discriminator with a reward function that acts at the dialogue action level and returns the reward 2308 Findings of the Association"
2020.findings-emnlp.209,P18-1203,1,0.912303,"urrent context of the conversation (Chen et al., 2017). The development of RL in robotics and other domains has brought a new view on learning dialogue policies (Williams and Young, 2007; Gaˇsi´c and Young, 2014; Su et al., 2017): it allows us to train with far more data than can be feasibly collected from actual users. The aim of TDSs is to maximize positive user feedback. TDSs based on RL are amenable to training with user simulators instead of real humans (Schatzmann et al., 2007; Li et al., 2016). User simulators rely on a reward function that scores system actions given dialogue context (Peng et al., 2018b; Williams et al., 2017; Dhingra et al., 2016; Su et al., 2016). The most straightforward way to design a dialogue reward function is to score the agent based on the dialogue status in a rule-based fashion: if the dialogue ends successfully, a large positive reward will be returned; if the dialogue fails, the reward will be a large negative value; if the dialogue is still ongoing, a small negative value will be returned to encourage shorter sessions (Peng et al., 2018b). However, the rule-based solution is inflexible as it assigns the same negative reward to all the system actions before the"
2020.findings-emnlp.209,D17-1237,1,0.855125,"generated dialogues by distinguishing human-generated dialogues from machine-generated ones, and then make full use of the learned information to guide the dialogue policy learning in a new domain in the style of transfer learning. To summarize, our contributions are: • A reward learning method that is applicable to off-policy RL methods in dialogue training. • A reward learning method that can alleviate the problem of local optima for adversarial dialogue training. • A reward function that can transfer knowledge learned in existing domains to a new dialogue domain. 2 Related Work RL methods (Peng et al., 2017; Lipton et al., 2018; Li et al., 2017; Su et al., 2018; Dhingra et al., 2016; Williams et al., 2017; Li et al., 2019), have been widely utilized to train a dialogue agent by interacting with users. The reward used to update the dialogue policy is usually from a reward function predefined with domain knowledge and it could become very complex, e.g., in the case of multi-domain dialogue scenarios. To provide the dialogue policy with a high quality reward signal, Peng et al. (2018a) proposed to make use of the adversarial loss as an extra critic in addition to shape the main reward function. Ins"
2020.findings-emnlp.209,N07-2038,0,0.564354,"ers assistance with completing tasks. TDSs need dialogue policies to select appropriate actions at each dialogue step according to the current context of the conversation (Chen et al., 2017). The development of RL in robotics and other domains has brought a new view on learning dialogue policies (Williams and Young, 2007; Gaˇsi´c and Young, 2014; Su et al., 2017): it allows us to train with far more data than can be feasibly collected from actual users. The aim of TDSs is to maximize positive user feedback. TDSs based on RL are amenable to training with user simulators instead of real humans (Schatzmann et al., 2007; Li et al., 2016). User simulators rely on a reward function that scores system actions given dialogue context (Peng et al., 2018b; Williams et al., 2017; Dhingra et al., 2016; Su et al., 2016). The most straightforward way to design a dialogue reward function is to score the agent based on the dialogue status in a rule-based fashion: if the dialogue ends successfully, a large positive reward will be returned; if the dialogue fails, the reward will be a large negative value; if the dialogue is still ongoing, a small negative value will be returned to encourage shorter sessions (Peng et al., 2"
2020.findings-emnlp.209,W17-5518,0,0.0528842,"using both on-policy and off-policy RL methods; and (2) has potential to transfer knowledge from existing domains to a new domain. 1 Introduction Task-oriented Dialogue Systems (TDSs), such as Siri, Google Assistant, and Amazon Alexa, aim to offer users assistance with completing tasks. TDSs need dialogue policies to select appropriate actions at each dialogue step according to the current context of the conversation (Chen et al., 2017). The development of RL in robotics and other domains has brought a new view on learning dialogue policies (Williams and Young, 2007; Gaˇsi´c and Young, 2014; Su et al., 2017): it allows us to train with far more data than can be feasibly collected from actual users. The aim of TDSs is to maximize positive user feedback. TDSs based on RL are amenable to training with user simulators instead of real humans (Schatzmann et al., 2007; Li et al., 2016). User simulators rely on a reward function that scores system actions given dialogue context (Peng et al., 2018b; Williams et al., 2017; Dhingra et al., 2016; Su et al., 2016). The most straightforward way to design a dialogue reward function is to score the agent based on the dialogue status in a rule-based fashion: if t"
2020.findings-emnlp.209,P16-1230,0,0.0417952,"Missing"
2020.findings-emnlp.209,D18-1416,1,0.597324,"logues from machine-generated ones, and then make full use of the learned information to guide the dialogue policy learning in a new domain in the style of transfer learning. To summarize, our contributions are: • A reward learning method that is applicable to off-policy RL methods in dialogue training. • A reward learning method that can alleviate the problem of local optima for adversarial dialogue training. • A reward function that can transfer knowledge learned in existing domains to a new dialogue domain. 2 Related Work RL methods (Peng et al., 2017; Lipton et al., 2018; Li et al., 2017; Su et al., 2018; Dhingra et al., 2016; Williams et al., 2017; Li et al., 2019), have been widely utilized to train a dialogue agent by interacting with users. The reward used to update the dialogue policy is usually from a reward function predefined with domain knowledge and it could become very complex, e.g., in the case of multi-domain dialogue scenarios. To provide the dialogue policy with a high quality reward signal, Peng et al. (2018a) proposed to make use of the adversarial loss as an extra critic in addition to shape the main reward function. Inspired by the success of adversarial learning in other r"
2020.findings-emnlp.209,D19-1010,0,0.638153,"ful reward when dialogue finishes, which can delay the penalty for low-quality actions and a high reward for high-quality ones during the conversation itself. Liu and Lane (2018) address the difficulties listed above by employing adversarial training for policy learning by jointly training two systems: (1) a policy model that decides which action to take at each turn, and (2) a discriminator that marks if a dialogue was successful or not. Feedback from the discriminator is used as a reward to push the policy model to complete a task indistinguishably from humans. Improving upon this solution, Takanobu et al. (2019) propose to replace the discriminator with a reward function that acts at the dialogue action level and returns the reward 2308 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 2308–2317 c November 16 - 20, 2020. 2020 Association for Computational Linguistics for the given action relying on the dialogue state, system action, and next dialogue state as its input. However, the described methods are limited to policy gradient-based algorithms, such as REINFORCE (Williams, 1992) and Proximal Policy Optimization (PPO) (Schulman et al., 2017), to alternatively update the"
2020.findings-emnlp.209,P17-1062,0,0.246959,"e conversation (Chen et al., 2017). The development of RL in robotics and other domains has brought a new view on learning dialogue policies (Williams and Young, 2007; Gaˇsi´c and Young, 2014; Su et al., 2017): it allows us to train with far more data than can be feasibly collected from actual users. The aim of TDSs is to maximize positive user feedback. TDSs based on RL are amenable to training with user simulators instead of real humans (Schatzmann et al., 2007; Li et al., 2016). User simulators rely on a reward function that scores system actions given dialogue context (Peng et al., 2018b; Williams et al., 2017; Dhingra et al., 2016; Su et al., 2016). The most straightforward way to design a dialogue reward function is to score the agent based on the dialogue status in a rule-based fashion: if the dialogue ends successfully, a large positive reward will be returned; if the dialogue fails, the reward will be a large negative value; if the dialogue is still ongoing, a small negative value will be returned to encourage shorter sessions (Peng et al., 2018b). However, the rule-based solution is inflexible as it assigns the same negative reward to all the system actions before the dialogue ends. The spars"
2020.findings-emnlp.209,P19-3011,1,\N,Missing
2020.sigdial-1.37,P19-1080,0,0.0137241,"inquire about information to accomplish tasks for users. Goal-oriented dialog systems can be grouped into three classes based on their architectures, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018; Wu et al., 2019; Gao et al., 2019b), and a joint word-level policy model combines dia"
2020.sigdial-1.37,W19-5932,0,0.0447506,"Missing"
2020.sigdial-1.37,N18-2118,0,0.0666864,"dialog systems, which are designed to mimic human conversations rather than complete speciﬁc tasks and are often implemented as end-to-end systems, a goal-oriented dialog system has access to an external database on which to inquire about information to accomplish tasks for users. Goal-oriented dialog systems can be grouped into three classes based on their architectures, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint mod"
2020.sigdial-1.37,D19-5602,0,0.0328958,"Missing"
2020.sigdial-1.37,D18-1547,0,0.142513,"Kim et al., 2019; Li et al., 2020). Although end-to-end systems are evaluated in a system-wise manner, none of such systems is compared with its pipeline counterpart. Furthermore, unlike the component-wise assessment, system-wise evaluation requires simulated users or human users to interact with the system to be evaluated via multi-turn conversations to complete tasks. To this end, we conduct both simulated and human evaluations on dialog systems with a wide variety of conﬁgurations and settings using a standardized dialog system platform, Convlab (Lee et al., 2019b), on the MultiWOZ corpus (Budzianowski et al., 2018). Our work attempts to shed light on evaluating and comparing goal-oriented dialog systems by conducting a system-wise evaluation and a detailed empirical analysis. Speciﬁcally, we strive to answer the following research questions: (RQ1) Which conﬁgurations lead to better goaloriented dialog systems? (§3.1); (RQ2) Whether the component-wise, single-turn metrics are consistent with system-wise, multi-turn metrics for evaluation? (§3.2); (RQ3) How does the performance vary when a system is evaluated using tasks of different complexities, e.g., from single-domain to multi-domain tasks? (§3.3); (R"
2020.sigdial-1.37,P19-1360,0,0.188546,"s the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018; Wu et al., 2019; Gao et al., 2019b), and a joint word-level policy model combines dialog policy and NLG (Chen et al., 2019; Zhao et al., 2019; Budzianowski and Vuli´c, 2019). It is particularly challenging to properly evaluate and compare the overall performance of goaloriented dialog systems due to the wide variety of system conﬁgurations and evaluation settings. Nu297 Proceedings of the SIGdial 2020 Conference, pages 297–310 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics merous approaches have been proposed to tackle different components in pipeline systems, whereas these modules are merely evaluated separately. Most studies only compare the proposed models with baselines"
2020.sigdial-1.37,N19-1423,0,0.0233249,"in MultiWOZ, and replicate the performance reported in the original papers or on the leaderboard. 299 NLU A natural language understanding module identiﬁes user intents and extracts associated information from users’ raw utterances. We consider 1 All state-of-the-art models mentioned in this paper are based on the open-source code that is available and executable as of February 29, 2020. two approaches that can handle multi-intents as reference: a RNN-based model MILU which extends (Hakkani-T¨ur et al., 2016) and is ﬁne-tuned on multiple domains, intents and slots; and a ﬁne-tuned BERT model (Devlin et al., 2019). Following the joint tagging scheme (Zheng et al., 2017), the labels of intent detection and slot ﬁlling are annotated for domain classiﬁcation during training. Both models use dialog history up to the last dialog turn as context. Note that there can be multiple intents or slots in one sentence, we calculate two F1 scores for intents and slots, respectively. DST A dialog state tracker encodes the extracted information as a compact set of dialog state that contains a set of informable slots and their corresponding values (user constraints), and a set of requested slots2 . We have implemented a"
2020.sigdial-1.37,W14-4337,0,0.0315072,"eturn evaluation, BLEU, inform rate and success rate are provided. 3 3.1 Empirical Analysis Performance under Different Settings (RQ1) We compare the performance of three types of systems, pipeline, joint-model and end-to-end. Results in Table 1 show that pipeline systems often achieve better overall performance than the joint models and end-to-end systems because using ﬁne-grained labels at the component level can help pipeline systems improve the task success rate. 2 Dialog state can include everything a system must know in order to make a decision about what to do next, e.g., DSTC2 corpus (Henderson et al., 2014) contains search method representing user intents in the dialog state, but only aforementioned items are taken into account as our experiments are conducted on MultiWOZ in this paper. 300 NLU with DST or joint DST It is essential to predict dialog states to determine what a user has expressed and wants to inquire. The dialog state is used to query the database, predict the system dialog act, and generate a dialog response. Although many studies have focused on the wordlevel DST that directly predicts the state using the ID 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 Conﬁguration NLU DST Policy NLG"
2020.sigdial-1.37,P17-1045,1,0.88059,"hree classes based on their architectures, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018; Wu et al., 2019; Gao et al., 2019b), and a joint word-level policy model combines dialog policy and NLG (Chen et al., 2019; Zhao et al., 2019; Budzianowski and Vuli´c, 2019). It is partic"
2020.sigdial-1.37,P19-1546,0,0.197239,"rall performance of different combinations (Kim et al., 2019; Li et al., 2020). Although end-to-end systems are evaluated in a system-wise manner, none of such systems is compared with its pipeline counterpart. Furthermore, unlike the component-wise assessment, system-wise evaluation requires simulated users or human users to interact with the system to be evaluated via multi-turn conversations to complete tasks. To this end, we conduct both simulated and human evaluations on dialog systems with a wide variety of conﬁgurations and settings using a standardized dialog system platform, Convlab (Lee et al., 2019b), on the MultiWOZ corpus (Budzianowski et al., 2018). Our work attempts to shed light on evaluating and comparing goal-oriented dialog systems by conducting a system-wise evaluation and a detailed empirical analysis. Speciﬁcally, we strive to answer the following research questions: (RQ1) Which conﬁgurations lead to better goaloriented dialog systems? (§3.1); (RQ2) Whether the component-wise, single-turn metrics are consistent with system-wise, multi-turn metrics for evaluation? (§3.2); (RQ3) How does the performance vary when a system is evaluated using tasks of different complexities, e.g."
2020.sigdial-1.37,W16-3602,0,0.106933,"eciﬁc tasks and are often implemented as end-to-end systems, a goal-oriented dialog system has access to an external database on which to inquire about information to accomplish tasks for users. Goal-oriented dialog systems can be grouped into three classes based on their architectures, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint word"
2020.sigdial-1.37,P19-3011,1,0.912839,"Missing"
2020.sigdial-1.37,P18-1133,0,0.104061,"s, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018; Wu et al., 2019; Gao et al., 2019b), and a joint word-level policy model combines dialog policy and NLG (Chen et al., 2019; Zhao et al., 2019; Budzianowski and Vuli´c, 2019). It is particularly challenging to properly evalu"
2020.sigdial-1.37,N18-1187,0,0.0173212,"their architectures, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018; Wu et al., 2019; Gao et al., 2019b), and a joint word-level policy model combines dialog policy and NLG (Chen et al., 2019; Zhao et al., 2019; Budzianowski and Vuli´c, 2019). It is particularly challenging"
2020.sigdial-1.37,D17-1237,1,0.937191,"as end-to-end systems, a goal-oriented dialog system has access to an external database on which to inquire about information to accomplish tasks for users. Goal-oriented dialog systems can be grouped into three classes based on their architectures, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and D"
2020.sigdial-1.37,P19-1079,0,0.0194091,"hich are designed to mimic human conversations rather than complete speciﬁc tasks and are often implemented as end-to-end systems, a goal-oriented dialog system has access to an external database on which to inquire about information to accomplish tasks for users. Goal-oriented dialog systems can be grouped into three classes based on their architectures, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (b"
2020.sigdial-1.37,D19-1013,0,0.0219569,"in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018; Wu et al., 2019; Gao et al., 2019b), and a joint word-level policy model combines dialog policy and NLG (Chen et al., 2019; Zhao et al., 2019; Budzianowski and Vuli´c, 2019). It is particularly challenging to properly evaluate and compare th"
2020.sigdial-1.37,P18-2069,0,0.0258295,"ng. Both models use dialog history up to the last dialog turn as context. Note that there can be multiple intents or slots in one sentence, we calculate two F1 scores for intents and slots, respectively. DST A dialog state tracker encodes the extracted information as a compact set of dialog state that contains a set of informable slots and their corresponding values (user constraints), and a set of requested slots2 . We have implemented a rule-based DST to update the slot values in the dialog state based on the output of NLU. We then compare four word-level DST: a multi-domain classiﬁer MDBT (Ramadan et al., 2018) which enumerates all possible candidate slots and values, SUMBT (Lee et al., 2019a) that uses a BERT encoder and a slotutterance matching architecture for classiﬁcation, TRADE (Wu et al., 2019) that shares knowledge among domains to directly generate slot values, and COMER (Ren et al., 2019) which applies a hierarchical encoder-decoder model for state generation. We use two metrics for evaluation. The joint goal accuracy compares the predicted dialog states to the ground truth at each dialog turn, and the output is considered correct if and only if all the predicted values exactly match the g"
2020.sigdial-1.37,D19-1196,0,0.0302745,"Missing"
2020.sigdial-1.37,P18-1136,0,0.0485947,"Missing"
2020.sigdial-1.37,N07-2038,0,0.142779,"goals, the numbers of goals involving 1/2/3 domains are 328/549/123, respectively. 2.3 Evaluation Metrics 2.5 Platform and Simulator We use the open-source end-to-end dialog system platform, ConvLab (Lee et al., 2019b), as our experimental platform. ConvLab enables researchers to develop a dialog system using preferred architectures and supports system-wise simulated evaluation. It also provides an integration of crowdsourcing platforms such as Amazon Mechanical Turk for human evaluation. To automatically evaluate a multi-turn dialog system, Convlab implements an agenda-based user simulator (Schatzmann et al., 2007). Given a user goal, the simulator’s policy uses a stack-like structure with complex hand-crafted heuristics to inform its goal and mimics complex user behaviors during a conversation. Since the system interacts with the simulator in natural language, the user simulator directly takes system utterances as input and outputs a user response. The overall architecture of user simulator is presented in Fig. 3. It consists of three modules: NLU, policy, and NLG. We use the default conﬁguration of the simulator in Convlab: a RNN-based model MILU (Multi-Intent Language Understanding, extended (Hakkani"
2020.sigdial-1.37,W19-5921,0,0.0736932,"onding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018; Wu et al., 2019; Gao et al., 2019b), and a joint word-level policy model combines dialog policy and NLG (Chen et al., 2019; Zhao et al., 2019; Budzianowski and Vuli´c, 2019). It is particularly challenging to properly evaluate and compare the overall performance"
2020.sigdial-1.37,W01-1614,0,0.195413,"k of ro304 bustness in dealing with real human conversations. 4 Related Work Developers have been facing many problems when evaluating a goal-oriented dialog system. A range of well-deﬁned automatic metrics have been designed for different components in the system, e.g., joint goal accuracy in DST and task success rate in policy optimization introduced in Table 2b and 2c. A broadly accepted evaluation scheme for the goaloriented dialog was ﬁrst proposed by PARADISE (Walker et al., 1997). It estimates the user satisfaction by measuring two types of aspects, namely dialog cost and task success. Paek (2001) suggests that a useful dialog metric should provide an estimate of how well the goal is met and allow for a comparative judgement of different systems. Though a model can be optimized against these metrics via supervised learning, each component is trained or evaluated separately, thus difﬁcult to reﬂect real user satisfaction. As human evaluation by asking crowd-sourcing workers to interact with a dialog system is much expensive (Ultes et al., 2013; Su et al., 2016) and prone to be affected by subjective factors (Higashinaka et al., 2010; Schmitt and Ultes, 2015), researchers have tried to r"
2020.sigdial-1.37,J00-3003,0,0.470465,"Missing"
2020.sigdial-1.37,P02-1040,0,0.107816,"log policies: a hand-crafted policy, and a reinforcement learning policy GDPL (Takanobu et al., 2019) that jointly learns a reward function. We also include in our comparison three joint models, known as word-level policies, which combine the policy and the NLG module to produce natural language responses from dialog states. They are MDRG (Wen et al., 2017) where an attention mechanism is conditioned on the dialog states, HDSA (Chen et al., 2019) that decodes response from predicted hierarchical dialog acts, and LaRL (Zhao et al., 2019) which uses a latent action framework. We use BLEU score (Papineni et al., 2002), inform rate and task success rate as metrics for evaluation. Note that the inform rate and task success for evaluating policies are computed at the turn level, while the ones used in system-wise evaluation are computed at the dialog level. NLG A natural language generation module generates a natural language response from a dialog act representation. We experiment with two models: a retrieval-based model that samples a sentence randomly from the corpus using dialog acts, and a generation-based model SCLSTM (Wen et al., 2015) which appends a sentence planning cell in RNN. To evaluate the perf"
2020.sigdial-1.37,P16-1230,0,0.0656448,"Missing"
2020.sigdial-1.37,2020.acl-main.59,1,0.883409,"Missing"
2020.sigdial-1.37,D19-1010,1,0.945403,"ems, a goal-oriented dialog system has access to an external database on which to inquire about information to accomplish tasks for users. Goal-oriented dialog systems can be grouped into three classes based on their architectures, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018;"
2020.sigdial-1.37,P17-4013,0,0.0248449,"Missing"
2020.sigdial-1.37,P19-1078,0,0.0308496,"Missing"
2020.sigdial-1.37,W15-4641,0,0.0212453,"r than complete speciﬁc tasks and are often implemented as end-to-end systems, a goal-oriented dialog system has access to an external database on which to inquire about information to accomplish tasks for users. Goal-oriented dialog systems can be grouped into three classes based on their architectures, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For"
2020.sigdial-1.37,N13-1064,0,0.0307734,"t proposed by PARADISE (Walker et al., 1997). It estimates the user satisfaction by measuring two types of aspects, namely dialog cost and task success. Paek (2001) suggests that a useful dialog metric should provide an estimate of how well the goal is met and allow for a comparative judgement of different systems. Though a model can be optimized against these metrics via supervised learning, each component is trained or evaluated separately, thus difﬁcult to reﬂect real user satisfaction. As human evaluation by asking crowd-sourcing workers to interact with a dialog system is much expensive (Ultes et al., 2013; Su et al., 2016) and prone to be affected by subjective factors (Higashinaka et al., 2010; Schmitt and Ultes, 2015), researchers have tried to realize automatic evaluation of dialog systems. Simulated evaluation (Araki and Doshita, 1996; Eckert et al., 1997) is widely used in recent works (Williams et al., 2017; Peng et al., 2017; Takanobu et al., 2019, 2020) and platforms (Ultes et al., 2017; Lee et al., 2019b; Papangelis et al., 2020; Zhu et al., 2020), where the system interacts with a user simulator which mimics human behaviors. Such evaluation can be conducted at the dialog act or natur"
2020.sigdial-1.37,N19-1123,0,0.091931,"r unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018; Wu et al., 2019; Gao et al., 2019b), and a joint word-level policy model combines dialog policy and NLG (Chen et al., 2019; Zhao et al., 2019; Budzianowski and Vuli´c, 2019). It is particularly challenging to properly evaluate and compare the overall performance of goaloriented dialog systems due to the wide variety of system conﬁgurations and evaluation settings. Nu297 Proceedings of the SIGdial 2020 Conference, pages 297–310 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics merous approaches have been proposed to tackle different components in pipeline systems, whereas these modules are merely evaluated separately. Most studies only compare the proposed models with baselines of the same module"
2020.sigdial-1.37,P97-1035,0,0.908929,"are vulnerable to the variation of human language (e.g., the sentence highlighted in brown in Table 7), which demonstrates a lack of ro304 bustness in dealing with real human conversations. 4 Related Work Developers have been facing many problems when evaluating a goal-oriented dialog system. A range of well-deﬁned automatic metrics have been designed for different components in the system, e.g., joint goal accuracy in DST and task success rate in policy optimization introduced in Table 2b and 2c. A broadly accepted evaluation scheme for the goaloriented dialog was ﬁrst proposed by PARADISE (Walker et al., 1997). It estimates the user satisfaction by measuring two types of aspects, namely dialog cost and task success. Paek (2001) suggests that a useful dialog metric should provide an estimate of how well the goal is met and allow for a comparative judgement of different systems. Though a model can be optimized against these metrics via supervised learning, each component is trained or evaluated separately, thus difﬁcult to reﬂect real user satisfaction. As human evaluation by asking crowd-sourcing workers to interact with a dialog system is much expensive (Ultes et al., 2013; Su et al., 2016) and pro"
2020.sigdial-1.37,P17-1113,0,0.0280956,"original papers or on the leaderboard. 299 NLU A natural language understanding module identiﬁes user intents and extracts associated information from users’ raw utterances. We consider 1 All state-of-the-art models mentioned in this paper are based on the open-source code that is available and executable as of February 29, 2020. two approaches that can handle multi-intents as reference: a RNN-based model MILU which extends (Hakkani-T¨ur et al., 2016) and is ﬁne-tuned on multiple domains, intents and slots; and a ﬁne-tuned BERT model (Devlin et al., 2019). Following the joint tagging scheme (Zheng et al., 2017), the labels of intent detection and slot ﬁlling are annotated for domain classiﬁcation during training. Both models use dialog history up to the last dialog turn as context. Note that there can be multiple intents or slots in one sentence, we calculate two F1 scores for intents and slots, respectively. DST A dialog state tracker encodes the extracted information as a compact set of dialog state that contains a set of informable slots and their corresponding values (user constraints), and a set of requested slots2 . We have implemented a rule-based DST to update the slot values in the dialog s"
2020.sigdial-1.37,D15-1199,0,0.0926213,"Missing"
2020.sigdial-1.37,P18-1135,0,0.0250954,"kanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018; Wu et al., 2019; Gao et al., 2019b), and a joint word-level policy model combines dialog policy and NLG (Chen et al., 2019; Zhao et al., 2019; Budzianowski and Vuli´c, 2019). It is particularly challenging to properly evaluate and compare the overall performance of goaloriented dialog systems due to the wide variety of system conﬁgurations and evaluation settings. Nu297 Proceedings of the SIGdial 2020 Conference, pages 297–310 c 1st virtual meeting, 01-03 July 2020. 2020 Association for Computational Linguistics merous approaches have been proposed to tackle different components in pipeline"
2020.sigdial-1.37,E17-1042,0,0.133925,"Missing"
2020.sigdial-1.37,2020.acl-demos.19,1,0.793267,"to reﬂect real user satisfaction. As human evaluation by asking crowd-sourcing workers to interact with a dialog system is much expensive (Ultes et al., 2013; Su et al., 2016) and prone to be affected by subjective factors (Higashinaka et al., 2010; Schmitt and Ultes, 2015), researchers have tried to realize automatic evaluation of dialog systems. Simulated evaluation (Araki and Doshita, 1996; Eckert et al., 1997) is widely used in recent works (Williams et al., 2017; Peng et al., 2017; Takanobu et al., 2019, 2020) and platforms (Ultes et al., 2017; Lee et al., 2019b; Papangelis et al., 2020; Zhu et al., 2020), where the system interacts with a user simulator which mimics human behaviors. Such evaluation can be conducted at the dialog act or natural language level. The advantages of using simulated evaluation are that it can support multi-turn language interaction in a full end-to-end fashion and generate dialogs unseen in the original corpus. 5 Conclusion and Discussion In this paper, we have presented the system-wise evaluation result and empirical analysis to estimate the practicality of goal-oriented dialog systems with a number of conﬁgurations and approaches. Though our experiments are only c"
2020.sigdial-1.37,P17-1062,0,0.12801,"s can be grouped into three classes based on their architectures, as illustrated in Fig. 1. Corresponding author DST State Introduction ∗ Semantic Info The ﬁrst class is the pipeline (or modular) systems which typically consist of the four components: Natural Language Understanding (NLU) (Goo et al., 2018; Pentyala et al., 2019), Dialog State Tracker (DST) (Xie et al., 2015; Lee and Stent, 2016), Dialog Policy (Peng et al., 2017; Takanobu et al., 2019), and Natural Language Generation (NLG) (Wen et al., 2015; Balakrishnan et al., 2019). The second class is the end-to-end (or unitary) systems (Williams et al., 2017; Dhingra et al., 2017; Liu et al., 2018; Lei et al., 2018; Qin et al., 2019; Mehri et al., 2019), which use a machine-learned neural model to generate a system response directly from a dialog history. The third one lies in between the above two types, where some systems use joint models that combine some (but not all) of the four dialog components. For example, a joint wordlevel DST model combines NLU and DST (Zhong et al., 2018; Wu et al., 2019; Gao et al., 2019b), and a joint word-level policy model combines dialog policy and NLG (Chen et al., 2019; Zhao et al., 2019; Budzianowski and Vuli´"
2020.sigdial-1.37,W01-0902,0,\N,Missing
2021.acl-long.240,P17-1171,0,0.224177,"d approach by combining answers from both readers can effectively take advantages of extractive and generative answer inference strategies and outperform single models as well as homogeneous ensembles. Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively. 1 Introduction Open-domain question answering (QA) has been a long standing problem in natural language understanding, information retrieval, and related fields (Chen and Yih, 2020). An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference. In this work, we study two paradigms of reader modules, i.e. extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al., 2020; Izacard and Grave, 2021) readers. The extractive reader extracts contiguous spans from the retrieved passages whereas the generative reader sequentially decodes the answer string which might not be contained in the retrieved passages. ∗ Equal Contribution"
2021.acl-long.240,2020.acl-tutorials.8,0,0.0234693,"provide large improvements over previous state-of-the-art models. We demonstrate that an hybrid approach by combining answers from both readers can effectively take advantages of extractive and generative answer inference strategies and outperform single models as well as homogeneous ensembles. Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively. 1 Introduction Open-domain question answering (QA) has been a long standing problem in natural language understanding, information retrieval, and related fields (Chen and Yih, 2020). An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference. In this work, we study two paradigms of reader modules, i.e. extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al., 2020; Izacard and Grave, 2021) readers. The extractive reader extracts contiguous spans from the retrieved passages whereas the generative reader sequentially decodes th"
2021.acl-long.240,2020.acl-main.501,1,0.893399,"p(se (j)), respectively. Since there are usually multiple plausible mentions for open-domain QA, during training, it is typical to maximize either the marginal log-likelihood (MML) of all correct spans (Karpukhin et al., 2020) or the log-likelihood of the most likely correct span (HardEM) (Min et al., 2019). During inference, the prediction is made based on the candidate P answer string score, obtaining as Pa (y) = (i,j)∈Y Ps (i, j), where Y is the set of spans corresponding to the answer string y. 2.1.2 Multi-objective for Weakly-supervised QA The multi-objective formulation is introduced in Cheng et al. (2020) for improving weakly supervised document-level QA. Different from Cheng et al. (2020) where only MML is considered for the multi-objective formulation, we found combining HardEM with MML is more effective for open-domain QA based on our experiments (§4.1). Specifically, we combine a multi-passage HardEM loss with K passage-level MML losses over a batch of K passages LEXT = log max PsM (i, j) + (i,j) X 1 X log PsP (ik , j k ), (1) K k k k where PsM , PsP is the multi-passage level and passage level span probabilities respectively. Posterior Differential Regularization Due to the noisy supervis"
2021.acl-long.240,2021.naacl-main.85,1,0.878413,"h to combine the predictions from extractive and generative readers. It achieves state-of-theart results on NaturalQuestions (Kwiatkowski et al., 2019) and TriviaQA (Joshi et al., 2017). In UnitedQA, the extractive reader (UnitedQAE) and generative reader (UnitedQA-G) are built upon the pretrained language models, ELECTRA (Clark et al., 2020) and T5 (Raffel et al., 2020), respectively. For the UnitedQA-E, we adopt a weakly-supervised training objective to address the noisy supervision issue caused by the heuristicsbased labeling and incorporate the posterior differential regularization (PDR) (Cheng et al., 2021) to improve the model robustness. The UnitedQA-G follows the T5 Fusion-in-Decoder (FID) (Izacard and Grave, 2021) and we make two improvements: first, we add a group of attention bias parameters into the decoder cross-attention block to feature the ranking information of retrieved contexts; second, we add the adversarial training (Ju et al., 2019; Jiang et al., 2020; Pereira et al., 2021) to improve the model generalization ability. The experimental results highlight the effec3080 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International"
2021.acl-long.240,2020.acl-main.197,1,0.741561,"Missing"
2021.acl-long.240,P18-1078,0,0.0190922,"ate of the passage index during the training. Instead, in this work, we focus on a hybrid reader approach for open-domain QA. By simply combing answer predictions from extractive and generative models, our UnitedQA achieves significant improvements over state-of-the-art models. Reading Comprehension with Noisy Labels There has been a line of work on improving distantly-supervised reading comprehension models by developing learning methods and model architectures that can better use noisy labels. Most of them focus on the document-level QA, where all paragraphs share the same document context. Clark and Gardner (2018) propose a paragraphpair ranking objective for learning with multiple paragraphs so that the model can distinguish relevant paragraphs from irrelevant ones. In (Lin et al., 2018), a coarse-to-fine model is proposed to handle label noise by aggregating information from relevant paragraphs and then extracting answers from selected ones. Min et al. (2019) propose a hard EM learning scheme where only passage-level loss is considered for document-level QA. More recently, different probabilistic assumptions with corresponding training and inference methods are examined in (Cheng et al., 2020) again"
2021.acl-long.240,P17-1147,0,0.0306167,"to Google Search, each with answers identified by human annotators in Wikipedia. The open-domain version of NaturalQuestions (Lee et al., 2019) only consider questions with short answers, i.e. answers with less than 5 tokens. In the NaturualQuestions, the questions are considered to be more information seeking given that the question askers didn’t know the answer beforehand. In addition, we use another evaluation set, i.e. the dev set introduced recently by the EfficientQA competition (Min et al., 2021), which is constructed in the same way as the original NaturalQuestions dataset. TriviaQA (Joshi et al., 2017) contains trivia question-answer pairs that were scraped from the web. Different from NaturalQuestions, the questions here are written with known answers in mind. Specifically, the unfiltered set has been used for developing open-domain QA models. Implementation details For a fair comparison, we use the same retrieval module as Karpukhin et al. (2020) for NaturalQuestions and TriviaQA to mitigate the impact of retrieval difference. Specifically, we use DPR (single) for NaturalQuestions and BM25+DPR (multi) for TriviaQA because of their best end-to-end performance (Karpukhin et al. 2020). For a"
2021.acl-long.240,N19-1423,0,0.190193,"ven question. Then, the module of hybrid readers produces answer candidates from the set of retrieved passages. Last, the re-ranking module combines the answer candidates with linear interpolation and produce the final answer. Retrieval Following Karpukhin et al. (2020), we consider two methods, BM25 and dense passage retrieval (DPR), for retrieving the support passages for a given question. For BM25, passages are encoded as bag of words (BOW), and inverse document frequencies are used as the ranking function. For DPR, passages and questions are represented as dense vectors based on two BERT (Devlin et al., 2019) models. The relevance score is then computed based on the dot production between the query and passage vectors. In this paper, we adopt the same implementation as Karpukhin et al. (2020) for retrieving passages. Specifically, the English Wikipedia dump from Dec. 20, 2018 is used as the source documents for retrieval, with the removal of semi-structured data, such as tables or lists. Each document is split into disjoint 100-word passages as the basic retrieval unit. The top-100 passages are then passed for reading. Reading We combine the generative reader and the extractive reader to produce a"
2021.acl-long.240,2020.emnlp-main.550,0,0.0772381,"both readers can effectively take advantages of extractive and generative answer inference strategies and outperform single models as well as homogeneous ensembles. Our approach outperforms previous state-of-the-art models by 3.3 and 2.7 points in exact match on NaturalQuestions and TriviaQA respectively. 1 Introduction Open-domain question answering (QA) has been a long standing problem in natural language understanding, information retrieval, and related fields (Chen and Yih, 2020). An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference. In this work, we study two paradigms of reader modules, i.e. extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al., 2020; Izacard and Grave, 2021) readers. The extractive reader extracts contiguous spans from the retrieved passages whereas the generative reader sequentially decodes the answer string which might not be contained in the retrieved passages. ∗ Equal Contribution Recent work on open-domain QA (Karpukhin e"
2021.acl-long.240,Q19-1026,0,0.0162195,"2: Comparison to state-of-the-art models on the test sets of NaturualQuestions (NQ) and TriviaQA. Exact match score is used for evaluation. The overall best model is in Box , the best single model is in bold, and the best model with the smallest reader size is in underline. work (Lee et al., 2019; Karpukhin et al., 2020). Both datasets (see Table 1 for statistics) have been heavily studied in recent work (Lee et al., 2019; Min et al., 2019; Karpukhin et al., 2020; Guu et al., 2020). We follow the standard evaluation protocol and use exact match (EM) as the evaluation metric. NaturalQuestions (Kwiatkowski et al., 2019) is composed of questions by real users to Google Search, each with answers identified by human annotators in Wikipedia. The open-domain version of NaturalQuestions (Lee et al., 2019) only consider questions with short answers, i.e. answers with less than 5 tokens. In the NaturualQuestions, the questions are considered to be more information seeking given that the question askers didn’t know the answer beforehand. In addition, we use another evaluation set, i.e. the dev set introduced recently by the EfficientQA competition (Min et al., 2021), which is constructed in the same way as the origin"
2021.acl-long.240,P19-1612,0,0.0877117,"48.2 51.4 57.9 65.0 67.6 UnitedQA-Ebase (Ours) UnitedQA-Elarge (Ours) UnitedQA-Glarge (Ours) Extractive Extractive Generative 110 330 770 47.7 51.8 52.3 66.3 68.9 68.6 UnitedQA-Elarge ++ (Ours) UnitedQA-Glarge ++ (Ours) UnitedQA (Ours) Ensemble Ensemble Hybrid 3x330 3x770 2x770+330 52.4 53.3 54.7 69.6 69.2 70.5 Table 2: Comparison to state-of-the-art models on the test sets of NaturualQuestions (NQ) and TriviaQA. Exact match score is used for evaluation. The overall best model is in Box , the best single model is in bold, and the best model with the smallest reader size is in underline. work (Lee et al., 2019; Karpukhin et al., 2020). Both datasets (see Table 1 for statistics) have been heavily studied in recent work (Lee et al., 2019; Min et al., 2019; Karpukhin et al., 2020; Guu et al., 2020). We follow the standard evaluation protocol and use exact match (EM) as the evaluation metric. NaturalQuestions (Kwiatkowski et al., 2019) is composed of questions by real users to Google Search, each with answers identified by human annotators in Wikipedia. The open-domain version of NaturalQuestions (Lee et al., 2019) only consider questions with short answers, i.e. answers with less than 5 tokens. In the"
2021.acl-long.240,2021.eacl-main.74,0,0.480985,"n-domain question answering (QA) has been a long standing problem in natural language understanding, information retrieval, and related fields (Chen and Yih, 2020). An typical open-domain QA system follows the retrieval-reader framework (Chen et al., 2017; Guu et al., 2020; Karpukhin et al., 2020), where the relevant passages are first retrieved from a large text corpus, and a reader module then navigates multiple passages for answer inference. In this work, we study two paradigms of reader modules, i.e. extractive (Karpukhin et al., 2020; Guu et al., 2020) and generative (Lewis et al., 2020; Izacard and Grave, 2021) readers. The extractive reader extracts contiguous spans from the retrieved passages whereas the generative reader sequentially decodes the answer string which might not be contained in the retrieved passages. ∗ Equal Contribution Recent work on open-domain QA (Karpukhin et al., 2020; Guu et al., 2020; Lewis et al., 2020; Izacard and Grave, 2021) explores either an extractive reader or a generative reader exclusively. We hypothesize that extractive and generative readers adopt different answer inference strategies, thus a hybrid extractive/generative reader can be a better option for open-dom"
2021.acl-long.240,2021.eacl-main.86,0,0.0381828,"Missing"
2021.acl-long.240,P18-1161,0,0.0188873,"nerative models, our UnitedQA achieves significant improvements over state-of-the-art models. Reading Comprehension with Noisy Labels There has been a line of work on improving distantly-supervised reading comprehension models by developing learning methods and model architectures that can better use noisy labels. Most of them focus on the document-level QA, where all paragraphs share the same document context. Clark and Gardner (2018) propose a paragraphpair ranking objective for learning with multiple paragraphs so that the model can distinguish relevant paragraphs from irrelevant ones. In (Lin et al., 2018), a coarse-to-fine model is proposed to handle label noise by aggregating information from relevant paragraphs and then extracting answers from selected ones. Min et al. (2019) propose a hard EM learning scheme where only passage-level loss is considered for document-level QA. More recently, different probabilistic assumptions with corresponding training and inference methods are examined in (Cheng et al., 2020) again for documentlevel QA with distant supervision. In our work, we further extend the multi-objective formulation proposed in (Cheng et al., 2020) with the hard EM learning (Min et a"
2021.acl-long.240,D19-1284,0,0.116279,"itions from the k-th passage and NULL indicates special positions if pk does not support answering the question. Similarly, the multi-passage level probability is computed by normalizing over each answer positions P across P all K relevant pas∗ sages, P i.e. P Zb = Zb = k I k exp(sb (i)), Ze = Ze∗ = k I k exp(se (j)), respectively. Since there are usually multiple plausible mentions for open-domain QA, during training, it is typical to maximize either the marginal log-likelihood (MML) of all correct spans (Karpukhin et al., 2020) or the log-likelihood of the most likely correct span (HardEM) (Min et al., 2019). During inference, the prediction is made based on the candidate P answer string score, obtaining as Pa (y) = (i,j)∈Y Ps (i, j), where Y is the set of spans corresponding to the answer string y. 2.1.2 Multi-objective for Weakly-supervised QA The multi-objective formulation is introduced in Cheng et al. (2020) for improving weakly supervised document-level QA. Different from Cheng et al. (2020) where only MML is considered for the multi-objective formulation, we found combining HardEM with MML is more effective for open-domain QA based on our experiments (§4.1). Specifically, we combine a mult"
2021.acl-long.240,2021.naacl-main.424,1,0.791885,"Missing"
2021.acl-long.240,2021.naacl-main.466,0,0.0262702,"ive model does not. This difference suggests that it is worth exploring better temporal modeling strategies to improve the extractive model in the future. 5 Related Work Open-domain QA Open-domain QA requires a system to answer questions based on evidence retrieved from a large corpus such as Wikipedia (Voorhees, 2000; Chen et al., 2017). Recent progress has been made towards improving evidence retrieval through both sparse vector models like TF-IDF or BM25 (Chen et al., 2017; Min et al., 2019), and dense vector models based on BERT (Lee et al., 2019; Karpukhin et al., 2020; Guu et al., 2020; Qu et al., 2021). Generally, the dense representations complement the sparse vector methods for passage retrieval as they can potentially give 3087 Generative Extractive tive open-domain QA, where the input passages are given by a retrieval model and are typically from different documents. Relative Accuracy 0.10 0.05 6 0.00 0.05 wh wh at i whch e whn ho o wh w ere wh wh at i whch e whn ho o wh w ere 0.10 Figure 2: Relative accuracy of different WH questions. The relative accuracy is the relative change of a WH category accuracy to the overall model accuracy. high similarity to semantically related text pairs,"
2021.acl-long.240,voorhees-tice-2000-trec,0,0.367013,"during training. In contrast, the answers to what questions can play a much richer syntactic role in context, making it more difficult for both extractive and generative models to perform well. Interestingly, the generative model exhibits the strength for temporal reasoning, whereas the extractive model does not. This difference suggests that it is worth exploring better temporal modeling strategies to improve the extractive model in the future. 5 Related Work Open-domain QA Open-domain QA requires a system to answer questions based on evidence retrieved from a large corpus such as Wikipedia (Voorhees, 2000; Chen et al., 2017). Recent progress has been made towards improving evidence retrieval through both sparse vector models like TF-IDF or BM25 (Chen et al., 2017; Min et al., 2019), and dense vector models based on BERT (Lee et al., 2019; Karpukhin et al., 2020; Guu et al., 2020; Qu et al., 2021). Generally, the dense representations complement the sparse vector methods for passage retrieval as they can potentially give 3087 Generative Extractive tive open-domain QA, where the input passages are given by a retrieval model and are typically from different documents. Relative Accuracy 0.10 0.05"
2021.acl-long.316,2020.emnlp-main.550,0,0.0418601,"Missing"
2021.acl-long.316,P17-1171,0,0.530685,"ves state-of-the-art performance on Natural Questions and TriviaQA datasets under the extractive QA setup when equipped with an extractive reader, and consistently outperforms other retrieval methods when the same generative reader is used.1 1 Introduction Open-domain question answering (OpenQA) aims to answer factoid questions without a pre-specified domain and has numerous real-world applications. In OpenQA, a large collection of documents (e.g., Wikipedia) are often used to seek information pertaining to the questions. One of the most common approaches uses a retriever-reader architecture (Chen et al., 2017), which first retrieves a small subset of documents using the question as the query and then reads the retrieved documents to extract (or generate) an answer. The retriever is crucial as it is infeasible to examine every piece of information in the entire document collection (e.g., millions of Wikipedia passages) and the retrieval accuracy bounds the performance of the (extractive) reader. ∗ Work was done during internship at Microsoft Azure AI. Our code is available at https://github.com/ morningmoni/GAR. 1 Early OpenQA systems (Chen et al., 2017) use classic retrieval methods such as TF-IDF"
2021.acl-long.316,Q19-1026,0,0.0800737,"2seq learning with the question as the input and various freely accessible in-domain contexts as the output such as the answer, the sentence where the answer belongs to, and the title of a passage that contains the answer. We then append the generated contexts to the question as the generationaugmented query for retrieval. We demonstrate that using multiple contexts from diverse generation targets is beneficial as fusing the retrieval results of different generation-augmented queries consistently yields better retrieval accuracy. We conduct extensive experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Trivia) (Joshi et al., 2017) datasets. The results reveal four major advantages of G AR: (1) G AR, combined with BM25, achieves significant gains over the same BM25 model that uses the original queries or existing unsupervised query expansion (QE) methods. (2) G AR with sparse representations (BM25) achieves comparable or even better performance than the current state-of-the-art retrieval methods, such as DPR (Karpukhin et al., 2020), that use dense representations. (3) Since G AR uses sparse representations to measure lexical overlap2 , it is complementary to dense representati"
2021.acl-long.316,N19-1423,0,0.184595,"cient (including the cost of the generation model), closing the gap between sparse and dense retrieval methods. reader design of the major baselines for a fair comparison, while virtually any existing QA reader can be used with G AR. 4.1 For the extractive setup, we largely follow the design of the extractive reader in DPR (Karpukhin et al., 2020). Let D = [d1 , d2 , ..., dk ] denote the list of retrieved passages with passage relevance scores D. Let Si = [s1 , s2 , ..., sN ] denote the top N text spans in passage di ranked by span relevance scores Si . Briefly, the DPR reader uses BERT-base (Devlin et al., 2019) for representation learning, where it estimates the passage relevance score Dk for each retrieved passage dk based on the [CLS] tokens of all retrieved passages D, and assigns span relevance scores Si for each candidate span based on the representations of its start and end tokens. Finally, the span with the highest span relevance score from the passage with the highest passage relevance score is chosen as the answer. We refer the readers to Karpukhin et al. (2020) for more details. Passage-level Span Voting. Many extractive QA methods (Chen et al., 2017; Min et al., 2019b; Guu et al., 2020;"
2021.acl-long.316,P17-1147,0,0.271232,"various freely accessible in-domain contexts as the output such as the answer, the sentence where the answer belongs to, and the title of a passage that contains the answer. We then append the generated contexts to the question as the generationaugmented query for retrieval. We demonstrate that using multiple contexts from diverse generation targets is beneficial as fusing the retrieval results of different generation-augmented queries consistently yields better retrieval accuracy. We conduct extensive experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and TriviaQA (Trivia) (Joshi et al., 2017) datasets. The results reveal four major advantages of G AR: (1) G AR, combined with BM25, achieves significant gains over the same BM25 model that uses the original queries or existing unsupervised query expansion (QE) methods. (2) G AR with sparse representations (BM25) achieves comparable or even better performance than the current state-of-the-art retrieval methods, such as DPR (Karpukhin et al., 2020), that use dense representations. (3) Since G AR uses sparse representations to measure lexical overlap2 , it is complementary to dense representations: by fusing the retrieval results of G A"
2021.acl-long.316,P19-1612,0,0.194727,"ieval for OpenQA, respectively. For query expansion, we re-emphasize that G AR is the first QE approach designed for OpenQA and most of the recent approaches are not applicable or efficient enough for OpenQA since they have task-specific objectives, require external supervision that was shown to transfer poorly to OpenQA, or take many days to train (Sec. 2). We thus compare with a classic unsupervised QE method RM3 (Abdul-Jaleel et al., 2004) that does not need external resources for a fair comparison. For passage reading, we compare with both extractive (Min et al., 2019a; Asai et al., 2019; Lee et al., 2019; Min et al., 2019b; Guu et al., 2020; Karpukhin et al., 2020) and generative (Brown et al., 2020; Roberts et al., 2020; Min et al., 2020; Lewis et al., 2020a; Izacard and Grave, 2020) methods when equipping G AR with the corresponding reader. 5.4 Implementation Details Retriever. We use Anserini (Yang et al., 2017) for text retrieval of BM25 and G AR with its default parameters. We conduct grid search for the QE baseline RM3 (Abdul-Jaleel et al., 2004). Generator. We use BART-large (Lewis et al., 2019) to generate query contexts in G AR. When there are multiple desired targets (such as multip"
2021.acl-long.316,2020.acl-main.703,0,0.0842252,"Missing"
2021.acl-long.316,W15-4640,0,0.0161974,"on both the ranking and score of the passages. As the generator and retriever are largely independent now, it is also interesting to study how to jointly or iteratively optimize generation and retrieval such that the generator is aware of the retriever and generates query contexts more beneficial for the retrieval stage. Last but not least, it is very likely that better results can be obtained by more extensive hyper-parameter tuning. Applicability to other tasks. Beyond OpenQA, G AR also has great potentials for other tasks that involve text matching such as conversation utterance selection (Lowe et al., 2015; Dinan et al., 2020) or information retrieval (Nguyen et al., 2016; Craswell et al., 2020). The default generation target is always available for supervised tasks. For example, for conversation utterance selection one can use the reference utterance as the default target and then match the concatenation of the conversation history and the generated utterance with the provided utterance candidates. For article search, the default target could be (part of) the ground-truth article itself. Other generation targets are more taskspecific and can be designed as long as they can be fetched from the"
2021.acl-long.316,D19-1284,0,0.509075,"uses BERT-base (Devlin et al., 2019) for representation learning, where it estimates the passage relevance score Dk for each retrieved passage dk based on the [CLS] tokens of all retrieved passages D, and assigns span relevance scores Si for each candidate span based on the representations of its start and end tokens. Finally, the span with the highest span relevance score from the passage with the highest passage relevance score is chosen as the answer. We refer the readers to Karpukhin et al. (2020) for more details. Passage-level Span Voting. Many extractive QA methods (Chen et al., 2017; Min et al., 2019b; Guu et al., 2020; Karpukhin et al., 2020) measure the probability of span extraction in different retrieved passages independently, despite that their collective signals may provide more evidence in determining the correct answer. We propose a simple yet effective passage-level span voting mechanism, which aggregates the predictions of the spans in the same surface form from different retrieved passages. Intuitively, if a text span is considered as the answer multiple times in different passages, it is more likely to be the correct answer. Specifically, G AR calculates a normalized score p("
2021.acl-long.316,2020.emnlp-main.466,0,0.715239,"ormation of the questions. G AR extends to contexts relevant to the questions by extracting information inside PLMs and helps sparse methods achieve comparable or better performance than dense methods (Guu et al., 2020; Karpukhin et al., 2020), while enjoying the simplicity and efficiency of sparse representations. G AR can also be used with dense representations to seek for even better performance, which we leave as future work. Generative QA. Generative QA generates answers through seq2seq learning instead of extracting answer spans. Recent studies on generative OpenQA (Lewis et al., 2020a; Min et al., 2020; Izacard and Grave, 2020) are orthogonal to G AR in that they focus on improving the reading stage and directly reuse DPR (Karpukhin et al., 2020) as the retriever. Unlike generative QA, the goal of G AR is not to generate perfect answers to the questions but pertinent contexts that are helpful for retrieval. Another line in generative QA learns to generate answers without relevant passages as the evidence but solely the question itself using PLMs (Roberts et al., 2020; Brown et al., 2020). G AR further confirms that one can extract factual knowledge from PLMs, which is not limited to the ans"
2021.acl-long.316,D17-1061,0,0.408758,"There have been some recent studies on query reformulation with text generation for other retrieval tasks, which, for example, rewrite the queries to context-independent (Yu et al., 2020; Lin et al., 2020; Vakulenko et al., 2020) or well-formed (Liu et al., 2019) ones. However, these methods require either task-specific data (e.g., conversational contexts, ill-formed queries) or external resources such as paraphrase data (Zaiem and Sadat, 2019; Wang et al., 2020) that cannot or do not transfer well to OpenQA. Also, some rely on timeconsuming training process like reinforcement learning (RL) (Nogueira and Cho, 2017; Liu et al., 2019; Wang et al., 2020) that is not efficient enough for OpenQA (more discussions in Sec. 2). In this paper, we propose GenerationAugmented Retrieval (G AR), which augments a query through text generation of a pre-trained language model (PLM). Different from prior studies that reformulate queries, G AR does not require external resources or downstream feedback via RL as supervision, because it does not rewrite the query but expands it with heuristically discov4089 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International J"
2021.acl-long.316,2020.emnlp-main.437,0,0.0256691,"Missing"
2021.acl-long.341,D18-1547,0,0.231421,"to two categories: (i) Corpora with structured semantic labels (Wen et al., 2017; Shah et al., 2018). These datasets are often specifically annotated, and used to study an individual module in the dialog pipeline. For example, DialoGLUE (Mehri et al., 2020) is a recently proposed benchmark with a focus on NLU and DST tasks. (ii) Corpora with an implicit user goal (Lowe et al., 2015). These datasets are often without semantic labels but can be used in end-to-end (E2E) dialog modeling (Li et al., 2016; Zhu, 2020; Wu et al., 2019; Zhu et al., 2019a; Lee et al., 2019; Zhu et al., 2020). MultiWOZ (Budzianowski et al., 2018) is the most related work to R ADDLE. It is a large-scale multi-turn conversational corpus across several domains. It can be used to develop individual dialog modules as separate tasks for existing modularbased methods, or serves as a benchmark for E2E dialog modeling methods. R ADDLE inherits the advantages of MultiWOZ in its flexibility for separate/joint task modeling and its comprehensiveness in multi-domain data coverage, but differs significantly in two aspects: an emphasis on limited data settings and an unique robustness checklist. Both are essential qualities in building task bots at"
2021.acl-long.341,2020.acl-main.11,0,0.0105579,"Nie et al., 2019). From this perspective, R ADDLE provides an unique benchmark for assessing PLMs with a robustness orientation. 3 Tasks R ADDLE is centered on five English dialog scenarios in daily life, which cover a broad range of data collection schemes, task types and complexities. As our first goal of R ADDLE is to spur development of generalizable dialog systems, we design the benchmark such that a good performance requires a model to leverage substantial knowledge (e.g., pretrained parameters) learned from its previous life cycle, while still maintaining some task-specific components (Coope et al., 2020; Henderson et al., 2020; Peng et al., 2020a; Wu et al., 2020b). Specifically, we deliberately keep a small number of training examples for each scenario. This is consistent with the common practice that only limited labelled data is provided when deploying a dialog system to new domains. Table 1 shows the data statistics. Four domains in the standard-setting are sampled from MultiWOZ 2.0 (Budzianowski et al., 2018). Reminder is intentionally only utilized for unseen entity tracking. Because it is a humanmachine corpus with a relatively smaller action space meaning that the impact of policy le"
2021.acl-long.341,N19-1423,0,0.0265098,"ne platform for model evaluation and fair comparison based on privately-held test data, inspired by GLUE (Wang et al., 2018). To the best of our knowledge, R ADDLE is the first online platform for DST and E2E tasks in the dialog community. This can reduce the inconsistency caused by different researchers/teams using varying processing/evaluation scripts to dilute where the gain comes from. 2.2 Evaluation of Pre-Trained Models Pre-trained language models (PLMs) have substantially advanced the state of the art across a variety of language understanding and generation tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; 4419 Standard Language Variations / Speech Errors Unseen OOD Attraction Train Hotel Restaurant Reminder Attraction 50 400 50 800 Dialog State Tracking / End-to-End Modeling DST / IC DST / OOD Joint Goal Accuracy / Combined Score JGA / Acc. JGA / F1 Domain Attraction Train Hotel #Train #Test Task Metrics 50 100 50 200 50 200 Restaurant 50 200 100 200 200 200 Table 1: Dataset descriptions and statistics. DST is short for Dialog State Tracking, E2E denotes End-to-End modeling, and IC stands for Intent Classification. Joint Goal Accuracy"
2021.acl-long.341,2020.acl-main.54,0,0.190859,"training examples is limited to 50, an accepted scale that users can provide. Though it is possible to train a single model for each task from scratch without outside sources of knowledge, we expect that our focus on data-scarce settings will render this approach uncompetitive. Furthermore, a typical task-oriented dialog system uses a modularized pipeline that has four modules and executes sequentially. Recent research has shown promising results on parameterizing the modularized pipeline using a single neural autoregressive model, and training it in an end-to-end manner (Peng et al., 2020a; Ham et al., 2020; Hosseini-Asl et al., 2020). In fact, a single autoregressive model can significantly ease the workflow of training and deploying dialog systems for new tasks, compared to existing modularized tools and methods. Therefore, we design the benchmark to allow evaluations on end-to-end dialog modeling, in addition to the modularized evaluation on dialog state tracking. To reveal the gap between the complexity of dialogs in lab environments and that in real scenarios, we construct a suite of tasks to study the robustness of models. We describe these tasks below and in Table 1. 4420 On the evaluatio"
2021.acl-long.341,H90-1021,0,0.801448,"Missing"
2021.acl-long.341,2020.findings-emnlp.196,0,0.0241559,"rom this perspective, R ADDLE provides an unique benchmark for assessing PLMs with a robustness orientation. 3 Tasks R ADDLE is centered on five English dialog scenarios in daily life, which cover a broad range of data collection schemes, task types and complexities. As our first goal of R ADDLE is to spur development of generalizable dialog systems, we design the benchmark such that a good performance requires a model to leverage substantial knowledge (e.g., pretrained parameters) learned from its previous life cycle, while still maintaining some task-specific components (Coope et al., 2020; Henderson et al., 2020; Peng et al., 2020a; Wu et al., 2020b). Specifically, we deliberately keep a small number of training examples for each scenario. This is consistent with the common practice that only limited labelled data is provided when deploying a dialog system to new domains. Table 1 shows the data statistics. Four domains in the standard-setting are sampled from MultiWOZ 2.0 (Budzianowski et al., 2018). Reminder is intentionally only utilized for unseen entity tracking. Because it is a humanmachine corpus with a relatively smaller action space meaning that the impact of policy learning on models is larg"
2021.acl-long.341,W14-4337,0,0.105333,"Missing"
2021.acl-long.341,D19-1131,0,0.0434263,"Missing"
2021.acl-long.341,2020.emnlp-main.378,1,0.930663,"Attraction Train Hotel Restaurant Reminder Attraction 50 400 50 800 Dialog State Tracking / End-to-End Modeling DST / IC DST / OOD Joint Goal Accuracy / Combined Score JGA / Acc. JGA / F1 Domain Attraction Train Hotel #Train #Test Task Metrics 50 100 50 200 50 200 Restaurant 50 200 100 200 200 200 Table 1: Dataset descriptions and statistics. DST is short for Dialog State Tracking, E2E denotes End-to-End modeling, and IC stands for Intent Classification. Joint Goal Accuracy (JGA) is used for DST and Combined score is used for E2E. Keskar et al., 2019; Dong et al., 2019; Peng et al., 2020b,c; Li et al., 2020a). PLMs are often trained to predict words based on their context on massive text data, and the learned models can be fine-tuned to quickly adapt to various downstream tasks, exhibiting strong generalization capacity even with just a few in-domain training examples. Building task bots at scale requires the model to deal with the limited data problem for each domain, which can be used as a testbed to evaluate the generalization ability of PLMs. To this end, we limit the number of task-specific training examples in R ADDLE to evaluate the sample-efficiency of models. Meanwhile, task-oriented di"
2021.acl-long.341,N16-1014,1,0.785156,"sing data-driven approaches, a number of conversational corpora have been released. They are roughly grouped into two categories: (i) Corpora with structured semantic labels (Wen et al., 2017; Shah et al., 2018). These datasets are often specifically annotated, and used to study an individual module in the dialog pipeline. For example, DialoGLUE (Mehri et al., 2020) is a recently proposed benchmark with a focus on NLU and DST tasks. (ii) Corpora with an implicit user goal (Lowe et al., 2015). These datasets are often without semantic labels but can be used in end-to-end (E2E) dialog modeling (Li et al., 2016; Zhu, 2020; Wu et al., 2019; Zhu et al., 2019a; Lee et al., 2019; Zhu et al., 2020). MultiWOZ (Budzianowski et al., 2018) is the most related work to R ADDLE. It is a large-scale multi-turn conversational corpus across several domains. It can be used to develop individual dialog modules as separate tasks for existing modularbased methods, or serves as a benchmark for E2E dialog modeling methods. R ADDLE inherits the advantages of MultiWOZ in its flexibility for separate/joint task modeling and its comprehensiveness in multi-domain data coverage, but differs significantly in two aspects: an em"
2021.acl-long.341,2021.ccl-1.108,0,0.0705963,"Missing"
2021.acl-long.341,W15-4640,0,0.030952,"vided together with the benchmark. 2 2.1 Related Work Dialog Benchmarks To drive the progress of building dialog systems using data-driven approaches, a number of conversational corpora have been released. They are roughly grouped into two categories: (i) Corpora with structured semantic labels (Wen et al., 2017; Shah et al., 2018). These datasets are often specifically annotated, and used to study an individual module in the dialog pipeline. For example, DialoGLUE (Mehri et al., 2020) is a recently proposed benchmark with a focus on NLU and DST tasks. (ii) Corpora with an implicit user goal (Lowe et al., 2015). These datasets are often without semantic labels but can be used in end-to-end (E2E) dialog modeling (Li et al., 2016; Zhu, 2020; Wu et al., 2019; Zhu et al., 2019a; Lee et al., 2019; Zhu et al., 2020). MultiWOZ (Budzianowski et al., 2018) is the most related work to R ADDLE. It is a large-scale multi-turn conversational corpus across several domains. It can be used to develop individual dialog modules as separate tasks for existing modularbased methods, or serves as a benchmark for E2E dialog modeling methods. R ADDLE inherits the advantages of MultiWOZ in its flexibility for separate/joint"
2021.acl-long.341,P17-1163,0,0.0596231,"Missing"
2021.acl-long.341,2020.findings-emnlp.17,1,0.536769,"opriate responses leading to frustrating user experience. These scenarios are common for deployed systems in the real world, but are largely ignored in existing dialog benchmarks. To the best of our knowledge, R ADDLE presents the first work to fill this gap. To better understand the challenges posed by R ADDLE, we conduct experiments with simple baselines and state-of-the-art task-oriented dialog models. We find that grounded pre-trained models with a unified multi-task learning objective outperform models separately trained on each domain. Moreover, even the best performing model (S OLOIST (Peng et al., 2020a)) in our evaluation achieves a fairly low score in robustness analysis. This suggests that our baseline models can handle common inputs with strong regularities, but struggle with anomalous inputs that require deeper reasoning. In summary, our key contributions are: (i) A novel dialog benchmark with an emphasis on limited data and multiple domains/tasks, which formally creates a scenario to evaluate the grounding and generalization ability of pre-trained models. (ii) A crowd-sourced diagnostic evaluation dataset to cover a broad range of real-world sophistication to study model robustness. ("
2021.acl-long.341,N18-1202,0,0.0161655,"DDLE provides an online platform for model evaluation and fair comparison based on privately-held test data, inspired by GLUE (Wang et al., 2018). To the best of our knowledge, R ADDLE is the first online platform for DST and E2E tasks in the dialog community. This can reduce the inconsistency caused by different researchers/teams using varying processing/evaluation scripts to dilute where the gain comes from. 2.2 Evaluation of Pre-Trained Models Pre-trained language models (PLMs) have substantially advanced the state of the art across a variety of language understanding and generation tasks (Peters et al., 2018; Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019; 4419 Standard Language Variations / Speech Errors Unseen OOD Attraction Train Hotel Restaurant Reminder Attraction 50 400 50 800 Dialog State Tracking / End-to-End Modeling DST / IC DST / OOD Joint Goal Accuracy / Combined Score JGA / Acc. JGA / F1 Domain Attraction Train Hotel #Train #Test Task Metrics 50 100 50 200 50 200 Restaurant 50 200 100 200 200 200 Table 1: Dataset descriptions and statistics. DST is short for Dialog State Tracking, E2E denotes End-to-End modeling, and IC stands for Intent Classification"
2021.acl-long.341,2020.emnlp-main.66,0,0.0610675,"strong generalization capacity even with just a few in-domain training examples. Building task bots at scale requires the model to deal with the limited data problem for each domain, which can be used as a testbed to evaluate the generalization ability of PLMs. To this end, we limit the number of task-specific training examples in R ADDLE to evaluate the sample-efficiency of models. Meanwhile, task-oriented dialogs pose a unique set of challenges for PLMs (Gao et al., 2020): a dialog is intrinsically goal-driven, multi-turn and often informal/noisy. Indeed, dialog-specific PLMs are proposed (Wu et al., 2020a; Peng et al., 2020a). However, the robustness of PLMs to linguistic perturbations often occurring in dialog settings (See Section 4 for details) is largely unexplored. Note that our notion of robustness emphasizes natural language variations, which is different from adversarial examples/training that aim to fool a trained model (Nie et al., 2019). From this perspective, R ADDLE provides an unique benchmark for assessing PLMs with a robustness orientation. 3 Tasks R ADDLE is centered on five English dialog scenarios in daily life, which cover a broad range of data collection schemes, task typ"
2021.acl-long.341,P18-1134,0,0.0157197,"thanks. System : thank you , goodbye . User : no, that&apos;s it. thanks. System : thank you , goodbye . System : thank you , goodbye . User : no, that&apos;s it. thanks. (g) Unseen entities (h) Out-of-domain utterance Figure 1: Illustration of different language perturbations in the robustness diagnostic checklist. The standard dialog , (f) shows speech error , example is shown in (a). Based on it, (b)-(e) are four types of language variations (e) shows unseen entities , and (h) shows out-of-domain utterance . In each case, some representative examples are highlighted in red text. and highly dynamic (Xu and Hu, 2018). Therefore, unseen entities are common in dialogs, i.e., entities that are not observed during training, but appear in the testing stage. In Figure 1(g), the entity Bellevue downtown is in the knowledge base but never appears in model training, a robust DST should be able to recognize it as a city/place, via generalizing from other similar entities learned during training. Out-of-Domain Utterances Most deployed task-oriented dialog systems are built for a closed set of target domains. Thus, they are fragile when dealing with out-of-domain (OOD) utterances (Lee and Shalyminov, 2019). Failure t"
2021.acl-long.341,2020.cl-1.2,1,0.873063,"Missing"
2021.acl-long.341,2020.sigdial-1.33,1,0.836879,"approaches, a number of conversational corpora have been released. They are roughly grouped into two categories: (i) Corpora with structured semantic labels (Wen et al., 2017; Shah et al., 2018). These datasets are often specifically annotated, and used to study an individual module in the dialog pipeline. For example, DialoGLUE (Mehri et al., 2020) is a recently proposed benchmark with a focus on NLU and DST tasks. (ii) Corpora with an implicit user goal (Lowe et al., 2015). These datasets are often without semantic labels but can be used in end-to-end (E2E) dialog modeling (Li et al., 2016; Zhu, 2020; Wu et al., 2019; Zhu et al., 2019a; Lee et al., 2019; Zhu et al., 2020). MultiWOZ (Budzianowski et al., 2018) is the most related work to R ADDLE. It is a large-scale multi-turn conversational corpus across several domains. It can be used to develop individual dialog modules as separate tasks for existing modularbased methods, or serves as a benchmark for E2E dialog modeling methods. R ADDLE inherits the advantages of MultiWOZ in its flexibility for separate/joint task modeling and its comprehensiveness in multi-domain data coverage, but differs significantly in two aspects: an emphasis on l"
2021.acl-long.341,N15-1020,1,0.871466,"Missing"
2021.acl-long.341,D19-1123,1,0.92634,"ersational corpora have been released. They are roughly grouped into two categories: (i) Corpora with structured semantic labels (Wen et al., 2017; Shah et al., 2018). These datasets are often specifically annotated, and used to study an individual module in the dialog pipeline. For example, DialoGLUE (Mehri et al., 2020) is a recently proposed benchmark with a focus on NLU and DST tasks. (ii) Corpora with an implicit user goal (Lowe et al., 2015). These datasets are often without semantic labels but can be used in end-to-end (E2E) dialog modeling (Li et al., 2016; Zhu, 2020; Wu et al., 2019; Zhu et al., 2019a; Lee et al., 2019; Zhu et al., 2020). MultiWOZ (Budzianowski et al., 2018) is the most related work to R ADDLE. It is a large-scale multi-turn conversational corpus across several domains. It can be used to develop individual dialog modules as separate tasks for existing modularbased methods, or serves as a benchmark for E2E dialog modeling methods. R ADDLE inherits the advantages of MultiWOZ in its flexibility for separate/joint task modeling and its comprehensiveness in multi-domain data coverage, but differs significantly in two aspects: an emphasis on limited data settings and an unique"
2021.acl-long.341,W18-5446,0,0.0602182,"Missing"
2021.acl-long.341,W19-5905,1,0.930737,"ersational corpora have been released. They are roughly grouped into two categories: (i) Corpora with structured semantic labels (Wen et al., 2017; Shah et al., 2018). These datasets are often specifically annotated, and used to study an individual module in the dialog pipeline. For example, DialoGLUE (Mehri et al., 2020) is a recently proposed benchmark with a focus on NLU and DST tasks. (ii) Corpora with an implicit user goal (Lowe et al., 2015). These datasets are often without semantic labels but can be used in end-to-end (E2E) dialog modeling (Li et al., 2016; Zhu, 2020; Wu et al., 2019; Zhu et al., 2019a; Lee et al., 2019; Zhu et al., 2020). MultiWOZ (Budzianowski et al., 2018) is the most related work to R ADDLE. It is a large-scale multi-turn conversational corpus across several domains. It can be used to develop individual dialog modules as separate tasks for existing modularbased methods, or serves as a benchmark for E2E dialog modeling methods. R ADDLE inherits the advantages of MultiWOZ in its flexibility for separate/joint task modeling and its comprehensiveness in multi-domain data coverage, but differs significantly in two aspects: an emphasis on limited data settings and an unique"
2021.acl-long.341,E17-1042,0,0.0685182,"Missing"
2021.acl-long.341,2020.acl-demos.19,1,0.840829,". They are roughly grouped into two categories: (i) Corpora with structured semantic labels (Wen et al., 2017; Shah et al., 2018). These datasets are often specifically annotated, and used to study an individual module in the dialog pipeline. For example, DialoGLUE (Mehri et al., 2020) is a recently proposed benchmark with a focus on NLU and DST tasks. (ii) Corpora with an implicit user goal (Lowe et al., 2015). These datasets are often without semantic labels but can be used in end-to-end (E2E) dialog modeling (Li et al., 2016; Zhu, 2020; Wu et al., 2019; Zhu et al., 2019a; Lee et al., 2019; Zhu et al., 2020). MultiWOZ (Budzianowski et al., 2018) is the most related work to R ADDLE. It is a large-scale multi-turn conversational corpus across several domains. It can be used to develop individual dialog modules as separate tasks for existing modularbased methods, or serves as a benchmark for E2E dialog modeling methods. R ADDLE inherits the advantages of MultiWOZ in its flexibility for separate/joint task modeling and its comprehensiveness in multi-domain data coverage, but differs significantly in two aspects: an emphasis on limited data settings and an unique robustness checklist. Both are essenti"
2021.acl-long.537,P18-1063,1,0.834288,"; then the PageRank algorithm is applied to obtain the rank 5 We also tested some other heuristics: e.g., the first sentence of the last email, the last 3-5 sentences of the email thread, etc. However, none of them perform better than Lead1-Email. scores for each sentence, and top-rank sentences are selected as the summary. BertSumExt. Liu and Lapata (2019b) propose to build a sentence extractor upon BERT (Devlin et al., 2019) to perform extractive summarization, which achieves a good performance on CNN/DM. 3.2 Abstractive Fast Abs RL. As the simple non-pretrained abstractive baseline, we use Chen and Bansal (2018), which is a hybrid model that first extracts sentences from the source document, then rewrites the extracted sentences by an abstractive rewriter. They pair summary sentences with the extracted sentences to train the abstractive rewriter. Adapting their model to our email thread summarization task, we make two adjustments: (1) We extract emails instead of sentences, which is a natural unit for email thread; (2) Since summary sentences usually follow the temporal order of the emails, we enhance this pairing procedure by using the Neeleman-Wunsch algorithm (Needleman and Wunsch, 1970; Rameshkum"
2021.acl-long.537,2021.ccl-1.108,0,0.0342117,"Missing"
2021.acl-long.537,2020.acl-main.173,0,0.0179186,"ely. for the model is to identify the roles of different speakers and their relations, i.e., who does what to whom. As shown in the second example of Table 6, the model wrongly takes “2 fixes in 382 are in the patch installer” as information provided by Nilesh, whereas it is supposed to be by Diana. The same issue can also be observed in the first example: Om is just summarizing what Nihar said instead of telling Nihar. This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models (Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020). 6902 5.3 Correlation with Human Judgement ROUGE (Lin, 2004) measures n-gram overlap and BERTScore (Zhang et al., 2019) is essentially based on “soft” uni-gram matching. However, according to our analysis presented above, the email thread summarization models mainly fail to be abstractive, salient, and faithful, which are hard to be evaluated by n-gram overlap. Furthermore, as pointed out by Bhandari et al. (2020), different datasets usually require different evaluation metrics. Therefore, here, we study the correlation between automatic metrics and human judgments. Specifically, we evaluate"
2021.acl-long.537,2020.acl-main.450,0,0.0235029,"ort and E MAIL S UMlong tasks, respectively. for the model is to identify the roles of different speakers and their relations, i.e., who does what to whom. As shown in the second example of Table 6, the model wrongly takes “2 fixes in 382 are in the patch installer” as information provided by Nilesh, whereas it is supposed to be by Diana. The same issue can also be observed in the first example: Om is just summarizing what Nihar said instead of telling Nihar. This is considered as a type of unfaithfulness, which has been widely identified as a common issue of abstractive summarization models (Wang et al., 2020; Durmus et al., 2020; Maynez et al., 2020). 6902 5.3 Correlation with Human Judgement ROUGE (Lin, 2004) measures n-gram overlap and BERTScore (Zhang et al., 2019) is essentially based on “soft” uni-gram matching. However, according to our analysis presented above, the email thread summarization models mainly fail to be abstractive, salient, and faithful, which are hard to be evaluated by n-gram overlap. Furthermore, as pointed out by Bhandari et al. (2020), different datasets usually require different evaluation metrics. Therefore, here, we study the correlation between automatic metrics and"
2021.acl-long.537,W04-3252,0,0.0233489,"old summary. “Ext-Oracle-R1” in Table 2 is computed from an oracle summary that maximizes ROUGE-1 (Lin, 2004). Lead. This model simply picks the first sentence from the source document as the summary, which has surprisingly good performance on CNN/DM dataset (Narayan et al., 2018). We test two variants by selecting: (1) the first sentence of the email thread, which is usually the subject (see the example in Table 1), referred as Lead-1; (2) the first sentence of the email thread (the subject) plus the first sentences of every email, named Lead-1-Email.5 TextRank. This is a graph-based method (Mihalcea and Tarau, 2004). It first builds a graph between sentences by their embedding similarities; then the PageRank algorithm is applied to obtain the rank 5 We also tested some other heuristics: e.g., the first sentence of the last email, the last 3-5 sentences of the email thread, etc. However, none of them perform better than Lead1-Email. scores for each sentence, and top-rank sentences are selected as the summary. BertSumExt. Liu and Lapata (2019b) propose to build a sentence extractor upon BERT (Devlin et al., 2019) to perform extractive summarization, which achieves a good performance on CNN/DM. 3.2 Abstract"
2021.acl-long.537,2020.acl-main.459,0,0.294098,"rayan et al., 2018). However, living in an information era, we are facing with diverse content 1 Our code and summary data have been made available at: https://github.com/ZhangShiyue/EmailSum in different structures. The summarization need is varied along with different application scenarios. Recently, there is an increasing research interest in diverse summarization tasks (Gao et al., 2020), e.g., timeline (Allan et al., 2001), query-based (Li and Li, 2014), multi-modal (Zhu et al., 2018), meeting (Carletta et al., 2006), dialogue or discussion thread (Misra et al., 2015; Gliwa et al., 2019; Rameshkumar and Bailey, 2020), etc. Following the branch of dialogue or thread summarization, we introduce a new abstractive Email Thread Summarization (E MAIL S UM) dataset. Email threads are widely used at work. An email thread is a special type of dialogue that usually has a specific structure (sender, receiver, greeting line, main body, and the signature), contains technical information, and involves multiple speakers. Unlike a conversational dialog turn, an email in a 6895 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Lan"
2021.acl-long.537,2020.findings-emnlp.19,0,0.0342911,"T5) to generate summaries for unlabelled threads, then mix the model-labeled and human-labeled data to finetune T5 again, referred as SemiSupx (x stands for the unlabelled data source we use, i.e., W3C, Avocado, or together). Hierarchical T5. Hierarchical summarization models have been shown to improve the performance of multi-document summarization task (Liu and Lapata, 2019a). Although an email thread can be treated as a single document due to the temporal dependency between consecutive emails, it also has a clear turn structure that encourages using of the hierarchical encoders. Recently, Zhu et al. (2020) proposed a hierarchical model (HMNet) for meeting summarization. Inspired by their work, we propose a hierarchical model that is similar to HMNet in structure but uses T5 as the backbone, therefore, it can take advantage of both the hierarchical structure and the pre-trained knowledge. As shown in Figure 1, this model contains two encoders: the token-level encodes the whole email Experiments Evaluation Metrics ROUGE (Lin, 2004) is a commonly used automatic metric for summarization tasks. It has several variants: (1) ROUGE-1 (R1) measures the unigram overlap between the generated and reference"
2021.eacl-main.2,N18-1150,1,0.915286,"Missing"
2021.eacl-main.2,P17-1123,0,0.0469643,"Missing"
2021.eacl-main.2,W09-0611,0,0.0245224,"Missing"
2021.eacl-main.2,D17-1090,0,0.0624025,"Missing"
2021.eacl-main.2,C18-1150,0,0.0181187,"learning sentence representations (Clark et al., 2020). To our best knowledge, we are the first to leverage contrastive learning and establish set-induced penalization in the context of question generation. Question Generation: Most prior work on question generation has been on single document i.e. given a document and an answer phrase in the document, generate a question that is answered by the answer phrase (Heilman, 2011; Rus et al., 2010). For a survey, see Pan et al. (2019). However, in our work, we aim to generate a multi-document question that is answerable by multiple input documents. Fan et al. (2018) propose a visual question generation model to generate natural questions about images using reinforcement learning where they use naturalness and human-like as reward signals. In our work, we use retrieval statistics, similar to Nogueira and Cho (2017), derived from a document-question ranker as the reward for training our coordinator model in isolation, rather than the entire generating pipeline. 5 Conclusion We proposed a novel coordinator model that can generate questions that are more grounded on documents of interest. This coordinator model consists of transformer blocks, and is trained"
2021.eacl-main.2,D16-1026,0,0.0531351,"Missing"
2021.eacl-main.2,D13-1176,0,0.090163,"Missing"
2021.eacl-main.2,W18-6326,0,0.055818,"Missing"
2021.eacl-main.2,D15-1166,0,0.0545656,"Missing"
2021.eacl-main.2,D17-1259,0,0.023444,"p. This task is particularly challenging because i) there does not exist direct supervised ground-truth multi-document question given positive and negative sets of documents. ii) The whole procedure involves multiple aspects including language understanding, inter-document information aggregation, coordinative planning and language generation. In theory, the generator can be trained to maximize the chance that the generated question specifically retrieves the given document cluster, using RL. However, the space of possible sequence is prohibitively large which results in large variance in RL (Lewis et al., 2017). To effectively reduce the search space of RL, we employ a hybrid supervised learning (SL) and RL strategy. We also propose a novel reward-shaping auxiliary objecDocument-specific Generator: At the first pre-training stage, we load the publicly available GPT-2 (Radford et al., 2019) model as our underlying document-specific generator. The GPT-2 model leverages massive out-domain data and serves as a good initialization to generate grammatical and 13 Training an Inter-generator Coordinator via RL: Next, we train a coordinator system using policy gradient to optimize the reward described above."
2021.eacl-main.2,D18-1478,0,0.042988,"Missing"
2021.eacl-main.2,I17-1047,1,0.884813,"Missing"
2021.eacl-main.2,P17-2031,0,0.0694702,"Missing"
2021.eacl-main.2,D14-1162,0,0.0839586,"Missing"
2021.eacl-main.2,N18-1202,0,0.0455274,"Missing"
2021.eacl-main.2,2020.findings-emnlp.3,0,0.0200936,", model-generated questions from this method are rather generic and not specific to the input documents. Right: contrastive modeling, which considers both positive and negative document sets, and learns to generate questions that are more grounded on the positive document set. guity by suggesting clarification options back to the user in the form of questions (Braslavski et al., 2017; Aliannejadi et al., 2019; Zamani et al., 2020). However, asking the right clarification questions is a challenging information-seeking task, given a plethora of possible questions (Rao and Daumé III, 2018, 2019; Qi et al., 2020). One workaround is to take informational cues from the search engine results given the initial query. The clarification options are then generated from non-ranked and non-overlapping thematic partitions of the search engine results. The whole pipeline is akin to the pseudo-relevance feedback (Rocchio, 1971; Cao et al., 2008). This can significantly reduce the search space, and has the potential to generate correct clarification questions within the context (Cho Introduction User queries on web search engines can sometimes be vague. Search engines may resolve this ambi† Work done when the auth"
2021.eacl-main.2,W00-1009,0,0.444303,"Missing"
2021.eacl-main.2,P19-1220,0,0.0396745,"Missing"
2021.eacl-main.2,radev-etal-2002-evaluating,0,0.178572,"larity; VE for Vector Extrema similarity; and GM for Greedy Matching. consider the top-10 documents retrieved for q 0 as our negative set D− . In total, we gather 100K train/10K dev/10K eval data points. Details of the pre-processing, usage of additional annotations from the secondary dataset, and experimental configuration are in Appendix. negative set representational information. T 1 Xh X t t LH (θ) = wi,θ log wi,θ T t=1 i∈D+ i X t t log vi,θ + vi,θ (15) i∈D− Automatic evaluation: The generated questions are evaluated through standard retrieval-based metrics: MRR and MRR10 (Voorhees, 1999; Radev et al., 2002), nDCG (Järvelin and Kekäläinen, 2002), precision, mAP. These metrics are computed from the 10 positive and 10 negative document sets (=: Out-Sample IR). In addition, as a standardized evaluation routine in the MS-MARCO Retrieval task, for each generated question, we use Lucene‡ to retrieve the most relevant 100 MARCO documents via BM25 (Robertson and Zaragoza, 2009), and use the retrieved document set and a trained model to rank (document, generated question) pairs, thus compute the retrieval statistics (=: Search-Engine Augmented IR). The generated questions are also evaluated in We finally"
2021.eacl-main.2,W18-2711,0,0.41782,"Missing"
2021.eacl-main.2,D17-1061,0,0.0265605,"question generation has been on single document i.e. given a document and an answer phrase in the document, generate a question that is answered by the answer phrase (Heilman, 2011; Rus et al., 2010). For a survey, see Pan et al. (2019). However, in our work, we aim to generate a multi-document question that is answerable by multiple input documents. Fan et al. (2018) propose a visual question generation model to generate natural questions about images using reinforcement learning where they use naturalness and human-like as reward signals. In our work, we use retrieval statistics, similar to Nogueira and Cho (2017), derived from a document-question ranker as the reward for training our coordinator model in isolation, rather than the entire generating pipeline. 5 Conclusion We proposed a novel coordinator model that can generate questions that are more grounded on documents of interest. This coordinator model consists of transformer blocks, and is trained through reinforcement learning and an effective auxiliary: Set-induced Contrastive Regularization. The rewards are derived from a publicly available state-of-the-art pre-trained ranker (Section 2) to compute retrieval statistics among D+ and D− . Our no"
2021.eacl-main.2,2001.mtsummit-papers.46,0,0.618375,"Missing"
2021.eacl-main.2,W18-6545,0,0.0389249,"Missing"
2021.eacl-main.2,2020.acl-demos.30,1,0.722314,"ion generation: generate a clarification question by finding an “overlap” among documents in each cluster. In principle, the clarification questions should be specific to each cluster rather than generic and bland, otherwise it is counter to the objective of clarification (Radlinski and Craswell, 2017). In this work, we focus on developing a multi-document question generator to generate cluster-specific questions in the iii) step. Nevertheless, we believe our approach can be readily applied to multi-document text generation such as summarization (Liu and Lapata, 2019) and response generation (Zhang et al., 2020). We address this challenge by leveraging contrastive learning. Given a set of positive documents D+ and a set of negative documents D− (where D− is yet semantically close to D+ ), we propose a new strategy to generate a question that is semantically relevant to D+ and far away from D− . Ideally, the model would use both D+ and D− to identify distinguishing features between the two sets and constrain the generation to be specific to D+ . The similarity between the D+ and D− makes the generation more challenging and forces the model to be as specific as possible in order to distinguish between"
2021.eacl-main.2,D08-1079,0,0.132409,"Missing"
2021.eacl-main.2,P18-1178,0,0.0433462,"Missing"
2021.emnlp-main.527,D17-1215,0,0.07046,"Missing"
2021.emnlp-main.527,2020.acl-main.197,1,0.762507,"Missing"
2021.emnlp-main.527,W07-1401,0,0.034758,"ors on the IWSLT’14 De-En dataset. 4.3 Natural Language Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019a), which is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Dataset details can be found in Table 7 (Appendix B.2). Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b)6 . Training details are presented in Appendix B.2. Results. Table 3 summarizes experiment results on the GLUE development set. We can see that SALT outperforms BERTBASE in all the tasks. Further, our method is particularly effective for small datasets, such as RTE, MRPC, and CoLA, where we achieve 9"
2021.emnlp-main.527,2020.emnlp-main.102,1,0.802988,"Missing"
2021.emnlp-main.527,P19-1441,1,0.930642,"g is to build models that generalize well on the unperturbed test data. Note that robustness and generalization are different concepts. Recent works (Raghunathan et al., 2020; Min et al., 2020) showed that adversarial training can hurt generalization performance, i.e., accuracy on clean data. As such, adversarial training needs to be treated with great caution. Therefore, in NLP, this technique requires refined tuning of, for example, the training algorithm and the perturbation strength.  Fine-tuning pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019b; He et al., 2020) is state-ofthe-art for natural language understanding tasks such as the GLUE (Wang et al., 2019a) benchmark. Recently, there are works that use adversarial pre-training (Liu et al., 2020a) and adversarialregularized fine-tuning methods such as SMART (Jiang et al., 2020), FreeLB (Zhu et al., 2019), and FreeAT (Shafahi et al., 2019) to improve model generalization and robustness (Cheng et al., 2021). 3 Method Natural language inputs are discrete symbols (e.g., words), instead of continuous ones. Therefore, a common approach to generate perturbations is to learn continuous emb"
2021.emnlp-main.527,2020.acl-demos.16,1,0.821996,"Missing"
2021.emnlp-main.527,2021.ccl-1.108,0,0.0264572,"Missing"
2021.emnlp-main.527,2020.acl-main.441,0,0.0414905,"nd SALT on STS-B (upper) and SST-2 (lower) datasets. tion suffers from over-strong perturbations, such that the model cannot fit the unperturbed data well. This is supported by the fact that the training loss of SALT is smaller than that of SMART, which means SALT fits the data better. SALT also yields a smaller loss than SMART on the validation data, indicating that the Stackelberg game-formulated model exhibits better generalization performance.  Adversarial robustness. Even though the primary focus of SALT is model generalization, we still test its robustness on the Adversarial-NLI (ANLI, Nie et al. 2020) dataset. The dataset contains 163k data, which are collected via a humanand-model-in-the-loop approach. From Table 6, we can see that SALT improves model robustness upon conventional methods (i.e., SMART). Figure 4: Probing experiments. Each violin plot is based on 10 runs with different random seeds. datasets while keeping the representations fixed. Such a method directly measures the quality of representations generated by different models. As illustrated in Fig. 4, SALT outperforms the baseline methods by large margins.  Classification Model Calibration. Adversarial regularization also he"
2021.emnlp-main.527,N19-4009,0,0.012301,"esented in Appendix B.1. BLEU Transformer (Vaswani et al., 2017) FreeAT (Shafahi et al., 2019) FreeLB (Zhu et al., 2019) SMART (Jiang et al., 2020) 28.4 29.0 29.0 29.1 SALT 29.6 Table 5: sacreBLEU score on WMT’16 En-De. All the baseline results are from our re-implementation. Implementation. Recall that to generate adversarial examples, we perturb the word embeddings. In NMT experiments, we perturb both the source-side and the target-side embeddings. This strategy is empirically demonstrated (Sato et al., 2019) to be more effective than perturbing only one side of the inputs. We use Fairseq5 (Ott et al., 2019) to implement our algorithms. We adopt the Transformerbase (Vaswani et al., 2017) architecture in all the low-resource experiments, except IWSLT’14 DeEn. In this dataset, we use a model smaller than Transformer-base by decreasing the hidden dimension size from 2048 to 1024, and decreasing the number of heads from 8 to 4 (while dimension of each head doubles). For the rich-resource experi5 https://github.com/pytorch/fairseq Results. Experimental results for the low-resource experiments are summarized in Table 2. Notice that SMART, which utilizes conventional adversarial regularization, consiste"
2021.emnlp-main.527,W18-6301,0,0.0249322,"Missing"
2021.emnlp-main.527,N18-1202,0,0.0119944,"t from the above fields, in NLP, the goal of adversarial training is to build models that generalize well on the unperturbed test data. Note that robustness and generalization are different concepts. Recent works (Raghunathan et al., 2020; Min et al., 2020) showed that adversarial training can hurt generalization performance, i.e., accuracy on clean data. As such, adversarial training needs to be treated with great caution. Therefore, in NLP, this technique requires refined tuning of, for example, the training algorithm and the perturbation strength.  Fine-tuning pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Radford et al., 2019; Liu et al., 2019b; He et al., 2020) is state-ofthe-art for natural language understanding tasks such as the GLUE (Wang et al., 2019a) benchmark. Recently, there are works that use adversarial pre-training (Liu et al., 2020a) and adversarialregularized fine-tuning methods such as SMART (Jiang et al., 2020), FreeLB (Zhu et al., 2019), and FreeAT (Shafahi et al., 2019) to improve model generalization and robustness (Cheng et al., 2021). 3 Method Natural language inputs are discrete symbols (e.g., words), instead of continuous ones. Therefore, a common"
2021.emnlp-main.527,W18-6319,0,0.0123062,"raining (Adv). Similar observations were also reported in Miyato et al. (2017); Sato et al. (2019). This is because Adv generates perturbations using the correct examples, thus, the label information are “leaked” (Kurakin et al., 2017). Additionally, we can see that SALT is particularly effective in this low-resource setting, where it outperforms all the baselines by large margins. In comparison with the vanilla Transformer model, SALT achieves up to 2 BLEU score improvements on all the three datasets. Table 5 summarizes experiment results on the WMT’16 En-De dataset. We report the sacreBLEU (Post, 2018) score, which is a detokenzied version of the BLEU score that better reflects translation quality. We can see that SALT outperforms all the baseline methods by notable margins, and it improves upon the vanilla Transformer model by 1.2 BLEU score. 6567 (a) Number of unrolling steps. (b) Perturbation strength , `2 case. (c) Perturbation strength , `∞ case. Figure 1: Relation between BLEU score and different factors on the IWSLT’14 De-En dataset. 4.3 Natural Language Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmar"
2021.emnlp-main.527,D16-1264,0,0.0085038,"e that SALT outperforms all the baseline methods by notable margins, and it improves upon the vanilla Transformer model by 1.2 BLEU score. 6567 (a) Number of unrolling steps. (b) Perturbation strength , `2 case. (c) Perturbation strength , `∞ case. Figure 1: Relation between BLEU score and different factors on the IWSLT’14 De-En dataset. 4.3 Natural Language Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019a), which is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Dataset details can be found in Table 7 (Appendix B.2). Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a,"
2021.emnlp-main.527,P19-1020,0,0.168192,"ularization (SALT), where we for1 Introduction mulate adversarial regularization as a Stackelberg Adversarial regularization (Miyato et al., 2017) has game (Von Stackelberg, 2010). The concept arises been shown to improve the generalization perfor- from economics, where two firms are competing mance of deep learning models in various natural in a market, and one of the them is in the leading language processing (NLP) tasks, such as language position by acknowledging the opponent’s strategy. modeling (Wang et al., 2019b), machine translation In Stackelberg adversarial regularization, a leader (Sato et al., 2019), natural language understand- solves for the model parameters, and a follower ing (Jiang et al., 2020), and reading comprehen- generates input perturbations. The leader procures ∗ Corresponding author. its advantage by considering what the best response 6562 Abstract Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 6562–6577 c November 7–11, 2021. 2021 Association for Computational Linguistics of the follower is, i.e., how will the follower respond after observing the leader’s decision. Then, the leader minimizes its loss, anticipating the predicte"
2021.emnlp-main.527,P16-1162,0,0.0314093,"Missing"
2021.emnlp-main.527,D13-1170,0,0.00291882,"nsformer model by 1.2 BLEU score. 6567 (a) Number of unrolling steps. (b) Perturbation strength , `2 case. (c) Perturbation strength , `∞ case. Figure 1: Relation between BLEU score and different factors on the IWSLT’14 De-En dataset. 4.3 Natural Language Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019a), which is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Dataset details can be found in Table 7 (Appendix B.2). Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b)6 . Training details are presented in Appendix B.2. Results. Table 3 summarizes experiment resu"
2021.emnlp-main.527,Q19-1040,0,0.0142734,"le margins, and it improves upon the vanilla Transformer model by 1.2 BLEU score. 6567 (a) Number of unrolling steps. (b) Perturbation strength , `2 case. (c) Perturbation strength , `∞ case. Figure 1: Relation between BLEU score and different factors on the IWSLT’14 De-En dataset. 4.3 Natural Language Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019a), which is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Dataset details can be found in Table 7 (Appendix B.2). Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b)6 . Training details are presented in Appendix B."
2021.emnlp-main.527,N18-1101,0,0.0149337,"nguage Understanding Datasets. We demonstrate the effectiveness of SALT on the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2019a), which is a collection of nine NLU tasks. The benchmark includes question answering (Rajpurkar et al., 2016), linguistic acceptability (CoLA, Warstadt et al. 2019), sentiment analysis (SST, Socher et al. 2013), text similarity (STS-B, Cer et al. 2017), paraphrase detection (MRPC, Dolan and Brockett 2005), and natural language inference (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. 2006; Giampiccolo et al. 2007; Bentivogli et al. 2009; Williams et al. 2018) tasks. Dataset details can be found in Table 7 (Appendix B.2). Implementation. We evaluate our algorithm by fine-tuning a pre-trained BERT-base (Devlin et al., 2019) model. Our implementation is based on the MT-DNN code-base (Liu et al., 2019a, 2020b)6 . Training details are presented in Appendix B.2. Results. Table 3 summarizes experiment results on the GLUE development set. We can see that SALT outperforms BERTBASE in all the tasks. Further, our method is particularly effective for small datasets, such as RTE, MRPC, and CoLA, where we achieve 9.4, 4.3, and 6.3 absolute improvements, respect"
2021.emnlp-main.812,2020.emnlp-demos.22,0,0.0250449,"3 4.5 Table 2: Dataset statistics. The WN18RR dataset is significantly sparser than the FB15K-237 dataset. by the computed scores, the rank of the ground truth target entity is used to further compute various ranking metrics such as mean reciprocal rank (MRR) and hits@k, k ∈ {1, 3, 10}. We report all of these ranking metrics under the filtered setting proposed in Bordes et al. (2013) where valid entities except the ground truth target entity are filtered out from the rank list. 3.3 Experimental Setup We implement our proposed method in PyTorch (Paszke et al., 2019) under the LibKGE framework (Broscheit et al., 2020). To perform a fair comparison with some early baseline methods, we reproduce their results using hyper-parameter configurations from LibKGE.7 All data and evaluation metrics can be found in LibKGE. Our model consists of a three-layer entity Transformer and a six-layers context Transformer. Each 7 These configurations consider many recent training techniques and are found by extensive searches. Thus the results are generally much better then the original reported ones. 10399 Transformer layer has eight heads. The dimension size of hidden states is 320 across all layers except that we use 1280"
2021.emnlp-main.812,2020.acl-main.617,0,0.136704,"x in the input embeddings its graph neighborhood is denser (in which case, layer and the linear transformation of this classification layer. 10398 FB15K-237 Model WN18RR Hits↑ #Params MRR↑ @1 @3 @10 .266 .218 .249 .250 .247 .241 .272 .266 .264 .246 .390 .345 .378 .377 .372 .375 .400 .394 .390 .380 Hits↑ #Params MRR↑ @1 @3 @10 RESCAL (Nickel et al., 2011) TransE (Bordes et al., 2013) DistMult (Yang et al., 2015) ComplEx (Trouillon et al., 2016) ConvE (Dettmers et al., 2018) RotatE (Sun et al., 2018) CoKE (Wang et al., 2019) TuckER (Balazevic et al., 2019) CompGCN (Vashishth et al., 2020) RotH (Chami et al., 2020) 6M 2M 4M 4M 9M 15M 10M 8M .356 .310 .342 .343 .338 .338 .364 .358 .355 .344 .535 .495 .531 .532 .521 .533 .549 .544 .535 .535 6M 21M 21M 5M 36M 20M 17M 21M .467 .232 .451 .479 .439 .476 .484 .470 .479 .496 .439 .061 .414 .441 .409 .428 .450 .443 .443 .449 .478 .366 .466 .495 .452 .492 .496 .482 .494 .514 .516 .522 .523 .552 .499 .571 .553 .526 .546 .586 HittER 16M .373 .279 .409 .558 24M .503 .462 .516 .584 Table 1: Comparison between the proposed method and baseline methods. Results of RotatE, CoKE, TuckER, CompGCN, and RotH are taken from their original papers. Numbers in bold represent the"
2021.emnlp-main.812,2021.acl-long.336,0,0.031638,"in the knowledge graph used to train HittER. We include more details of the experiments in the Appendix. embeddings (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015). Another line of work uses tensor factorization methods to match entities semantically. Starting from simple bi-linear transformations in the euclidean space (Nickel et al., 2011; Yang et al., 2015), numerous complicated transformations in various spaces have been hence proposed (Trouillon et al., 2016; Ebisu and Ichise, 2018; Sun et al., 2018; Zhang et al., 2019a; Chami et al., 2020; Tang et al., 2020; Chao et al., 2021). Such methods effectively capture the intuition from observation of data but suffer from unobserved geometric properties and are generally limited in expressiveness. In light of recent advances in deep learning, many more powerful neural network modules such as Convolutional Neural Networks (Dettmers et al., 2018), Capsule Networks (Nguyen et al., 2019), and Transformers (Wang et al., 2019) are also introduced to capture the interaction between entity and relation embeddings. These methods produce rich representations and better performance on predicting missing links in knowledge graphs. How"
2021.emnlp-main.812,N19-1423,0,0.495292,"of the knowledge graphs are still highly incomplete (West et al., 2014). Our approach achieves new state-of-the-art results on two standard benchmark datasets: FB15K-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al., 2018). Unlike the previous shallow KGE methods that cannot be trivially utilized by widely used Transformer-based models for language tasks (Peters et al., 2019), our approach benefits from the unified Transformer architecture and its extensibility. As a case study, we show how to integrate the learned representations of HittER into pre-trained language models like BERT (Devlin et al., 2019). Our experiments demonstrate that HittER significantly improves BERT on two Freebase factoid question answering (QA) datasets: FreebaseQA (Jiang et al., 2019) and Webques1 GNN methods’ depth is tied to their receptive fields and thus constrained by over-smoothing issues (Liu et al., 2020). tionSP (Yih et al., 2016). Our experimental code as well as multiple pretrained models are publicly available.2 2 HittER We introduce our proposed hierarchical Transformer model (Figure 2) in this section. In Section 2.1, we provide the background about how link prediction can be done with a simple Transfor"
2021.emnlp-main.812,P19-1598,0,0.0283755,"ediction task to balance information from the relational context and the source entity itself. Experimental results show that HittER achieves new stateof-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets. 1 Introduction Knowledge graphs (KG) are a major form of knowledge bases where knowledge is stored as graphstructured data. Because of their broad applications in various intelligent systems including natural language understanding (Logan et al., 2019; Zhang et al., 2019b; Hayashi et al., 2020) and reasoning (Riedel et al., 2013; Xiong et al., 2017; Bauer et al., 2018; Verga et al., 2021), learning representations of knowledge graphs has been studied in a large body of literature. To learn high quality representations of knowledge graphs, many researchers adopt the idea of mapping the entities and relations in a knowledge graph to points in a vector space. These knowledge graph embedding (KGE) methods usually leverage geometric properties in the vector space, such country Sunnyvale adjoins Cupertino state California Figure 1: An example su"
2021.emnlp-main.812,P15-1067,0,0.0284913,"dataset, in which case the input entities for HittER cannot be provided. Thus we also report results under a filtered setting, i.e., a subset retaining only examples whose context entity and answer entity both exist on the FB15K-237 dataset. Our experimental results in Table 6 show that HittER’s representation significantly enhances BERT’s question answering ability, especially when the questions are related to entities in the knowledge graph used to train HittER. We include more details of the experiments in the Appendix. embeddings (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015). Another line of work uses tensor factorization methods to match entities semantically. Starting from simple bi-linear transformations in the euclidean space (Nickel et al., 2011; Yang et al., 2015), numerous complicated transformations in various spaces have been hence proposed (Trouillon et al., 2016; Ebisu and Ichise, 2018; Sun et al., 2018; Zhang et al., 2019a; Chami et al., 2020; Tang et al., 2020; Chao et al., 2021). Such methods effectively capture the intuition from observation of data but suffer from unobserved geometric properties and are generally limited in expressiveness. In ligh"
2021.emnlp-main.812,N19-1028,0,0.104032,"FB15K-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al., 2018). Unlike the previous shallow KGE methods that cannot be trivially utilized by widely used Transformer-based models for language tasks (Peters et al., 2019), our approach benefits from the unified Transformer architecture and its extensibility. As a case study, we show how to integrate the learned representations of HittER into pre-trained language models like BERT (Devlin et al., 2019). Our experiments demonstrate that HittER significantly improves BERT on two Freebase factoid question answering (QA) datasets: FreebaseQA (Jiang et al., 2019) and Webques1 GNN methods’ depth is tied to their receptive fields and thus constrained by over-smoothing issues (Liu et al., 2020). tionSP (Yih et al., 2016). Our experimental code as well as multiple pretrained models are publicly available.2 2 HittER We introduce our proposed hierarchical Transformer model (Figure 2) in this section. In Section 2.1, we provide the background about how link prediction can be done with a simple Transformer scoring function. We then describe the detailed architecture of our proposed model in Section 2.2. Finally, we discuss our strategies to learn balanced con"
2021.emnlp-main.812,2020.tacl-1.28,0,0.0164964,"cal Methods in Natural Language Processing, pages 10395–10407 c November 7–11, 2021. 2021 Association for Computational Linguistics Vashishth et al., 2020). However, these methods are usually restricted in expressiveness because of the shallow network architecture they use.1 In this paper, we present HittER, a deep hierarchical Transformer model to learn representations of entities and relations in a knowledge graph jointly by aggregating information from graph neighborhoods. Although prior work shows Transformers can learn relational knowledge from large amounts of unstructured textual data (Jiang et al., 2020; Manning et al., 2020), HittER explicitly operates over structured inputs using a hierarchical architecture. Essentially, HittER consists of two levels of Transformer blocks. As shown in Figure 2, the bottom block provides relation-dependent entity embeddings for the neighborhood around an entity and the top block aggregates information from its graph context. To ensure HittER work across graphs of different properties, we further design a masked entity prediction task to balance the contextual relational information and information from the training entity itself. We evaluate the proposed me"
2021.emnlp-main.812,P19-1466,0,0.019398,"e to capture different relations between entities. (2019) borrow the idea from Graph Attention Early work uses translational distance-based scor- Networks (Veliˇckovi´c et al., 2018), using a biing functions defined on top of entity and relation linear attention mechanism to selectively gather useful information from neighbor entities. Differsimplify the modeling architecture, we also make the number of tokens known to all models. ent from their simple single-layer attention formu10402 lation, we use the advanced Transformer to capture both the entity-relation and entity-context interactions. Nathani et al. (2019) also propose an attention-based feature embedding to capture multihop neighbor information, but unfortunately, their reported results have been proven to be unreliable in a recent re-evaluation (Sun et al., 2020). 6 Conclusion and Future Work In this work, we proposed HittER, a novel Transformer-based model with effective training strategies for learning knowledge graph embeddings in complex multi-relational graphs. We show that with contextual information from a local neighborhood, our proposed method outperforms all previous approaches in long-standing link prediction tasks, achieving new S"
2021.emnlp-main.812,N19-1226,0,0.0189244,"ckel et al., 2011; Yang et al., 2015), numerous complicated transformations in various spaces have been hence proposed (Trouillon et al., 2016; Ebisu and Ichise, 2018; Sun et al., 2018; Zhang et al., 2019a; Chami et al., 2020; Tang et al., 2020; Chao et al., 2021). Such methods effectively capture the intuition from observation of data but suffer from unobserved geometric properties and are generally limited in expressiveness. In light of recent advances in deep learning, many more powerful neural network modules such as Convolutional Neural Networks (Dettmers et al., 2018), Capsule Networks (Nguyen et al., 2019), and Transformers (Wang et al., 2019) are also introduced to capture the interaction between entity and relation embeddings. These methods produce rich representations and better performance on predicting missing links in knowledge graphs. However, they are restricted by the amount of information that can be encoded in a single node embedding and the great effort to memorize local connectivity patterns. 5.2 Context-aware Methods Various forms of graph contexts have been proven effective in recent work on neural networks operating in graphs under the message passing framework (Bruna et al., 20"
2021.emnlp-main.812,D19-1005,0,0.0350681,"Missing"
2021.emnlp-main.812,N13-1008,0,0.0415593,"e entity itself. Experimental results show that HittER achieves new stateof-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets. 1 Introduction Knowledge graphs (KG) are a major form of knowledge bases where knowledge is stored as graphstructured data. Because of their broad applications in various intelligent systems including natural language understanding (Logan et al., 2019; Zhang et al., 2019b; Hayashi et al., 2020) and reasoning (Riedel et al., 2013; Xiong et al., 2017; Bauer et al., 2018; Verga et al., 2021), learning representations of knowledge graphs has been studied in a large body of literature. To learn high quality representations of knowledge graphs, many researchers adopt the idea of mapping the entities and relations in a knowledge graph to points in a vector space. These knowledge graph embedding (KGE) methods usually leverage geometric properties in the vector space, such country Sunnyvale adjoins Cupertino state California Figure 1: An example subgraph sampled from FB15K237. Four nodes (entities) are connected by three diff"
2021.emnlp-main.812,2020.acl-main.412,0,0.0373134,"erties in the vector space, such country Sunnyvale adjoins Cupertino state California Figure 1: An example subgraph sampled from FB15K237. Four nodes (entities) are connected by three different types of relations representing facts like Sunnyvale belongs to the state of California. as translation (Bordes et al., 2013), bilinear transformations (Yang et al., 2015, DistMult), or rotation (Sun et al., 2018). Multi-layer convolutional networks are also used for KGE (Dettmers et al., 2018, ConvE). Such KGE methods are conceptually simple and can be applied to tasks like factoid question answering (Saxena et al., 2020) and language modeling (Peters et al., 2019). However, it is rather challenging to encode all of the information about an entity into a single vector. For example, to infer the missing object in the incomplete triplet &lt;Sunnyvale, county, ?&gt; (Figure 1), traditional KGE methods rely on the geographic information stored in the embedding of Sunnyvale. While we can read such information from its graph context, e.g., from a neighbor node that represents the state it belongs to (i.e., California). In this way, we allow the model to store and utilize information about an entity via its relational cont"
2021.emnlp-main.812,2020.acl-main.241,0,0.0295181,"related to entities in the knowledge graph used to train HittER. We include more details of the experiments in the Appendix. embeddings (Bordes et al., 2013; Wang et al., 2014; Lin et al., 2015; Ji et al., 2015). Another line of work uses tensor factorization methods to match entities semantically. Starting from simple bi-linear transformations in the euclidean space (Nickel et al., 2011; Yang et al., 2015), numerous complicated transformations in various spaces have been hence proposed (Trouillon et al., 2016; Ebisu and Ichise, 2018; Sun et al., 2018; Zhang et al., 2019a; Chami et al., 2020; Tang et al., 2020; Chao et al., 2021). Such methods effectively capture the intuition from observation of data but suffer from unobserved geometric properties and are generally limited in expressiveness. In light of recent advances in deep learning, many more powerful neural network modules such as Convolutional Neural Networks (Dettmers et al., 2018), Capsule Networks (Nguyen et al., 2019), and Transformers (Wang et al., 2019) are also introduced to capture the interaction between entity and relation embeddings. These methods produce rich representations and better performance on predicting missing links in k"
2021.emnlp-main.812,W15-4007,0,0.205256,"n task, which is one of the canonical tasks in statistical relational learning (SRL). Link prediction (essentially KG completion) serves as a good proxy to evaluate the effectiveness of learned graph representations, by measuring the ability of a model to generalize relational knowledge stored in training graphs to unseen facts. Meanwhile, it has an important application to knowledge graph completion given the fact that most of the knowledge graphs are still highly incomplete (West et al., 2014). Our approach achieves new state-of-the-art results on two standard benchmark datasets: FB15K-237 (Toutanova and Chen, 2015) and WN18RR (Dettmers et al., 2018). Unlike the previous shallow KGE methods that cannot be trivially utilized by widely used Transformer-based models for language tasks (Peters et al., 2019), our approach benefits from the unified Transformer architecture and its extensibility. As a case study, we show how to integrate the learned representations of HittER into pre-trained language models like BERT (Devlin et al., 2019). Our experiments demonstrate that HittER significantly improves BERT on two Freebase factoid question answering (QA) datasets: FreebaseQA (Jiang et al., 2019) and Webques1 GNN"
2021.emnlp-main.812,2021.naacl-main.288,0,0.0291972,"es new stateof-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets. 1 Introduction Knowledge graphs (KG) are a major form of knowledge bases where knowledge is stored as graphstructured data. Because of their broad applications in various intelligent systems including natural language understanding (Logan et al., 2019; Zhang et al., 2019b; Hayashi et al., 2020) and reasoning (Riedel et al., 2013; Xiong et al., 2017; Bauer et al., 2018; Verga et al., 2021), learning representations of knowledge graphs has been studied in a large body of literature. To learn high quality representations of knowledge graphs, many researchers adopt the idea of mapping the entities and relations in a knowledge graph to points in a vector space. These knowledge graph embedding (KGE) methods usually leverage geometric properties in the vector space, such country Sunnyvale adjoins Cupertino state California Figure 1: An example subgraph sampled from FB15K237. Four nodes (entities) are connected by three different types of relations representing facts like Sunnyvale be"
2021.emnlp-main.812,D17-1060,0,0.0248142,"rimental results show that HittER achieves new stateof-the-art results on multiple link prediction datasets. We additionally propose a simple approach to integrate HittER into BERT and demonstrate its effectiveness on two Freebase factoid question answering datasets. 1 Introduction Knowledge graphs (KG) are a major form of knowledge bases where knowledge is stored as graphstructured data. Because of their broad applications in various intelligent systems including natural language understanding (Logan et al., 2019; Zhang et al., 2019b; Hayashi et al., 2020) and reasoning (Riedel et al., 2013; Xiong et al., 2017; Bauer et al., 2018; Verga et al., 2021), learning representations of knowledge graphs has been studied in a large body of literature. To learn high quality representations of knowledge graphs, many researchers adopt the idea of mapping the entities and relations in a knowledge graph to points in a vector space. These knowledge graph embedding (KGE) methods usually leverage geometric properties in the vector space, such country Sunnyvale adjoins Cupertino state California Figure 1: An example subgraph sampled from FB15K237. Four nodes (entities) are connected by three different types of relat"
2021.emnlp-main.812,P16-2033,0,0.0694591,"d Transformer-based models for language tasks (Peters et al., 2019), our approach benefits from the unified Transformer architecture and its extensibility. As a case study, we show how to integrate the learned representations of HittER into pre-trained language models like BERT (Devlin et al., 2019). Our experiments demonstrate that HittER significantly improves BERT on two Freebase factoid question answering (QA) datasets: FreebaseQA (Jiang et al., 2019) and Webques1 GNN methods’ depth is tied to their receptive fields and thus constrained by over-smoothing issues (Liu et al., 2020). tionSP (Yih et al., 2016). Our experimental code as well as multiple pretrained models are publicly available.2 2 HittER We introduce our proposed hierarchical Transformer model (Figure 2) in this section. In Section 2.1, we provide the background about how link prediction can be done with a simple Transformer scoring function. We then describe the detailed architecture of our proposed model in Section 2.2. Finally, we discuss our strategies to learn balanced contextual representations of an entity in Section 2.3. 2.1 Transformers for Link Prediction A knowledge graph can be viewed as a set of triplets (G = {(es , rp"
2021.emnlp-main.812,2020.emnlp-main.595,0,0.0696895,"Missing"
2021.emnlp-main.813,D18-1398,0,0.0285564,"ty-aware self-attention. 10414 These methods are designed for standard supervised learning, and have a limited generalization ability in few-shot settings, as empirically shown in (Fritzler et al., 2019). Prototype-based methods recently become popular few-shot learning approaches in machine learning community. It was firstly studied in the context of image classification (Vinyals et al., 2016; Sung et al., 2018; Zhao et al., 2020), and has recently been adapted to different NLP tasks such as text classification (Wang et al., 2018; Geng et al., 2019; Bansal et al., 2020), machine translation (Gu et al., 2018) and relation classification (Han et al., 2018). The work closest related to ours is (Fritzler et al., 2019) which explores prototypical network on fewshot NER, but only utilizes RNNs as the backbone model and does not leverage the power of largescale Transformer-based architectures for word representations. Our work is similar to (Ziyadi et al., 2020; Wiseman and Stratos, 2019) in that all of them utilize the nearest neighbor criterion to assign the entity type, but differs in that (Ziyadi et al., 2020; Wiseman and Stratos, 2019) consider every individual token instance for nearest neighbor c"
2021.emnlp-main.813,D18-1514,0,0.0225138,"re designed for standard supervised learning, and have a limited generalization ability in few-shot settings, as empirically shown in (Fritzler et al., 2019). Prototype-based methods recently become popular few-shot learning approaches in machine learning community. It was firstly studied in the context of image classification (Vinyals et al., 2016; Sung et al., 2018; Zhao et al., 2020), and has recently been adapted to different NLP tasks such as text classification (Wang et al., 2018; Geng et al., 2019; Bansal et al., 2020), machine translation (Gu et al., 2018) and relation classification (Han et al., 2018). The work closest related to ours is (Fritzler et al., 2019) which explores prototypical network on fewshot NER, but only utilizes RNNs as the backbone model and does not leverage the power of largescale Transformer-based architectures for word representations. Our work is similar to (Ziyadi et al., 2020; Wiseman and Stratos, 2019) in that all of them utilize the nearest neighbor criterion to assign the entity type, but differs in that (Ziyadi et al., 2020; Wiseman and Stratos, 2019) consider every individual token instance for nearest neighbor comparison, while ours considers prototypes for"
2021.emnlp-main.813,P19-1524,0,0.294934,"both few-shot and training-free settings compared with existing methods. 1 Prototype Methods Linear Classifier Fine-tuning 2 Noisy Supervised Pretraining 3 Self-Training Figure 1: An overview of methods studied in our paper. Linear classifier fine-tuning is a default baseline that updates an NER model from pre-trained Roberta/BERT. We study three orthogonal strategies to improve NER models in the limited labeled data settings. has shown remarkable success in NER in recent years, especially with self-supervised pre-trained language models (PLMs) such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019c). State-of-the-art (SoTA) NER models are often initialized with PLM weights, fine-tuned with standard supervised learning. One classic approach is to add a linear classifier on top of the representations provided by PLMs, and fine-tune the entire model using a cross-entropy objective on domainspecific labels (Devlin et al., 2019). Despite its simplicity, the approach generally results in good performance on benchmarks and serves as a strong baseline in this study. 1 Introduction Unfortunately, even with these PLMs, building Named Entity Recognition (NER) involves pro- NER systems still remai"
2021.emnlp-main.813,2020.findings-emnlp.17,1,0.896572,"s persons, organizations, loca- to achieve a reasonable accuracy. However, this is tions, medical codes, dates and quantities. NER in contrast to the real-world application scenarios, serves as an important first component for tasks where only very small amounts of labeled data are such as information extraction (Ritter et al., 2012), available for new domains, such as medical (Hofer information retrieval (Guo et al., 2009), question et al., 2018) domain. The cost of building NER sysanswering (Moll´a et al., 2006), task-oriented dia- tems at scale with rich annotations (i.e., hundreds logues (Peng et al., 2020a; Gao et al., 2019) and of different enterprise use-cases/domains) can be other language understanding applications (Nadeau prohibitively expensive. This draws attentions to and Sekine, 2007; Shaalan, 2014). Deep learning a challenging but practical research problem: fewshot NER. ∗ Work performed during an internship at Microsoft † Corresponding author To deal with the challenge of few-shot learning, 10408 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 10408–10423 c November 7–11, 2021. 2021 Association for Computational Linguistics we focus on i"
2021.emnlp-main.813,N18-1202,0,0.0399813,"Missing"
2021.emnlp-main.813,N18-1109,0,0.0253222,"ypes are related but different (e.g., Musician and Artist are more fine-grained types of Person in the downstream task). The associated types on each token can be noisy. (d) Self-training: An NER system (teacher model) trained on a small labeled dataset is used to predict soft labels for sentences in a large unlabeled dataset. The joint of the predicted dataset and original dataset is used to train a student model. 3.1 Prototype-based Methods Meta-learning (Ravi and Larochelle, 2017) has shown promising results for few-shot image classification (Tian et al., 2020) and sentence classification (Yu et al., 2018; Geng et al., 2019). It is natural to adapt this idea to few-shot NER. The core idea is to use episodic classification paradigm to simulate few-shot settings during model training. Specifically in each episode, M entity types (usually M &lt; |Y|) are randomly sampled from DL , containing a ×K support set S = {(Xi , Yi }M (K sentences per i=1 ˆ i, Y ˆ i }M ×K 0 (K 0 type) and a query set Q = {(X i=1 sentences per type). We build our method based on prototypical network (Snell et al., 2017), which introduces the notion of prototypes, representing entity types as vectors in the same representation"
2021.findings-acl.29,N19-1423,0,0.0291568,"answers, we first take the predictions of the generative reader (G) in Sec. 2.3, which is trained on the passages without reranking and used for final passage reading in R3. It represents an apple-to-apple comparison to R2 without any additional information but higher-quality input. We also experiment with an extractive reader (E) that has access to all retrieved passages, where the goal is to study whether we can rerank passages via other signals and further improve G such that it outperforms both G and E when they are in R2. We use the extractive reader in Mao et al. (2020) with BERT-base (Devlin et al., 2019) representation and span voting. For the generative reader, we either take its top-1 prediction with greedy decoding or sample 10 answers with decoding parameters as follows. We set sampling temperature to 5/2 and the top probability in nucleus sampling to 0.5/0.5 on NQ/Trivia, Quality of Reranking Signals We first analyze the EM of the top-N reader predictions A[:N ] . We consider a question correctly answered as long as one of the top-N predictions matches the ground-truth answer. The standard EM is a special case with N = 1. As listed in Table 3, the reader EM can be improved by up to 24.0"
2021.findings-acl.29,P17-1147,0,0.338917,"the top predicted answers of the reader before reranking. Intuitively, the top predictions of the reader are closely related to the ground-truth answer and even if the predicted answers are partially correct or incorrect, they may still provide useful signals suggesting which passages may contain the correct answer (Mao et al., 2020). We conduct experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and Triv344 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 344–350 August 1–6, 2021. ©2021 Association for Computational Linguistics iaQA (Trivia) (Joshi et al., 2017) datasets. We demonstrate that R3 with R IDER, without any additional training, achieves 10 to 20 absolute gains in top-1 retrieval accuracy, and 1 to 4 gains in Exact Match (EM) compared to the R2 architecture. R IDER also outperforms two state-of-theart transformer-based supervised reranking models that require expensive training and inference. Notably, using only 1,024 tokens (7.8 passages on average) as the input of a generative reader, R IDER achieves EM=47.5/63.5 on NQ/Trivia when the predictions of the same generative reader (EM=45.3/62.2 in R2) are used for reranking, and EM=48.3/66.4"
2021.findings-acl.29,2020.emnlp-main.550,0,0.0956417,"Missing"
2021.findings-acl.29,Q19-1026,0,0.20036,"ropose a simple and effective passage reranking method, named Reader-guIDEd Reranker (R IDER), which does not require any training and reranks the retrieved passages solely based on their lexical overlap with the top predicted answers of the reader before reranking. Intuitively, the top predictions of the reader are closely related to the ground-truth answer and even if the predicted answers are partially correct or incorrect, they may still provide useful signals suggesting which passages may contain the correct answer (Mao et al., 2020). We conduct experiments on the Natural Questions (NQ) (Kwiatkowski et al., 2019) and Triv344 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 344–350 August 1–6, 2021. ©2021 Association for Computational Linguistics iaQA (Trivia) (Joshi et al., 2017) datasets. We demonstrate that R3 with R IDER, without any additional training, achieves 10 to 20 absolute gains in top-1 retrieval accuracy, and 1 to 4 gains in Exact Match (EM) compared to the R2 architecture. R IDER also outperforms two state-of-theart transformer-based supervised reranking models that require expensive training and inference. Notably, using only 1,024 tokens (7.8 passages o"
2021.findings-acl.29,D18-1053,0,0.210592,"rank the most relevant passages at the very top. One line of work (Mao et al., 2020; Karpukhin et al., 2020) aims to improve the retriever and shows that significantly better QA performance can be achieved when the retrieval results are improved. ∗ Work was done during internship at Microsoft Azure AI. Our code is available at https://github.com/ morningmoni/GAR. 1 An alternative solution is to rerank the initial retrieval results via a reranker, which is widely used in information retrieval (Nogueira and Cho, 2019; Qiao et al., 2019) and explored in early OpenQA systems (Wang et al., 2018a; Lee et al., 2018). However, current state-of-the-art OpenQA systems (Karpukhin et al., 2020; Izacard and Grave, 2020b) do not distinguish the order of the retrieved passages and instead equally consider a large number of retrieved passages (e.g., 100), which could be computationally prohibitive as the model size of the readers becomes larger (Izacard and Grave, 2020b). We argue that a Retriever-Reranker-Reader (R3) architecture is beneficial in terms of both model effectiveness and efficiency: passage reranking improves the retrieval accuracy of the retriever at top positions and allows the reader to achieve c"
2021.findings-acl.29,P19-1612,0,0.105441,"Missing"
2021.findings-acl.29,2020.acl-main.703,0,0.0970835,"Missing"
2021.findings-acl.29,2020.acl-tutorials.8,0,0.55013,"atural Questions dataset and 66.4 EM on the TriviaQA dataset when only 1,024 tokens (7.8 passages on average) are used as the reader input after passage reranking.1 1 Introduction Current open-domain question answering (OpenQA) systems often follow a RetrieverReader (R2) architecture, where the retriever first retrieves relevant passages and the reader then reads the retrieved passages to form an answer. Since the retriever retrieves passages from a large candidate pool (e.g., millions of Wikipedia passages), it often fails to rank the most relevant passages at the very top. One line of work (Mao et al., 2020; Karpukhin et al., 2020) aims to improve the retriever and shows that significantly better QA performance can be achieved when the retrieval results are improved. ∗ Work was done during internship at Microsoft Azure AI. Our code is available at https://github.com/ morningmoni/GAR. 1 An alternative solution is to rerank the initial retrieval results via a reranker, which is widely used in information retrieval (Nogueira and Cho, 2019; Qiao et al., 2019) and explored in early OpenQA systems (Wang et al., 2018a; Lee et al., 2018). However, current state-of-the-art OpenQA systems (Karpukhin et al"
2021.findings-acl.29,D19-1284,0,0.224656,"Missing"
2021.findings-acl.29,2020.emnlp-main.466,0,0.746661,"ng itself despite the noise in reader predictions. 2.3 Passage Reading We consider a scenario where the number of passages that can be used for QA is limited (sometimes deliberately) due to reasons such as insufficient computational resources, the limit of model input length, or requirement for faster responses. We use a generative reader initialized by BARTlarge (Lewis et al., 2019), which concatenates the question and top-10 retrieved passages, trims them to 1,024 tokens (7.8 passages are left on average) as the input, and learns to generate the answer in a seq2seq manner (Mao et al., 2020; Min et al., 2020). We further add a simple shuffle strategy during reader training, which randomly shuffles the top retrieved passages before concatenation. In this way, the reader appears to be more robust to the reranked passages during inference and achieves better performance after reranking. Passage Reranking Given an initially retrieved passage list R and topN predictions of the reader A[:N ] , R IDER forms a reranked passage list R0 as follows. R IDER scans R from the beginning of the list and appends to R0 every passage p ∈ R if p contains any reader prediction a ∈ A[:N ] after string normalization (re"
2021.findings-acl.29,2020.findings-emnlp.63,0,0.119668,"ess to much more passages. 4.4 Table 4: End-to-end QA comparison of state-of-theart methods. R IDER results in up to 4.2 EM gains. Top-1 Comparison w. Supervised Reranking Finally, we compare R IDER with two state-of-theart supervised reranking models. The first reranker is a BERT-base cross-encoder (Nogueira and Cho, 2019), which is popularly used for passage reranking in information retrieval. The cross-encoder concatenates the query and passage, and makes a binary relevance decision for each query-passage pair. The second one generates relevance labels as target tokens in a seq2seq manner (Nogueira et al., 2020). We use BART-large as the base model and “YES/NO” as the target tokens. As listed in Table 6, R IDER, without any train347 ing, outperforms the two transformer-based supervised rerankers on retrieval accuracy. Also, for QA performance, the best EM we obtain using the supervised rerankers is merely 46.3 on NQ. Such results further demonstrate the effectiveness of R IDER, which has the advantage of utilizing information from multiple passages (when the reader makes predictions), while the other rerankers consider query-passage pairs independently. 4.5 Runtime Efficiency The reranking step of R"
2021.findings-acl.29,2020.emnlp-main.437,0,0.155175,"Missing"
2021.findings-acl.42,W15-4918,0,0.0545411,"Missing"
2021.findings-acl.42,W07-0718,0,0.0285569,".04 0.00 -0.11 -0.12 0.04 0.07 -0.10 -0.12 0.07 -0.17 0.55 0.98 0.11 0.07 0.57 0.19 0.15 0.07 0.13 0.01** -0.01 -0.03 -0.09 -0.14 -0.03 0.01 -0.03 -0.09 0.01 0.03 p-value 0.82 0.64 0.18 0.03* 0.69 0.82 0.59 0.18 0.83 0.64 Table 5: Correlation (Corr) for 250 annotated XSUM and 250 SAMSUM generated summaries with fine-grained labeling. The arrow next to “Corr” indicates the direction of a correct correlation. 6 Related Work Prior work concerning evaluation of automatic metrics and human evaluation for NLG systems has mainly focused on general analysis of output quality or coherence and fluency (Callison-Burch et al., 2007; Graham, 2015; Fabbri et al., 2021), rather than factuality. Recent efforts by NLP researchers have drawn attention to the issue of factual errors 481 6.1 Discussion of Meta Evaluation and Conclusion Our analyses show that in contrast to prior work on factual consistency that mostly concentrated on one specific domain and dataset, our GO FIGURE framework is effective at evaluating sensitivity and validity of factual consistency metrics with only reference summaries, rather than requiring computationally intensive testing across summarization model variants to identify metric strengths and sho"
2021.findings-acl.42,P18-1060,0,0.0284132,"factuality level of Si may be unclear. Metric bounds provide points of comparison. A bounded but insensitive factuality metric may assign higher values to mostly nonfactual or unrelated summaries over summaries that are close to the reference. A metric that is sensitive only to a subset of errors might ignore a significant number of modelgenerated errors (Figure 1). Prior work such as Reiter and Belz (2009) highlight the risk of claiming validity without testing generality. The scoring function H(D, Si ) represented by human evaluation is a gold standard for assessment of generation quality (Chaganty et al., 2018), so M (D, Si ) should be an approximation. Table 1: Details of factuality metric conditions. Here M is a metric scoring function, D is a source document and Si is a summary. M (D, Sr ) where D is the source document and Sr is a randomly sampled summary from the corpus.2 We define the Upper Bound for the metric as M (D, Sf ), where Sf is the reference groundtruth summary. Since our controlled experiments use transformed versions of the reference summary with injected errors, the original reference is guaranteed to be at least as factually consistent as a transformed summary. To test sensitivit"
2021.findings-acl.42,P19-1264,1,0.837289,"Missing"
2021.findings-acl.42,2020.acl-main.454,0,0.0371766,"factual consistency and standard generation metrics, including QA metrics. It also reveals that while QA metrics generally improve over standard metrics that measure factuality across domains, performance is highly dependent on the way in which questions are generated. 1 2 Introduction The goal of text generation systems is to produce text that is fluent, coherent, relevant, as well as factually correct. Recent progress in neural approaches to building semantically constrained text generation systems has shown tremendous improvements in this direction (Liu and Lapata, 2019; Guo et al., 2018; Durmus et al., 2020; Wang et al., 2020). However, an important issue in text generation systems is that they can yield factually inconsistent text, caused by somewhat distorted or fabricated facts about the source text. Especially in document summarization tasks, models that abstract away salient aspects, have been shown to generate text ∗ Work done while first author was interning at MSR. Factuality Metric Meta Evaluation Since reference summaries may be an incomplete representation of the salient facts in a source document or unavailable, we consider factuality in terms of how well candidate summaries are fact"
2021.findings-acl.42,N19-1395,0,0.0436521,"Missing"
2021.findings-acl.42,P19-1213,0,0.0411927,"Missing"
2021.findings-acl.42,D19-5409,0,0.0132681,"o arise in realistic sum3 Evaluation Datasets 2 While this may not be the strictest lower bound in theoretical terms, we consider it appropriate as an empirical lower bound since the content is irrelevant to the document. A single random summary is used. 3 For our experiments, we inject up to a maximum of x errors with x ∈ {1, 2, 3}. We evaluate metrics on three datasets: 1-sentence BBC news summaries from the XSUM extreme summarization dataset (Narayan et al., 2018), multi-sentence summaries from the 479 CNN/DailyMail dataset (Nallapati et al., 2016), and the recently released SAMSUM corpus (Gliwa et al., 2019) consisting of English language conversations written by linguists and aligned multisentence summaries. 3.1 Diagnostic Datasets To test the ability of proposed metrics to fulfill our predefined conditions, we set up two diagnostic datasets consisting of (i) transformed reference summaries with simulated factuality errors that allow us to induce and measure factuality levels in a controlled setting and (ii) summaries generated by state-of-the-art transformer summarization models that allows us to measure the effectiveness of metrics in a real data setting. We sample 500 source / summary pairs f"
2021.findings-acl.42,D15-1013,0,0.0302286,"7 -0.10 -0.12 0.07 -0.17 0.55 0.98 0.11 0.07 0.57 0.19 0.15 0.07 0.13 0.01** -0.01 -0.03 -0.09 -0.14 -0.03 0.01 -0.03 -0.09 0.01 0.03 p-value 0.82 0.64 0.18 0.03* 0.69 0.82 0.59 0.18 0.83 0.64 Table 5: Correlation (Corr) for 250 annotated XSUM and 250 SAMSUM generated summaries with fine-grained labeling. The arrow next to “Corr” indicates the direction of a correct correlation. 6 Related Work Prior work concerning evaluation of automatic metrics and human evaluation for NLG systems has mainly focused on general analysis of output quality or coherence and fluency (Callison-Burch et al., 2007; Graham, 2015; Fabbri et al., 2021), rather than factuality. Recent efforts by NLP researchers have drawn attention to the issue of factual errors 481 6.1 Discussion of Meta Evaluation and Conclusion Our analyses show that in contrast to prior work on factual consistency that mostly concentrated on one specific domain and dataset, our GO FIGURE framework is effective at evaluating sensitivity and validity of factual consistency metrics with only reference summaries, rather than requiring computationally intensive testing across summarization model variants to identify metric strengths and shortcomings. We"
2021.findings-acl.42,D19-1320,0,0.0123154,"h or samplebased decoding strategies. We then annotate the generated summaries for fine-grained factual errors using the types in Figure 1 to create a hand-curated factual consistency diagnostic dataset. 4 Factuality Metrics for Evaluation We mainly focus on meta-evaluating most recently proposed factual consistency metrics which use two types of proxy natural language understanding (NLU) objectives aimed at implicitly capturing factuality in generated text: question-answering (QA) and a masked token prediction cloze task. For QA we evaluate using SummaQA (which uses QA pairs from the source, Scialom et al., 2019) and FEQA (which uses QA pairs from the summary, Durmus et al., 2020), while for the cloze task setting we use BLANC-Help and BLANC-Tune (Vasilyev et al., 2020, see the appendix for details of metrics). We also measure the factual-awareness of BERTScore (Zhang et al., 2020), a summarization metric that is aimed primarily at improving coherency rather than factual consistency, and standard summarization evaluation metrics (e.g. ROUGE (Lin, 2004)). 4 See the Appendix for details of linguistic feature extraction for injecting errors. 5 5.1 Meta-Analysis of Factuality Metrics Controlled Data Exper"
2021.findings-acl.42,2020.acl-main.704,0,0.0307123,"Missing"
2021.findings-acl.42,2020.emnlp-main.373,1,0.796905,"Missing"
2021.findings-acl.42,2020.eval4nlp-1.2,0,0.0168669,"ated factual consistency diagnostic dataset. 4 Factuality Metrics for Evaluation We mainly focus on meta-evaluating most recently proposed factual consistency metrics which use two types of proxy natural language understanding (NLU) objectives aimed at implicitly capturing factuality in generated text: question-answering (QA) and a masked token prediction cloze task. For QA we evaluate using SummaQA (which uses QA pairs from the source, Scialom et al., 2019) and FEQA (which uses QA pairs from the summary, Durmus et al., 2020), while for the cloze task setting we use BLANC-Help and BLANC-Tune (Vasilyev et al., 2020, see the appendix for details of metrics). We also measure the factual-awareness of BERTScore (Zhang et al., 2020), a summarization metric that is aimed primarily at improving coherency rather than factual consistency, and standard summarization evaluation metrics (e.g. ROUGE (Lin, 2004)). 4 See the Appendix for details of linguistic feature extraction for injecting errors. 5 5.1 Meta-Analysis of Factuality Metrics Controlled Data Experiments We provide the results of the sensitivity analysis over our controlled data on the XSUM domain in Table 2, on CNNDM in Table 3 and on SAMSUM in Table 4."
2021.findings-acl.42,2020.acl-main.450,0,0.0139054,"and standard generation metrics, including QA metrics. It also reveals that while QA metrics generally improve over standard metrics that measure factuality across domains, performance is highly dependent on the way in which questions are generated. 1 2 Introduction The goal of text generation systems is to produce text that is fluent, coherent, relevant, as well as factually correct. Recent progress in neural approaches to building semantically constrained text generation systems has shown tremendous improvements in this direction (Liu and Lapata, 2019; Guo et al., 2018; Durmus et al., 2020; Wang et al., 2020). However, an important issue in text generation systems is that they can yield factually inconsistent text, caused by somewhat distorted or fabricated facts about the source text. Especially in document summarization tasks, models that abstract away salient aspects, have been shown to generate text ∗ Work done while first author was interning at MSR. Factuality Metric Meta Evaluation Since reference summaries may be an incomplete representation of the salient facts in a source document or unavailable, we consider factuality in terms of how well candidate summaries are factually grounded with"
2021.findings-acl.42,2020.findings-emnlp.203,0,0.0170721,".28 / 84.19 84.13 / 84.07 80.79 Correlation p-value -1.00 / -0.96 0.05* / 0.18 -1.00 / -0.91 0.01** / 0.28 -0.99 / -0.94 0.11 / 0.23 -1.00 0.05* -1.00 / -0.99 0.02* / 0.07 -1.00 &lt;0.01** / 0.03* -1.00 0.03* -1.00 / -0.99 0.05* / 0.08 -1.00 0.01** / 0.04* -1.00 /-0.99 0.01** / 0.07 Table 4: Results of simulated factual error data experiments (SAMSUM, average of 5 runs). (See Table 2 caption for details.) Metric BLANC-Help BLANC-Tune SummaQA-C SummaQA-F1 FEQA R-1 R-2 R-3 R-L BERTScore XSUM and hallucinations in the output of neural summarization models (Cao et al., 2018; Massarelli et al., 2019; Zhao et al., 2020; Falke et al., 2019b; Goodrich et al., 2019; Celikyilmaz et al., 2020). A number of works have highlighted the effectiveness of QA and cloze task objectives for evaluating or improving factuality on specific domains (Eyal et al., 2019; Huang et al., 2020). We aim to evaluate these metrics more broadly, and consider a wider range of domains (notably dialogue). SAMSUM Corr (- ←) p-value Corr (- ←) 0.04 0.00 -0.11 -0.12 0.04 0.07 -0.10 -0.12 0.07 -0.17 0.55 0.98 0.11 0.07 0.57 0.19 0.15 0.07 0.13 0.01** -0.01 -0.03 -0.09 -0.14 -0.03 0.01 -0.03 -0.09 0.01 0.03 p-value 0.82 0.64 0.18 0.03* 0.69 0."
2021.findings-emnlp.348,D17-1215,0,0.0740587,"Missing"
2021.findings-emnlp.348,2020.acl-main.197,1,0.864432,"Missing"
2021.findings-emnlp.348,N19-4009,0,0.0204156,"ss-inducing regularization and Bregman proximal point optimization. Machine Translation Datasets. We use three low-resource datasets2 : English-German from IWSLT’14, EnglishVietnamese from IWSLT’15, and English-French from IWSLT’16. We also use a rich-resource dataset: English-German from WMT’16. Dataset statistics are summarized in Table 4. Implementation. In NMT tasks, we have the source-side and the target-side inputs. We add perturbations to both of their embeddings (Sato et al., 2019). This has demonstrated to be more effective than adding perturbations to a single side. We use Fairseq3 (Ott et al., 2019) to implement our algorithms. For En-Vi and En-Fr experiments, we use the Transformer-base architecture (Vaswani et al., 2017). For En-De (IWSLT’14) experiments, we modify4 the Transformer-base architecture by decreasing the hidden dimension size from 2048 to 1024, and decreasing the number of heads from 8 to 4 (while dimension of each head doubles). For En-De (WMT’16) experiments, we use the Transformer-big (Vaswani et al., 2017) architecture. The training details are presented in Appendix B.1. 2 https://iwslt.org/ https://github.com/pytorch/fairseq 4 https://github.com/pytorch/fairseq/ tree/"
2021.findings-emnlp.348,W18-6301,0,0.0517244,"Missing"
2021.findings-emnlp.348,N18-1202,0,0.0274412,"Missing"
2021.findings-emnlp.348,W18-6319,0,0.0202505,"Missing"
2021.findings-emnlp.348,D13-1170,0,0.00298141,"tly eral Language Understanding Evaluation (GLUE) (i.e., 5 in Fig. 2b), the model cannot adapt to the benchmark (Wang et al., 2019a), which is a col- perturbations well; and if we cache the perturbalection of nine natural language inference tasks. tions too infrequently (i.e., inf in Fig. 2b), staleness The benchmark includes question answering (Ra- of the perturbations hinders model generalization. jpurkar et al., 2016), linguistic acceptability (CoLA,  Robustness to the number of neighbors. In Warstadt et al. 2019), sentiment analysis (SST, Fig. 2c, notice that ARCH is robust to the number Socher et al. 2013), text similarity (STS-B, Cer of neighbors. We also examine a variant of the et al. 2017), paraphrase detection (MRPC, Dolan KNN memory-saving strategy (R-1-NN): namely and Brockett 2005), and natural language inference 5 (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. https://github.com/namisan/mt-dnn 4124 (a) Moving average. (b) Epochs between caching. (c) Number of neighbors. (d) Proportion of caching. Figure 2: Parameter study on the IWSLT’14 De-En dataset. Each error bar is based on three runs using different random seeds. Each dashed line signifies the SMART baseline. In (b), inf means w"
2021.findings-emnlp.348,Q19-1040,0,0.0211577,"imporDatasets. We conduct experiments on the Gen- tant. If we cache the perturbations too frequently eral Language Understanding Evaluation (GLUE) (i.e., 5 in Fig. 2b), the model cannot adapt to the benchmark (Wang et al., 2019a), which is a col- perturbations well; and if we cache the perturbalection of nine natural language inference tasks. tions too infrequently (i.e., inf in Fig. 2b), staleness The benchmark includes question answering (Ra- of the perturbations hinders model generalization. jpurkar et al., 2016), linguistic acceptability (CoLA,  Robustness to the number of neighbors. In Warstadt et al. 2019), sentiment analysis (SST, Fig. 2c, notice that ARCH is robust to the number Socher et al. 2013), text similarity (STS-B, Cer of neighbors. We also examine a variant of the et al. 2017), paraphrase detection (MRPC, Dolan KNN memory-saving strategy (R-1-NN): namely and Brockett 2005), and natural language inference 5 (RTE & MNLI, Dagan et al. 2006; Bar-Haim et al. https://github.com/namisan/mt-dnn 4124 (a) Moving average. (b) Epochs between caching. (c) Number of neighbors. (d) Proportion of caching. Figure 2: Parameter study on the IWSLT’14 De-En dataset. Each error bar is based on three runs"
2021.findings-emnlp.348,N18-1101,0,0.0998151,"Missing"
2021.findings-emnlp.380,P15-2017,0,0.0247172,"pable of engaging 1 Introduction in socially appropriate and empathetic conversaRecent progress in the field of natural language pro- tions with humans is still very challenging. Fig. 1 cessing (NLP) and computer vision (CV) has led to shows the examples of two images with comment considerable advances in the domains of image cap- threads for two NICE-Settings. The caption for the tioning, visual question answering, visual dialog first image generated by a captioning model of the and visual storytelling (Mao et al., 2015; Vinyals NICE-Setting I is “Some houses are at the foot of et al., 2015; Devlin et al., 2015; Chen and Zitnick, a mountain”. While this somewhat faithfully de2015; Donahue et al., 2015; Karpathy and Fei-Fei, scribes the image, imagine you posted the picture 2015; Kiros et al., 2014a,b; Gao et al., 2019; Shum on social media and someone responded with that et al., 2018). Most image captioning tasks focus on statement, would that spark an engaging conversagenerating literal descriptions of content either di- tion or feel like an empathetic response? Probably rectly or in the form of searching or understanding. not. A conversation is grounded not only in visible 4456 Findings of the Ass"
2021.findings-emnlp.380,W04-1013,0,0.0418555,"he ImageNet dataset, to extract image features. For models that rely on object detection (e.g., BUTD and VLP) we used an object detector pretrained on the visual genome dataset with 1,600 object classes. The feature vector v for each image had a dimension of 2048. Baseline Models. We provide the results using “off-the-shelf” baseline models on the proposed NICE-Setting I to benchmark performance. This is important to provide a comprehensive picture of the current performance of state-of-the-art methods on this task. The details of the baseline models can be found in the Appendix C.1. ROUGE-L (Lin, 2004), and SPICE (Anderson et al., 2016) evaluation results are reported in Table 2. The results show that the baseline models, including state-of-the-art image captioning models such as BUTD (Anderson et al., 2018), perform relatively poorly. Human Evaluation. We had 200 images and the corresponding generated comments from each model annotated by human labelers. We used the same procedure as the annotation described in Sec. 4.1. The labelers rated each generated comment in terms of how engaging, emotional, empathetic, appropriate and relevant it was. Table 2 shows the average scores for each model"
2021.findings-emnlp.380,2021.ccl-1.108,0,0.0397665,"Missing"
2021.findings-emnlp.380,P18-1238,0,0.348776,"e-training approach, MAGIC, to simulate human commenting on NICE dataset, which aims to generate targeted comments on a given image weakly supervised by affect features. Experiments show that MAGIC outperforms baseline methods on the NICE-Setting II. 2 Related Work With the recent advances in deep learning, a growing number of researchers are interested in studying vision and language jointly. Vision-language understanding has become one of the key components of conversational agents, such as Cortana (Microsoft, 2014). A great deal of focus has been paid to image captioning (Lin et al., 2014; Sharma et al., 2018a; Young et al., 2014), which typically focuses on literal descriptions of image content. However, in social conversations, people usually engage with others using language with emotions, opinions and subjectivity. For example, image commenting on human-machine interaction system has rich stylistic features. In this paper, we introduce the image comment generation task, where the aim is to build models that produce more engaging comments grounded in visual images. Specifically, we present a pre-training model for this task. There are several pre-trained models that address various tasks across"
2021.findings-emnlp.380,P16-1170,0,0.0333674,"Missing"
2021.findings-emnlp.380,P02-1040,0,0.112045,"Missing"
2021.findings-emnlp.380,N18-1202,0,0.0341132,"rs using language with emotions, opinions and subjectivity. For example, image commenting on human-machine interaction system has rich stylistic features. In this paper, we introduce the image comment generation task, where the aim is to build models that produce more engaging comments grounded in visual images. Specifically, we present a pre-training model for this task. There are several pre-trained models that address various tasks across the language and vision space. Large-scale pre-trained models have achieved stateof-art results on many natural language processing and generation tasks (Peters et al., 2018; Devlin et al., 2018; Yang et al., 2019; Liu et al., 2019; Radford et al., 2019). Pre-trained models learn representations using tasks such as predicting words based on their context. GPT-2 and CTRL are examples of language generation models that leverage pre-training. We use a well validated linguistic style representation to control the comment generation. We extract affect features for auto-labeling which are used to learn a control input related to word categories. Some researches have also combined vision and language features in pre-trained models for various downstream vision-language"
2021.findings-emnlp.380,P19-1539,1,0.832727,"g inference, each token is generated one by one via beam search with a beam size of two. Implementation of MAGIC. We split the 6,720,536 image-comment pairs of NICE-Setting II data to 6,550,536 image-comment pairs for training, 100,000 image-comment pairs for validation, and 70,000 image-comment pairs for testing. We trained MAGIC 30 epochs with batch size 32 on each GPU using a machine with 4xV100 32G GPUs and the learning rate was 5e − 5. Total training time is about 7 days. BertF1 for comparison). Finally, we tested the diversity of generated comments. We tested Entropy4 and Distinct2 from Qin et al. (2019). As MAGIC is pre-trained on large volume of data, they have higher diversity than ShowAttTell and BUTD. Figure 6 shows some generated comment samples from MAGIC model comparing with BUTD model. More samples of Generated Image Comments by MAGIC on NICE-Setting II are included in Appendix D.2. 5.3 Adapt Pre-training MAGIC Model to Domain-Specific Task Pre-training MAGIC model is also flexible to be adapted to related domain-specific conditional generation tasks. The affect feature can be replaced with emotional or personalized features for other conditional image-text generation tasks. Appendix"
2021.findings-emnlp.380,D19-1514,0,0.0186567,"ang et al., 2019; Liu et al., 2019; Radford et al., 2019). Pre-trained models learn representations using tasks such as predicting words based on their context. GPT-2 and CTRL are examples of language generation models that leverage pre-training. We use a well validated linguistic style representation to control the comment generation. We extract affect features for auto-labeling which are used to learn a control input related to word categories. Some researches have also combined vision and language features in pre-trained models for various downstream vision-language tasks (Lu et al., 2019; Tan and Bansal, 2019; Zhou et al., 2019; Chen et al., 2019; Alberti et al., 2019; Li et al., 2020, 2019). One of the closest pre-trained generation models that compare with our work is the unified vision language pre-training (VLP) model. However, VLP focuses on generating image captions and lacks the ability to generate expressive, 1 stylistic responses. To alleviate this problem, we Users can access code and to download the dataset at our official website: https://nicedataset.github. propose our MAGIC pre-training model to fill this io/. Use of the code and dataset are governed by an End User gap and the propos"
2021.findings-emnlp.380,N15-3006,0,0.0493806,"Missing"
2021.findings-emnlp.380,Q14-1006,0,0.0564822,"AGIC, to simulate human commenting on NICE dataset, which aims to generate targeted comments on a given image weakly supervised by affect features. Experiments show that MAGIC outperforms baseline methods on the NICE-Setting II. 2 Related Work With the recent advances in deep learning, a growing number of researchers are interested in studying vision and language jointly. Vision-language understanding has become one of the key components of conversational agents, such as Cortana (Microsoft, 2014). A great deal of focus has been paid to image captioning (Lin et al., 2014; Sharma et al., 2018a; Young et al., 2014), which typically focuses on literal descriptions of image content. However, in social conversations, people usually engage with others using language with emotions, opinions and subjectivity. For example, image commenting on human-machine interaction system has rich stylistic features. In this paper, we introduce the image comment generation task, where the aim is to build models that produce more engaging comments grounded in visual images. Specifically, we present a pre-training model for this task. There are several pre-trained models that address various tasks across the language and visi"
2021.mtsummit-loresmt.6,W16-4117,0,0.0549587,"Missing"
2021.mtsummit-loresmt.6,2020.acl-main.147,0,0.0289353,"in Inuktitut in addition to data augmentation and pretraining. Using Transformer-based models for translation In recent times, there have been several work that use variations of Transformer (Vaswani et al., 2017) model for the task of machine translation. Chen et al. (2018) combine the power of recurrent neural network and transformer. Dehghani et al. (2019) introduce universal transformers as a generalization of transformers whereas Deng et al. (2018) combine transformer architecture with several other techniques such as BPE, back translation, data selection, model ensembling and reranking. Bugliarello and Okazaki (2020) incorporate syntactic knowledge into transformer model to show improvements on English to German, Turkish and Japanese translation tasks. Currey and Heafield (2019) introduce two methods to incorporate English syntax when translating from English into other Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 59 languages with Transformers. Liu et al. (2020b) introduce mBART, an auto-encoder pretrained on large-scale monolingual corpora and show gains on several languages. Using TPRs T"
2021.mtsummit-loresmt.6,P18-1008,0,0.0190961,"Inuktitut and also explored using morphological segmentation for alignment as well as neural and statistical machine translation. This work was followed up by Knowles et al. (2020) who introduce additional methods techniques on the Inuktitut dataset. Roest et al. (2020) and Scherrer et al. (2020) also investigated morphological segmentation in Inuktitut in addition to data augmentation and pretraining. Using Transformer-based models for translation In recent times, there have been several work that use variations of Transformer (Vaswani et al., 2017) model for the task of machine translation. Chen et al. (2018) combine the power of recurrent neural network and transformer. Dehghani et al. (2019) introduce universal transformers as a generalization of transformers whereas Deng et al. (2018) combine transformer architecture with several other techniques such as BPE, back translation, data selection, model ensembling and reranking. Bugliarello and Okazaki (2020) incorporate syntactic knowledge into transformer model to show improvements on English to German, Turkish and Japanese translation tasks. Currey and Heafield (2019) introduce two methods to incorporate English syntax when translating from Engli"
2021.mtsummit-loresmt.6,W19-5203,0,0.022571,"ations of Transformer (Vaswani et al., 2017) model for the task of machine translation. Chen et al. (2018) combine the power of recurrent neural network and transformer. Dehghani et al. (2019) introduce universal transformers as a generalization of transformers whereas Deng et al. (2018) combine transformer architecture with several other techniques such as BPE, back translation, data selection, model ensembling and reranking. Bugliarello and Okazaki (2020) incorporate syntactic knowledge into transformer model to show improvements on English to German, Turkish and Japanese translation tasks. Currey and Heafield (2019) introduce two methods to incorporate English syntax when translating from English into other Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 59 languages with Transformers. Liu et al. (2020b) introduce mBART, an auto-encoder pretrained on large-scale monolingual corpora and show gains on several languages. Using TPRs TPRs have gained traction recently with the interest in neurosymbolic computation to achieve out-of-domain generalization. They have been used in a variety of domains"
2021.mtsummit-loresmt.6,I17-1015,0,0.0236615,"e Transformers more sample-efficient, enabling them to perform better from smaller amounts of data. 1 Introduction The task of machine translation has seen major progress in recent times with the advent of large-scale Transformer-based models (e.g., Vaswani et al., 2017; Dehghani et al., 2019; Liu et al., 2020a). However, there has been less progress on language pairs that specifically involve morphologically rich languages. Moreover, although there has been previous work that builds linguistic structure into translation models to deal with morphological complexity (Sennrich and Haddow, 2016; Dalvi et al., 2017; Matthews et al., 2018), to the best to our knowledge there has not been work that applies such strategies to large-scale Transformer-based models. We hypothesize that providing Transformers access to structured linguistic representations can significantly boost their performance on translation into languages with complex morphology that encodes linguistic structure. In this work, we investigate two methods for introducing such structural bias into Transformer-based models. In the first method, we use the TP-Transformer (TPT) (Schlag et al., 2019), in which a traditional Transformer is augmen"
2021.mtsummit-loresmt.6,P16-1162,0,0.0728165,"Missing"
2021.mtsummit-loresmt.6,N18-2074,0,0.0303896,"nsard 5 Training 28,694,211 4,000,000 1,300,000 207,678 1,312,791 Validation 3,586,776 500,000 65,000 3,007 5,494 Test 3,586,777 500,000 65,000 3,000 6,181 Experimental Results We aim to answer the following research questions (RQ) through our experimentation: 1. Do either or both of our structural methods improve translation? 2. If so, how does that advantage interact with: (a) Training data quantity? (b) Transfer learning? (c) Morphological richness of language? As a baseline, we trained the standard Transformer model (Vaswani et al., 2017) with the addition of relative position embeddings (Shaw et al., 2018). Model training details and computing resources can be found in Section 1 and 2 of the supplementary materials. For each model, we used either byte pair encoding (BPE) (Sennrich et al., 2016) or morphological tokenization as described in §3. In order to see how our changes relate to sample efficiency, we Proceedings of the 18th Biennial Machine Translation Summit, Virtual USA, August 16 - 20, 2021 4th Workshop on Technologies for MT of Low Resource Languages Page 55 vary the size of the subset of the Open Subtitles dataset used for training. We used the SETimes dataset to finetune these model"
2021.mtsummit-loresmt.6,2020.blackboxnlp-1.23,1,0.736321,"and show gains on several languages. Using TPRs TPRs have gained traction recently with the interest in neurosymbolic computation to achieve out-of-domain generalization. They have been used in a variety of domains, including mathematical problem solving (Schlag et al., 2019), reasoning (Schlag and Schmidhuber, 2018), image captioning (Huang et al., 2018), question-answering (Palangi et al., 2018), and program synthesis (Chen et al., 2020). A separate line of work uses TPRs as an interpretation tool to understand representations in networks that do not explicitly use TPRs (McCoy et al., 2019; Soulos et al., 2020). 8 Conclusion We investigated two methods for improving translation into morphologically rich languages with Transformers. The TP-Transformer adds an additional component to Transformer attention to represent relational structure. This model had the largest improvement on smaller datasets and modest improvement on larger datasets. We also investigated morphological tokenization which had substantial improvements on small datasets and transfer learning. When used together, our methods improve on the state of the art for translation from English into Inuktitut by 8 BLEU. The models were analyze"
2021.mtsummit-loresmt.6,A97-1047,0,0.11972,"pora (Supplementary materials section 4). Higher values of this measure correspond to more regular structure/information in words, and thus, greater morphological complexity. We computed the measure over the first 100,000 characters of the test set of each dataset. We computed CD as 1.89 for the Hansard dataset, while the Turkish datasets ranged from CD 1.45-1.49. This corresponds to the relatively large increase in BLEU seen for Inuktitut. 7 Related Work Translating into Morphologically-rich languages Previous work has leveraged morphology for translating into morphologically-rich languages. Turhan (1997) uses a recursive symbolic system to translate from English into Turkish including a morphological generator. Ataman et al. (2020) use hierarchical latent variable models to model both character and morpheme level statistics for translating into morphologically rich languages (Arabic, Czech, Turkish) with GRUs. Passban et al. (2018a) introduce a character-level neural machine translation model for translating into morphologically rich languages which incorporates a morphology lookup table into the decoder whereas Passban et al. (2018b) propose a subword-level model that uses separate embedding"
2021.naacl-main.381,W18-2501,0,0.0190822,"Missing"
2021.naacl-main.381,N18-1065,0,0.0237488,"Missing"
2021.naacl-main.381,N19-1419,0,0.0638963,"Missing"
2021.naacl-main.381,N18-1114,1,0.83673,"We also believe there is a connection between the above two effects, as the structural, syntactic information favors a lower-dimensional or even discrete space while the distributed, semantic information favors a higher-dimensional space. 5 Related Work Explicit TPR Structures in Neural Networks While earlier TPR work based on (Smolensky, 1990) focused on computability rather than learnability questions, recently TPRs have been incorporated into several recurrent deep learning models in order to solve various NLP tasks including Part-of-Speech tagging, constituency parsing, image captioning (Huang et al., 2018, 2019), question answering (Palangi et al., 2018; Schlag and Schmidhuber, 2018), and natural-to-formal language generation (program synthesis) (Chen et al., 2020). Most recently, TPRs have been introduced into Transformer architectures, starting with Schlag et al. (2019) which introduced the TP-T RANSFORMER to improve the performance and interpretability of mathematical problem solving models. This model generated continuous role vectors by directly projecting from layer inputs, whereas our model indexes from a dictionary of role embeddings to form the role vectors which are shown to reside i"
2021.naacl-main.381,D18-1206,0,0.316024,"and (3) demonstrate the emergent structures in representations by revealing the disentangled syntactic and semantic information encoded in the role and filler spaces. To test the ability of our TP-T RANSFORMER with discrete roles against the standard Transformer and the TP-T RANSFORMER with continuous roles, we build several models from scratch on a number of summarization datasets spanning different degrees of abstractiveness, output summary lengths, and domains. Our TP-T RANSFORMER significantly outperforms the standard Transformer and the TP-T RANSFORMER with continuous roles on the XSum (Narayan et al., 2018), Wikihow (Koupaee and Wang, 2018), and Arxiv (Cohan et al., 2018) datasets and achieves competitive performance on the CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset, mea- 2 The TP-T RANSFORMER sured by automatic metrics including ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). We build our TP-T RANSFORMER based on the Our human evaluations on XSum and Wikihow Transformer architecture used in Raffel et al. datasets also correlate with the automatic metrics, (2020). A TP-T RANSFORMER encoder applied demonstrating that summaries generated by our TP- to a sequenc"
2021.naacl-main.381,P19-1209,0,0.036398,"Missing"
2021.naacl-main.381,C18-1101,0,0.0419582,"Missing"
2021.naacl-main.381,W04-1013,0,0.0317552,"RMER with continuous roles, we build several models from scratch on a number of summarization datasets spanning different degrees of abstractiveness, output summary lengths, and domains. Our TP-T RANSFORMER significantly outperforms the standard Transformer and the TP-T RANSFORMER with continuous roles on the XSum (Narayan et al., 2018), Wikihow (Koupaee and Wang, 2018), and Arxiv (Cohan et al., 2018) datasets and achieves competitive performance on the CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset, mea- 2 The TP-T RANSFORMER sured by automatic metrics including ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). We build our TP-T RANSFORMER based on the Our human evaluations on XSum and Wikihow Transformer architecture used in Raffel et al. datasets also correlate with the automatic metrics, (2020). A TP-T RANSFORMER encoder applied demonstrating that summaries generated by our TP- to a sequence of tokens i = 1, ..., I can be seen as T RANSFORMER are indeed better than the Trans- a 2-dimensional lattice of cells (i, l) where i is the 4781 2.2 Linear TPR Concat Concat Multi-Head Attention Fillers (F) Roles (R) Scaled Dot-Product Attention Role-Attention Linear L"
2021.naacl-main.381,W19-4825,0,0.0481743,"Missing"
2021.naacl-main.381,Q18-1005,0,0.021196,"lag and Schmidhuber, 2018), and natural-to-formal language generation (program synthesis) (Chen et al., 2020). Most recently, TPRs have been introduced into Transformer architectures, starting with Schlag et al. (2019) which introduced the TP-T RANSFORMER to improve the performance and interpretability of mathematical problem solving models. This model generated continuous role vectors by directly projecting from layer inputs, whereas our model indexes from a dictionary of role embeddings to form the role vectors which are shown to reside in a highly discrete space. only the semantic content (Liu and Lapata, 2018, 2019). To preserve salient source relations and generate abstractive summaries of the source document, previous work infused models with semantic parsers: while Song et al. (2018) introduces a new structure-infused copy mechanism that combines the source syntactic structure with the copy mechanism, Liao et al. (2018) uses abstract meaning representations (AMR). While these approaches require that the document sentence semantic parsers are provided beforehand, our models can implicitly learn to approximate the syntactic structure and semantic content in their representations. 6 Conclusion In"
2021.naacl-main.381,P19-1500,0,0.033989,"Missing"
2021.naacl-main.381,P14-5010,0,0.00451574,"Missing"
2021.naacl-main.381,2020.acl-main.173,0,0.0184431,"erated by the TP-T RANSFORMER are significantly better in grammar. This corroborates our claim that having the TPR can improve the model’s ability to follow the correct syntax in composing the summary. On the Wikihow dataset, the Transformer receives more votes in regarding the saliency. However, our TP-T RANSFORMER maintains an advantage in grammar and achieves significantly better overall preferences. Unfaithful XSum Examples It is well-known that the XSum dataset contains a portion of unfaithful reference summaries that mention facts not included in the source article (Durmus et al., 2020; Maynez et al., 2020). Therefore, we are interested to find out whether our TP-T RANSFORMER is better than the baseline only at expressing the faithful content or it can also generate some external, “unfaithful"" facts that the baseline can’t cover. To answer this question, we randomly sample 100 examples from the XSum dev set and manually examine the source document, reference summary, and the two generated summaries. Among these 100 examples, we identify 71 examples whose reference summary includes “unfaithful"" facts that are not mentioned in the source. In 21 out of 71 examples, the Transformer baseline manages"
2021.naacl-main.381,K16-1028,0,0.203201,"ability of our TP-T RANSFORMER with discrete roles against the standard Transformer and the TP-T RANSFORMER with continuous roles, we build several models from scratch on a number of summarization datasets spanning different degrees of abstractiveness, output summary lengths, and domains. Our TP-T RANSFORMER significantly outperforms the standard Transformer and the TP-T RANSFORMER with continuous roles on the XSum (Narayan et al., 2018), Wikihow (Koupaee and Wang, 2018), and Arxiv (Cohan et al., 2018) datasets and achieves competitive performance on the CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset, mea- 2 The TP-T RANSFORMER sured by automatic metrics including ROUGE (Lin, 2004) and METEOR (Denkowski and Lavie, 2014). We build our TP-T RANSFORMER based on the Our human evaluations on XSum and Wikihow Transformer architecture used in Raffel et al. datasets also correlate with the automatic metrics, (2020). A TP-T RANSFORMER encoder applied demonstrating that summaries generated by our TP- to a sequence of tokens i = 1, ..., I can be seen as T RANSFORMER are indeed better than the Trans- a 2-dimensional lattice of cells (i, l) where i is the 4781 2.2 Linear TPR Concat Concat Mult"
2021.naacl-main.381,N18-2102,1,0.854901,"r summarizing any type of company internal datasets (e.g., internal documents, reports, meetings, legal forms, etc.) to further improve the productivity and efficiency of the users in their daily activities without needing to read long documents. Structured Representations for Abstractive Summarization Compared to the extractive methods, abstractive summarization models usually fail to show extractive properties, and have ten- Failure mode. Even though our models yield facdency to copy text from the source (See et al., 2017; tually consistent summaries, as judged by human Paulus et al., 2018; Pasunuru and Bansal, 2018; Ce- evaluation, they can still generate factually inconlikyilmaz et al., 2018). More recent approaches sistent summaries or sometimes hallucinate inforthat use standard transformers deal with this issue mation that the source document does not include. by introducing hierarchical structures to encode lo- This might be due to the bias or noise in the traincal and global information separately focusing on ing data. Model builders wanting to use our archi4788 tecture to build models on their company internal datasets should build models with consideration of intellectual properties and privacy"
2021.naacl-main.381,P17-1099,0,0.305661,"at least 0.3 advantage are bolded. Arxiv (Cohan et al., 2018) is a long document summarization dataset of scientific publications from arXiv.org (113k). The task is to generate the abstract from the paper body. (2019), while a sentence in an XSum or Wikihow summary usually aggregates information from multiple source sentences. 3.2 CNN/Daily Mail (Hermann et al., 2015; Nallapati et al., 2016) dataset contains 93k articles from CNN and 220k articles from the Daily Mail. Every article is accompanied by a few human-written bullet points about its content. We use the nonanonymized version used in See et al. (2017). Experimental Setup The Transformer and the two TP-T RANSFORMERS all have 6 layers, 8 heads per layer, dimension per head dk =64, model dimension dm =512, and feedforward dimension df =2048 for the encoder and decoder. Our TP-T RANSFORMER with discrete roles has Nr =50 role embeddings of dimension dr =64 at every layer. For each dataset above, we train the all three models from scratch using an Adafactor Optimizer (Shazeer and Stern, 2018) with square root learning rate decay and dropout rate of 0.1. We evaluate the models using automatic metrics including ROUGE F1 score and METEOR. Dataset A"
2021.naacl-main.381,C18-1146,0,0.0207274,"starting with Schlag et al. (2019) which introduced the TP-T RANSFORMER to improve the performance and interpretability of mathematical problem solving models. This model generated continuous role vectors by directly projecting from layer inputs, whereas our model indexes from a dictionary of role embeddings to form the role vectors which are shown to reside in a highly discrete space. only the semantic content (Liu and Lapata, 2018, 2019). To preserve salient source relations and generate abstractive summaries of the source document, previous work infused models with semantic parsers: while Song et al. (2018) introduces a new structure-infused copy mechanism that combines the source syntactic structure with the copy mechanism, Liao et al. (2018) uses abstract meaning representations (AMR). While these approaches require that the document sentence semantic parsers are provided beforehand, our models can implicitly learn to approximate the syntactic structure and semantic content in their representations. 6 Conclusion In this work, we enrich the Transformer model with the structured Tensor Product Representation for abstractive summarization tasks. We represent every token as a pair of role and fill"
2021.naacl-main.381,P19-1452,0,0.137421,"ely one-hot, thus restricting the role vectors to a highly discrete space. This structural inductive bias encourages the TP-T RANSFORMER to encode the syntactic information in the discrete roles while isolating the semantics in the continuous fillers. former’s generations. Furthermore, to investigate the structural representation that naturally emerges during training and the advantage of having compositional TPR hidden states, we design a suite of decoder probing tasks to explore the information encoded in the role, filler, and TPR space. We adopt the encoder probing task design presented in Tenney et al. (2019b) and create four decoder probing tasks: Part-of-speech tagging (POS), Dependency Labeling (DEP), Semantic Role Labeling (SRL), and Named Entity Labeling (NEL). Our findings collectively show that the decoder’s role vectors encode a wealth of syntactic structures, aiding the decoder in deducing the syntactic features (e.g., being a proper noun, being the object of the root predicate) of the next token to be generated. The decoder’s filler vectors on the other hand encode more semantic information (e.g., being a person’s name). Furthermore, we observe that having the compositional TPR results"
2021.naacl-main.414,P17-1171,0,0.0244406,"as it involves making nuet al. (2019) control higher level attributes of text, anced changes to text according to natural lansuch as style, tone, or topic. Our task instead guage commands. We also believe this task has 5266 uses natural language commands, which can flexibly express different types of constraints, ranging from low-level lexical ones, to high-level topical ones. In this sense, we can also draw the parallel to dialog response generation (Ghazvininejad et al., 2018; Dinan et al., 2018), task-oriented dialog (Gao et al., 2018), or open domain question answering (Min et al., 2019; Chen et al., 2017), that also involve user responses or queries, although these tasks are not concerned with text generation in the context of document creation. senting them. Related to Wikipedia data, Pryzant et al. (2020) also used Wikipedia revision histories to learn to debias text, whereas we considered general edits. Iso et al. (2020) propose a factbased text editing task, but they do not consider control or other types of edits. Another related task to text editing is text paraphrasing (Gupta et al., 2018), however paraphrasing usually conserves the meaning of a sentence. While the edits we consider inc"
2021.naacl-main.414,P18-1082,0,0.163969,"the office. add years in office Barack Obama was the 44th President of the United States from 2009 to 2017 and the first African-American to hold the office. Figure 1: An illustration of our interactive text generation setting. This is an example generated by our model. The blue panels represent the text being edited, taken from the document shown on the right. The orange panels represent user edit commands. The model grounds edits in query results from a commercial search engine. A long-standing goal of natural language processing research has been to generate long-form text (Lebowitz, 1985; Fan et al., 2018; Rashkin et al., 2020). Recent large generative language models such as GPT-2 (Radford et al., 2019), and GPT3 (Brown et al., 2020), demonstrate an impressive ability to generate fluent text, but their outputs are difficult to control beyond a prompt, and they manifest a tendency to hallucinate facts (Wiseman et al., 2017). Much recent work has thus focused on making such models more controllable (Keskar et al., 2019; Hu et al., 2017; Zhang et al., 2020; Dathathri et al., 2019), and factually grounded (Guu et al., 2020; Liu et al., 2018b). Most such work only considers a one-shot generation s"
2021.naacl-main.414,D18-1028,0,0.0813115,"of our full model are broken down by edit intention labels in Table 6. The columns report the same metrics as Ablations The middle rows of Table 5 show the in our main table of results, with the exception of results for three ablations of our model. The first S-BLEU, which reports the BLEU score between ablation removes everything but the source senthe source sentence and target, and the last coltence s. This is similar to the paraphrase setumn, which reports the number of test edits that ting (Gupta et al., 2018), and the editing setting were classified into each category. With the caveat in Faruqui et al. (2018) and Yin et al. (2018). that intention labels come from an automatic clasWe can see that including the context, grounding, sifier and not human annotation, we can observe and command as additional inputs yields signifithat our model has varying performance across cant improvements over only using the source sendifferent types of edits. The model performs very tence. We can also see from the second ablation well on fluency edits, but worse on content edits. that the commands are a crucial element in the This comes at no surprise given that fluency edmodel’s performance. This is not surprising s"
2021.naacl-main.414,P18-5002,1,0.889002,"Missing"
2021.naacl-main.414,Q18-1031,0,0.0603054,"h pretrained language model weights, yields encouraging results on both automatic and human evaluations. Additionally, our ablation studies showed the crucial role played by the user command and grounding. Breaking down our results by types of edits, we saw that our model not only performs well on easier fluency edits, but also on much harder content edits. Finally, we discussed future research directions for interactive document generation, as well as possible extensions to other domains such as images or code. Acknowledgments Text Editing Several previous works have focused on text editing. Guu et al. (2018) generate The authors would like to thank Thomas Hofsentences by editing prototypes taken from their mann, as well as Sudha Rao, Matt Richardtraining corpus, although they use editing only as a son, Zhang Li, Kosh Narayanan, and Chandra means for language modeling. Wu et al. (2019) exChikkareddy for their helpful suggestions. pand upon Guu et al. (2018)’s setting, but for dialog. More related to our own setting, Faruqui et al. (2018) propose WikiAtomicEdits, a dataset of edReferences its crawled from Wikipedia. However, they consider a much narrower definition of edits than our Giusepppe Attar"
2021.naacl-main.414,P17-1141,0,0.0461026,"Missing"
2021.naacl-main.414,W19-2405,0,0.0163809,"n generating long-form narratives (Jain et al., 2017). While earlier work in Story Generation focused more on plan-based architectures (Lebowitz, 1985), more recent work moved towards end-to-end approaches (Fan et al., 2018) allowing generation to be unconstrained and creative. As narratives are often aimed at particular goals expressed in terms of outlines and plans, much of the literature in Story Generation is framed as a form of controllable generation, using storylines (Peng et al., 2018), events (Martin et al., 2017; Harrison et al., 2017), plot words or word skeletons (Xu et al., 2018; Ippolito et al., 2019), plans (Yao et al., 2019), story ending (Tambwekar et al., 2019), and outlines (Rashkin et al., 2020) as various forms of constraints. Our work takes a significantly different approach, as we treat document or story generation as an iterative process that allows a human to generate a full document from scratch, but also allows constraints to be more dynamic (e.g., add nationality in Table 9 only if the system missed that the first time). 8 Conclusion In this work we argued that text generation should be interactive, and, as a means towards that end, we proposed a general text editing task, wh"
2021.naacl-main.414,2020.acl-main.17,0,0.0328833,"om low-level lexical ones, to high-level topical ones. In this sense, we can also draw the parallel to dialog response generation (Ghazvininejad et al., 2018; Dinan et al., 2018), task-oriented dialog (Gao et al., 2018), or open domain question answering (Min et al., 2019; Chen et al., 2017), that also involve user responses or queries, although these tasks are not concerned with text generation in the context of document creation. senting them. Related to Wikipedia data, Pryzant et al. (2020) also used Wikipedia revision histories to learn to debias text, whereas we considered general edits. Iso et al. (2020) propose a factbased text editing task, but they do not consider control or other types of edits. Another related task to text editing is text paraphrasing (Gupta et al., 2018), however paraphrasing usually conserves the meaning of a sentence. While the edits we consider include meaning-preserving edits, we are mostly interested in edits that affect meaning. Story Generation The task of Document Generation considered in our work bears similarity with work on generating long-form narratives (Jain et al., 2017). While earlier work in Story Generation focused more on plan-based architectures (Leb"
2021.naacl-main.414,J06-4003,0,0.0584681,"Missing"
2021.naacl-main.414,N19-1238,0,0.0634756,"Missing"
2021.naacl-main.424,2020.acl-demos.16,1,0.894533,"Missing"
2021.naacl-main.424,W18-5446,0,0.0506617,"Missing"
2021.naacl-main.424,P19-1334,0,0.0569524,"Missing"
2021.naacl-main.424,N18-1101,0,0.113283,"Missing"
2021.naacl-main.424,2020.repl4nlp-1.8,1,0.887899,"training steps to where the model errs the most. Experiments show that TAT can significantly improve accuracy over standard adversarial training on GLUE and attain new state-of-the-art zero-shot results on XNLI. Our code will be released at: https://github. com/namisan/mt-dnn. 1 (a) BERT with standard fine-tuning Introduction Adversarial training has proven effective in improving model generalization and robustness in computer vision (Madry et al., 2017; Goodfellow et al., 2014) and natural language processing (NLP) (Zhu et al., 2019; Jiang et al., 2019; Cheng et al., 2019; Liu et al., 2020a; Pereira et al., 2020; Cheng et al., 2020). It works by augmenting the input with a small perturbation to steer the current model prediction away from the correct label, thus forcing subsequent training to make the model more robust and generalizable. Aside from some prior work in computer vision (Dong et al., 2018; Tramèr et al., 2017), most adversarial training approaches adopt non-targeted attacks, where the model prediction is not driven towards a specific incorrect label. In NLP, the cutting-edge research in adversarial training tends to focus on making adversarial training less expensive (e.g., by reusing ba"
2021.naacl-main.424,D16-1264,0,0.12567,"Missing"
2021.naacl-main.424,D18-1187,0,0.0412606,"Missing"
2021.naacl-main.424,D13-1170,0,0.0221058,"Missing"
2021.naacl-main.85,P07-1033,0,0.358043,"Missing"
2021.naacl-main.85,N19-1423,0,0.725941,"ce the model robustness. Here, we first provide a theoretical connection of two recent methods under this framework, i.e. Virtual Adversarial Training (VAT) (Miyato et al., 2018) and Jacobian Regularization (JR) (Sokoli´c et al., 2017). In addition, we propose to generalize VAT and random perturbation training (RPT) (Miyato et al., 2018) with a family of probability distribution metrics, f -divergences, and characterize their connection with JR. Given that large-scale pretrained neural language models have demonstrated their superior generalization for downstream NLP tasks under both matched (Devlin et al., 2019; Liu et al., 2019) and mismatched evaluations (Hendrycks et al., 2020), we systematically study the regularization framework using BERT (Devlin et al., 2019) on a diverse set of tasks in terms of both in-domain and out-ofdomain generalization. Specifically, we use representative datasets (Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016, 2018) from sentiment analysis, textual entailment and question answering (QA) for in-domain training and evaluation. In order to assess the resulting model generalization over domain shift and adversarial attack, we then consider out-of-doma"
2021.naacl-main.85,D19-5801,0,0.0634066,"rt and end position probabilities, respectively. 4 Experiments tasks. Following the literature, we report the exact match (EM) and F1 scores for QA datasets and classification accuracy for textual entailment and sentiment analysis. For model training, we use MNLI (Williams et al., 2018) and SST-2 (Socher et al., 2013) and SQuAD v1.1/v2.0 (Rajpurkar et al., 2016, 2018), respectively. The corresponding development set is used for evaluating the indomain generalization. To evaluate the out-of-domain generalization with domain shift, we use the BioAQS dataset (Tsatsaronis et al., 2015) from MRQA (Fisch et al., 2019) and the IMDB dataset (Maas et al., 2011). Unlike SQuAD which is based on Wikipedia, BioAQS is a biomedical QA dataset constructed on PubMed articles. Compared with SST-2 containing pithy export reviews (Socher et al., 2013), IMDB includes lengthy movie reviews from nonexperts (Maas et al., 2011). We directly apply the QA model trained on SQuAD v2.0 and the sentiment classifier trained on SSS-2 to BioAQS and IMDB, respectively. To evaluate the model robustness towards adversarial attack, we use two challenging adversarial datasets, i.e. Adversarial SQuAD (Jia and Liang, 2017) and HANS (McCoy e"
2021.naacl-main.85,N18-2017,0,0.0421731,"Missing"
2021.naacl-main.85,D19-6115,0,0.0287828,"Missing"
2021.naacl-main.85,2020.acl-main.244,0,0.02355,"Missing"
2021.naacl-main.85,P19-1147,0,0.0291196,"Missing"
2021.naacl-main.85,D17-1215,0,0.399946,"f domain adaptation. Various methods (Blitzer neural language models (Devlin et al., 2019; Liu et al., 2007; Daumé III, 2007) have been develet al., 2019) have better generalization, but experi- oped for training models to learn effectively from ence performance reduction caused by domain shift both in-domain (source) and out-of-domain (tar(Hendrycks et al., 2020). Textual entailment mod- get) datasets. Additionally, the recent discovery on els trained on MNLI (Williams et al., 2018) have the prevalence of data biases (Gururangan et al., picked up superficial cues focusing on either the 2018; Jia and Liang, 2017; McCoy et al., 2019), 1 unintended correlations between input and output Code is available at https://github.com/ hao-cheng/f-divergence. learned by statistical models, ignites the develop1078 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1078–1089 June 6–11, 2021. ©2021 Association for Computational Linguistics ment of model debiasing techniques (Clark et al., 2019; He et al., 2019). These methods leverage discovered data biases to improve model generalization over adversarial datasets (Ji"
2021.naacl-main.85,2020.acl-main.197,1,0.909739,"Missing"
2021.naacl-main.85,D16-1207,0,0.0464418,"Missing"
2021.naacl-main.85,D16-1264,0,0.0762475,"of probability distribution metrics, f -divergences, and characterize their connection with JR. Given that large-scale pretrained neural language models have demonstrated their superior generalization for downstream NLP tasks under both matched (Devlin et al., 2019; Liu et al., 2019) and mismatched evaluations (Hendrycks et al., 2020), we systematically study the regularization framework using BERT (Devlin et al., 2019) on a diverse set of tasks in terms of both in-domain and out-ofdomain generalization. Specifically, we use representative datasets (Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016, 2018) from sentiment analysis, textual entailment and question answering (QA) for in-domain training and evaluation. In order to assess the resulting model generalization over domain shift and adversarial attack, we then consider out-of-domain datasets (Tsatsaronis et al., 2015; Maas et al., 2011) and challenge adversarial datasets (Jia and Liang, 2017; McCoy et al., 2019) in a zero-shot learning fashion. to consistently better empirical robustness over BERT with the standard fine-tuning. Furthermore, we find that different f -divergences lead to different generalization behaviors for in-dom"
2021.naacl-main.85,D13-1170,0,0.0388175,"g (RPT) (Miyato et al., 2018) with a family of probability distribution metrics, f -divergences, and characterize their connection with JR. Given that large-scale pretrained neural language models have demonstrated their superior generalization for downstream NLP tasks under both matched (Devlin et al., 2019; Liu et al., 2019) and mismatched evaluations (Hendrycks et al., 2020), we systematically study the regularization framework using BERT (Devlin et al., 2019) on a diverse set of tasks in terms of both in-domain and out-ofdomain generalization. Specifically, we use representative datasets (Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016, 2018) from sentiment analysis, textual entailment and question answering (QA) for in-domain training and evaluation. In order to assess the resulting model generalization over domain shift and adversarial attack, we then consider out-of-domain datasets (Tsatsaronis et al., 2015; Maas et al., 2011) and challenge adversarial datasets (Jia and Liang, 2017; McCoy et al., 2019) in a zero-shot learning fashion. to consistently better empirical robustness over BERT with the standard fine-tuning. Furthermore, we find that different f -divergences lead t"
2021.naacl-main.85,2021.ccl-1.108,0,0.0769547,"Missing"
2021.naacl-main.85,P11-1015,0,0.0421506,"uations (Hendrycks et al., 2020), we systematically study the regularization framework using BERT (Devlin et al., 2019) on a diverse set of tasks in terms of both in-domain and out-ofdomain generalization. Specifically, we use representative datasets (Socher et al., 2013; Williams et al., 2018; Rajpurkar et al., 2016, 2018) from sentiment analysis, textual entailment and question answering (QA) for in-domain training and evaluation. In order to assess the resulting model generalization over domain shift and adversarial attack, we then consider out-of-domain datasets (Tsatsaronis et al., 2015; Maas et al., 2011) and challenge adversarial datasets (Jia and Liang, 2017; McCoy et al., 2019) in a zero-shot learning fashion. to consistently better empirical robustness over BERT with the standard fine-tuning. Furthermore, we find that different f -divergences lead to different generalization behaviors for in-domain, domain shift and adversarial settings. In our study, VAT with symmetric divergence achieve better generalization for in-domain and domain shift cases, while VAT with asymmetric divergence achieve more robustness toward adversarial attack. More importantly, we show that a BERT-base model trained"
2021.naacl-main.85,P19-1334,0,0.191525,"Various methods (Blitzer neural language models (Devlin et al., 2019; Liu et al., 2007; Daumé III, 2007) have been develet al., 2019) have better generalization, but experi- oped for training models to learn effectively from ence performance reduction caused by domain shift both in-domain (source) and out-of-domain (tar(Hendrycks et al., 2020). Textual entailment mod- get) datasets. Additionally, the recent discovery on els trained on MNLI (Williams et al., 2018) have the prevalence of data biases (Gururangan et al., picked up superficial cues focusing on either the 2018; Jia and Liang, 2017; McCoy et al., 2019), 1 unintended correlations between input and output Code is available at https://github.com/ hao-cheng/f-divergence. learned by statistical models, ignites the develop1078 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1078–1089 June 6–11, 2021. ©2021 Association for Computational Linguistics ment of model debiasing techniques (Clark et al., 2019; He et al., 2019). These methods leverage discovered data biases to improve model generalization over adversarial datasets (Jia and Liang, 2017; Mc"
2021.naacl-main.85,2020.repl4nlp-1.8,1,0.883027,"Missing"
2021.naacl-main.85,P18-2124,0,0.0559141,"Missing"
2021.naacl-main.85,N18-1101,0,0.431737,"cenarios (Nie et al., 2019; Hsieh with domain shift has been a long-standing goal et al., 2019). For example, large-scale pretrained of domain adaptation. Various methods (Blitzer neural language models (Devlin et al., 2019; Liu et al., 2007; Daumé III, 2007) have been develet al., 2019) have better generalization, but experi- oped for training models to learn effectively from ence performance reduction caused by domain shift both in-domain (source) and out-of-domain (tar(Hendrycks et al., 2020). Textual entailment mod- get) datasets. Additionally, the recent discovery on els trained on MNLI (Williams et al., 2018) have the prevalence of data biases (Gururangan et al., picked up superficial cues focusing on either the 2018; Jia and Liang, 2017; McCoy et al., 2019), 1 unintended correlations between input and output Code is available at https://github.com/ hao-cheng/f-divergence. learned by statistical models, ignites the develop1078 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 1078–1089 June 6–11, 2021. ©2021 Association for Computational Linguistics ment of model debiasing techniques (Clark et al.,"
C02-1012,W98-1120,0,\N,Missing
C02-1012,M98-1004,0,\N,Missing
C02-1012,M98-1012,0,\N,Missing
C02-1012,M98-1014,0,\N,Missing
C02-1012,M98-1021,0,\N,Missing
C02-1012,J92-4003,0,\N,Missing
C02-1012,M98-1017,0,\N,Missing
C08-1128,W06-1655,0,0.0211706,"e 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Microsoft Corporation∗ One Microsoft Way Redmond, WA 98052, USA {jfgao,kristout}@microsoft.com frequencies are obtained using manually annotated data. This method is sub-optimal for MT. For example, d(paper) and d(card) can be two words or composed into one word dd(cards). Since d ddoes not exist in the manual lexicon, it cannot be generated by this method. In addition to unigram segmentation, other methods have been proposed. For example, (Gao et al., 2005) described an adaptive CWS system, and (Andrew, 2006) employed a conditional random field model for sequence segmentation. However, these methods are not specifically developed for the MT application, and significant improvements in translation performance need to be shown. In (Xu et al., 2004) and (Xu et al., 2005), word segmentations are integrated into MT systems during model training and translation. We refine the method in training using a Bayesian semisupervised CWS approach motivated by (Goldwater et al., 2006). We describe a generative model which consists of a word model and two alignment models, representing the monolingual and bilingu"
C08-1128,J93-2003,0,0.0237847,"ach table with infinite number of seats (approximately corresponding to Chinese word frequencies). The Dirichlet Process model can be viewed intuitively as a cache model (Goldwater et al., 2006). Each word fj in the corpus is either retrieved from a cache or generated anew given the previously observed words f−j : where the maximization is taken over Chinese word sequences whose character sequence is cK 1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993). Our MT system uses a phrase-based decoder and the log-linear model described in (Zens and Ney, 2004). Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The feature weights are tuned on the development set using a downhill simplex algorithm (Press et al., 2002). The language model is a statistical ngram model estimated using modified Kneser-Ney smoothing. 3 Unigram Dirichlet Process Model for CWS The simplest version of our model is based on"
C08-1128,P02-1038,1,0.192261,"ly corresponding to Chinese word types), each table with infinite number of seats (approximately corresponding to Chinese word frequencies). The Dirichlet Process model can be viewed intuitively as a cache model (Goldwater et al., 2006). Each word fj in the corpus is either retrieved from a cache or generated anew given the previously observed words f−j : where the maximization is taken over Chinese word sequences whose character sequence is cK 1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993). Our MT system uses a phrase-based decoder and the log-linear model described in (Zens and Ney, 2004). Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The feature weights are tuned on the development set using a downhill simplex algorithm (Press et al., 2002). The language model is a statistical ngram model estimated using modified Kneser-Ney smoothing. 3 Unigram Dirichlet Process Model for CWS Th"
C08-1128,P02-1040,0,0.110305,"step is done according to the algorithm in Table 1. Using this algorithm, we obtain a new segmentation of the Chinese data and train the translation models using this segmentation as in the baseline MT system. To segment the test data for translation, we use a unigram model, trained with maximum likelihood estimation off of the final segmentation of the training corpus FT . 6 Translation Experiments We performed experiments using our models for CWS on a large and a small data track. We evaluated performance by measuring WER (word error rate), PER (position-independent word error rate), BLEU (Papineni et al., 2002) and TER (translation error rate) (Snover et al., 2006) using multiple references. 6.1 Translation Task: Large Track NIST We first report the experiments using our monolingual unigram Dirichlet Process model for word segmentation on the NIST machine translation task (NIST, 2005). Because of the computational requirements, we only employed the monolingual word model for this large data track, i.e. the feature weights were λ1 = 1, λ2 = 0, λ3 = 0. Therefore, no alignment information needs to be maintained in this case. The bilingual training corpus is a superset of corpora in the news domain coll"
C08-1128,2006.amta-papers.25,0,0.0127207,"g this algorithm, we obtain a new segmentation of the Chinese data and train the translation models using this segmentation as in the baseline MT system. To segment the test data for translation, we use a unigram model, trained with maximum likelihood estimation off of the final segmentation of the training corpus FT . 6 Translation Experiments We performed experiments using our models for CWS on a large and a small data track. We evaluated performance by measuring WER (word error rate), PER (position-independent word error rate), BLEU (Papineni et al., 2002) and TER (translation error rate) (Snover et al., 2006) using multiple references. 6.1 Translation Task: Large Track NIST We first report the experiments using our monolingual unigram Dirichlet Process model for word segmentation on the NIST machine translation task (NIST, 2005). Because of the computational requirements, we only employed the monolingual word model for this large data track, i.e. the feature weights were λ1 = 1, λ2 = 0, λ3 = 0. Therefore, no alignment information needs to be maintained in this case. The bilingual training corpus is a superset of corpora in the news domain collected from different sources. We took LDC (LDC, 2003) a"
C08-1128,C96-2141,1,0.472239,"mber of seats (approximately corresponding to Chinese word frequencies). The Dirichlet Process model can be viewed intuitively as a cache model (Goldwater et al., 2006). Each word fj in the corpus is either retrieved from a cache or generated anew given the previously observed words f−j : where the maximization is taken over Chinese word sequences whose character sequence is cK 1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993). Our MT system uses a phrase-based decoder and the log-linear model described in (Zens and Ney, 2004). Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The feature weights are tuned on the development set using a downhill simplex algorithm (Press et al., 2002). The language model is a statistical ngram model estimated using modified Kneser-Ney smoothing. 3 Unigram Dirichlet Process Model for CWS The simplest version of our model is based on a unigram Dirichlet Proce"
C08-1128,W04-1118,1,0.723099,"ted data. This method is sub-optimal for MT. For example, d(paper) and d(card) can be two words or composed into one word dd(cards). Since d ddoes not exist in the manual lexicon, it cannot be generated by this method. In addition to unigram segmentation, other methods have been proposed. For example, (Gao et al., 2005) described an adaptive CWS system, and (Andrew, 2006) employed a conditional random field model for sequence segmentation. However, these methods are not specifically developed for the MT application, and significant improvements in translation performance need to be shown. In (Xu et al., 2004) and (Xu et al., 2005), word segmentations are integrated into MT systems during model training and translation. We refine the method in training using a Bayesian semisupervised CWS approach motivated by (Goldwater et al., 2006). We describe a generative model which consists of a word model and two alignment models, representing the monolingual and bilingual information, respectively. In our methods, we first segment Chinese text using a unigram segmenter, and then learn new word types and word distributions, which are suitable for MT. Our experiments on both large (NIST) and small (IWSLT) dat"
C08-1128,2005.iwslt-1.18,1,0.694023,"is sub-optimal for MT. For example, d(paper) and d(card) can be two words or composed into one word dd(cards). Since d ddoes not exist in the manual lexicon, it cannot be generated by this method. In addition to unigram segmentation, other methods have been proposed. For example, (Gao et al., 2005) described an adaptive CWS system, and (Andrew, 2006) employed a conditional random field model for sequence segmentation. However, these methods are not specifically developed for the MT application, and significant improvements in translation performance need to be shown. In (Xu et al., 2004) and (Xu et al., 2005), word segmentations are integrated into MT systems during model training and translation. We refine the method in training using a Bayesian semisupervised CWS approach motivated by (Goldwater et al., 2006). We describe a generative model which consists of a word model and two alignment models, representing the monolingual and bilingual information, respectively. In our methods, we first segment Chinese text using a unigram segmenter, and then learn new word types and word distributions, which are suitable for MT. Our experiments on both large (NIST) and small (IWSLT) data tracks of Chinese-to"
C08-1128,N04-1033,1,0.749729,"ache model (Goldwater et al., 2006). Each word fj in the corpus is either retrieved from a cache or generated anew given the previously observed words f−j : where the maximization is taken over Chinese word sequences whose character sequence is cK 1 . 2.2 Translation system Once we have segmented the Chinese sentences into words, we train standard alignment models in both directions with GIZA++ (Och and Ney, 2002) using models of IBM-1 (Brown et al., 1993), HMM (Vogel et al., 1996) and IBM-4 (Brown et al., 1993). Our MT system uses a phrase-based decoder and the log-linear model described in (Zens and Ney, 2004). Features in the log-linear model include translation models in two directions, a language model, a distortion model and a sentence length penalty. The feature weights are tuned on the development set using a downhill simplex algorithm (Press et al., 2002). The language model is a statistical ngram model estimated using modified Kneser-Ney smoothing. 3 Unigram Dirichlet Process Model for CWS The simplest version of our model is based on a unigram Dirichlet Process (DP) model, using only monolingual information. Different from a standard unigram model for CWS, our model can introduce new Chine"
C08-1128,W03-1730,0,0.127363,"Missing"
C08-1128,J05-4005,1,0.845047,"the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some rights reserved. Microsoft Corporation∗ One Microsoft Way Redmond, WA 98052, USA {jfgao,kristout}@microsoft.com frequencies are obtained using manually annotated data. This method is sub-optimal for MT. For example, d(paper) and d(card) can be two words or composed into one word dd(cards). Since d ddoes not exist in the manual lexicon, it cannot be generated by this method. In addition to unigram segmentation, other methods have been proposed. For example, (Gao et al., 2005) described an adaptive CWS system, and (Andrew, 2006) employed a conditional random field model for sequence segmentation. However, these methods are not specifically developed for the MT application, and significant improvements in translation performance need to be shown. In (Xu et al., 2004) and (Xu et al., 2005), word segmentations are integrated into MT systems during model training and translation. We refine the method in training using a Bayesian semisupervised CWS approach motivated by (Goldwater et al., 2006). We describe a generative model which consists of a word model and two align"
C08-1128,P06-1085,0,\N,Missing
C08-1128,P02-1017,0,\N,Missing
C10-1041,D07-1090,0,0.0200831,"ing provides a flexible modeling framework for incorporating a wide variety of features that would be difficult to model under the noisy channel framework. Second, we explore the use of Web scale LMs for query spelling correction. While traditional LM research focuses on how to make the model “smarter” via how to better estimate the probability of unseen words (Chen and Goodman, 1999); and how to model the grammatical structure of language (e.g., Charniak, 2001), recent studies show that significant improvements can be achieved using “stupid” n-gram models trained on very large corpora (e.g., Brants et al., 2007). We adopt the latter strategy in this study. We present a distributed infrastructure to efficiently train and apply Web scale LMs. In addition, we observe that search queries are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366, Beijing, August 2010 multi-term phrase into anothe"
C10-1041,P00-1037,0,0.799464,"are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366, Beijing, August 2010 multi-term phrase into another multi-term phrase. Compared to traditional error models that account for transformation probabilities between single characters or substrings (e.g., Kernighan et al., 1990; Brill and Moore, 2000), the phrase-based error model is more effective in that it captures inter-term dependencies crucial for correcting real-word errors, prevalent in search queries. We also present a novel method of extracting large amounts of query-correction pairs from search logs. These pairs, implicitly judged by millions of users, are used for training the error models. Experiments show that each of the extensions leads to significant improvements over its baseline methods that were state-of-the-art until this work, and that the combined method yields a system which outperforms the noisy channel speller by"
C10-1041,P01-1017,0,0.0257571,"aps the feature vector to a real-valued score, indicating the likelihood that this candidate is a desirable correction. We will demonstrate that ranking provides a flexible modeling framework for incorporating a wide variety of features that would be difficult to model under the noisy channel framework. Second, we explore the use of Web scale LMs for query spelling correction. While traditional LM research focuses on how to make the model “smarter” via how to better estimate the probability of unseen words (Chen and Goodman, 1999); and how to model the grammatical structure of language (e.g., Charniak, 2001), recent studies show that significant improvements can be achieved using “stupid” n-gram models trained on very large corpora (e.g., Brants et al., 2007). We adopt the latter strategy in this study. We present a distributed infrastructure to efficiently train and apply Web scale LMs. In addition, we observe that search queries are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proc"
C10-1041,D07-1019,0,0.517248,"ovements are likely given a larger data set. 7 Conclusions and Future Work This paper explores the use of massive Web corpora and search logs for improving a rankerbased search query speller. We show significant improvements over a noisy channel speller using fine-grained features, Web scale LMs, and a phrase-based error model that captures internword dependencies. There are several techniques we are exploring to make further improvements. First, since a query speller is developed for improving the Web search results, it is natural to use features from search results in ranking, as studied in Chen et al. (2007). The challenge is efficiency. Second, in addition to query reformulation sessions, we are exploring other search logs from which we might extract more pairs for error model training. One promising data source is clickthrough data (e.g., Agichtein et al, 2006; Gao et al., 2009). For instance, we might try to learn a transformation from the title or anchor text of a document to the query that led to a click on that document. Finally, the phrase-based error model is inspired by phrase-based SMT systems. We are introducing more SMT techniques such as alignment and translation rule exaction. In a"
C10-1041,D07-1021,1,0.830034,"odels based on different edit distance functions (e.g., Kucich, 1992; Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002). Brill and Moore’s substring-based error model, considered to be state-of-the-art among these models, acts as the baseline against which we compare our models. On the other hand, real-word spelling correction tries to detect incorrect usages of a valid word based on its context, such as &quot;peace&quot; and &quot;piece&quot; in the context &quot;a _ of cake&quot;. N-gram LMs and naïve Bayes classifiers are commonly used models (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). While almost all of the spellers mentioned above are based on a pre-defined dictionary (either a lexicon against which the edit distance is computed, or a set of real-word confusion pairs), recent research on query spelling correction focuses on exploiting noisy Web corpora and query logs to infer knowledge about spellings and word usag in queries (Cucerzan and Brill 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Whitelaw et al., 2009). Like those spellers designed for regular text, most of these query spelling systems are also based on the noisy channel framework. 359 3 A Ranker-Based Spel"
C10-1041,W04-3238,0,0.95372,"improvements over the stateof-the-art baseline methods. 1 Daniel Micol Microsoft Corporation Xiaolong Li Introduction Search queries present a particular challenge for traditional spelling correction methods. New search queries emerge constantly. As a result, many queries contain valid search terms, such as proper nouns and names, which are not well established in the language. Therefore, recent research has focused on the use of Web corpora and search logs, rather than human-compiled lexicons, to infer knowledge about spellings and word usages in search queries (e.g., Whitelaw et al., 2009; Cucerzan and Brill, 2004). The spelling correction problem is typically formulated under the framework of the noisy channel model. Given an input query , we want to find the best spelling correction among all candidates: (1) Applying Bayes&apos; Rule, we have (2) where the error model models the transformation probability from C to Q, and the language model (LM) models the likelihood that C is a correctly spelled query. This paper extends a noisy channel speller designed for regular text to search queries in three ways: using a ranker (Section 3), using Web scale LMs (Section 4), and using phrase-based error models (Sectio"
C10-1041,O01-2002,1,0.69124,"the n-gram LM collection used in this study, and then present a distributed n-gram LM platform based on which these LMs are built and served for the speller. 4.1 Web Scale Language Models Table 1 summarizes the data sets and Web scale n-gram LMs used in this study. The collection is built from high quality English Web documents containing trillions of tokens, served by a popular commercial search engine. The collection con360 ( ) { where is the count of the n-gram in the training corpus and is a normalization factor. is a discount function for smoothing. We use modified absolute discounting (Gao et al., 2001), whose parameters can be efficiently estimated and performance converges to that of more elaborate state-of-the-art techniques like Kneser-Ney smoothing in large data (Nguyen et al. 2007). 4.2 Distributed N-gram LM Platform The platform is developed on a distributed computing system designed for storing and analyzing massive data sets, running on large clusters consisting of hundreds of commodity servers connected via high-bandwidth network. We use the SCOPE (Structured Computations Optimized for Parallel Execution) programming model (Chaiken et al., 2008) to train the Web scale n-gram LMs sh"
C10-1041,C90-2036,0,0.664032,"erve that search queries are composed in a language style different from that of regular text. We thus train multiple LMs using different texts associated with Web corpora and search queries. Third, we propose a phrase-based error model that captures the probability of transforming one 358 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 358–366, Beijing, August 2010 multi-term phrase into another multi-term phrase. Compared to traditional error models that account for transformation probabilities between single characters or substrings (e.g., Kernighan et al., 1990; Brill and Moore, 2000), the phrase-based error model is more effective in that it captures inter-term dependencies crucial for correcting real-word errors, prevalent in search queries. We also present a novel method of extracting large amounts of query-correction pairs from search logs. These pairs, implicitly judged by millions of users, are used for training the error models. Experiments show that each of the extensions leads to significant improvements over its baseline methods that were state-of-the-art until this work, and that the combined method yields a system which outperforms the n"
C10-1041,N03-1017,0,0.00776907,"Missing"
C10-1041,P06-1129,0,0.399916,"such as &quot;peace&quot; and &quot;piece&quot; in the context &quot;a _ of cake&quot;. N-gram LMs and naïve Bayes classifiers are commonly used models (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). While almost all of the spellers mentioned above are based on a pre-defined dictionary (either a lexicon against which the edit distance is computed, or a set of real-word confusion pairs), recent research on query spelling correction focuses on exploiting noisy Web corpora and query logs to infer knowledge about spellings and word usag in queries (Cucerzan and Brill 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Whitelaw et al., 2009). Like those spellers designed for regular text, most of these query spelling systems are also based on the noisy channel framework. 359 3 A Ranker-Based Speller The noisy channel model of Equation (2) does not have the flexibility to incorporate a wide variety of features useful for spelling correction, e.g., whether a candidate appears as a Wikipedia document title. We thus generalize the speller to a ranker-based system. Let f be a feature vector of a query and candidate correction pair (Q, C). The ranker maps f to a real value y that indicates how likely C is a desi"
C10-1041,J04-4002,0,0.0349005,"t&form=QBRE&qs=n http://www.bing.com/search? q=harry+potter+theme+park&FORM=SSRE Figure 3. A sample of query reformulation sessions from 3 popular search engines. These sessions show that a user first issues the query &quot;harrypotter sheme part&quot;, and then clicks on the resulting spell suggestion &quot;harry potter theme park&quot;. To find the maximum probability assignment efficiently, we use a dynamic programming approach, similar to the monotone decoding algorithm described in Och (2002). 5.2 Training the Error Model Given a set of (Q, C) pairs as training data, we follow a method commonly used in SMT (Och and Ney, 2004) to extract bi- phrases and estimate their replacement probabilities. A detailed description is discussed in Sun et al. (2010). We now describe how (Q, C) pairs are generated automatically from massive query reformulation sessions of a commercial Web browser. A query reformulation session contains a list of URLs that record user behaviors that relate to the query reformulation functions, provided by a Web search engine. For example, most commercial search engines offer the &quot;did you mean&quot; function, suggesting a possible alternate interpretation or spelling of a user-issued query. Figure 3 shows"
C10-1041,P10-1028,1,0.481669,"e n-gram platform provides a DLL for n-gram batch lookup. In the server, an n-gram LM is stored in the form of multiple lists of key-value pairs, where the key is the hash of an n-gram string and the value is either the n-gram probability or backoff parameter. 5 Phrase-Based Error Models The goal of an error model is to transform a correctly spelled query C into a misspelled query Q. Rather than replacing single words in isolation, the phrase-based error model replaces sequences of words with sequences of words, thus incorporating contextual information. The training procedure closely follows Sun et al. (2010). For instance, we might learn that “theme part” can be replaced by “theme park” with relatively high probability, even though “part” is not a misspelled word. We use this generative story: first the correctly spelled query C is broken into K non-empty word sequences c1, …, ck, then each is replaced with a new non-empty word sequence q1, …, qk, finally these phrases are permuted and concatenated to form the misspelled Q. Here, c and q denote consecutive sequences of words. To formalize this generative process, let S denote the segmentation of C into K phrases c1…cK, and let T denote the K repl"
C10-1041,P02-1019,0,0.610347,"Missing"
C10-1041,D09-1093,0,0.550154,"ns leads to significant improvements over the stateof-the-art baseline methods. 1 Daniel Micol Microsoft Corporation Xiaolong Li Introduction Search queries present a particular challenge for traditional spelling correction methods. New search queries emerge constantly. As a result, many queries contain valid search terms, such as proper nouns and names, which are not well established in the language. Therefore, recent research has focused on the use of Web corpora and search logs, rather than human-compiled lexicons, to infer knowledge about spellings and word usages in search queries (e.g., Whitelaw et al., 2009; Cucerzan and Brill, 2004). The spelling correction problem is typically formulated under the framework of the noisy channel model. Given an input query , we want to find the best spelling correction among all candidates: (1) Applying Bayes&apos; Rule, we have (2) where the error model models the transformation probability from C to Q, and the language model (LM) models the likelihood that C is a correctly spelled query. This paper extends a noisy channel speller designed for regular text to search queries in three ways: using a ranker (Section 3), using Web scale LMs (Section 4), and using phrase"
C10-1041,W06-1626,0,0.030213,"Missing"
C10-1041,H05-1120,0,\N,Missing
C10-2016,C04-1080,0,0.0567204,"Missing"
C10-2016,D07-1031,0,0.160017,"lish, we highlight some of the strengths and weaknesses of each of the algorithms in POS tagging task and attempt to explain the differences based on some preliminary linguistics analysis. Comparing to English, we find that all algorithms perform rather poorly in Chinese in 1-to-1 accuracy result but are more competitive in many-to-1 accuracy. We attribute one possible explanation of this to the algorithms’ inability to correctly produce tags that match the desired tag count distribution. 1 Introduction Recently, there has been much work on unsupervised POS tagging using Hidden Markov Models (Johnson, 2007; Goldwater & Griffiths, 2007). Three common approaches are Expectation Maximization (EM), Variational Bayes (VB) and Gibbs Sampling (GS). EM was first used in POS tagging in (Merialdo, 1994) which showed that except in conditions where there are no labeled training data at all, EM performs very poorly. Gao and Johnson (2008) compared EM, VB and GS in English against the Penn Treebank Wall Street Journal (WSJ) text. Their experiments on English showed that GS outperforms EM and VB in almost all cases. Other notable studies in the unsupervised and semi-supervised POS domain include the use of p"
C10-2016,P08-1100,0,0.0318164,"Missing"
C10-2016,A94-1009,0,0.244133,"Missing"
C10-2016,D08-1036,1,0.921682,"dy, we analyze and compare the performance of three classes of unsupervised learning algorithms on Chinese and report the experimental results on the CTB. We establish a baseline for unsupervised POS tagging in Chinese. We then compare and analyze the results between Chinese and English, we explore some of the strengths and weaknesses of each of the algorithms in POS tagging task and attempt to explain the differences based on some preliminary linguistics analysis. 2 Models In this section, we provide a brief overview of the three unsupervised learning methods for POS tagging as described in (Gao & Johnson, 2008), which all uses a traditional bigram Hidden Markov Model (HMM). HMM is a well135 Coling 2010: Poster Volume, pages 135–143, Beijing, August 2010 known statistical model, used for sequential modeling. To put it formally, let be the set of possible states and be the set of possible observations. In the case for POS tagging using a bigram model, the set corresponds to the set of POS tags and the set corresponds to the set of words in the language. (2) and backward probability . See (Mannings & Schutze, 1999) for details on the calculation. 2.1 Figure 1: Graphical model of an HMM for a bigram POS"
C10-2016,P07-1094,0,0.0387751,"Missing"
C10-2016,N06-1041,0,0.0520223,"Missing"
C10-2016,N09-2054,0,0.0280765,"Missing"
C10-2016,J94-2001,0,0.354707,"Missing"
C10-2016,W04-3236,0,0.0515625,"Missing"
C10-2016,A00-1031,0,0.224737,"Missing"
C10-2016,C02-1145,0,0.095044,"Missing"
C10-2016,W93-0305,0,\N,Missing
D07-1021,O01-2002,1,0.915604,"Missing"
D07-1021,P96-1010,0,0.0450962,"many people. Walter Mossberg of the Wall Street Journal called out the contextual speller (the blue squiggles) as one of the most notable features in Office 2007: There are other nice additions. In Word, Outlook and PowerPoint, there is now contextual spell checking, which points to a wrong word, even if the spelling is in the dictionary. For example, if you type “their” instead of “they&apos;re,” Office catches the mistake. It really works. 1 The use of contextual language models in spelling correction has been discussed elsewhere: (Church and Gale, 1991), (Mays et al, 1991), (Kukich, 1992) and (Golding and Schabes, 1996). This paper will focus on how to deploy such methods to millions and millions of users. Depending on the particular application and requirements, we need to make different tradeoffs among: 1. Space (for compressed language model), 2. Runtime (for n-gram lookup), and 3. Accuracy (losses for n-gram estimates). HashTBO optimizes space at the expense of the other two. We recommend HashTBO when space concerns dominate the other concerns; otherwise, use ZipTBO. There are many applications where space is extremely tight, especially on cell phones. HashTBO was developed for contextual spelling in Mic"
D07-1021,P02-1023,1,\N,Missing
D08-1011,J07-2003,0,0.127138,"ion effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the wor"
D08-1011,W07-0717,0,0.033654,"s from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million parallel sentence-pairs selected from the training corpus allowed by the constrained training"
D08-1011,P06-1121,0,0.056997,"Missing"
D08-1011,W07-0711,1,0.77443,"ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million parallel sentence-pairs selected from the training corpus allowed by the constrained training condition of MT08. 5.3.1 Comparison with TER alignment In the IHMM-based method, the smoothing factor for surface similarity model is set to ρ = 3, the interpolation factor of the overall similarity model is set to α = 0.3, and the controlling factor of the distance-based distortion parameters is set to K=2. These settings are optimized on the dev set. Individual system results and system combination results using both IHMM and TER alignment, on both the dev and test"
D08-1011,P05-3026,0,0.323475,"onnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model. There have been other hypothesis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections. 5.1 Implementation details In our implementation, the backbone is selected with MBR. Only the top hypothesis from each single system is considered as a backbone"
D08-1011,P08-2021,0,0.424303,"while it is only used implicitly via parameter initialization for IBM Model-1 training by Matusov et al. (2006). On the other hand, the TER-based alignment model is similar to a coarse-grained, nonnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model. There have been other hypothesis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as"
D08-1011,P07-1091,0,0.038442,"Missing"
D08-1011,N06-1014,0,0.101297,"ld be based on exact match: the surface similarity model, psur (ej |ei ) , would take the value 1.0 if e’= e, and ( f k |ei ) where pt 2s ( fk |ei ) is the translation model from the target-to-source word alignment model. In our method, pt 2 s (null |ei ) for all target words is 100 The other direction, ps 2t (ei |null ) , is available from the source-to-target translation model. 2 Usually a small back-off value is assigned instead of 0. 1 p(a j  i |a j1  i, I ) depend only on the jump distance (i - i') (Vogel et al., 1996): p(i |i, I )  c(i  i) I c(l  i) (5) l 1 As suggested by Liang et al. (2006), we can group the distortion parameters {c(d)}, d= i - i', into a few buckets. In our implementation, 11 buckets are used for c(≤-4), c(-3), ... c(0), ..., c(5), c(≥6). The probability mass for transitions with jump distance larger than 6 and less than -4 is uniformly divided. By doing this, only a handful of c(d) parameters need to be estimated. Although it is possible to estimate them using the EM algorithm on a small development set, we found that a particularly simple model, described below, works surprisingly well in our experiments. Since both the backbone and the hypothesis are in the"
D08-1011,E06-1005,0,0.643756,"significantly outperforms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation. 1 Introduction* System combination has been applied successfully to various machine translation tasks. Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007). A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores. The consensus output is then derived by selecting one word from each set of alternatives, to produce the sequence with the best overall score, which could be assigned in various ways such as by voting, by * Mei Yang performed this work when she was an intern with Microsoft Research. using posterior probability estimates, or by using a combination of these measures and other features. Constructing a confusion network requires choosing"
D08-1011,2007.mtsummit-papers.43,1,0.70273,"Missing"
D08-1011,P02-1040,0,0.108033,"sis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections. 5.1 Implementation details In our implementation, the backbone is selected with MBR. Only the top hypothesis from each single system is considered as a backbone. A uniform posteriori probability is assigned to all hypotheses. TER is used as loss function in the MBR computation. Similar to (Rosti et al., 2007), each word in the confusion network is associated with a word posterior probability. Given a system S, each of its hypotheses is assigned with a rank-based score of 1/(1+r)η, where r is the rank of the hypothesis, and η is a r"
D08-1011,N03-1017,0,0.0156845,"Missing"
D08-1011,P05-1034,0,0.0651166,"to the system combination, 10-best hypotheses for each source sentence in the dev and test sets are collected from each of the eight single systems. All outputs on the MT08 test set were true-cased before scoring using a log-linear conditional Markov model proposed by Toutanova et al. (2008). However, to save computation effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All"
D08-1011,N07-1029,0,0.604428,"emented in GIZA++, and heuristically combines results from aligning in both directions. System combination based on this approach gives an improvement over the best single system. However, the number of hypothesis pairs for training is limited by the size of the test corpus. Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples. Therefore, GIZA++ training on such a data set may be unreliable. Bangalore et al. (2001) used a multiple stringmatching algorithm based on Levenshtein edit distance, and later Sim et al. (2007) and Rosti et al. (2007) extended it to a TER-based method for hypothesis alignment. TER (Snover et al., 2006) 3 This only happens if no hypothesis word is aligned to a backbone word but some hypothesis words are aligned to the null associated with that backbone word. 102 measures the minimum number of edits, including substitution, insertion, deletion, and shift of blocks of words, that are needed to modify a hypothesis so that it exactly matches the other hypothesis. The best alignment is the one that gives the minimum number of translation edits. TER-based confusion network construction and"
D08-1011,P07-1040,0,0.668355,"rms the state-of-the-art TER-based alignment model in our experiments on NIST benchmark datasets. Our combined SMT system using the proposed method achieved the best Chinese-to-English translation result in the constrained training track of the 2008 NIST Open MT Evaluation. 1 Introduction* System combination has been applied successfully to various machine translation tasks. Recently, confusion-network-based system combination algorithms have been developed to combine outputs of multiple machine translation (MT) systems to form a consensus output (Bangalore, et al. 2001, Matusov et al., 2006, Rosti et al., 2007, Sim et al., 2007). A confusion network comprises a sequence of sets of alternative words, possibly including null’s, with associated scores. The consensus output is then derived by selecting one word from each set of alternatives, to produce the sequence with the best overall score, which could be assigned in various ways such as by voting, by * Mei Yang performed this work when she was an intern with Microsoft Research. using posterior probability estimates, or by using a combination of these measures and other features. Constructing a confusion network requires choosing one of the hypothes"
D08-1011,W08-0329,0,0.532485,"1 training by Matusov et al. (2006). On the other hand, the TER-based alignment model is similar to a coarse-grained, nonnormalized version of our IHMM, in which the similarity model assigns no penalty to an exact surface match and a fixed penalty to all substitutions, insertions, and deletions, and the distortion model simply assigns no penalty to a monotonic jump, and a fixed penalty to all other jumps, equal to the non-exact-match penalty in the similarity model. There have been other hypothesis alignment methods. Karakos, et al. (2008) proposed an ITGbased method for hypothesis alignment, Rosti et al. (2008) proposed an incremental alignment method, and a heuristic-based matching algorithm was proposed by Jayaraman and Lavie (2005). 5 Evaluation In this section, we evaluate our IHMM-based hypothesis alignment method on the Chinese-toEnglish (C2E) test in the constrained training track of the 2008 NIST Open MT Evaluation (NIST, 2008). We compare to the TER-based method used by Rosti et al. (2007). In the following experiments, the NIST BLEU score is used as the evaluation metric (Papineni et al., 2002), which is reported as a percentage in the following sections. 5.1 Implementation details In our"
D08-1011,2006.amta-papers.25,0,0.112438,"ombination based on this approach gives an improvement over the best single system. However, the number of hypothesis pairs for training is limited by the size of the test corpus. Also, MT hypotheses from the same source sentence are correlated with each other and these hypothesis pairs are not i.i.d. data samples. Therefore, GIZA++ training on such a data set may be unreliable. Bangalore et al. (2001) used a multiple stringmatching algorithm based on Levenshtein edit distance, and later Sim et al. (2007) and Rosti et al. (2007) extended it to a TER-based method for hypothesis alignment. TER (Snover et al., 2006) 3 This only happens if no hypothesis word is aligned to a backbone word but some hypothesis words are aligned to the null associated with that backbone word. 102 measures the minimum number of edits, including substitution, insertion, deletion, and shift of blocks of words, that are needed to modify a hypothesis so that it exactly matches the other hypothesis. The best alignment is the one that gives the minimum number of translation edits. TER-based confusion network construction and system combination has demonstrated superior performance on various large-scale MT tasks (Rosti. et al, 2007)"
D08-1011,C96-2141,0,0.703549,"nice car a sedan he has hypothesis set he have ε good car E4 a ε sedan he has (c) hypothesis alignment EB  argmin TER( E, E) EE EE e.g., EB = E1 (b) backbone selection he have ε good car he has ε nice sedan it ε a nice car he has a ε sedan (d) confusion network Figure 1: Confusion-network-based combination. MT system 3 Indirect-HMM-based Hypothesis Alignment In confusion-network-based system combination for SMT, a major difficulty is aligning hypotheses to the backbone. One possible statistical model for word alignment is the HMM, which has been widely used for bilingual word alignment (Vogel et al., 1996, Och and Ney, 2003). In this paper, we propose an indirect-HMM method for monolingual hypothesis alignment. 99 3.1 IHMM for hypothesis alignment e1I  (e1,..., eI ) denote the backbone, e1J  (e1,..., eJ ) a hypothesis to be aligned to e1I , and a1J  (a1,..., aJ ) the alignment that specifies Let the position of the backbone word aligned to each hypothesis word. We treat each word in the backbone as an HMM state and the words in the hypothesis as the observation sequence. We use a first-order HMM, assuming that the emission probability p(ej |ea j ) depends only on the backbone word, and"
D08-1011,D07-1077,0,0.0163745,"were true-cased before scoring using a log-linear conditional Markov model proposed by Toutanova et al. (2008). However, to save computation effort, the results on the dev set are reported in case insensitive BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different"
D08-1011,P06-1066,0,0.209309,"ve BLEU (ciBLEU) score instead. 5.3 Experimental results In our main experiments, outputs from a total of eight single MT systems were combined. As listed in Table 1, Sys-1 is a tree-to-string system proposed by Quirk et al., (2005); Sys-2 is a phrasebased system with fast pruning proposed by Moore and Quirk (2008); Sys-3 is a phrase-based system with syntactic source reordering proposed by Wang et al. (2007a); Sys-4 is a syntax-based preordering system proposed by Li et. al. (2007); Sys5 is a hierarchical system proposed by Chiang (2007); Sys-6 is a lexicalized re-ordering system proposed by Xiong et al. (2006); Sys-7 is a twopass phrase-based system with adapted LM proposed by Foster and Kuhn (2007); and Sys-8 is a hierarchical system with two-pass rescoring using a parser-based LM proposed by Wang et al., (2007b). All systems were trained within the confines of the constrained training condition of NIST MT08 evaluation. These single systems are optimized with maximum-BLEU training on different subsets of the previous NIST MT test data. The bilingual translation models used to compute the semantic similarity are from the worddependent HMMs proposed by He (2007), which are trained on two million par"
D08-1011,P08-1059,0,\N,Missing
D08-1011,J03-1002,0,\N,Missing
D08-1011,2005.eamt-1.20,0,\N,Missing
D08-1011,P08-1066,0,\N,Missing
D08-1036,E03-1009,0,0.300792,"aluation measure; e.g., a tagger which assigns all words the same single part-of-speech tag does disturbingly well under Variation of Information, suggesting that a poor tagger may score well under VI. In order to avoid this problem we focus here on evaluation measures that construct an explicit mapping between the gold-standard part-of-speech tags and the HMM’s states. Perhaps the most straightforward approach is to map each HMM state to the part-of-speech tag it co-occurs with most frequently, and use this mapping to map each HMM state sequence t to a sequence of part-of-speech tags. But as Clark (2003) observes, this approach has several defects. If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state. We can partially address this by cross-validation. We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech. We call the accuracy of the resulting tagging the crossvalidation accu"
D08-1036,P07-1094,0,0.65589,"6; MacKay, 2003). Instead, rather than commiting to a single value for the parameters θ many Bayesians often prefer to work with the full posterior distribution P(θ |d), as this naturally reflects the uncertainty in θ’s value. In all but the simplest models there is no known closed form for the posterior distribution. However, the Bayesian literature describes a number of methods for approximating the posterior P(θ |d). Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers (Goldwater and Griffiths, 2007; Johnson, 2007). These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t |w) of POS tags (i.e., HMM hidden states) t given words w. This recent literature reports contradictory results about these Bayesian inference methods. John345 son (2007) compared two Bayesian inference algorithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM"
D08-1036,N06-1041,0,0.387997,"has several defects. If a system is permitted to posit an unbounded number of states (which is not the case here) it can achieve a perfect score on by assigning each word token its own unique state. We can partially address this by cross-validation. We divide the corpus into two equal parts, and from the first part we extract a mapping from HMM states to the parts-of-speech they co-occur with most frequently, and use that mapping to map the states of the second part of the corpus to parts-of-speech. We call the accuracy of the resulting tagging the crossvalidation accuracy. Finally, following Haghighi and Klein (2006) and Johnson (2007) we can instead insist that at most one HMM state can be mapped to any part-of-speech tag. Following these authors, we used a greedy algorithm to associate states with POS tags; the accuracy of the resulting tagging is called the greedy 1-to-1 All − 50 All − 17 120K − 50 120K − 17 24K − 50 24K − 17 EM 0.40527 0.43101 0.29303 0.35202 0.18618 0.28165 VB 0.46123 0.51379 0.34679 0.36010 0.23823 0.36599 0.43424 0.36984 0.44125 0.29953 0.36811 GSe,p 0.47826 GSe,b 0.49371 0.46568 0.38888 0.44341 0.34404 0.37032 ? 0.45028 0.42785 0.43652 0.39182 0.39164 GSc,p 0.49910 0.41162 0.42278"
D08-1036,N07-1018,1,0.547162,"pointwise sampler requires O(nm) time per iteration, while a blocked sampler requires O(nm2 ) time per iteration, where m is the number of HMM states and n is the length of the training corpus. Second, the sampler can either be explicit or collapsed. An explicit sampler represents and samples the HMM parameters θ and φ in addition to the states t, while in a collapsed sampler the HMM parameters are integrated out, and only the states t are sampled. The difference between explicit and collapsed samplers corresponds exactly to the difference between the two PCFG sampling algorithms presented in Johnson et al. (2007). An iteration of the pointwise explicit Gibbs sampler consists of resampling θ and φ given the stateto-state transition counts n and state-to-word emission counts n0 using (5), and then resampling each state ti given the corresponding word wi and the neighboring states ti−1 and ti+1 using (6). θ t |nt , α ∼ Dir(nt + α) φt |n0t , α0 ∼ Dir(n0t + α0 ) (5) P(ti |wi , t−i , θ, φ) ∝ θti |ti−1 φwi |ti θti+1 |ti (6) The Dirichlet distributions in (5) are non-uniform; nt is the vector of state-to-state transition counts in t leaving state t in the current state vector t, while n0t is the vector of sta"
D08-1036,D07-1031,1,0.666356,"er than commiting to a single value for the parameters θ many Bayesians often prefer to work with the full posterior distribution P(θ |d), as this naturally reflects the uncertainty in θ’s value. In all but the simplest models there is no known closed form for the posterior distribution. However, the Bayesian literature describes a number of methods for approximating the posterior P(θ |d). Monte Carlo sampling methods and Variational Bayes are two kinds of approximate inference methods that have been applied to Bayesian inference of unsupervised HMM POS taggers (Goldwater and Griffiths, 2007; Johnson, 2007). These methods can also be used to approximate other distributions that are important to us, such as the conditional distribution P(t |w) of POS tags (i.e., HMM hidden states) t given words w. This recent literature reports contradictory results about these Bayesian inference methods. John345 son (2007) compared two Bayesian inference algorithms, Variational Bayes and what we call here a point-wise collapsed Gibbs sampler, and found that Variational Bayes produced the best solution, and that the Gibbs sampler was extremely slow to converge and produced a worse solution than EM. On the other h"
D08-1036,P05-1044,0,0.128934,"(b) 1-to-1 accuracy as a function of running time on a 3GHz dual quad-core Pentium for the four different Gibbs samplers on all data and 50 hidden states. Each iteration took approximately 96 sec. for the collapsed blocked sampler, 7.5 sec. for the collapsed pointwise sampler, 25 sec. for the explicit blocked sampler and 4.4 sec. for the explicit pointwise sampler. 350 accuracy. The studies presented by Goldwater and Griffiths (2007) and Johnson (2007) differed in the number of states that they used. Goldwater and Griffiths (2007) evaluated against the reduced tag set of 17 tags developed by Smith and Eisner (2005), while Johnson (2007) evaluated against the full Penn Treebank tag set. We ran all our estimators in both conditions here (thanks to Noah Smith for supplying us with his tag set). Also, the studies differed in the size of the corpora used. The largest corpus that Goldwater and Griffiths (2007) studied contained 96,000 words, while Johnson (2007) used all of the 1,173,766 words in the full Penn WSJ treebank. For that reason we ran all our estimators on corpora containing 24,000 words and 120,000 words as well as the full treebank. We ran each estimator with the eight different combinations of"
D09-1053,N04-4006,0,0.559189,"earch ranking: Model Interpolation approaches and error-driven learning approaches. In model interpolation approaches, the adaptation data is used to derive a domain-specific model (also called in-domain model), which is then combined with the background model trained on the background data. This appealingly simple concept provides fertile ground for experimentation, depending on the level at which the combination is implemented (Bellegarda, 2004). In error-driven learning approaches, the background model is adjusted so as to minimize the ranking errors the model makes on the adaptation data (Bacchiani et al., 2004; Gao et al. 2006). This is arguably more powerful than model interpolation for two reasons. First, by defining a proper error function, the method can optimize more directly the measure used to assess the final quality of the Web search system, e.g., Normalized Discounted Cumulative Gain (Javelin & Kekalainen, 2000) in this study. Second, in this framework, the model can be adjusted to be as fine-grained as necessary. In this study we developed a set of error-driven learning methods based on a boosting algorithm where, in an incremental manner, not only each feature weight could be 505 Procee"
D09-1154,H05-1120,0,0.385902,"completely equivalent, as opposed to Categories 6 through 9, where synonymy is more context- or user-dependent. This will ensure that the search results by query expansion will avoid the problem of compromised precision. 3 Related Work In information retrieval, the problem of vocabulary mismatch between the query and the terms in the document has been addressed in many ways, as mentioned in Section 1, achieving varying degrees of success in the retrieval task. In particular, our work is closely related to research in spelling correction for English web queries (e.g., Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007). Among these, Li et al. (2006) and Chen et al. (2007) incorporate both string and semantic similarity in their discriminative models of spelling correction, similarly to our approach. In Li et al. (2006), semantic similarity was computed as distributional similarity of the terms using query strings in the log as context. Chen et al. (2007) point out that this method suffers from the data sparseness problem in that the statistics for rarer terms are unreliable, and propose using web search results as extended contextual information. Their method, however, i"
D09-1154,J96-1002,0,0.0499694,"r method works independently of the character types used, and targets a wide range of term variations that are both orthographically and semantically similar, including spelling errors, valid alternative spellings, transliterations and abbreviations. As described in Section 4, we define the problem of term variation identifica1484 Proceedings of the 2009 Conference on Empirical Methods in Natural Language Processing, pages 1484–1492, c Singapore, 6-7 August 2009. 2009 ACL and AFNLP tion as a binary classification task, and build two types of classifiers according to the maximum entropy model (Berger et al., 1996) and the MART algorithm (Friedman, 2001), where all term similarity metrics are incorporated as features and are jointly optimized. Another important contribution of our approach is that we derive our semantic similarity models by mining user query logs, which has been explored for the purposes of collecting related words (e.g., Jones et al., 2006a), improving search results ranking (e.g., Craswell and Szummer, 2007) and learning query intention (e.g., Li et al., 2008), but not for the task of collecting term variations. We show that our semantic similarity models are not only effective in the"
D09-1154,C04-1086,0,0.0180361,"spelled using any of the character types, as we have seen in the example for the word 'protein' in Section 1. Though there are certainly preferred character types for spelling each word, variations are still very common in Japanese text and search queries. Alterations are particularly common among hiragana, katakana and kanji (e.g. さば~サバ~ 鯖 saba 'mackerel'), and between katakana and Roman alphabet (e.g. フ ェデッ クス fedekkusu fedex). This latter case constitutes the problem of transliteration, which has been extensively studied in the context of machine translation (e.g. Knight and Graehl, 1998; Bilac and Tanaka, 2004; Brill et al., 2001). 2.3 Term Variation by Re-write Categories Table 1 shows the re-write categories of related terms observed in web query logs, drawing on our own data analysis as well as on previous work such as Jones et al. (2006a) and Okazaki et al. (2008b). Categories 1 though 9 represent strictly synonymous relations; in addition, terms in Categories 1 through 5 are also similar orthographically or in pronunciation. Categories 10 1 In a dictionary of 200K entries, we find that on average each kanji character has 2.5 readings, with three characters (直,生,空) with as many as 11 readings."
D09-1154,P00-1037,0,0.501412,"r session information (i.e., a set of queries issued by the same user within a time frame, e.g., Jones et al., 2006a) or the URLs clicked as a result of the query (e.g., Craswell and Szummer, 2007; Li et al., 2008). This additional data serves as an approximation to the meaning of the query; we use both user session and click-through data for discovering term variations. Our work also draws on some previous work on string transformation, including spelling normalization and transliteration. In addition to the simple Levenshtein distance, we also use generalized string-to-string edit distance (Brill and Moore, 2000), which we trained on aligned katakana-English word pairs in the same manner as Brill et al. (2001). As mentioned in Section 2.2, our work also tries to address the individual problems targeted by such component technologies as Japanese katakana variation, English-tokatakana transliteration and katakana-to-English back-transliteration in a unified framework. 4 Discriminative Model of Identifying Term Variation Recent work in spelling correction (Ahmed and Kondrak, 2005; Li et al., 2006; Chen et al., 2007) and normalization (Okazaki et al., 2008b) formulates the task in a discriminative framewo"
D09-1154,D07-1019,0,0.574937,"ories 6 through 9, where synonymy is more context- or user-dependent. This will ensure that the search results by query expansion will avoid the problem of compromised precision. 3 Related Work In information retrieval, the problem of vocabulary mismatch between the query and the terms in the document has been addressed in many ways, as mentioned in Section 1, achieving varying degrees of success in the retrieval task. In particular, our work is closely related to research in spelling correction for English web queries (e.g., Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007). Among these, Li et al. (2006) and Chen et al. (2007) incorporate both string and semantic similarity in their discriminative models of spelling correction, similarly to our approach. In Li et al. (2006), semantic similarity was computed as distributional similarity of the terms using query strings in the log as context. Chen et al. (2007) point out that this method suffers from the data sparseness problem in that the statistics for rarer terms are unreliable, and propose using web search results as extended contextual information. Their method, however, is expensive as it requires web search"
D09-1154,I08-1047,1,0.835762,"distance function and phonetic similarity from query log data, is impractically large and must be pruned by using a language model. Our approach differs from these methods in that we exploit user query logs to derive semantic knowledge of terms, which is 1486 used both for the purpose of generating a candidate set efficiently and as features in the term variation identification model. Acquiring semantic knowledge from a large quantity of web query logs has become popular in recent years. Some use only query strings and their counts for learning word similarity (e.g., Sekine and Suzuki, 2007; Komachi and Suzuki, 2008), while others use additional information, such as the user session information (i.e., a set of queries issued by the same user within a time frame, e.g., Jones et al., 2006a) or the URLs clicked as a result of the query (e.g., Craswell and Szummer, 2007; Li et al., 2008). This additional data serves as an approximation to the meaning of the query; we use both user session and click-through data for discovering term variations. Our work also draws on some previous work on string transformation, including spelling normalization and transliteration. In addition to the simple Levenshtein distance"
D09-1154,P06-1129,0,0.626686,"opposed to Categories 6 through 9, where synonymy is more context- or user-dependent. This will ensure that the search results by query expansion will avoid the problem of compromised precision. 3 Related Work In information retrieval, the problem of vocabulary mismatch between the query and the terms in the document has been addressed in many ways, as mentioned in Section 1, achieving varying degrees of success in the retrieval task. In particular, our work is closely related to research in spelling correction for English web queries (e.g., Cucerzan and Brill, 2004; Ahmad and Kondrak, 2005; Li et al., 2006; Chen et al., 2007). Among these, Li et al. (2006) and Chen et al. (2007) incorporate both string and semantic similarity in their discriminative models of spelling correction, similarly to our approach. In Li et al. (2006), semantic similarity was computed as distributional similarity of the terms using query strings in the log as context. Chen et al. (2007) point out that this method suffers from the data sparseness problem in that the statistics for rarer terms are unreliable, and propose using web search results as extended contextual information. Their method, however, is expensive as it"
D09-1154,C04-1176,0,0.0295285,"iation by Character Type Spelling variations are commonly observed both within and across character types in Japanese. Within a character type, the most prevalent is the variation observed in katakana words. Katakana is used to transliterate words from English and other foreign languages, and therefore reflects the variations in the sound adaptation from the source language. For example, the word 'spaghetti' is transliterated into six different forms (スパゲッティ supagetti, スパゲッティー supagettii, スパゲッテイ supagettei, スパゲティ supageti, ス パ ゲ テ ィ ー supagetii, ス パ ゲ テ イ supagetei) within a newspaper corpus (Masuyama et al., 2004). Spelling variants are also prevalent across character types: in theory, a word can be spelled using any of the character types, as we have seen in the example for the word 'protein' in Section 1. Though there are certainly preferred character types for spelling each word, variations are still very common in Japanese text and search queries. Alterations are particularly common among hiragana, katakana and kanji (e.g. さば~サバ~ 鯖 saba 'mackerel'), and between katakana and Roman alphabet (e.g. フ ェデッ クス fedekkusu fedex). This latter case constitutes the problem of transliteration, which has been ex"
D09-1154,I08-2127,0,0.0653147,"Missing"
D09-1154,D08-1047,0,0.598345,"Missing"
D09-1154,W04-3238,0,0.727359,"where semantic equivalence of terms is sought, as it represents a very special case of paraphrase. This paper addresses the problem of identifying term variations in Japanese, specifically for the purpose of query expansion in web search, which appends additional terms to the original query string for better retrieval quality. Query expansion has been shown to be effective in improving web search results in English, where different methods of generating the expansion terms have been attempted, including relevance feedback (e.g., Salton and Buckley, 1990), correction of spelling errors (e.g., Cucerzan and Brill, 2004), stemming or lemmatization (e.g., Frakes, 1992), use of manually- (e.g., Aitchison and Gilchrist, 1987) or automatically- (e.g., Rasmussen 1992) constructed thesauri, and Latent Semantic Indexing (e.g., Deerwester et al, 1990). Though many of these methods can be applied to Japanese query expansion, there are unique problems posed by Japanese search queries, the most challenging of which is that valid alternative spellings of a word are extremely common due to the multiple script types employed in the language. For example, the word for 'protein' can be spelled as たんぱくしつ, タンパク質, 蛋白質, たん白質 and"
D09-1154,J93-1003,0,0.0538367,"Missing"
D09-1154,I08-1007,0,\N,Missing
D09-1154,J98-4003,0,\N,Missing
D11-1033,eck-etal-2004-language,0,0.0830212,"significant interest in using two translation models, one trained on a larger general-domain corpus and the other on a smaller in-domain corpus, to translate in-domain text. After all, if one has access to an in-domain corpus with which to select data from a general-domain corpus, then one might as well use the in-domain data, too. The expectation is that the larger general-domain model should dominate in regions where the smaller in-domain model lacks coverage due to sparse (or non-existent) ngram counts. In practice, most practical systems also perform target-side language model adaptation (Eck et al., 2004); we eschew this in order to isolate the effects of translation model adaptation alone. Directly concatenating the phrase tables into one larger one isn’t strongly motivated; identical phrase pairs within the resulting table can lead to unpredictable behavior during decoding. Nakov (2008) handled identical phrase pairs by prioritizing the source tables, however in our experience identical entries in phrase tables are not very common when comparing across domains. Foster and Kuhn (2007) interpolated the in- and general-domain phrase tables together, assigning either linear or log-linear weights"
D11-1033,W07-0711,1,0.148887,"baseline single-corpus systems are in Table 1. Corpus IWSLT General Phrases 515k 1,478m Dev 45.43 42.62 Test 37.17 40.51 Table 1: Baseline translation results for in-domain and general-domain systems. 4 357 System Description In order to highlight the data selection work, we used an out-of-the-box Moses framework using GIZA++ (Och and Ney, 2003) and MERT (Och, 2003) to train and tune the machine translation systems. The only exception was the phrase table for the large out-of-domain system trained on 12m sentence pairs, which we trained on a cluster using a word-dependent HMM-based alignment (He, 2007). We used the Moses decoder to produce all the system outputs, and scored them with the NIST mt-eval31a 4 tool used in the IWSLT evalutation. 3.4 We conducted our experiments on the International Workshop on Spoken Language Translation (IWSLT) Chinese-to-English DIALOG task 2 , consisting of transcriptions of conversational speech in a travel setting. Two corpora are needed for the adaptation task. Our in-domain data consisted of the IWSLT corpus of approximately 30,000 sentences in Chinese and English. Our general-domain corpus was 12 million parallel sentences comprising a variety of publicl"
D11-1033,D07-1036,0,0.420476,"Missing"
D11-1033,J03-1002,0,0.0408997,"ed above, on the IWSLT corpus. The resulting model had a phrase table with 515k entries. The general-domain baseline was substantially larger, having been trained on 12 million sentence pairs, and had a phrase table containing 1.5 billion entries. The BLEU scores of the baseline single-corpus systems are in Table 1. Corpus IWSLT General Phrases 515k 1,478m Dev 45.43 42.62 Test 37.17 40.51 Table 1: Baseline translation results for in-domain and general-domain systems. 4 357 System Description In order to highlight the data selection work, we used an out-of-the-box Moses framework using GIZA++ (Och and Ney, 2003) and MERT (Och, 2003) to train and tune the machine translation systems. The only exception was the phrase table for the large out-of-domain system trained on 12m sentence pairs, which we trained on a cluster using a word-dependent HMM-based alignment (He, 2007). We used the Moses decoder to produce all the system outputs, and scored them with the NIST mt-eval31a 4 tool used in the IWSLT evalutation. 3.4 We conducted our experiments on the International Workshop on Spoken Language Translation (IWSLT) Chinese-to-English DIALOG task 2 , consisting of transcriptions of conversational speech in a"
D11-1033,P03-1021,0,0.33736,". The resulting model had a phrase table with 515k entries. The general-domain baseline was substantially larger, having been trained on 12 million sentence pairs, and had a phrase table containing 1.5 billion entries. The BLEU scores of the baseline single-corpus systems are in Table 1. Corpus IWSLT General Phrases 515k 1,478m Dev 45.43 42.62 Test 37.17 40.51 Table 1: Baseline translation results for in-domain and general-domain systems. 4 357 System Description In order to highlight the data selection work, we used an out-of-the-box Moses framework using GIZA++ (Och and Ney, 2003) and MERT (Och, 2003) to train and tune the machine translation systems. The only exception was the phrase table for the large out-of-domain system trained on 12m sentence pairs, which we trained on a cluster using a word-dependent HMM-based alignment (He, 2007). We used the Moses decoder to produce all the system outputs, and scored them with the NIST mt-eval31a 4 tool used in the IWSLT evalutation. 3.4 We conducted our experiments on the International Workshop on Spoken Language Translation (IWSLT) Chinese-to-English DIALOG task 2 , consisting of transcriptions of conversational speech in a travel setting. Two c"
D11-1033,D10-1044,0,\N,Missing
D11-1033,D09-1074,0,\N,Missing
D11-1033,W08-0320,0,\N,Missing
D11-1033,P10-2041,0,\N,Missing
D11-1033,P07-2045,0,\N,Missing
D11-1033,W07-0733,0,\N,Missing
D11-1033,W07-0702,0,\N,Missing
D11-1033,I08-2088,0,\N,Missing
D11-1033,W07-0717,0,\N,Missing
D12-1056,P12-2073,1,0.881367,"Missing"
D12-1056,P01-1005,0,0.0169201,"correction is motivated by the success on transliteration (e.g., Sherif and Kondrak, 2007; Cherry and Suzuki, 2009) and letter-tophoneme conversion (e.g., Jiampojamarn et al., 2007; Rama et al., 2009). One big challenge of the spelling correction research is the general lack of naturally occurring paired data of contextual spelling errors and their correction. Previous work has therefore either focused on the task of correcting out-of-vocabulary words out of context (e.g., Brill and Moore, 2000; Toutanova and Moore, 2002), or has resorted to innovative methods of data collection. For example, Banko and Brill (2001) generate data artificially by substituting words from a confusion word set in text for building a contextual speller; Whitelaw et al. (2009) use word frequency and edit distance information to harvest error pairs from a web corpus in an unsupervised manner; Bertoldi et al. (2010) intentionally corrupt clean text by adding noise to the data. Another approach to spelling error data collection uses web search query logs, available in large quantity (albeit to limited institutions), and limit its focus on the task of correcting misspelled queries (e.g., Cucerzan and Brill, 2004; Gao et al., 2010;"
D12-1056,N10-1064,0,0.0194146,"turally occurring paired data of contextual spelling errors and their correction. Previous work has therefore either focused on the task of correcting out-of-vocabulary words out of context (e.g., Brill and Moore, 2000; Toutanova and Moore, 2002), or has resorted to innovative methods of data collection. For example, Banko and Brill (2001) generate data artificially by substituting words from a confusion word set in text for building a contextual speller; Whitelaw et al. (2009) use word frequency and edit distance information to harvest error pairs from a web corpus in an unsupervised manner; Bertoldi et al. (2010) intentionally corrupt clean text by adding noise to the data. Another approach to spelling error data collection uses web search query logs, available in large quantity (albeit to limited institutions), and limit its focus on the task of correcting misspelled queries (e.g., Cucerzan and Brill, 2004; Gao et al., 2010; Sun et al., 2010; Duan and Hsu, 2011). The problem of data collection is particularly difficult for pinyin error correction, as pinyin is not a final form of text in Chinese, so it is not recorded in final text. Zheng et al. (2011a) study a log of pinyin input method and use the"
D12-1056,P00-1037,0,0.891629,"o-phone conversion and transliteration between different scripts. In particular, our substring-based approach to spelling correction is motivated by the success on transliteration (e.g., Sherif and Kondrak, 2007; Cherry and Suzuki, 2009) and letter-tophoneme conversion (e.g., Jiampojamarn et al., 2007; Rama et al., 2009). One big challenge of the spelling correction research is the general lack of naturally occurring paired data of contextual spelling errors and their correction. Previous work has therefore either focused on the task of correcting out-of-vocabulary words out of context (e.g., Brill and Moore, 2000; Toutanova and Moore, 2002), or has resorted to innovative methods of data collection. For example, Banko and Brill (2001) generate data artificially by substituting words from a confusion word set in text for building a contextual speller; Whitelaw et al. (2009) use word frequency and edit distance information to harvest error pairs from a web corpus in an unsupervised manner; Bertoldi et al. (2010) intentionally corrupt clean text by adding noise to the data. Another approach to spelling error data collection uses web search query logs, available in large quantity (albeit to limited institu"
D12-1056,P00-1031,0,0.283645,"ot a final form of text in Chinese, so it is not recorded in final text. Zheng et al. (2011a) study a log of pinyin input method and use the backspace key to learn the user mistyping behavior, but they do so only for the purpose of data analysis, and do not build a statistical model from this data. Text input methods have been commercially available for decades for inputting Chinese and Japanese, but have also recently become available for other non-Roman script languages including Arabic and the languages of India.1 Early research work on text input methods includes e.g., Mori et al. (1998), Chen and Lee (2000) and Gao et al. (2002), all of which approach the problem using a noisy channel model. Discriminative approaches have also been proposed, e.g., Suzuki and Gao (2005); Tokunaga et al. (2011). There is only a very limited amount of work that deals with spelling correction in the context of text input: Zheng et al. (2011b) represents a recent work based on a noisy channel model, which defines our baseline. Their work is strictly word-based and only handles the correction of out-of-vocabulary pinyin words into in-vocabulary pinyin words, while our substring-based model is not limited by these cons"
D12-1056,D09-1111,1,0.883603,"arch in Section 2. We then describe our approach to the spelling correction task (Section 3) and the end-to-end conversion task (Section 4). We summarize our contribution and conclude with remarks for future directions in Section 5. 2 Related Work The current work builds on many previous works on the task of monotone substring-based transduction, including spelling correction, letterto-phone conversion and transliteration between different scripts. In particular, our substring-based approach to spelling correction is motivated by the success on transliteration (e.g., Sherif and Kondrak, 2007; Cherry and Suzuki, 2009) and letter-tophoneme conversion (e.g., Jiampojamarn et al., 2007; Rama et al., 2009). One big challenge of the spelling correction research is the general lack of naturally occurring paired data of contextual spelling errors and their correction. Previous work has therefore either focused on the task of correcting out-of-vocabulary words out of context (e.g., Brill and Moore, 2000; Toutanova and Moore, 2002), or has resorted to innovative methods of data collection. For example, Banko and Brill (2001) generate data artificially by substituting words from a confusion word set in text for build"
D12-1056,W04-3238,0,0.783816,"ection. For example, Banko and Brill (2001) generate data artificially by substituting words from a confusion word set in text for building a contextual speller; Whitelaw et al. (2009) use word frequency and edit distance information to harvest error pairs from a web corpus in an unsupervised manner; Bertoldi et al. (2010) intentionally corrupt clean text by adding noise to the data. Another approach to spelling error data collection uses web search query logs, available in large quantity (albeit to limited institutions), and limit its focus on the task of correcting misspelled queries (e.g., Cucerzan and Brill, 2004; Gao et al., 2010; Sun et al., 2010; Duan and Hsu, 2011). The problem of data collection is particularly difficult for pinyin error correction, as pinyin is not a final form of text in Chinese, so it is not recorded in final text. Zheng et al. (2011a) study a log of pinyin input method and use the backspace key to learn the user mistyping behavior, but they do so only for the purpose of data analysis, and do not build a statistical model from this data. Text input methods have been commercially available for decades for inputting Chinese and Japanese, but have also recently become available f"
D12-1056,C10-1041,1,0.875118,"and Brill (2001) generate data artificially by substituting words from a confusion word set in text for building a contextual speller; Whitelaw et al. (2009) use word frequency and edit distance information to harvest error pairs from a web corpus in an unsupervised manner; Bertoldi et al. (2010) intentionally corrupt clean text by adding noise to the data. Another approach to spelling error data collection uses web search query logs, available in large quantity (albeit to limited institutions), and limit its focus on the task of correcting misspelled queries (e.g., Cucerzan and Brill, 2004; Gao et al., 2010; Sun et al., 2010; Duan and Hsu, 2011). The problem of data collection is particularly difficult for pinyin error correction, as pinyin is not a final form of text in Chinese, so it is not recorded in final text. Zheng et al. (2011a) study a log of pinyin input method and use the backspace key to learn the user mistyping behavior, but they do so only for the purpose of data analysis, and do not build a statistical model from this data. Text input methods have been commercially available for decades for inputting Chinese and Japanese, but have also recently become available for other non-Roman"
D12-1056,D09-1093,0,0.0606547,"ersion (e.g., Jiampojamarn et al., 2007; Rama et al., 2009). One big challenge of the spelling correction research is the general lack of naturally occurring paired data of contextual spelling errors and their correction. Previous work has therefore either focused on the task of correcting out-of-vocabulary words out of context (e.g., Brill and Moore, 2000; Toutanova and Moore, 2002), or has resorted to innovative methods of data collection. For example, Banko and Brill (2001) generate data artificially by substituting words from a confusion word set in text for building a contextual speller; Whitelaw et al. (2009) use word frequency and edit distance information to harvest error pairs from a web corpus in an unsupervised manner; Bertoldi et al. (2010) intentionally corrupt clean text by adding noise to the data. Another approach to spelling error data collection uses web search query logs, available in large quantity (albeit to limited institutions), and limit its focus on the task of correcting misspelled queries (e.g., Cucerzan and Brill, 2004; Gao et al., 2010; Sun et al., 2010; Duan and Hsu, 2011). The problem of data collection is particularly difficult for pinyin error correction, as pinyin is no"
D12-1056,N07-1047,0,0.0603347,"correction task (Section 3) and the end-to-end conversion task (Section 4). We summarize our contribution and conclude with remarks for future directions in Section 5. 2 Related Work The current work builds on many previous works on the task of monotone substring-based transduction, including spelling correction, letterto-phone conversion and transliteration between different scripts. In particular, our substring-based approach to spelling correction is motivated by the success on transliteration (e.g., Sherif and Kondrak, 2007; Cherry and Suzuki, 2009) and letter-tophoneme conversion (e.g., Jiampojamarn et al., 2007; Rama et al., 2009). One big challenge of the spelling correction research is the general lack of naturally occurring paired data of contextual spelling errors and their correction. Previous work has therefore either focused on the task of correcting out-of-vocabulary words out of context (e.g., Brill and Moore, 2000; Toutanova and Moore, 2002), or has resorted to innovative methods of data collection. For example, Banko and Brill (2001) generate data artificially by substituting words from a confusion word set in text for building a contextual speller; Whitelaw et al. (2009) use word frequen"
D12-1056,C90-2036,0,0.551618,"o our study, though their approach differs from ours in that we build an integrated system that include the feature functions of both error correction and character conversion sub-systems. 3 Substring-based Spelling using a Log-linear Model Correction In this section, we describe our approach to pinyin error correction within a log-linear framework. Though our current target is pinyin error correction, the method described in this section is applicable to any language of interest. The spelling correction problem has been standardly formulated within the framework of noisy channel model (e.g., Kernighan et al., 1990). Let A be the input phonetic string in pinyin. The task of spelling correction is to search for the best 1 A few examples include Google Transliterate (http://www.google.com/transliterate/) and Microsoft Maren (http://www.microsoft.com/middleeast/egypt/cmic/maren/) / ILIT (http://specials.msn.co.in/ilit/Hindi.aspx). Quillpad (http://quillpad.in/) is also popularly used in India. 611 correction candidate in pinyin C* among all possible corrections for each potentially misspelled pinyin A: | Applying Bayes' Rule and dropping the constant denominator, we have | | where the error model models the"
D12-1056,N03-1017,0,0.0204735,"beyond Chinese and Japanese. In this paper, we propose a novel, unified system of text input with spelling correction, using 609 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 609–618, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics Chinese pinyin-to-hanzi conversion as an example. We first formulate the task of pinyin spelling correction as a substring-based monotone translation problem, inspired by phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004): we consider the pinyin input (potentially with errors) as the source language and the error-corrected pinyin as the target, and build a log-linear model for spelling correction. In doing so, we also propose a novel, unsupervised method of collecting parallel training data from user input logs. We then build an integrated end-to-end text input system that directly converts a potentially erroneous input pinyin sequence into a desired hanzi sequence, also formulated as a monotone phrase-based SMT problem, in which the feature functions of the substring-based error correction"
D12-1056,mcenery-xiao-2004-lancaster,0,0.0171469,".3 The performance of pinyin error correction was evaluated on two data sets: (1) log-test: the test set of the data in Section 3.1, which is derived in the same way as the training data but is noisy, consisting of 5,000 phrases of which 2,020 are misspelled; (2) CHIME: the gold standard from the CHIME data set made available by Zheng et al. (2011b), 5 which is also used in the end-to-end evaluation in Section 4. This data set consists of 2,000 sentence pairs of pinyin input with errors and the target hanzi characters, constructed by collecting actual user typing logs of the Lancaster corpus (McEnery and Xiao, 2004), which includes text from newspaper, fiction, and essays. 6 The CHIME data set does not include the corrected pinyin string; we therefore generated this by running a text-to-pinyin utility, 7 and created the pairs before and after error correction for evaluating our pinyin spelling correction module. The set contains 11,968 words of which 908 are misspelled. The results of the evaluation are given in Table 2. They are for phrase/word-level accuracy, as the log-derived data set is for each phrase (a user5 4 Consistency here implies two things. First, there must be at least one aligned characte"
D12-1056,P03-1021,0,0.0355508,"ure functions derived from the error and language models, we also use word and phrase penalties as feature functions, which are commonly used in SMT. These features also make sense in the current context, as using fewer phrase means encouraging longer ones with more context, and the target character length can capture tendencies to delete or insert words in errors. Overall, the log-linear model uses 7 feature functions: 4 derived from the translation models, word and phrase penalties, and the language model. The model weights were trained using the minimum error rate training algorithm (MERT, Och, 2003). We tried MERT with two objective functions: one that uses the 4-gram BLEU score as straightforwardly adapted from SMT, and the other that minimizes the character error rate (CER). CER is based on the edit distance between the reference and system output, which is used for evaluating the IME accuracy (Section 4.3). It is more directly related with the word/phrase-level accuracy, which we used to evaluate the error correction module in isolation, than the BLEU metric. As we will show below, however, using different objective functions turned out to have only a minimal impact on the spelling co"
D12-1056,J04-4002,0,0.0431888,"Japanese. In this paper, we propose a novel, unified system of text input with spelling correction, using 609 Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural c Language Learning, pages 609–618, Jeju Island, Korea, 12–14 July 2012. 2012 Association for Computational Linguistics Chinese pinyin-to-hanzi conversion as an example. We first formulate the task of pinyin spelling correction as a substring-based monotone translation problem, inspired by phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004): we consider the pinyin input (potentially with errors) as the source language and the error-corrected pinyin as the target, and build a log-linear model for spelling correction. In doing so, we also propose a novel, unsupervised method of collecting parallel training data from user input logs. We then build an integrated end-to-end text input system that directly converts a potentially erroneous input pinyin sequence into a desired hanzi sequence, also formulated as a monotone phrase-based SMT problem, in which the feature functions of the substring-based error correction component are integ"
D12-1056,N09-3016,0,0.0169106,") and the end-to-end conversion task (Section 4). We summarize our contribution and conclude with remarks for future directions in Section 5. 2 Related Work The current work builds on many previous works on the task of monotone substring-based transduction, including spelling correction, letterto-phone conversion and transliteration between different scripts. In particular, our substring-based approach to spelling correction is motivated by the success on transliteration (e.g., Sherif and Kondrak, 2007; Cherry and Suzuki, 2009) and letter-tophoneme conversion (e.g., Jiampojamarn et al., 2007; Rama et al., 2009). One big challenge of the spelling correction research is the general lack of naturally occurring paired data of contextual spelling errors and their correction. Previous work has therefore either focused on the task of correcting out-of-vocabulary words out of context (e.g., Brill and Moore, 2000; Toutanova and Moore, 2002), or has resorted to innovative methods of data collection. For example, Banko and Brill (2001) generate data artificially by substituting words from a confusion word set in text for building a contextual speller; Whitelaw et al. (2009) use word frequency and edit distance"
D12-1056,P07-1119,0,0.0177507,"he background of this research in Section 2. We then describe our approach to the spelling correction task (Section 3) and the end-to-end conversion task (Section 4). We summarize our contribution and conclude with remarks for future directions in Section 5. 2 Related Work The current work builds on many previous works on the task of monotone substring-based transduction, including spelling correction, letterto-phone conversion and transliteration between different scripts. In particular, our substring-based approach to spelling correction is motivated by the success on transliteration (e.g., Sherif and Kondrak, 2007; Cherry and Suzuki, 2009) and letter-tophoneme conversion (e.g., Jiampojamarn et al., 2007; Rama et al., 2009). One big challenge of the spelling correction research is the general lack of naturally occurring paired data of contextual spelling errors and their correction. Previous work has therefore either focused on the task of correcting out-of-vocabulary words out of context (e.g., Brill and Moore, 2000; Toutanova and Moore, 2002), or has resorted to innovative methods of data collection. For example, Banko and Brill (2001) generate data artificially by substituting words from a confusion"
D12-1056,P10-1028,1,0.803141,"generate data artificially by substituting words from a confusion word set in text for building a contextual speller; Whitelaw et al. (2009) use word frequency and edit distance information to harvest error pairs from a web corpus in an unsupervised manner; Bertoldi et al. (2010) intentionally corrupt clean text by adding noise to the data. Another approach to spelling error data collection uses web search query logs, available in large quantity (albeit to limited institutions), and limit its focus on the task of correcting misspelled queries (e.g., Cucerzan and Brill, 2004; Gao et al., 2010; Sun et al., 2010; Duan and Hsu, 2011). The problem of data collection is particularly difficult for pinyin error correction, as pinyin is not a final form of text in Chinese, so it is not recorded in final text. Zheng et al. (2011a) study a log of pinyin input method and use the backspace key to learn the user mistyping behavior, but they do so only for the purpose of data analysis, and do not build a statistical model from this data. Text input methods have been commercially available for decades for inputting Chinese and Japanese, but have also recently become available for other non-Roman script languages"
D12-1056,H05-1034,1,0.817292,"the user mistyping behavior, but they do so only for the purpose of data analysis, and do not build a statistical model from this data. Text input methods have been commercially available for decades for inputting Chinese and Japanese, but have also recently become available for other non-Roman script languages including Arabic and the languages of India.1 Early research work on text input methods includes e.g., Mori et al. (1998), Chen and Lee (2000) and Gao et al. (2002), all of which approach the problem using a noisy channel model. Discriminative approaches have also been proposed, e.g., Suzuki and Gao (2005); Tokunaga et al. (2011). There is only a very limited amount of work that deals with spelling correction in the context of text input: Zheng et al. (2011b) represents a recent work based on a noisy channel model, which defines our baseline. Their work is strictly word-based and only handles the correction of out-of-vocabulary pinyin words into in-vocabulary pinyin words, while our substring-based model is not limited by these constraints. The current work also has an affinity to the task of speech translation in that the parallel data between the input (speech signal) and the output (text in"
D12-1056,P02-1019,0,0.241728,"transliteration between different scripts. In particular, our substring-based approach to spelling correction is motivated by the success on transliteration (e.g., Sherif and Kondrak, 2007; Cherry and Suzuki, 2009) and letter-tophoneme conversion (e.g., Jiampojamarn et al., 2007; Rama et al., 2009). One big challenge of the spelling correction research is the general lack of naturally occurring paired data of contextual spelling errors and their correction. Previous work has therefore either focused on the task of correcting out-of-vocabulary words out of context (e.g., Brill and Moore, 2000; Toutanova and Moore, 2002), or has resorted to innovative methods of data collection. For example, Banko and Brill (2001) generate data artificially by substituting words from a confusion word set in text for building a contextual speller; Whitelaw et al. (2009) use word frequency and edit distance information to harvest error pairs from a web corpus in an unsupervised manner; Bertoldi et al. (2010) intentionally corrupt clean text by adding noise to the data. Another approach to spelling error data collection uses web search query logs, available in large quantity (albeit to limited institutions), and limit its focus"
D12-1056,W11-3502,0,0.10037,"in the context of text input: Zheng et al. (2011b) represents a recent work based on a noisy channel model, which defines our baseline. Their work is strictly word-based and only handles the correction of out-of-vocabulary pinyin words into in-vocabulary pinyin words, while our substring-based model is not limited by these constraints. The current work also has an affinity to the task of speech translation in that the parallel data between the input (speech signal) and the output (text in foreign language) is not directly available, but is mediated by a corrected (transcribed) form of input. Zhang et al. (2011) is thus relevant to our study, though their approach differs from ours in that we build an integrated system that include the feature functions of both error correction and character conversion sub-systems. 3 Substring-based Spelling using a Log-linear Model Correction In this section, we describe our approach to pinyin error correction within a log-linear framework. Though our current target is pinyin error correction, the method described in this section is applicable to any language of interest. The spelling correction problem has been standardly formulated within the framework of noisy ch"
D12-1056,P11-2085,0,0.0309945,"Missing"
D12-1061,J93-2003,0,0.0377637,"id sparse data problems while the correlation model relies on pure counts of term frequencies. However, the SMT system is used as a black box in their experiments. So the relative contribution of different SMT components is not verified empirically. In this study we break this black box in order to build a better, simpler QE system. We will show that the proposed lexicon models outperform significantly the term correlation model, and that a simpler QE system that incorporates the lexicon models can beat the sophisticated, black-box SMT system. 2.1 The word model takes the form of IBM Model 1 (Brown et al. 1993; Berger and Lafferty 1999). Let be a query, be an expansion term candidate, the translation probability from to is defined as | We view search queries and Web documents as two different languages, and cast QE as a means to bridge the language gap by translating queries to documents, represented by their titles. In this section, we will describe three translation models that are based on terms, triplets, and topics, respectively, and the way these models are learned from query-title pairs extracted from clickthrough data. 667 ∑ ( |) ( |) (1) |is the unsmoothed unigram probawhere bility of word"
D12-1061,C10-1041,1,0.928271,"re composed using different vocabularies and language styles. Query expansion (QE) is an effective strategy to address the problem. It expands a query issued by a user with additional related terms, called expansion terms, so that more relevant documents can be retrieved. In this paper we explore the use of clickthrough data and translation models for QE. We select expansion terms for a query according to how likely it is that the expansion terms occur in the title of a document that is relevant to the query. Assuming that a query is parallel to the titles of documents clicked for that query (Gao et al. 2010a), three lexicon models are trained on query-title pairs extracted from clickthrough data. The first is a word model that learns the translation probability between single words. The second model uses lexicalized triplets to incorporate word dependencies for translation. The third is a bilingual topic model, which represents a query as a distribution of hidden topics and learns the translation between a query and a title term at the semantic level. We will show that the word model provides a rich set of expansion candidates while the triplet and topic models can effectively select good expans"
D12-1061,D08-1039,0,0.0149355,"ating document titles from queries over the entire training corpus: ∏ | (2) where both the titles and the paired queries are viewed as bag of words. The translation probability | takes the form of IBM Model 1 as | ∏∑ ( | ) (3) where is a constant, is the length of , and is the length of . To find the optimal word translation probabilities of IBM Model 1, we used the EM algorithm, where the number of iterations is determined empirically on held-out data. 2.2 2 Lexicon Models Word Model Triplet Model The word model is context independent. The triplet model, which is originally proposed for SMT (Hasan et al. 2008), is intended to capture inter-term dependencies for selecting expansion terms. The model is based on lexicalized triplets ( ) which can be understood as two query terms triggering one expansion term. The translation probability of given for the triplet model is parameterized as | ∑ ∑ ( | ) (4) where Z is a normalization factor based on the corresponding query length, i.e., , and ( | ) is the probability of translating into given another query word . Since can be any word in that is not necessary to be adjacent to , the triple model is able to combine local (i.e. word and phrase level) and glo"
D12-1061,N03-1017,0,0.0589454,"n is that Cui et al. performed the evaluation using documents and search logs collected from the Encarta website, which is much cleaner and more homogenous than the data sets we used. The result suggests that although QE improves the recall of relevant documents, it is also likely to introduce noise that hurts the precision of document retrieval. SMT (Row 3) is a SMT-based QE system. Following Riezler et al. (2008), the system is an implementation of a phrase-based SMT system with a standard set of features for translation model and language model, combined under a log linear model framework (Koehn et al. 2003). Different from Riezler et al.’s system where the translation model is trained on query-snippet pairs and the language model on queries, in our implementation the trans673 lation model is trained on query-title pairs and the language model on titles. To apply the system to QE, expansion terms of a query are taken from those terms in the 10-best translations of the query that have not been seen in the original query string. We see that SMT significantly outperforms TC in NDCG at all levels. The result confirms the conclusion of Riezler et al., demonstrating that context information is crucial"
D12-1061,N04-4024,0,0.255327,") where is the unigram probability computed using a unigram language model trained on the collection of document titles. The fifth type of cliques contains a pair of terms appearing in consecutive order in the expansion. The potential functions are defined as ∑ ∑ ∑ ∑ ∑ ∑ (10) where there are in total 8 ’s to be estimated. Although the MRF is by nature a generative | model, it is not always appropriate to train the parameters using conventional likelihood based apwhere | is the bigram probability com- proaches due to the metric divergence problem puted using a bigram language model trained on (Morgan et al. 2004): i.e., the maximum likelihood the collection of document titles. estimate is unlikely to be the one that optimizes the The sixth type of cliques contains a pair of evaluation metric. In this study the effectiveness of terms appearing unordered within the expansion. a QE method is evaluated by first issuing a set of The potential functions are defined as queries which are expanded using the method to a (11) search engine and then measuring the Web search performance. Better QE methods are supposed to | lead to better Web search results using the correspondingly expanded query set. For this rea"
D12-1061,C08-1093,0,0.273874,"onal Linguistics tomatically from document collections (Jing and Croft 1994), the log-based method is superior in that it explicitly captures the correlation between query terms and document terms, and thus can bridge the lexical gap between them more effectively. Second, since search logs retrain querydocument pairs clicked by millions of users, the term correlations reflect the preference of the majority of users. Third, the term correlations evolve along with the accumulation of user logs, thus can reflect updated user interests at a specific time. However, as pointed out by Riezler et al. (2008), Cui et al.’s correlation-based method suffers low precision of QE partly because the correlation model does not explicitly capture context information and is susceptible to noise. Riezler et al. developed a QE system by retraining a standard phrase-based statistical machine translation (SMT) system using query-snippet pairs extracted from clickthrough data (Riezler et al. 2008; Riezler and Liu 2010). The SMT-based system can produce cleaner, more relevant expansion terms because rich context information useful for filtering noisy expansions is captured by combining language model and phrase"
D12-1061,J10-3010,0,0.0113366,"ence of the majority of users. Third, the term correlations evolve along with the accumulation of user logs, thus can reflect updated user interests at a specific time. However, as pointed out by Riezler et al. (2008), Cui et al.’s correlation-based method suffers low precision of QE partly because the correlation model does not explicitly capture context information and is susceptible to noise. Riezler et al. developed a QE system by retraining a standard phrase-based statistical machine translation (SMT) system using query-snippet pairs extracted from clickthrough data (Riezler et al. 2008; Riezler and Liu 2010). The SMT-based system can produce cleaner, more relevant expansion terms because rich context information useful for filtering noisy expansions is captured by combining language model and phrase translation model in its decoder. Furthermore, in the SMT system all component models are properly smoothed using sophisticated techniques to avoid sparse data problems while the correlation model relies on pure counts of term frequencies. However, the SMT system is used as a black box in their experiments. So the relative contribution of different SMT components is not verified empirically. In this s"
D14-1002,P14-1066,1,0.0924512,"de PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significantly extends the approach so that the deep neural network can be trained on large-scale query-document pairs giving much better performance. The use of the convolutional neural network for text processing, central to our DSSM, was also described in Collobert et al. (2011) and Shen et al. (2014) but with very different applications. The DSSM described in Sec"
D14-1002,J93-2003,0,0.0385454,"combination (Row 9) is obtained by incorporating the DSSM feature vectors of source and target documents (i.e., 600 features in total) in the ranker. We thus conclude that on both tasks, automatic highlighting and contextual entity search, features drawn from the output layers of our deep semantic model result in significant gains after being added to a set of non-semantic features, and in comparison to other types of semantic models used in the past. |is the unigram probability of word where | in , and is the probability of translating into , trained on source-target document pairs using EM (Brown et al. 1993). The translation-based approach allows any pair of non-identical but semantically related words to have a nonzero matching score. As a result, it significantly outperforms BM25. BTLM (Row 4) follows the best performing bilingual topic model described in Gao et al. (2011), which is an extension of PLSA (Hofmann 1999). The model is trained on source-target document pairs using the EM algorithm with a constraint enforcing a source document and its target document to not only share the same prior topic distribution, but to also have similar fractions of words assigned to each topic. BLTM defines"
D14-1002,C14-1140,1,0.829571,"Missing"
D14-1002,D10-1025,0,0.0126544,"for interestingness. The task of contextual entity search, which is formulated as an information retrieval problem in this paper, is also related to research on entity resolution (Stefanidis et al. 2013). Latent Semantic Analysis (Deerwester et al. 1990) is arguably the earliest semantic model designed for IR. Generative topic models widely used for IR include PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significa"
D14-1002,D12-1110,0,0.00801862,"erative topic models widely used for IR include PLSA (Hofmann 1990) and LDA (Blei et al. 2003). Recently, these models have been extended to handle cross-lingual cases, where there are pairs of corresponding documents in different languages (e.g., Dumais et al. 1997; Gao et al. 2011; Platt et al. 2010; Yih et al. 2011). By exploiting deep architectures, deep learning techniques are able to automatically discover from training data the hidden structures and the associated features at different levels of abstraction useful for a variety of tasks (e.g., Collobert et al. 2011; Hinton et al. 2012; Socher et al. 2012; Krizhevsky et al., 2012; Gao et al. 2014). Hinton and Salakhutdinov (2010) propose the most original approach based on an unsupervised version of the deep neural network to discover the hierarchical semantic structure embedded in queries and documents. Huang et al. (2013) significantly extends the approach so that the deep neural network can be trained on large-scale query-document pairs giving much better performance. The use of the convolutional neural network for text processing, central to our DSSM, was also described in Collobert et al. (2011) and Shen et al. (2014) but with very differ"
D14-1002,W11-0329,0,0.0767306,"eywords of local feature vectors, the document. Figure 3 presents a sample of document snippets and their keywords detected by the DSSM according to the procedure elaborated in Figure 2. It is interesting to see that many names are identified as keywords although the DSSM is not designed explicitly for named entity recognition. useful to the corresponding tasks, with a manageable vector size. tanh 1 where ces. 3.2 and tanh (3) tanh (4) are learned linear projection matriTraining the DSSM To optimize the parameters of the DSSM of Figure 1, i.e., , , , we use a pair-wise rank loss as objective (Yih et al. 2011). Consider a source document and two candidate target documents and , where is more interesting than to a user when reading . We construct and , , where two pairs of documents , the former is preferred and should have a higher (2) where the max operation is performed for each dimension of across 1, … , respectively. 5 … the comedy festival formerly known as the us comedy arts festival is a comedy festival held each year in las vegas nevada from its 1985 inception to 2008 . it was held annually at the wheeler opera house and other venues in aspen colorado . the primary sponsor of the festival w"
D14-1002,2011.iwslt-evaluation.19,0,\N,Missing
D14-1002,N09-1054,0,\N,Missing
D14-1132,J92-4003,0,0.0612884,"+j , we have: model. We experiment with these variants and extensions: ∂γθT h(f, e) ∂hk (e, f ) = γλk ∂sφ (o, pp) ∂sφ (o, pp) • SparseHRMLocal: This feature set is exclu= γλk N (o, pp, e, f ) sively based on the local phrase-pair and By using the following definition: 1254 consists of features over the first and last word of both the source and target phrase.5 We use four different word representations: The word identity itself, but only for the 80 most common source and target language words. The three other word representations are based on Brown clustering with either 20, 50 or 80 classes (Brown et al., 1992). There is one feature for every orientation type. • SparseHRM: The main feature set of Cherry (2013). This is an extension of SparseHRMLocal adding features based on the first and last word of both the source and the target of the hierarchical block at the top of the stack. There are also features based on the source words in-between the current phrase and the hierarchical block at the top of the stack. • SparseHRM+UncommonWords: This set is identical to SparseHRM, except that wordidentity features are not restricted to the 80 most frequent words, but can be instantiated for all words, regard"
D14-1132,N13-1003,0,0.0676183,"red interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to"
D14-1132,N09-1025,0,0.0205233,"extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to fit millions of parameters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with"
D14-1132,J07-2003,0,0.116682,"ering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in p"
D14-1132,P13-4034,0,0.0438564,"Missing"
D14-1132,D08-1089,1,0.946153,"ingle phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum like"
D14-1132,N04-1035,1,0.812734,"train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work"
D14-1132,P06-1121,1,0.750834,"discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer fea"
D14-1132,N13-1048,1,0.922125,"dels to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, c Octobe"
D14-1132,P14-1066,1,0.937605,"mensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, c October 25-29, 2014, Doh"
D14-1132,W14-3360,0,0.0343077,"t attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1250–1260, c October 25-29, 2014, Doha, Qatar. 2014 Associ"
D14-1132,P12-1031,0,0.116388,"parse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 1"
D14-1132,D11-1125,0,0.0430797,"of features to 3M (SparseHRM+BiPhrases) results in a slightly better average gain of 0.3 BLEU for CLL but but expected BLEU still achieves a much higher improvement of 1.5 BLEU. Because our gains with likelihood training are similar to what Cherry (2013) reported for his maximum entropy model, we conclude that the objective function is the most important factor to achieving good accuracy. 7.4 Comparison to PRO In our final experiment we compare expected BLEU training to pair-wise ranked optimization (PRO), a popular off the shelf trainer for machine translation models with large feature sets (Hopkins and May, 2011).11 Previous work has shown that PRO does not scale to truly large feature sets with millions of types (Yu et al., 2013) and we therefore restrict ourselves to our smallest 11 MIRA is another popular optimizer but as previously mentioned, even the best publicly available implementation does not scale to large training sets (Eidelman et al., 2013). set (SparseHRMLocal) of just over 4.4K features. We train PRO on the development set comprising of 2,525 sentences, a setup that is commonly used by standard machine translation optimizers. In this setting, PRO directly learns weights for the baselin"
D14-1132,N03-1017,0,0.233513,"features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering mode"
D14-1132,P07-2045,0,0.0295763,"ient beam search (Och and Ney, 2004). Early phrase-based models simply relied on a linear distortion feature, which measures the distance between the first word of the current source phrase and the last word of the previous source phrase (Koehn et al., 2003; Och and Ney, 2004). Unfortunately, this approach is agnostic to the actual phrases being reordered, and does not take into account that certain phrases are more likely to be reordered than others. This shortcoming led to a range of lexicalized reordering models that capture exactly those preferences for individual phrases (Tillmann, 2003; Koehn et al., 2007). Reordering models generally assume a sequence of English phrases e = {¯ e1 , . . . , e¯n } currently hypothesized by the decoder, a phrase alignment a = {a1 , . . . , an } that defines a foreign phrase f¯ai for each English phrase e¯i , and an orientation oi which describes how a phrase pair should be reordered with respect to the previous phrases. There are typically three orientation types and the exact definition depends on the specific models which we describe below. Orientations can be determined during decoding and from wordaligned training corpora. Most models estimate a probability d"
D14-1132,P06-1096,0,0.0617731,"Missing"
D14-1132,C12-1121,0,0.0133419,"to be useful. To prevent overfitting, we experimented with `2 regularization, but found that it did not improve test accuracy. We also tuned the probability scaling parameter γ (Eq. 6) but found γ = 1 to be very good among other settings. We evaluate the performance on a held-out validation set during training and stop whenever the objective changes less than a factor of 0.0003. For our PRO experiments, we tuned three hyper-parameters controlling `2 regularization, sentence-level BLEU smoothing, and length. The latter is important to eliminate PRO’s tendency to produce too short translations (Nakov et al., 2012). 7.1 Scaling the Feature Set We first compare our baseline, a likelihood trained hierarchical reordering model (HRM; Galley & Manning, 2008), to various expected BLEU trained models, starting with SparseHRMLocal, inspired by Cherry (2013) and compare it to SparseHRM+BiPhrases, a set that is three orders of magnitudes larger. Our results on French-English translation (Table 1) and German-English translation (Table 2) show that the expected BLEU trained models scale to millions of features and that we outperform the baseline by up to 2.0 BLEU on newstest2012 for French-English and by up to 1.1"
D14-1132,2009.mtsummit-papers.10,0,0.125252,"Missing"
D14-1132,J04-4002,0,0.749151,"s of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maxi"
D14-1132,P03-1021,0,0.208745,"ights, we generate n-best lists for the foreign sentences in the training data using the setup described in the experimental section (§7). The n-best lists serve as an approximation to E(f ), the set of possible translations of f , used in the next step for expected BLEU training of the reordering model (§5). 3. Next, we fix θ, set θm+1 = 1, . . . θm+j = 1 and optimize φ with respect to the loss function on the training data using stochastic gradient descent.2 4. Finally, we fix φ and re-optimize θ in the presence of the discriminative reordering model using Minimum Error Rate Training (MERT; Och 2003; §7). l(φ) = − xBLEU(φ) X pθ,φ (e|f ) sBLEU(e, e(i) ) =− e∈E(f ) pθ,φ (e|f ) = P (6) ∂l(φ) X ∂l(φ) ∂sφ (o, pp) = ∂φ ∂sφ (o, pp) ∂φ o,pp X = −δo,pp u(o, pp) o,pp Expected BLEU Objective Function The expected BLEU objective (Gao and He, 2013; Gao et al., 2014) allows us to efficiently optimize a large scale discriminative reordering model towards the desired task-specific metric, which in our setting is BLEU. 2 exp{γθT h(f, e)} T 0 e0 ∈E(f ) exp{γθ h(f, e )} where θT h(f, e) includes the discriminative reordering model hm+1 (e, f ), . . . , hm+j (e, f ) parameterized by φ, and γ ∈ [0, inf) is a"
D14-1132,P02-1040,0,0.0906386,"llow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Manning (2008), a maximum likelihood-based model trained on millions of sentences to fit millions of parameters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with lit"
D14-1132,W10-1748,0,0.0531366,"eters. Ideally, we would like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natur"
D14-1132,W11-2119,0,0.0180257,"ould like to scale sparse reordering models to similar dimensions but recent attempts to increase the amount of training data for MIRA was met with little success (Eidelman et al., 2013). In this paper we propose much larger sparse ordering models that combine the scalability of likelihood-based approaches with the higher accuracy of maximum BLEU training (§3). We train on the output of a hierarchical reordering model-based system and scale to millions of features learned on hundreds of thousands of sentences (§4). Specifically, we use the expected BLEU objective function (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014; Green et al., 2014) which allows us to train models that use training data and feature sets that are two to three orders of magnitudes larger than in previous work (§5). Our models significantly outperform the state-of-the-art hierarchical lexicalized reordering model on two language pairs and we demonstrate that richer feature sets result in significantly higher accuracy than with a feature set similar to Cherry (2013). We also demonstrate that our 1250 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processi"
D14-1132,N03-2036,0,0.0831567,"ering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering mode"
D14-1132,D08-1065,0,0.0274346,") X ∂l(φ) ∂sφ (o, pp) = ∂φ ∂sφ (o, pp) ∂φ o,pp X = −δo,pp u(o, pp) o,pp Expected BLEU Objective Function The expected BLEU objective (Gao and He, 2013; Gao et al., 2014) allows us to efficiently optimize a large scale discriminative reordering model towards the desired task-specific metric, which in our setting is BLEU. 2 exp{γθT h(f, e)} T 0 e0 ∈E(f ) exp{γθ h(f, e )} where θT h(f, e) includes the discriminative reordering model hm+1 (e, f ), . . . , hm+j (e, f ) parameterized by φ, and γ ∈ [0, inf) is a tuned scaling factor that flattens the distribution for γ < 1 and sharpens it for γ > 1 (Tromble et al., 2008).4 Next, we define the gradient of the expected BLEU loss function l(φ). To simplify our notation we omit the local context c in sφ (o, pp, c) (Eq. 3) from now on and assume it to be part of pp. Using the observation that the loss does not explicitly depend on φ, we get: We found that re-optimizing θ after a few iterations of stochastic gradient descent in step 3 did not improve accuracy. 5 (5) We tuned θm+1 , . . . θm+j on the development set but found that setting them uniformly to one resulted in faster training and equal accuracy. where δo,pp is the error term for orientation o of phrase p"
D14-1132,J97-3002,0,0.339127,"ows us to train a simple linear discriminative reordering model with millions of sparse features on hundreds of thousands of sentences resulting in significant improvements. A comparison to likelihood training demonstrates that expected BLEU is vastly more effective. Our best results improve a hierarchical lexicalized reordering baseline by up to 2.0 BLEU in a single-reference setting on a French-English WMT 2012 setup. 1 Introduction Modeling reordering for phrase-based machine translation has been a long standing problem. Contrary to synchronous context free grammarbased translation models (Wu, 1997; Galley et al., 2004; Galley et al., 2006; Chiang, 2007), phrasebased models (Koehn et al., 2003; Och and Ney, 2004) have no in-built notion of reordering beyond what is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation,"
D14-1132,P06-1066,0,0.283179,"t is captured in a single phrase pair, and the first phrase-based decoders simply scored interphrase reorderings using a restricted linear distortion feature, which scores a phrase reordering proportionally to the length of its displacement. While phrase-based models allow in theory completely unrestricted reordering patterns, movements are generally limited to a finite distance for complexity reasons. To address this limitation, extensive prior work focused on richer feature sets, in particular on lexicalized reordering models trained with maximum likelihood-based approaches (Tillmann, 2003; Xiong et al., 2006; Galley and Manning, 2008; Nguyen et al.,2009;§2). More recently, Cherry (2013) proposed a very effective sparse ordering model relying on a set of only a few thousand indicator features which are trained towards a task-specific metric such as BLEU (Papineni et al., 2002). These features are simply added to the log-linear framework of translation that is trained with the Margin Infused Relaxed Algorithm (MIRA; Chiang et al., 2009) on a small development set of a few thousand sentences. While simple, the approach outperforms the state-of-the-art hierarchical reordering model of Galley and Mann"
D14-1132,D13-1112,0,0.029053,"ill achieves a much higher improvement of 1.5 BLEU. Because our gains with likelihood training are similar to what Cherry (2013) reported for his maximum entropy model, we conclude that the objective function is the most important factor to achieving good accuracy. 7.4 Comparison to PRO In our final experiment we compare expected BLEU training to pair-wise ranked optimization (PRO), a popular off the shelf trainer for machine translation models with large feature sets (Hopkins and May, 2011).11 Previous work has shown that PRO does not scale to truly large feature sets with millions of types (Yu et al., 2013) and we therefore restrict ourselves to our smallest 11 MIRA is another popular optimizer but as previously mentioned, even the best publicly available implementation does not scale to large training sets (Eidelman et al., 2013). set (SparseHRMLocal) of just over 4.4K features. We train PRO on the development set comprising of 2,525 sentences, a setup that is commonly used by standard machine translation optimizers. In this setting, PRO directly learns weights for the baseline features (§7) as well as the 4.4K indicator features corresponding to the sparse reordering model. For expected BLEU t"
D14-1132,N04-4026,0,\N,Missing
D14-1132,W12-3102,0,\N,Missing
D14-1132,2005.iwslt-1.8,0,\N,Missing
D16-1127,D11-1054,1,\N,Missing
D16-1127,W00-0306,0,\N,Missing
D16-1127,P02-1040,0,\N,Missing
D16-1127,P10-1083,0,\N,Missing
D16-1127,P15-1152,0,\N,Missing
D16-1127,P16-1094,1,\N,Missing
D16-1127,D16-1230,0,\N,Missing
D16-1127,P11-1028,0,\N,Missing
D16-1127,P16-1153,1,\N,Missing
D16-1189,P09-1010,0,0.0402293,"al., 2012; Sordoni et al., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity"
D16-1189,P11-1028,0,0.0834282,"l., 2015) inspired significant progress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from t"
D16-1189,P16-1153,1,0.917358,"2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement learning tasks in that the language in both state and action spaces is unconstrained and"
D16-1189,D15-1239,1,0.645903,"rements) of popularity, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), the rate of posts related to a source rumor (Lukasik et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for in the karma score. Of the prior work on Reddit, the task explored here is most similar to (Jaech et al., 2015) in that it involves choosing relatively high karma comments (or threads) from a time-limited set rather than directly predicting comment (or post) karma. Prior work on popularity prediction used supervised learning; this is the first work that frames tracking hot topics in social media with deep reinforcement learning. 4 4.1 Characterizing a combinatorial action space Notatio"
D16-1189,P15-2085,0,0.0167351,"hus, our work is more closely related to popularity prediction for social media and online news. These studies have explored a variety of definitions (or measurements) of popularity, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), the rate of posts related to a source rumor (Lukasik et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for in the karma score. Of the prior work on Reddit, the task explored here is most similar to (Jaech et al., 2015) in that it involves choosing relatively high karma comments (or threads) from a time-limited set rather than directly predicting comment (or post) karma. Prior work on popularity prediction used supervised learning; this"
D16-1189,D15-1001,0,0.0186737,"gress by combining deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement"
D16-1189,D16-1261,0,0.0321309,"t al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement learning tasks in that the language in both state and action spaces is unconstrained and quite rich. Dulac-Arnold et al. (2016) also investigated a problem of large discrete action spaces. A Wolpertinger architecture is proposed to reduce computational complexity of evaluating all actions. While a combinatorial action space can be large and discrete, their method"
D16-1189,N15-1020,1,0.920583,"deep learning with reinforcement learning (Mnih et al., 2015; Silver et al., 2016; Lillicrap et al., 2016; Duan et al., 2016). In natural language processing, reinforcement learning has been applied successfully to dialogue systems that generate natural language and converse with a human user (Scheffler and Young, 2002; Singh et al., 1999; Wen et al., 2016). There has also been interest in mapping text instructions to sequences of executable actions and extracting textual knowledge to improve game control performance (Branavan et al., 2009; Branavan et al., 2011). Recently, Narasimhan et al. (2015) studied the task of text-based games with a deep Q-learning framework. He et al. (2016) proposed to use a separate deep network for handling natural language actions and to model Q-values via state-action interaction. Nogueira and Cho (2016) have also proposed a goal-driven web navigation task for languagebased sequential decision making. Narasimhan et al. (2016) applied reinforcement learning for acquiring and incorporating external evidence to improve information extraction accuracy. The study that we present with Reddit popularity tracking differs from these other text-based reinforcement"
D16-1189,P14-1017,0,0.1357,"l is not to track topics based on frequency, but rather based on reader response. Thus, our work is more closely related to popularity prediction for social media and online news. These studies have explored a variety of definitions (or measurements) of popularity, including: the volume of comments in response to blog posts (Yano and Smith, 2010) and news articles (Tasgkias et al., 2009; Tatar et al., 2011), the number of Twitter shares of news articles (Bandari et al., 2012), the number of reshares on Facebook (Cheng et al., 2014) and retweets on Twitter (Suh et al., 2010; Hong et al., 2011; Tan et al., 2014; Zhao et al., 2015), the rate of posts related to a source rumor (Lukasik et al., 2015), and the difference in the number of reader up and down votes on posts and comments in Reddit discussion forums (Lakkaraju et al., 2013; Jaech et al., 2015). An advantage of working with the Reddit data is that both positive and negative reactions are accounted for in the karma score. Of the prior work on Reddit, the task explored here is most similar to (Jaech et al., 2015) in that it involves choosing relatively high karma comments (or threads) from a time-limited set rather than directly predicting comm"
D16-1238,D15-1159,0,0.523057,"y component that stores continuous embeddings for all headwords. In other words, we consider all possible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005). The consideration 2204 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with a soft headword embedding, which is comput"
D16-1238,P16-1231,0,0.528654,"Missing"
D16-1238,D15-1041,0,0.0751496,"ings for all headwords. In other words, we consider all possible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005). The consideration 2204 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with a soft headword embedding, which is computed as the probability-weighted sum of all head"
D16-1238,D12-1133,0,0.0746592,"et al., 2013) on Chinese Gigawords (Graff et al., 2005). 2209 † and ∗ are provided in (Alberti et al., 2015) and (Andor et al., 2016), respectively. C&M (2014) Dyer et al. (2015) BiAtt-DP Dev UAS LAS 84.0 82.4 87.2 85.9 87.7 85.3 Test UAS LAS 83.9 82.4 87.2 85.7 88.1 85.7 Table 2: Parsing accuracy on CTB dev and test sets. the transition-based parsers, it achieves better accuracy than Chen and Manning (2014), which uses a feed-forward neural network, and Dyer et al. (2015), which uses three stack LSTM networks. Compared with the integrated parsing and tagging models, the BiAtt-DP outperforms Bohnet and Nivre (2012) but has a small gap to Alberti et al. (2015). On CTB, it achieves best UAS and similar LAS. This may be caused by that the relation vocabulary size is relatively smaller than the average sentence length, which biases the joint objective to be more sensitive to UAS. The parsing speed is around 50–60 sents/sec measured on a desktop with Intel Core i7 CPU @ 3.33GHz using single thread. Next, in Table 3 we show the parsing accuracy of the proposed BiAtt-DP on 12 languages in the CoNLL 2006 shared task, including comparison with state-of-the-art parsers. Specifically, we show UAS of the 3rd-order"
D16-1238,W06-2920,0,0.243172,"AS Scores of MSTParsers Language Arabic Bulgarian Czech Danish Dutch German Japanese Portuguese Slovene Spanish Swedish Turkish Average 1st-order 78.30 (2.02) 90.98 (3.00) 86.18 (4.88) 89.84 (1.80) 82.89 (4.54) 89.54 (3.17) 93.38 (0.14) 89.92 (3.17) 82.09 (4.54) 83.79 (4.59) 88.27 (1.95) 74.81 (3.74) 85.83 (2.85) 2nd-order 78.75 (1.57) 91.56 (2.42) 87.30 (3.76) 90.50 (1.14) 84.11 (3.32) 90.14 (2.57) 92.92 (0.60) 91.08 (2.01) 83.25 (3.38) 84.33 (4.05) 89.05 (1.17) 74.39 (4.16) 86.45 (2.23) Table 5: UAS scores of 1st-order and 2-nd order MSTParsers on 12 languages in the CoNLL 2006 shared task (Buchholz and Marsi, 2006). We use the numbers reported in (Lei et al., 2014). Numbers in brackets indicate the absolute improvement of the proposed BiAtt-DP over the MSTParsers. References Here, we use the following definition of squared Hellinger distance for countable space √ 1X √ ( pi − qi )2 2 i where p, q ∈ ∆k are two k-simplexes. Introducing g ∈ ∆k , the squared Hellinger distance can be upper bounded as H 2 (p, q) ≤ inequality defined for a metric, and the CauchySchwarz’s inequality, respectively. Using the relationship between the KL-divergence and the squared Hellinger distance, (8) can be further bounded by"
D16-1238,D14-1082,0,0.880815,"ar order via sequentially querying the memory component that stores continuous embeddings for all headwords. In other words, we consider all possible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005). The consideration 2204 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with"
D16-1238,de-marneffe-etal-2006-generating,0,0.168224,"Missing"
D16-1238,P15-1033,0,0.296271,"s continuous embeddings for all headwords. In other words, we consider all possible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005). The consideration 2204 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with a soft headword embedding, which is computed as the probabili"
D16-1238,D10-1125,0,0.165089,"Missing"
D16-1238,P14-1130,0,0.540772,"berti et al. (2015). On CTB, it achieves best UAS and similar LAS. This may be caused by that the relation vocabulary size is relatively smaller than the average sentence length, which biases the joint objective to be more sensitive to UAS. The parsing speed is around 50–60 sents/sec measured on a desktop with Intel Core i7 CPU @ 3.33GHz using single thread. Next, in Table 3 we show the parsing accuracy of the proposed BiAtt-DP on 12 languages in the CoNLL 2006 shared task, including comparison with state-of-the-art parsers. Specifically, we show UAS of the 3rd-order RBGParser as reported in (Lei et al., 2014) since it also uses low-dimensional continuous embeddings. However, there are several major differences between the RBGParser and the BiAtt-DP. First, in (Lei et al., 2014), the lowdimensional continuous embeddings are derived Language Arabic Bulgarian Czech Danish Dutch German Japanese Portuguese Slovene Spanish Swedish Turkish BiAtt-DP 80.34 [68.58] 93.96 [89.55] 91.16 [85.14] 91.56 [85.53] 87.15 [82.41] 92.71 [89.80] 93.44 [90.67] 92.77 [88.44] 86.01 [75.90] 88.74 [84.03] 90.50 [84.05] 78.43 [66.16] RBGParser 79.95 93.50 90.50 91.39 86.41 91.97 93.71 91.92 86.24 88.00 91.00 76.84 Best Publi"
D16-1238,P14-2050,0,0.0429362,"4 (Czech), 176 (Danish), 220 (Dutch), 200 (German), 128 (Japanese), 168 (Portuguese), 128 (Slovene), 144 (Spanish), 176 (Swedish), and 128 (Turkish). 4.3 Type Results We first compare our parser with state-of-the-art neural transition-based dependency parsers on PTB and CTB. For English, we also compare with stateof-the-art graph-based dependency parsers. The results are shown in Table 1 and Table 2, respectively. It can be seen that the BiAtt-DP outperforms all other graph-based parsers on PTB. Compared with 3 For English, we use the dependency-based word embeddings at https://goo.gl/tWke3I (Levy and Goldberg, 2014). For Chinese, we pre-train 192-dimension skip-gram embeddings (Mikolov et al., 2013) on Chinese Gigawords (Graff et al., 2005). 2209 † and ∗ are provided in (Alberti et al., 2015) and (Andor et al., 2016), respectively. C&M (2014) Dyer et al. (2015) BiAtt-DP Dev UAS LAS 84.0 82.4 87.2 85.9 87.7 85.3 Test UAS LAS 83.9 82.4 87.2 85.7 88.1 85.7 Table 2: Parsing accuracy on CTB dev and test sets. the transition-based parsers, it achieves better accuracy than Chen and Manning (2014), which uses a feed-forward neural network, and Dyer et al. (2015), which uses three stack LSTM networks. Compared wi"
D16-1238,N06-1014,0,0.0587503,"reviously developed uni-directional counterpart, because the former exploits richer contextual information. Intuitively, we can use two separate uni-directional RNNs where each one constructs its respective attended encoder context vectors for computing RNN hidden states. However, the drawback of this approach is that the decoder would often produce different alignments resulting in discrepancies for the forward and backward directions. In this paper, we design a training objective function to enforce attention agreement between both directions, inspired by the alignmentby-agreement idea from Liang et al. (2006). Specifically, we develop a dependency parser (BiAtt-DP) using a bi-directional attention model based on the memory network. Given that the golden alignment is observed for dependency parsing in the training stage, we further derive a simple and interpretable approximation for the agreement objective, which makes a natural connection between the latent and observed alignment cases. The proposed BiAtt-DP parses a sentence in a linear order via sequentially querying the memory component that stores continuous embeddings for all headwords. In other words, we consider all possible arcs during the"
D16-1238,D10-1004,0,0.0192923,"headwords in our model. The model learns to retrieve the most relevant information from the input memory to make decisions on headwords and head-modifier relations. Graph-based Dependency Parsing: In addition to the transition-based parsers, another line of research in dependency parsing uses graph-based models. Graph-based parser usually build a dependency tree from a directed graph and learns to scoring the possible arcs. Due to this nature, nonprojective parsing can be done straightforwardly by most graph-based dependency parsers. The MSTParser (McDonald et al., 2005) and the TurboParser (Martins et al., 2010) are two examples of graphbased parsers. The MSTParser formulates the parsing as searching for the MST, whereas the TurboParser performs approximate variational inference over a factor graph. The RBGParser proposed in (Lei et al., 2014) can also be viewed as a graph-based parser, which scores arcs using low-dimensional continuous features derived from low-rank tensors as well as features used by MSTParser/TurboParser. It also employs a sampler-based algorithm for parsing (Zhang et al., 2014). Neural Attention Model: The proposed BiAttDP is closely related to the memory network (Sukhbaatar et a"
D16-1238,P13-2109,0,0.141805,"Missing"
D16-1238,E06-1011,0,0.745372,"ural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with a soft headword embedding, which is computed as the probability-weighted sum of all headword embeddings in the memory component. To the best of our knowledge, this is the first attempt to apply memory network models to graphbased dependency parsing. Moreover, it is the first extension of neural attention models from unidirection to multi-direction by enforcing"
D16-1238,H05-1066,0,0.657082,"Missing"
D16-1238,W03-3017,0,0.43084,"Missing"
D16-1238,N15-1068,0,0.141622,"Missing"
D16-1238,N12-1054,0,0.0745638,"Missing"
D16-1238,N03-1033,0,0.0797064,"Missing"
D16-1238,P15-1032,0,0.63797,"y querying the memory component that stores continuous embeddings for all headwords. In other words, we consider all possible arcs during the parsing. This formulation is adopted by graph-based parsers such as the MSTParser (McDonald et al., 2005). The consideration 2204 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 2204–2214, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics of all possible arcs makes the proposed BiAtt-DP different from many recently developed neural dependency parsers (Chen and Manning, 2014; Weiss et al., 2015; Alberti et al., 2015; Dyer et al., 2015; Ballesteros et al., 2015), which use a transitionbased algorithm by modeling the parsing procedure as a sequence of actions on buffers. Moreover, unlike most graph-based parsers which may suffer from high computational complexity when utilizing high-order parsing history (McDonald and Pereira, 2006), the proposed BiAtt-DP can implicitly inject such information into the model while keeping the computational complexity in the order of O(n2 ) for a sentence with n words. This is achieved by feeding the RNN in the query component with a soft headword embe"
D16-1238,W03-3023,0,0.371894,". Formally, for the forward case, the qt can ˜ t ; xt ]). Albe computed as qt = GRU (qt−1 , [m though the RNN is able to capture long-span context information to some extent, the local context may very easily dominate the hidden state. Therefore, this additional soft headword embedding allows the model to access long-span context information in a different channel. On the other hand, by recursively feeding both the query vector and the soft headword embedding into the RNN, the model implicitly captures high-order parsing history information, which can potentially improve the parsing accuracy (Yamada and Matsumoto, 2003; McDonald and Pereira, 2006). However, for a graph-based dependency parser, utilizing parsing history features is computationally expensive. For example, an k-th order MSTParser (McDonald and Pereira, 2006) has O(nk+1 ) complexity for a sentence of n words. In contrast, the BiAtt-DP implicitly captures high-order parsing history while keeping the complexity in the order of O(n2 ), i.e. for each direction. we compute n(n+1) pair-wise probabilities at,j for t = 1, · · · , n and j = 0, · · · , n. In this paper, we choose to use soft headword embeddings rather than making hard decisions on headwo"
D16-1238,D08-1059,0,0.28016,"Missing"
D16-1238,D12-1030,0,0.0391693,"Missing"
D16-1238,P14-2107,0,0.13558,"Missing"
D16-1238,D13-1093,0,0.0277992,"Missing"
D16-1238,P14-1019,0,0.032224,"ly by most graph-based dependency parsers. The MSTParser (McDonald et al., 2005) and the TurboParser (Martins et al., 2010) are two examples of graphbased parsers. The MSTParser formulates the parsing as searching for the MST, whereas the TurboParser performs approximate variational inference over a factor graph. The RBGParser proposed in (Lei et al., 2014) can also be viewed as a graph-based parser, which scores arcs using low-dimensional continuous features derived from low-rank tensors as well as features used by MSTParser/TurboParser. It also employs a sampler-based algorithm for parsing (Zhang et al., 2014). Neural Attention Model: The proposed BiAttDP is closely related to the memory network (Sukhbaatar et al., 2015) for question answering, as well as the neural attention models for machine translation (Bahdanau et al., 2015) and constituency parsing (Vinyals et al., 2015b). The way we query the memory component and obtain the soft headword embeddings is essentially the attention mechanism. However, different from the above studies where the alignment information is latent, in dependency parsing, the arc between the modifier and headword is known during training. Thus, we can utilize these labe"
D17-1237,P17-1045,1,0.83128,"we need to book air tickets, reserve a hotel, rent a car, etc. in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints. Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight’s arrival time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and so on. It is common to learn a task-completion dialogue agent using reinforcement learning (RL); see Su et al. (2016); Cuay´ahuitl (2017); Williams et al. (2017); Dhingra et al. (2017) and Li et al. (2017a) for a few recent examples. Compared to these dialogue agents developed for individual domains, the composite task presents additional challenges to commonly used, flat RL approaches such as DQN (Mnih et al., 2015). The first challenge is reward sparsity. Dialogue policy learning for composite tasks requires exploration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory. Thus, the reward signals (usually provided by users at the end of a conversation) are delaye"
D17-1237,W17-5526,0,0.00786429,"Missing"
D17-1237,I17-1074,1,0.409205,"ts, reserve a hotel, rent a car, etc. in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints. Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight’s arrival time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and so on. It is common to learn a task-completion dialogue agent using reinforcement learning (RL); see Su et al. (2016); Cuay´ahuitl (2017); Williams et al. (2017); Dhingra et al. (2017) and Li et al. (2017a) for a few recent examples. Compared to these dialogue agents developed for individual domains, the composite task presents additional challenges to commonly used, flat RL approaches such as DQN (Mnih et al., 2015). The first challenge is reward sparsity. Dialogue policy learning for composite tasks requires exploration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory. Thus, the reward signals (usually provided by users at the end of a conversation) are delayed and sparse. As we"
D17-1237,W11-2033,0,0.125959,"ition (Dietterich, 2000). In this paper, we choose the options framework for its conceptual simplicity and generality (Sutton et al., 1998); more details are found in the next section. Our work is also motivated by hierarchicalDQN (Kulkarni et al., 2016) which integrates hierarchical value functions to operate at different temporal scales. The model achieved superior performance on a complicated ATARI game “Montezuma’s Revenge” with a hierarchical structure. A related but different extension to singledomain dialogues is multi-domain dialogues, where each domain is handled by a separate agent (Lison, 2011; Gasic et al., 2015a,b; Cuay´ahuitl et al., 2016). In contrast to compositedomain dialogues studied in this paper, a conversation in a multi-domain dialogue normally involves one domain, so completion of a task does not require solving sub-tasks in different domains. Consequently, work on multi-domain dialogues focuses on different technical challenges such as transfer learning across different domains (Gasic 2232 et al., 2015a) and domain selection (Cuay´ahuitl et al., 2016). 3 Dialogue Policy Learning Our composite task-completion dialogue agent consists of four components: (1) an LSTMbased"
D17-1237,D15-1199,0,0.0314686,"Missing"
D17-1237,N07-2038,0,0.83697,"A Rule+ Agent requests and informs all the slots in a pre-defined order exhaustedly, and then confirms with the user about the reserved tickets. The average turn of this agent is longer than that of the Rule agent. • A flat RL Agent is trained with a standard flat deep reinforcement learning method (DQN) which learns a flat dialogue policy using extrinsic rewards only. 4.3 User Simulator Training reinforcement learners is challenging because they need an environment to interact with. In the dialogue research community, it is common to use simulated users as shown in Figure 3 for this purpose (Schatzmann et al., 2007; Asri et al., 2016). In this work, we adapted the publiclyavailable user simulator, developed by Li et al. (2016), to the composite task-completion dialogue setting using the human-human conversation data described in Section 4.1.2 During training, the simulator provides the agent with an (extrinsic) reward signal at the end of the dialogue. A dialogue is considered to be successful only when a travel plan is made successfully, and the information provided by the agent satisfies user’s constraints. At the end of each dialogue, the agent receives a positive reward of 2⇤max turn (max turn = 60"
D17-1237,P17-1062,0,0.331325,"r to make a travel plan, we need to book air tickets, reserve a hotel, rent a car, etc. in a collective way so as to satisfy a set of cross-subtask constraints, which we call slot constraints. Examples of slot constraints for travel planning are: hotel check-in time should be later than the flight’s arrival time, hotel check-out time may be earlier than the return flight depart time, the number of flight tickets equals to that of hotel check-in people, and so on. It is common to learn a task-completion dialogue agent using reinforcement learning (RL); see Su et al. (2016); Cuay´ahuitl (2017); Williams et al. (2017); Dhingra et al. (2017) and Li et al. (2017a) for a few recent examples. Compared to these dialogue agents developed for individual domains, the composite task presents additional challenges to commonly used, flat RL approaches such as DQN (Mnih et al., 2015). The first challenge is reward sparsity. Dialogue policy learning for composite tasks requires exploration in a much larger state-action space, and it often takes many more conversation turns between user and agent to fulfill a task, leading to a much longer trajectory. Thus, the reward signals (usually provided by users at the end of a c"
D17-1237,E06-1032,0,\N,Missing
D17-1237,J09-4008,0,\N,Missing
D17-1237,P02-1040,0,\N,Missing
D17-1237,P09-2025,0,\N,Missing
D17-1237,P14-2074,0,\N,Missing
D17-1237,P15-1044,0,\N,Missing
D17-1237,D16-1230,0,\N,Missing
D17-1237,P16-2043,0,\N,Missing
D17-1237,P16-2008,0,\N,Missing
D17-1237,W16-3622,0,\N,Missing
D17-1237,C16-1105,0,\N,Missing
D17-1237,W17-5525,0,\N,Missing
D17-1237,W16-6644,0,\N,Missing
D17-1237,N16-1086,0,\N,Missing
D19-1156,P02-1040,0,0.112419,"task that aims to automatically generate a text description for a given image. The task is fundamental to a wide range of applications, including image retrieval (Rui et al., 1999) and vision language navigation (Wang et al., 2019). Though remarkable progress has been made (Gan et al., 2017; Karpathy and Li, 2015), the automatic evaluation of image captioning systems remains a challenge, particularly with respect to quantifying the generation errors made by these systems (Bernardi et al., 2016). Existing metrics for caption evaluation can be grouped into two categories: 1) rule-based metrics (Papineni et al., 2002; Vedantam et al., 2015) that are based on exact string matching, and 2) learning-based metrics (Cui et al., 2018; Sharif 1 Code is released at SeleenaJM/CapEval. BLEU4: 0.00 CIDEr: 0.05 https://github.com/ et al., 2018) that predict the probability of a testing caption as a human-generated caption by using a learning model. In general, prior work has shown that description adequacy with respect to the ground truth data is a main concern for evaluating text generation systems (Gatt and Krahmer, 2018). Though this aspect has been emphasized by prior work for assessing image captions (Papineni e"
D19-1156,D18-1437,0,0.0319399,"Missing"
D19-1156,W05-0909,0,0.184356,"al., 2015) that are based on exact string matching, and 2) learning-based metrics (Cui et al., 2018; Sharif 1 Code is released at SeleenaJM/CapEval. BLEU4: 0.00 CIDEr: 0.05 https://github.com/ et al., 2018) that predict the probability of a testing caption as a human-generated caption by using a learning model. In general, prior work has shown that description adequacy with respect to the ground truth data is a main concern for evaluating text generation systems (Gatt and Krahmer, 2018). Though this aspect has been emphasized by prior work for assessing image captions (Papineni et al., 2002; Banerjee and Lavie, 2005; Gao et al., 2019), one common limitation of existing metrics is the lack of interpretability to the description errors because existing metrics only provide a composite score for the caption quality. Without fine-grained analysis, the developers may not be able to understand the specific description errors made by their developed captioning systems. To fill this gap, we propose an evaluation method called REO that considers three specific pieces of information for measuring each caption with respect to: 1) Relevance: relevant information of a candidate caption with respect to the ground trut"
D19-1156,P18-3003,0,0.0324909,"Missing"
D19-1156,P14-2074,0,0.0361266,"Missing"
D19-1159,D14-1162,0,0.0821077,"puting top-k rollouts. 3 P RETRAINED LM S AND S TOCHASTIC S AMPLING 1495 • fθx→e : x → e, where x = [x1 , · · · , xL ] is represented as its (contextualized) word embedding form e = [e1 , · · · , eL ], with ei as the representation for word xi ; • fθe→z : e → z t : For each embedded instruction e, we ground its representations as ci,t for state st via neural attention. To handle language variability, one may aggregate features of multiple instructions Ct = {ci,t }M i=1 1 PM into a single joint feature z t = M c .4 i,t i=1 Previous methods in VLN learn e either from pretrained word embeddings (Pennington et al., 2014) which do not take into account word context, or from scratch. As a result, their representations do not capture contextual information within each instruction. More importantly, they tend to overfit the training instructions associated with seen environments, limiting their utility in unseen environments. To remedy these issues, we propose to represent e with contextualized word embeddings produced using large-scale pretrained language models, such as BERT and GPT. Instruction Encoder. The agent’s memory vector ht−1 captures the perception and action history and is used to attend to the instr"
D19-1159,N19-1268,0,0.10248,". gio et al., 2015). Two widely used training strategies are student-forcing and teacher-forcing (described in detail in Section 2.2). It is well-known that the sequence length determines which training strategy is more effective. In the VLN literature, student-forcing has been widely used, as early work (Anderson et al., 2018) used long trajectories (up to 20 steps) with a simple discrete action space. Most recent work, however, has relied on a panoramic action space (Fried et al., 2018) in which most trajectories are only up to seven steps long. In such cases, teacher-forcing is preferable (Tan et al., 2019). Neither strategy is perfect: teacher-forcing has exposure bias, while studentforcing’s random actions can cause an agent to deviate far from the correct path, rendering the original instruction invalid.2 To tackle these challenges, we have developed two techniques to enable the agent to navigate more efficiently. For the first challenge, we leverage the recent large-scale pretrained language models, BERT (Devlin et al., 2019) and GPT (Radford et al., 2018), to improve the agent’s robustness in unseen environments. We show that large-scale language-only pretraining improves generalization in"
D19-1159,N19-1423,0,\N,Missing
D19-1159,N19-1197,1,\N,Missing
D19-1190,I17-2069,0,0.0689783,"different styles into a shared latent space where the ”content” information is preserved and ”style” information is discarded. An adversarial discriminator is used to align the latent spaces of two different styles. However, Yang et al. (2018) point out the difficulty of training an adversarial discriminator and proposed instead the use of language models as discriminator. Like Shen et al. (2017); Yang et al. (2018), we align latent spaces for different styles. However we also align latent spaces encoded by different models (S2S and AE). Stylized response generation is a relatively new task. Akama et al. (2017) use a stylized conversation corpus to fine-tune a conversation model pretrained on a background conversation dataset. However, stylized texts are usually in non-conversational format, as in the present setting. Niu and Bansal (2018) proposed a method that takes the weighted average of the token probability distribution predicted by a S2S trained on background conversational dataset and that predicted by a LM trained on style dataset as the token probability. They observed reduced relevance and attributed this to the fact that the LM was not trained to attend to conversation context and S2S wa"
D19-1190,W14-4012,0,0.0260137,"Missing"
D19-1190,N19-1125,1,0.816761,"oder to promote generation of response for that target speaker. However non-conversational data cannot be used. Luan et al. (2017) applied a multi-task learning approach to utilize non-conversational data. A S2S model, taking in conversational data, and an autoencoder (AE), taking in nonconversational data, share the decoder and are trained alternately. However, Gao et al. (2019b) observed that sharing the decoder may not truly allow S2S and AE to share the latent space, and thus S2S may not fully utilize what is learned by AE. Unlike Li et al. (2016b) using labelled persona IDs, Zhang et al. (2019) have proposed using a self-supervised method to extract persona features from conversation history. This allows modeling persona dynamically, which agrees with the fact that even the same person can speak in different style in different scenarios. Multi-task learning McCann et al. (2018); Liu et al. (2019); Luan et al. (2017); Gao et al. (2019b); Zhang et al. (2017) aggregates the strengths of each specific task, and induces regularization effects (Liu et al., 2019) as the model is trained to learn a more universal representation. However a simple multi-task approach (Luan et al., 2017) may l"
D19-1190,N19-1320,0,0.0371334,"ecific responses from conversational data and non-parallel non-conversation style data. 2) We generalize the S PACE F USION model of (Gao et al., 2019b) to non-parallel data by a new 2 Integrated into Microsoft Icecaps toolkit (Shiv et al., 2019) https://github.com/microsoft/ icecaps. regularization method. 3) We present a visualization analysis that provides intuitive insights into the drawbacks of alternative approaches. 2 Related Work Text style transfer is a related but distinct task. It usually preserves the content (Yang et al., 2018; Hu et al., 2017; Fu et al., 2018; Shen et al., 2017; Gong et al., 2019). In contrast, content of conversational responses in a given context can be semantically diverse. Various approaches have been proposed for non-parallel data setup. Fu et al. (2018) proposed to use separate decoders for different styles and a classifier to measure style strength. Shen et al. (2017) proposed to map texts of two different styles into a shared latent space where the ”content” information is preserved and ”style” information is discarded. An adversarial discriminator is used to align the latent spaces of two different styles. However, Yang et al. (2018) point out the difficulty o"
D19-1190,N16-1014,1,0.955002,"and continuously control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines. 1 1 Introduction A social chatbot designed to establish long-term emotional connections with users must generate responses that not only match the content of user input and context, but also do so in a desired target style (Zhou et al., 2018; Li et al., 2016b; Luan et al., 2016; Gao et al., 2019a). A conversational agent that speaks in a polite, professional tone is likely to facilitate service in customer relationship scenarios; likewise, an agent that sounds like an cartoon character or a superhero can be more engaging in a theme park. The master of response style is also an important step towards humanlike chatbots. As highlighted in social psychology studies (Niederhoffer and Pennebaker, 2002a,b), when two people are talking, they tend to match ∗ Now at Alexa AI, Amazon. An implementation of our model and the scripts to generate the datasets"
D19-1190,P16-1094,1,0.952451,"and continuously control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines. 1 1 Introduction A social chatbot designed to establish long-term emotional connections with users must generate responses that not only match the content of user input and context, but also do so in a desired target style (Zhou et al., 2018; Li et al., 2016b; Luan et al., 2016; Gao et al., 2019a). A conversational agent that speaks in a polite, professional tone is likely to facilitate service in customer relationship scenarios; likewise, an agent that sounds like an cartoon character or a superhero can be more engaging in a theme park. The master of response style is also an important step towards humanlike chatbots. As highlighted in social psychology studies (Niederhoffer and Pennebaker, 2002a,b), when two people are talking, they tend to match ∗ Now at Alexa AI, Amazon. An implementation of our model and the scripts to generate the datasets"
D19-1190,P19-1441,1,0.789342,"ional data, share the decoder and are trained alternately. However, Gao et al. (2019b) observed that sharing the decoder may not truly allow S2S and AE to share the latent space, and thus S2S may not fully utilize what is learned by AE. Unlike Li et al. (2016b) using labelled persona IDs, Zhang et al. (2019) have proposed using a self-supervised method to extract persona features from conversation history. This allows modeling persona dynamically, which agrees with the fact that even the same person can speak in different style in different scenarios. Multi-task learning McCann et al. (2018); Liu et al. (2019); Luan et al. (2017); Gao et al. (2019b); Zhang et al. (2017) aggregates the strengths of each specific task, and induces regularization effects (Liu et al., 2019) as the model is trained to learn a more universal representation. However a simple multi-task approach (Luan et al., 2017) may learn separate representations for each dataset (Gao et al., 2019b). To address this, in previous work (Gao et al., 2019b), we proposed the S PACE F USION model featuring a regularization technique that explicitly encourages alignment of latent spaces for a universal representation. S PACE F USION, however,"
D19-1190,I17-1061,1,0.723732,"ghly correspond to contents and style intensity, respectively, illustrated by examples taken from Table 2. linguistic style of each other, sometime even regardless of their intentions. Achieving this level of performance, however, is challenging. Lacking parallel data in different conversational styles, researchers often resort to what we will term style datasets that are in non-conversational format (e.g. news, novels, blogs). Since the contents and formats of these are quite different from conversation data, existing approaches tend to generate responses that are either less style-specific (Luan et al., 2017) or less context-relevant (Niu and Bansal, 2018). We suggest that this trade-off between appropriateness and style stems from profound differences 1814 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1814–1823, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics between conversation and style datasets in format, style and contents that impede joint learning. One approach has been to combine these only during decoding: Niu and Bansal (2018) t"
D19-1190,P16-2020,0,0.0288575,"control the style level. We demonstrate this method using dialogues from Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, the system generates responses of the targeted style and outperforms competitive baselines. 1 1 Introduction A social chatbot designed to establish long-term emotional connections with users must generate responses that not only match the content of user input and context, but also do so in a desired target style (Zhou et al., 2018; Li et al., 2016b; Luan et al., 2016; Gao et al., 2019a). A conversational agent that speaks in a polite, professional tone is likely to facilitate service in customer relationship scenarios; likewise, an agent that sounds like an cartoon character or a superhero can be more engaging in a theme park. The master of response style is also an important step towards humanlike chatbots. As highlighted in social psychology studies (Niederhoffer and Pennebaker, 2002a,b), when two people are talking, they tend to match ∗ Now at Alexa AI, Amazon. An implementation of our model and the scripts to generate the datasets are available at htt"
D19-1190,Q18-1027,0,0.414647,"ty, respectively, illustrated by examples taken from Table 2. linguistic style of each other, sometime even regardless of their intentions. Achieving this level of performance, however, is challenging. Lacking parallel data in different conversational styles, researchers often resort to what we will term style datasets that are in non-conversational format (e.g. news, novels, blogs). Since the contents and formats of these are quite different from conversation data, existing approaches tend to generate responses that are either less style-specific (Luan et al., 2017) or less context-relevant (Niu and Bansal, 2018). We suggest that this trade-off between appropriateness and style stems from profound differences 1814 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 1814–1823, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics between conversation and style datasets in format, style and contents that impede joint learning. One approach has been to combine these only during decoding: Niu and Bansal (2018) trained two models separately, a Sequence-to-Sequ"
D19-1190,P02-1040,0,0.104322,"Missing"
D19-1190,P19-1539,1,0.883551,"Missing"
D19-1190,P19-3021,1,0.834508,"m Reddit data and two sets of sentences with distinct styles (arXiv and Sherlock Holmes novels). Automatic and human evaluation show that, without sacrificing appropriateness, our system can generate responses in a targeted style and outperforms competitive baselines. Our contribution can be summarized thus: 1) We introduce an end-to-end approach that generates style-specific responses from conversational data and non-parallel non-conversation style data. 2) We generalize the S PACE F USION model of (Gao et al., 2019b) to non-parallel data by a new 2 Integrated into Microsoft Icecaps toolkit (Shiv et al., 2019) https://github.com/microsoft/ icecaps. regularization method. 3) We present a visualization analysis that provides intuitive insights into the drawbacks of alternative approaches. 2 Related Work Text style transfer is a related but distinct task. It usually preserves the content (Yang et al., 2018; Hu et al., 2017; Fu et al., 2018; Shen et al., 2017; Gong et al., 2019). In contrast, content of conversational responses in a given context can be semantically diverse. Various approaches have been proposed for non-parallel data setup. Fu et al. (2018) proposed to use separate decoders for differe"
D19-1220,W05-0909,0,0.528279,"o measure the quality of a generated caption given an image and human-written reference captions (Bernardi et al., 2016). In general, prior solutions to this task can be divided into three groups. First, human evaluation is typically conducted by employing human annotators to assess captions (e.g., via Amazons Mechanical Turk) (Bernardi et al., 2016; Aditya et al., 2015a; Wang et al., 2018b). Second, automatic rule-based evaluation measures assess the similarity between references and generated captions. Many metrics in this group were extended from other related tasks (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004). BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) were initially developed to evaluate machine translation outcomes based on n-gram precision and recall. ROUGE (Lin, 2004) was originally used in text summarization, which measures the overlap of ngrams using recall. Recently, two metrics were specifically built for visual captioning: 1) CIDEr (Vedantam et al., 2015) measures n-gram similarity based on TF-IDF; and 2) SPICE (Anderson et al., 2016) quantifies graph similarity based on the scene graphs built from captions. Overall, these metrics focus on text-level co"
D19-1220,P18-1241,0,0.0188994,"still an insufficiently addressed issue. Since numerous captions with varying quality can be produced by machines, it is important to propose an automatic evaluation metric that is highly consistent with human judges, easy to implement and interpret. Prior evaluation metrics typically considered several aspects, including content relevance, information correctness, grammaticality, and if expressions are human-like (Hodosh et al., 2013; Bernardi et al., 2016). Rule-based metrics inspired by linguistic features such as n-gram overlapping are commonly used to evaluate machine-generated captions (Chen et al., 2018; Karpathy and Li, 2015; Huang et al., 2019). However, these metrics primarily evaluate a candidate caption based on references without taking image content into account, and the possible information loss caused by references may bring biases to the evaluation process. Moreover, the ambiguity inherent to natural language presents a challenge for rule-based met2141 Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing, pages 2141–2152, c Hong Kong, China, November 3–7, 2019. 2019 Associati"
D19-1220,P01-1020,0,0.0651441,"Missing"
D19-1220,P14-2074,0,0.127016,"ed Metrics Given our emphasis on metric interpretability and efficiency, we selected six rule-based metrics that have been widely used to evaluate image captions for comparison. The metrics are BLEU-1, BLEU4, ROUGE-L, METEOR, CIDEr, and SPICE. We use MS COCO evaluation tool5 to implement all metrics. Before testing, the input texts were lowercased and tokenized by using the ptbtokenizer.py script from the same tool package. 4.3 Evaluation Our examination of metric performances mainly focuses on the caption-level correlation with human judgments. Following prior studies (Anderson et al., 2016; Elliott and Keller, 2014), we use Kendall’s tau (τ ) and Spearman’s rho (ρ) rank correlation to evaluate pairwise scores between metrics and human decisions in the Composite and Flickr 8K datasets. Composite Flickr8k τ ρ τ ρ BLEU-1 BLEU-4 ROUGE-L METEOR CIDEr SPICE 0.280 0.205 0.307 0.379 0.378 0.419 0.353 0.352 0.383 0.469 0.472 0.514 0.323 0.138 0.323 0.418 0.439 0.449 0.404 0.387 0.404 0.519 0.542 0.596 Ours RRS WDS TIGEr 0.388 0.433 0.454 0.479 0.526 0.553 0.418 0.464 0.493 0.521 0.572 0.606 Table 1: Caption-level correlation between metrics and human grading scores in Composite and Flickr 8K dataset by using Kend"
D19-1220,D16-1044,0,0.0452865,"applied a deep learning framework to build a joint embedding space for images and texts, and some of these studies train a model with ranking loss (Fang et al., 2015; Lee et al., 2018; Frome et al., 2013) and some use clas2 We find in our experiments that we can simply use the pre-trained model as is to evaluate systems developed on different datasets, and the pre-trained model is very efficient to run. Thus, the cost of computing TIGEr is not much higher than that of computing other traditional metrics. Figure 2: TIGEr framework. sification loss (e.g., softmax function) (Jabri et al., 2016; Fukui et al., 2016). In this work, we take advantage of a state-of-the-art model for imagetext matching (Lee et al., 2018) and propose an automatic evaluation metric for image captioning based on the matching results. Our goal is to capture comprehensive information from input data while also providing an explainable method to assess the quality of an image description. 3 The TIGEr Metric The overall framework of TIGEr is shown in Figure 2. With the assumption that a good machinegenerated caption C should generate a description of an image V like a human would, we compare C against a set of reference captions pr"
D19-1220,P17-1103,0,0.0139068,"es may not fully cover the image content because both references and captions are incomplete and selective translations of image contents made by human judges or automated systems. The ground truth can only be fully revealed by taking the images themselves into account for evaluation. Finally, machine-learned metrics use a trained model to predict the likelihood of a testing caption as a human-generated description (Cui et al., 2018; Dai et al., 2017). Prior studies have applied learning-based evaluation for related text generation tasks, e.g., machine translation (CorstonOliver et al., 2001; Lowe et al., 2017; Kulesza and Shieber, 2004). Most recently, Cui et al. (2018) trained a hybrid neural network model for 2142 caption evaluation based on image and text features. This work mainly focuses on the generation of adversarial data used for model training. Despite improving the consistency with human decisions, this approach may involve high computational cost and lead to overfitting (Gao et al., 2019). Besides, the interpretability of the evaluation is limited due to the black-box nature of end-to-end learning models. An even more serious problem of using machine-learned metrics is the so-called “g"
D19-1220,P02-1040,0,0.106379,"caption evaluation is to measure the quality of a generated caption given an image and human-written reference captions (Bernardi et al., 2016). In general, prior solutions to this task can be divided into three groups. First, human evaluation is typically conducted by employing human annotators to assess captions (e.g., via Amazons Mechanical Turk) (Bernardi et al., 2016; Aditya et al., 2015a; Wang et al., 2018b). Second, automatic rule-based evaluation measures assess the similarity between references and generated captions. Many metrics in this group were extended from other related tasks (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004). BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) were initially developed to evaluate machine translation outcomes based on n-gram precision and recall. ROUGE (Lin, 2004) was originally used in text summarization, which measures the overlap of ngrams using recall. Recently, two metrics were specifically built for visual captioning: 1) CIDEr (Vedantam et al., 2015) measures n-gram similarity based on TF-IDF; and 2) SPICE (Anderson et al., 2016) quantifies graph similarity based on the scene graphs built from captions. Overall, these metr"
D19-1220,P18-1083,1,0.903839,"ectiveness, which 1 Code is released at SeleenaJM/CapEval. https://github.com/ deepens our understanding of the characteristics of different metrics. 2 Related Work Caption Evaluation The goal of image caption evaluation is to measure the quality of a generated caption given an image and human-written reference captions (Bernardi et al., 2016). In general, prior solutions to this task can be divided into three groups. First, human evaluation is typically conducted by employing human annotators to assess captions (e.g., via Amazons Mechanical Turk) (Bernardi et al., 2016; Aditya et al., 2015a; Wang et al., 2018b). Second, automatic rule-based evaluation measures assess the similarity between references and generated captions. Many metrics in this group were extended from other related tasks (Papineni et al., 2002; Banerjee and Lavie, 2005; Lin, 2004). BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005) were initially developed to evaluate machine translation outcomes based on n-gram precision and recall. ROUGE (Lin, 2004) was originally used in text summarization, which measures the overlap of ngrams using recall. Recently, two metrics were specifically built for visual captioning: 1)"
D19-1220,2004.tmi-1.8,0,0.0673708,"ver the image content because both references and captions are incomplete and selective translations of image contents made by human judges or automated systems. The ground truth can only be fully revealed by taking the images themselves into account for evaluation. Finally, machine-learned metrics use a trained model to predict the likelihood of a testing caption as a human-generated description (Cui et al., 2018; Dai et al., 2017). Prior studies have applied learning-based evaluation for related text generation tasks, e.g., machine translation (CorstonOliver et al., 2001; Lowe et al., 2017; Kulesza and Shieber, 2004). Most recently, Cui et al. (2018) trained a hybrid neural network model for 2142 caption evaluation based on image and text features. This work mainly focuses on the generation of adversarial data used for model training. Despite improving the consistency with human decisions, this approach may involve high computational cost and lead to overfitting (Gao et al., 2019). Besides, the interpretability of the evaluation is limited due to the black-box nature of end-to-end learning models. An even more serious problem of using machine-learned metrics is the so-called “gaming of the metric”: if a g"
D19-1254,D17-1087,0,0.0878952,"sfer an MRC model trained in a high-resource domain to other lowresource domains is critical for scalable MRC. While it is difficult to collect annotated questionanswer pairs in a new domain, it is generally feasible to obtain a large amount of unlabeled text in a given domain. In this work, we focus on adapting an MRC model trained in a source domain to other new domains, where only unlabeled passages are available. This domain adaptation issue has been a main challenge in MRC research, and the only existing work that investigated this was the two-stage synthesis network (SynNet) proposed in Golub et al. (2017). Specifically, SynNet first generates pseudo question-answer pairs in the target domain, and then uses the generated data as augmentation to fine-tune a pre-trained MRC model. However, the source-domain labeled data and targetdomain pseudo data are directly combined without considering domain differences (see Figure 1(a), where the two feature distributions in two domains are independently clustered). Directly transfering a model from one domain to another could be counter-effective, or even hurt the performance of the pre-trained model due to domain variance. To achieve effective domain tran"
D19-1254,P19-1407,0,0.033498,"Missing"
D19-1254,Q18-1039,0,0.0344356,"ere N = |S |+ |Tgen |. In order to learn domaininvariant representations from the encoder, we update θe to maximize the loss while updating θc to minimize the loss in an adversarial fashion. The overall objective function is defined as: LC (θe , θc ) = L(θe , θd , θc ) = LD (θe , θd ) − λLC (θe , θc ) , (4) where λ is a trade-off parameter that balances the two terms. To optimize our model, instead of alternately updating the adversaries like in GAN (Goodfellow et al., 2014), we use the gradient-reversal layer (Ganin and Lempitsky, 2015) to jointly optimize all the components, as suggested in Chen et al. (2018). 5 5.1 Experiments Experimental Setting Datasets We validate our proposed method on three benchmarks: SQuAD (Rajpurkar et al., 2514 Dataset Domain SQuAD (v1.1) Wiki NewsQA News MS MARCO (v1) Web Train 87,600 92,549 82,430 Dev Test 10,570 − 5,166 5,165 10,047 9,650 Method EM/F1 SQuAD → NewsQA SAN 36.68/52.79 SynNet + SAN 35.19/49.61 AdaMRC 38.46/54.20 AdaMRC with GT questions 39.37,54.63 NewsQA → SQuAD SAN 56.83/68.62 SynNet + SAN 50.34/62.42 AdaMRC 58.20/69.75 AdaMRC with GT questions 58.82/70.14 SQuAD → MS MARCO (BLEU-1/ROUGE-L) SAN 13.06/25.80 SynNet + SAN 12.52/25.47 AdaMRC 14.09/26.09 Ada"
D19-1254,W14-4012,0,0.0610233,"Missing"
D19-1254,N18-1143,0,0.0201654,"(Trischler et al., 2016) and MS MARCO (Bajaj et al., 2016)). Different from previous work that focused on improving the state of the art on particular MRC datasets, we study the MRC task from a different angle, and aim at addressing a critical yet challenging problem: how to transfer an MRC model learned from a high-resource domain to other lowresource domains in an unsupervised manner. Although important for the MRC task, where annotated data are limited in real-life applications, this problem has not yet been well investigated. There were some relevant studies along this line. For example, Chung et al. (2018) adapted a pretrained model to TOEFL and MCTest dataset, and Wiese et al. (2017) applied transfer learning to the biomedical domain. However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available. To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC. Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains. Our approach is also related t"
D19-1254,N19-1423,0,0.0523789,"o learn domaininvariant representations, which are beneficial for transferring knowledge learned from one domain to another. Based on this, an answer decoder is then used to decode domain-invariant representation into an answer span. The proposed approach is validated on a set of popular benchmarks, including SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MS MARCO (Bajaj et al., 2016), using state-of-the-art MRC models including SAN (Liu et al., 2018) and BiDAF (Seo et al., 2017). Since pre-trained large-scale language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have shown strong performance to learn representations that are generalizable to various tasks, in this work, to further demonstrate the versatility of the proposed model, we perform additional experiments to demonstrate that AdaMRC can also be combined with ELMo and BERT to further boost the performance. The main contributions of this paper are summarized as follows: (i) We propose AdaMRC, an adversarial domain adaptation framework that is specifically designed for MRC. (ii) We perform comprehensive evaluations on several benchmarks, demonstrating that the proposed method is generalizable t"
D19-1254,P17-1123,0,0.067565,"Missing"
D19-1254,D17-1090,0,0.0605507,"Missing"
D19-1254,I17-2072,0,0.0160204,"ansiting the feature distribution from the source domain to the target domain (Gong et al., 2012; Long et al., 2015). Another school of research focuses on learning domain-invariant representations (Glorot et al., 2011) (e.g., via adversarial learning (Ganin et al., 2016; Tzeng et al., 2017)). Domain adaptation has been successfully applied to many tasks, such as image classification (Tzeng et al., 2017), speech recognition (Doulaty et al., 2015), sentiment classification (Ganin et al., 2016; Li et al., 2017), machine translation (Johnson et al., 2017; Zoph et al., 2016), relation extraction (Fu et al., 2017), and paraphrase identification (Shah et al., 2018). Compared to these areas, the application to MRC presents additional challenges, since besides missing labeled data (i.e., answer spans), the questions in the target domain are also unavailable. To our best knowledge, we are the first to investigate the usage of adversarial domain adaptation for the MRC task. There are many prevailing unsupervised techniques for domain adaptation. Our proposed approach is inspired by the seminal work of Ganin et al. (2016) to validate its potential of solving domain adaptation problem on a new task, without a"
D19-1254,P16-1154,0,0.0127507,"d both methods, and empirically observed that a simple NER system provides more robust results, which is used in our experiments. Now, we describe how the question generation (QG) model is trained. Given the passage ps = (p1 , p2 , ..., pT ) and answer as = (astart , aend ) from the source domain, the QG model with parameter θQG learns the conditional probability of generating a question q s = (q1 , q2 , ..., qT 0 ), i.e., P (q s |ps , as ). We implement the QG model as a sequence-to-sequence model with attention mechanism (Bahdanau et al., 2015), and also apply the copy mechanism proposed in Gu et al. (2016); Gulcehre et al. (2016) to handle rare/unknown words. Specifically, the QG model consists of a lexicon encoding layer, a BiLSTM contextual encoding layer, and an LSTM decoder. For lexicon encoding, each word token pi of a passage is mapped into a concatenation of GloVe vectors (Pennington et al., 2014), part-of-speech (POS) tagging embedding, and named-entity-recognition (NER) embedding. We further insert answer information by appending an additional zero/one feature (similar to Yang et al. (2017)) to model the appearance of answer tokens in the passage. The output of the lexicon encoding lay"
D19-1254,P16-1014,0,0.0269573,"d empirically observed that a simple NER system provides more robust results, which is used in our experiments. Now, we describe how the question generation (QG) model is trained. Given the passage ps = (p1 , p2 , ..., pT ) and answer as = (astart , aend ) from the source domain, the QG model with parameter θQG learns the conditional probability of generating a question q s = (q1 , q2 , ..., qT 0 ), i.e., P (q s |ps , as ). We implement the QG model as a sequence-to-sequence model with attention mechanism (Bahdanau et al., 2015), and also apply the copy mechanism proposed in Gu et al. (2016); Gulcehre et al. (2016) to handle rare/unknown words. Specifically, the QG model consists of a lexicon encoding layer, a BiLSTM contextual encoding layer, and an LSTM decoder. For lexicon encoding, each word token pi of a passage is mapped into a concatenation of GloVe vectors (Pennington et al., 2014), part-of-speech (POS) tagging embedding, and named-entity-recognition (NER) embedding. We further insert answer information by appending an additional zero/one feature (similar to Yang et al. (2017)) to model the appearance of answer tokens in the passage. The output of the lexicon encoding layer is appended with CoVe"
D19-1254,Q17-1024,0,0.040402,"2015). One line of research on domain adaptation focuses on transiting the feature distribution from the source domain to the target domain (Gong et al., 2012; Long et al., 2015). Another school of research focuses on learning domain-invariant representations (Glorot et al., 2011) (e.g., via adversarial learning (Ganin et al., 2016; Tzeng et al., 2017)). Domain adaptation has been successfully applied to many tasks, such as image classification (Tzeng et al., 2017), speech recognition (Doulaty et al., 2015), sentiment classification (Ganin et al., 2016; Li et al., 2017), machine translation (Johnson et al., 2017; Zoph et al., 2016), relation extraction (Fu et al., 2017), and paraphrase identification (Shah et al., 2018). Compared to these areas, the application to MRC presents additional challenges, since besides missing labeled data (i.e., answer spans), the questions in the target domain are also unavailable. To our best knowledge, we are the first to investigate the usage of adversarial domain adaptation for the MRC task. There are many prevailing unsupervised techniques for domain adaptation. Our proposed approach is inspired by the seminal work of Ganin et al. (2016) to validate its potential of"
D19-1254,N15-1092,1,0.839052,", and Wiese et al. (2017) applied transfer learning to the biomedical domain. However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available. To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC. Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains. Our approach is also related to multi-task learning (Xu et al., 2019; Caruana, 1997; Liu et al., 2015, 2019) and semisupervised learning (Yang et al., 2017) for MRC. 2511 Loss LD Decoder Loss LC Answerstart, Answerend Domain label Answer Module Domain Classiﬁer Discriminator Pseudo Question Encoder Source Domain Question Generator Encoder Target Domain Cross Attention Cross Attention Attention-based Decoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Document Question Document Pseudo Question Document & Answer Figure 2: Illustration of the proposed AdaMRC model for unsupervised domain ad"
D19-1254,P19-1441,1,0.878178,"Missing"
D19-1254,P18-1157,1,0.40104,"y trained using adversarial learning to enforce domain-invariant representation learning. Comprehensive evaluations demonstrate that our approach (i) is generalizable to different MRC models and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semisupervised learning. 1 3 Introduction Recently, many neural network models have been developed for Machine Reading Comprehension (MRC), with performance comparable to human in specific settings (Gao et al., 2019). However, most state-of-the-art models (Seo et al., 2017; Liu et al., 2018; Yu et al., 2018) rely on large amount of human-annotated in-domain data to achieve the desired performance. Although there exists a number of large-scale MRC datasets (Rajpurkar et al., 2016; Trischler et al., 2016; Bajaj et al., 2016; Zhang et al., 2018), collecting such highquality datasets is expensive and time-consuming, which hinders real-world applications for domainspecific MRC. ∗ Most of this work was done when the first author was an intern at Microsoft Dynamics 365 AI Research. Therefore, the ability to transfer an MRC model trained in a high-resource domain to other lowresource do"
D19-1254,N19-1271,1,0.809712,"odel to TOEFL and MCTest dataset, and Wiese et al. (2017) applied transfer learning to the biomedical domain. However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available. To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC. Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains. Our approach is also related to multi-task learning (Xu et al., 2019; Caruana, 1997; Liu et al., 2015, 2019) and semisupervised learning (Yang et al., 2017) for MRC. 2511 Loss LD Decoder Loss LC Answerstart, Answerend Domain label Answer Module Domain Classiﬁer Discriminator Pseudo Question Encoder Source Domain Question Generator Encoder Target Domain Cross Attention Cross Attention Attention-based Decoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Document Question Document Pseudo Question Document & Answer Figure 2: Illustration of the proposed AdaMRC"
D19-1254,D14-1162,0,0.0835287,"n, the QG model with parameter θQG learns the conditional probability of generating a question q s = (q1 , q2 , ..., qT 0 ), i.e., P (q s |ps , as ). We implement the QG model as a sequence-to-sequence model with attention mechanism (Bahdanau et al., 2015), and also apply the copy mechanism proposed in Gu et al. (2016); Gulcehre et al. (2016) to handle rare/unknown words. Specifically, the QG model consists of a lexicon encoding layer, a BiLSTM contextual encoding layer, and an LSTM decoder. For lexicon encoding, each word token pi of a passage is mapped into a concatenation of GloVe vectors (Pennington et al., 2014), part-of-speech (POS) tagging embedding, and named-entity-recognition (NER) embedding. We further insert answer information by appending an additional zero/one feature (similar to Yang et al. (2017)) to model the appearance of answer tokens in the passage. The output of the lexicon encoding layer is appended with CoVe vectors (McCann et al., 2017), and then passed to the Bidirectional LSTM contextual encoding layer, producing a sequence of hidden states. The decoder is another LSTM with attention and copy mechanism over the encoder hidden states. At each time step, the generation probability"
D19-1254,N18-1202,0,0.0161392,"way, the encoder is enforced to learn domaininvariant representations, which are beneficial for transferring knowledge learned from one domain to another. Based on this, an answer decoder is then used to decode domain-invariant representation into an answer span. The proposed approach is validated on a set of popular benchmarks, including SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2016), and MS MARCO (Bajaj et al., 2016), using state-of-the-art MRC models including SAN (Liu et al., 2018) and BiDAF (Seo et al., 2017). Since pre-trained large-scale language models, such as ELMo (Peters et al., 2018) and BERT (Devlin et al., 2019), have shown strong performance to learn representations that are generalizable to various tasks, in this work, to further demonstrate the versatility of the proposed model, we perform additional experiments to demonstrate that AdaMRC can also be combined with ELMo and BERT to further boost the performance. The main contributions of this paper are summarized as follows: (i) We propose AdaMRC, an adversarial domain adaptation framework that is specifically designed for MRC. (ii) We perform comprehensive evaluations on several benchmarks, demonstrating that the pro"
D19-1254,D16-1264,0,0.766718,"dels and datasets, (ii) can be combined with pre-trained large-scale language models (such as ELMo and BERT), and (iii) can be extended to semisupervised learning. 1 3 Introduction Recently, many neural network models have been developed for Machine Reading Comprehension (MRC), with performance comparable to human in specific settings (Gao et al., 2019). However, most state-of-the-art models (Seo et al., 2017; Liu et al., 2018; Yu et al., 2018) rely on large amount of human-annotated in-domain data to achieve the desired performance. Although there exists a number of large-scale MRC datasets (Rajpurkar et al., 2016; Trischler et al., 2016; Bajaj et al., 2016; Zhang et al., 2018), collecting such highquality datasets is expensive and time-consuming, which hinders real-world applications for domainspecific MRC. ∗ Most of this work was done when the first author was an intern at Microsoft Dynamics 365 AI Research. Therefore, the ability to transfer an MRC model trained in a high-resource domain to other lowresource domains is critical for scalable MRC. While it is difficult to collect annotated questionanswer pairs in a new domain, it is generally feasible to obtain a large amount of unlabeled text in a gi"
D19-1254,N18-1058,0,0.0443019,"S and the target domain dataset Tgen . The goal of the decoder θd is to predict P (a|p, q). The objective function is denoted as: 1 P|S| LD (θe , θd ) = log P (a(i) |p(i) , q (i) ) , |S |i=1 (2) where the superscript (i) indicates the i-th sample. It is worthwhile to emphasize that unlike Golub et al. (2017), we only use source domain data to update the decoder, without using pseudo target domain data. This is because the synthetic question-answer pairs could be noisy, and directly using such data for decoder training may lead to degraded performance of the answer module, as observed both in Sachan and Xing (2018) and in our experiments. The synthetic target domain data and source domain data are both used to update the encoder θe and the domain classifier θc . The classifier predicts a domain label d given the feature representation from the encoder. The objective function is: 1 PN log P (d(i) |p(i) , q (i) ) , (3) N i=1 where N = |S |+ |Tgen |. In order to learn domaininvariant representations from the encoder, we update θe to maximize the loss while updating θc to minimize the loss in an adversarial fashion. The overall objective function is defined as: LC (θe , θc ) = L(θe , θd , θc ) = LD (θe , θd"
D19-1254,P18-1156,0,0.0711302,"Missing"
D19-1254,P17-1096,0,0.190327,"o the biomedical domain. However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available. To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC. Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains. Our approach is also related to multi-task learning (Xu et al., 2019; Caruana, 1997; Liu et al., 2015, 2019) and semisupervised learning (Yang et al., 2017) for MRC. 2511 Loss LD Decoder Loss LC Answerstart, Answerend Domain label Answer Module Domain Classiﬁer Discriminator Pseudo Question Encoder Source Domain Question Generator Encoder Target Domain Cross Attention Cross Attention Attention-based Decoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Lexicon & Contextual Encoding Document Question Document Pseudo Question Document & Answer Figure 2: Illustration of the proposed AdaMRC model for unsupervised domain adaptation of MRC. In this work, we focus on purely unsup"
D19-1254,D16-1163,0,0.0257237,"search on domain adaptation focuses on transiting the feature distribution from the source domain to the target domain (Gong et al., 2012; Long et al., 2015). Another school of research focuses on learning domain-invariant representations (Glorot et al., 2011) (e.g., via adversarial learning (Ganin et al., 2016; Tzeng et al., 2017)). Domain adaptation has been successfully applied to many tasks, such as image classification (Tzeng et al., 2017), speech recognition (Doulaty et al., 2015), sentiment classification (Ganin et al., 2016; Li et al., 2017), machine translation (Johnson et al., 2017; Zoph et al., 2016), relation extraction (Fu et al., 2017), and paraphrase identification (Shah et al., 2018). Compared to these areas, the application to MRC presents additional challenges, since besides missing labeled data (i.e., answer spans), the questions in the target domain are also unavailable. To our best knowledge, we are the first to investigate the usage of adversarial domain adaptation for the MRC task. There are many prevailing unsupervised techniques for domain adaptation. Our proposed approach is inspired by the seminal work of Ganin et al. (2016) to validate its potential of solving domain adap"
D19-1254,D18-1131,0,0.0333913,"domain to the target domain (Gong et al., 2012; Long et al., 2015). Another school of research focuses on learning domain-invariant representations (Glorot et al., 2011) (e.g., via adversarial learning (Ganin et al., 2016; Tzeng et al., 2017)). Domain adaptation has been successfully applied to many tasks, such as image classification (Tzeng et al., 2017), speech recognition (Doulaty et al., 2015), sentiment classification (Ganin et al., 2016; Li et al., 2017), machine translation (Johnson et al., 2017; Zoph et al., 2016), relation extraction (Fu et al., 2017), and paraphrase identification (Shah et al., 2018). Compared to these areas, the application to MRC presents additional challenges, since besides missing labeled data (i.e., answer spans), the questions in the target domain are also unavailable. To our best knowledge, we are the first to investigate the usage of adversarial domain adaptation for the MRC task. There are many prevailing unsupervised techniques for domain adaptation. Our proposed approach is inspired by the seminal work of Ganin et al. (2016) to validate its potential of solving domain adaptation problem on a new task, without any supervision for the target domain. There are als"
D19-1254,D18-1427,0,0.0363646,"Missing"
D19-1254,W17-2309,0,0.0187619,"vious work that focused on improving the state of the art on particular MRC datasets, we study the MRC task from a different angle, and aim at addressing a critical yet challenging problem: how to transfer an MRC model learned from a high-resource domain to other lowresource domains in an unsupervised manner. Although important for the MRC task, where annotated data are limited in real-life applications, this problem has not yet been well investigated. There were some relevant studies along this line. For example, Chung et al. (2018) adapted a pretrained model to TOEFL and MCTest dataset, and Wiese et al. (2017) applied transfer learning to the biomedical domain. However, both studies assumed that annotated data in the target domain (either questions or question-answer pairs) are available. To the best of our knowledge, SynNet (Golub et al., 2017) is the only work that also studied domain adaptation for MRC. Compared with SynNet, the key difference in our model is adversarial learning, which enables domain-invariant representation learning for better model adaptation to low-resource domains. Our approach is also related to multi-task learning (Xu et al., 2019; Caruana, 1997; Liu et al., 2015, 2019) a"
D19-1407,N19-1021,1,0.702936,"nst learning an optimal generative model, as its parameters may be always updated based on sub-optimal posteriors (Kim et al., 2018). The second reason is the so-called posterior collapse issue, which occurs when learning VAEs with an auto-regressive decoder (Bowman et al., 2015). It produces undesirable outcomes: the encoder yields meaningless posteriors that are very close to the prior, while the decoder tends to ignore the latent codes in generation (Bowman et al., 2015). Several attempts have been made to alleviate this issue (Bowman et al., 2015; Higgins et al., 2017; Zhao et al., 2017a; Fu et al., 2019; He et al., 2019). These two seemingly unrelated issues are studied independently. In this paper, we argue that the posterior collapse issue is partially due to the restrictive Gaussian assumption, as it limits the optimization space of the encoder/decoder in a given distribution family. (i) To break the assumption, we propose to use sample-based representations for natural language, thus leading to implicit latent features. Such a representation is much more expressive than Gaussian-based posteriors. (ii) This implicit representation allows us to extend VAE and develop new LVM that further m"
D19-1407,P13-2121,0,0.0830814,"Missing"
D19-1407,E17-2068,0,0.0269343,"modeling. Reviews with user rating above three are considered positive, and those below three are considered negative. The pre-processing allows sentiment analysis on sentence level with feasible sentiment, ending up with shorter sentences with each at most 15 words than those in language modeling. Finally, we get two sets of unaligned reviews: 250K negative sentences, and 350K positive ones. Other dataset details are shown in Appendix C.2.1. Evaluation Metrics. (1) Acc: the accuracy of transferring sentences into another sentiment measured by an automatic classifier: the “fasttext” library (Joulin et al., 2017); (2) BLEU: the consistency between the transferred text and the original; (3) PPL: the reconstruction perplexity of original sentences without altering sentiment; (4) RPPL: Input: ARAE: iVAEMI : it was super dry and had a weird taste to the entire slice . it was super nice and the owner was super sweet and helpful . it was super tasty and a good size with the best in the burgh . Input: ARAE: iVAEMI : so i only had half of the regular fries and my soda . it ’s the best to eat and had a great meal . so i had a huge side and the price was great . Input: ARAE: iVAEMI : i am just not a fan of this"
D19-1407,D17-1230,0,0.522024,"me in language VAEs has been first used in (Bowman et al., 2015). An effective cyclical KL annealing schedule is used in (Fu et al., 2019), where the KL annealing process is repeated multiple times. KL term weighting scheme is also adopted in βVAE (Higgins et al., 2017) for disentanglement. On model architecture side, dilated CNN was considered to replace auto-regressive LSTMs for decoding (Yang et al., 2017). The bag-of-word auxiliary 4.2 Implicit Feature Learning Sample-based distributions, as well as implicit features, have been widely used in representation learning (Donahue et al., 2017; Li et al., 2017a). Vanilla autoencoders learn point masses of latent features rather than their distributions. Adversarial variational Bayes introduces an auxiliary discriminator network like GANs (Goodfellow et al., 2014; Makhzani et al., 2015) to learn almost arbitrarily distributed latent variables (Mescheder et al., 2017; Pu et al., 2017b). We explore the similar spirit in the natural language processing (NLP) domain. Amortized MCMC and particle based methods are introduced for LVM learning in (Li et al., 2017d; Pu et al., 2017a; Chen et al., 2018). Coupled variational Bayes (Dai et al., 2018) emphasizes"
D19-1407,I17-1099,0,0.347452,"me in language VAEs has been first used in (Bowman et al., 2015). An effective cyclical KL annealing schedule is used in (Fu et al., 2019), where the KL annealing process is repeated multiple times. KL term weighting scheme is also adopted in βVAE (Higgins et al., 2017) for disentanglement. On model architecture side, dilated CNN was considered to replace auto-regressive LSTMs for decoding (Yang et al., 2017). The bag-of-word auxiliary 4.2 Implicit Feature Learning Sample-based distributions, as well as implicit features, have been widely used in representation learning (Donahue et al., 2017; Li et al., 2017a). Vanilla autoencoders learn point masses of latent features rather than their distributions. Adversarial variational Bayes introduces an auxiliary discriminator network like GANs (Goodfellow et al., 2014; Makhzani et al., 2015) to learn almost arbitrarily distributed latent variables (Mescheder et al., 2017; Pu et al., 2017b). We explore the similar spirit in the natural language processing (NLP) domain. Amortized MCMC and particle based methods are introduced for LVM learning in (Li et al., 2017d; Pu et al., 2017a; Chen et al., 2018). Coupled variational Bayes (Dai et al., 2018) emphasizes"
D19-1407,D16-1230,0,0.0349397,"51 0.070 0.112 0.245 0.052 0.102 0.413 Dataset: Dailydialog 0.270 0.265 0.341 0.270 0.222 0.278 0.270 0.242 0.306 0.907 0.923 0.948 0.495 0.543 0.578 0.774 0.811 0.846 0.747 0.938 0.830 0.806 0.973 0.940 0.075 0.177 0.327 0.081 0.222 0.583 iVAEMI 0.427 0.254 0.319 0.930 0.670 0.900 0.828 0.692 0.391 0.668 0.355 0.239 0.285 0.951 0.609 0.872 0.897 0.975 0.501 0.868 Table 8: Dialog response generation on two datasets. Evaluation Metrics. We adopt several widely used numerical metrics to systematically evaluate the response generation, including BLEU score (Papineni et al., 2002), BOW Embedding (Liu et al., 2016) and Distinct (Li et al., 2015), as used in (Gu et al., 2018). For each testing context, we sample 10 responses from each model. Baseline. We benchmark representative baselines and state-of-the-art approaches, include: SeqGAN, a GAN based model for sequence generation (Li et al., 2017b); CVAE baseline (Zhao et al., 2017b); dialogue WAE, a conditional Wasserstein auto-encoder for response generation (Gu et al., 2018). 3953 • BLEU measures how much a generated response contains n-gram overlaps with the references. We compute 4-gram BLEU. For each test context, we sample 10 responses from the mod"
D19-1407,J93-2004,0,0.0680815,"per presents the first to effectively apply implicit feature representations, to NLP. 3949 Dataset PTB Yahoo Yelp num. sents. min. len. max. len. avg. len. 42,000 2 78 21.9 100,000 21 201 79.9 100,000 21 201 96.7 Methods VAE β(0.5)-VAE SA-VAE Cyc-VAE iVAE iVAEMI Table 1: Statistics of datasets for language modeling. 5 Experiments In this section, the effectiveness of our methods is validated by largely producing state-of-the-art metrics on a broad range of text generation tasks under various scenarios. 5.1 Language Modeling Datasets. We consider three public datasets, the Penn Treebank (PTB) (Marcus et al., 1993; Bowman et al., 2015), Yahoo, and Yelp corpora (Yang et al., 2017; He et al., 2019). PTB is a relatively small dataset with sentences of varying lengths, whereas Yahoo and Yelp contain larger amounts of with long sentences. Detailed statistics of these datasets are shown in Table 1. VAE β(0.4)-VAE SA-VAE Lag-VAE iVAE iVAEMI VAE β(0.4)-VAE SA-VAE Lag-VAE iVAE iVAEMI Evaluation metrics. Two categories of metrics are used to study VAEs for language modeling: • To characterize the modeling ability of the observed sentences, we use the negative ELBO MI↑ AU↑ 0.8 3.1 0.7 1.8 3.5 12.2 2 5 2 5 32 32 0"
D19-1407,P02-1040,0,0.103669,"0.887 0.705 0.803 0.713 0.521 0.415 0.651 0.070 0.112 0.245 0.052 0.102 0.413 Dataset: Dailydialog 0.270 0.265 0.341 0.270 0.222 0.278 0.270 0.242 0.306 0.907 0.923 0.948 0.495 0.543 0.578 0.774 0.811 0.846 0.747 0.938 0.830 0.806 0.973 0.940 0.075 0.177 0.327 0.081 0.222 0.583 iVAEMI 0.427 0.254 0.319 0.930 0.670 0.900 0.828 0.692 0.391 0.668 0.355 0.239 0.285 0.951 0.609 0.872 0.897 0.975 0.501 0.868 Table 8: Dialog response generation on two datasets. Evaluation Metrics. We adopt several widely used numerical metrics to systematically evaluate the response generation, including BLEU score (Papineni et al., 2002), BOW Embedding (Liu et al., 2016) and Distinct (Li et al., 2015), as used in (Gu et al., 2018). For each testing context, we sample 10 responses from each model. Baseline. We benchmark representative baselines and state-of-the-art approaches, include: SeqGAN, a GAN based model for sequence generation (Li et al., 2017b); CVAE baseline (Zhao et al., 2017b); dialogue WAE, a conditional Wasserstein auto-encoder for response generation (Gu et al., 2018). 3953 • BLEU measures how much a generated response contains n-gram overlaps with the references. We compute 4-gram BLEU. For each test context, w"
D19-1407,D14-1162,0,0.0830053,"Missing"
D19-1407,P18-1101,0,0.171967,"e, i.e. we first sample x from dataset and then sample z ∼ qφ (z|x). In (11), variational posterior is regularized as a whole qφ (z), encouraging posterior samples from different sentences to cooperate to satisfy the objective. It implies a solution that each sentence is represented as a local region in the latent space, the aggregated representation of all sentences match the prior; This avoids the degenerated solution from (3) that the feature representation of individual sentence spans over the whole space. Connection to mutual information The proposed latent variable model coincides with (Zhao et al., 2018, 2017a) where mutual information is introduced into the optimization, based on the following decomposition result (Please see detailed 3948 proof in Appendix A): −KL (qφ (z) k p(z)) = I(x, z) − Ex KL (qφ (z|x) k p(z)) , where I(x, z) is the mutual information between z and x under the joint distribution qφ (x, z) = q(x)qφ (z|x). Therefore, the objective in (11) also maximizes the mutual information between individual sentences and their latent features. We term the new LVM objective as iVAEMI : LiVAEMI = Ex∼D Ez∼qφ (z|x) logpθ (x|z) − KL (qφ (z) k p(z)) (12) Training scheme Note that the aggr"
D19-1407,P17-1061,0,0.238687,"osterior collapse” issue. We demonstrate the effectiveness and versatility of our models in various text generation scenarios, including language modeling, unaligned style transfer, and dialog response generation. The source code to reproduce our experimental results is available on GitHub1 . 1 Introduction Deep latent variable models (LVM) such as variational auto-encoder (VAE) (Kingma and Welling, 2013; Rezende et al., 2014) are successfully applied for many natural language processing tasks, including language modeling (Bowman et al., 2015; Miao et al., 2016), dialogue response generation (Zhao et al., 2017b), controllable text generation (Hu et al., 2017) and neural machine translation (Shah and Barber, 2018) etc. One advantage of VAEs is the flexible distribution-based latent representation. It captures holistic properties of input, such as style, topic, and high-level linguis1 https://github.com/fangleai/ Implicit-LVM tic/semantic features, which further guide the generation of diverse and relevant sentences. However, the representation capacity of VAEs is restrictive due to two reasons. The first reason is rooted in the assumption of variational posteriors, which usually follow spherical Gau"
D19-6002,N16-1098,0,0.0565833,"Missing"
D19-6002,P02-1014,0,0.333723,"t “An object cannot fit in a container because either the object (trophy) is too big or the container (suitcase) is too small.” In this paper, we study two classic commonsense reasoning tasks: the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problem (PDP) (Levesque et al., 2011; Davis and Marcus, 2015). Both tasks are formulated as an anaphora resolution problem, which is a form of co-reference resolution, where a machine (AI agent) must identify the antecedent of an ambiguous pronoun in a statement. WSC and PDP differ from other co-reference resolution tasks (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016) in that commonsense knowledge, which cannot be explicitly decoded from the given text, is needed to solve the problem, as illustrated in the examples in Table 1. Comparing with other commonsense reasoning tasks, such as COPA (Roemmele et al., 2011), Story Cloze Test (Mostafazadeh et al., 2016), Event2Mind (Rashkin et al., 2018), SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker (Levesque et al., 2011), 13 Proceedings of the First Workshop on Commonsense In"
D19-6002,P19-1459,0,0.0608768,"Missing"
D19-6002,W18-4105,0,0.166059,"model trained on raw text from books and news to calculate the probabilities of the natural language sentences which are constructed from a statement by replacing the to-be-resolved pronoun in the statement with each of its candidate references (antecedent), and then pick the candidate with the highest probability as the answer. Kocijan et al. (2019) showed that a significant improvement can be achieved by fine-tuning a pre-trained masked language model (BERT in their case) on a small amount of WSC labeled data. The second category of models are semantic similarity models. Wang et al. (2019); Opitz and Frank (2018) formulated WSC and PDP as a semantic matching problem, and proposed to use two variations of the Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to compute the semantic similarity score between each candidate antecedent and the pronoun by (1) mapping the candidate and the pronoun and their context into two vectors, respectively, in a hidden space using deep neural networks, and (2) computing cosine similarity between the two vectors. The candidate with the highest score is selected as the result. The two categories of models use different inductive biases when predicting outputs"
D19-6002,P19-1478,0,0.221553,"Missing"
D19-6002,D16-1038,0,0.0233878,"it in a container because either the object (trophy) is too big or the container (suitcase) is too small.” In this paper, we study two classic commonsense reasoning tasks: the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problem (PDP) (Levesque et al., 2011; Davis and Marcus, 2015). Both tasks are formulated as an anaphora resolution problem, which is a form of co-reference resolution, where a machine (AI agent) must identify the antecedent of an ambiguous pronoun in a statement. WSC and PDP differ from other co-reference resolution tasks (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016) in that commonsense knowledge, which cannot be explicitly decoded from the given text, is needed to solve the problem, as illustrated in the examples in Table 1. Comparing with other commonsense reasoning tasks, such as COPA (Roemmele et al., 2011), Story Cloze Test (Mostafazadeh et al., 2016), Event2Mind (Rashkin et al., 2018), SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker (Levesque et al., 2011), 13 Proceedings of the First Workshop on Commonsense Inference in Natural L"
D19-6002,D12-1071,0,0.567047,"g, China, November 3, 2019. 2019 Association for Computational Linguistics and yet are challenging for machines. For example, the WNLI task, which is derived from WSC, is considered the most challenging NLU task in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Most machine learning models can hardly outperform the naive baseline of majority voting (scored at 65.1) 1 , including BERT (Devlin et al., 2018a) and Distilled MT-DNN (Liu et al., 2019a). While traditional methods of commonsense reasoning rely heavily on human-crafted features and knowledge bases (Rahman and Ng, 2012a; Sharma et al., 2015; Sch¨uller, 2014; Bailey et al., 2015; Liu et al., 2017), we explore in this study machine learning approaches using deep neural networks (DNN). Our method is inspired by two categories of DNN models proposed recently. The first are neural language models trained on large amounts of text data. Trinh and Le (2018) proposed to use a neural language model trained on raw text from books and news to calculate the probabilities of the natural language sentences which are constructed from a statement by replacing the to-be-resolved pronoun in the statement with each of its cand"
D19-6002,N15-1092,1,0.823619,"between the two vectors. The candidate with the highest score is selected as the result. The two categories of models use different inductive biases when predicting outputs given inputs, and thus capture different views of the data. While language models measure the semantic co1 See the GLUE leaderboard at gluebenchmark.com/leaderboard herence and wholeness of a statement where the pronoun to be resolved is replaced with its candidate antecedent, DSSMs measure the semantic relatedness of the pronoun and its candidate in their context. Therefore, inspired by multi-task learning (Caruana, 1997; Liu et al., 2015, 2019b), we propose a hybrid neural network (HNN) model that combines the strengths of both neural language models and a semantic similarity model. As shown in Figure 1, HNN consists of two component models, a masked language model and a deep semantic similarity model. The two component models share the same text encoder (BERT), but use different model-specific input and output layers. The final output score is the combination of the two model scores. The architecture of HNN bears a strong resemblance to that of MultiTask Deep Neural Network (MT-DNN) (Liu et al., 2019b), which consists of a B"
D19-6002,P18-1043,0,0.0335149,"an anaphora resolution problem, which is a form of co-reference resolution, where a machine (AI agent) must identify the antecedent of an ambiguous pronoun in a statement. WSC and PDP differ from other co-reference resolution tasks (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016) in that commonsense knowledge, which cannot be explicitly decoded from the given text, is needed to solve the problem, as illustrated in the examples in Table 1. Comparing with other commonsense reasoning tasks, such as COPA (Roemmele et al., 2011), Story Cloze Test (Mostafazadeh et al., 2016), Event2Mind (Rashkin et al., 2018), SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker (Levesque et al., 2011), 13 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 13–21 c Hongkong, China, November 3, 2019. 2019 Association for Computational Linguistics and yet are challenging for machines. For example, the WNLI task, which is derived from WSC, is considered the most challenging NLU task in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al"
D19-6002,P19-1441,1,0.846218,"et al., 2011), 13 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 13–21 c Hongkong, China, November 3, 2019. 2019 Association for Computational Linguistics and yet are challenging for machines. For example, the WNLI task, which is derived from WSC, is considered the most challenging NLU task in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Most machine learning models can hardly outperform the naive baseline of majority voting (scored at 65.1) 1 , including BERT (Devlin et al., 2018a) and Distilled MT-DNN (Liu et al., 2019a). While traditional methods of commonsense reasoning rely heavily on human-crafted features and knowledge bases (Rahman and Ng, 2012a; Sharma et al., 2015; Sch¨uller, 2014; Bailey et al., 2015; Liu et al., 2017), we explore in this study machine learning approaches using deep neural networks (DNN). Our method is inspired by two categories of DNN models proposed recently. The first are neural language models trained on large amounts of text data. Trinh and Le (2018) proposed to use a neural language model trained on raw text from books and news to calculate the probabilities of the natural la"
D19-6002,J01-4004,0,0.14275,"sense knowledge that “An object cannot fit in a container because either the object (trophy) is too big or the container (suitcase) is too small.” In this paper, we study two classic commonsense reasoning tasks: the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problem (PDP) (Levesque et al., 2011; Davis and Marcus, 2015). Both tasks are formulated as an anaphora resolution problem, which is a form of co-reference resolution, where a machine (AI agent) must identify the antecedent of an ambiguous pronoun in a statement. WSC and PDP differ from other co-reference resolution tasks (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016) in that commonsense knowledge, which cannot be explicitly decoded from the given text, is needed to solve the problem, as illustrated in the examples in Table 1. Comparing with other commonsense reasoning tasks, such as COPA (Roemmele et al., 2011), Story Cloze Test (Mostafazadeh et al., 2016), Event2Mind (Rashkin et al., 2018), SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker (Levesque et al., 2011), 13 Proceedings of the First Works"
D19-6002,W18-5446,0,0.0997573,"Missing"
D19-6002,N19-1094,1,0.922246,"e a neural language model trained on raw text from books and news to calculate the probabilities of the natural language sentences which are constructed from a statement by replacing the to-be-resolved pronoun in the statement with each of its candidate references (antecedent), and then pick the candidate with the highest probability as the answer. Kocijan et al. (2019) showed that a significant improvement can be achieved by fine-tuning a pre-trained masked language model (BERT in their case) on a small amount of WSC labeled data. The second category of models are semantic similarity models. Wang et al. (2019); Opitz and Frank (2018) formulated WSC and PDP as a semantic matching problem, and proposed to use two variations of the Deep Structured Similarity Model (DSSM) (Huang et al., 2013) to compute the semantic similarity score between each candidate antecedent and the pronoun by (1) mapping the candidate and the pronoun and their context into two vectors, respectively, in a hidden space using deep neural networks, and (2) computing cosine similarity between the two vectors. The candidate with the highest score is selected as the result. The two categories of models use different inductive biases"
D19-6002,D18-1009,0,0.0262604,"m, which is a form of co-reference resolution, where a machine (AI agent) must identify the antecedent of an ambiguous pronoun in a statement. WSC and PDP differ from other co-reference resolution tasks (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016) in that commonsense knowledge, which cannot be explicitly decoded from the given text, is needed to solve the problem, as illustrated in the examples in Table 1. Comparing with other commonsense reasoning tasks, such as COPA (Roemmele et al., 2011), Story Cloze Test (Mostafazadeh et al., 2016), Event2Mind (Rashkin et al., 2018), SWAG (Zellers et al., 2018), ReCoRD (Zhang et al., 2018), and so on, WSC and PDP better approximate real human reasoning, can be easily solved by native English-speaker (Levesque et al., 2011), 13 Proceedings of the First Workshop on Commonsense Inference in Natural Language Processing, pages 13–21 c Hongkong, China, November 3, 2019. 2019 Association for Computational Linguistics and yet are challenging for machines. For example, the WNLI task, which is derived from WSC, is considered the most challenging NLU task in the General Language Understanding Evaluation (GLUE) benchmark (Wang et al., 2018). Most machine learni"
E14-1003,D13-1176,0,0.0658436,"eural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies. Recurrent architectures have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012) outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). Recent work has also shown successful applications to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013). We extend this work by modeling Minimum Translation Units with recurrent neural networks. Specifically, we introduce two recurrent neural network-based MTU models to address the isWe introduce recurrent neural networkbased Minimum Translation Unit (MTU) models which make predictions based on an unbounded history of previous bilingual contexts. Traditional back-off n-gram models suffer under the sparse nature of MTUs which makes estimation of highorder sequence models challenging. We tackle the sparsity problem by modeling MTUs both as bags-of-words and as a sequence of individual source and"
E14-1003,W06-3114,0,0.0109025,"rate training (MERT; Och, 2003). The 1-best output by the phrase-based decoder is the baseline accuracy. As a second baseline we experiment with a trigram back-off MTU model trained on all extracted MTUs, denoted as n-gram MTU. The trigram MTU model is estimated with the same modified Kneser-Ney framework as the target side language model. All MTU models are trained in target left-to-right MTU order which performed well in initial experiments. Evaluation. We test our approach on two different data sets. First, we train a German to English system based on the data of the WMT 2006 shared task (Koehn and Monz, 2006). The parallel corpus includes about 35M words of parliamentary proceedings for training, a development set and two test sets with 2000 sentences each. Second, we experiment with a French to English system based on 102M words of training data from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for about 5m words which are newswire; all MTU models are trained on the newswire subset since we found similar accuracy to using all data in initial experiments. We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 s"
E14-1003,W12-2703,0,0.131776,"context issue either, since predictions are based on a fixed-size context, similar to back-off n-gram models. We therefore focus in this paper on recurrent neural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies. Recurrent architectures have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012) outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). Recent work has also shown successful applications to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013). We extend this work by modeling Minimum Translation Units with recurrent neural networks. Specifically, we introduce two recurrent neural network-based MTU models to address the isWe introduce recurrent neural networkbased Minimum Translation Unit (MTU) models which make predictions based on an unbounded history of previous bilingual contexts. Traditional back-off n-gram models suffer under the sparse nature of MTUs which mak"
E14-1003,N03-1017,0,0.0394319,"Missing"
E14-1003,P07-2045,0,0.00595061,"ed to a convolutional output layer yt by weights summarized in the sparse 4 24 In initial experiments we found this model to be over twenty times slower than the atomic MTU RNN model with estimated training times of over 6 weeks. This was despite using a vastly smaller vocabulary and by computing the word layer on a, by current standards, high-end GPU (NVIDIA Tesla K20c) using sparse matrix optimizations (cuSPARSE) for the convolutional layer. 5 mt 1 src MTU D 0 1 ht V 1 0 mt+1 0 W ht-1 5.1 T ct Experimental Setup Baselines. We experiment with an in-house phrase-based system similar to Moses (Koehn et al., 2007), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pM LE (e|f ) and vice versa pM LE (f |e), as well as lexical weighting estimates pLW (e|f ) and pLW (f |e), word and phrase-penalties, a linear distortion feature and a lexicalized reordering feature. The baseline includes a standard modified Kneser-Ney wordbased language model trained on the target-side of the parallel corpora described below. Log-linear weights are estimated with minimum error rate training (MERT; Och, 2003). The 1-best output by the phrase-based decoder"
E14-1003,D13-1106,1,0.695842,"aper on recurrent neural network architectures, which address the limited context issue by basing predictions on an unbounded history of previous events which allows to capture long-span dependencies. Recurrent architectures have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011a; Mikolov, 2012) outperforming multi-layer feedforward based networks in perplexity and word error rate for speech recognition (Arisoy et al., 2012; Sundermeyer et al., 2013). Recent work has also shown successful applications to machine translation (Mikolov, 2012; Auli et al., 2013; Kalchbrenner and Blunsom, 2013). We extend this work by modeling Minimum Translation Units with recurrent neural networks. Specifically, we introduce two recurrent neural network-based MTU models to address the isWe introduce recurrent neural networkbased Minimum Translation Unit (MTU) models which make predictions based on an unbounded history of previous bilingual contexts. Traditional back-off n-gram models suffer under the sparse nature of MTUs which makes estimation of highorder sequence models challenging. We tackle the sparsity problem by modeling MTUs both as bags-of-words and as a s"
E14-1003,W05-0823,0,0.0608087,"Missing"
E14-1003,N12-1005,0,0.579306,"ersity of Maryland, College Park Michael Auli, Qin Gao, Jianfeng Gao Microsoft Research Redmond, WA, USA ynhu@cs.umd.edu {michael.auli,qigao,jfgao}@microsoft.com Abstract amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited context issue either, since predictions are based on a fixed-size context, similar to back-off n-"
E14-1003,J92-4003,0,0.17638,"Missing"
E14-1003,P13-2121,0,0.0223918,"Missing"
E14-1003,P03-1021,0,0.123998,"phrase-based system similar to Moses (Koehn et al., 2007), scoring translations by a set of common features including maximum likelihood estimates of source given target mappings pM LE (e|f ) and vice versa pM LE (f |e), as well as lexical weighting estimates pLW (e|f ) and pLW (f |e), word and phrase-penalties, a linear distortion feature and a lexicalized reordering feature. The baseline includes a standard modified Kneser-Ney wordbased language model trained on the target-side of the parallel corpora described below. Log-linear weights are estimated with minimum error rate training (MERT; Och, 2003). The 1-best output by the phrase-based decoder is the baseline accuracy. As a second baseline we experiment with a trigram back-off MTU model trained on all extracted MTUs, denoted as n-gram MTU. The trigram MTU model is estimated with the same modified Kneser-Ney framework as the target side language model. All MTU models are trained in target left-to-right MTU order which performed well in initial experiments. Evaluation. We test our approach on two different data sets. First, we train a German to English system based on the data of the WMT 2006 shared task (Koehn and Monz, 2006). The paral"
E14-1003,N06-1002,0,0.0800215,"ity problem by modeling MTUs both as bags-of-words and as a sequence of individual source and target words. Our best results improve the output of a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.5 BLEU, and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU. 1 Introduction Classical phrase-based translation models rely heavily on the language model and the reordering model to capture dependencies between phrases. Sequence models over Minimum Translation Units (MTUs) have been shown to complement both syntax-based (Quirk and Menezes, 2006) as well as phrase-based (Zhang et al., 2013) models by explicitly modeling relationships between phrases. MTU models have been traditionally estimated using standard back-off n-gram techniques (Quirk and Menezes, 2006; Crego and Yvon, 2010; Zhang et al., 2013), similar to wordbased language models (§2). However, the estimation of higher-order n-gram models becomes increasingly difficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harde"
E14-1003,W12-2707,0,0.0646677,"Missing"
E14-1003,D07-1045,0,0.150337,"Missing"
E14-1003,W12-2702,0,0.018086,"um Translation Modeling with Recurrent Neural Networks Yuening Hu Department of Computer Science University of Maryland, College Park Michael Auli, Qin Gao, Jianfeng Gao Microsoft Research Redmond, WA, USA ynhu@cs.umd.edu {michael.auli,qigao,jfgao}@microsoft.com Abstract amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited c"
E14-1003,D13-1140,0,0.0985043,"g with Recurrent Neural Networks Yuening Hu Department of Computer Science University of Maryland, College Park Michael Auli, Qin Gao, Jianfeng Gao Microsoft Research Redmond, WA, USA ynhu@cs.umd.edu {michael.auli,qigao,jfgao}@microsoft.com Abstract amount of previous context that is often not sufficient to capture important aspects of human language (Rastrow et al., 2012). Recently, several feed-forward neural networkbased models have achieved impressive improvements over traditional back-off n-gram models in language modeling (Bengio et al., 2003; Schwenk et al., 2007; Schwenk et al., 2012; Vaswani et al., 2013), as well as translation modeling (Allauzen et al., 2011; Le et al., 2012; Gao et al., 2013). These models tackle the data sparsity problem by representing words in continuous space rather than as discrete units. Similar words are grouped in the same sub-space rather than being treated as separate entities. Neural network models can be seen as functions over continuous representations exploiting the similarity between words, thereby making the estimation of probabilities over higherorder n-grams easier. However, feed-forward networks do not directly address the limited context issue either, si"
E14-1003,N13-1002,1,0.723742,"and as a sequence of individual source and target words. Our best results improve the output of a phrase-based statistical machine translation system trained on WMT 2012 French-English data by up to 1.5 BLEU, and we outperform the traditional n-gram based MTU approach by up to 0.8 BLEU. 1 Introduction Classical phrase-based translation models rely heavily on the language model and the reordering model to capture dependencies between phrases. Sequence models over Minimum Translation Units (MTUs) have been shown to complement both syntax-based (Quirk and Menezes, 2006) as well as phrase-based (Zhang et al., 2013) models by explicitly modeling relationships between phrases. MTU models have been traditionally estimated using standard back-off n-gram techniques (Quirk and Menezes, 2006; Crego and Yvon, 2010; Zhang et al., 2013), similar to wordbased language models (§2). However, the estimation of higher-order n-gram models becomes increasingly difficult due to data sparsity issues associated with large n-grams, even when training on over one hundred billion words (Heafield et al., 2013); bilingual units are much sparser than words and are therefore even harder to estimate. Another drawback of n-gram mod"
E14-1003,P13-2071,0,\N,Missing
E14-1003,N13-1090,0,\N,Missing
E14-1003,N13-1001,0,\N,Missing
E14-1003,W11-2135,0,\N,Missing
H05-1027,W02-1001,0,0.107516,"he least CER. Therefore, the best estimators are those which minimize the expected error rate on unseen test data. Since the distribution of test data is unknown, we can approximately minimize the error rate on some given training data (Vapnik 1999). Toward this end, we have developed a very simple heuristic training procedure called minimum sample risk, as presented in the next section. 3 Minimum Sample Risk 3.1 Problem Definition We follow the general framework of linear discriminant models described in (Duda et al. 2001). In the rest of the paper we use the following notation, adapted from Collins (2002). • Training data is a set of example input/output pairs. In LM for IME, training samples are represented as {Ai, WiR}, for i = 1…M, where each Ai is an input phonetic string and WiR is the reference transcript of Ai. • We assume some way of generating a set of candidate word strings given A, denoted by GEN(A). In our experiments, GEN(A) consists of top N word strings converted from A using a baseline IME system that uses only a word trigram model. • We assume a set of D+1 features fd(W), for d = 0…D. The features could be arbitrary functions that map W to real values. Using vector notation, w"
H05-1027,W02-1032,1,0.740514,"CER. Therefore, the best estimators are those which minimize the expected error rate on unseen test data. Since the distribution of test data is unknown, we can approximately minimize the error rate on some given training data (Vapnik 1999). Toward this end, we have developed a very simple heuristic training procedure called minimum sample risk, as presented in the next section. 3 Minimum Sample Risk 3.1 Problem Definition We follow the general framework of linear discriminant models described in (Duda et al. 2001). In the rest of the paper we use the following notation, adapted from Collins (2002). • Training data is a set of example input/output pairs. In LM for IME, training samples are represented as {Ai, WiR}, for i = 1…M, where each Ai is an input phonetic string and WiR is the reference transcript of Ai. • We assume some way of generating a set of candidate word strings given A, denoted by GEN(A). In our experiments, GEN(A) consists of top N word strings converted from A using a baseline IME system that uses only a word trigram model. • We assume a set of D+1 features fd(W), for d = 0…D. The features could be arbitrary functions that map W to real values. Using vector notation, w"
H05-1027,P03-1021,0,0.052351,"quation (4) is a step function of λ, thus cannot be optimized directly by regular gradientbased procedures – a grid search has to be used instead. However, there are problems with simple grid search: using a large grid could miss the optimal solution whereas using a fine-grained grid would lead to a very slow algorithm. Secondly, in 211 the case of LM, there are millions of candidate features, some of which are highly correlated. We address these issues respectively in the next two subsections. 3.3 Grid Line Search Our implementation of a grid search is a modified version of that proposed in (Och 2003). The modifications are made to deal with the efficiency issue due to the fact that there is a very large number of features and training samples in our task, compared to only 8 features used in (Och 2003). Unlike a simple grid search where the intervals between any two adjacent grids are equal and fixed, we determine for each feature a sequence of grids with differently sized intervals, each corresponding to a different value of sample risk. As shown in Equation (4), the loss function (i.e. sample risk) over all training samples is the sum of the loss function (i.e. Er(.)) of each training sa"
H05-1027,P05-1034,0,0.036735,"Missing"
H05-1027,H05-1034,1,0.792962,"e 3, but also better generalization properties (fewer test errors), as shown in Figure 4. 4.4 Domain Adaptation Results Though MSR achieves impressive performance in CER reduction over the comparison methods, as described in Section 4.2, the experiments are all performed using newspaper text for both training and testing, which is not a realistic scenario if we are to deploy the model in an application. This section reports the results of additional experiments in which we adapt a model trained on one domain to a different domain, i.e., in a so-called cross-domain LM adaptation paradigm. See (Suzuki and Gao 2005) for a detailed report. The data sets we used stem from five distinct sources of text. The Nikkei newspaper corpus described in Section 4.1 was used as the background domain, on which the word trigram model was trained. We used four adaptation domains: Yomiuri (newspaper corpus), TuneUp (balanced corpus containing newspapers and other sources of text), Encarta (encyclopedia) and Shincho (collection of novels). For each of the four domains, we used an 72,000-sentence subset as adaptation training data, a 5,000-sentence subset as held-out data and another 5,000-sentence subset as test data. Simi"
H05-1034,N04-4006,0,0.078475,"orted to slightly outperform linear interpolation. Discriminative approaches to LM adaptation, on the other hand, aim at using the adaptation data to directly minimize the errors on the adaptation data made by the background model. These techniques have been applied successfully to the task of language modeling in non-adaptation (Roark et al., 265 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 265–272, Vancouver, October 2005. 2005 Association for Computational Linguistics 2004) as well as adaptation (Bacchiani et al., 2004) scenarios. In this paper, we present comparative experimental results on four language model adaptation techniques and evaluate them from various angles, attempting to elucidate the characteristics of these models. The four models we compare are a maximum a posteriori (MAP) method and three discriminative training methods, namely the boosting algorithm (Collins, 2000), the average perceptron (Collins, 2002) and the minimum sample risk method (Gao et al., 2005). Our evaluation of these techniques is unique in that we go beyond simply comparing them in terms of character error rate (CER): we us"
H05-1034,W02-1001,0,0.723499,"Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 265–272, Vancouver, October 2005. 2005 Association for Computational Linguistics 2004) as well as adaptation (Bacchiani et al., 2004) scenarios. In this paper, we present comparative experimental results on four language model adaptation techniques and evaluate them from various angles, attempting to elucidate the characteristics of these models. The four models we compare are a maximum a posteriori (MAP) method and three discriminative training methods, namely the boosting algorithm (Collins, 2000), the average perceptron (Collins, 2002) and the minimum sample risk method (Gao et al., 2005). Our evaluation of these techniques is unique in that we go beyond simply comparing them in terms of character error rate (CER): we use a metric of distributional similarity to measure the distance between background and adaptation domains, and attempt to correlate it with the CER of each adaptation method. We also propose a novel metric for measuring the side effects of adapted models using the notion of backward compatibility, which is very important from a software deployment perspective. Our experiments are conducted in the setting of"
H05-1034,W02-1032,1,0.624441,"ity. This quantity indeed reflects the in-domain variability of text: newspaper and encyclopedia articles are highly edited text, following style guidelines and often with repetitious content. In contrast, Shincho is a collection of novels, on which no style or content restriction is imposed. We use these metrics in the interpretation of CER results in Section 5. 4.3 Results of LM adaptation The discriminative training procedure was carried out as follows: for each input phonetic string A in the adaptation training set, we produced a word lattice using the baseline trigram models described in Gao et al. (2002b). We kept the top 20 hypotheses from this lattice as the candidate conversion set GEN(A). The lowest CER hypothesis in the lattice rather than the reference transcript was used as WR. We used unigram and bigram features that occurred more than once in the training set. We compared the performance of discriminative methods against a MAP estimation method as the baseline, in this case the linear interpolation 269 method. Specifically, we created a word trigram model using the adaptation data for each domain, which was then linearly interpolated at the word level with the baseline model. The pr"
H05-1034,H05-1027,1,0.845961,"(HLT/EMNLP), pages 265–272, Vancouver, October 2005. 2005 Association for Computational Linguistics 2004) as well as adaptation (Bacchiani et al., 2004) scenarios. In this paper, we present comparative experimental results on four language model adaptation techniques and evaluate them from various angles, attempting to elucidate the characteristics of these models. The four models we compare are a maximum a posteriori (MAP) method and three discriminative training methods, namely the boosting algorithm (Collins, 2000), the average perceptron (Collins, 2002) and the minimum sample risk method (Gao et al., 2005). Our evaluation of these techniques is unique in that we go beyond simply comparing them in terms of character error rate (CER): we use a metric of distributional similarity to measure the distance between background and adaptation domains, and attempt to correlate it with the CER of each adaptation method. We also propose a novel metric for measuring the side effects of adapted models using the notion of backward compatibility, which is very important from a software deployment perspective. Our experiments are conducted in the setting of Japanese Kana-Kanji conversion, as we believe this tas"
H05-1034,P03-1021,0,0.0116143,"ibution in reducing SR in a sequential manner. Conceptually, MSR operates like any multidimensional function optimization approach: a direction (i.e., feature) is selected and SR is minimized along that direction using a line search, i.e., adjusting the parameter of the selected feature while keeping all other parameters fixed. This is repeated until SR stops decreasing. Regular numerical line search algorithms cannot be applied directly because, as described above, the value of a feature parameter versus SR is not smooth and there are many local minima. MSR thus adopts the method proposed by Och (2003). Let GEN(A) be the set of n-best candidate word strings that could be converted from A. By adjusting λd for a selected feature fd, we can find a set of intervals for λd within which a particular candidate word string is selected. We can compute Er(.) for the candidate and use it as the Er(.) value for the corresponding interval. As a result, we obtain an ordered sequence of Er(.) values and a corresponding sequence of λ intervals for each training sample. By summing Er(.) values over all training samples, we obtain a global sequence of SR and the corresponding global sequence of λd intervals."
H05-1034,I05-1083,1,0.682746,"aracteristics of these domains are measured using the information theoretic notion of cross entropy, which is described in the next subsection. For the experiment of LM adaptation, we used the training data consisting of 8,000 sentences and test data of 5,000 sentences from each adaptation domain. Another 5,000-sentence subset was used as held-out data for each domain, which was used to determine the values of tunable parameters. All the corpora used in our experiments are presegmented into words using a baseline lexicon consisting of 167,107 entries. 4.2 Computation of domain characteristics Yuan et al. (2005) introduces two notions of domain characteristics: a within-domain notion of diversity, and a cross-domain concept of similarity. Diversity is measured by the entropy of the corpus and indicates the inherent variability within the domain. Similarity, on the other hand, is intended to capture the difficulty of a given adaptation task, and is measured by the cross entropy. For the computation of these metrics, we extracted 1 million words from the training data of each domain respectively, and created a lexicon consisting of the words in our baseline lexicon plus all words in the corpora used fo"
I05-1083,W02-1001,0,0.58065,"omain of data. This paper presents an empirical study of several LM adaptation methods on the task of Japanese text input. In particular, we focus on the so-called cross-domain LM adaptation paradigm, i.e. to adapt a LM trained on one domain to a different domain, for which only a small amount of training data is available. The LM adaptation methods investigated in this paper can be grouped into two categories: maximum a posterior (MAP) and discriminative training. Linear interpolation is representative of the MAP methods [1]. The other three methods, including the boosting [2] and perceptron [3] algorithms and minimum sample risk (MSR) method [4], are discriminative methods, each of which uses a different training algorithm. We carried out experiments over many training data sizes on four distinct adaptation corpora, the characteristics of which were measured using the informationtheoretic notion of cross entropy. We found that discriminative training methods 1 This research was conducted while the author was visiting Microsoft Research Asia. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 957 – 968, 2005. © Springer-Verlag Berlin Heidelberg 2005 958 W. Yuan, J. Gao, and H. Suzuki"
I05-1083,H05-1027,1,0.841664,"y of several LM adaptation methods on the task of Japanese text input. In particular, we focus on the so-called cross-domain LM adaptation paradigm, i.e. to adapt a LM trained on one domain to a different domain, for which only a small amount of training data is available. The LM adaptation methods investigated in this paper can be grouped into two categories: maximum a posterior (MAP) and discriminative training. Linear interpolation is representative of the MAP methods [1]. The other three methods, including the boosting [2] and perceptron [3] algorithms and minimum sample risk (MSR) method [4], are discriminative methods, each of which uses a different training algorithm. We carried out experiments over many training data sizes on four distinct adaptation corpora, the characteristics of which were measured using the informationtheoretic notion of cross entropy. We found that discriminative training methods 1 This research was conducted while the author was visiting Microsoft Research Asia. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 957 – 968, 2005. © Springer-Verlag Berlin Heidelberg 2005 958 W. Yuan, J. Gao, and H. Suzuki outperformed the LI method in all cases, and were m"
I05-1083,P99-1004,0,0.0352836,"are interested in measuring the complexity of the LM adaptation task. We therefore define the similarity between two domains using the cross entropy. We will also use the metric that approximates the entropy of the corpus to capture the in-domain diversity of a corpus in Section 5.2.2 3.2 LM Adaptation Methods In this paper, two major approaches to cross-domain adaptation have been investigated: maximum a posteriori (MAP) estimation and discriminative training methods. 2 There are other well-known metrics of similarity within NLP literature, such as the mutual information or cosine similarity [7], which we do not discuss in this paper. 960 W. Yuan, J. Gao, and H. Suzuki In MAP estimation methods, adaptation data is used to adjust the parameters of the background model so as to maximize the likelihood of the adaptation data [1]. Discriminative training methods to LM adaptation, on the other hand, aim at using the adaptation data to directly minimize the errors on the adaptation data made by the background model. These techniques have been applied successfully to the task of language modeling in non-adaptation [8] as well as adaptation scenarios [9] for speech recognition. But most of t"
I05-1083,N04-4006,0,0.282641,"tual information or cosine similarity [7], which we do not discuss in this paper. 960 W. Yuan, J. Gao, and H. Suzuki In MAP estimation methods, adaptation data is used to adjust the parameters of the background model so as to maximize the likelihood of the adaptation data [1]. Discriminative training methods to LM adaptation, on the other hand, aim at using the adaptation data to directly minimize the errors on the adaptation data made by the background model. These techniques have been applied successfully to the task of language modeling in non-adaptation [8] as well as adaptation scenarios [9] for speech recognition. But most of them focused on the investigation of performance of certain methods for LM adaptation, without analyzing in detail the underlying reasons of different performance achieved by different methods. In this paper we attempt to investigate the effectiveness of different discriminative methods in an IME adaptation scenario, with a particular emphasis on correlating their performance with the characteristics of adaptation domain. 4 LM Adaptation Methods We implement four methods in our experiments. The Linear Interpolation (LI) falls into the framework of MAP while"
I05-1083,P03-1021,0,0.00476397,"an iterative procedure. In each iteration, MSR selects a feature that is estimated to be most effective in terms of reducing SR(.), and then optimizes the current model by adjusting the parameters of the selected feature. MSR, however, differs from the boosting method in that MSR tries to optimize the sample risk directly while the boosting optimizes the loss function that is an upper bound of the sample risk. As mentioned earlier, SR(.) can be optimized using regular gradient-based optimization algorithms. MSR therefore uses a particular implementation of line search, originally proposed in [11], to optimize the current model by adjusting the parameter of a selected feature while keeping other parameters fixed. Assuming fd is the selected feature, its parameter λd is optimized by line search as follows. Recall that Er(WR,W) is the function that measures the number of conversion errors in W versus its reference transcript WR. The value of SR(.) is the sum of Er(.) over all training samples. For each A in training set, let GEN(A) be the set of n-best candidate word strings that could be converted from A. By adjusting λd, we obtain for each training sample an ordered sequence of λd inte"
I05-1083,P02-1034,0,\N,Missing
I05-1083,P02-1062,0,\N,Missing
I05-2040,W98-1119,0,0.072442,"tion answering, and machine translation. EDT is an extension of the task of coreference resolution in that in EDT we not only resolve the coreference between mentions but also detect the entities. Each of those entities may have one or more mentions. In the ACE project, there are five types of entities defined in EDT: person (PER), geography political Entity (GPE), organization (ORG), location (LOC), and facility (FAC). Many traditional coreference techniques can be extended to EDT for entity tracking. Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents. Recent research (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycherah et al., 2003; Luo et al., 2004) focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name, nominal and pronoun phrase. One common approach applied by them is to first train a binary statistical model to measure how likely a pair of This work is done while the first author is visiting Microsoft Research Asia. 232 words. However MS"
I05-2040,N01-1008,0,0.201653,"Missing"
I05-2040,N03-2014,0,0.0413118,"Missing"
I05-2040,W04-0705,0,0.0517128,"Missing"
I05-2040,P04-1018,0,0.0896313,"s may have one or more mentions. In the ACE project, there are five types of entities defined in EDT: person (PER), geography political Entity (GPE), organization (ORG), location (LOC), and facility (FAC). Many traditional coreference techniques can be extended to EDT for entity tracking. Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents. Recent research (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycherah et al., 2003; Luo et al., 2004) focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name, nominal and pronoun phrase. One common approach applied by them is to first train a binary statistical model to measure how likely a pair of This work is done while the first author is visiting Microsoft Research Asia. 232 words. However MSRSeg can’t well match the standard of ACE EDT evaluation for either types or boundaries. The difference of the standard of named entity between MSRSeg and ACE cause more than half of the errors for NAME mention detection"
I05-2040,P98-2143,0,0.0235618,"nd machine translation. EDT is an extension of the task of coreference resolution in that in EDT we not only resolve the coreference between mentions but also detect the entities. Each of those entities may have one or more mentions. In the ACE project, there are five types of entities defined in EDT: person (PER), geography political Entity (GPE), organization (ORG), location (LOC), and facility (FAC). Many traditional coreference techniques can be extended to EDT for entity tracking. Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents. Recent research (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycherah et al., 2003; Luo et al., 2004) focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name, nominal and pronoun phrase. One common approach applied by them is to first train a binary statistical model to measure how likely a pair of This work is done while the first author is visiting Microsoft Research Asia. 232 words. However MSRSeg can’t well"
I05-2040,J01-4004,0,0.487285,"the coreference between mentions but also detect the entities. Each of those entities may have one or more mentions. In the ACE project, there are five types of entities defined in EDT: person (PER), geography political Entity (GPE), organization (ORG), location (LOC), and facility (FAC). Many traditional coreference techniques can be extended to EDT for entity tracking. Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents. Recent research (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycherah et al., 2003; Luo et al., 2004) focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name, nominal and pronoun phrase. One common approach applied by them is to first train a binary statistical model to measure how likely a pair of This work is done while the first author is visiting Microsoft Research Asia. 232 words. However MSRSeg can’t well match the standard of ACE EDT evaluation for either types or boundaries. The difference of the standard of named entity"
I05-2040,J95-4004,0,0.112318,"ns is an essential problem for EDT/coreference, there has been relatively less previous research. Ng and Cardie (2002) shows that improving the recall of noun phrase identification can improve the performance of a coreference system. Florian et al. (2004) formulate the mention detection problem as a character-based classification problem. They assign for each character in the text a label, indicating whether it is the start of a specific mention, inside a specific mention, or outside of any mention. In this paper, we propose a unified EDT model based on the Transformation Based Learning (TBL, Brill, 1995) framework for Chinese. The model consists of two sub models: a mention detection model and a coreference model. The first sub-model is used to adapt existing Chinese word segmentation and Named Entity (NE) recognition system to a specific EDT standard. TBL is a widely used machine learning method, but it is the first time it is applied to coreference resolution. In addition, a feedback technique is proposed to further improve the performance of the system. The rest of the paper is organized as follows. In section 2, we propose the unified TBL Chinese EDT model framework. We describe the four"
I05-2040,M95-1005,0,0.472854,"Missing"
I05-2040,N04-1001,0,0.148554,"of allowable templates for rules, and an objective function for learning. mentions corefer; and then followed by a greedy procedure to group the mentions into entities. Mention detection is to find all the named entity, noun or noun phrase, pronoun or pronoun phrase. Therefore, it needs Named Entity Recognition, but not only. Though the detection of entity mentions is an essential problem for EDT/coreference, there has been relatively less previous research. Ng and Cardie (2002) shows that improving the recall of noun phrase identification can improve the performance of a coreference system. Florian et al. (2004) formulate the mention detection problem as a character-based classification problem. They assign for each character in the text a label, indicating whether it is the start of a specific mention, inside a specific mention, or outside of any mention. In this paper, we propose a unified EDT model based on the Transformation Based Learning (TBL, Brill, 1995) framework for Chinese. The model consists of two sub models: a mention detection model and a coreference model. The first sub-model is used to adapt existing Chinese word segmentation and Named Entity (NE) recognition system to a specific EDT"
I05-2040,P03-1023,0,0.098023,"ween mentions but also detect the entities. Each of those entities may have one or more mentions. In the ACE project, there are five types of entities defined in EDT: person (PER), geography political Entity (GPE), organization (ORG), location (LOC), and facility (FAC). Many traditional coreference techniques can be extended to EDT for entity tracking. Early work on pronoun anaphora resolution usually uses rule-based methods (e.g. Hobbs 1976; Ge et al., 1998; Mitkov, 1998), which try to mine the cues of the relation between the pronouns and its antecedents. Recent research (Soon et al., 2001; Yang et al., 2003; Ng and Cardie, 2002; Ittycherah et al., 2003; Luo et al., 2004) focuses on the use of statistical machine learning methods and tries to resolve references among all kinds of noun phases, including name, nominal and pronoun phrase. One common approach applied by them is to first train a binary statistical model to measure how likely a pair of This work is done while the first author is visiting Microsoft Research Asia. 232 words. However MSRSeg can’t well match the standard of ACE EDT evaluation for either types or boundaries. The difference of the standard of named entity between MSRSeg and"
I05-2040,P04-1059,1,\N,Missing
I05-2040,C98-2138,0,\N,Missing
I05-2040,W03-1701,1,\N,Missing
I08-1059,P01-1017,0,0.0447053,"Missing"
I08-1059,W07-1604,0,0.537721,"Felice and Pulman (2007) utilize a set of sophisticated syntactic and semantic analysis features to predict 5 common English prepositions. Obviously, this is impractical in a setting where noisy non-native text is subjected to proofing. Meanwhile, work on automated error detection on non-native text focuses primarily on detection of errors, rather than on the more difficult task of supplying viable corrections (e.g., Chodorow and Leacock, 2000). More recently, Han et al. (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al. (2003) and Chodorow et al. (2007) present techniques of automatic preposition choice modeling. These more recent efforts, nevertheless, do not attempt to integrate their methods into a more general proofing application designed to assist non-native speakers when writing English. Finally, Yi et al. (2008) designed a system that uses web counts to determine correct article usage for a given sentence, targeting ESL users. 4 System Description Our system consists of three major components: 1. Suggestion Provider (SP) 2. Language Model (LM) 3. Example Provider (EP) The Suggestion Provider contains modules for each error type discu"
I08-1059,W07-1607,0,0.562737,"Missing"
I08-1059,J93-1003,0,0.101425,"on sites are determined heuristically from the sequence of POS tags. Based on these features, we train a classifier for preposition choice and determiner choice. Currently we train decision tree classifiers with the WinMine toolkit (Chickering 2002). We also experimented with linear SVMs, but decision trees performed better overall and training and parameter optimization were considerably more efficient. Before training the classifiers, we perform feature ablation by imposing a count cutoff of 10, and by limiting the number of features to the top 75K features in terms of log likelihood ratio (Dunning 1993). We train two separate classifiers for both determiners and preposition:  decision whether or not a determiner/preposition should be present (presence/absence or pa classifier)  decision which determiner/preposition is the most likely choice, given that a determiner/preposition is present (choice or ch classifier) In the case of determiners, class values for the ch classifier are a/an and the. Preposition choice (equivalent to the ―confusion set‖ of a contextual speller) is limited to a set of 13 prepositions that figure prominently in the errors observed in the JLE corpus: about, as, at, b"
I08-1059,W02-2105,1,0.815053,"Missing"
I08-1059,han-etal-2004-detecting,0,0.212248,"Turner and Charniak (2007), for example, utilize a language model based on a statistical parser for Penn Tree Bank data. Similarly, De Felice and Pulman (2007) utilize a set of sophisticated syntactic and semantic analysis features to predict 5 common English prepositions. Obviously, this is impractical in a setting where noisy non-native text is subjected to proofing. Meanwhile, work on automated error detection on non-native text focuses primarily on detection of errors, rather than on the more difficult task of supplying viable corrections (e.g., Chodorow and Leacock, 2000). More recently, Han et al. (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al. (2003) and Chodorow et al. (2007) present techniques of automatic preposition choice modeling. These more recent efforts, nevertheless, do not attempt to integrate their methods into a more general proofing application designed to assist non-native speakers when writing English. Finally, Yi et al. (2008) designed a system that uses web counts to determine correct article usage for a given sentence, targeting ESL users. 4 System Description Our system consists of three major components: 1"
I08-1059,I08-2082,1,0.357362,"Missing"
I08-1059,N04-2006,0,0.131483,"Missing"
I08-1059,P00-1067,1,0.853874,"ot a significant problem for native speakers and hence remains unaddressed in proofing tools such as the grammar checker in Microsoft Word (Heidorn 2000). Plainly there is an Targeted Error Types Our system currently targets eight different error types: 1. Preposition presence and choice: In the other hand, ... (On the other hand ...) 2. Definite and indefinite determiner presence and choice: I am teacher... (am a teacher) 3. Gerund/infinitive confusion: I am interesting in this book. (interested in) 4. Auxiliary verb presence and choice: My teacher does is a good teacher (my teacher is...) 1 Liu et al. 2000 take a similar approach, retrieving example sentences from a large corpus. 449 5. Over-regularized verb inflection: I writed a letter (wrote) 6. Adjective/noun confusion: This is a China book (Chinese book) 7. Word order (adjective sequences and nominal compounds): I am a student of university (university student) 8. Noun pluralization: They have many knowledges (much knowledge) In this paper we will focus on the two most prominent and difficult errors: choice of determiner and prepositions. Empirical justification for targeting these errors comes from inspection of several corpora of non-nat"
I08-1059,W00-0708,0,0.590938,"Missing"
I08-1059,P06-1132,0,0.116761,"Missing"
I08-1059,N07-2045,0,0.363375,"Missing"
I08-1059,N07-1007,0,\N,Missing
I08-2082,P06-1032,1,0.502303,"iciency. 1 Google.com Live.com Yahoo.com 306,000 52,407 386,000 1,490,000 38,336,308 4,250,000 Table 1: Web Hits for Phrasal Usages Introduction Proofing technology for native speakers of English has been a focus of work for decades, and some tools like spell checkers and grammar checkers have become standard features of document processing software products. However, designing an English proofing system for English as a Second Language (ESL) users presents a major challenge: ESL writing errors vary greatly among users with different language backgrounds and proficiency levels. Recent work by Brockett et al. (2006) utilized phrasal Statistical Machine Translation (SMT) techniques to correct ESL writing errors and demonstrated that this data-intensive SMT approach is very promising, but they also pointed out SMT approach relies on the availability of large amount of training data. The expense and difficulty of collecting large quantities of 619 raw and edited ESL prose pose an obstacle to this approach. In this work we consider the prospect of using the Web, with its billions of web pages, as a data source with the potential to aid ESL writers. Our research is motivated by the observation that ESL users"
I08-2082,I08-1059,1,0.290661,"Missing"
I08-2082,W03-0209,0,0.0402081,"Missing"
I08-2082,W00-0708,0,0.411089,"Missing"
I08-2082,W00-0726,0,0.0128042,"4), the BNC is also used as a source of collocations, with collocation instances and translation counterparts from the bilingual corpus identified and shown to ESL users. In contrast to this earlier work, our system uses the web as a corpus, with string frequency counts from a search engine index used to indicate whether a particular collocation is being used correctly. 3 Web-based English Proofing System for ESL Users (ESL-WEPS) The architecture of ESL-WEPS, which consists of four main components, is shown in Fig.1. Parse ESL Sentence and Identify Check Points ESL-WEPS first tags and chunks (Sang and Buckholz, 2000) the input ESL sentence1 , and identifies the elements of the structures in the sentence to be checked according to certain heuristics: when 1 One in-house HMM chunker trained on English Penn Treebank was used. ID 1 2 3 4 5 Pre-editing version Which team can take the champion? I attend to Pyoung Taek University. I’m a Japanese and studying Info and Computer Science at Keio University. Her works are kinda erotic but they will never arouse any obscene, devil thoughts which might destroy the soul of the designer. I think it is so beautiful to go the way of theology and very attractive too, especi"
I08-2082,P98-2196,0,0.223464,"Missing"
I08-2082,P04-3019,0,\N,Missing
I08-2082,C98-2191,0,\N,Missing
I17-1047,N15-1053,0,0.036937,"Missing"
I17-1047,P04-1077,0,0.0230767,"ext C. The function V counts the number of verbs in the hypothesis and |h |denotes the number of tokens in the hypothesis. The function idf is the inverse document frequency, computing how common a hypothesis is across all the generated N-best lists. Here D is the set of all N-best lists and d is a specific N|D| best list. We define idf(h, D) = log |{d∈D:h∈d}| , where we set N =10 to cut short each N-best list. These parameters were selected following reranking experiments on the validation set. We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011). 5.2 Retrieval Models In addition to generation, we implemented two retrieval models customized for the tasks of question and response generation. Work in vision and language has demonstrated the effectiveness of retrieval models, where one uses the annotation (e.g., caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016b; Devlin et al., 2015; 3 An example generic question is where is this? and a generic response is I don’t know. 467 Do you think this happened"
I17-1047,P15-2017,0,0.200084,"des answers. Figure 2 contrasts an example ICG conversation with the VisDial dataset. As this example shows, IGC involves natural conversations with the image as the grounding, where the literal objects (e.g., the pumpkins) may not even be mentioned in the conversation at all, whereas VisDial targets explicit image understanding. More recently, Das et al. (2017b) have explored the VisDial dataset with richer models that incorporate deep-reinforcement learning. Related Work Vision and Language Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al."
I17-1047,D16-1230,0,0.0151424,"Example question and response generations on IGCCrowd test set. All the generation models use beam search with reranking. In the textual context, <UTT> separates different utterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual Context Sensitive Model (V-Ret). This model uses only the provided image for retrieval. First, we find a set of K nearest training images for the given test image based on cosine similarity of the f c7 vision feature vectors. Then we"
I17-1047,W15-4640,0,0.0312834,"ce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logue corpus (Lowe et al., 2015) is the largest corpus of dialogues (almost 1 million mainly 3-turn dialogues) for the specific topic of troubleshooting Ubuntu problems. On the other hand, for openended conversation modeling (chitchat), now a high demand application in AI, shared datasets with which to track progress are severely lacking. The ICG task presented here lies nicely in the continuum between the two, where the visual grounding of event-centric images constrains the topic of conversation to contentful utterances. To enable benchmarking of progress in the IGC task, we constructed the IGCCrowd dataset for validation"
I17-1047,P15-2073,1,0.665818,"tterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual Context Sensitive Model (V-Ret). This model uses only the provided image for retrieval. First, we find a set of K nearest training images for the given test image based on cosine similarity of the f c7 vision feature vectors. Then we retrieve those K annotations as our pool of K candidates. Finally, we compute the textual similarity among the questions in the pool according to a Smoothed-BLEU (Lin and Och,"
I17-1047,W16-1007,1,0.460669,"Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logu"
I17-1047,D11-1125,0,0.00844008,"|h |denotes the number of tokens in the hypothesis. The function idf is the inverse document frequency, computing how common a hypothesis is across all the generated N-best lists. Here D is the set of all N-best lists and d is a specific N|D| best list. We define idf(h, D) = log |{d∈D:h∈d}| , where we set N =10 to cut short each N-best list. These parameters were selected following reranking experiments on the validation set. We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011). 5.2 Retrieval Models In addition to generation, we implemented two retrieval models customized for the tasks of question and response generation. Work in vision and language has demonstrated the effectiveness of retrieval models, where one uses the annotation (e.g., caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016b; Devlin et al., 2015; 3 An example generic question is where is this? and a generic response is I don’t know. 467 Do you think this happened on highway <EOS> <GO> day this not . . decoder This was not the way I imag"
I17-1047,P16-1170,1,0.732217,"Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logu"
I17-1047,N16-1147,1,0.891951,"Missing"
I17-1047,P02-1040,0,0.122669,"at this baseball game. <UTT> Nice, which team won? My team won this game. 10 for me and 28 for my dad. ding ding ding! No it wasn’t too bad of a bang up. Yes. Nah, I’m at home now. lords cricket ground . beautiful. He’s not mine! Table 4: Example question and response generations on IGCCrowd test set. All the generation models use beam search with reranking. In the textual context, <UTT> separates different utterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual C"
I17-1047,N16-1014,1,0.87514,"tistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following two consecutive conversational steps: • Question Generation: Gi"
I17-1047,P16-1094,1,0.153491,"tistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following two consecutive conversational steps: • Question Generation: Gi"
I17-1047,D16-1090,0,0.027098,"sations with the image as the grounding, where the literal objects (e.g., the pumpkins) may not even be mentioned in the conversation at all, whereas VisDial targets explicit image understanding. More recently, Das et al. (2017b) have explored the VisDial dataset with richer models that incorporate deep-reinforcement learning. Related Work Vision and Language Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Qu"
I17-1047,D11-1054,0,0.0139427,"ional questions and responses for the IGCCrowd contexts and initial questions. Table 1 shows three full conversations found in the IGCCrowd dataset. These examples show show that eventful images lead to conversations that are semantically rich and appear to involve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models t"
I17-1047,P15-1152,0,0.0151711,"olve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following tw"
I17-1047,N15-1020,1,0.516832,"rich and appear to involve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC"
I17-1047,N15-1173,0,0.0116461,"Missing"
I17-1047,H89-1033,0,0.533497,"Missing"
I17-1047,N10-1020,1,\N,Missing
I17-1047,P98-1013,0,\N,Missing
I17-1047,C98-1013,0,\N,Missing
I17-1061,P16-2020,1,0.793373,"Missing"
I17-1061,D17-1279,1,0.830225,"Missing"
I17-1061,P15-2073,1,0.174602,"Missing"
I17-1061,P03-1021,0,0.0260752,"maximum length of the generated candidates was set at 20 tokens. At each time step, we first examine all B × B possible next-word candidates, and add all hypotheses ending with an EOS token to the N-best list. We then preserve the top-B unfinished hypotheses and move to the next word position. We then use LSTM-MMI to rerank the N-best list and use the 1-best result of the re-ranked list in all evaluation. where p(R|M, v) is the probability of the generated response given message M and the respondents user ID. |R |is the length of the target and γ is the associated penalty weight. We use MERT (Och, 2003) to optimize γ and λ on BLEU using N-best lists of response candidates generated from the development set. To compute p(M |R), we train an inverse S EQ 2S EQ model by swapping messages and responses. The reverse S EQ 2S EQ models p(M |R) is trained with no user information considered. 6.4 MTASK -M Table 2: Performance on the Twitter dataset of 2-layer S EQ 2S EQ models and MMI models. Distinct-1 and distinct-2 are respectively the number of distinct unigrams and bigrams divided by total number of generated words. Baseline log p(R|M, v) + λ log p(M |R) + γ|R| MTASK -S Table 1: Perplexity for st"
I17-1061,N15-1124,0,0.0209474,"Missing"
I17-1061,P02-1040,0,0.120451,"wapping messages and responses. The reverse S EQ 2S EQ models p(M |R) is trained with no user information considered. 6.4 MTASK -M Table 2: Performance on the Twitter dataset of 2-layer S EQ 2S EQ models and MMI models. Distinct-1 and distinct-2 are respectively the number of distinct unigrams and bigrams divided by total number of generated words. Baseline log p(R|M, v) + λ log p(M |R) + γ|R| MTASK -S Table 1: Perplexity for standard S EQ 2S EQ and the user model on the Twitter Persona dev set. As in previous work (Sordoni et al., 2015), we use BLEU and human evaluation for evaluation. BLEU (Papineni et al., 2002) has been shown to correlate fairly well with human judgment at a document- and corpus-level, including on the response generation task.2 We also report perplexity as an indicator of model capability. We additionally report degree of diversity by calculating the number of distinct unigrams and bigrams in generated responses. The value is scaled by total number of generated tokens to avoid favoring long sentences (shown as distinct-1 and distinct-2). Finally, we present a human evaluation that validates our main findings. 6.3 Baseline 7 Training and Decoding Experimental Results The perplexity"
I17-1061,N16-1014,1,0.777511,"ovements over baseline model quality, generating responses that capture more precisely speakers’ traits and speaking styles. The model offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers. 1 Figure 1: Existing neural conversational models (baseline) tend to produce generic responses. The system presented in this paper better represents the speaker role (support person), domain of expertise (technical), and speaking style (courteous). sponses that are often commonplace and uninteresting (Li et al., 2016a; Shao et al., 2017). This is illustrated in Fig. 1, where the output of a standard Sequence-to-Sequence conversation model is contrasted with that of the best system presented in this work. The baseline system generates a desultory answer that offers no useful information and is unlikely to inspire user confidence. The output of the second system, however, strongly reflects the agent’s role in providing technical support. It not only evidences domain knowledge, but also manifests the professional politeness associated with a speaker in that role. The challenge for neural conversation systems"
I17-1061,D11-1054,0,0.0941847,"to exploit directly, since, not being in conversational format, it does not mesh easily with existing source-target conversational models. Introduction Conversational engines are key components of intelligent “personal assistants” such as Apple’s Siri and Amazon’s Alexa. These assistants can perform simple tasks, answer questions, provide recommendations, and even engage in chitchats (De Mori et al., 2008; Chen et al., 2015, 2016). The emergence of these agents has been paralleled by burgeoning interest in training natural-sounding dialog systems from conversational exchanges between humans (Ritter et al., 2011; Sordoni et al., 2015; Luan et al., 2014, 2015; Vinyals and Le, 2015). A major challenge for data-driven systems is how to generate output that corresponds to specific traits that the agent needs to adopt, as they tend to generate “consensus” re* This work was performed at Microsoft. 605 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 605–614, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al."
I17-1061,P16-1094,1,0.933706,"ovements over baseline model quality, generating responses that capture more precisely speakers’ traits and speaking styles. The model offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers. 1 Figure 1: Existing neural conversational models (baseline) tend to produce generic responses. The system presented in this paper better represents the speaker role (support person), domain of expertise (technical), and speaking style (courteous). sponses that are often commonplace and uninteresting (Li et al., 2016a; Shao et al., 2017). This is illustrated in Fig. 1, where the output of a standard Sequence-to-Sequence conversation model is contrasted with that of the best system presented in this work. The baseline system generates a desultory answer that offers no useful information and is unlikely to inspire user confidence. The output of the second system, however, strongly reflects the agent’s role in providing technical support. It not only evidences domain knowledge, but also manifests the professional politeness associated with a speaker in that role. The challenge for neural conversation systems"
I17-1061,P16-1009,0,0.0899948,"to specific traits that the agent needs to adopt, as they tend to generate “consensus” re* This work was performed at Microsoft. 605 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 605–614, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al., 2016a). This is a technique that has seen success in machine translation, where large monolingual data sets have been used to improve translation models (Sennrich et al., 2016). The intuition is that if two tasks are related, then joint training and parameter sharing can enable one task to benefit the other. In our case, this sharing is between two models: On one hand, a standard Sequence-to-Sequence conversational models is trained to predict the current response given the previous context. On the other hand, using the non-conversational data, we introduce an autoencoder multi-task learning strategy that predicts the response given the same sequence, but with the target parameters tied with the general conversational model. Our experiments with 4M conversation trip"
I17-1061,D17-1235,0,0.028605,"line model quality, generating responses that capture more precisely speakers’ traits and speaking styles. The model offers the benefits of being algorithmically simple and easy to implement, and not relying on large quantities of data representing specific individual speakers. 1 Figure 1: Existing neural conversational models (baseline) tend to produce generic responses. The system presented in this paper better represents the speaker role (support person), domain of expertise (technical), and speaking style (courteous). sponses that are often commonplace and uninteresting (Li et al., 2016a; Shao et al., 2017). This is illustrated in Fig. 1, where the output of a standard Sequence-to-Sequence conversation model is contrasted with that of the best system presented in this work. The baseline system generates a desultory answer that offers no useful information and is unlikely to inspire user confidence. The output of the second system, however, strongly reflects the agent’s role in providing technical support. It not only evidences domain knowledge, but also manifests the professional politeness associated with a speaker in that role. The challenge for neural conversation systems, then, is that an ag"
I17-1061,D16-1230,0,0.0284166,"Missing"
I17-1061,N15-1020,1,0.282437,"since, not being in conversational format, it does not mesh easily with existing source-target conversational models. Introduction Conversational engines are key components of intelligent “personal assistants” such as Apple’s Siri and Amazon’s Alexa. These assistants can perform simple tasks, answer questions, provide recommendations, and even engage in chitchats (De Mori et al., 2008; Chen et al., 2015, 2016). The emergence of these agents has been paralleled by burgeoning interest in training natural-sounding dialog systems from conversational exchanges between humans (Ritter et al., 2011; Sordoni et al., 2015; Luan et al., 2014, 2015; Vinyals and Le, 2015). A major challenge for data-driven systems is how to generate output that corresponds to specific traits that the agent needs to adopt, as they tend to generate “consensus” re* This work was performed at Microsoft. 605 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 605–614, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP In this paper we address the joint problems of blandness and data scarcity with multi-task learning (Caruana, 1998; Liu et al., 2015; Luan et al., 2016a). This is a te"
I17-1061,N15-1092,1,0.125072,"Missing"
I17-1061,D15-1199,0,0.0128467,"Missing"
I17-1096,D14-1162,0,0.0806857,"to handle multiple passages, we incorporated an extra passage ranker component. The architecture is shown in Figure 1. In brief, the embedding/encoder layers first build representations of Q and P . The aggregation layer uses co-attention to fuse information from the QP pair. The output layer is a recurrent net that maintains intermediate state and dynamically decides at which turn to generate the answer. 3.1 Detailed description of ReasoNet++ Embedding Layer: We adopt three types of embeddings to represent input word tokens in Q and P . For word embeddings, we use pre-trained GloVe vectors (Pennington et al., 2014). To address the out-of-vocabulary problem, we also include character and letter 3-gram embeddings. Character embeddings are fed into a convolutional neural network (CNN) as in (Kim, 2014), then max-pooled to form a fixed-size vector for each token. For letter 3-gram embeddings, we follow Huang et al. (2013) by first hashing each word as a bag of letter 3-gram, then feeding them into another CNN. The concatenation of all embeddings are then fed to a two-layer Highway Network (Srivastava et al., 2015). Therefore, we obtain the 959 Pi=m−1 p p H p , H q C q , i=0 H:i c are all 2d by n matrices, U"
I17-1096,D16-1264,0,0.366414,"models have embraced this kind of multiple-turn strategy; they generate predictions by making multiple passes through the same text and integrating intermediate information in the process (Hill et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Shen et al., 2016). While state-of-the-art results have been achieved by these models, there has yet to be an in-depth analysis of the impact of the multiple-turn strategy to reasoning. This paper attempts to fill this gap. We provide empirical results and analysis on two challenging RC datasets: the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), and the Microsoft Machine Reading We find that multiple-turn reasoning outperforms single-turn reasoning across the board for various types of question and answer types. Furthermore, the flexibility to dynamically decide the 957 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 957–966, c Taipei, Taiwan, November 27 – December 1, 2017 2017 AFNLP query source answer (A) #questions (Q) #passages (P ) SQuAD crowdsourced span of words 100K questions 23K paragraphs MS MARCO user logs free-form text 100K queries 1M paragraphs MS MARCO: MS MARCO is a la"
I17-1096,D13-1020,0,0.0623584,"Missing"
I17-1096,D14-1181,0,0.00420097,"e aggregation layer uses co-attention to fuse information from the QP pair. The output layer is a recurrent net that maintains intermediate state and dynamically decides at which turn to generate the answer. 3.1 Detailed description of ReasoNet++ Embedding Layer: We adopt three types of embeddings to represent input word tokens in Q and P . For word embeddings, we use pre-trained GloVe vectors (Pennington et al., 2014). To address the out-of-vocabulary problem, we also include character and letter 3-gram embeddings. Character embeddings are fed into a convolutional neural network (CNN) as in (Kim, 2014), then max-pooled to form a fixed-size vector for each token. For letter 3-gram embeddings, we follow Huang et al. (2013) by first hashing each word as a bag of letter 3-gram, then feeding them into another CNN. The concatenation of all embeddings are then fed to a two-layer Highway Network (Srivastava et al., 2015). Therefore, we obtain the 959 Pi=m−1 p p H p , H q C q , i=0 H:i c are all 2d by n matrices, U is a 8d by n matrix. Finally, to incorporate the full context, the “memory cells” of the passage are computed by a bidirectional GRU on top of U : final embedding for the words in Q as a"
I17-1096,W04-1013,0,0.0155775,"e down answers in free-form text, and according to the authors of MS MARCO, the complexity of answer varies from a single “yes/no” or entity name (e.g. Q: “What is the capital of Italy”; A: Rome), to long textual answers (e.g. Q: “What is the agenda for Hollande’s state visit to Washington?”). Long textual answers may need to be derived through reasoning across multiple pieces of text. The dataset is partitioned into a 82,430 training, a 10,047 development, and 9,650 test tuples. Since the answer is free-form text, the evaluation metrics of choice are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004). To apply the same RC model to both SQuAD (where answers are text spans in P ) and MS MARCO (where answer are free-form text), we search for spans in MS MARCO’s passages that maximizes the ROUGEL score with the raw free-form answers. Our training data uses these spans as labels, but we evaluate our model with respect to the raw free-form answers; this has an upper bound of 94.23 BLEU and 87.53 ROUGE-L on the dev set. By this construction, there are multiple number of passages to read for each question, but the answer span might only involve a few passages (i.e. the ones that include the max R"
I17-1096,D16-1013,0,0.0289406,"Missing"
I17-1096,P02-1040,0,0.0987244,"on retrieval system. The judges write down answers in free-form text, and according to the authors of MS MARCO, the complexity of answer varies from a single “yes/no” or entity name (e.g. Q: “What is the capital of Italy”; A: Rome), to long textual answers (e.g. Q: “What is the agenda for Hollande’s state visit to Washington?”). Long textual answers may need to be derived through reasoning across multiple pieces of text. The dataset is partitioned into a 82,430 training, a 10,047 development, and 9,650 test tuples. Since the answer is free-form text, the evaluation metrics of choice are BLEU (Papineni et al., 2002) and ROUGE-L (Lin, 2004). To apply the same RC model to both SQuAD (where answers are text spans in P ) and MS MARCO (where answer are free-form text), we search for spans in MS MARCO’s passages that maximizes the ROUGEL score with the raw free-form answers. Our training data uses these spans as labels, but we evaluate our model with respect to the raw free-form answers; this has an upper bound of 94.23 BLEU and 87.53 ROUGE-L on the dev set. By this construction, there are multiple number of passages to read for each question, but the answer span might only involve a few passages (i.e. the one"
I17-5003,N16-1014,1,0.743479,"ieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism to control the dialogue act during generation in order to avoid repetition. Social Chat Bots Social bots are of growing importance in facilitating smooth interaction between humans and their electronic devices. Recently, researcher have begun to explore data-driven generation of conversational responses within the framework of nerual machine translation (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a), as illustrated in Figure 2. However, the generated responses are often too general to carry meaningful information, such as “I don’t know.”, which can serve as a response to any user questions. A mutual information based model was proposed to address the issue, a mutual information model is proposed by Li et al. (2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al. (2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots"
I17-5003,P16-1094,1,0.83828,"ieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism to control the dialogue act during generation in order to avoid repetition. Social Chat Bots Social bots are of growing importance in facilitating smooth interaction between humans and their electronic devices. Recently, researcher have begun to explore data-driven generation of conversational responses within the framework of nerual machine translation (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a), as illustrated in Figure 2. However, the generated responses are often too general to carry meaningful information, such as “I don’t know.”, which can serve as a response to any user questions. A mutual information based model was proposed to address the issue, a mutual information model is proposed by Li et al. (2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al. (2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots"
I17-5003,D16-1127,1,0.772136,"ieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism to control the dialogue act during generation in order to avoid repetition. Social Chat Bots Social bots are of growing importance in facilitating smooth interaction between humans and their electronic devices. Recently, researcher have begun to explore data-driven generation of conversational responses within the framework of nerual machine translation (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016a), as illustrated in Figure 2. However, the generated responses are often too general to carry meaningful information, such as “I don’t know.”, which can serve as a response to any user questions. A mutual information based model was proposed to address the issue, a mutual information model is proposed by Li et al. (2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al. (2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots"
I17-5003,I17-1074,1,0.653231,"Missing"
I17-5003,P17-1163,0,0.0437381,"Missing"
I17-5003,W16-3601,0,0.0238467,"works, including supervised learning and reinforcement learning (Yang et al., 2017). Wen et al. (2016) and Bordes and Weston (2016) introduced a network-based end-to-end trainable task-oriented dialogue system. The authors treated training a dialogue system as learning a mapping from dialogue histories to system responses, and applied an encoder-decoder model.However, the system is trained in a supervised fashion that requires a lot of training data. Thus, the agent cannot learn a robust dialogue policy since it never explore the unknown space that is not covered by the limited training data. Zhao and Eskenazi (2016) presented an end-toend reinforcement learning (RL) approach to dialogue state tracking and policy learning. They show some promising results when applying the agent to the task of guessing the famous person a user is thinking of. Dhingra et al. (2017) proposed an end-to-end differentiable KB-Infobot for efficient information access. Li et al. (2017b) presented an end-to-end neural dialogue system for task completion. The agent can handle a wide varity of question types, including user-initated request. 5 Instructors Yun-Nung (Vivian) Chen is currently an assistant professor at the Department"
I17-5003,N15-1020,1,0.821506,"these models (Chen et al., 2016a,b). In addition, the importance of the NLU module is investigated in Li et al. (2017a), showing that different types of errors from NLU can degrade the whole system’s performance in a reinforcement learning setting. Natural Language Generation NLG approaches can be grouped into two categories, one focuses on generating text using templates or rules (linguistic) methods, the other uses corpus-based statistical methods (Oh and Rudnicky, 2002). The RNN-based models have been applied to language generation for both social bots and task-orientated dialogue systems (Sordoni et al., 2015; Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned 7 Source: conversation history … because encoder of your game? EOS Yeah I’m on my Yeah I’m on my way decoder Target: response Figure 2: Illustration of a sequence-to-sequence model for chit-chat dialogues. 4 data by jointly optimizing sentence planning and surface realization, and language variation can be achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism to control the dialogue act during generation in order to"
I17-5003,W17-5505,0,0.0173344,"inforcement learning (Li et al., 2016c). Furthermore, Li et al. (2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots are originally developed for different purposes, there is a trend of combining both as a step towards building an open-domain dialogue agent. For example, on the one hand, Ghazvininejad et al. (2017) presented a fully data-driven and knowledge-grounded neural conversation model aimed at producing more contentful responses without slot filling. On the other hand, Zhao et al. (2017) proposed a task-oriented dialogue agented based on the encoder-decoder model with chatting capability. End-to-End Task-Oriented Dialogue System Awaring the representation power of deep neural networks, there are more and more attempts to learning dialogue systems in an end-to-end fashion using different learning frameworks, including supervised learning and reinforcement learning (Yang et al., 2017). Wen et al. (2016) and Bordes and Weston (2016) introduced a network-based end-to-end trainable task-oriented dialogue system. The authors treated training a dialogue system as learning a mapping"
I17-5003,W15-4639,0,0.0679508,"Missing"
I17-5003,D15-1199,0,0.023949,"tion, the importance of the NLU module is investigated in Li et al. (2017a), showing that different types of errors from NLU can degrade the whole system’s performance in a reinforcement learning setting. Natural Language Generation NLG approaches can be grouped into two categories, one focuses on generating text using templates or rules (linguistic) methods, the other uses corpus-based statistical methods (Oh and Rudnicky, 2002). The RNN-based models have been applied to language generation for both social bots and task-orientated dialogue systems (Sordoni et al., 2015; Vinyals and Le, 2015; Wen et al., 2015b). The RNN-based NLG can learn from unaligned 7 Source: conversation history … because encoder of your game? EOS Yeah I’m on my Yeah I’m on my way decoder Target: response Figure 2: Illustration of a sequence-to-sequence model for chit-chat dialogues. 4 data by jointly optimizing sentence planning and surface realization, and language variation can be achieved by sampling from output candidates (Wen et al., 2015a). Moreover, Wen et al. (2015b) improved the prior work by adding a gating mechanism to control the dialogue act during generation in order to avoid repetition. Social Chat Bots Socia"
I17-5003,W13-4073,0,\N,Missing
I17-5003,P17-1045,1,\N,Missing
J05-4005,J96-1002,0,0.0113923,"Missing"
J05-4005,J95-4004,0,0.0136868,"ecall rates (Wu 2003). Therefore, we do not assume that an application-independent universal word segmentation standard exists. We argue instead for the existence of multiple segmentation standards, each for a specific application. It is undesirable to develop a set of application-specific segmenters. A better solution would be to develop a generic segmenter with customizable output that is able to provide alternative segmentation units according to the specification that is either predefined or implied in the application data. To achieve this, we present a transformation-based learning (TBL; Brill 1995) method, to be described in Section 6. We implement the pragmatic approach to Chinese word segmentation in an adaptive Chinese word segmenter called MSRSeg. It consists of two components: (1) a generic segmenter that is based on the linear mixture model framework of word breaking and unknown word detection and that can adapt to domain-specific vocabularies, and (2) a set of output adaptors for adapting the output of (1) to different application-specific standards. Evaluation on five test sets with different standards shows that the adaptive system achieves state-of-the-art performance on all t"
J05-4005,O97-4005,0,0.0121829,"mance of these methods thus depends to a large degree upon the coverage of the dictionary, which unfortunately may never be complete because new words appear constantly. Therefore, in addition to the dictionary, many systems also contain special components for unknown word identification. 533 Computational Linguistics Volume 31, Number 4 In particular, statistical methods have been widely applied because they use a probabilistic or cost-based scoring mechanism rather than a dictionary to segment the text. These methods have three drawbacks. First, some of these methods (e.g., Lin et al. 1993; Chang and Su 1997) identify OOV (out-of-vocabulary) words without identifying their types. For instance, one might identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmenters, OOV identifica"
J05-4005,W03-1721,0,0.0815062,"ir types. For instance, one might identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmenters, OOV identification is considered a separate process from segmentation (e.g., Chen 2003; Wu and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words are usually two or more characters long and are often segmented into single characters. He then uses different components to detect OOV words of different types in a cascaded manner after the basic word segmentation. We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is Sproat et al. (1996), which is based on weighted finite-state transduc"
J05-4005,O98-3002,0,0.0319127,"ght identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmenters, OOV identification is considered a separate process from segmentation (e.g., Chen 2003; Wu and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words are usually two or more characters long and are often segmented into single characters. He then uses different components to detect OOV words of different types in a cascaded manner after the basic word segmentation. We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our approach is similarly m"
J05-4005,W02-1001,0,0.00419524,"d feature value f (s, wR )), and decreases the parameter values whose models were “overestimated” (i.e., f (s, w) is larger than f (s, wR )). Empirically, the sequence of these updates, when iterated over all training samples, provides a reasonable approximation to descending the gradient with respect to the original loss function of Equation (5). Although this method cannot guarantee a globally optimal solution, it is chosen for our modeling because of its efficiency and because it achieved the best results in our experiments. The algorithm is similar to the perceptron algorithm described in Collins (2002). The key difference is that, instead of using the delta rule of Equation (8) (as shown in line 5 of Figure 4), Collins (2002) updates parameters using the rule: λdt+1 ← λtd + fd (wRi ) − fd (wi ). Our pilot study shows that our algorithm achieves slightly better results. 4.3 Discussions on Robustness The training methods described in Section 4.2 aim at minimizing errors in a training set. But test sets can be different. The robustness issue concerns how well the minimal error rate in the training set preserves in the test set. According to Dudewicz and Mishra (1988), the MSE function in gener"
J05-4005,P00-1073,1,0.796307,"hose words whose probability of appearing in a new document is lower than general lexical words. Let Pi (k) be the probability of word wi that occurs k times in a document. In our experiments, we assume that P(NW |wi ) can be approximated by the probability of wi occurring less than K times in a new document: P(NW |wi ) ≈ K1  Pi (k), (14) k=0 where the constant K (7 in our experiments) is dependent on the size of the document: The larger the document, the larger the value. Pi (k) can be estimated using several term ¨ distribution models (Chapter 15.3 in Manning and Schutze [1999]). Following Gao and Lee (2000), we use K-Mixture (Katz 1996) which estimates Pi (k) as Pi (k) = (1 − a)δk, 0 + a ( β )k , β+1 β+1 (15) where δk,0 =1 if k=0; 0, otherwise. α and β are parameters that can be fit using the observed mean λ and the observed inverse document frequency IDF as follows: λ= cf , IDF = log N , N df β = λ × 2IDF − 1 = cf − df , and a = λ , β df (16) where cf is the total number of occurrence of word wi in training data, df is the number of documents in training data in which wi occurs, and N is the total number of documents. In our implementation, the training data contain approximately 40,000 documen"
J05-4005,O01-2002,1,0.812473,"Missing"
J05-4005,P03-1035,1,0.48854,"Missing"
J05-4005,P04-1059,1,0.834246,"tions. This inspires the development of an adaptive Chinese word segmenter. However, most of the previous segmenters have been developed according to a standard that assumes a single correct segmentation. The only adaptive system, to the best of our knowledge, is the customizable segmenter described in Wu (2003), in which the display of the segmentation output can be customized by users.3 The adaptation method we will describe in Section 6 can be viewed as an improved version in that the adaptation rules (or transformations) are acquired automatically from application data via the TBL method (Gao et al. 2004). Though the use of TBL for Chinese word segmentation is not new (see Palmer [1997]; Hockenmaier and Brew [1998]), none of the previous work is aimed at standards adaptation. 2.4 Evaluation The performance of Chinese word segmenters is generally reported in terms of precision and recall. However, a comparison across systems could be very difficult for two reasons. First, the “correct” segmentation is not clearly defined. It is common that for a given sentence there are multiple plausible word segmentations. As shown in Sproat et al. (1996), the rate of agreement between two human judges is les"
J05-4005,H05-1027,1,0.678464,"ss function. We will present in turn the loss function and the optimization algorithm. 4.2.1 Loss Function. Assume that we can measure the number of segmentation errors in w by comparing it with a reference segmentation wR using an error function Er(wR , w) (i.e., editing distance, in our case). The training criterion that directly minimizes the segmentation errors over the training data is λ ∗ = arg min λ  Er(wRi , w(si , λ )), (4) i=1...M where w(si , λ ) is the segmentation determined by Equation (3), where it is denoted as w∗ . Equation (4) is referred to as the minimum sample risk (MSR; Gao et al. 2005) criterion hereafter. Notice that without knowing the “true” distribution of the data, the best λ can be chosen approximately based on training samples. This is known as the principle of empirical risk minimization (ERM; Vapnik 1998): If the segmenter were trained using exactly the MSR criterion, it would converge to a Bayes risk performance (minimal error rate) as the training size goes to infinity. However, Er(.) is a piecewise constant function of the model parameter λ , and thus a poor candidate for optimization by any simple gradient-type numerical search. For example, the gradient cannot"
J05-4005,O97-4003,0,0.051105,"Approach Figure 2 Taxonomy of morphologically derived words (MDWs) in MSRSeg. 3.2 MSR Standard The taxonomy employed here has been specified in detail in the MSR standard. There are two general guidelines for the development of the standard: 1. The standard should be applicable to a wide variety of NLP tasks, of which some representative examples are Chinese text input, IR, TTS, ASR, and MT. 2. The standard should be compatible with existing standards, of which representative examples are the Chinese NE standards in ET/ER-99, the Mainland standard (GB/T), Taiwan’s ROCLING standard (CNS14366; Huang et al. 1997), and the UPenn Chinese Treebank (Xia 1999), as much as possible.4 We are seeking a standard that is “linguistically felicitous, computationally feasible, and [ensures] data uniformity” (Huang et al. 1997; Sproat and Shih 2002). The MSR standard consists of a set of specific rules that aims at unambiguously determining the word segmentation of a Chinese sentence, given a reference lexicon. The development of the standard is an iterative procedure, interacting with the development of a gold test set (which we will describe in the next section). We begin with an initial set of 4 MET is a Chinese"
J05-4005,W03-1701,1,0.693218,"hods use metrics that are based on statistical features such as mutual information, term frequency, and their variants. They require a reasonably large training corpus. The new words detected are mostly proper nouns and other relatively frequent words. Unfortunately, new words, under our definition of the term, may not be detected.  534  ^x Gao et al. Chinese Word Segmentation: A Pragmatic Approach Fewer methods have been proposed for an on-line approach, and that is the focus of this article. Some recent advances in on-line NWI explore the use of machine learning approaches. For example, Li et al. (2003) define NWI as a binary classification problem and use support vector machines (SVM) to combine various linguistically motivated features to determine whether a Chinese character sequence is a word. Our method is an extension of that of Li et al. in that NWI is not a stand-alone process in our system but an integral part of word segmentation. We shall show experimentally the benefit of the integration in Section 5.5. 2.3 Standards Adaptation As described earlier, while Chinese words are supposed to be well-defined, unambiguous, and static linguistic entities, we are more concerned with segment"
J05-4005,O93-1004,0,0.0610786,"1994). The performance of these methods thus depends to a large degree upon the coverage of the dictionary, which unfortunately may never be complete because new words appear constantly. Therefore, in addition to the dictionary, many systems also contain special components for unknown word identification. 533 Computational Linguistics Volume 31, Number 4 In particular, statistical methods have been widely applied because they use a probabilistic or cost-based scoring mechanism rather than a dictionary to segment the text. These methods have three drawbacks. First, some of these methods (e.g., Lin et al. 1993; Chang and Su 1997) identify OOV (out-of-vocabulary) words without identifying their types. For instance, one might identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmen"
J05-4005,P03-1021,0,0.00297496,"t the source–channel models are the rationale behind our system, e.g., the decoding process described in Section 5.6 follows the framework. Linear models are just another representation based on the optimization algorithm of class model weights. 4.2 Linear Models The framework of linear models is derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and has been recently introduced into NLP tasks by Collins and Duffy (2001). It is also related to (log-)linear models described in Berger, Della Pietra, and Della Pietra (1996), Xue (2003); Och (2003), and Peng, Feng, and McCallum (2004). We use the following notation in the rest of the article. r r 546 Training data are a set of example input/output pairs. In Chinese word segmentation, we have training samples {si , wRi }, for i = 1 . . . M, where each si is an input Chinese character sequence and each wRi is the reference segmentation (i.e., word class sequence) of si . We assume a set of D + 1 features fd (s, w), for d = 0 . . . D. The features are arbitrary functions that map (s, w) to real values. Using vector notation, we have f(s, w) ∈ D+1 , where f(s, w) = {f0 (s, w), f1 (s, w), ."
J05-4005,P97-1041,0,0.0213808,"Missing"
J05-4005,C04-1081,0,0.204751,"still informative. 568 Gao et al. Chinese Word Segmentation: A Pragmatic Approach Table 22 Cross-system comparison results. # OAS Segmenters LN PN ON errors P R F P R F P R F 63 49 20 7 .935 .854 .767 .876 .442 .720 .736 .864 .600 .782 .752 .870 .907 .945 .780 .830 .744 .781 .787 .897 .818 .856 .784 .862 .642 .713 .817 .799 .469 .131 .216 .617 .600 .222 .342 .696 MSWS LCWS PBWS MSRSeg Table 23 Comparisons against other segmenters: In Column 1, SXX indicates participating sites in the 1st SIGHAN International Chinese Word Segmentation Bakeoff, and CRFs indicates the word segmenter reported in (Peng et al. 2004). In Columns 2 to 5, entries contain the F-measure of each segmenter on different open runs, with the best performance in bold. Column Site-Avg is the average F-measure over the data sets on which a segmenter reported results of open runs, where a bolded entry indicates the segmenter outperforms MSRSeg. Column Our-Avg is the average F-measure of MSRSeg over the same data sets, where a bolded entry indicates that MSRSeg outperforms the other segmenter. ASo S01 S02 S03 S04 S05 S06 S07 S08 S09 S10 S11 S12 CRFs MSRSeg ASc .872 CTBo CTBc .881 .912 .829 .881 .874 .942 .945 HKo HKc PKo PKc Site-Avg O"
J05-4005,W03-1719,0,0.120733,"into the first three: descriptive, expository, and narrative. Practical writing is just an umbrella term for practical writing such as notes, letter, e-mails, and marriage announcements. 541 Computational Linguistics Volume 31, Number 4 Figure 3 Fragments of the MSR gold test set. Therefore, we evaluate MSRSeg using five corpora, each corresponding to a different standard, and consistent train–test splits, as shown in Table 4. MSR is described in previous sections, and the other four are standards used in SIGHAN’s First International Chinese Word Segmentation Bakeoff (or Bakeoff for brevity) (Sproat and Emerson 2003). In the Bakeoff corpora, OOV is defined as the set of words in the test corpus not occurring in the training corpus. In experiments, we always consider the following adaptation paradigm. Suppose we have a general predefined standard according to which we create a large amount of training data. We then develop a generic word segmenter. Whenever we deploy the segmenter for any application, we customize the output of the segmenter according to an application-specific standard that can be partially acquired from a given small amount of application data (called adaptation data). The MSR standard d"
J05-4005,J96-3004,0,0.39825,"on is considered a separate process from segmentation (e.g., Chen 2003; Wu and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words are usually two or more characters long and are often segmented into single characters. He then uses different components to detect OOV words of different types in a cascaded manner after the basic word segmentation. We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our approach is similarly motivated but is based on a different mechanism: linear mixture models. As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information. Many types of OOV words that are not covered in Sproat’s system can be dealt with in our system. The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Colli"
J05-4005,C02-1012,1,0.213291,"Missing"
J05-4005,O03-5001,1,0.841659,"Missing"
J05-4005,W01-0512,0,0.00770431,"Missing"
J05-4005,W00-1207,1,0.539293,"or instance, one might identify a string as a unit but fail to identify that it is a person name. Second, many current statistical methods do not incorporate linguistic knowledge effectively into segmentation. For example, Teahan et al. (2000) and Dai et al. (1999) do not use any linguistic knowledge. Thus, the identified OOV words are likely to be linguistically implausible, and consequently, additional manual checking is needed for some subsequent tasks such as parsing. Third, in many current segmenters, OOV identification is considered a separate process from segmentation (e.g., Chen 2003; Wu and Jiang 2000; Chen and Bai 1998). For instance, Chen (2003) assumes that OOV words are usually two or more characters long and are often segmented into single characters. He then uses different components to detect OOV words of different types in a cascaded manner after the basic word segmentation. We believe that the identification of OOV words should not be treated as a problem separate from word segmentation. We propose a unified approach that solves both problems simultaneously. A previous work along this line is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our app"
J05-4005,O03-4001,1,0.946382,"ion. Word breaking refers to the process of segmenting known words that are predefined in a lexicon. Word segmentation refers to the process of both lexicon word segmentation and unknown word detection. 2 New words in this article refer to out-of-vocabulary words that are neither recognized as named entities or factoids nor derived by morphological rules. These words are mostly domain-specific and/or time-sensitive (see Section 5.5 for details). 532 Gao et al. Chinese Word Segmentation: A Pragmatic Approach formation retrieval (IR) systems prefer shorter “words” to obtain higher recall rates (Wu 2003). Therefore, we do not assume that an application-independent universal word segmentation standard exists. We argue instead for the existence of multiple segmentation standards, each for a specific application. It is undesirable to develop a set of application-specific segmenters. A better solution would be to develop a generic segmenter with customizable output that is able to provide alternative segmentation units according to the specification that is either predefined or implied in the application data. To achieve this, we present a transformation-based learning (TBL; Brill 1995) method, t"
J05-4005,O03-4002,0,0.770183,"ear mixture models. As we shall see, the models provide a more flexible framework for incorporating various kinds of lexical and statistical information. Many types of OOV words that are not covered in Sproat’s system can be dealt with in our system. The linear models we used are originally derived from linear discriminant functions widely used for pattern classification (Duda, Hart, and Stork 2001) and have been recently introduced into NLP tasks by Collins and Duffy (2001). Other frameworks of Chinese word segmentation, which are similar to the linear models, include maximum entropy models (Xue 2003) and conditional random fields (Peng, Feng, and McCallum 2004). They also use a unified approach to word breaking and OOV identification. 2.2 More on New Word Identification In this article, we use the term “new words” to refer to OOV words other than named entities, factoids, and morphologically derived words. “New words” are mostly domain‘cellular’) and time-sensitive political, social, or cultural specific terms (e.g., ‘Three Links’; ‘SARS’). There have been two general approaches terms (e.g., to NWI. The first is to acquire new words from large corpora off-line and put them into a dictiona"
J05-4005,W03-1718,1,\N,Missing
J05-4005,P98-2206,0,\N,Missing
J05-4005,C98-2201,0,\N,Missing
J05-4005,J00-3004,0,\N,Missing
J05-4005,W03-1726,0,\N,Missing
N06-1059,W03-0508,0,\N,Missing
N06-1059,P02-1040,0,\N,Missing
N06-1059,N04-1019,0,\N,Missing
N06-1059,N03-1020,1,\N,Missing
N06-1059,W04-1013,1,\N,Missing
N12-3006,D09-1111,1,0.814882,"Missing"
N12-3006,J93-2004,0,0.04636,"the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty,"
N12-3006,J05-1004,0,0.0277178,". The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty, and shared task organizers are now"
N12-3006,P06-1055,0,0.0153341,"Missing"
N12-3006,C08-1094,0,0.0225179,"Missing"
N12-3006,J08-2002,1,0.0600596,"abels, like ARG0, ARG1, …, ARG5 for core arguments, and labels like ARGMTMP,ARGM-LOC, etc. for adjunct-like arguments. The meaning of the numbered arguments is verb-specific, with ARG0 typically representing an agent-like role, and ARG1 a patient-like role. This implementation of an SRL system follows the approach described in (Xue and Palmer, 04), and includes two log-linear models for argument identification and classification. A single syntax tree generated by the MSR SPLAT split-merge parser is used as input. Non-overlapping arguments are derived using the dynamic programming algorithm by Toutanova et al. (2008). 3 3.1 Other Language Analysis Functionality Sentence Boundary / Tokenization This analyzer identifies sentence boundaries and breaks the input into tokens. Both are represented as offsets of character ranges. Each token has both a raw form from the string and a normalized form in the PTB specification, e.g., open and close parentheses are replaced by -LRB- and -RRB-, respectively, to remove ambiguity with parentheses indicating syntactic structure. A finite state machine using simple rules and abbreviations detects sentence boundaries with high accuracy, and a set of regular expressions toke"
N12-3006,W04-3212,0,0.0291436,"Missing"
N12-3006,J03-4003,0,\N,Missing
N13-1002,P11-2031,0,0.0145787,"5-gram models being better than the 3-gram. Here the individual 3-gram models are better than the baseline at significance level 0.02 and their combination is better than the baseline at our earlier defined threshold of 0.01. The withinphrase MTU MMs (results shown in the last two rows) improve upon the baseline slightly, but here again the improvements mostly stem from the use of context across phrase boundaries. Our final results on German-English are better than the best result of 27.30 from the shared task (Koehn and Monz, 2006). Thanks to the reviewers for referring us to recent work by (Clark et al., 2011) that pointed out problems with significance tests for machine translation, where the randomness and local optima in the MERT weight tuning method lead to a large variance in development and test set performance across different runs of optimization (using a different random seed or starting point). (Clark et al., 2011) proposed a stratified approximate randomization statistical significance test, which controls for optimizer instability. Using this test, for the English-Bulgarian system, we confirmed that the combination of four 3-gram MMs and the combination of 5-gram MMs is better than the"
N13-1002,P11-1105,0,0.148685,"nguage pairs, looking at distinct orders in isolation and combination. 12 Proceedings of NAACL-HLT 2013, pages 12–21, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics 2 Related work Marino et al. (2006) proposed a translation model using a Markov model of bilingual n-grams, demonstrating state-of-the-art performance compared to conventional phrase-based models. Crego and Yvon (2010) further explored factorized n-gram approaches, though both models considered rather large n-grams; this paper focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than"
N13-1002,D09-1117,0,0.259413,"similar to recombination in a standard-phrase based decoder with the difference that it is not always the last two target MTUs that define the context needed by future extensions. The weights λ of different models are trained on a development set using MER training to maximize the BLEU score of the resulting model. Note that this method of model combination was not considered in any of the previous works comparing different decompositions. The system combination method is motivated by prior work in machine translation which combined left-to-right and right-to-left machine translation systems (Finch and Sumita, 2009). Similarly, we perform sentence-level system combination between systems using different MTU Markov models to come up with most likely translations. If we have k systems guessing hypotheses based on MM1 , . . . , MMk respectively, we generate 1000best lists from each system, resulting in a pool of up to 1000k possible distinct translations. Each of the candidate hypotheses from MMi is scored with its Markov model log-probability logP MMi (h). We compute normalized probabilities for each system’s n-best by exponentiating and normalizing: Pi (h) ∝ P MMi (h). If a hypothesis h is not in system i"
N13-1002,W07-0711,0,0.0146267,"lopment set consists of 1,497 sentences, the English side from WMT 2009 news test data, and the Bulgarian side a human translation thereof. The test set comes from the same mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starre"
N13-1002,W06-3114,0,0.190924,"T 2002 and 2003 test sets as the development set and the NIST 2005 test set as our test set. The baseline phrasal system uses a 5-gram language model with modified Kneser-Ney smoothing (Kenser and Ney, 1995), trained on the Xinhua portion of the English Gigaword corpus (238M English words). For German-English we used the dataset from Language Chs-En Deu-En En-Bgr Training 1 Mln 751 K 4 Mln Dev NIST02+03 WMT06dev 1,497 Test NIST05 WMT06test 2,498 Model Baseline L2RT R2LT L2RS R2LS 4 MMs 4 MMs phrs Table 3: Data sets for different language pairs. the WMT 2006 shared task on machine translation (Koehn and Monz, 2006). The parallel training set contains approximately 751K sentences. We also used the English monolingual data of around 1 million sentences for language model training. The development set contains 2000 sentences. The final test set (the in-domain test set for the shared task) also contains 2000 sentences. Two Kneser-Ney language models were used as separate features: a 4gram LM trained on the parallel portion of the data, and a 5-gram LM trained on the monolingual corpus. For English-Bulgarian we used a dataset containing sentences from several data sources: JRCAcquis (Steinberger et al., 2006"
N13-1002,N03-1017,0,0.189237,"“that the source and target side of a tuple of words are synchronized, i.e. that they occur in the same order in their respective languages” (Crego and Yvon, 2010). For language pairs with significant typological divergences, such as Chinese-English, it is quite difficult to extract a synchronized sequence of units; in the limit, the smallest synchronized unit may be the whole sentence. Other approaches explore incorporation into syntax-based MT systems or replacing the phrasal translation system altogether. Introduction The translation procedure of a classical phrasebased translation model (Koehn et al., 2003) first divides the input sentence into a sequence of phrases, translates each phrase, explores reorderings of these translations, and then scores the resulting candidates with a linear combination of models. Conventional models include phrase-based channel models that effectively model each phrase as a large unigram, reordering models, and target language models. Of these models, only the target language model ∗ This research was conducted during the author’s internship at Microsoft Research We investigate the addition of MTUs to a phrasal translation system to improve modeling of context and"
N13-1002,W04-3250,0,0.0168628,"ate prediction. The best decomposition order varies from language to language: right-to-left in source order is best for Chinese-English, right-to-left in target order is best for German-English and left-to-right or rightto-left in target order are best in English-Bulgarian. We computed statistical significance tests, testing the difference between the L2RT model (the standard in prior work) and models achieving higher test set performance. The models that are significantly better at significance α &lt; 0.01 are marked with a star in the table. We used a paired bootstrap test with 10,000 trials (Koehn, 2004). Next we evaluate the methods for combining decomposition orders introduced in Sections 4.1 and 4.2. The results are reported in Table 2. The upper part of the table focuses on combining different 18 Model Baseline-1 TgtProduct TgtSysComb TgtDynamic Baseline-2 AllProduct AllSyscomb Chs-En Dev Test 24.04 25.09 25.27 25.84* 24.49 25.27 24.07 25.10 26.48 27.96 28.68 29.59* 27.02 28.30 Deu-En Dev Test 30.14 30.14 30.47 30.49 30.20 30.15 30.60 30.41 30.14 30.14 31.54 31.36* 30.20 30.17 En-Bgr Dev Test 49.86 46.45 51.04 47.27* 50.46 46.31 49.99 46.52 49.86 46.45 51.50 48.10* 50.90 46.53 Table 2: Le"
N13-1002,J06-4004,0,0.727376,"Missing"
N13-1002,C08-1074,1,0.791289,"me mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report detailed experiments on ChineseEnglish, and then ve"
N13-1002,P02-1038,0,0.0382882,"approximations to the trigram MTU Markov model scores as future scores, since not all needed context is available for a hypothesis at the time of construction. As additional context becomes available, the exact score can be computed. 2 4.1 Basic decomposition order combinations We first introduce two methods of combining different decomposition orders: product and system combination. The product method arises naturally in the machine translation setting, where probabilities from different models are multiplied together and further weighted to form the log-linear model for machine translation (Och and Ney, 2002). We define a similar scoring function using a set of MTU Markov models MM 1 , ..., MM k for a hypothesis h as follows: Score(h) = λ1 logP MM1 (h) + ... + λk logP MMk (h) Figure 2: Lexical selection. 2 We use this constrained MT setting to evaluate the performance of models using different MTU decomposition orders and models using combinations of decomposition orders. The simplified setting allows 15 We apply hypothesis recombination, which can merge hypotheses that are indistinguishable with respect to future continuations. This is similar to recombination in a standard-phrase based decoder w"
N13-1002,P03-1021,0,0.0140661,"f. The test set comes from the same mixture of sources as the training set. For this system we used a single four-gram target language model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report detail"
N13-1002,P02-1040,0,0.102213,"uage model trained on the target side of the parallel corpus. All systems used phrase tables with a maximum length of seven words on either side and lexicalized reordering models. For the Chinese-English system we used GIZA++ alignments, and for the other two we used alignments by an HMM model augmented with word-based distortion (He, 2007). The alignments were symmetrized and then combined with the heuristics ”grow-diag-final-and”. 5 We tune parameters using MERT (Och, 2003) with random restarts (Moore and Quirk, 2008) on the development set. Case-insensitive BLEU-4 is our evaluation metric (Papineni et al., 2002). 19 3-gram models Dev Test 32.58 31.78 33.05 32.78* 33.05 32.96* 32.90 33.00* 32.94 32.98* 33.22 33.07* 32.58 31.78 5-gram models Dev Test 32.58 31.78 33.16 32.88* 33.16 32.81* 32.98 32.98* 33.09 32.96* 33.37 33.00* 32.58 31.78 Table 4: Reranking with 3-gram and 5-gram MTU translation models on Chinese-English. Starred results on the test set indicate significantly better performance than the baseline. 6.3 MT reranking experiments We first report detailed experiments on ChineseEnglish, and then verify our main conclusions on the other language pairs. Table 4 looks at the impact of individual"
N13-1002,N06-1002,1,0.92188,"anslation model using a Markov model of bilingual n-grams, demonstrating state-of-the-art performance compared to conventional phrase-based models. Crego and Yvon (2010) further explored factorized n-gram approaches, though both models considered rather large n-grams; this paper focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than augmenting them. We believe that Markov rules provide an additional signal but are not a replacement. Their distributions should be more informative than the so-called “lexical weighting” models, and less sparse than relative frequency estimates, thoug"
N13-1002,N03-1033,1,0.0127916,"text for disambiguation and it is not clear apriori which would perform best. We compare all four decomposition orders (source order left-to-right and right-to-left, and target order left-to-right and rightto-left). Although the independence assumptions of left-to-right and right-to-left are the same, the resulting models may be different due to smoothing. In addition to studying these four basic decomposition orders, we report performance of two cyclic orders: cyclic in source or target sentence order. These models are inspired by the cyclic dependency network model proposed for POS tagging (Toutanova et al., 2003) and also used as a baseline in previous work on dynamic decomposition orders (Tsuruoka and Tsujii, 2005). 1 The probability according to the cyclic orders is defined by conditioning each MTU on both its left and right neighbor MTUs. For example, the probability of the sentence pair in Figure 1 under the source cyclic order, using a 3-gram model is defined as: P(M1|M2) · P(M2|M1, M3) · P(M3|M2, M4) · P(M4|M3, M5) · P(M5|M4). All n-gram Markov models over MTUs are esti1 The correct application of such models requires sampling to find the highest scoring sequence, but we apply the max product ap"
N13-1002,H05-1059,0,0.328021,"unigrams to capture contextual information, n-grams of minimal translation units allow a robust contextual model that is less constrained by segmentation. 3.2 MTU enumeration orders When defining a joint probability distribution over MTUs of an aligned sentence pair, it is necessary to define a decomposition, or generation order for the sentence pair. For a single sequence in language modeling or synchronized sequences in channel modeling, the default enumeration order has been left-to-right. Different decomposition orders have been used in part-of-speech tagging and named entity recognition (Tsuruoka and Tsujii, 2005). Intuitively, information from the left or right could be more useful for particular disambiguation choices. Our research on different decomposition orders was motivated by this work. When applying such ideas to machine translation, there are additional challenges and opportunities. The task exhibits much more ambiguity – the number of possible MTUs is in the millions. An opportunity arises from the reordering phenomenon in machine translation: while in POS tagging the natural decomposition orders to study are only left-to-right and right-to-left, in machine translation we can further disting"
N13-1002,P11-1086,0,0.364188,"er focuses on small units with asynchronous orders in source and target. Durrani et al. (2011) developed a joint model that captures translation of contiguous and gapped units as well as reordering. Two prior approaches explored similar models in syntax based systems. MTUs have been used in dependency translation models (Quirk and Menezes, 2006) to augment syntax directed translation systems. Likewise in target language syntax systems, one can consider Markov models over minimal rules, where the translation probability of each rule is adjusted to include context information from parent rules (Vaswani et al., 2011). Most prior work tends to replace the existing probabilities rather than augmenting them. We believe that Markov rules provide an additional signal but are not a replacement. Their distributions should be more informative than the so-called “lexical weighting” models, and less sparse than relative frequency estimates, though potentially not as effective for truly non-compositional units. Therefore, we explore the inclusion of all such information. Also, unlike prior work, we explore combinations of multiple decomposition orders, as well as dynamic decompositions. The most useful context for t"
N13-1002,steinberger-etal-2006-jrc,0,\N,Missing
N13-1048,J93-2003,0,0.0320478,"e easily incorporated into a standard phrase-based SMT system, requiring no code change in the runtime engine. In the rest of the paper, Section 2 presents the MRF model for phrase translation. Section 3 describes the way the model parameters are estimated. Section 4 presents the experimental results on two Europarl translation tasks. Section 5 reviews previous work that lays the foundation of this study. Section 6 concludes the paper. 2 Model The traditional translation models are directional models that are based on conditional probabilities. As suggested by the noisy-channel model for SMT (Brown et al. 1993):  ∗ = argmax | = argmax ()|  (1)  The Bayes rule leads us to invert the conditioning of translation probability from a foreign (source) sentence  to an English (target) translation . However, in practice, the implementation of state-of-the-art phrase-based SMT systems uses a weighted log-linear combination of several models ℎ(, , ) including the logarithm of the phrase probability (and the lexical weight) in source-totarget and target-to-source directions (Och and Ney 2004)  ∗ = argmax ∑   ℎ (, , ) (2) = argmax  (, )  where  in ℎ(, , ) is a hidden struct"
N13-1048,P05-1033,0,0.244434,"Missing"
N13-1048,N09-1025,0,0.0657147,"riminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoret"
N13-1048,D08-1024,0,0.0811986,"n probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, bu"
N13-1048,W06-3105,0,0.129937,"Missing"
N13-1048,P08-2010,0,0.0584937,"learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze obj"
N13-1048,N04-1035,0,0.20719,"Missing"
N13-1048,D12-1061,1,0.847411,"y recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2007; Gao et al. 2012) and POS tagging (Haghighi and Klein 2006). 3 For comparison, the method of He and Deng (2012) also achieved very similar results to ours using the same experimental setting, as described in Section 4. Another undirected graphical model that has been more widely used for NLP is a CRF (Lafferty et al. 2001). An MRF differs from a CRF in that its partition function is no longer observation dependent. As a result, learning an MRF is harder than learning a CRF using maximum likelihood estimation (Haghighi and Klein 2006). Our work provides an alternative learning method that is based on discrimina"
N13-1048,N12-1023,0,0.175958,"Missing"
N13-1048,N06-1041,0,0.0235448,"tart to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansion (Metzler et al. 2007; Gao et al. 2012) and POS tagging (Haghighi and Klein 2006). 3 For comparison, the method of He and Deng (2012) also achieved very similar results to ours using the same experimental setting, as described in Section 4. Another undirected graphical model that has been more widely used for NLP is a CRF (Lafferty et al. 2001). An MRF differs from a CRF in that its partition function is no longer observation dependent. As a result, learning an MRF is harder than learning a CRF using maximum likelihood estimation (Haghighi and Klein 2006). Our work provides an alternative learning method that is based on discriminative training. 6 Conclusions The contribut"
N13-1048,D08-1039,0,0.0402308,"g in phrase-based SMT systems (Koehn et al. 2003). The potential function is defined as  ,  =   (, ), where the feature  (, ), called the word-pair feature, is an indicator function whose value is 1 if  is a word in target phrase  and f is a word in source phrase , and 0 otherwise. The third type of cliques contains three word nodes. Two of them are in one language and the third in the other language. A potential over such a clique is intended to capture inter-word dependencies for selecting word translations. The potential function is inspired by the triplet lexicon model (Hasan et al. 2008) which is based on lexicalized triplets (, , ’) . It can be understood as two source (or target) words triggering one target (or source) word. The potential function is defined as  ,   ,  =   (,   , ), where the feature  (,   , ), called the triplet feature, is an indicator function whose value is 1 if  is a word in target phrase  and  and ’ are two different words in source phrase , and 0 otherwise. For any clique that contains nodes in only one language we assume that   = 1 for all setting of the clique, which has no impact on scoring a phrase pair. One m"
N13-1048,W07-0711,1,0.812388,"EN, the training set contains 688K sentence pairs, with 21 words per sentence on average. The development set contains 2000 sentences. We used 2000 sentences from the WMT05 shared task as TEST1, and the 2000 sentences from the WMT06 shared task as TEST2. Two baseline phrase-based SMT systems, each for one language pair, are developed as follows. These baseline systems are used in our experiments both for comparison purpose and for generating N-best lists for discriminative training. First, we performed word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extracted the phrase table from the word aligned bilingual texts (Koehn et al. 2003). The maximum phrase length is set to four. Other models used in a baseline system include a lexicalized reordering model, word count and phrase count, and a trigram language model trained on the English training data provided by the WMT06 shared task. A fast beam-search phrasebased decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. The decoder is modified so as to output the Viterbi derivation for each translation hypothesis. The metric used for evaluation is case insensitiv"
N13-1048,P12-1031,1,0.925646,". The most common method of constructing the phrase table takes a two-phase approach. First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase is parameter estimation, where each phrase pair is assigned with some scores that are estimated based on counting of words or phrases on the same word-aligned training data. There has been a lot of research on improving the quality of the phrase table using more principled methods for phrase extraction (e.g., Lamber and Banchs 2005), parameter estimation (e.g., Wuebker et al. 2010; He and Deng 2012), or both (e.g., Marcu and Wong 2002; Denero et al. 2006). The focus of this paper is on the parameter estimation phase. We revisit the problem of scoring a phrase translation pair by developing a new phrase translation model based on Markov random fields (MRFs) and large-scale discriminative training. We strive to address the following three primary concerns. First of all, instead of parameterizing a phrase translation pair using a set of scoring functions that are learned independently (e.g., phrase translation probabilities and lexical weights) we use a general, statistical framework in whi"
N13-1048,D11-1125,0,0.211763,"evious work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrieval (Metzler and Croft 2005), query expansi"
N13-1048,J10-4005,0,0.0159991,"jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper. To this end, we resort to a large-scale discriminative training approach, following the pioneering work of Liang et al. (2006). Although there are established methods of tuning a handful of features on small training sets, such as the MERT method (Och 2003), the development of discriminative training methods for millions of features on millions of sentence pairs is still an ongoing area of research. A recent survey is due to Koehn (2010). In this paper we show that by using stochastic gradient ascent and an N-best list based 450 Proceedings of NAACL-HLT 2013, pages 450–459, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics expected BLEU as the objective function, largescale discriminative training can lead to significant improvements. The third primary concern is the ease of adoption of the proposed method. To this end, we use a simple and well-established learning method, ensuring that the results can be easily reproduced. We also develop the features for the MRF model in such a way that the"
N13-1048,W06-3114,0,0.0158381,"es on the training set are of artificially high quality with the derivations containing artificially long phrase pairs. The discrepancy between the translations on training and test sets could hurt the training performance. However, we found in our experiments that the impact of over-fitting on the quality of the trained MRF models is negligible1. 4 Experiments We conducted our experiments on two Europarl translation tasks, German-to-English (DE-EN) and French-to-English (FR-EN). The data sets are published for the shared task in NAACL 2006 Workshop on Statistical Machine Translation (WMT06) (Koehn and Monz 2006). For DE-EN, the training set contains 751K sentence pairs, with 21 words per sentence on average. The official development set used for the shared 1 As pointed out by one of the reviewers, the fact that our training works fine without leave-one-out is probably due to the small phrase length limit (i.e., 4) we used. If a longer phrase limit (e.g., 7) is used the result might be different. We leave it to future work. 455 DE-EN (TEST2) FR-EN (TEST2) 27.3 26.0 25.6 26.0 30.8 30.7 30.5 31.4 Table 1: Baseline results in BLEU. The results of top ranked systems are reported in Koehn and Monz (2006)2."
N13-1048,N03-1017,0,0.239642,"and  is source phrase, and 0 otherwise. While the conditional probabilities in a directional translation model are estimated using relative frequencies of phrase pairs extracted from word-aligned parallel sentences, the parameter of the phrase-pair function  is learned discriminatively, as we will describe in Section 3. Second, we consider cliques that contain two word nodes, one in source phrase and the other in target phrase. A potential over such a clique captures word-to-word translation dependencies similar to the use the IBM Model 1 for lexical weighting in phrase-based SMT systems (Koehn et al. 2003). The potential function is defined as  ,  =   (, ), where the feature  (, ), called the word-pair feature, is an indicator function whose value is 1 if  is a word in target phrase  and f is a word in source phrase , and 0 otherwise. The third type of cliques contains three word nodes. Two of them are in one language and the third in the other language. A potential over such a clique is intended to capture inter-word dependencies for selecting word translations. The potential function is inspired by the triplet lexicon model (Hasan et al. 2008) which is based on lexicalized"
N13-1048,N04-1023,0,0.113554,"ties based on this alignment. The latter learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective funct"
N13-1048,2005.mtsummit-posters.11,0,0.0840419,"Missing"
N13-1048,P06-1096,0,0.533593,"lation in a unified way. To this end, we propose the use of a MRF model. Second, because the phrase model has to work with other component models in an SMT system in order to produce good translations and the quality of translation is measured via BLEU score, it is desirable to optimize the parameters of the phrase model jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper. To this end, we resort to a large-scale discriminative training approach, following the pioneering work of Liang et al. (2006). Although there are established methods of tuning a handful of features on small training sets, such as the MERT method (Och 2003), the development of discriminative training methods for millions of features on millions of sentence pairs is still an ongoing area of research. A recent survey is due to Koehn (2010). In this paper we show that by using stochastic gradient ascent and an N-best list based 450 Proceedings of NAACL-HLT 2013, pages 450–459, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics expected BLEU as the objective function, largescale discrimina"
N13-1048,W02-1018,0,0.0604325,"ucting the phrase table takes a two-phase approach. First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase is parameter estimation, where each phrase pair is assigned with some scores that are estimated based on counting of words or phrases on the same word-aligned training data. There has been a lot of research on improving the quality of the phrase table using more principled methods for phrase extraction (e.g., Lamber and Banchs 2005), parameter estimation (e.g., Wuebker et al. 2010; He and Deng 2012), or both (e.g., Marcu and Wong 2002; Denero et al. 2006). The focus of this paper is on the parameter estimation phase. We revisit the problem of scoring a phrase translation pair by developing a new phrase translation model based on Markov random fields (MRFs) and large-scale discriminative training. We strive to address the following three primary concerns. First of all, instead of parameterizing a phrase translation pair using a set of scoring functions that are learned independently (e.g., phrase translation probabilities and lexical weights) we use a general, statistical framework in which arbitrary features extracted from"
N13-1048,N04-4024,0,0.0291741,"among different language pairs and depend to a large degree upon the amount and quality of training data. We leave a comprehensive study of features to future work. 3 Training This section describes the way the parameters of the MRF model are estimated. Although MRFs are by nature generative models, it is not always appropriate to train the parameters using conventional likelihood based approaches mainly for two reasons. The first is due to the difficulty in computing the partition function in Equation (4), especially in a task of our scale. The second is due to the metric divergence problem (Morgan et al. 2004). That is, the maximum likelihood estimation is unlikely to be optimal for the evaluation metric under consideration, as demonstrated on a variety of tasks including machine translation (Och 2003) and information retrieval (Metzler and Croft 2005; Gao et al. 2005). Therefore, we propose a large-scale discriminative training approach that uses stochastic gradient ascent and an N-best list based expected BLEU as the objective function. We cast machine translation as a structured classification task (Liang et al. 2006). It maps an input source sentence  to an output pair (, ) where  is the ou"
N13-1048,2007.mtsummit-papers.43,0,0.0181846,"iments both for comparison purpose and for generating N-best lists for discriminative training. First, we performed word alignment on the training set using a hidden Markov model with lexicalized distortion (He 2007), then extracted the phrase table from the word aligned bilingual texts (Koehn et al. 2003). The maximum phrase length is set to four. Other models used in a baseline system include a lexicalized reordering model, word count and phrase count, and a trigram language model trained on the English training data provided by the WMT06 shared task. A fast beam-search phrasebased decoder (Moore and Quirk 2007) is used and the distortion limit is set to four. The decoder is modified so as to output the Viterbi derivation for each translation hypothesis. The metric used for evaluation is case insensitive BLEU score (Papineni et al. 2002). We also performed a significance test using the paired ttest. Differences are considered statistically significant when the p-value is less than 0.05. Table 1 2 The official results are accessible http://www.statmt.org/wmt06/shared-task/results.html at # Systems 1 2 3 4 5 DE-EN 27.0 FR-EN TEST1 TEST2 TEST1 TEST2 26.8 26.0 27.3 α 27.2 α 26.8 αβ 26.8 αβ 26.0 27.1 α 26"
N13-1048,J04-4002,0,0.595513,"Missing"
N13-1048,P03-1021,0,0.807988,"dels in an SMT system in order to produce good translations and the quality of translation is measured via BLEU score, it is desirable to optimize the parameters of the phrase model jointly with other component models with respect to an objective function that is closely related to the evaluation metric under consideration, i.e., BLEU in this paper. To this end, we resort to a large-scale discriminative training approach, following the pioneering work of Liang et al. (2006). Although there are established methods of tuning a handful of features on small training sets, such as the MERT method (Och 2003), the development of discriminative training methods for millions of features on millions of sentence pairs is still an ongoing area of research. A recent survey is due to Koehn (2010). In this paper we show that by using stochastic gradient ascent and an N-best list based 450 Proceedings of NAACL-HLT 2013, pages 450–459, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics expected BLEU as the objective function, largescale discriminative training can lead to significant improvements. The third primary concern is the ease of adoption of the proposed method. To th"
N13-1048,P02-1040,0,0.0895129,"Missing"
N13-1048,W11-2119,0,0.20822,"the 1-best translation hypotheses In our experiments we find that using sBLEU defined above leads to a small but consistent improvement over other variations of sentence-level BLEU proposed previously (e.g., Liang et al. 2006). In particular, the use of the scaling factor ) in computing  makes  of the baseline’s 1best output close to perfect on training data, and has an effect of forcing the discriminative training to improve BLEU by improving n-gram precisions rather than by improving brevity penalty. 3.2 Parameter Estimation We use an N-best list based expected BLEU, a variant of that in Rosti et al. (2011), as the objective function for parameter optimization. Given the current model  , the expected BLEU, denoted by xBLEU(), over one training sample i.e., a labeled N-best list GEN() generated from a pair of source and target sentences (,   ), is defined as 454 1 Initialize , assuming  is fixed during training 2 For t = 1…T (T = the total number of iterations) 3 For each training sample (labeled 100-best list) 4 Compute ી | for each translation hypothesis  based on the current model = (, ) 5 Update the model via  =  + ∙ (), where is the learning rate and the gradient computed"
N13-1048,P12-1002,0,0.698607,"defined in Equation (11). Let . = ,  ∗ , ∗  − ,   ,   . The hinge loss under the N-best re-ranking framework is defined as max (0,1 −  .) . It is easy to verify that to train a model using this version of hinge loss, the update rule of Equation (12) can be rewritten as  &*+ , if 0 =  ∗ 3 (14)  ') = / &*+  + ,., $ℎ 12 where 0 is the highest scored candidate in GEN. Following Shalev-Shwartz (2012), by setting = 1 , we reach the Perceptron-based training algorithm that has been widely used in previous studies of discriminative training for SMT (e.g., Liang et al. 2006; Simianer et al. 2012). The logistic loss log(1 + exp(− .)) leads to an update rule similar to that of hinge loss  ') = /  &*+ ,  &*+ + , (.)., if 0 =  ∗ 3 (15) $ℎ 12 where ી   = 1/(1 + exp(  )). The log loss is widely used when a probabilistic interpretation of the trained model is desired, as in conditional random fields (CRFs) (Lafferty et al. 2001). Given a training sample, log loss is defined as log   ∗ |, where  ∗ is the oracle translation hypothesis with respect to its reference translation.   ∗ | is computed as Equation (10). So, unlike hinge loss and logistic loss, log loss ta"
N13-1048,P06-1091,0,0.0442089,"ward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith (2012) also analyze objective functions, but more from a theoretical viewpoint. The proposed MRF-based translation model is inspired by previous work of applying MRFs for information retrie"
N13-1048,D07-1080,0,0.377455,"alignment. The latter learn phrase translation probabilities discriminatively, which is similar to our approach. But He and Deng’s method involves multiple stages, and is not straightforward to implement3. Our method differs from previous work in its use of a MRF model that is simple and easy to understand, and a stochastic gradient ascent based training method that is efficient and easy to implement. A large portion of previous studies on discriminative training for SMT either use a handful of features or use small training sets of a few thousand sentences (e.g., Och 2003; Shen et al. 2004; Watanabe et al. 2007; Duh and Kirchhoff 2008; Chiang et al. 2008; Chiang et al. 2009). Although there is growing interest in large-scale discriminative training (e.g., Liang et al. 2006; Tillmann and Zhang 2006; Blunsom et al. 2008; Hopkins and May 2011; Zhang et al. 2011), only recently does some improvement start to be observed (e.g., Simianer et al. 2012; He and Deng 2012). It still remains uncertain if the improvement is attributed to new features, new training algorithms, objective functions, or simply large amounts of training data. We show empirically the importance of objective functions. Gimple and Smith"
N13-1048,P10-1049,0,0.566799,"nslation (SMT) system. The most common method of constructing the phrase table takes a two-phase approach. First, the bilingual phrase pairs are extracted heuristically from an automatically word-aligned training data. The second phase is parameter estimation, where each phrase pair is assigned with some scores that are estimated based on counting of words or phrases on the same word-aligned training data. There has been a lot of research on improving the quality of the phrase table using more principled methods for phrase extraction (e.g., Lamber and Banchs 2005), parameter estimation (e.g., Wuebker et al. 2010; He and Deng 2012), or both (e.g., Marcu and Wong 2002; Denero et al. 2006). The focus of this paper is on the parameter estimation phase. We revisit the problem of scoring a phrase translation pair by developing a new phrase translation model based on Markov random fields (MRFs) and large-scale discriminative training. We strive to address the following three primary concerns. First of all, instead of parameterizing a phrase translation pair using a set of scoring functions that are learned independently (e.g., phrase translation probabilities and lexical weights) we use a general, statistic"
N13-1048,P08-1024,0,\N,Missing
N15-1020,D13-1106,1,0.323212,"bels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses c"
N15-1020,W05-0909,0,0.0736125,"uced a corpus of 29M Twitter triples. Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples. Judgments on a 5-point scale were obtained from 3 raters apiece. This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3 . The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation. 5.2 Automatic Evaluation We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3. A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse. The dataset construction method just described yields only a single reference for each status. Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness. As we see in Section 6.3, it turns out that by optimizing systems towards BLEU using m"
N15-1020,D14-1179,0,0.0151747,"Missing"
N15-1020,P14-1129,0,0.0340037,"when generating long responses. 4.3 Dynamic-Context Generative Model II Because DCGM-I does not distinguish between c and m, that model has the propensity to underestimate the strong dependency that holds between m and r. Our third model (DCGM-II) addresses this issue by concatenating the two linear mappings of the bag-ofwords representations bc and bm in the input layer of the feed-forward network representing c and m (see Figure 3 right). Concatenating continuous representations prior to deep architectures is a common strategy to obtain order-sensitive representations (Bengio et al., 2003; Devlin et al., 2014). The forward equations for the context encoder are: 1 > 1 k1 = [b> c Wf , bm Wf ], > ` k` = σ(k`−1 Wf ) for ` = 2, · · · , L (8) where [x, y] denotes the concatenation of x and y vectors. In DCGM-II, the bias on the recurrent hidden state and the probability distribution over the next token are computed as described in Eq. 7. 5 Experimental Setting 5.1 Dataset Construction For computational efficiency and to alleviate the burden of human evaluators, we restrict the context sequence c to a single sentence. Hence, our dataset is composed of “triples” τ ≡ (cτ , mτ , rτ ) consisting of three sent"
N15-1020,P14-1066,1,0.560619,"stems remain hand-coded: in particular, the labels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Mode"
N15-1020,D14-1002,1,0.788594,"stems remain hand-coded: in particular, the labels and attributes defining dialog states. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Mode"
N15-1020,D13-1176,0,0.046151,"tates. In contrast, the dialog state in our neural network model is completely latent and directly optimized towards end-to-end performance. In this sense, 197 we believe the framework of this paper is a significant milestone towards more data-driven and less hand-coded dialog processing. Continuous representations of words and phrases estimated by neural network models have been applied on a variety of tasks ranging from Information Retrieval (IR) (Huang et al., 2013; Shen et al., 2014), Online Recommendation (Gao et al., 2014b), Machine Translation (MT) (Auli et al., 2013; Cho et al., 2014; Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014), and Language Modeling (LM) (Bengio et al., 2003; Collobert and Weston, 2008). Gao et al. (2014a) successfully use an embedding model to refine the estimation of rare phrase-translation probabilities, which is traditionally affected by sparsity problems. Robustness to sparsity is a crucial property of our method, as it allows us to capture context information while avoiding unmanageable growth of model parameters. Our work extends the Recurrent Neural Network Language Model (RLM) of (Mikolov et al., 2010), which uses continuous representations to estimate a probabilit"
N15-1020,P07-2045,0,0.0499714,"3.58 references per example on average (Table 1). The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively. 5.3 Feature Sets The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004). These log-linear models comprise the following feature sets: MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007). Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses. For the translation probabilities, we built a very large phrase table of 160.7 million entries by first filtering out Twitterisms (e.g., long sequences of vowels, hashtags), and then selecting candidate phrase pairs using Fisher’s exact test (Ritter et al., 2011). We also included MT decoder features specifical"
N15-1020,J04-4002,0,0.0179799,"a set of candidate triples {˜ τ }, human evaluators are asked to rate the quality of the response within the new triples {(cτ , mτ , rτ˜ )}. After human evaluation, we retain the references for which the score is 4 or better on a 5 point scale, resulting in 3.58 references per example on average (Table 1). The average lengths for the responses in the multi-reference tuning and test sets are 8.75 and 8.13 tokens respectively. 5.3 Feature Sets The response generation systems evaluated in this paper are parameterized as log-linear models in a framework typical of statistical machine translation (Och and Ney, 2004). These log-linear models comprise the following feature sets: MT MT features are derived from a large response generation system built along the lines of Ritter et al. (2011), which is based on a phrase-based MT decoder similar to Moses (Koehn et al., 2007). Our MT feature set includes the following features that are common in Moses: forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, and a modified Kneser-Ney language model (Kneser and Ney, 1995) trained on Twitter responses. For the translation probabilities, we built a very lar"
N15-1020,P03-1021,0,0.041731,"r generalization. The last layer embeds the context vector into the hidden space of the decoder RLM. 5.5 Rescoring Setup We evaluate the proposed models by rescoring the n-best candidate responses obtained using the MT phrase-based decoder and the IR system. In contrast to MT, the candidate responses provided by IR have been created by humans and are less affected by fluency issues. The different n-best lists will provide a comprehensive testbed for our experiments. First, we augment the n-best list of the tuning set with the scores of the model of interest. Then, we run an iteration of MERT (Och, 2003) to estimate the log-linear weights of the new features. At test time, we rescore the test n-best list with the new weights. 6 Results 6.1 Lower and Upper Bounds Table 2 shows the expected upper and lower bounds for this task as suggested by BLEU scores for human responses and a random response baseline. The RAN DOM system comprises responses randomly extracted from the triples corpus. HUMAN is computed by choosing one reference amongst the multi-reference set for each context-status pair.4 Although the scores are lower than those usually reported in SMT tasks, the ranking of the three systems"
N15-1020,P02-1040,0,0.121444,"an 3 times in the corpus. This produced a corpus of 29M Twitter triples. Additionally, we hired crowdsourced raters to evaluate approximately 33K candidate triples. Judgments on a 5-point scale were obtained from 3 raters apiece. This yielded a set of 4232 triples with a mean score of 4 or better that was then randomly binned into a tuning set of 2118 triples and a test set of 2114 triples3 . The mean length of responses in these sets was approximately 11.5 tokens, after cleanup (e.g., stripping of emoticons), including punctuation. 5.2 Automatic Evaluation We evaluate all systems using BLEU (Papineni et al., 2002) and METEOR (Banerjee and Lavie, 2005), and supplement these results with more targeted human pairwise comparisons in Section 6.3. A major challenge in using these automated metrics for response generation is that the set of reasonable responses in our task is potentially vast and extremely diverse. The dataset construction method just described yields only a single reference for each status. Accordingly, we extend the set of references using an IR approach to mine potential responses, after which we have human judges rate their appropriateness. As we see in Section 6.3, it turns out that by o"
N15-1020,D11-1054,0,0.803346,"on-context-sensitive Machine Translation and Information Retrieval baselines. 1 message response yeah i’m on my way now ok good luck ! Figure 1: Example of three consecutive utterances occurring between two Twitter users A and B. Introduction Until recently, the goal of training open-domain conversational systems that emulate human conversation has seemed elusive. However, the vast quantities of conversational exchanges now available on social media websites such as Twitter and Reddit raise the prospect of building data-driven models that can begin to communicate conversationally. The work of Ritter et al. (2011), for example, demonstrates that a response generation system can be constructed from Twitter conversations using statistical machine translation techniques, where a status post by a Twitter user is “translated” into a plausible looking response. However, an approach such as that presented in Ritter et al. (2011) does not address the challenge of *The entirety of this work was conducted while at Microsoft Research. † Corresponding authors: Alessandro Sordoni (sordonia@iro.umontreal.ca) and Michel Galley (mgalley@microsoft.com). generating responses that are sensitive to the context of the conv"
N15-1092,P14-1023,0,0.0130009,"milar concepts; e.g., feature generation, dimensionality reduction, and vector space models. The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics. Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Sch¨olkopf et al., 1998). Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014). Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised o"
N15-1092,D14-1082,0,0.226541,"odong Liu†∗, Jianfeng Gao‡ , Xiaodong He‡ , Li Deng‡ , Kevin Duh† and Ye-yi Wang‡ Nara Institute of Science and Technology, 8916-5 Takayama, Ikoma, Nara 630-0192, Japan ‡ Microsoft Research, One Microsoft Way, Redmond, WA 98052, USA xiaodong-l@is.naist.jp, {jfgao,xiaohe,deng}@microsoft.com kevinduh@is.naist.jp, yeyiwang@microsoft.com † Abstract representations learned from large corpora. Further, since these representations are usually in a lowdimensional vector space, they result in more compact models than those built from surface-form features. A recent successful example is the parser by (Chen and Manning, 2014), which is not only accurate but also fast. Methods of deep neural networks (DNNs) have recently demonstrated superior performance on a number of natural language processing tasks. However, in most previous work, the models are learned based on either unsupervised objectives, which does not directly optimize the desired task, or singletask supervised objectives, which often suffer from insufficient training data. We develop a multi-task DNN for learning representations across multiple tasks, not only leveraging large amounts of cross-task data, but also benefiting from a regularization effect"
N15-1092,P14-1066,1,0.29944,"n. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 201"
N15-1092,D14-1002,1,0.736534,"n. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 201"
N15-1092,D14-1070,0,0.0132056,"Missing"
N15-1092,P14-1062,0,0.00783704,"yyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other methods for sentence/document representations, such as those based on convolutional networks (Kalchbrenner et al., 2014; Shen et al., 2014; Gao et al., 2014b), parse tree structure (Irsoy and Cardie, 2014), and run-time inference (Le and Mikolov, 2014). The synergy between multi-task learning and neural nets is quite natural; the general idea dates back to (Caruana, 1997). The main challenge is in designing the tasks and the network structure. For example, (Collobert et al., 2011) defined part-of-speech tagging, chunking, and named entity recognition as multiple tasks in a single sequence labeler; (Bordes et al., 2012) defined multiple data sources as tasks in their relation extraction system. While conceptual"
N15-1092,N13-1090,0,0.587375,"g large amounts of cross-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations uni"
N15-1092,D14-1079,0,0.0296496,"feature generation, dimensionality reduction, and vector space models. The main motivation is similar: to abstract away from surface forms in words, sentences, or documents, in order to alleviate sparsity and approximate semantics. Traditional techniques include LSA (Deerwester et al., 1990), ESA (Gabrilovich and Markovitch, 2007), PCA (Karhunen, 1998), and non-linear kernel variants (Sch¨olkopf et al., 1998). Recently, learningbased approaches inspired by neural networks, especially DNNs, have gained in prominence, due to their favorable performance (Huang et al., 2013; Baroni et al., 2014; Milajevs et al., 2014). Popular methods for learning word representations include (Collobert et al., 2011; Mikolov et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing a"
N15-1092,D14-1162,0,0.118051,"s-task data, but also benefiting from a regularization effect that leads to more general representations to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks. Introd"
N15-1092,D13-1170,0,0.00800131,"tions to help tasks in new domains. Our multi-task DNN approach combines tasks of multiple-domain classification (for query classification) and information retrieval (ranking for web search), and demonstrates significant gains over strong baselines in a comprehensive set of domain adaptation. 1 However, existing vector-space representation learning methods are far from optimal. Most previous methods are based on unsupervised objectives such as word prediction for training (Mikolov et al., 2013c; Pennington et al., 2014). Other methods use supervised training objectives on a single task, e.g. (Socher et al., 2013), and thus are often constrained by limited amounts of training data. Motivated by the success of multi-task learning (Caruana, 1997), we propose in this paper a multi-task DNN approach for representation learning that leverages supervised data from many tasks. In addition to the benefit of having more data for training, the use of multi-task also profits from a regularization effect, i.e., reducing overfitting to a specific task, thus making the learned representations universal across tasks. Introduction Recent advances in deep neural networks (DNNs) have demonstrated the importance of learn"
N15-1092,P10-1040,0,0.0115602,"et al., 2013c; Mnih and Kavukcuoglu, 2013; Pennington et al., 2014): all are based on unsupervised objec4 The trends differ slightly in the Nightlife domain. We believe this may be due to data bias on test data (only 298 samples). 919 tives of predicting words or word frequencies from raw text. End-to-end neural network models for specific tasks (e.g. parsing) often use these word representations as initialization, which are then iteratively improved by optimizing a supervised objective (e.g. parsing accuracy). A selection of successful applications of this approach include sequence labeling (Turian et al., 2010), parsing (Chen and Manning, 2014), sentiment (Socher et al., 2013), question answering (Iyyer et al., 2014) and translation modeling (Gao et al., 2014a). Our model takes queries and documents as input, so it learns sentence/document representations. This is currently an open research question, the challenge being how to properly model semantic compositionality of words in vector space (Huang et al., 2013; M. Baroni and Zamparelli, 2013; Socher et al., 2013). While we adopt a bag-of-words approach for practical reasons (memory and run-time), our multi-task framework is extensible to other meth"
N15-1092,2014.lilt-9.5,0,\N,Missing
N16-1014,P12-3007,0,0.0207071,"ilize a mutual information objective in the retrieval component of image caption retrieval. Below, we focus on the challenge of using MMI in response generation, comparing the performance of MMI models against maximum likelihood. 2 3 Related work The approach we take here is data-driven and end-toend. This stands in contrast to conventional dialog systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as ik , fk and ok . We distinguish e and h wh"
N16-1014,P15-2073,1,0.256939,"ecutive message-response pairs spoken by different characters. We randomly selected two subsets as development and test datasets, each containing 2k pairs, with source and target length restricted to the range of [6,18]. 5.2 7 IMSDB (http://www.imsdb.com/) is a relatively small database of around 0.4 million sentences and thus not suitable for open domain dialogue training. 115 Model S EQ 2S EQ MMI-antiLM Evaluation For parameter tuning and final evaluation, we used B LEU (Papineni et al., 2002), which was shown to correlate reasonably well with human judgment on the response generation task (Galley et al., 2015). In the case of the Twitter models, we used multireference B LEU. As the IMSDB data is too limited to support extraction of multiple references, only single reference B LEU was used in training and evaluating the OSDb models. We did not follow Vinyals et al. (2015) in using perplexity as evaluation metric. Perplexity is unlikely to be a useful metric in our scenario, since our proposed model is designed to steer away from the standard S EQ 2S EQ model in order to diversify the outputs. We report degree of diversity by calculating the number of distinct unigrams and bigrams in generated respon"
N16-1014,P14-1066,1,0.170591,"systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as ik , fk and ok . We distinguish e and h where ek denotes the vector for an individual text unit (for example, a word or sentence) at time step k while hk denotes the vector computed by LSTM model at time k by combining ek and hk−1 . ck is the cell state vector at time k, and σ denotes the sigmoid function. Then, the vector representation hk for each time step 2 Augmenting our technique"
N16-1014,D13-1111,0,0.0100372,"Missing"
N16-1014,P07-2045,0,0.0627244,"et We first report performance on Twitter datasets in Table 2, along with results for different models (i.e., Machine Translation and MT+neural reranking) reprinted from Sordoni et al. (2015) on the same dataset. The baseline is the S EQ 2S EQ model with its standard likelihood objective and a beam size of 200. We compare this baseline against greedy-search S EQ 2S EQ (Vinyals and Le, 2015), which can help achieve higher diversity by increasing search errors.8 Machine Translation is the phrase-based MT system described in (Ritter et al., 2011). MT features include commonly used ones in Moses (Koehn et al., 2007), e.g., forward and backward maximum likelihood “translation” probabilities, word and phrase penalties, linear distortion, etc. For more details, refer to Sordoni et al. (2015). MT+neural reranking is the phrase-based MT system, reranked using neural models. N-best lists are first generated from the MT system. Recurrent neural models generate scores for N-best list candidates given the input messages. These generated scores are re-incorporated to rerank all the candidates. Additional features to score [1, 2, 3, 4]-gram matches between context and response and between message and context (conte"
N16-1014,P15-1002,0,0.00622466,"200. The top examples are the responses with the highest average probability loglikelihoods in the N-best list. Lower-ranked, less-generic responses were manually chosen. speech recognition (Bahl et al., 1986; Brown, 1987), as an optimization objective that measures the mutual dependence between inputs and outputs. Below, we present practical strategies for neural generation models that use MMI as an objective function. We show that use of MMI results in a clear decrease in the proportion of generic response sequences, generating correspondingly more varied and interesting outputs. al., 2015; Luong et al., 2015) has inspired attempts to extend these neural techniques to response generation. Sordoni et al. (2015) improved upon Ritter et al. (2011) by rescoring the output of a phrasal SMT-based conversation system with a S EQ 2S EQ model that incorporates prior context. (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015; Wen et al., 2015) apply direct end-to-end S EQ 2S EQ models These S EQ 2S EQ models are Long Short-Term Memory (LSTM) neural networks (Hochreiter and Schmidhuber, 1997) that can implicitly capture compositionality and long-span dependencies. (Wen et al., 2015) attempt to le"
N16-1014,P03-1021,0,0.0536546,"sponses (T ) interchanged. 4.5 4.5.1 Decoding MMI-antiLM As described in Section 4.3.1, decoding using log p(T |S) − λU (T ) can be readily implemented by predicting tokens at each time-step. In addition, we found in our experiments that it is also important to take into account the length of responses in decod114 ing. We thus linearly combine the loss function with length penalization, leading to an ultimate score for a given target T as follows: Score(T ) = p(T |S) − λU (T ) + γNt (15) where Nt denotes the length of the target and γ denotes associated weight. We optimize γ and λ using MERT (Och, 2003) on N-best lists of response candidates. The N-best lists are generated using the decoder with beam size B = 200. We set a maximum length of 20 for generated candidates. At each time step of decoding, we are presented with B × B candidates. We first add all hypotheses with an EOS token being generated at current time step to the N-best list. Next we preserve the top B unfinished hypotheses and move to next time step. We therefore maintain beam size of 200 constant when some hypotheses are completed and taken down by adding in more unfinished hypotheses. This will lead the size of final N-best"
N16-1014,W00-0306,0,0.0299638,"oes not require identifying lexical overlap to foster diversity.2 On a somewhat different task, Mao et al. (2015, Section 6) utilize a mutual information objective in the retrieval component of image caption retrieval. Below, we focus on the challenge of using MMI in response generation, comparing the performance of MMI models against maximum likelihood. 2 3 Related work The approach we take here is data-driven and end-toend. This stands in contrast to conventional dialog systems, which typically are template- or heuristicdriven even where there is a statistical component (Levin et al., 2000; Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Walker et al., 2003; Pieraccini et al., 2009; Young et al., 2010; Wang et al., 2011; Banchs and Li, 2012; Chen et al., 2013; Ameixa et al., 2014; Nio et al., 2014). We follow a newer line of investigation, originally introduced by Ritter et al. (2011), which frames response generation as a statistical machine translation (SMT) problem. Recent progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et 111 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xNx }, an LSTM associates each time"
N16-1014,P02-1040,0,0.120743,"Missing"
N16-1014,D11-1054,0,0.770418,"tantive gains in B LEU scores on two conversational datasets and in human evaluations. 1 Introduction Conversational agents are of growing importance in facilitating smooth interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response"
N16-1014,P15-1152,0,0.523144,"h interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting. In practice, however, neural conversation models tend to ge"
N16-1014,N15-1020,1,0.849031,"of growing importance in facilitating smooth interaction between humans and their electronic devices, yet conventional dialog systems continue to face major challenges in the form of robustness, scalability and domain adaptation. Attention has thus turned to learning conversational patterns from data: researchers have begun to explore data-driven generation of conversational responses within the framework of statistical machine translation (SMT), either phrase-based (Ritter et al., 2011), or using neural networks to rerank, or directly in the form of sequence-to-sequence (S EQ 2S EQ) models (Sordoni et al., 2015; Vinyals and Le, 2015; Shang et al., 2015; Serban et al., 2015; Wen et al., 2015). S EQ 2S EQ models offer the promise of scalability and language-independence, together with the capacity * The entirety of this work was conducted at Microsoft. to implicitly learn semantic and syntactic relations between pairs, and to capture contextual dependencies (Sordoni et al., 2015) in a way not possible with conventional SMT approaches (Ritter et al., 2011). An engaging response generation system should be able to output grammatical, coherent responses that are diverse and interesting. In practice, howe"
N16-1014,D15-1199,0,0.506634,"Missing"
N18-1016,P05-1018,0,0.054586,"ds to fine-tune neural generation models using automatic measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a wo"
N18-1016,D16-1127,1,0.931436,"gure 1: The generator is rewarded for imitating the discourse structure of the gold sequence. Importantly, most automatic measures are based on local n-gram patterns, providing only a limited and myopic perspective of overall text quality. As a result, while models trained to directly optimize these measures can yield improvements on the same measures, they may not lead to better quality in terms of overall coherence or discourse structure. Indeed, recent studies have reported cases where commonly used measures do not align well with desired aspects of generation quality (Rennie et al., 2017; Li et al., 2016). The challenge, however, is to define a global score that can measure the complex aspects of text quality beyond local n-gram patterns. In this paper, we investigate learning neural rewards and their use in a reinforcement learning regime with a specific focus on learning more discourse-aware and coherent text generation. Our approach shares the spirit of the work of Lowe et al. (2017), where neural scores were learned to approximate human judgments of dialogue quality. The key difference is that our rewards can be fully automatically constructed without requiring human judgments and can be t"
N18-1016,J08-1001,0,0.328444,"Missing"
N18-1016,N04-1015,0,0.102263,"models using automatic measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a word embedding and sj is a sente"
N18-1016,W04-1013,0,0.0121079,"al loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Proceedings of NAACL-HLT 2018, pages 173–184 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics underlying text generator (see"
N18-1016,P09-1068,0,0.0299016,"ilar to our work is work on using neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical resu"
N18-1016,P17-1103,0,0.0265649,"ity in terms of overall coherence or discourse structure. Indeed, recent studies have reported cases where commonly used measures do not align well with desired aspects of generation quality (Rennie et al., 2017; Li et al., 2016). The challenge, however, is to define a global score that can measure the complex aspects of text quality beyond local n-gram patterns. In this paper, we investigate learning neural rewards and their use in a reinforcement learning regime with a specific focus on learning more discourse-aware and coherent text generation. Our approach shares the spirit of the work of Lowe et al. (2017), where neural scores were learned to approximate human judgments of dialogue quality. The key difference is that our rewards can be fully automatically constructed without requiring human judgments and can be trained in an unsupervised manner. More specifically, we propose a neural reward learning scheme that is trained to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many exis"
N18-1016,mori-etal-2014-flow,0,0.0430861,"n et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our teacher-trained generator better models the latent event sequences of cooking recipes,"
N18-1016,D14-1179,0,0.018117,"Missing"
N18-1016,P02-1040,0,0.101245,"ng text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Proceedings of NAACL-HLT 2018, pages 173–184 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics underlying text generator (see Figure 1), which is trained u"
N18-1016,D17-1103,0,0.0163234,"del for the mixed reward was chosen as the one that achieved the highest average geometric mean of BLEU-4 reward and average relative ordering reward for each generated sequence y in the development set: Mixed Training As the model learns parameters to optimize the amount of reward it receives from the teacher, it is not explicity encouraged to produce fluent generations. The model quickly learns to generate simple sequences that exploit the teacher for high rewards despite being incoherent recipes (e.g., Figure 4). Consequently, it is possible that generated sequences are no longer readable (Pasunuru and Bansal, 2017; Paulus et al., 2018). Title: Chili Grits Ingredients: boiling water, butter, shredded cheddar cheese, jalapenos, eggs, chicken cream of soup, salt Generated Recipe: Here . T rb4 (y) X r¯ = rRO (yt ) T (17) t=1 Figure 4: Recipe generated from a self-critical model with no mixed training where rb4 is the BLEU-4 score of the whole generated sequence, and rRO is computed using Equa177 Model Cross-entropy (MLE) BLEU-4 (Rennie et al., 2017) CIDEr (Rennie et al., 2017) ROUGE-L (Paulus et al., 2018) BLEU-1 (γ = 0.97) BLEU-4 (γ = 0.99) CIDEr (γ = 0.97) ROUGE-L (γ = 0.97) Absolute Ordering (AO) Relati"
N18-1016,D15-1114,1,0.861518,"image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our teacher-trained generator better models the latent event sequences o"
N18-1016,D16-1032,1,0.893981,"to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft"
N18-1016,P16-1027,0,0.0227005,"sing neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn this script knowledge and reward recipe generators for exhibiting it. 8 Conclusion We introduce the absolute ordering and relative ordering teachers, two neural networks that score a sequence’s adherence to discourse structure in long text. The teachers are used to compute rewards for a self-critical reinforcement learning framework, allowing a recipe generator to be rewarded for capturing temporal semantics of the cooking domain. Empirical results demonstrate that our tea"
N18-1016,D14-1218,0,0.0229299,"measures such as CIDEr as the reward. However, because most existing automatic measures focus on local n-gram patterns, fine-tuning on those measures may yield deteriorated text despite increased automatic scores, especially for tasks that require long coherent generation (§6.1). Since writing out a scoring term that quantifies the quality of discourse coherence is an open research question, we take inspiration from previous research that learns the overall ordering structure of a document as an approximation of the discourse structure (Barzilay and Lapata, 2005, 2008; Barzilay and Lee, 2004; Li and Hovy, 2014), and propose two neural teachers that can learn to score an ordered sequence of sentences. The scores from these neural teachers are then used to formulate rewards (§4.2) that guide coherent long text generation systems in a policy gradient reinforcement learning setup. Notably, the neural teachers are trained offline on gold sequences in an unsupervised manner prior to training the generator. They are not trained jointly with the generator and their parameters are fixed during policy learning. 2.1 … GRU sj = Lj X xij (1) i=1 where xij is a word embedding and sj is a sentence embedding. Each"
N18-1016,D17-1197,0,0.0277131,"nce ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning methods such as the REINFORCE algorithm (Williams, 1992). ∗ Gold Recipe Work done while author was at Microsoft Research 173 Procee"
N18-1016,D17-1062,0,0.0267216,"0.056 0.054 0.052 0.050 0.048 State Change BLEU-4 0.95 0.97 0.98 0.300 0.295 0.290 0.285 0.280 0.275 Figure 5: Action and State Change BLEU Metrics for different initializations of `max and γ guage model to deteriorate. Interestingly, a higher `max leads to better performance on global coherence scores, implying that relative order rewards conditioned on more sentences allow the model to learn longer-range context co-occurrences. 7 Most similar to our work is work on using neural and embedding rewards to improve dialogue (Li et al., 2016), image captioning (Ren et al., 2017), simplification (Zhang and Lapata, 2017), and paraphrase generation (Li et al., 2017). While these works use single-sentence similarity rewards for short generation tasks, our work designs teachers to reward long-range ordering patterns. Finally, our teachers can be seen as rewarding generators that approximate script patterns in recipes. Previous work in learning script knowledge (Schank and Abelson, 1975) has focused on extracting scripts from long texts (Chambers and Jurafsky, 2009; Pichotta and Mooney, 2016), with some of that work focusing on recipes (Kiddon et al., 2015; Mori et al., 2014, 2012). Our teachers implicitly learn"
N18-1016,D15-1044,0,0.164506,"d in an unsupervised manner. More specifically, we propose a neural reward learning scheme that is trained to capture crosssentence ordering structure as a means to approximate the desired discourse structure in documents. The learned teacher computes rewards for the Introduction Defining an ideal loss for training text generation models remains an open research question. Many existing approaches based on variants of recurrent neural networks (Hochreiter and Schmidhuber, 1997; Cho et al., 2014) are trained using cross-entropy loss (Bahdanau et al., 2015; Vinyals et al., 2015; Xu et al., 2015; Rush et al., 2015), often augmented with additional terms for topic coverage or task-specific supervision (Kiddon et al., 2016; Yang et al., 2017). Training with cross-entropy, however, does not always correlate well with achieving high scores on commonly used evaluation measures such as ROUGE (Lin, 2004), BLEU (Papineni et al., 2002), or CIDEr (Vedantam et al., 2015). Another current line of research therefore explores training generation models that directly optimize the target evaluation measure (Wu et al., 2016; Ranzato et al., 2015; Paulus et al., 2018; Rennie et al., 2017) using reinforcement learning met"
N18-1016,D17-1239,0,0.0387937,"Missing"
N18-1016,1983.tc-1.13,0,0.67195,"Missing"
N19-1021,W14-3346,0,0.0226809,"choosing a college, and the model learns to generate responses from Caller Bob. The cyclical schedule generated highly diverse answers that cover multiConditional VAE for Dialog We use a cyclical schedule to improve the latent codes in (Zhao et al., 2017), which are key to diverse dialog-response generation. Follow247 ple plausible dialog acts. On the contrary, the responses from the monotonic schedule are limited to repeat plain responses, i.e., “i’m not sure”. Quantitative results are shown in Table 3, using the evaluation metrics from (Zhao et al., 2017). (i) Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified n-gram precision with a length penalty. We use BLEU-1 to 4 as our lexical similarity metric and normalize the score to 0 to 1 scale. (ii) Cosine Distance of Bag-of-word Embedding (Liu et al., 2016): a simple method to obtain sentence embeddings is to take the average or extreme of all the word embeddings in the sentences. We used Glove embedding and denote the average method as A bow and extreme method as E bow. The score is normalized to [0, 1]. Higher values indicate more plausible responses. The BoW indeed reduces the K"
N19-1021,D16-1230,0,0.0326472,"diverse dialog-response generation. Follow247 ple plausible dialog acts. On the contrary, the responses from the monotonic schedule are limited to repeat plain responses, i.e., “i’m not sure”. Quantitative results are shown in Table 3, using the evaluation metrics from (Zhao et al., 2017). (i) Smoothed Sentence-level BLEU (Chen and Cherry, 2014): BLEU is a popular metric that measures the geometric mean of modified n-gram precision with a length penalty. We use BLEU-1 to 4 as our lexical similarity metric and normalize the score to 0 to 1 scale. (ii) Cosine Distance of Bag-of-word Embedding (Liu et al., 2016): a simple method to obtain sentence embeddings is to take the average or extreme of all the word embeddings in the sentences. We used Glove embedding and denote the average method as A bow and extreme method as E bow. The score is normalized to [0, 1]. Higher values indicate more plausible responses. The BoW indeed reduces the KL vanishing issue, as indicated by the increased KL and decreased reconstruction perplexity. When applying the proposed cyclical schedule to CVAE, we also see a reduced KL vanishing issue. Interestingly, it also yields the highest BLEU scores. This suggests that the cy"
N19-1021,J93-2004,0,0.0651737,"rted. Since SA-VAE tends to overfit, we report its best results in row M⇤ . cal schedule (while keeping all other settings the same). The default hyper-parameters of the cyclical schedule are used in all cases unless stated otherwise. We study the impact of hyper-parameters in the SM, and show that larger M can provide higher performance for various R. We show the major results in this section, and put more details in the SM. The monotonic and cyclical schedules are denoted as M and C, respectively. 6.1 Language Modeling We first consider language modeling on the Penn Tree Bank (PTB) dataset (Marcus et al., 1993). Language modeling with VAEs has been a challenging problem, and few approaches have been shown to produce rich generative models that do not collapse to standard language models. Ideally a deep generative model trained with variational inference would pursue higher ELBO, making use of the latent space (i.e., maintain a nonzero KL term) while accurately modeling the underlying distribution (i.e., lower reconstruction errors). We implemented different schedules based on the code4 published by Kim et al. (2018). The latent variable is 32-dimensional, and 40 epochs are used. We compare the propo"
N19-1021,D16-1031,0,0.04067,"cycles as warm re-starts. The effectiveness of cyclical annealing is validated on a broad range of NLP tasks, including language modeling, dialog response generation and unsupervised language pre-training. 1 Introduction Variational autoencoders (VAEs) (Kingma and Welling, 2013; Rezende et al., 2014) have been applied in many NLP tasks, including language modeling (Bowman et al., 2015; Miao et al., 2016), dialog response generation (Zhao et al., 2017; Wen et al., 2017), semi-supervised text classification (Xu et al., 2017), controllable text generation (Hu et al., 2017), and text compression (Miao and Blunsom, 2016). A prominent component of a VAE is the distribution-based latent representation for text sequence observations. This flexible representation allows the VAE to explicitly model holistic properties of sentences, such as style, topic, and high-level linguistic and semantic features. Samples from the prior latent distribution can produce ⇤ Corresponding author † Equal Contribution 240 Proceedings of NAACL-HLT 2019, pages 240–250 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics vanishing issue, and develop an understanding of the strengths and weaknes"
N19-1021,P17-1061,0,0.139526,"med sentences through simple deterministic decoding (Bowman et al., 2015). Due to the sequential nature of text, an autoregressive decoder is typically employed in the VAE. This is often implemented with a recurrent neural network (RNN); the long short-term memory (LSTM) (Hochreiter and Schmidhuber, 1997) RNN is used widely. This introduces one notorious issue when a VAE is trained using traditional methods: the decoder ignores the latent variable, yielding what is termed the KL vanishing problem. Several attempts have been made to ameliorate this issue (Yang et al., 2017; Dieng et al., 2018; Zhao et al., 2017; Kim et al., 2018). Among them, perhaps the simplest solution is monotonic KL annealing, where the weight of the KL penalty term is scheduled to gradually increase during training (Bowman et al., 2015). While these techniques can effectively alleviate the KL-vanishing issue, a proper unified theoretical interpretation is still lacking, even for the simple annealing scheme. In this paper, we analyze the variable dependency in a VAE, and point out that the autoregressive decoder has two paths (formally defined in Section 3.1) that work together to generate text sequences. One path is conditione"
N19-1094,D15-1075,0,0.0312843,"onent of natural language understanding, commonsense reasoning has been included in an increasing number of tasks for evaluation: COPA (Roemmele et al., 2011) assesses commonsense causal reasoning by selecting an alternative, which has a more plausible causal relation with the given premise. Story Cloze Test (ROCStories, Mostafazadeh et al. 2016) evaluates story understanding, story generation, and script learning by choosing the most sensible ending to a short story. JOCI (Zhang et al., 2017) generalizes the natural language inference (NLI) framework (Cooper et al., 1996; Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) and evaluates commonsense inference by predicting the ordinal likelihood of a hypothesis given a context. Event2Mind (Rashkin et al., 2018b) models stereotypical intents and reactions of people, described in short free-form text. SWAG (Zellers et al., 2018) frames commonsense inference as multiple-choice questions for follow-up events given some context. ReCoRD (Zhang et al., 2018) 1 We don’t believe it is possible to construct such a knowledge base given that the world is changing constantly. 2 DSSMs can be applied to a wide range of tasks depending on the definition"
N19-1094,D16-1245,0,0.0172502,"ith candidates. To make a fair comparison to Trinh and Le (2018)’s work, we also train our single model on the corpus of Gutenberg only. We can see that both of our methods get significant improvement on the PDP dataset, and our UDSSM-II can achieve much better performance on the WSC dataset. We also report our ensemble model (nine models with different hyperparameters) trained with both corpus of Gutenberg and 1 Billion Word, and it also achieve better performance than Google Language Model trained with the same corpus. Finally, we also compare to the pre-trained Coreference Resolution Tool (Clark and Manning, 2016a,b)12 , and we can see that it doesn’t adapt to our commonsense reasoning tasks and can’t tell 4.4 Analysis WSC 1: Ours 1: WSC 2: Ours 2: Paul tried to call George on the phone, but he wasn’t successful. He tried to call 911 using her cell phone but that he could n’t get the phone to work. Paul tried to call George on the phone, but he was n’t available . He tried twice to call her but she did not answer the phone . Table 3: Comparison of the data from WSC and our training data. Our sentences are retrieved from the UDSSM-II training dataset based on the BM25 value for analysis. The pseudo lab"
N19-1094,P16-1061,0,0.0223517,"ith candidates. To make a fair comparison to Trinh and Le (2018)’s work, we also train our single model on the corpus of Gutenberg only. We can see that both of our methods get significant improvement on the PDP dataset, and our UDSSM-II can achieve much better performance on the WSC dataset. We also report our ensemble model (nine models with different hyperparameters) trained with both corpus of Gutenberg and 1 Billion Word, and it also achieve better performance than Google Language Model trained with the same corpus. Finally, we also compare to the pre-trained Coreference Resolution Tool (Clark and Manning, 2016a,b)12 , and we can see that it doesn’t adapt to our commonsense reasoning tasks and can’t tell 4.4 Analysis WSC 1: Ours 1: WSC 2: Ours 2: Paul tried to call George on the phone, but he wasn’t successful. He tried to call 911 using her cell phone but that he could n’t get the phone to work. Paul tried to call George on the phone, but he was n’t available . He tried twice to call her but she did not answer the phone . Table 3: Comparison of the data from WSC and our training data. Our sentences are retrieved from the UDSSM-II training dataset based on the BM25 value for analysis. The pseudo lab"
N19-1094,P18-1043,0,0.170671,"(Miller, 1995) for external supervision to train word embeddings and solve the WSC challenge. Recently, Trinh and Le (2018) first used raw text from books/news to train a neural Language Model (LM), and then emIntroduction Commonsense reasoning is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day (Davis and Marcus, 2015). It is one of the key challenges in natural language understanding, and has drawn increasing attention in recent years (Levesque et al., 2011; Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b; Zellers et al., 2018; Trinh and Le, 2018). However, due to the lack of labeled training data or comprehensive hand-crafted knowledge bases, commonsense reasoning tasks such as Winograd Schema Challenge (Levesque et al., 2011) are still far from being solved. In this work, we propose two effective unsupervised models for commonsense reasoning, and evaluate them on two classic commonsense reasoning tasks: Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP). Compared to other commonsense reasoning tasks, ∗ B. The demonstraWork done when the author was at Microsoft 882 P"
N19-1094,N16-1098,0,0.0368582,"different types of pairwise training data, thus the model structures are different, as illustrated in Figure 1(b) and 1(c), respectively. Experiments demonstrated that our methods outperform stat-of-the-art performance on the tasks of WSC and PDP. 2 Related Work As a key component of natural language understanding, commonsense reasoning has been included in an increasing number of tasks for evaluation: COPA (Roemmele et al., 2011) assesses commonsense causal reasoning by selecting an alternative, which has a more plausible causal relation with the given premise. Story Cloze Test (ROCStories, Mostafazadeh et al. 2016) evaluates story understanding, story generation, and script learning by choosing the most sensible ending to a short story. JOCI (Zhang et al., 2017) generalizes the natural language inference (NLI) framework (Cooper et al., 1996; Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) and evaluates commonsense inference by predicting the ordinal likelihood of a hypothesis given a context. Event2Mind (Rashkin et al., 2018b) models stereotypical intents and reactions of people, described in short free-form text. SWAG (Zellers et al., 2018) frames commonsense inference as multiple-choic"
N19-1094,J01-4004,0,0.563139,"In the following sections, we will use uppercase symbols in bold, e.g., Sx , to represent matrices. Lowercase symbols in bold, e.g., hx , represent vectors. A regular uppercase symbol, e.g., S x , represents a lexical sequence. A regular lowercase symbol, e.g., xi or y, represents a token. Among all these commonsense reasoning tasks, the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP) (Levesque et al., 2011) are known as the most challenging tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 7"
N19-1094,P02-1014,0,0.464835,"ections, we will use uppercase symbols in bold, e.g., Sx , to represent matrices. Lowercase symbols in bold, e.g., hx , represent vectors. A regular uppercase symbol, e.g., S x , represents a lexical sequence. A regular lowercase symbol, e.g., xi or y, represents a token. Among all these commonsense reasoning tasks, the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP) (Levesque et al., 2011) are known as the most challenging tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted featu"
N19-1094,N15-1082,0,0.0589557,"tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted features. Sharma et al. (2015); Sch¨uller (2014); Bailey et al. (2015); Liu et al. (2017) utilized expensive knowledge bases in their reasoning processes. Recently, Trinh and Le 2018 applied neural language models trained with a massive amount of unlabeled data to the Winograd Schema Challenge and improved the performance by a large margin. In contrast, our unsupervised method based on DSSM significantly outperforms the previous state-of-theart method,"
N19-1094,N18-1101,0,0.0235238,"uage understanding, commonsense reasoning has been included in an increasing number of tasks for evaluation: COPA (Roemmele et al., 2011) assesses commonsense causal reasoning by selecting an alternative, which has a more plausible causal relation with the given premise. Story Cloze Test (ROCStories, Mostafazadeh et al. 2016) evaluates story understanding, story generation, and script learning by choosing the most sensible ending to a short story. JOCI (Zhang et al., 2017) generalizes the natural language inference (NLI) framework (Cooper et al., 1996; Dagan et al., 2006; Bowman et al., 2015; Williams et al., 2018) and evaluates commonsense inference by predicting the ordinal likelihood of a hypothesis given a context. Event2Mind (Rashkin et al., 2018b) models stereotypical intents and reactions of people, described in short free-form text. SWAG (Zellers et al., 2018) frames commonsense inference as multiple-choice questions for follow-up events given some context. ReCoRD (Zhang et al., 2018) 1 We don’t believe it is possible to construct such a knowledge base given that the world is changing constantly. 2 DSSMs can be applied to a wide range of tasks depending on the definition of (x, y). For example,"
N19-1094,D16-1038,0,0.0539894,"uppercase symbols in bold, e.g., Sx , to represent matrices. Lowercase symbols in bold, e.g., hx , represent vectors. A regular uppercase symbol, e.g., S x , represents a lexical sequence. A regular lowercase symbol, e.g., xi or y, represents a token. Among all these commonsense reasoning tasks, the Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP) (Levesque et al., 2011) are known as the most challenging tasks for commonsense reasoning. Although both tasks are based on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted features. Sharma et al. ("
N19-1094,D18-1009,0,0.130991,"nal supervision to train word embeddings and solve the WSC challenge. Recently, Trinh and Le (2018) first used raw text from books/news to train a neural Language Model (LM), and then emIntroduction Commonsense reasoning is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day (Davis and Marcus, 2015). It is one of the key challenges in natural language understanding, and has drawn increasing attention in recent years (Levesque et al., 2011; Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b; Zellers et al., 2018; Trinh and Le, 2018). However, due to the lack of labeled training data or comprehensive hand-crafted knowledge bases, commonsense reasoning tasks such as Winograd Schema Challenge (Levesque et al., 2011) are still far from being solved. In this work, we propose two effective unsupervised models for commonsense reasoning, and evaluate them on two classic commonsense reasoning tasks: Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP). Compared to other commonsense reasoning tasks, ∗ B. The demonstraWork done when the author was at Microsoft 882 Proceedings of NAACL-HLT 2"
N19-1094,N18-1202,0,0.0325233,"lassification model based on whether the word pairs appear in the knowledge base. One limitation of these methods is that they rely heavily on the external knowledge bases. Another limitation is that they just linearly aggregate the embeddings of the words in the context, and that’s hard to integrate the word order information. Instead, our model with LSTM can better represent the contextual information. Besides, our model don’t need any external knowledge bases, and achieve a significant improvement on both of the datasets. We further compare our models with the unsupervised baselines, ELMo (Peters et al., 2018) which selects the candidate based on the cosine similarity of the hidden states of noun and pronoun. Another unsupervised baseline, Google Language Model for commonsense reasoning (Trinh and Le, 2018), which compares the perplexities of the new sentences by replacing the pronoun with candidates. To make a fair comparison to Trinh and Le (2018)’s work, we also train our single model on the corpus of Gutenberg only. We can see that both of our methods get significant improvement on the PDP dataset, and our UDSSM-II can achieve much better performance on the WSC dataset. We also report our ensem"
N19-1094,D12-1071,0,0.431322,"ased on pronoun disambiguation, a subtask of coreference resolution (Soon et al., 2001; Ng and Cardie, 2002; Peng et al., 2016), PDP and WSC differ from normal pronoun disambiguation due to their unique properties, which are based on commonsense, selecting the most likely antecedent from both candidates in the directly preceding context. Previous efforts on solving the Winograd Schema Challenge and Pronoun Disambiguation Problems mostly rely on human-labeled data, sophisticated rules, hand-crafted features, or external knowledge bases (Peng et al., 2015; Bailey et al., 2015; Sch¨uller, 2014). Rahman and Ng (2012) hired workers to annotate supervised training data and designed 70K hand-crafted features. Sharma et al. (2015); Sch¨uller (2014); Bailey et al. (2015); Liu et al. (2017) utilized expensive knowledge bases in their reasoning processes. Recently, Trinh and Le 2018 applied neural language models trained with a massive amount of unlabeled data to the Winograd Schema Challenge and improved the performance by a large margin. In contrast, our unsupervised method based on DSSM significantly outperforms the previous state-of-theart method, with the advantage of capturing more contextual information i"
N19-1094,P18-1213,0,0.0882579,"(Miller, 1995) for external supervision to train word embeddings and solve the WSC challenge. Recently, Trinh and Le (2018) first used raw text from books/news to train a neural Language Model (LM), and then emIntroduction Commonsense reasoning is concerned with simulating the human ability to make presumptions about the type and essence of ordinary situations they encounter every day (Davis and Marcus, 2015). It is one of the key challenges in natural language understanding, and has drawn increasing attention in recent years (Levesque et al., 2011; Roemmele et al., 2011; Zhang et al., 2017; Rashkin et al., 2018a,b; Zellers et al., 2018; Trinh and Le, 2018). However, due to the lack of labeled training data or comprehensive hand-crafted knowledge bases, commonsense reasoning tasks such as Winograd Schema Challenge (Levesque et al., 2011) are still far from being solved. In this work, we propose two effective unsupervised models for commonsense reasoning, and evaluate them on two classic commonsense reasoning tasks: Winograd Schema Challenge (WSC) and Pronoun Disambiguation Problems (PDP). Compared to other commonsense reasoning tasks, ∗ B. The demonstraWork done when the author was at Microsoft 882 P"
N19-1094,Q17-1027,1,\N,Missing
N19-1125,K16-1002,0,0.544301,"versity and relevance of the responses, compared to strong baselines on two datasets with one-tomany context-response mapping. 2 Related Work Grounded conversation models utilize extra context inputs besides conversation history, such as persona (Li et al., 2016b), textual knowledge (Ghazvininejad et al., 2017; Galley et al., 2019), dialog act (Zhao et al., 2017) and emotion (Huber et al., 2018). Our approach does not depend on such extra input and thus is complementary to this line of studies. Variational autoencoder (VAE) models explicitly model the uncertainty of responses in latent space. Bowman et al. (2016) used VAE with Long-Short Term Memory (LSTM) cells to generate sentences. The basic idea of VAE is to encode the input x into a probability distribution (e.g. Gaussian) z instead of a point encoding. However, it suffers from the vanishing latent variable problem (Bowman et al., 2016; Zhao et al., 2017) when applied to text generation tasks. Bowman et al. (2016); Fu et al. (2019) proposed to tackle this problem with word dropping and specific KL annealing methods. Zhao et al. (2017) proposed to add a bag-of-word loss, complementary to KL annealing. Applying this to a CVAE conversation model, th"
N19-1125,W14-4012,0,0.196356,"Missing"
N19-1125,N19-1021,1,0.840516,"et al., 2018). Our approach does not depend on such extra input and thus is complementary to this line of studies. Variational autoencoder (VAE) models explicitly model the uncertainty of responses in latent space. Bowman et al. (2016) used VAE with Long-Short Term Memory (LSTM) cells to generate sentences. The basic idea of VAE is to encode the input x into a probability distribution (e.g. Gaussian) z instead of a point encoding. However, it suffers from the vanishing latent variable problem (Bowman et al., 2016; Zhao et al., 2017) when applied to text generation tasks. Bowman et al. (2016); Fu et al. (2019) proposed to tackle this problem with word dropping and specific KL annealing methods. Zhao et al. (2017) proposed to add a bag-of-word loss, complementary to KL annealing. Applying this to a CVAE conversation model, they showed that even greedy decoding can generate diverse responses. However, as VAE/CVAE conversation models can be limited to a simple latent representations such as standard Gaussian distribution, Gu et al. (2018) proposed to enrich the latent space by leveraging a Gaussian mixture prior. Our work takes a geometrical approach that is fundamentally different from probabilistic"
N19-1125,P16-1094,1,0.881173,"Yes I do. When? Figure 1: Illustration of one context and its multiple responses in the latent space induced by our model. Distance and direction from the predicted response vector given the context roughly match the relevance and diversity, respectively. Based on the example in Table 2.2 Introduction The field of neural response generation is advancing rapidly both in terms of research and commercial applications (Gao et al., 2019; Zhou et al., 2018; Yoshino et al., 2019; Zhang et al., 2019). Nevertheless, vanilla sequence-to-sequence (S2S) models often generate bland and generic responses (Li et al., 2016a). Li et al. (2016a) encourage diversity by re-ranking the beam search results according to their mutual information with the conversation context. However, as beam search itself often produces lists of nearly identical sequences, this method can require a large beam width (e.g. 200). As a result, re-ranking can be extremely 1 An implementation of our model is available at https: //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: ”I would love to play this game”. See Table 2 for more details. time-consuming, raising difficulties for real-time applications"
N19-1125,I17-1061,1,0.843614,"ted in Figure 1. To induce such a latent space, we leverage two different models: 1) a S2S model, producing the predicted response vector (the black dot at the center in Figure 1), and 2) an autoencoder (AE) model, yielding the vectors for potential responses (the colored dots). In order to make the S2S and AE share the same latent space (the cloud), we use the same decoder for both and train them jointly end-to-end with novel regularization terms. As this fuses the two latent spaces, we refer to our model as S PACE F USION. Regularization is necessary because only sharing the decoder, as in (Luan et al., 2017), does not necessarily align the latent spaces obtained by S2S and AE respectively or impose a disentangled structure onto the space. We introduce two regularization terms to tackle this issue. 1) interpolation term: we encourage a smooth semantic transition along the path between the predicted response vector and each target response vector (arrowed lines in Figure 1). This term effectively prevents semantically different responses from aligning in the same direction, essentially scattering them over different directions. 2) fusion term: we want the vectors from the two models to be distribut"
N19-1125,P02-1040,0,0.103428,"Missing"
N19-1125,P16-1009,0,0.0250463,"ibutions in representation and difficulties in training. Decoding and ranking encourage diversity during the decoding stage. As “vanilla” beam search often produces lists of nearly identical sequences, Vijayakumar et al. (2016) propose to include a dissimilarity term in the objective of beam search decoding. Li et al. (2016a) re-ranked the results 1230 obtained by beam search based on mutual information with the context using a separately trained response-to-context S2S model. ? S2S encoder +? ℒ fuse Multi-task learning is another line of studies related to the present work (see Section 3.2). Sennrich et al. (2016) use multi-task learning to improve neural machine translation by utilizing monolingual data, which usually far exceeds the amount of parallel data. A similar idea is applied by Luan et al. (2017) to conversational modeling, involving two tasks: 1) a S2S model that learns a context-to-response mapping using conversation data, and 2) an AE model that utilizes speakerspecific non-conversational data. The decoders of S2S and AE were shared, and the two tasks were trained alternately. 3 3.1 The S PACE F USION Model Problem statement Let D = [(x0 , y0 ), (x1 , y1 ), · · · , (xn , yn )] denote a con"
N19-1125,1983.tc-1.13,0,0.435323,"Missing"
N19-1125,N16-1014,1,0.836698,"Yes I do. When? Figure 1: Illustration of one context and its multiple responses in the latent space induced by our model. Distance and direction from the predicted response vector given the context roughly match the relevance and diversity, respectively. Based on the example in Table 2.2 Introduction The field of neural response generation is advancing rapidly both in terms of research and commercial applications (Gao et al., 2019; Zhou et al., 2018; Yoshino et al., 2019; Zhang et al., 2019). Nevertheless, vanilla sequence-to-sequence (S2S) models often generate bland and generic responses (Li et al., 2016a). Li et al. (2016a) encourage diversity by re-ranking the beam search results according to their mutual information with the conversation context. However, as beam search itself often produces lists of nearly identical sequences, this method can require a large beam width (e.g. 200). As a result, re-ranking can be extremely 1 An implementation of our model is available at https: //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: ”I would love to play this game”. See Table 2 for more details. time-consuming, raising difficulties for real-time applications"
N19-1125,P17-1061,0,0.528488,": //github.com/golsun/SpaceFusion 2 For simplicity, we omitted the response at the center: ”I would love to play this game”. See Table 2 for more details. time-consuming, raising difficulties for real-time applications. This highlights the need to improve the diversity of candidates before re-ranking, and the need to optimize for diversity during training rather than just at the decoding stage. While various approaches have been explored to diversify the output of conversation models, the improvement often comes at the cost of decreased response relevance along other dimensions. For instance, Zhao et al. (2017) present an approach to enhancing diversity by mapping diverse responses to a probability distribution using a conditional variational autoencoder (CVAE). Despite the improved response diversity, this approach reduces response relevance as measured against the baseline. One possible reason for this diversityrelevance trade-off is that such probabilistic approaches are not explicitly encouraged to induce a disentangled representation in latent space for 1229 Proceedings of NAACL-HLT 2019, pages 1229–1238 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguis"
N19-1271,D11-1033,1,0.731321,"and add to S 6: Assign mini-batches in S in a random order to obtain a sequence B = (b1 , ..., bL ), where L = N1 + bαN1 c 7: for each mini-batch b ∈ B do 8: Perform P gradient update on M with loss l(b) = (Q,P,A)∈b l(Q, P, A) 9: end for 10: Evaluate development set performance 11: end for Output: Model with best evaluation performance the ratio gives the same weight to every auxiliary data, but the relevance of every data point to the target task can vary greatly. We develop a novel re-weighting method to resolve these problems, using ideas inspired by data selection in machine translation (Axelrod et al., 2011; van der Wees et al., 2017). We use (Qk , P k , Ak ) to represent a data point from the kth task for 1 ≤ k ≤ K, with k = 1 being the target task. Since the passage styles are hard to evaluate, we only evaluate data points based on Qk and Ak . Note that only data from auxiliary task (2 ≤ k ≤ K) is re-weighted; target task data always have weight 1. Our scores consist of two parts, one for questions and one for answers. For questions, we create language models (detailed in Section 5.2) using questions from each task, which we represent as LMk for the k-th task. For each question Qk from auxilia"
N19-1271,P17-1171,0,0.0270339,"g questions on multi-task learning for MRC: 1. Can we improve the performance of existing MRC systems using multi-task learning? 2. How does multi-task learning affect the performance if we combine it with other external data? 3. How does the learning algorithm change the performance of multi-task MRC? 4. How does our method compare with existing MTL methods? We first present our experiment details and results for MT-SAN. Then, we provide a comprehensive study on the effectiveness of various MTL algorithms in Section 5.4. At last, we provide some additional results on combining MTL with DrQA (Chen et al., 2017) to show the flexibility of our approach 1 . 5.1 Datasets We conducted experiments on SQuAD (Rajpurkar et al., 2016), NewsQA(Trischler et al., 2017), MS MARCO (v1, Nguyen et al.,2016) and WDW (Onishi et al., 2016). Dataset statistics is shown in Table 1. Although similar in size, these datasets are quite different in domains, lengths of text, and types of task. In the following experiments, we will validate whether including external datasets as additional input information (e.g., pre-trained language model on these datasets) helps boost the performance of MRC systems. 1 We include the results"
N19-1271,W14-4012,0,0.0611353,"Missing"
N19-1271,D17-1087,0,0.0712978,"ain from MTL can be easily combined with existing methods to obtain further performance gain. Third, the proposed sampling and re-weighting scheme can further improve the multi-task learning performance. 2 Related Work Studies in machine reading comprehension mostly focus on architecture design of neural networks, such as bidirectional attention (Seo et al., 2016), dynamic reasoning (Xu et al., 2017), and parallelization (Yu et al., 2018). Some recent work has explored transfer learning that leverages outdomain data to learn MRC models when no training data is available for the target domain (Golub et al., 2017). In this work, we explore multi-task learning to make use of the data from other domains, while we still have access to target domain training data. Multi-task learning (Caruana, 1997) has been widely used in machine learning to improve generalization using data from multiple tasks. For natural language processing, MTL has been successfully applied to low-level parsing tasks (Collobert et al., 2011), sequence-to-sequence learning (Luong et al., 2015), and web search (Liu et al., 2015). More recently, (McCann et al., 2018) proposes to cast all tasks from parsing to translation as a QA problem"
N19-1271,P16-1086,0,0.0659381,"Missing"
N19-1271,N15-1092,1,0.779501,"that leverages outdomain data to learn MRC models when no training data is available for the target domain (Golub et al., 2017). In this work, we explore multi-task learning to make use of the data from other domains, while we still have access to target domain training data. Multi-task learning (Caruana, 1997) has been widely used in machine learning to improve generalization using data from multiple tasks. For natural language processing, MTL has been successfully applied to low-level parsing tasks (Collobert et al., 2011), sequence-to-sequence learning (Luong et al., 2015), and web search (Liu et al., 2015). More recently, (McCann et al., 2018) proposes to cast all tasks from parsing to translation as a QA problem and use a single network to solve all of them. However, their results show that multi-task learning hurts the performance of most tasks when tackling them together. Differently, we focus on applying MTL to the MRC task and show significant improvement over single-task baselines. Our sample re-weighting scheme bears some resemblance to previous MTL techniques that assign weights to tasks (Kendall et al., 2018). However, our method gives a more granular score for each sample and provides"
N19-1271,P19-1441,1,0.887037,"Missing"
N19-1271,P18-1157,1,0.49377,"ion (MRC) has gained growing interest in the research community (Rajpurkar et al., 2016; Yu et al., 2018). In an MRC task, the machine reads a text passage and a question, and generates (or selects) an answer based on the passage. This requires the machine to possess strong comprehension, inference and reasoning capabilities. Over the past few years, there has been much progress in building end-toend neural network models (Seo et al., 2016) for MRC. However, most public MRC datasets (e.g., SQuAD, MS MARCO, TriviaQA) are typically small (less than 100K) compared to the model size (such as SAN (Liu et al., 2018c,b) with around 10M parameters). To prevent over-fitting, recently there have been some studies on using pre-trained word embeddings (Pennington et al., 2014) and contextual embeddings in the MRC model training, as well as back-translation approaches (Yu et al., 2018) for data augmentation. ∗ Most of this work was performed when the author was interning at Microsoft. Multi-task learning (Caruana, 1997) is a widely studied area in machine learning, aiming at better model generalization by combining training datasets from multiple tasks. In this work, we explore a multi-task learning (MTL) fram"
N19-1271,D16-1241,0,0.131818,"d with more flexibility on various MRC tasks. MTL is also faster and easier to train than embedding/LM methods: our approach requires no pre-trained models, whereas back translation and ELMo both rely on large models that would need days to train on multiple GPUs (Jozefowicz et al., 2016; Peters et al., 2018). We validate our MTL framework with two state-of-the-art models on four datasets from different domains. Experiments show that our methods lead to a significant performance gain over single-task baselines on SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017) and WhoDid-What (Onishi et al., 2016), while achieving state-of-the-art performance on the latter two. For example, on NewsQA (Trischler et al., 2017), our model surpassed human performance by 13.4 (46.5 vs 59.9) and 3.2 (72.6 vs 69.4) absolute points in terms of exact match and F1. The contribution of this work is three-fold. First, we apply multi-task learning to the MRC task, which brings significant improvements over single-task baselines. Second, the performance gain from MTL can be easily combined with existing methods to obtain further performance gain. Third, the proposed sampling and re-weighting scheme can further impro"
N19-1271,D14-1162,0,0.0943077,"sage and a question, and generates (or selects) an answer based on the passage. This requires the machine to possess strong comprehension, inference and reasoning capabilities. Over the past few years, there has been much progress in building end-toend neural network models (Seo et al., 2016) for MRC. However, most public MRC datasets (e.g., SQuAD, MS MARCO, TriviaQA) are typically small (less than 100K) compared to the model size (such as SAN (Liu et al., 2018c,b) with around 10M parameters). To prevent over-fitting, recently there have been some studies on using pre-trained word embeddings (Pennington et al., 2014) and contextual embeddings in the MRC model training, as well as back-translation approaches (Yu et al., 2018) for data augmentation. ∗ Most of this work was performed when the author was interning at Microsoft. Multi-task learning (Caruana, 1997) is a widely studied area in machine learning, aiming at better model generalization by combining training datasets from multiple tasks. In this work, we explore a multi-task learning (MTL) framework to enable the training of one universal model across different MRC tasks for better generalization. Intuitively, this multi-task MRC model can be viewed"
N19-1271,N18-1202,0,0.314219,"pose two MTL training algorithms to improve the performance. The first method simply adopts a sampling scheme, which randomly selects training data from the auxiliary tasks controlled by a ratio hyperparameter; The second algorithm incorporates recent ideas of data selection in machine translation (van der Wees et al., 2017). It learns the sample weights from the auxiliary tasks automatically through language models. Prior to this work, many studies have used upstream datasets to augment the performance of MRC models, including word embedding (Pennington et al., 2014), language models (ELMo) (Peters et al., 2018) and machine translation (Yu et al., 2018). These methods aim to obtain a robust semantic encoding of both passages and questions. Our MTL method is orthogonal to these methods: rather than enriching semantic embedding with external knowledge, we leverage existing MRC datasets across different domains, which help make the whole comprehension process more robust and universal. Our experiments show that MTL can bring further performance boost when combined with contextual representations from pre-trained language models, e.g., ELMo (Peters et al., 2018). 2644 Proceedings of NAACL-HLT 2019, pages"
N19-1271,P18-2124,0,0.041427,"Missing"
N19-1271,P18-1158,0,0.155803,"l validate whether including external datasets as additional input information (e.g., pre-trained language model on these datasets) helps boost the performance of MRC systems. 1 We include the results in the appendix due to space limitations. 2648 Model Dev Set Performance Single Model without Language Models EM,F1 BiDAF (Seo et al., 2016) SAN (Liu et al., 2018c) MT-SAN on SQuAD (single task, ours) MT-SAN on SQuAD+NewsQA(ours) MT-SAN on SQuAD+MARCO(ours) MT-SAN on SQuAD+NewsQA+MARCO(ours) 67.7, 77.3 76.24, 84.06 76.84, 84.54 78.60, 85.87 77.79, 85.23 78.72, 86.10 Single Model with ELMo SLQA+ (Wang et al., 2018a) MT-SAN on SQuAD (single task, ours) MT-SAN on SQuAD+NewsQA(ours) MT-SAN on SQuAD+MARCO(ours) MT-SAN on SQuAD+NewsQA+MARCO(ours) BERT (Devlin et al., 2018) Human Performance (test set) 80.0, 87.0 80.04, 86.54 81.36, 87.71 80.37, 87.17 81.58, 88.19 84.2, 91.1 82.30, 91.22 Table 2: Performance of our method to train SAN in multi-task setting, competing published results, leaderboard results and human performance, on SQuAD dataset (single model). Note that BERT uses a much larger language model, and is not directly comparable with our results. We expect our test performance is roughly similar o"
N19-1271,P18-1178,0,0.0350362,"Missing"
N19-1271,D17-1147,0,0.0609615,"Missing"
N19-1271,K17-1028,0,0.0130536,"-Net3 SAN4 MT-SAN MT-SAN: SQuAD+MARCO MT-SAN: 3 datasets Single Model With ELMo MT-SAN MT-SAN: SQuAD+MARCO MT-SAN: 3 datasets Human Performance (test set) Scores 33.99, 32.09 38.62, 38.01 -, 45.65 43.85, 46.14 34.13, 42.65 34.29, 43.47 36.99, 43.64 34.57, 42.88 37.02, 43.89 37.12, 44.12 48.02, 49.72 Table 4: Performance of our method to train SAN in multi-task setting, competing published results and human performance, on MS MARCO dataset. The scores stand for (BLEU-1, ROUGE-L) respectively. All SAN results are our results. “3 dataset” means we train using SQuAD+NewsQA+MARCO. References: 1 : (Weissenborn et al., 2017). 2 : implemented by (Shen et al., 2017). 3 :(Wang et al., 2018b). 4 : (Liu et al., 2018c) or cross-passage ranking (Wang et al., 2018b). We also test the robustness of our algorithm by performing another set of experiments on SQuAD and WDW. WDW is much more different than the other three datasets (SQuAD, NewsQA, MS MARCO): WDW guarantees that the answer is always a person, whereas the percentage of 2650 Model MT-SAN (Single Task) MT-SAN (S+W) SOTA(Yang et al., 2016). Human Performance SQuAD 76.8, 84.5 77.6, 85.1 86.2, 92.2 82.3, 91.2 Model Performance SQuAD + MARCO EM,F1 Simple Combine (Alg."
N19-1271,D16-1264,0,0.727668,"ent domains. Inspired by recent ideas of data selection in machine translation, we develop a novel sample re-weighting scheme to assign sample-specific weights to the loss. Empirical study shows that our approach can be applied to many existing MRC models. Combined with contextual representations from pre-trained language models (such as ELMo), we achieve new state-of-the-art results on a set of MRC benchmark datasets. We release our code at https://github.com/ xycforgithub/MultiTask-MRC. 1 Introduction Machine Reading Comprehension (MRC) has gained growing interest in the research community (Rajpurkar et al., 2016; Yu et al., 2018). In an MRC task, the machine reads a text passage and a question, and generates (or selects) an answer based on the passage. This requires the machine to possess strong comprehension, inference and reasoning capabilities. Over the past few years, there has been much progress in building end-toend neural network models (Seo et al., 2016) for MRC. However, most public MRC datasets (e.g., SQuAD, MS MARCO, TriviaQA) are typically small (less than 100K) compared to the model size (such as SAN (Liu et al., 2018c,b) with around 10M parameters). To prevent over-fitting, recently the"
N19-1271,I17-1096,1,0.8607,"datasets Single Model With ELMo MT-SAN MT-SAN: SQuAD+MARCO MT-SAN: 3 datasets Human Performance (test set) Scores 33.99, 32.09 38.62, 38.01 -, 45.65 43.85, 46.14 34.13, 42.65 34.29, 43.47 36.99, 43.64 34.57, 42.88 37.02, 43.89 37.12, 44.12 48.02, 49.72 Table 4: Performance of our method to train SAN in multi-task setting, competing published results and human performance, on MS MARCO dataset. The scores stand for (BLEU-1, ROUGE-L) respectively. All SAN results are our results. “3 dataset” means we train using SQuAD+NewsQA+MARCO. References: 1 : (Weissenborn et al., 2017). 2 : implemented by (Shen et al., 2017). 3 :(Wang et al., 2018b). 4 : (Liu et al., 2018c) or cross-passage ranking (Wang et al., 2018b). We also test the robustness of our algorithm by performing another set of experiments on SQuAD and WDW. WDW is much more different than the other three datasets (SQuAD, NewsQA, MS MARCO): WDW guarantees that the answer is always a person, whereas the percentage of 2650 Model MT-SAN (Single Task) MT-SAN (S+W) SOTA(Yang et al., 2016). Human Performance SQuAD 76.8, 84.5 77.6, 85.1 86.2, 92.2 82.3, 91.2 Model Performance SQuAD + MARCO EM,F1 Simple Combine (Alg. 1) 77.1, 84.6 Loss Uncertainty 77.3, 84."
N19-1271,P17-1075,0,0.0501405,"Missing"
N19-1271,W17-2623,0,0.462706,"r model, on the other hand, can be trained with more flexibility on various MRC tasks. MTL is also faster and easier to train than embedding/LM methods: our approach requires no pre-trained models, whereas back translation and ELMo both rely on large models that would need days to train on multiple GPUs (Jozefowicz et al., 2016; Peters et al., 2018). We validate our MTL framework with two state-of-the-art models on four datasets from different domains. Experiments show that our methods lead to a significant performance gain over single-task baselines on SQuAD (Rajpurkar et al., 2016), NewsQA (Trischler et al., 2017) and WhoDid-What (Onishi et al., 2016), while achieving state-of-the-art performance on the latter two. For example, on NewsQA (Trischler et al., 2017), our model surpassed human performance by 13.4 (46.5 vs 59.9) and 3.2 (72.6 vs 69.4) absolute points in terms of exact match and F1. The contribution of this work is three-fold. First, we apply multi-task learning to the MRC task, which brings significant improvements over single-task baselines. Second, the performance gain from MTL can be easily combined with existing methods to obtain further performance gain. Third, the proposed sampling and"
O01-2002,J90-2002,0,0.254599,"Missing"
O01-2002,J92-4003,0,0.436679,"Missing"
O01-2002,A88-1019,0,0.0107942,"Missing"
O01-2002,W99-0617,0,0.038655,"Missing"
O01-2002,C90-2036,0,0.0692247,"Missing"
O01-2002,P93-1024,0,0.354987,"Missing"
O03-5001,M95-1012,0,0.019977,"the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, howe"
O03-5001,M98-1014,0,0.0748286,"s and NE abbreviations. The rest of this paper is organized as follows: Section 2 briefly discusses related work. Section 3 presents in detail the class-based LM for Chinese NE identification. Section 4 discusses our methods of identifying NE abbreviations. Section 5 reports experimental results. Section 6 presents conclusions and future work. 2. Related Work Traditionally, the approaches to NE identification have been rule-based. They attempt to perform matching against a sequence of words in much the same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 19"
O03-5001,W02-2002,0,0.0703239,"et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems s"
O03-5001,J92-4003,0,0.0859177,"Missing"
O03-5001,J95-4004,0,0.0190207,"sman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the"
O03-5001,W02-2004,0,0.0435105,"[Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Som"
O03-5001,M98-1017,0,0.55844,"3 presents in detail the class-based LM for Chinese NE identification. Section 4 discusses our methods of identifying NE abbreviations. Section 5 reports experimental results. Section 6 presents conclusions and future work. 2. Related Work Traditionally, the approaches to NE identification have been rule-based. They attempt to perform matching against a sequence of words in much the same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collin"
O03-5001,W99-0613,0,0.130951,"Missing"
O03-5001,W02-2010,0,0.018642,"ikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Some representative systems include those developed in [Sun et al., 1994; Chen and Lee, 1994; Chen et al., 1998; Yu et al., 1998; Zhang, 2001; Sun et al., 2002]. We will mainly introduce two systems, namely, the rule-based NTU system for Chinese [Chen et al., 1998] and the machine learning"
O03-5001,M98-1020,0,0.0290977,"n. Section 4 discusses our methods of identifying NE abbreviations. Section 5 reports experimental results. Section 6 presents conclusions and future work. 2. Related Work Traditionally, the approaches to NE identification have been rule-based. They attempt to perform matching against a sequence of words in much the same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002]"
O03-5001,O01-2002,1,0.856911,"Missing"
O03-5001,M95-1014,0,0.0121619,"g NE abbreviations. Section 5 reports experimental results. Section 6 presents conclusions and future work. 2. Related Work Traditionally, the approaches to NE identification have been rule-based. They attempt to perform matching against a sequence of words in much the same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], c"
O03-5001,M98-1007,0,0.0931121,"Missing"
O03-5001,W02-2013,0,0.0298652,"d approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Some representative systems include those developed in [Sun et al., 1994; Chen and Lee, 1994; Chen et al., 1998; Yu et al.,"
O03-5001,M98-1015,0,0.0421973,"Missing"
O03-5001,W02-2020,0,0.0215295,"able. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Some representative systems include those developed in [Sun et al., 1994; Chen and Lee, 1994; Chen et al., 1998; Yu et al., 1998; Zhang, 2001; Sun et al., 2002]. We will mainly"
O03-5001,M98-1021,0,0.039939,"riefly discusses related work. Section 3 presents in detail the class-based LM for Chinese NE identification. Section 4 discusses our methods of identifying NE abbreviations. Section 5 reports experimental results. Section 6 presents conclusions and future work. 2. Related Work Traditionally, the approaches to NE identification have been rule-based. They attempt to perform matching against a sequence of words in much the same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vas"
O03-5001,A97-1028,0,0.0754129,"Missing"
O03-5001,W02-2025,0,0.0140736,"proaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Some representative systems include those developed in [Sun et al., 1994; Chen and Lee, 1994; Chen et al., 1998; Yu et al., 1998; Zhang, 2001; Sun et al., 2002]. We will mainly introduce two systems, namely, the rule-based NTU system for Chi"
O03-5001,W98-1120,0,0.0327242,"lack et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to"
O03-5001,C02-1012,1,0.613407,"same way that a general regular expression matcher does. Some of these systems are, FACILE [Black et al., 1998], IsoQuest's NetOwl [Krupha and Hausman, 1998], the LTG system [Mikheev et al., 1998], the NTU system [Chen et al., 1998], LaSIE [Humphreys et al., 1998], the Oki system [Fukumoto et al., 1998], and the Proteus system [Grishman, 1995]. However, the rule-based approaches are neither robust nor portable. Recently, research on NE identification has focused on machine learning approaches, including the hidden Markov model [Bikel et al., 1999; Miller et al., 1998; Gotoh and Renals, 2000; Sun et al., 2002; Zhou and Su, 2002], maximum entropy model [Borthwick, 1999], decision tree [Sekine et al., 1998], transformation-based learning [Brill, 1995; Aberdeen et al., 1995; Black and Vasilakopoulos, 2002], boosting [Collins, 2002; Carreras et al., 2002; Tsukamoto et al., 2002; Wu et al., 2002], the voted perceptron [Collins, 2002], conditional Markov model [Jansche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, ha"
O03-5001,W02-2035,0,0.0900186,"Missing"
O03-5001,M98-1016,0,0.0156729,"nsche, 2002], support vector machine [McNamee and Mayfield, 2002; Takeuchi and Collier, 2002], memory-based learning [Sang, 2002] and learning approaches stacking [Florian, 2002]. Some systems, especially those for English NE identification, have A Class-based Language Model Approach to Chinese Named Entity Identification 3 been applied to practical applications. When it comes to the Chinese language, however, NE identification systems still cannot achieve satisfactory performance. Some representative systems include those developed in [Sun et al., 1994; Chen and Lee, 1994; Chen et al., 1998; Yu et al., 1998; Zhang, 2001; Sun et al., 2002]. We will mainly introduce two systems, namely, the rule-based NTU system for Chinese [Chen et al., 1998] and the machine learning based BBN system [Bikel et al., 1999], because these are representative of the two different approaches. Generally speaking, the NTU system employs the rule-based method. It utilizes different types of information and models, including character conditions, statistic information, titles, punctuation marks, organization and location keywords, speech-act and locative verbs, cache model and n-gram model. Different kinds of NEs employ di"
O03-5001,M98-1004,0,\N,Missing
O03-5001,M98-1012,0,\N,Missing
O03-5001,dorr-etal-2000-chinese,0,\N,Missing
O03-5001,W98-0708,0,\N,Missing
O03-5001,chang-etal-1998-taxonomy,0,\N,Missing
O03-5001,J98-1005,0,\N,Missing
O03-5001,W98-0718,0,\N,Missing
O03-5001,W98-0709,0,\N,Missing
O03-5001,W98-0705,0,\N,Missing
O03-5001,W98-0704,0,\N,Missing
O03-5001,J93-1007,0,\N,Missing
O03-5001,W02-1817,0,\N,Missing
O03-5001,J01-1001,0,\N,Missing
O03-5001,C94-1101,0,\N,Missing
O03-5001,C92-2070,0,\N,Missing
O03-5001,P00-1064,0,\N,Missing
O03-5001,W00-1202,0,\N,Missing
O03-5001,W02-2029,0,\N,Missing
O03-5001,W02-2031,0,\N,Missing
O03-5001,C02-1080,0,\N,Missing
O03-5001,P97-1061,0,\N,Missing
O03-5001,P95-1026,0,\N,Missing
O03-5001,P99-1020,0,\N,Missing
O03-5001,W00-1318,0,\N,Missing
O03-5001,J94-4003,0,\N,Missing
O03-5001,P91-1019,0,\N,Missing
O03-5001,P02-1060,0,\N,Missing
O03-5001,C02-1044,0,\N,Missing
O03-5001,1996.amta-1.13,0,\N,Missing
O03-5001,P96-1006,0,\N,Missing
O03-5001,P95-1025,0,\N,Missing
O03-5001,C96-1089,0,\N,Missing
O03-5001,P98-1117,0,\N,Missing
O03-5001,C98-1113,0,\N,Missing
O03-5001,W95-0111,0,\N,Missing
O03-5001,M98-1009,0,\N,Missing
O03-5001,P02-1062,0,\N,Missing
P00-1067,P93-1002,0,0.0604293,"Missing"
P00-1067,P91-1022,0,0.0208785,"Missing"
P00-1067,W93-0301,0,0.0648142,"Missing"
P00-1067,J93-2003,0,0.00789834,"Missing"
P00-1067,P93-1001,0,\N,Missing
P02-1023,P00-1073,1,0.803588,"criterion, which is used to estimate the performance loss of the pruned model. Given the pruning criterion, a simple thresholding algorithm for pruning bigram models can be described as follows: 1. Select a threshold θ. 2. Compute the performance loss due to pruning each bigram individually using the pruning criterion. 3. Remove all bigrams with performance loss less than θ. 4. Re-compute backoff weights. Figure 1: Thresholding algorithm for bigram pruning The algorithm in Figure 1 together with several pruning criteria has been studied previously (Seymore and Rosenfeld, 1996; Stolcke, 1998; Gao and Lee, 2000; etc). A comparative study of these techniques is presented in (Goodman and Gao, 2000). In this paper, three pruning criteria will be studied: probability, rank, and entropy. Probability serves as the baseline pruning criterion. It is derived from perplexity which has been widely used as a LM evaluation measure. Rank and entropy have been previously used as a metric for LM evaluation in (Clarkson and Robinson, 2001). In the current paper, these two measures will be studied for the purpose of backoff n-gram model pruning. In the next section, we will describe how pruning criteria are developed"
P02-1024,J90-2002,0,0.064307,"ACM. We then describe in detail the methodology of constructing the ACM. The effectiveness of the ACM is evaluated on a realistic application, namely Japanese Kana-Kanji conversion. Experimental results show substantial improvements of the ACM in comparison with classical cluster models and word n-gram models at the same model size. Our analysis shows that the high-performance of the ACM lies in the asymmetry of the model. 1 Introduction The n-gram model has been widely applied in many applications such as speech recognition, machine translation, and Asian language text input [Jelinek, 1990; Brown et al., 1990; Gao et al., 2002]. It is a stochastic model, which predicts the next word (predicted word) given the previous n-1 words (conditional words) in a word sequence. 1 hangli@microsoft.com The cluster n-gram model is a variant of the word n-gram model in which similar words are classified in the same cluster. This has been demonstrated as an effective way to deal with the data sparseness problem and to reduce the memory sizes for realistic applications. Recent research [Yamamoto et al., 2001] shows that using different clusters for predicted and conditional words can lead to cluster models that ar"
P02-1024,J92-4003,0,0.924381,"ed word) given the previous n-1 words (conditional words) in a word sequence. 1 hangli@microsoft.com The cluster n-gram model is a variant of the word n-gram model in which similar words are classified in the same cluster. This has been demonstrated as an effective way to deal with the data sparseness problem and to reduce the memory sizes for realistic applications. Recent research [Yamamoto et al., 2001] shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models, which use the same clusters for both words [Brown et al., 1992]. This is the basis of the asymmetric cluster model (ACM), which will be formally defined and empirically studied in this paper. Although similar models have been used in previous studies [Goodman and Gao, 2000; Yamamoto et al., 2001], several issues have not been completely investigated. These include: (1) an effective methodology for constructing the ACM, (2) a thorough comparative study of the ACM with classical cluster models and word models when they are applied to a realistic application, and (3) an analysis of the reason why the ACM is superior. The goal of this study is to address the"
P02-1024,O01-2002,1,0.844613,"Missing"
P02-1024,P93-1024,0,0.107216,"model. This paper is organized as follows: Section 1 introduces our research topic, and then Section 2 reviews related work. Section 3 defines the ACM and describes in detail the method of model construction. Section 4 first introduces the Japanese Kana-Kanji conversion task; it then presents our main experiments and a discussion of our findings. Finally, conclusions are presented in Section 5. 2 Related Work A large amount of previous research on clustering has been focused on how to find the best clusters [Brown et al., 1992; Kneser and Ney, 1993; Yamamoto and Sagisaka, 1999; Ueberla, 1996; Pereira et al., 1993; Bellegarda et al., 1996; Bai et al., 1998]. Only small differences have been observed, however, in the performance of the different techniques for constructing clusters. In this study, we focused our research on novel techniques for using clusters – the ACM, in which different clusters are used for predicted and conditional words respectively. The discussion of the ACM in this paper is an extension of several studies below. The first similar cluster model was presented by Goodman and Gao [2000] in which the clustering techniques were combined with Stolcke’s [1998] pruning to reduce the langu"
P02-1024,W97-0309,0,0.0420057,"Missing"
P02-1024,P01-1068,0,0.387971,"ny applications such as speech recognition, machine translation, and Asian language text input [Jelinek, 1990; Brown et al., 1990; Gao et al., 2002]. It is a stochastic model, which predicts the next word (predicted word) given the previous n-1 words (conditional words) in a word sequence. 1 hangli@microsoft.com The cluster n-gram model is a variant of the word n-gram model in which similar words are classified in the same cluster. This has been demonstrated as an effective way to deal with the data sparseness problem and to reduce the memory sizes for realistic applications. Recent research [Yamamoto et al., 2001] shows that using different clusters for predicted and conditional words can lead to cluster models that are superior to classical cluster models, which use the same clusters for both words [Brown et al., 1992]. This is the basis of the asymmetric cluster model (ACM), which will be formally defined and empirically studied in this paper. Although similar models have been used in previous studies [Goodman and Gao, 2000; Yamamoto et al., 2001], several issues have not been completely investigated. These include: (1) an effective methodology for constructing the ACM, (2) a thorough comparative st"
P03-1035,O93-1004,0,0.434107,"ianfeng Li, Wenfeng Yang and Xiaodan Zhu for their help with evaluating our system. a large degree upon the coverage of the dictionary, which unfortunately may never be complete because new words appear constantly. Therefore, in addition to the dictionary, many systems also contain special components for unknown word identification. In particular, statistical methods have been widely applied because they utilize a probabilistic or cost-based scoring mechanism, instead of the dictionary, to segment the text. These methods however, suffer from three drawbacks. First, some of these methods (e.g. Lin et al., 1993) identify unknown words without identifying their types. For instance, one would identify a string as a unit, but not identify whether it is a person name. This is not always sufficient. Second, the probabilistic models used in these methods (e.g. Teahan et al., 2000) are trained on a segmented corpus which is not always available. Third, the identified unknown words are likely to be linguistically implausible (e.g. Dai et al., 1999), and additional manual checking is needed for some subsequent tasks such as parsing. We believe that the identification of unknown words should not be defined as"
P03-1035,J96-3004,0,0.676541,"me. This is not always sufficient. Second, the probabilistic models used in these methods (e.g. Teahan et al., 2000) are trained on a segmented corpus which is not always available. Third, the identified unknown words are likely to be linguistically implausible (e.g. Dai et al., 1999), and additional manual checking is needed for some subsequent tasks such as parsing. We believe that the identification of unknown words should not be defined as a separate problem from word segmentation. These two problems are better solved simultaneously in a unified approach. One example of such approaches is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our approach is motivated by the same inspiration, but is based on a different mechanism: the improved source-channel models. As we shall see, these models provide a more flexible framework to incorporate various kinds of lexical and statistical information. Some types of unknown words that are not discussed in Sproat’s system are dealt with in our system. 3 processed in different ways in our system. For example, the plausible word segmentation for the sentence in Figure 1(a) is as shown. Figure 1(b) is the output of our system, whe"
P03-1035,J00-3004,0,0.0796475,"tems also contain special components for unknown word identification. In particular, statistical methods have been widely applied because they utilize a probabilistic or cost-based scoring mechanism, instead of the dictionary, to segment the text. These methods however, suffer from three drawbacks. First, some of these methods (e.g. Lin et al., 1993) identify unknown words without identifying their types. For instance, one would identify a string as a unit, but not identify whether it is a person name. This is not always sufficient. Second, the probabilistic models used in these methods (e.g. Teahan et al., 2000) are trained on a segmented corpus which is not always available. Third, the identified unknown words are likely to be linguistically implausible (e.g. Dai et al., 1999), and additional manual checking is needed for some subsequent tasks such as parsing. We believe that the identification of unknown words should not be defined as a separate problem from word segmentation. These two problems are better solved simultaneously in a unified approach. One example of such approaches is Sproat et al. (1996), which is based on weighted finite-state transducers (FSTs). Our approach is motivated by the s"
P03-1035,C02-1012,1,\N,Missing
P03-1066,P01-1017,0,0.0165917,"and discuss several factors that we believe to have the most significant impact on the performance of DLM. The discussion includes: (1) the use of DLM as a parser, (2) the definition of the mapping function Φ, and (3) the method of unsupervised dependency structure acquisition. One basic approach to using linguistic structure for language modeling is to extend the conventional language model P(W) to P(W, T), where T is a parse tree of W. The extended model can then be used as a parser to select the most likely parse by T* = argmaxT P(W, T). Many recent studies (e.g., Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001) adopt this approach. Similarly, dependency-based models (e.g., Collins, 1996; Chelba et al., 1997) use a dependency structure D of W instead of a parse tree T, where D is extracted from syntactic trees. Both of these models can be called grammar-based models, in that they capture the syntactic structure of a sentence, and the model parameters are estimated from syntactically annotated corpora such as the Penn Treebank. DLM, on the other hand, is a non-grammar-based model, because it is not based on any syntactic annotation: the dependency structure used in language modeling was"
P03-1066,P96-1025,0,0.312096,"dependency 1 The model in Equation (3) is not strictly probabilistic because it drops the probabilities of illegal dependencies (e.g., crossing dependencies). between wi and wj. The maximum likelihood estimation (MLE) of P(dij) is given by P (d ij ) = C ( wi , w j , R ) (4) C ( wi , w j ) where C(wi, wj, R) is the number of times wi and wj have a dependency relation in a sentence in training data, and C(wi, wj) is the number of times wi and wj are seen in the same sentence. To deal with the data sparseness problem of MLE, we used the backoff estimation strategy similar to the one proposed in Collins (1996), which backs off to estimates that use less conditioning context. More specifically, we used the following three estimates: (5) η + η3 η η E1 = 1 E23 = 2 E4 = 4 , δ1 δ2 + δ3 δ4 wj, it tries to link wj to each of its previous words wi, and push the generated dependency dij into a stack. When a dependency crossing or a cycle is detected in the stack, the dependency with the lowest dependency probability in conflict is eliminated. The algorithm is outlined in Figures 2 and 3. DEPENDENCY-PARSING(W) for j Å 1 to LENGTH(W) for i Å j-1 downto 1 PUSH dij = (wi, wj) into the stack Dj if a dependency c"
P03-1066,P99-1059,0,0.0277679,"guaranteed to find the most probable D, we opted for it because it works in a left-to-right manner, and is very efficient and simple to implement. In our experiments, we found that the algorithm performs reasonably well on average, and its speed and simplicity make it a better choice in DLM training where we need to parse a large amount of training data iteratively, as described in Section 3.3. The parsing algorithm is a slightly modified version of that proposed in Yuret (1998). It reads a sentence left to right; after reading each new word 2 For parsers that use bigram lexical dependencies, Eisner and Satta (1999) presents parsing algorithms that are O(n4) or O(n3). We thank Joshua Goodman for pointing this out. w2 (a) w3 w1 w2 w3 w4 (b) Figure 3. (a) An example of a dependency cycle: given that P(d23) is smaller than P(d12) and P(d13), d23 is removed (represented as dotted line). (b) An example of a dependency crossing: given that P(d13) is smaller than P(d24), d13 is removed. Let the dependency probability be the measure of the strength of a dependency, i.e., higher probabilities mean stronger dependencies. Note that when a strong new dependency crosses multiple weak dependencies, the weak dependenci"
P03-1066,W02-1032,1,0.880426,"Missing"
P03-1066,J01-2004,0,0.0219856,"ral factors that we believe to have the most significant impact on the performance of DLM. The discussion includes: (1) the use of DLM as a parser, (2) the definition of the mapping function Φ, and (3) the method of unsupervised dependency structure acquisition. One basic approach to using linguistic structure for language modeling is to extend the conventional language model P(W) to P(W, T), where T is a parse tree of W. The extended model can then be used as a parser to select the most likely parse by T* = argmaxT P(W, T). Many recent studies (e.g., Chelba and Jelinek, 2000; Charniak, 2001; Roark, 2001) adopt this approach. Similarly, dependency-based models (e.g., Collins, 1996; Chelba et al., 1997) use a dependency structure D of W instead of a parse tree T, where D is extracted from syntactic trees. Both of these models can be called grammar-based models, in that they capture the syntactic structure of a sentence, and the model parameters are estimated from syntactically annotated corpora such as the Penn Treebank. DLM, on the other hand, is a non-grammar-based model, because it is not based on any syntactic annotation: the dependency structure used in language modeling was learned direct"
P04-1059,J95-4004,0,0.0498692,"ment in this field shows that, problem is handled in this framework. We explore in addition to ambiguity resolution and unknown several features and describe how to create training word detection, the usefulness of a Chinese word data by sampling. We evaluate the performance of segmenter also depends crucially on its ability to our segmentation system using an annotated test set, adapt to different domains of texts and different where new words are simulated by sampling. We segmentation standards. then describe a transformation-based learning (TBL, The need of adaptation involves two research Brill, 1995) method that is used to adapt our system issues that we will address in this paper. The first is to different segmentation standards. We compare new word detection. Different domains/applications the adaptive system to other state-of-the-art systems may have different vocabularies which contain new using four test sets in the SIGHAN’s First Internawords/terms that are not available in a general tional Chinese Word Segmentation Bakeoff, each of dictionary. In this paper, new words refer to OOV which is constructed according to a different segwords other than named entities, factoids and mor- me"
P04-1059,P00-1073,1,0.894198,"ose words whose probability to appear in a new document is lower than general lexical words. Let Pi(k) be the probability of word wi that occurs k times in a document. In our experiments, we assume that P(NW|wi) can be approximated by the probability of wi occurring less than K times in a new document: K −1 P ( NW |wi ) ≈ ∑ Pi (k ) , (5) k =0 where the constant K is dependent on the size of the document: The larger the document, the larger the value. Pi(k) can be estimated using several term distribution models (see Chapter 15.3 in Manning and Schütze, 1999). Following the empirical study in (Gao and Lee, 2000), we use K-Mixture (Katz, 1996) which estimate Pi(k) as α β k ( ) , (6) β +1 β +1 where δk,0=1 if k=0, 0 otherwise. α and β are parameters that can be fit using the observed mean λ Pi (k ) = (1 − α )δ k , 0 + and the observed inverse document frequency IDF as follow: cf N λ = , IDF = log , N df λ cf − df , and α = , β = λ × 2 IDF − 1 = df β where cf is the total number of occurrence of word wi in training data, df is the number of documents in training data that wi occurs in, and N is the total number of documents. In our implementation, the training data contain approximately 40 thousand docu"
P04-1059,P03-1035,1,0.63764,"subsets of the AS training set of different sizes, and observed the same trend. However, even with a much smaller adaptation data set (e.g. 250K), we still outperform the best bakeoff results. 6 Related Work Many methods of Chinese word segmentation have been proposed (See Wu and Tseng, 1993; Sproat and Shih, 2001 for reviews). However, it is difficult to compare systems due to the fact that there is no widely accepted standard. There has been less work on dealing with NWI and standard adaptation. All feature functions in Figure 1, except the NW function, are derived from models presented in (Gao et al., 2003). The linear models are similar to what was presented in Collins and Duffy (2001). An alternative to linear models is the log-linear models suggested by Och (2003). See Collins (2002) for a comparison of these approaches. The features for NWI were studied in Wu & Jiang (2000) and Li et al. (2004). The use of sampling was proposed in Della Pietra et al. (1997) and Rosenfeld et al. (2001). There is also a related work on this line in Japanese (Uchimoto et al., 2001). A detailed discussion on differences among the four Bakeoff standards is presented in Wu (2003), which also proposes an adaptive s"
P04-1059,P03-1021,0,0.0866914,"fferent ways (e.g. name entity models are n-gram models trained on 2 corpora whereas factoid models use derivation rules and have binary values). The dynamic value ranges of different class models can be so different that it is improper to combine all models through simple multiplication as Equation (1). In this study we use linear models. The method is derived from linear discriminant functions widely used for pattern classification (Duda et al., 2001), and has been recently introduced into NLP tasks by Collins and Duffy (2001). It is also related to loglinear models for machine translation (Och, 2003). In this framework, we have a set of M+1 feature functions fi(S,W), i = 0,…,M. They are derived from the context model (i.e. f0(W)) and M class models, each for one word class, as shown in Figure 1: For probabilistic models such as the context model or person name model, the feature functions are defined as the negative logarithm of the corresponding probabilistic models. For each feature function, there is a model parameter λi. The best word segmentation W* is determined by the decision rule as M W * = arg max Score(λ0M , S ,W ) = arg max ∑ λi f i ( S ,W ) (2) W W i =0 Below we describe how"
P04-1059,P97-1041,0,0.593397,"Missing"
P04-1059,W03-1719,0,0.346254,"Missing"
P04-1059,W01-0512,0,0.170452,"(e.g. 蜂窝式 ‘cellular’) four test sets. It demonstrates the possibility of and time-sensitive political, social or cultural terms having a single adaptive Chinese word segmenter (e.g. 三通‘Three Links’, 非典 ‘SARS’). that is capable of supporting multiple user applicaThe second issue concerns the customizable tions. display of word segmentation. Different Chinese Abstract 1 This work was done while Hongqiao Li, Xinsong Xia and Haowei Qin were visiting Microsoft Research (MSR) Asia. We thank Xiaodan Zhu for his early contribution, and the three reviewers, one of whom alerted us the related work of (Uchimoto et al., 2001). Word Class2 Model Feature Functions, f(S,W) Context Model Word class based trigram, P(W). -log(P(W)) Lexical Word (LW) Morphological Word (MW) Named Entity (NE) ----Character/word bigram, P(S|NE). 1 if S forms a word lexicon entry, 0 otherwise. 1 if S forms a morph lexicon entry, 0 otherwise. -log(P(S|NE)) Factoid (FT) New Word (NW) ----- 1 if S can be parsed using a factoid grammar, 0 otherwise Score of SVM classifier Figure 1: Context model, word classes, and class models, and feature functions. 2 Chinese Word Segmentation with Linear Models Let S be a Chinese sentence which is a character"
P04-1059,W00-1207,1,0.900394,"res are chosen due to their effectiveness and availability for on-line detection. They are Independent Word Probability (IWP), Anti-Word Pair (AWP), and Word Formation Analogy (WFA). Below we describe each feature in turn. In Section 3.2, we shall describe the way the training data (new word list) for the classifier is created by sampling. IWP is a real valued feature. Most Chinese characters can be used either as independent words or component parts of multi-character words, or both. The IWP of a single character is the likelihood for this character to appear as an independent word in texts (Wu and Jiang, 2000): IWP ( x ) = C ( x, W ) . C( x) (4) where C(x, W) is the number of occurrences of the character x as an independent word in training data, and C(x) is the total number of x in training data. We assume that the IWP of a character string is the product of the IWPs of the component characters. Intuitively, the lower the IWP value, the more likely the character string forms a new word. In our implementation, the training data is word-segmented. AWP is a binary feature derived from IWP. For example, the value of AWP of an NW_11 candidate ab is defined as: AWP(ab)=1 if IWP(a)>θ or IWP(b) >θ, 0 othe"
P04-1059,O03-4001,1,0.882954,"ong@founder.com & Shanghai Jiaotong university, Shanghai. haoweiqin@sjtu.edu.cn NLP-enabled applications may have different requirements that call for different granularities of word segmentation. For example, speech recogniThis paper presents a Chinese word segmentation system which can adapt to different tion systems prefer “longer words” to achieve domains and standards. We first present a stahigher accuracy whereas information retrieval tistical framework where domain-specific systems prefer “shorter words” to obtain higher words are identified in a unified approach to recall rates, etc. (Wu, 2003). Given a word segword segmentation based on linear models. mentation specification (or standard) and/or some We explore several features and describe how application data used as training data, a segmenter to create training data by sampling. We then with customizable display should be able to provide describe a transformation-based learning alternative segmentation units according to the method used to adapt our system to different specification which is either pre-defined or implied word segmentation standards. Evaluation of in the data. the proposed system on five test sets with difIn this"
P04-1059,W03-1726,0,\N,Missing
P04-1059,W01-1802,0,\N,Missing
P06-1029,N04-4006,0,0.0304322,"the best approximation to the lasso solution, and leads to a significant improvement, in terms of character error rate, over boosting and the traditional maximum likelihood estimation. 1 Introduction Language modeling (LM) is fundamental to a wide range of applications. Recently, it has been shown that a linear model estimated using discriminative training methods, such as the boosting and perceptron algorithms, outperforms significantly a traditional word trigram model trained using maximum likelihood estimation (MLE) on several tasks such as speech recognition and Asian language text input (Bacchiani et al. 2004; Roark et al. 2004; Gao et al. 2005; Suzuki and Gao 2005). The success of discriminative training methods is largely due to fact that unlike the traditional approach (e.g., MLE) that maximizes the function (e.g., likelihood of training data) that is loosely associated with error rate, discriminative training methods aim to directly minimize the error rate on training data even if they reduce the likelihood. However, given a finite set of training samples, discriminative training methods could lead to an arbitrary complex model for the purpose of achieving zero training error. It is well-known"
P06-1029,J05-1003,0,0.5509,"om W and A in IME as words Our goal then is to search for the best parameter have unique readings, i.e. P(A|W) = 1. So the set λ which minimizes the sample risk, as in decision of Equation (1) depends solely upon Equation (4): P(W), making IME an ideal evaluation test bed def (4) λ MSR = arg min Er(WiR , Wi* ( Ai , λ )) . for LM. λ i =1... M In this study, the LM task for IME is formulated under the framework of linear models (e.g., However, (4) cannot be optimized easily since Duda et al. 2001). We use the following notation, Er(.) is a piecewise constant (or step) function of λ adapted from Collins and Koo (2005): and its gradient is undefined. Therefore, dis• Training data is a set of example in- criminative methods apply different approaches put/output pairs. In LM for IME, training sam- that optimize it approximately. The boosting ples are represented as {Ai, WiR}, for i = 1…M, algorithm described below is one of such apwhere each Ai is an input phonetic string and WiR proaches. is the reference transcript of Ai. • We assume some way of generating a set of 3 Boosting candidate word strings given A, denoted by This section gives a brief review of the boosting GEN(A). In our experiments, GEN(A) consi"
P06-1029,W02-1032,1,0.897166,"Missing"
P06-1029,H05-1027,1,0.879473,"Missing"
P06-1029,H05-1034,1,0.607207,"a significant improvement, in terms of character error rate, over boosting and the traditional maximum likelihood estimation. 1 Introduction Language modeling (LM) is fundamental to a wide range of applications. Recently, it has been shown that a linear model estimated using discriminative training methods, such as the boosting and perceptron algorithms, outperforms significantly a traditional word trigram model trained using maximum likelihood estimation (MLE) on several tasks such as speech recognition and Asian language text input (Bacchiani et al. 2004; Roark et al. 2004; Gao et al. 2005; Suzuki and Gao 2005). The success of discriminative training methods is largely due to fact that unlike the traditional approach (e.g., MLE) that maximizes the function (e.g., likelihood of training data) that is loosely associated with error rate, discriminative training methods aim to directly minimize the error rate on training data even if they reduce the likelihood. However, given a finite set of training samples, discriminative training methods could lead to an arbitrary complex model for the purpose of achieving zero training error. It is well-known that complex models exhibit high variance and perform poo"
P06-1029,I05-1083,1,0.880966,"Missing"
P06-1062,J00-1004,0,0.02177,"Missing"
P06-1062,P91-1022,0,0.541924,"Missing"
P06-1062,J93-2003,0,0.0132601,"des in T F and T E ; K i and K j are the degrees of N iF and is combined with T[iE+1, j ] to be aligned with N Ej . The time complexity of the decoding algoF ( F E ) ( F Pr T [ m ,n ] T [i , j ] , A = Pr T [ m ,n ] N iE .TC[1, K ]T [iE+1, j ] , rithm is O( |T |× |T |×(degree(T ) + degree(T )) ) , where the degree of a tree is defined as the largest degree of its nodes. ) F F A where K is the degree of N iE . Finally, the node translation probability is modeled as Pr(NlF NjE ) ≈ Pr(NlF .l NiE .l)Pr(NlF .t NiE .t) . And ( ) the text translation probability Pr t F t E is model using IBM model I (Brown et al 1993). 4.2 Parameter Estimation Using Expectation-Maximization Our tree alignment model involves three categories of parameters: the text translation probability Pr t F t E , tag mapping probability Pr l l ' , and ( ( ) ) node deletion probability pd . Conventional parallel data released by LDC are used to train IBM model I for estimating the text translation probability Pr t F t E . One way to estimate ( ) Pr (l l ) and ' 5 E 2 E Aligning Sentences Using Tree Alignment Model To exploit the HTML structure similarities between parallel web documents, a cascaded approach is used in our sentence align"
P06-1062,P05-1074,0,0.00854072,"Missing"
P06-1062,P93-1002,0,0.101428,"Missing"
P06-1062,P93-1001,0,0.136988,"Missing"
P06-1062,1994.amta-1.11,0,0.0989515,"Missing"
P06-1062,1999.mtsummit-1.79,0,0.716737,"Missing"
P06-1062,P03-1058,0,0.0247213,"Missing"
P06-1062,P03-1010,0,0.107258,"Missing"
P06-1062,P94-1012,0,0.100897,"Missing"
P06-1062,J97-3002,0,0.0913856,"Missing"
P06-1062,P01-1067,0,0.0614498,"Missing"
P06-1062,moore-2002-fast,0,0.0195621,"Missing"
P06-1062,N04-1034,0,0.0584508,"Missing"
P06-1062,J03-3002,0,0.67772,"parallel sentences. 6 pd is to manually align nodes between parallel DOM trees, and use them as training corpora for maximum likelihood estimation. However, this is a very time-consuming and error-prone procedure. In this paper, the inside outside algorithm presented in (Lari and Young, 1990) is extended 1, K i pute the best alignment between N iF and N Ej . F T[m ,n ] , then Dynamic Programming for Decoding Web Document Model Pair Verification To verify whether a candidate web document pair is truly parallel, a binary maximum entropy based classifier is used. Following (Nie et al 1999) and (Resnik and Smith, 2003), three features are used: (i) file length ratio; (ii) HTML tag similarity; (iii) sentence alignment score. 493 The HTML tag similarity feature is computed as follows: all of the HTML tags of a given web page are extracted, and concatenated as a string. Then, a minimum edit distance between the two tag strings associated with the candidate pair is computed, and the HMTL tag similarity score is defined as the ratio of match operation number to the total operation number. The sentence alignment score is defined as the ratio of the number of aligned sentences and the total number of sentences in"
P06-1062,W90-0102,0,\N,Missing
P06-1062,C90-3045,0,\N,Missing
P06-1062,P91-1023,0,\N,Missing
P06-1062,J93-1006,0,\N,Missing
P07-1104,W06-1655,1,0.814278,"Missing"
P07-1104,A00-2018,0,0.0713209,"ion (1) is used to discriminatively re-rank the candidate list using additional features which may or may not be included in the baseline model. Since 828 We follow the experimental paradigm of parse re-ranking outlined in Charniak and Johnson (2005), and fed the features extracted by their program to the five rerankers we developed. Each uses a linear model trained using one of the five estimators. These rerankers attempt to select the best parse ? for a sentence ? from the 50-best list of possible parses ??? ? for the sentence. The linear model combines the log probability calculated by the Charniak (2000) parser as a feature with 1,219,272 additional features. We trained the feaBaseline ME/L2 ME/L1 AP Boosting BLasso F-Score 0.8986 0.9176 0.9165 0.9164 0.9131 0.9133 # features time (min) # train iter 1,211,026 19,121 939,248 6,714 8,085 62 37 2 495 239 129 174 8 92,600 56,500 Table 1: Performance summary of estimators on parsing re-ranking (ME/L2: ME with L2 regularization; ME/L1: ME with L1 regularization) ME/L2 ME/L2 ME/L1 AP Boost Blasso &lt;&lt; ~ &lt;&lt; &lt;&lt; ME/L1 &gt;&gt; ~ &lt; ~ AP ~ ~ &lt;&lt; &lt; Boost &gt;&gt; &gt; &gt;&gt; BLasso &gt;&gt; ~ &gt; ~ ~ Table 2: Statistical significance test results (“&gt;&gt;” or “&lt;&lt;” means P-value &lt; 0.01; &gt;"
P07-1104,P05-1022,1,0.17373,"6), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularizat"
P07-1104,W02-1001,0,0.121044,"ures are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior ("
P07-1104,P06-1029,1,0.803398,"ely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boosting in the LM adaptation task. Ng (2004) showed that for logistic regression, L1 regularization outperforms L2 regularization on artificial datasets which contain many completely irrelevant features. Goodman (2003) showed that in two out of three tasks, an ME estimator with a one-sided Laplacian prior (i.e., L1 regularization with the constraint that all feature weights are positive) outperformed a comparable estimator using a Gaussian prior (i.e., L2 regularization). Riezler and Vasserman (2004) showed that a"
P07-1104,N04-1039,0,0.300415,"Missing"
P07-1104,P99-1069,1,0.742056,"n into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low computational cost in training (e.g., Collins 2002). While recent studies claim advantages for L1 regularization, this study is the first of which we are aware to systematically compare it to a range of estimators on a diverse set of NLP tasks. Gao et al. (2006) showed that BLasso, due to its explicit use of L1 regularization, outperformed Boos"
P07-1104,W03-1018,0,0.505913,"ith L1 regularization This estimator also minimizes the negative conditional log-likelihood, but uses an L1 (or Lasso) penalty. That is, ?(?) in Equation (2) is defined according to ? ? = ? ? ?? . L1 regularization typically leads to sparse solutions in which many feature weights are exactly zero, so it is a natural candidate when feature selection is desirable. By contrast, L2 regularization produces solutions in which most weights are small but non-zero. Optimizing the L1-regularized objective function is challenging because its gradient is discontinuous whenever some parameter equals zero. Kazama and Tsujii (2003) described an estimation method that constructs an equivalent constrained optimization problem with twice the number of variables. However, we found that this method is impractically slow for large-scale NLP tasks. In this work we use the orthant-wise limited-memory quasi-Newton algorithm (OWL-QN), which is a modification of L-BFGS that allows it to effectively handle the discontinuity of the gradient (Andrew and Gao 2007). We provide here a high-level description of the algorithm. A quasi-Newton method such as L-BFGS uses first order information at each iterate to build an approximation to th"
P07-1104,W02-2018,0,0.215148,"the parameters. Here,  is a parameter that controls the amount of regularization, optimized on held-out data. This is one of the most popular estimators, largely due to its appealing computational properties: both ? ? and ?(?) are convex and differentiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently. In our experiments, we used the limited memory quasi-Newton algorithm (or L-BFGS, Nocedal and Wright 1999) to find the optimal ? because this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002). Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in Riezler et al. (2002), where ? ? is defined as the likelihood of the best parses ? ∈ ?(?) relative to the n-best parser output ??? ? , (i.e., ? ? ⊑ ???(?)): ? ? = − ??=1 log ? ? ∈?(? ? ) ?(?? |?? ). We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on both tasks it leads to a significant improvement in performance for the L2-regularied ME estimator but not for the L1-regularied ME estimator. 2.2 ME esti"
P07-1104,W04-3223,0,0.0632296,"ver a large number of weakly informative features. The first intuition motivates feature selection methods such as Boosting and BLasso (e.g., Collins 2000; Zhao and Yu, 2004), which usually work best when many features are completely irrelevant. L1 or Lasso regularization of linear models, introduced by Tibshirani (1996), embeds feature selection into regularization so that both an assessment of the reliability of a feature and the decision about whether to remove it are done in the same framework, and has generated a large amount of interest in the NLP community recently (e.g., Goodman 2003; Riezler and Vasserman 2004). If on the other hand most features are noisy but at least weakly correlated with the target, it may be reasonable to attempt to reduce noise by averaging over all of the features. ME estimators with L2 regularization, which have been widely used in NLP tasks (e.g., Chen and Rosenfeld 2000; Charniak and Johnson 2005; Johnson et al. 1999), tend to produce models that have this property. In addition, the perceptron algorithm and its variants, e.g., the voted or averaged perceptron, is becoming increasingly popular due to their competitive performance, simplicity in implementation and low comput"
P07-1104,P02-1035,1,0.821172,"gely due to its appealing computational properties: both ? ? and ?(?) are convex and differentiable, so gradient-based numerical algorithms can be used to find the global minimum efficiently. In our experiments, we used the limited memory quasi-Newton algorithm (or L-BFGS, Nocedal and Wright 1999) to find the optimal ? because this method has been shown to be substantially faster than other methods such as Generalized Iterative Scaling (Malouf 2002). Because for some sentences there are multiple best parses (i.e., parses with the same F-Score), we used the variant of ME estimator described in Riezler et al. (2002), where ? ? is defined as the likelihood of the best parses ? ∈ ?(?) relative to the n-best parser output ??? ? , (i.e., ? ? ⊑ ???(?)): ? ? = − ??=1 log ? ? ∈?(? ? ) ?(?? |?? ). We applied this variant in our experiments of parse re-ranking and LM adaptation, and found that on both tasks it leads to a significant improvement in performance for the L2-regularied ME estimator but not for the L1-regularied ME estimator. 2.2 ME estimation with L1 regularization This estimator also minimizes the negative conditional log-likelihood, but uses an L1 (or Lasso) penalty. That is, ?(?) in Equation (2) is"
P07-1104,N03-1033,1,0.14229,"ontext are ME models. Following previous work (Ratnaparkhi, 1996), we assume that the tag of a word is independent of the tags of all preceding words given the tags of the previous two words (i.e., ?=2 in the equation above). The local models at each position include features of the current word, the previous word, the next word, and features of the previous two tags. In addition to lexical identity of the words, we used features of word suffixes, capitalization, and number/special character signatures of the words. We used the standard splits of the Penn Treebank from the tagging literature (Toutanova et al. 2003) for training, development and test sets. The training set comprises Sections 0-18, the development set — Sections 19-21, and the test set — Sections 22-24. We compared training the ME models using L1 and L2 regularization. For each of the two types of regularization we selected the best value of the regularization constant using grid search to optimize the accuracy on the development set. We report final accuracy measures on the test set in Table 6. The results on this task confirm the trends we have seen so far. There is almost no difference in 3 Only the L2 vs. AP comparison is significant"
P07-1104,J05-1003,0,\N,Missing
P10-1028,P06-1129,0,0.255299,"r a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the da"
P10-1028,J04-4002,0,0.0557688,"using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformation probabilities. In this study we argue that it is beneficial to capture contextual information in the error model. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrase-based error model where we assume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for W"
P10-1028,H05-1120,0,0.426726,"n text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to"
P10-1028,D08-1047,0,0.0208292,"word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as ""peace"" and ""piece"" in the context ""a _ of cake"". A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular"
P10-1028,P00-1037,0,0.920791,"or the development of a query speller system. In this paper we present a novel method of extracting large amounts of query-correction pairs from the clickthrough data. These pairs, implicitly judged by millions of users, are used to train a set of spelling error models. Among these models, the most effective one is a phrase-based error model that captures the probability of transforming one multi-term phrase into another multi-term phrase. Comparing to traditional error models that account for transformation probabilities between single characters (Kernighan et al., 1990) or sub-word strings (Brill and Moore, 2000), the phrase-based model is more powerful in that it captures some contextual information by retaining inter-term dependencies. We show that this information is crucial to detect the correction of a query term, because unlike in regular written text, any query word can be a valid search term and in many cases the only way for a speller system to make the judgment is to explore its usage according to the contextual information. We conduct a set of experiments on a large data set, consisting of human-labeled The work was done when Xu Sun was visiting Microsoft Research Redmond. 266 Proceedings o"
P10-1028,D07-1019,0,0.747082,"ry, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models fr"
P10-1028,D07-1021,1,0.837998,"2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as ""peace"" and ""piece"" in the context ""a _ of cake"". A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correct"
P10-1028,W04-3238,0,0.73309,"r context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When designed to handle regular written text, both CSSC and non-word error speller systems rely on a pre-defined vocabulary (i.e., either a lexicon or a confusion set). However, in query spelling correction, it is impossible to compile such a vocabulary, and the boundary between the non-word and real-word errors is quite vague. Therefore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain muc"
P10-1028,D09-1154,1,0.767258,"evel. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and proved to be useful for Web search (Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009) and query reformulation (Wang and Zhai, 2008; Suzuki et al., 2009). We start with this same data with the hope of achieving similar improvements in our task. The data consist of a set of query sessions that were extracted from one year of log files from a commercial Web search engine. A query session contains a query issued by a user and a ranked list of links (i.e., URLs) returned to that same user along with records of which URLs were clicked. Following Suzuki et al. (2009), we extract query-correction pairs as follows. First, we extract pairs of queries Q1 and Q2 such that (1) they are issued by the same user; (2) Q2 was issued within 3 minutes of Q1; and"
P10-1028,P02-1019,0,0.516968,"r spelling correction, any word that is not found in a pre-compiled lexicon is considered to be misspelled. Then, a list of lexical words that are similar to the misspelled word are proposed as candidate spelling corrections. Most traditional systems use a manually tuned similarity function (e.g., edit distance function) to rank the candidates, as reviewed by Kukich (1992). During the last two decades, statistical error models learned on training data (i.e., query-correction pairs) have become increasingly popular, and have proven more effective (Kernighan et al., 1990; Brill and Moore, 2000; Toutanova and Moore, 2002; Okazaki et al., 2008). Real-word spelling correction is also referred to as context sensitive spelling correction (CSSC). It tries to detect incorrect usages of a valid word based on its context, such as ""peace"" and ""piece"" in the context ""a _ of cake"". A common strategy in CSSC is as follows. First, a pre-defined confusion set is used to generate candidate corrections, then a scoring model, such as a trigram language model or naïve Bayes classifier, is used to rank the candidates according to their context (e.g., Golding and Roth, 1996; Mangu and Brill, 1997; Church et al., 2007). When desi"
P10-1028,D09-1093,0,0.0826042,"fore, recent research on query spelling correction has focused on exploiting noisy Web data and query logs to infer knowledge about misspellings and word usage in search queries. Cucerzan and Brill (2004) discuss in detail the challenges of query spelling correction, and suggest the use of query logs. Ahmad and Kondrak (2005) propose a method of estimating an error model from query logs using the EM algorithm. Li et al. (2006) extend the error model by capturing word-level similarities learned from query logs. Chen et al. (2007) suggest using web search results to improve spelling correction. Whitelaw et al. (2009) present a query speller system in which both the error model and the language model are trained using Web data. Compared to Web corpora and query logs, clickthrough data contain much richer information about users’ search behavior. Although there has been a lot of research on using clickthrough data to improve Web document retrieval (e.g., Joachims, 2002; Agichtein et al., 2006; Gao et al., 2009), the data have not been fully explored for query spelling correction. This study tries to learn error models from clickthrough data. To our knowledge, this is the first such attempt using clickthroug"
P10-1028,C90-2036,0,0.296919,"been little research on exploiting the data for the development of a query speller system. In this paper we present a novel method of extracting large amounts of query-correction pairs from the clickthrough data. These pairs, implicitly judged by millions of users, are used to train a set of spelling error models. Among these models, the most effective one is a phrase-based error model that captures the probability of transforming one multi-term phrase into another multi-term phrase. Comparing to traditional error models that account for transformation probabilities between single characters (Kernighan et al., 1990) or sub-word strings (Brill and Moore, 2000), the phrase-based model is more powerful in that it captures some contextual information by retaining inter-term dependencies. We show that this information is crucial to detect the correction of a query term, because unlike in regular written text, any query word can be a valid search term and in many cases the only way for a speller system to make the judgment is to explore its usage according to the contextual information. We conduct a set of experiments on a large data set, consisting of human-labeled The work was done when Xu Sun was visiting M"
P10-1028,N03-1017,0,0.165419,"first such attempt using clickthrough data. Most of the speller systems reviewed above are based on the framework of the source channel model. Typically, a language model (source model) is used to capture contextual information, while an error model (channel model) is considered to be context free in that it does not take into account any contextual information in modeling word transformation probabilities. In this study we argue that it is beneficial to capture contextual information in the error model. To this end, inspired by the phrase-based statistical machine translation (SMT) systems (Koehn et al., 2003; Och and Ney, 2004), we propose a phrase-based error model where we assume that query spelling correction is performed at the phrase level. In what follows, before presenting the phrasebased error model, we will first describe the clickthrough data and the query speller system we used in this study. 3 Clickthrough Data and Spelling Correction This section describes the way the query-correction pairs are extracted from click267 through data. Two types of clickthrough data are explored in our experiment. The clickthrough data of the first type has been widely used in previous research and prove"
P14-1066,D13-1176,0,0.177152,"lation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 20"
P14-1066,N03-1017,0,0.177482,"continuous representation of a phrase. Finally, the translation score of a 1 Introduction source-target phrase pair is computed by the disThe phrase translation model, also known as the tance between their feature vectors. The main motivation behind the CPTM is to phrase table, is one of the core components of alleviate the data sparseness problem associated phrase-based statistical machine translation (SMT) with the traditional counting-based methods by systems. The most common method of constructing the phrase table takes a two-phase approach grouping phrases with a similar meaning across (Koehn et al. 2003). First, the bilingual phrase different languages. This style of grouping is pairs are extracted heuristically from an automat- made possible because of the distributed nature of ically word-aligned training data. The second the continuous-space representations for phrases. phase, which is the focus of this paper, is parame- No such sharing was possible in the original symter estimation where each phrase pair is assigned bolic space for representing words or phrases. In with some scores that are estimated based on this model, semantically or grammatically related counting these phrases or thei"
P14-1066,2005.mtsummit-posters.11,0,0.0297066,"Missing"
P14-1066,P06-1096,0,0.0100054,"ns a real-valued weight to each feature. The components GEN(. ), ? and ? define a loglinear model that maps ?? to an output sentence as follows: ? ∗ = argmax ?T ?(?? , ?, ?) (1) (?,?)∈GEN(?? ) which states that given ? and ?, argmax returns the highest scoring translation ? ∗ , maximizing over correspondences ?. In phrase-based SMT, ? consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since computing the argmax exactly is intractable, it is commonly performed approximatedly by beam search (Och and Ney 2004). Following Liang et al. (2006), we assume that every translation candidate is always coupled with a corresponding ?, called the Viterbi derivation, generated by (1). 4 A Continuous-Space Phrase Translation Model (CPTM) The architecture of the CPTM is shown in Figures 1 and 2, where for each pair of source and target phrases (?? , ?? ) in a source-target sentence pair, we first project them into feature vectors ??? and ??? in a latent, continuous space via a neural network with one hidden layer (as shown in Figure 2), and then compute the translation score, score(?? , ?? ), by the distance of their feature vectors in that s"
P14-1066,N13-1048,1,0.931372,"proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input ? ∈ ℱ to output ? ∈ ℰ. We are given  Training samples (?? , ?? ) for ? = 1 … ?, where eac"
P14-1066,P12-1031,1,0.819542,"ured in selecting translations. However, longer phrases occur less often in trainThis paper tackles the sparsity problem in ing data, leading to a severe data sparseness probestimating phrase translation probabilities lem in parameter estimation. There has been a by learning continuous phrase representaplethora of research reported in the literature on tions, whose distributed nature enables the improving parameter estimation for the phrase sharing of related phrases in their representranslation model (e.g., DeNero et al. 2006; tations. A pair of source and target phrases Wuebker et al. 2010; He and Deng 2012; Gao and are projected into continuous-valued vecHe 2013). tor representations in a low-dimensional This paper revisits the problem of scoring a latent space, where their translation score phrase translation pair by developing a Continuis computed by the distance between the ous-space Phrase Translation Model (CPTM). pair in this new space. The projection is The translation score of a phrase pair in this model performed by a neural network whose is computed as follows. First, we represent each weights are learned on parallel training phrase as a bag-of-words vector, called word vecdata. Exper"
P14-1066,P07-2045,0,0.0641989,"us representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires learning a mapping between input ? ∈ ℱ to output ? ∈ ℰ. We are given  Training samples (?? , ?? ) for ? = 1 … ?, where each source sentence ?? is paired with a reference translation in target language ?? ;  A procedure GEN to generate a list of N-best candidates GEN(?? ) for an input ?? , where GEN in this study is the baseline phrasebased SMT system, i.e., an in-house implementation of the Moses system (Koehn et al. 2007) that does not use the CPTM, and each ? ∈ GEN(?? ) is labeled by the sentence-level BLEU score (He and Deng 2012), denoted by sBleu(?? , ?) , which measures the quality of ? with respect to its reference translation ?? ;  A vector of features ? ∈ ℝ? that maps each (?? , ?) to a vector of feature values2; and  A parameter vector ? ∈ ℝ? , which assigns a real-valued weight to each feature. The components GEN(. ), ? and ? define a loglinear model that maps ?? to an output sentence as follows: ? ∗ = argmax ?T ?(?? , ?, ?) (1) (?,?)∈GEN(?? ) which states that given ? and ?, argmax returns the hig"
P14-1066,W06-3114,0,0.0110009,"es of source given target phrase mappings ???? (?|?) and vice versa ???? (?|?), as well as lexical weighting estimates ??? (?|?) and ??? (?|?), word and phrase penalties, a linear distortion feature, and a lexicalized reordering feature. The baseline includes a standard 5-gram modified Kneser-Ney language model trained on the target side of the parallel corpora described below. Log-linear weights are estimated with the MERT algorithm (Och 2003). 704 Evaluation. We test our models on two different data sets. First, we train an English to French system based on the data of WMT 2006 shared task (Koehn and Monz 2006). The parallel corpus includes 688K sentence pairs of parliamentary proceedings for training. The development set contains 2000 sentences, and the test set contains other 2000 sentences, all from the official WMT 2006 shared task. Second, we experiment with a French to English system developed using 2.1M sentence pairs of training data, which amounts to 102M words, from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sentences, as the development set. We evaluate"
P14-1066,D13-1054,0,0.0498113,"00 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier"
P14-1066,W02-1018,0,0.0501549,"phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the strengths associated with these earlier studies. 3 The Log-Linear Model for SMT Phrase-based SMT is based on a log-linear model which requires le"
P14-1066,N13-1090,1,0.0323834,"e translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its translation in the target language. The recurrent continuous translation models proposed by Kalchbrenner and Blunsom (2013) also adopt the recurrent language model (Mikolov et al. 2010). But unlike the n-gram translation models above, they make no Markov assumptions about the dependency of the words in the target sentence. Continuous space models have also been used for generating translations for new words (Mikolov et al. 2013a) and ITG reordering (Li et al. 2013). There has been a lot of research on improving the phrase table in phrase-based SMT (Marcu and Wong 2002; Lamber and Banchs 2005; Denero et al. 2006; Wuebker et al. 2010; Zhang et al., 2011; He and Deng 2012; Gao and He 2013). Among them, (Gao and He 2013) is most relevant to the work described in this paper. They estimate phrase translation probabilities using a discriminative training method under the N-best reranking framework of SMT. In this study we use the same objective function to learn the continuous representations of phrases, integrating the st"
P14-1066,D09-1092,0,0.0576492,"Missing"
P14-1066,W11-2124,0,0.0188708,"similarity between a source phrase and its paired target phrase by projecting them into a common, continuous space that is language independent. The rest of the paper is organized as follows. Section 2 reviews previous work. Section 3 reviews the log-linear model for phrase-based SMT and Sections 4 presents the CPTM. Section 5 describes the way the model parameters are estimated, followed by the experimental results in Section 6. Finally, Section 7 concludes the paper. 1 version of such a model can be trained efficiently because the factor models used by Son et al. cannot be applied directly. Niehues et al. (2011) use different translation units in order to integrate the n-gram translation model into the phrasebased approach. However, it is not clear how a continuous 2 Related Work Representations of words or documents as continuous vectors have a long history. Most of the earlier latent semantic models for learning such vectors are designed for information retrieval (Deerwester et al. 1990; Hofmann 1999; Blei et al. 2003). In contrast, recent work on continuous space language models, which estimate the probability of a word sequence in a continuous space (Bengio et al. 2003; Mikolov et al. 2010), have"
P14-1066,P03-1021,0,0.658932,"CPTM is incorporated, is parameterized by (?, ?), where ? is a vector of a handful of parameters used in the log-linear model of (1), with one weight for each feature; and ? is the projection matrices used in the CPTM defined by (2) and (3). In our experiments we take three steps to learn (?, ?): 1. We use a baseline phrase-based SMT system to generate for each source sentence in training data an N-best list of translation hypotheses4. 2. We set ? to that of the baseline system and let ??+1 = 1, and optimize ? w.r.t. a loss function on training data5. 3. We fix ? , and optimize ? using MERT (Och 2003) to maximize BLEU on dev data. The translation score of a source phrase f and a target phrase e can be measured as the similarity (or distance) between their feature vectors. We choose the dot product as the similarity function3: score(?, ?) ≡ sim? (?? , ?? ) = ??T ?? (3) According to (2), we see that the value of the scoring function is determined by the projection matrices ? = {?1 , ?2 }. The CPTM of (2) and (3) can be incorporated into the log-linear model for SMT (1) by 3 In our experiments, we compare dot product and the cosine similarity functions and find that the former works better fo"
P14-1066,J04-4002,0,0.135511,"r vector ? ∈ ℝ? , which assigns a real-valued weight to each feature. The components GEN(. ), ? and ? define a loglinear model that maps ?? to an output sentence as follows: ? ∗ = argmax ?T ?(?? , ?, ?) (1) (?,?)∈GEN(?? ) which states that given ? and ?, argmax returns the highest scoring translation ? ∗ , maximizing over correspondences ?. In phrase-based SMT, ? consists of a segmentation of the source and target sentences into phrases and an alignment between source and target phrases. Since computing the argmax exactly is intractable, it is commonly performed approximatedly by beam search (Och and Ney 2004). Following Liang et al. (2006), we assume that every translation candidate is always coupled with a corresponding ?, called the Viterbi derivation, generated by (1). 4 A Continuous-Space Phrase Translation Model (CPTM) The architecture of the CPTM is shown in Figures 1 and 2, where for each pair of source and target phrases (?? , ?? ) in a source-target sentence pair, we first project them into feature vectors ??? and ??? in a latent, continuous space via a neural network with one hidden layer (as shown in Figure 2), and then compute the translation score, score(?? , ?? ), by the distance of"
P14-1066,P02-1040,0,0.0988536,"from the WMT 2012 campaign. The majority of the training data set is parliamentary proceedings except for 5M words which are newswire. We use the 2009 newswire data set, comprising 2525 sentences, as the development set. We evaluate on four newswire domain test sets from 2008, 2010 and 2011 as well as the 2010 system combination test set, containing 2034 to 3003 sentences. In this study we perform a detailed empirical comparison using the WMT 2006 data set, and verify our best models and results using the larger WMT 2012 data set. The metric used for evaluation is case insensitive BLEU score (Papineni et al. 2002). We also perform a significance test using the Wilcoxon signed rank test. Differences are considered statistically significant when the p-value is less than 0.05. 6.2 Results of the CPTM Table 1 shows the results measured in BLEU evaluated on the WMT 2006 data set, where Row 1 is the baseline system. Rows 2 to 4 are the systems enhanced by integrating different versions of the CPTM. Rows 5 to 7 present the results of previous models. Row 8 is our best system. Table 2 shows the main results on the WMT 2012 data set. CPTM is the model described in Sections 4. As illustrated in Figure 2, the num"
P14-1066,D10-1025,1,0.710259,"(Bengio et al. 2003; Mikolov et al. 2010), have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural net"
P14-1066,P10-1049,0,0.0665678,"formation can be captured in selecting translations. However, longer phrases occur less often in trainThis paper tackles the sparsity problem in ing data, leading to a severe data sparseness probestimating phrase translation probabilities lem in parameter estimation. There has been a by learning continuous phrase representaplethora of research reported in the literature on tions, whose distributed nature enables the improving parameter estimation for the phrase sharing of related phrases in their representranslation model (e.g., DeNero et al. 2006; tations. A pair of source and target phrases Wuebker et al. 2010; He and Deng 2012; Gao and are projected into continuous-valued vecHe 2013). tor representations in a low-dimensional This paper revisits the problem of scoring a latent space, where their translation score phrase translation pair by developing a Continuis computed by the distance between the ous-space Phrase Translation Model (CPTM). pair in this new space. The projection is The translation score of a phrase pair in this model performed by a neural network whose is computed as follows. First, we represent each weights are learned on parallel training phrase as a bag-of-words vector, called w"
P14-1066,W11-0329,1,0.895525,"have advanced the state of the art in language modeling, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to repres"
P14-1066,W11-2119,0,0.0732387,"Missing"
P14-1066,D07-1045,0,0.0166222,"e projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source"
P14-1066,C12-2104,0,0.199051,"shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as w"
P14-1066,W12-2702,0,0.016591,"gle words, are smooth function of these feature vectors, a small Abstract 699 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 699–709, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics change in the features should only lead to a small change in the translation score. The primary research task in developing the CPTM is learning the continuous representation of a phrase that is effective for SMT. Motivated by recent studies on continuous-space language models (e.g., Bengio et al. 2003; Mikolov et al. 2011; Schwenk et al., 2012), we use a neural network to project a word vector to a feature vector. Ideally, the projection would discover those latent features that are useful to differentiate good translations from bad ones, for a given source phrase. However, there is no training data with explicit annotation on the quality of phrase translations. The phrase translation pairs are hidden in the parallel source-target sentence pairs, which are used to train the traditional translation models. The quality of a phrase translation can only be judged implicitly through the translation quality of the sentences, as measured b"
P14-1066,N13-1120,1,0.781116,"Missing"
P14-1066,D13-1141,0,0.0690135,"ing, outperforming the traditional n-gram model on speech recognition (Mikolov et al. 2012; Sundermeyer et al. 2013) and machine translation (Mikolov 2012; Auli et al. 2013). Because these models are developed for monolingual settings, word embedding from these models is not directly applicable to translation. As a result, variants of such models for cross-lingual scenarios have been proposed so that words in different languages are projected into the shared latent vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier"
P14-1066,N13-1034,0,0.0171881,"rity of the phrases in the project space. As we see from Table 1, both latent semantic models, although leading to some slight improvement over Baseline, are much less effective than CPTM. Finally, we compare the CPTM with the Markov Random Field model using phrase features (MRFP in Tables 1 and 2), proposed by Gao and He (2013)7, on both the WMT 2006 and WMT 2012 datasets. MRFp is a state-of-the-art large scale discriminative training model that uses the same expected BLEU training criterion, which has proven to give superior performance across a range of MT tasks recently (He and Deng 2012, Setiawan and Zhou 2013, Gao and He 2013). Unlike CPTM, MRFp is a linear model that simply treats each phrase pair as a single feature. Therefore, although both are trained using the 7 Gao and He (2013) reported results of MRF models with different feature sets. We picked the MRF using phrase features only (MRFP) for comparison since we are mainly interested in phrase representation. 706 same expected BLEU based objective function, CPTM and MRFp model the translation relationship between two phrases from different angles. MRFp estimates one translation score for each phrase pair explicitly without parameter sharing,"
P14-1066,D12-1110,0,0.0585792,"Missing"
P14-1066,N12-1005,0,0.34284,"vector space (Dumais et al. 1997; Platt et al. 2010; Vinokourov et al. 2002; Yih et al. 2011; Gao et al. 2011; Huang et al. 2013; Zou et al. 2013). In principle, a phrase table can be derived using any of these cross-lingual models, although decoupling the derivation from the SMT training often results in suboptimal performance (e.g., measured in BLEU), as we will show in Section 6. Recently, there is growing interest in applying continuous-space models for translation. The most related to this study is the work of continuous space n-gram translation models (Schwenk et al. 2007; Schwenk 2012; Son et al. 2012), where the feed-forward neural network language model is extended to represent translation probabilities. However, these earlier studies focused on the ngram translation models, where the translation probability of a phrase or a sentence is decomposed as a product of n-gram probabilities as in a standard n-gram language model. Therefore, it is not clear how their approaches can be applied to the phrase translation model1, which is much more 700 widely used in modern SMT systems. In contrast, our model learns jointly the representations of a phrase in the source language as well as its transla"
P14-1066,W06-3105,0,\N,Missing
P14-1066,D13-1106,0,\N,Missing
P14-2023,P07-2045,0,0.00409186,"th language pairs. We tuned the learning rate µ of our mini-batch SGD trainer as well as the probability scaling parameter γ (3) on a held-out set and found simple settings of µ = 0.1 and γ = 1 to be good choices. To prevent over-fitting, we experimented with L2 regularization, but found no accuracy improvements, probably because SGD regularizes enough. We evaluate performance on a held-out set during training and stop whenever the objective changes less than 0.0003. The hidden layer uses 100 neurons unless otherwise stated. Experiments Baseline. We use a phrase-based system similar to Moses (Koehn et al., 2007) based on a set of common features including maximum likelihood estimates pM L (e|f ) and pM L (f |e), lexically weighted estimates pLW (e|f ) and pLW (f |e), word and phrase-penalties, a hierarchical reordering model (Galley and Manning, 2008), a linear distortion feature, and a modified Kneser-Ney language model trained on the target-side of the parallel data. Log-linear weights are tuned with MERT. Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English and German-English. Translation models are estimated on 102M words of parallel data for F"
P14-2023,N12-1005,0,0.350927,"er performance (Goodman, 1996; Och, 2003; Auli and Lopez, 2011). The expected BLEU objective provides an efficient way of achieving this for machine translation (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (§3). Most previous work on neural networks for machine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of direct decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feedforward network-based language models are more accurate in first-pass decoding than in rescoring. Decoder integration has the advantage for the neural network to directly influence search, unlike rescoring which is restricted to an n-best list or lattice. Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar to n-gram language models. Howeve"
P14-2023,W12-2703,0,0.0503028,"-specific metric often leads to better performance (Goodman, 1996; Och, 2003; Auli and Lopez, 2011). The expected BLEU objective provides an efficient way of achieving this for machine translation (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (§3). Most previous work on neural networks for machine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of direct decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feedforward network-based language models are more accurate in first-pass decoding than in rescoring. Decoder integration has the advantage for the neural network to directly influence search, unlike rescoring which is restricted to an n-best list or lattice. Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar t"
P14-2023,D11-1031,1,0.919574,"Missing"
P14-2023,D13-1106,1,0.82945,"odman, 1996; Och, 2003; Auli and Lopez, 2011). The expected BLEU objective provides an efficient way of achieving this for machine translation (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (§3). Most previous work on neural networks for machine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of direct decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feedforward network-based language models are more accurate in first-pass decoding than in rescoring. Decoder integration has the advantage for the neural network to directly influence search, unlike rescoring which is restricted to an n-best list or lattice. Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar to n-gram language models. However, for recurrent netw"
P14-2023,P06-1096,0,0.149188,"Missing"
P14-2023,D08-1089,0,0.0559234,"we experimented with L2 regularization, but found no accuracy improvements, probably because SGD regularizes enough. We evaluate performance on a held-out set during training and stop whenever the objective changes less than 0.0003. The hidden layer uses 100 neurons unless otherwise stated. Experiments Baseline. We use a phrase-based system similar to Moses (Koehn et al., 2007) based on a set of common features including maximum likelihood estimates pM L (e|f ) and pM L (f |e), lexically weighted estimates pLW (e|f ) and pLW (f |e), word and phrase-penalties, a hierarchical reordering model (Galley and Manning, 2008), a linear distortion feature, and a modified Kneser-Ney language model trained on the target-side of the parallel data. Log-linear weights are tuned with MERT. Evaluation. We use training and test data from the WMT 2012 campaign and report results on French-English and German-English. Translation models are estimated on 102M words of parallel data for French-English, and 99M words for German-English; about 6.5M words for each language pair are newswire, the remainder are parliamentary proceedings. We evaluate on six newswire domain test sets from 2008 to 2013 containing between 2034 to 3003 s"
P14-2023,N13-1048,1,0.795058,"Missing"
P14-2023,C12-1121,0,0.0345476,"Missing"
P14-2023,P14-1066,1,0.417006,"r Integration and Expected BLEU Training for Recurrent Neural Network Language Models Michael Auli Microsoft Research Redmond, WA, USA michael.auli@microsoft.com Abstract et al., 2012) or more recently, noise-contrastive estimation (Vaswani et al., 2013). However, it is widely appreciated that directly optimizing for a task-specific metric often leads to better performance (Goodman, 1996; Och, 2003; Auli and Lopez, 2011). The expected BLEU objective provides an efficient way of achieving this for machine translation (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (§3). Most previous work on neural networks for machine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of direct decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feedforward network-based language models are more acc"
P14-2023,P03-1021,0,0.521333,"for Computational Linguistics wt time algorithm, which unrolls the network and then computes error gradients over multiple time steps (Rumelhart et al., 1986); we use the expected BLEU loss (§3) to obtain the error with respect to the output activations. After training, the output layer represents scores s(wt+1 |w1 . . . wt , ht ) for the next word given the previous t input words and the current hidden layer configuration ht . yt 0 ht 0 1 0 U V 0 0 ht-1 W 3 We integrate the recurrent neural network language model as an additional feature into the standard log-linear framework of translation (Och, 2003). Formally, our phrase-based model is parameterized by M parameters Λ where each λm ∈ Λ, m = 1 . . . M is the weight of an associated feature hm (f, e). Function h(f, e) maps foreign sentences f and English sentences e to the vector h1 (f, e) . . . (f, e), and the model chooses translations according to the following decision rule: Figure 1: Structure of the recurrent neural network language model. 2 Expected BLEU Training Recurrent Neural Network LMs Our model has a similar structure to the recurrent neural network language model of Mikolov et al. (2010) which is factored into an input layer,"
P14-2023,P96-1024,0,0.102983,"Missing"
P14-2023,W10-1748,0,0.0572215,"Missing"
P14-2023,P12-1031,0,0.0907025,"Expected BLEU Objective Formally, we define our loss function l(θ) as the negative expected BLEU score, denoted as xBLEU(θ) for a given foreign sentence f : l(θ) = − xBLEU(θ) X pΛ,θ (e|f )sBLEU(e, e(i) ) = Next, we apply the quotient rule of differentiation: (2) ∂xBLEU(θ) ∂(G(θ)/Z(θ)) = ∂sθ (wt ) ∂sθ (wt )   1 ∂G(θ) ∂Z(θ) = − xBLEU(θ) Z(θ) ∂sθ (wt ) ∂sθ (wt ) e∈E(f ) δwt = where sBLEU(e, e(i) ) is a smoothed sentencelevel BLEU score with respect to the reference translation e(i) , and E(f ) is the generation set given by an n-best list.2 We use a sentence-level BLEU approximation similar to He and Deng (2012).3 The normalized probability pΛ,θ (e|f ) of a particular translation e given f is defined as: pΛ,θ (e|f ) = P exp{γΛT h(f, e)} e0 ∈E(f ) exp{γΛ T h(f, e0 )} Using the observation that θ is only relevant to the recurrent neural network hM +1 (e) (1) we have ∂γΛT h(f, e) ∂hM +1 (e) γλM +1 = γλM +1 = ∂sθ (wt ) ∂sθ (wt ) sθ (wt ) (3) which together with the chain rule, (3) and (4) allows us to rewrite δwt as follows: where ΛT h(f, e) includes the recurrent neural network hM +1 (e), and γ ∈ [0, inf) is a scaling factor that flattens the distribution for γ < 1 and sharpens it for γ > 1 (Tromble et"
P14-2023,W11-2119,0,0.0284544,"Missing"
P14-2023,E14-1003,1,0.582922,"A jfgao@microsoft.com Introduction Neural network-based language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012; Vaswani et al., 2013; Gao et al., 2014). In this paper we focus on recurrent neural network architectures which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011; Sundermeyer et al., 2013) with several subsequent applications in machine translation (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). Recurrent models have the potential to capture long-span dependencies since their predictions are based on an unbounded history of previous words (§2). In practice, neural network models for machine translation are usually trained by maximizing the likelihood of the training data, either via a crossentropy objective (Mikolov et al., 2010; Schwenk 136 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 136–142, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics wt time algorithm, which unrolls t"
P14-2023,D13-1176,0,0.060824,"crosoft Research Redmond, WA, USA jfgao@microsoft.com Introduction Neural network-based language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012; Vaswani et al., 2013; Gao et al., 2014). In this paper we focus on recurrent neural network architectures which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011; Sundermeyer et al., 2013) with several subsequent applications in machine translation (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). Recurrent models have the potential to capture long-span dependencies since their predictions are based on an unbounded history of previous words (§2). In practice, neural network models for machine translation are usually trained by maximizing the likelihood of the training data, either via a crossentropy objective (Mikolov et al., 2010; Schwenk 136 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 136–142, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics wt time algorith"
P14-2023,W12-2702,0,0.1388,"current network into firstpass decoding under an efficient approximation. Our best results improve a phrasebased statistical machine translation system trained on WMT 2012 French-English data by up to 2.0 BLEU, and the expected BLEU objective improves over a crossentropy trained model by up to 0.6 BLEU in a single reference setup. 1 Jianfeng Gao Microsoft Research Redmond, WA, USA jfgao@microsoft.com Introduction Neural network-based language and translation models have achieved impressive accuracy improvements on statistical machine translation tasks (Allauzen et al., 2011; Le et al., 2012b; Schwenk et al., 2012; Vaswani et al., 2013; Gao et al., 2014). In this paper we focus on recurrent neural network architectures which have recently advanced the state of the art in language modeling (Mikolov et al., 2010; Mikolov et al., 2011; Sundermeyer et al., 2013) with several subsequent applications in machine translation (Auli et al., 2013; Kalchbrenner and Blunsom, 2013; Hu et al., 2014). Recurrent models have the potential to capture long-span dependencies since their predictions are based on an unbounded history of previous words (§2). In practice, neural network models for machine translation are usual"
P14-2023,D08-1065,0,0.0131301,"Deng (2012).3 The normalized probability pΛ,θ (e|f ) of a particular translation e given f is defined as: pΛ,θ (e|f ) = P exp{γΛT h(f, e)} e0 ∈E(f ) exp{γΛ T h(f, e0 )} Using the observation that θ is only relevant to the recurrent neural network hM +1 (e) (1) we have ∂γΛT h(f, e) ∂hM +1 (e) γλM +1 = γλM +1 = ∂sθ (wt ) ∂sθ (wt ) sθ (wt ) (3) which together with the chain rule, (3) and (4) allows us to rewrite δwt as follows: where ΛT h(f, e) includes the recurrent neural network hM +1 (e), and γ ∈ [0, inf) is a scaling factor that flattens the distribution for γ < 1 and sharpens it for γ > 1 (Tromble et al., 2008).4 Next, we define the gradient of the expected BLEU loss function l(θ) using the observation that the loss does not explicitly depend on θ: δwt = X  = pΛ,θ (e|f )U (θ, e)λM +1 e∈E(f ), s.t.wt ∈e |e| XX e t=1 −δwt γ sθ (wt )  where U (θ, e) = sBLEU(e, ei ) − xBLEU(θ). t=1 =  X  ∂ exp{γΛT h(f, e)} 1 U (θ, e) Z(θ) ∂sθ (wt ) e∈E(f ), s.t.wt ∈e ∂l(θ) X X ∂l(θ) ∂sθ (wt ) = ∂θ ∂sθ (wt ) ∂θ e |e| Derivation of the Error Term δwt ∂sθ (wt ) ∂θ 4 Decoder Integration Directly integrating our recurrent neural network language model into first-pass decoding enables us to search a much larger space than"
P14-2023,D13-1140,0,0.314434,"on (Rosti et al., 2010; Rosti et al., 2011; He and Deng, 2012; Gao and He, 2013; Gao et al., 2014) instead of solely relying on traditional optimizers such as Minimum Error Rate Training (MERT) that only adjust the weighting of entire component models within the log-linear framework of machine translation (§3). Most previous work on neural networks for machine translation is based on a rescoring setup (Arisoy et al., 2012; Mikolov, 2012; Le et al., 2012a; Auli et al., 2013), thereby side stepping the algorithmic and engineering challenges of direct decoder-integration. One recent exception is Vaswani et al. (2013) who demonstrated that feedforward network-based language models are more accurate in first-pass decoding than in rescoring. Decoder integration has the advantage for the neural network to directly influence search, unlike rescoring which is restricted to an n-best list or lattice. Decoding with feed-forward architectures is straightforward, since predictions are based on a fixed size input, similar to n-gram language models. However, for recurrent networks we have to deal with the unbounded history, which breaks the usual dynamic programming assumptions for efficient search. We show how a sim"
P14-2023,D13-1112,0,0.0152677,"n the n-best list of the development set, augmented by scores corresponding to the neural network models. At test time we rescore n-best lists with the new weights. Neural Network Training. All neural network models are trained on the news portion of the parallel data, corresponding to 136K sentences, which we found to be most useful in initial experiments. As training data we use unique 100-best lists generated by the baseline system. We use the same data both for training the phrase-based system as well as the language model but find that the resulting bias did not hurt end-to-end accuracy (Yu et al., 2013). The vocabulary consists of words that occur in at least two different sentences, which is 31K words for both language pairs. We tuned the learning rate µ of our mini-batch SGD trainer as well as the probability scaling parameter γ (3) on a held-out set and found simple settings of µ = 0.1 and γ = 1 to be good choices. To prevent over-fitting, we experimented with L2 regularization, but found no accuracy improvements, probably because SGD regularizes enough. We evaluate performance on a held-out set during training and stop whenever the objective changes less than 0.0003. The hidden layer use"
P14-2023,W11-2135,0,\N,Missing
P15-1128,P14-1091,0,0.396713,"gely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The representation power of the semantic parse is thus controlled by t"
P15-1128,D14-1002,1,0.473482,"etworks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolution matrix: Wc Word hashing layer: ft x y x Figure 7: Extending an inferential chain with constraints and aggregation functions. word boundary symbol #. Then, it uses a convolutional layer to project the letter-trigram vectors of words within a con"
P15-1128,P14-1133,0,0.16292,"wever, most traditional approaches for semantic parsing are largely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The represent"
P15-1128,D13-1160,0,0.888293,"et al., 2014), but with some key differences. The nodes and edges in our query graph closely resemble the exact entities and predicates from the knowledge base. As a result, the graph can be straightforwardly translated to a logical form query that is directly executable. In contrast, the query graph in (Reddy et al., 2014) is mapped from the CCG parse of the question, and needs further transformations before mapping to subgraphs 2 y should be grounded to a CVT entity in this case. of the target knowledge base to retrieve answers. Semantically, our query graph is more related to simple λ-DCS (Berant et al., 2013; Liang, 2013), which is a syntactic simplification of λ-calculus when applied to graph databases. A query graph can be viewed as the tree-like graph pattern of a logical form in λ-DCS. For instance, the path from the answer node to an entity node can be described using a series of join operations in λ-DCS. Different paths of the tree graph are combined via the intersection operators. 3 Staged Query Graph Generation We focus on generating query graphs with the following properties. First, the tree graph consists of one entity node as the root, referred as the topic entity. Second, there exists"
P15-1128,D13-1161,0,0.0148159,"eved simply by executing the query. The semantic parse also provides a deeper understanding of the question, which can be used to justify the answer to users, as well as to provide easily interpretable information to developers for error analysis. However, most traditional approaches for semantic parsing are largely decoupled from the knowledge base, and thus are faced with several challenges when adapted to applications like QA. For instance, a generic meaning representation may have the ontology matching problem when the logical form uses predicates that differ from those defined in the KB (Kwiatkowski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Sem"
P15-1128,J13-2005,0,0.417175,"ski et al., 2013). Even when the representation language is closely related to the knowledge base schema, finding the correct predicates from the large vocabulary in the KB to relations described in the utterance remains a difficult problem (Berant and Liang, 2014). Inspired by (Yao and Van Durme, 2014; Bao et al., 2014), we propose a semantic parsing framework that leverages the knowledge base more tightly when forming the parse for an input question. We first define a query graph that can be straightforwardly mapped to a logical form in λcalculus and is semantically closely related to λDCS (Liang, 2013). Semantic parsing is then reduced to query graph generation, formulated as a search problem with staged states and actions. Each state is a candidate parse in the query graph representation and each action defines a way to grow the graph. The representation power of the semantic parse is thus controlled by the set of legitimate actions applicable to each state. In particular, we stage the actions into three main steps: locating the topic entity in the question, finding the main relationship between the answer and the topic entity, and expanding the query graph with additional constraints that"
P15-1128,D14-1067,0,0.894208,"ce, one of our constructions maps the question to a pattern by replacing the entity mention with a generic symbol <e&gt; and then compares it with a candidate chain, such as “who first voiced meg on <e&gt;” vs. cast-actor. The model consists of two neural networks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolu"
P15-1128,P13-1042,0,0.321803,"Missing"
P15-1128,Q14-1030,0,0.0642215,"to map entities retrieved by the query. The diamond node arg min constrains that the answer needs to be the earliest actor for this role. Equivalently, the logical form query in λ-calculus without the aggregation function is: λx.∃y.cast(FamilyGuy, y) ∧ actor(y, x) ∧ character(y, MegGriffin) Running this query graph against K as in Fig. 1 will match both LaceyChabert and MilaKunis before applying the aggregation function, but only LaceyChabert is the correct answer as she started this role earlier (by checking the from property of the grounded CVT node). Our query graph design is inspired by (Reddy et al., 2014), but with some key differences. The nodes and edges in our query graph closely resemble the exact entities and predicates from the knowledge base. As a result, the graph can be straightforwardly translated to a logical form query that is directly executable. In contrast, the query graph in (Reddy et al., 2014) is mapped from the CCG parse of the question, and needs further transformations before mapping to subgraphs 2 y should be grounded to a CVT entity in this case. of the target knowledge base to retrieve answers. Semantically, our query graph is more related to simple λ-DCS (Berant et al."
P15-1128,P15-1049,1,0.237944,"iority queue, which is formally defined in Appendix A. In the following subsections, we use a running example of finding the semantic parse of question qex = “Who first voiced Meg of Family Guy?” to describe the sequence of actions. 3.1 Family Guy Linking Topic Entity Starting from the initial state s0 , the valid actions are to create a single-node graph that corresponds to the topic entity found in the given question. For instance, possible topic entities in qex can either be FamilyGuy or MegGriffin, shown in Fig. 4. We use an entity linking system that is designed for short and noisy text (Yang and Chang, 2015). For each entity e in the knowledge base, the system first prepares a surface-form lexicon that lists all possible ways that e can be mentioned in text. This lexicon is created using various data sources, such as names and aliases of the entities, the anchor text in Web documents and the Wikipedia redirect table. Given a question, it considers all the y Family Guy cast Family Guy writer y Family Guy genre x Family Guy actor start x x Figure 5: Candidate core inferential chains start from the entity FamilyGuy. consecutive word sequences that have occurred in the lexicon as possible mentions, p"
P15-1128,D14-1071,0,0.379928,"t nodes indicating that certain conditions cannot be satisfied. Our graph generation method is inspired by (Yao and Van Durme, 2014; Bao et al., 2014). Unlike traditional semantic parsing approaches, it uses the knowledge base to help prune the search space when forming the parse. Similar ideas have also been explored in (Poon, 2013). Empirically, our results suggest that it is crucial to identify the core inferential chain, which matches the relationship between the topic entity in the question and the answer. Our CNN models can be analogous to the embedding approaches (Bordes et al., 2014a; Yang et al., 2014), but are more sophisticated. By allowing parameter sharing among different question-pattern and KB predicate pairs, the matching score of a rare or even unseen pair in the training data can still be predicted precisely. This is due to the fact that the prediction is based on the shared model parameters (i.e., projection matrices) that are estimated using all training pairs. 6 Conclusion In this paper, we present a semantic parsing framework for question answering using a knowledge base. We define a query graph as the meaning representation that can be directly mapped to a logical form. Semant"
P15-1128,P14-1090,0,0.722517,"Missing"
P15-1128,P14-2105,1,0.725471,"ctions maps the question to a pattern by replacing the entity mention with a generic symbol <e&gt; and then compares it with a candidate chain, such as “who first voiced meg on <e&gt;” vs. cast-actor. The model consists of two neural networks, one for the pattern and the other for the inferential chain. Both are mapped to k-dimensional vectors as the output of the networks. Their semantic similarity is then computed using some distance function, such as cosine. This continuous-space representation approach has been proposed recently for semantic parsing and question answering (Bordes et al., 2014a; Yih et al., 2014) and has shown better results compared to lexical matching approaches (e.g., word-alignment models). In this work, we adapt a convolutional neural network (CNN) framework (Shen et al., 2014b; Shen et al., 2014a; Gao et al., 2014) to this matching problem. The network architecture is illustrated in Fig. 6. The CNN model first applies a word hashing technique (Huang et al., 2013) that breaks a word into a vector of letter-trigrams (xt → ft in Fig. 6). For example, the bag of letter-trigrams of the word “who” are #-w-h, w-h-o, h-o-# after adding the actor x Meg Griffin Convolution matrix: Wc Word"
P15-1128,P13-1092,0,\N,Missing
P15-2073,W05-0909,0,0.0517039,"irable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the"
P15-2073,E06-1032,0,0.0476642,"on, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propos"
P15-2073,2003.mtsummit-papers.9,0,0.031569,"run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Ve"
P15-2073,N12-1017,0,0.0244703,"ri,j )  PP i g ∈ n-grams(hi ) maxj wi,j ·#g (hi ) n 1 e(1−ρ/η) if η &gt; ρ otherwise Discriminative B LEU (2) where ρ and η are respectively hypothesis and reference lengths.2 Then corpus-level n-gram precision is defined as:  P P i g ∈ n-grams(hi ) maxj #g (hi , ri,j ) P P pn = i g ∈ n-grams(hi ) #g (hi ) where #g (·) is the number of occurrences of n-gram g in a given  sentence, and #g (u, v) is a shorthand for min #g (u), #g (v) . It has been demonstrated that metrics such as B LEU show increased correlation with human judgment as the number of references increases (Przybocki et al., 2008; Dreyer and Marcu, 2012). Unfortunately, gathering multiple references is difficult in the case of conversations. Data gathered from naturally occurring conversations offer only one response per message. One could search (c, m) pairs that occur multiple times in conversational data with the hope of finding distinct responses, but this solution is not feasible. Indeed, the larger 1 Unless mentioned otherwise, B LEU refers to the original IBM B LEU as first described in (Papineni et al., 2002). 2 In the case of multiple references, B LEU selects the reference whose length is closest to that of the hypothesis. In a nuts"
P15-2073,D14-1020,0,0.0691491,"evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), emp"
P15-2073,N15-1124,0,0.0162952,"w model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of B LEU"
P15-2073,W07-0734,0,0.0243517,"ool”, “well then! Why were the biscuits needed?”); others are a little more plausible, but irrelevant or possibly topic changing (“ohh I love that song”). Higher-valued positive-weighted mined responses are typically reasonably appropriate and relevant (even though 447 3 For this work, we sought 2 additional annotations of the seed responses for consistency with the mined responses. As a result, scores for some seed responses slipped below our initial threshold of 4. Nonetheless, these responses were retained. test set.4 While much prior work assesses automatic metrics for MT and other tasks (Lavie and Agarwal, 2007; Hodosh et al., 2013) by computing correlations on observations consisting of single-sentence system outputs, it has been shown (e.g., Przybocki et al. (2008)) that correlation coefficients significantly increase as observation units become larger. For instance, corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that B LEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwi"
P15-2073,C12-1121,0,0.0718467,"corpus-level or system-level correlations tend to be much higher than sentence-level correlations; Graham and Baldwin (2014) show that B LEU is competitive with more recent and advanced metrics when assessed at the system level.5 Therefore, we define our observation unit size to be M = 100 sentences (responses),6 unless stated otherwise. We evaluate qi by averaging human ratings on the M sentences, and mi by computing metric scores on the same set of sentences.7 We compare three different metrics: B LEU, ∆B LEU, and sentence-level B LEU (sB LEU). The last computes sentence-level B LEU scores (Nakov et al., 2012) and averages them on the M sentences (akin to macro-averaging). Finally, unless otherwise noted, all versions of B LEU use n-gram order up to 2 (B LEU-2), as this achieves better correlation for all metrics on this data. extracted from a completely unrelated conversation), and in some cases can outscore the original response, as can be seen in the third set of examples. 4.2 Human Evaluation of System Outputs Responses generated by the 7 systems used in this study on the 2114-triple test set were hand evaluated by 5 crowdsourced raters each on a 5-point Likert-type scale. From these 7 systems,"
P15-2073,P03-1021,0,0.0436342,"rced raters each on a 5-point Likert-type scale. From these 7 systems, 12 system pairs were evaluated, for a total of about pairwise 126K ratings (12 · 5 · 2114). Here too, raters were asked to evaluate responses in terms of their relevance to both context and message. Outputs from different systems were randomly interleaved for presentation to the raters. We obtained human ratings on the following systems: Phrase-based MT: A phrase-based MT system similar to (Ritter et al., 2011), whose weights have been manually tuned. We also included four variants of that system, which we tuned with MERT (Och, 2003). These variants differ in their number of features, and augment (Ritter et al., 2011) with the following phrase-level features: edit distance between source and target, cosine similarity, Jaccard index and distance, length ratio, and DSSM score (Huang et al., 2013). RNN-based MT: the log-probability according to the RNN model of (Sordoni et al., 2015). Baseline: a random baseline. While ∆B LEU relies on human qualitative judgments, it is important to note that human judgments on multi-references (§ 4.1) and those on system outputs were collected completely independently. We also note that the"
P15-2073,P02-1040,0,0.116946,"f outputs are acceptable or even desirable. Tasks with intrinsically diverse targets range from machine translation, summarization, sentence compression, paraphrase generation, and image-to-text to generation of conversational interactions. A major hurdle for these tasks is automation of evaluation, since the space of plausible outputs can be enormous, and it is it impractical to run a new human evaluation every time a new model is built or parameters are modified. In Statistical Machine Translation (SMT), the automation problem has to a large extent been ameliorated by metrics such as B LEU (Papineni et al., 2002) and M ETEOR (Banerjee and Lavie, 2005) Although B LEU is not immune from criticism (e.g., Callison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result"
P15-2073,D11-1054,0,0.592121,"f a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference B LEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe ∆B LEU and investigate its characteristics in comparison to standard B LEU in the context of conversational response generation. We demonstrate that ∆B LEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for C"
P15-2073,N15-1020,1,0.452044,"to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU), a new metric that embeds human judgments concerning the quality of reference sentences directly into the computation of corpus-level multiple-reference B LEU. In effect, we push part of the burden of human evaluation into the automated metric, where it can be repeatedly utilized. Our testbed for this metric is data-driven conversation, a field that has begun to attract interest (Ritter et al., 2011; Sordoni et al., 2015) as an alternative to conventional rule-driven or scripted dialog systems. Intrinsic evaluation in this field is exceptionally challenging because the semantic space of possible responses resists definition and is only weakly constrained by conversational inputs. Below, we describe ∆B LEU and investigate its characteristics in comparison to standard B LEU in the context of conversational response generation. We demonstrate that ∆B LEU correlates well with human evaluation scores in this task and thus can 445 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistic"
P15-2073,P12-2008,0,0.0788347,"llison-Burch et al. (2006)), its properties are well understood, B LEU scores have been shown to correlate well with human judgments (Doddington, *The entirety of this work was conducted while at Microsoft Research. † Corresponding author: mgalley@microsoft.com 2002; Coughlin, 2003; Graham and Baldwin, 2014; Graham et al., 2015) in SMT, and it has allowed the field to proceed. B LEU has been less successfully applied to nonSMT generation tasks owing to the larger space of plausible outputs. As a result, attempts have been made to adapt the metric. To foster diversity in paraphrase generation, Sun and Zhou (2012) propose a metric called iB LEU in which the B LEU score is discounted by a B LEU score computed between the source and paraphrase. This solution, in addition to being dependent on a tunable parameter, is specific only to paraphrase. In image captioning tasks, Vendantam et al. (2015), employ a variant of B LEU in which n-grams are weighted by tf ·idf. This assumes the availability of a corpus with which to compute tf ·idf. Both the above can be seen as attempting to capture a notion of target goodness that is not being captured in B LEU. In this paper, we introduce Discriminative B LEU (∆B LEU"
P16-1094,P12-3007,0,0.0527842,"s the line of investigation initiated by Ritter et al. (2011) who treat generation of conversational dialog as a statistical machine translation (SMT) problem. Ritter et al. (2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et"
P16-1094,P15-2073,1,0.826361,"Missing"
P16-1094,P14-1066,1,0.260835,", 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a S EQ 2S EQ model conditioned on conversation history. Other researchers have recently used S EQ 2S EQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). Serban et al. (2015) propose a hierarchical neural model aimed at capturing dependencies over an extended conv"
P16-1094,N16-1014,1,0.261728,"? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized avatar-like agents, or game characters.1 For present purposes, we"
P16-1094,P15-1002,0,0.023784,"2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural language models (Sutskever et al., 2014; Gao et al., 2014; Bahdanau et al., 2015; Luong et al., 2015) has inspired efforts to extend these neural techniques to SMT-based conversational response generation. Sordoni et al. (2015) augments Ritter et al. (2011) by rescoring outputs using a S EQ 2S EQ model conditioned on conversation history. Other researchers have recently used S EQ 2S EQ to directly generate responses in an end-to-end fashion without relying on SMT phrase tables (Serban et al., 2015; Shang et al., 2015; Vinyals and Le, 2015). Serban et al. (2015) propose a hierarchical neural model aimed at capturing dependencies over an extended conversation history. Recent work by Li et al. ("
P16-1094,P03-1021,0,0.0474555,"rate generic and commonplace responses such as I don’t know, we follow Li et al. (2016) by reranking the generated N-best list using 997 a scoring function that linearly combines a length penalty and the log likelihood of the source given the target: log p(R|M, v) + λ log p(M |R) + γ|R| (11) where p(R|M, v) denotes the probability of the generated response given the message M and the respondent’s speaker ID. |R |denotes the length of the target and γ denotes the associated penalty weight. We optimize γ and λ on N-best lists of response candidates generated from the development set using MERT (Och, 2003) by optimizing B LEU. To compute p(M |R), we train an inverse S EQ 2S EQ model by swapping messages and responses. We trained standard S EQ 2S EQ models for p(M |R) with no speaker information considered. 5 Datasets 5.1 • Learning rate is set to 1.0. • Parameters are initialized by sampling from the uniform distribution [−0.1, 0.1]. • Gradients are clipped to avoid gradient explosion with a threshold of 5. • Vocabulary size is limited to 50,000. • Dropout rate is set to 0.2. Twitter Persona Dataset Data Collection Training data for the Speaker Model was extracted from the Twitter FireHose for"
P16-1094,W00-0306,0,0.39345,"nnotators. 2 Related Work This work follows the line of investigation initiated by Ritter et al. (2011) who treat generation of conversational dialog as a statistical machine translation (SMT) problem. Ritter et al. (2011) represents a break with previous and contemporaneous dialog work that relies extensively on hand-coded rules, typically either building statistical models on top of heuristic rules or templates (Levin et al., 2000; Young et al., 2010; Walker et al., 2003; Pieraccini et al., 2009; Wang et al., 2011) or learning generation rules from a minimal set of authored rules or labels (Oh and Rudnicky, 2000; Ratnaparkhi, 2002; Banchs and Li, 2012; Ameixa et al., 2014; Nio et al., 2014; Chen et al., 2013). More recently (Wen et al., 2015) have used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) to learn from unaligned data in order to reduce the heuristic space of sentence planning and surface realization. The SMT model proposed by Ritter et al., on the other hand, is end-to-end, purely data-driven, and contains no explicit model of dialog structure; the model learns to converse from human-to-human conversational corpora. Progress in SMT stemming from the use of neural languag"
P16-1094,P02-1040,0,0.111268,"Missing"
P16-1094,D11-1054,0,0.655954,"ou? Where were you born? I was born in Canada. Where are you from? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized ava"
P16-1094,walker-etal-2012-annotated,0,0.0254272,"to reduce the proportion of generic responses typical of S EQ 2S EQ systems. Yao et al. (2015) employ an intention network to maintain the relevance of responses. Modeling of users and speakers has been extensively studied within the standard dialog modeling framework (e.g., (Wahlster and Kobsa, 1989; Kobsa, 1990; Schatztnann et al., 2005; Lin and Walker, 2011)). Since generating meaningful responses in an open-domain scenario is intrinsically difficult in conventional dialog systems, existing models often focus on generalizing character style on the basis of qualitative statistical analysis (Walker et al., 2012; Walker et al., 2011). The present work, by contrast, is in the vein of the S EQ 2S EQ models of Vinyals and Le (2015) and Li et al. (2016), enriching these models by training persona vectors directly from conversational data and relevant side-information, and incorporating these directly into the decoder. 995 3 Sequence-to-Sequence Models Given a sequence of inputs X = {x1 , x2 , ..., xnX }, an LSTM associates each time step with an input gate, a memory gate and an output gate, respectively denoted as it , ft and ot . We distinguish e and h where et denotes the vector for an individual text"
P16-1094,D15-1199,0,0.168657,"Missing"
P16-1094,P15-1152,0,0.72735,"Missing"
P16-1094,N15-1020,1,0.919869,"rn? I was born in Canada. Where are you from? England, you? Where did you grow up? I grew up in Texas. How old are you? 16 and you? What’s your age? 18. What is your major? I’m majoring in psychology What did you study in college? English lit. Table 1: Inconsistent responses generated by a 4-layer S EQ 2S EQ model trained on 25 million Twitter conversation snippets. Introduction As conversational agents gain traction as user interfaces, there has been growing research interest in training naturalistic conversation systems from large volumes of human-to-human interactions (Ritter et al., 2011; Sordoni et al., 2015; Vinyals and Le, 2015; Li et al., 2016). One major issue for these data-driven systems is their propensity to select the response with greatest likelihood—in effect a consensus response of the humans represented in the training data. Outputs are frequently vague or non-committal (Li et al., 2016), and when not, they can be wildly inconsistent, as illustrated in Table 1. In this paper, we address the challenge of consistency and how to endow data-driven systems with the coherent “persona” needed to model humanlike behavior, whether as personal assistants, personalized avatar-like agents, or ga"
P16-1153,P09-1010,0,0.0211587,"There has been increasing interest in applying deep reinforcement learning to a variety problems, but only a few studies address problems with natural language state or action spaces. In language processing, reinforcement learning has been applied to a dialogue management system that converses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al., 2013). There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping text instructions to sequences of executable actions (Branavan et al., 2009). In some applications, it is possible to manually design features for state-action pairs, which are then used in reinforcement learning to learn a near-optimal policy (Li et al., 2009). Designing such features, however, require substantial domain knowledge. The work most closely related to our study inolves application of deep reinforcement to learning decision policies for parser-based text games. Narasimhan et al. (2015) applied a Long ShortTerm Memory DQN framework, which achieves higher average reward than the random and Bagof-Words DQN baselines. In this work, actions are constrained to"
P16-1153,P11-1028,0,0.0158818,"N can generalize well to “unseen” natural language descriptions of actions. 4 Related Work There has been increasing interest in applying deep reinforcement learning to a variety problems, but only a few studies address problems with natural language state or action spaces. In language processing, reinforcement learning has been applied to a dialogue management system that converses with a human user by taking actions that generate natural language (Scheffler and Young, 2002; Young et al., 2013). There has also been interest in extracting textual knowledge to improve game control performance (Branavan et al., 2011), and mapping text instructions to sequences of executable actions (Branavan et al., 2009). In some applications, it is possible to manually design features for state-action pairs, which are then used in reinforcement learning to learn a near-optimal policy (Li et al., 2009). Designing such features, however, require substantial domain knowledge. The work most closely related to our study inolves application of deep reinforcement to learning decision policies for parser-based text games. Narasimhan et al. (2015) applied a Long ShortTerm Memory DQN framework, which achieves higher average rewar"
P16-1153,D15-1001,0,0.52424,"Missing"
P16-1153,D15-1166,0,0.00887504,"Missing"
P17-1045,W14-4340,0,0.0295509,"ut as input, and selects an action at as output. The action space, denoted by A, consists of M + 1 actions — request(slot=i) for 1 ≤ i ≤ M will ask the user for the value of slot i, and inform(I) will inform the user with an ordered list of results I from the KB. The dialogue ends once the agent chooses inform. We adopt a modular approach, typical to goaloriented dialogue systems (Wen et al., 2016b), consisting of: a belief tracker module for identifying user intents, extracting associated slots, and tracking the dialogue state (Yao et al., 2014; Hakkani-T¨ur et al., 2016; Chen et al., 2016b; Henderson et al., 2014; Henderson, 2015); an interface with the database to query for relevant results (Soft-KB lookup); a summary module to summarize the state into a vector; a dialogue policy which selects the next system action based on current state (Young et al., 2013). We assume the agent only responds with dialogue acts. A templatebased Natural Language Generator (NLG) can be easily constructed for converting dialogue acts into natural language. 4.2 |{w ∈ ut } ∩ {w ∈ v}| |{w ∈ v}| stj [v] = Beliefs Summary  [v] + C stj [v] + btj + 1(req t [j] = 1) (5) ptj [v] ∝ pt−1 j Here C is a tuning parameter, and the n"
P17-1045,D16-1127,1,0.0992659,"l,jfgao}@microsoft.com Abstract y.v.chen@ieee.org factual questions, and sometimes also aimlessly chit-chat with the user, but they still lag far behind a human assistant in terms of both the variety and complexity of tasks they can perform. In particular, they lack the ability to learn from interactions with a user in order to improve and adapt with time. Recently, Reinforcement Learning (RL) has been explored to leverage user interactions to adapt various dialogue agents designed, respectively, for task completion (Gaˇsi´c et al., 2013), information access (Wen et al., 2016b), and chitchat (Li et al., 2016a). We focus on KB-InfoBots, a particular type of dialogue agent that helps users navigate a Knowledge Base (KB) in search of an entity, as illustrated by the example in Figure 1. Such agents must necessarily query databases in order to retrieve the requested information. This is usually done by performing semantic parsing on the input to construct a symbolic query representing the beliefs of the agent about the user goal, such as Wen et al. (2016b), Williams and Zweig (2016), and Li et al. (2017)’s work. We call such an operation a Hard-KB lookup. While natural, this approach has two drawback"
P17-1045,D15-1199,0,0.0476488,"Missing"
P17-1045,I17-1074,1,0.601884,"for task completion (Gaˇsi´c et al., 2013), information access (Wen et al., 2016b), and chitchat (Li et al., 2016a). We focus on KB-InfoBots, a particular type of dialogue agent that helps users navigate a Knowledge Base (KB) in search of an entity, as illustrated by the example in Figure 1. Such agents must necessarily query databases in order to retrieve the requested information. This is usually done by performing semantic parsing on the input to construct a symbolic query representing the beliefs of the agent about the user goal, such as Wen et al. (2016b), Williams and Zweig (2016), and Li et al. (2017)’s work. We call such an operation a Hard-KB lookup. While natural, this approach has two drawbacks: (1) the retrieved results do not carry any information about uncertainty in semantic parsing, and (2) the retrieval operation is non differentiable, and hence the parser and dialog policy are trained separately. This makes online endto-end learning from user feedback difficult once the system is deployed. In this work, we propose a probabilistic framework for computing the posterior distribution of the user target over a knowledge base, which we term a Soft-KB lookup. This distribution is const"
P17-1045,N07-2038,0,0.536228,"ing values denoted by X). periments that this framework allows the agent to achieve a higher task success rate in fewer dialogue turns. Further, the retrieval process is differentiable, allowing us to construct an end-to-end trainable KB-InfoBot, all of whose components are updated online using RL. Reinforcement learners typically require an environment to interact with, and hence static dialogue corpora cannot be used for their training. Running experiments on human subjects, on the other hand, is unfortunately too expensive. A common workaround in the dialogue community (Young et al., 2013; Schatzmann et al., 2007b; Scheffler and Young, 2002) is to instead use user simulators which mimic the behavior of real users in a consistent manner. For training KB-InfoBot, we adapt the publicly available2 simulator described in Li et al. (2016b). Evaluation of dialogue agents has been the subject of much research (Walker et al., 1997; M¨oller et al., 2006). While the metrics for evaluating an InfoBot are relatively clear — the agent should return the correct entity in a minimum number of turns — the environment for testing it not so much. Unlike previous KB-based QA systems, our focus is on multi-turn interaction"
P17-1045,2007.sigdial-1.48,0,0.0309207,"ing values denoted by X). periments that this framework allows the agent to achieve a higher task success rate in fewer dialogue turns. Further, the retrieval process is differentiable, allowing us to construct an end-to-end trainable KB-InfoBot, all of whose components are updated online using RL. Reinforcement learners typically require an environment to interact with, and hence static dialogue corpora cannot be used for their training. Running experiments on human subjects, on the other hand, is unfortunately too expensive. A common workaround in the dialogue community (Young et al., 2013; Schatzmann et al., 2007b; Scheffler and Young, 2002) is to instead use user simulators which mimic the behavior of real users in a consistent manner. For training KB-InfoBot, we adapt the publicly available2 simulator described in Li et al. (2016b). Evaluation of dialogue agents has been the subject of much research (Walker et al., 1997; M¨oller et al., 2006). While the metrics for evaluating an InfoBot are relatively clear — the agent should return the correct entity in a minimum number of turns — the environment for testing it not so much. Unlike previous KB-based QA systems, our focus is on multi-turn interaction"
P17-1045,W16-0105,0,0.0321301,"). These agents can perform simple tasks, answer 1 The source code is available at: https://github. com/MiuLab/KB-InfoBot 484 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 484–495 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1045 2 Related Work Entity-Centric Knowledge Base Movie=? Actor=Bill Murray Release Year=1993 Actor Release Year Groundhog Day Bill Murray 1993 Australia Nicole Kidman X Mad Max: Fury Road X 2015 Movie Our work is motivated by the neural GenQA (Yin et al., 2016a) and neural enquirer (Yin et al., 2016b) models for querying KBs via natural language in a fully “neuralized” way. However, the key difference is that these systems assume that users can compose a complicated, compositional natural language query that can uniquely identify the element/answer in the KB. The research task is to parse the query, i.e., turning the natural language query into a sequence of SQL-like operations. Instead we focus on how to query a KB interactively without composing such complicated queries in the first place. Our work is motivated by the observations that (1) users"
P17-1045,P97-1035,0,0.811706,"ers typically require an environment to interact with, and hence static dialogue corpora cannot be used for their training. Running experiments on human subjects, on the other hand, is unfortunately too expensive. A common workaround in the dialogue community (Young et al., 2013; Schatzmann et al., 2007b; Scheffler and Young, 2002) is to instead use user simulators which mimic the behavior of real users in a consistent manner. For training KB-InfoBot, we adapt the publicly available2 simulator described in Li et al. (2016b). Evaluation of dialogue agents has been the subject of much research (Walker et al., 1997; M¨oller et al., 2006). While the metrics for evaluating an InfoBot are relatively clear — the agent should return the correct entity in a minimum number of turns — the environment for testing it not so much. Unlike previous KB-based QA systems, our focus is on multi-turn interactions, and as such there are no publicly available benchmarks for this problem. We evaluate several versions of KB-InfoBot with the simulator and on real users, and show that the proposed Soft-KB lookup helps the reinforcement learner discover better dialogue policies. Initial experiments on the end-to-end agent also"
P17-1045,W16-3601,0,0.583178,"ity of the whole system. As a result, training of various components of the dialogue system is performed separately. The intent network and belief trackers are trained using supervised labels specifically collected for them; while the policy network and generation network are trained separately on the system utterances. We retain modularity of the network by keeping the belief trackers separate, but replace the hard lookup with a differentiable one. Dialogue agents can also interface with the database by augmenting their output action space with predefined API calls (Williams and Zweig, 2016; Zhao and Eskenazi, 2016; Bordes and Weston, 2016; Li et al., 2017). The API calls modify a query hypothesis maintained outside the end-toend system which is used to retrieve results from this KB. This framework does not deal with uncertainty in language understanding since the query hypothesis can only hold one slot-value at a time. Our approach, on the other hand, directly models the uncertainty to construct the posterior over the KB. Wu et al. (2015) presented an entropy minimization dialogue management strategy for InFind me the Bill Murray’s movie. When was it released? I think it came out in 1993. User Groundho"
P17-1045,D16-1233,0,0.025563,"Missing"
P17-1045,W16-0106,0,\N,Missing
P17-1070,buck-etal-2014-n,0,0.0428013,"Missing"
P17-1070,P15-1001,0,0.145211,"d. The sequence of embeddings for our example three-word sequence becomes: x = (x1 , x2 , hc M ). We use the same dimensionality for word embedding vectors xi and composed character sequence vectors hc M to ensure the two ways to define embeddings are compatible. Our hybrid source encoder architecture is similar to the one proposed by Luong and Manning (2016). Nested attention hybrid decoder In traditional word-based sequence-to-sequence models special target UNK tokens are used to represent outputs that are outside the target vocabulary. A post-processing UNK-replacement method is then used (Cho et al., 2015; Yuan and Briscoe, 2016) to replace these special tokens with target words. The hybrid model of (Luong and Manning, 2016) uses a jointly trained character-level decoder to generate target words corresponding to UNK tokens, and outperforms the traditional approach in the machine translation task. However, unlike machine translation, models for grammar correction conduct “translation” in the same language, and often need to apply a small number of local edits to the character sequence of a source word corresponding to the target UNK word. For example, rare but correct words such as entity names"
P17-1070,P06-1032,0,0.145092,"ificantly outperforms previous neural models for GEC as measured on the standard CoNLL14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography. 1 Introduction One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016). Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016). ∗ This work was conducted while the third author worked at Microsoft Research. Recently, neural machine translation (NMT) systems h"
P17-1070,W13-1703,0,0.636911,"0 ¯ , yc GRUc decNested (dc n−1 ) n>0 n−1 4.1 where cc n is the context vector obtained using character-level attention on the sequence ec and the last state of the character-level decoder dc n , computed following equations 2, 3 and 4, but using a different set of parameters. These equations show that the character-level decoder with nested attention can use both the wordlevel state dˆs , and the character-level context cc n Experiments Dataset and Evaluation We use standard publicly available datasets for training and evaluation. One data source is the NUS Corpus of Learner English (NUCLE) (Dahlmeier et al., 2013), which is provided as a training set for the CoNLL-13 and CoNLL-14 shared tasks. From the original corpus of size about 60K parallel sentences, we randomly selected close to 5K sentence pairs for use as a validation set, and 45K parallel sentences for use in training. A second data source 757 #Sent pairs Training 2,608,679 Validation 4,771 Development 1,381 Test get OOV, but follow Cho et al. (2015), Section 1,312 3.3 to use the NMT system’s attention weights instead. The target OOV is then replaced by the most likely correction of the source word from the wordcorrection lexicon, or by the so"
P17-1070,C10-1041,1,0.783437,"revious neural models for GEC as measured on the standard CoNLL14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography. 1 Introduction One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016). Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016). ∗ This work was conducted while the third author worked at Microsoft Research. Recently, neural machine translation (NMT) systems have achieved subst"
P17-1070,D16-1161,0,0.626732,"els for GEC as measured on the standard CoNLL14 benchmark dataset. Further analysis also shows that the superiority of the proposed model can be largely attributed to the use of the nested attention mechanism, which has proven particularly effective in correcting local errors that involve small edits in orthography. 1 Introduction One of the most successful approaches to grammatical error correction (GEC) is to cast the problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016). Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016). ∗ This work was conducted while the third author worked at Microsoft Research. Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation qualit"
P17-1070,Q17-1026,0,0.00769883,"elp of character and word-level encoders and decoders with two nested levels of attention. Our model is inspired by advances in sub-word level modeling in neural machine translation. We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention. We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task. Alternative methods for MT include modeling of word pieces to achieve open vocabulary (Sennrich et al., 2016), and more recently, fully character-level modeling (Lee et al., 2017). None of these models integrate two nested levels of attention although an empirical evaluation of these approaches for GEC would also be interesting. 3 specific sequence of hidden state vectors e: e = (h1 , . . . , hT ) The hidden state ht at time t is computed as: ft = GRUencf (ft−1 , xt ) , bt = GRUencb (bt+1 , xt ), ht = [ft ; bt ], where GRUencf and GRUencb stand for gated recurrent unit functions as described in Cho et al. (2014). We use the symbol GRU with different subscripts to represent GRU functions using different sets of parameters (for example, we used the encf and encb subscrip"
P17-1070,P16-1100,0,0.12905,"ctors and attention at both word and character levels, we model all contextual words, including OOVs, in a unified context vector representation. In particular, as we will discuss in Section 5, the character-level attention layer captures most useful information for correcting local errors that involve small edits in orthography. Our model differs substantially from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al. (2016) in the way we infuse information at both the word level and the character level. We extend the wordcharacter hybrid model of Luong and Manning (2016), which was originally developed for machine translation, by introducing a character attention layer. This allows the model to learn substitution patterns at both the character level and the word level in an end-to-end fashion, using sentencecorrection pairs. We validate the effectiveness of our model on the CoNLL-14 benchmark dataset (Ng et al., 2014). Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of Luong and Manning (2016), which we apply to GEC for the first time. When integrated with a large word-based n-gram language model"
P17-1070,W14-1701,0,0.533419,"y from the wordlevel S2S model of Yuan and Briscoe (2016) and the character-level S2S model of Xie et al. (2016) in the way we infuse information at both the word level and the character level. We extend the wordcharacter hybrid model of Luong and Manning (2016), which was originally developed for machine translation, by introducing a character attention layer. This allows the model to learn substitution patterns at both the character level and the word level in an end-to-end fashion, using sentencecorrection pairs. We validate the effectiveness of our model on the CoNLL-14 benchmark dataset (Ng et al., 2014). Results show that the proposed model outperforms all previous neural models for GEC, including the hybrid model of Luong and Manning (2016), which we apply to GEC for the first time. When integrated with a large word-based n-gram language model, our GEC system achieves an F0.5 of 45.15 on CoNLL-14, substantially exceeding the previFigure 1: Architecture of Nested Attention Hybrid Model ously reported top performance of 40.56 achieved by using a neural model and an external language model (Xie et al., 2016). 2 Related Work A variety of classifier-based and MT-based techniques have been applie"
P17-1070,P16-1208,0,0.266305,"he problem as (monolingual) machine translation (MT), where we translate from possibly ungrammatical English sentences to corrected ones (Brockett et al., 2006; Gao et al., 2010; Junczys-Dowmunt and Grundkiewicz, 2016). Such systems, which are based on phrasebased MT models that are typically trained on large sets of sentence-correction pairs, can correct global errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016). ∗ This work was conducted while the third author worked at Microsoft Research. Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015). Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016). In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC. The core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a seq"
P17-1070,Q16-1013,0,0.0537991,"ural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016). In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC. The core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a sequence of source words into a vector and then generates a sequence of target words from the vector. Unlike the phrase-based MT models, the S2S model can capture long-distance, or even global, word dependencies, which are crucial to correcting global grammatical errors and helping users achieve native speaker fluency (Sakaguchi et al., 2016). Thus, the S2S model is expected to perform better on GEC than phrase-based models. However, as we will show in this paper, to achieve the best performance on GEC, we still need to extend the standard S2S model to address several task-specific challenges, which we will describe below. First, a GEC model needs to deal with an extremely large vocabulary that consists of a large number of words and their (mis)spelling variations. Second, the GEC model needs to capture structure at different levels of granularity in order to correct errors of different types. For example, while correcting spellin"
P17-1070,W16-0528,0,0.0121622,"s, and multiple large language models. Neural approaches to the task are less explored. We believe that the advances from Junczys-Dowmunt and Grundkiewicz (2016) are complementary to the ones we propose for neural MT, and could be integrated with neural models to achieve even higher performance. Two prior works explored sequence to sequence neural models for GEC (Xie et al., 2016; Yuan and Briscoe, 2016), while Chollampatt et al. (2016) integrated neural features in a phrase-based system for the task. Neural models were also applied to the related sub-task of grammatical error identification (Schmaltz et al., 2016). Yuan and Briscoe (2016) demonstrated the promise of neural MT for GEC but did not adapt the basic sequence-to-sequence with attention to its unique challenges, falling back to traditional word-alignment models to address vocabulary coverage with a post-processing heuristic. Xie et al. (2016) built a character-level sequence 754 to sequence model, which achieves open vocabulary and character-level modeling, but has difficulty with global word-level decisions. The primary focus of our work is integration of character and word-level reasoning in neural models for GEC, to capture global fluency"
P17-1070,P16-1162,0,0.119168,"ants, while obtaining open vocabulary coverage. This is achieved with the help of character and word-level encoders and decoders with two nested levels of attention. Our model is inspired by advances in sub-word level modeling in neural machine translation. We build mostly on the hybrid model of Luong and Manning (2016) to expand its capability to correct rare words by fine-grained character-level attention. We directly compare our model to the one of Luong and Manning (2016) on the grammar correction task. Alternative methods for MT include modeling of word pieces to achieve open vocabulary (Sennrich et al., 2016), and more recently, fully character-level modeling (Lee et al., 2017). None of these models integrate two nested levels of attention although an empirical evaluation of these approaches for GEC would also be interesting. 3 specific sequence of hidden state vectors e: e = (h1 , . . . , hT ) The hidden state ht at time t is computed as: ft = GRUencf (ft−1 , xt ) , bt = GRUencb (bt+1 , xt ), ht = [ft ; bt ], where GRUencf and GRUencb stand for gated recurrent unit functions as described in Cho et al. (2014). We use the symbol GRU with different subscripts to represent GRU functions using differe"
P17-1070,P12-2039,0,0.0500203,"system’s attention weights instead. The target OOV is then replaced by the most likely correction of the source word from the wordcorrection lexicon, or by the source word itself if there are no available corrections. Table 1: Overview of the datasets used. Source NUCLE CLC lang-8 Total #Sent pairs 45,422 1,517,174 1,046,083 2,608,679 4.3 Table 2: Training data by source. is the Cambridge Learner Corpus (CLC) (Nicholls, 2003), from which we extracted a substantially larger set of parallel sentences. Finally, we used additional training examples from the Lang-8 Corpus of Learner English v1.0 (Tajiri et al., 2012). As Lang-8 data is crowd-sourced, we used heuristics to filter out noisy examples: we removed sentences longer than 100 words and sentence pairs where the correction was substantially shorter than the input text. Table 2 shows the number of sentence pairs from each source used for training. We evaluate the performance of the models on the standard sets from the CoNLL-14 shared task (Ng et al., 2014). We report final performance on the CoNLL-14 test set without alternatives, and analyze model performance on the CoNLL-13 development set (Dahlmeier et al., 2013). We use the development and valid"
P17-1070,N16-1042,0,0.203389,"errors such as word order and usage and local errors in spelling and inflection. The approach has proven superior to systems based on local classifiers that can only fix focused errors in prepositions, determiners, or inflected forms (Rozovskaya and Roth, 2016). ∗ This work was conducted while the third author worked at Microsoft Research. Recently, neural machine translation (NMT) systems have achieved substantial improvements in translation quality over phrase-based MT systems (Sutskever et al., 2014; Bahdanau et al., 2015). Thus, there is growing interest in applying neural systems to GEC (Yuan and Briscoe, 2016; Xie et al., 2016). In this paper, we significantly extend previous work, and explore new neural models to meet the unique challenges of GEC. The core component of most NMT systems is a sequence-to-sequence (S2S) model which encodes a sequence of source words into a vector and then generates a sequence of target words from the vector. Unlike the phrase-based MT models, the S2S model can capture long-distance, or even global, word dependencies, which are crucial to correcting global grammatical errors and helping users achieve native speaker fluency (Sakaguchi et al., 2016). Thus, the S2S mode"
P18-1157,P16-1223,0,0.0737543,"Missing"
P18-1157,P17-1171,0,0.12555,"rforming a kind of stochastic ensemble over the model’s successive predic1694 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1694–1704 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ants. A typical technique to obtain lexicon embedding is concatenation of its word embedding with other linguistic embedding such as those derived from Part-Of-Speech (POS) tags. For word embeddings, we use the pre-trained 300-dimensional GloVe vectors (Pennington et al., 2014) for the both Q and P . Following Chen et al. (2017), we use three additional types of linguistic features for each token pi in the passage P : x st-1 st st+1 • 9-dimensional POS tagging embedding for total 56 different types of the POS tags. Figure 1: Illustration of “stochastic prediction dropout” in the answer module during training. At each reasoning step t, the model combines memory (bottom row) with hidden states st−1 to generate a prediction (multinomial distribution). Here, there are three steps and three predictions, but one prediction is dropped and the final result is an average of the remaining distributions. • 8-dimensional named-e"
P18-1157,D17-1215,0,0.0159621,"provement as we increase T = 1 to T = 5, but after 5 steps the improvements have saturated. In fact, the EM/F1 scores drop slightly, but considering that the random initialization results in Table 3 show a standard deviation of 0.142 and a spread of 0.426 (for EM), we believe that the T = 10 result does not statistically differ from the T = 5 result. In summary, we think it is useful to perform some approximate hyper-parameter tuning for the number of steps, but it is not necessary to find the exact optimal value. Finally, we test SAN on two Adversarial SQuAD datasets, AddSent and AddOneSent (Jia and Liang, 2017), where the passages contain 1699 Seed# EM F1 Seed# EM F1 Seed 1 76.24 84.06 Seed 6 76.23 83.99 Seed 2 76.30 84.13 Seed 7 76.35 84.09 Seed 3 75.92 83.90 Seed 8 76.07 83.71 Seed 4 76.00 83.95 Seed 9 75.93 83.85 Seed 5 76.12 83.99 Seed 10 76.15 84.11 Mean: 76.131, Std. deviation: 0.142 (EM) Mean: 83.977, Std. deviation: 0.126 (F1) Table 3: Robustness of SAN (5-step) on different random seeds for initialization: best and worst scores are boldfaced. Note that our official submit is trained on seed 1. (a) EM comparison on different systems. SAN 1 step 2 step 3 step 4 step 5 step (b) F1 score compar"
P18-1157,W04-1013,0,0.0153045,"esults on MS MARCO MS MARCO (Nguyen et al., 2016) is a large scale real-word RC dataset which contains 100,100 (100K) queries collected from anonymized user logs from the Bing search engine. The characteristic of MS MARCO is that all the questions are real user queries and passages are extracted from real web documents. For each query, approximate 10 passages are extracted from public web documents. The answers are generated by humans. The data is partitioned into a 82,430 training, a 10,047 development and 9,650 test tuples. The evaluation metrics are BLEU(Papineni et al., 2002) and ROUGE-L (Lin, 2004) due to its free-form text answer style. To apply the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer. It has an upper bound of 93.45 BLEU and 93.82 ROUGE-L on the development set. The MS MARCO dataset contains multiple passages per query. Our model as shown in Figure 2 is developed to generate answer from a single passage. Thus, we need to extend it to handle multiple passages. Following (Shen et al., 2017), we take two steps to generate an answer to a query Q from J passages, P 1 , ..., P J . First, we run SAN on ev1701"
P18-1157,D17-1085,0,0.140626,"rk (prediction from final step) Fixed 5-step with Memory Network (prediction averaged from all steps) Dynamic steps (max 5) with ReasoNet Stochastic Answer Network (SAN ), Fixed 5-step EM 75.139 75.033 75.256 75.355 76.235 F1 83.367 83.327 83.215 83.360 84.056 Table 1: Main results—Comparison of different answer module architectures. Note that SAN performs best in both Exact Match and F1 metrics. Ensemble model results: BiDAF + Self Attention + ELMo (Peters et al., 2018) SAN (Ensemble model) AIR-FusionNet (Huang et al., 2017) DCN+ (Xiong et al., 2017) M-Reader (Hu et al., 2017) Conductor-net (Liu et al., 2017b) r-net (Wang et al., 2017) ReasoNet++ (Shen et al., 2017) Individual model results: BiDAF + Self Attention + ELMo(Peters et al., 2018) SAN (single model) AIR-FusionNet(Huang et al., 2017) RaSoR + TR (Salant and Berant, 2017) DCN+(Xiong et al., 2017) r-net(Wang et al., 2017) ReasoNet++(Shen et al., 2017) BiDAF (Seo et al., 2016) Human Performance Dev Set (EM/F1) -/78.619/85.866 -/-/-/74.8 / 83.3 77.7/83.7 75.4/82.9 Test Set (EM/F1) 81.003/87.432 79.608/86.496 78.978/86.016 78.852/85.996 77.678/84.888 76.996/84.630 76.9/84.0 75.0/82.6 -/76.235/84.056 75.3/83.6 -/74.5/83.1 72.3/80.6 70.8/79.4 6"
P18-1157,P02-1040,0,0.10049,"form by question type? Experiments results on MS MARCO MS MARCO (Nguyen et al., 2016) is a large scale real-word RC dataset which contains 100,100 (100K) queries collected from anonymized user logs from the Bing search engine. The characteristic of MS MARCO is that all the questions are real user queries and passages are extracted from real web documents. For each query, approximate 10 passages are extracted from public web documents. The answers are generated by humans. The data is partitioned into a 82,430 training, a 10,047 development and 9,650 test tuples. The evaluation metrics are BLEU(Papineni et al., 2002) and ROUGE-L (Lin, 2004) due to its free-form text answer style. To apply the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer. It has an upper bound of 93.45 BLEU and 93.82 ROUGE-L on the development set. The MS MARCO dataset contains multiple passages per query. Our model as shown in Figure 2 is developed to generate answer from a single passage. Thus, we need to extend it to handle multiple passages. Following (Shen et al., 2017), we take two steps to generate an answer to a query Q from J passages, P 1 , ..., P J . Fir"
P18-1157,D14-1162,0,0.0841458,"ep is still trained to generate the same answer; we are performing a kind of stochastic ensemble over the model’s successive predic1694 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 1694–1704 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics ants. A typical technique to obtain lexicon embedding is concatenation of its word embedding with other linguistic embedding such as those derived from Part-Of-Speech (POS) tags. For word embeddings, we use the pre-trained 300-dimensional GloVe vectors (Pennington et al., 2014) for the both Q and P . Following Chen et al. (2017), we use three additional types of linguistic features for each token pi in the passage P : x st-1 st st+1 • 9-dimensional POS tagging embedding for total 56 different types of the POS tags. Figure 1: Illustration of “stochastic prediction dropout” in the answer module during training. At each reasoning step t, the model combines memory (bottom row) with hidden states st−1 to generate a prediction (multinomial distribution). Here, there are three steps and three predictions, but one prediction is dropped and the final result is an average of"
P18-1157,N18-1202,0,0.08263,"Missing"
P18-1157,D16-1264,0,0.781631,"ial SQuAD, and the Microsoft MAchine Reading COmprehension Dataset (MS MARCO). 1 Introduction Machine reading comprehension (MRC) is a challenging task: the goal is to have machines read a text passage and then answer any question about the passage. This task is an useful benchmark to demonstrate natural language understanding, and also has important applications in e.g. conversational agents and customer service support. It has been hypothesized that difficult MRC problems require some form of multi-step synthesis and reasoning. For instance, the following example from the MRC dataset SQuAD (Rajpurkar et al., 2016) illustrates the need for synthesis of information across sentences and multiple steps of reasoning: Q: What collection does the V&A Theator & Performance galleries hold? P : The V&A Theator & Performance galleries opened in March 2009. ... They hold the UK’s biggest national collection of This kind of iterative process can be viewed as a form of multi-step reasoning. Several recent MRC models have embraced this kind of multistep strategy, where predictions are generated after making multiple passes through the same text and integrating intermediate information in the process. The first models"
P18-1157,D13-1020,0,0.0320734,"andidate answer span with its relevance score r(P j , Q) assigned by a passage ranker, and output the span with the maximum score as the answer. In our experiments, we use the passage ranker described in (Liu et al., 2018)5 . The ranker is trained on the same MS MARCO training data, and achieves 37.1 p@1 on the development set. The results in Table 7 show that SAN outperforms V-Net (Wang et al., 2018) and becomes the new state of the art6 . 6 Related Work The recent big progress on MRC is largely due to the availability of the large-scale datasets (Rajpurkar et al., 2016; Nguyen et al., 2016; Richardson et al., 2013; Hill et al., 2016), since it is possible to train large end-to-end neural network models. In spite of the variety of model structures and attenion types (Bahdanau et al., 2015; Chen et al., 2016; Xiong et al., 2016; Seo et al., 2016; Shen et al., 2017; Wang et al., 2017), a typical neural network MRC model first maps the symbolic representation of the documents and questions into a neural space, then search answers on top of it. We categorize these models into two groups based on the difference of the answer module: singlestep and multi-step reasoning. The key difference between the two is w"
P18-1157,I17-1096,1,0.832902,"his kind of iterative process can be viewed as a form of multi-step reasoning. Several recent MRC models have embraced this kind of multistep strategy, where predictions are generated after making multiple passes through the same text and integrating intermediate information in the process. The first models employed a predetermined fixed number of steps (Hill et al., 2016; Dhingra et al., 2016; Sordoni et al., 2016; Kumar et al., 2015). Later, Shen et al. (2016) proposed using reinforcement learning to dynamically determine the number of steps based on the complexity of the question. Further, Shen et al. (2017) empirically showed that dynamic multi-step reasoning outperforms fixed multi-step reasoning, which in turn outperforms single-step reasoning on two distinct MRC datasets (SQuAD and MS MARCO). In this work, we derive an alternative multi-step reasoning neural network for MRC. During training, we fix the number of reasoning steps, but perform stochastic dropout on the answer module (final layer predictions). During decoding, we generate answers based on the average of predictions in all steps, rather than the final step. We call this a stochastic answer network (SAN) because the stochastic drop"
P18-1157,P17-1018,0,0.243393,"step) Fixed 5-step with Memory Network (prediction averaged from all steps) Dynamic steps (max 5) with ReasoNet Stochastic Answer Network (SAN ), Fixed 5-step EM 75.139 75.033 75.256 75.355 76.235 F1 83.367 83.327 83.215 83.360 84.056 Table 1: Main results—Comparison of different answer module architectures. Note that SAN performs best in both Exact Match and F1 metrics. Ensemble model results: BiDAF + Self Attention + ELMo (Peters et al., 2018) SAN (Ensemble model) AIR-FusionNet (Huang et al., 2017) DCN+ (Xiong et al., 2017) M-Reader (Hu et al., 2017) Conductor-net (Liu et al., 2017b) r-net (Wang et al., 2017) ReasoNet++ (Shen et al., 2017) Individual model results: BiDAF + Self Attention + ELMo(Peters et al., 2018) SAN (single model) AIR-FusionNet(Huang et al., 2017) RaSoR + TR (Salant and Berant, 2017) DCN+(Xiong et al., 2017) r-net(Wang et al., 2017) ReasoNet++(Shen et al., 2017) BiDAF (Seo et al., 2016) Human Performance Dev Set (EM/F1) -/78.619/85.866 -/-/-/74.8 / 83.3 77.7/83.7 75.4/82.9 Test Set (EM/F1) 81.003/87.432 79.608/86.496 78.978/86.016 78.852/85.996 77.678/84.888 76.996/84.630 76.9/84.0 75.0/82.6 -/76.235/84.056 75.3/83.6 -/74.5/83.1 72.3/80.6 70.8/79.4 67.7/77.3 80.3/90.5 78.580/85"
P18-1157,P18-1178,0,0.0770301,"the same RC model, we search for a span in MS MARCO’s passages that maximizes the ROUGE-L score with the raw freeform answer. It has an upper bound of 93.45 BLEU and 93.82 ROUGE-L on the development set. The MS MARCO dataset contains multiple passages per query. Our model as shown in Figure 2 is developed to generate answer from a single passage. Thus, we need to extend it to handle multiple passages. Following (Shen et al., 2017), we take two steps to generate an answer to a query Q from J passages, P 1 , ..., P J . First, we run SAN on ev1701 SingleM odel ReasoNet++(Shen et al., 2017) V-Net(Wang et al., 2018) Standard 1-step in Table 1 SAN ROUGE BLEU 38.01 38.62 45.65 42.30 42.39 46.14 43.85 Table 7: MS MARCO devset results. ery (P j , Q) pair, generating J candidate answer spans, one from each passage. Then, we multiply the SAN score of each candidate answer span with its relevance score r(P j , Q) assigned by a passage ranker, and output the span with the maximum score as the answer. In our experiments, we use the passage ranker described in (Liu et al., 2018)5 . The ranker is trained on the same MS MARCO training data, and achieves 37.1 p@1 on the development set. The results in Table 7 show th"
P18-5002,P17-5004,0,0.0614004,"Missing"
P18-5002,N16-1014,1,0.6779,"ng for Dialogue. tion (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016). Such end-to-end models have been particularly successful with social bot scenarios, as they require little interaction with the user’s environment (no need for API calls) and such models cope well with free-form and open domain texts. However, neural responses are often too general to carry meaningful information, e.g., with the common response “I don’t know” which can serve as a reply to most user questions. A mutual information model is proposed by (Li et al., 2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al.(Li et al., 2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots are originally developed for different purposes, there is a trend of combining both as a step towards building an open-domain dialogue agent. For example, on the one hand, (Ghazvininejad et al., 2018) presented a fully data-driven and knowledge-grounded neural conversation model aimed at producing more contentfu"
P18-5002,P16-1094,1,0.803013,"ng for Dialogue. tion (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016). Such end-to-end models have been particularly successful with social bot scenarios, as they require little interaction with the user’s environment (no need for API calls) and such models cope well with free-form and open domain texts. However, neural responses are often too general to carry meaningful information, e.g., with the common response “I don’t know” which can serve as a reply to most user questions. A mutual information model is proposed by (Li et al., 2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al.(Li et al., 2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots are originally developed for different purposes, there is a trend of combining both as a step towards building an open-domain dialogue agent. For example, on the one hand, (Ghazvininejad et al., 2018) presented a fully data-driven and knowledge-grounded neural conversation model aimed at producing more contentfu"
P18-5002,D16-1127,1,0.858377,"ng for Dialogue. tion (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016). Such end-to-end models have been particularly successful with social bot scenarios, as they require little interaction with the user’s environment (no need for API calls) and such models cope well with free-form and open domain texts. However, neural responses are often too general to carry meaningful information, e.g., with the common response “I don’t know” which can serve as a reply to most user questions. A mutual information model is proposed by (Li et al., 2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c). Furthermore, Li et al.(Li et al., 2016b) presented a persona-based model to address the issue of speaker consistency in neural response generation. Although task-oriented dialogue systems and social bots are originally developed for different purposes, there is a trend of combining both as a step towards building an open-domain dialogue agent. For example, on the one hand, (Ghazvininejad et al., 2018) presented a fully data-driven and knowledge-grounded neural conversation model aimed at producing more contentfu"
P18-5002,P18-1203,1,0.724112,"l networks, so that they can be jointly optimized from user feedback signals using backpropagation and RL. The second is the use of advanced RL techniques to optimize dialogue policies in more complex scenarios. Examples include improved efficiency of exploration for faster learning, and hierarchical problem solving for composite-task dialogues where the reward signal is particularly sparse. We review several recent proposals, including the ones based on Bayesian models, curiosity-driven strategy, hierarchical reinforcement learning, adversarial learning, and the Dyna framework (Sutton, 1990; Peng et al., 2018) to integrate planning and learning, etc. We end this section by presenting a few example task-oriented systems from some of the leading players in the industry, including Microsoft’s Cortana, Amazon’s Alexa and Google’s Assistant. 2.4 Fully Data-Driven Conversation Models and Social Bots Social bots (also known as chatbots) are of growing importance in facilitating smooth interaction between humans and their electronic devices. Recently, researchers have begun to explore fully data-driven generation of conversational responses within the framework of neural machine transla3 Contributions and"
P18-5002,N15-1020,1,0.847016,"l task-oriented dialogue system. It consists of (1) a natural language understanding (NLU) 3 dialogue QA task-oriented chatbot top-level bot state understanding of user query intent understanding of user goal conversation history and user intent understanding of user top-level intent action clarification questions or answers dialogue-act and slot/value reward relevance of answer # of dialogue turns task success rate # of dialogue turns response user engagement options daily/monthly usage Table 2: Reinforcement Learning for Dialogue. tion (NMT) in the form of encoder-decoder or seq2seq models (Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016). Such end-to-end models have been particularly successful with social bot scenarios, as they require little interaction with the user’s environment (no need for API calls) and such models cope well with free-form and open domain texts. However, neural responses are often too general to carry meaningful information, e.g., with the common response “I don’t know” which can serve as a reply to most user questions. A mutual information model is proposed by (Li et al., 2016a), and is later improved by using deep reinforcement learning (Li et al., 2016c)."
P18-5002,N15-4004,1,0.881835,"Missing"
P18-5002,W17-5505,0,0.0375882,"Missing"
P19-1200,C18-1142,0,0.0208925,"the latent variable z. In the double-variable model ml-VAE-D, the feature vector is transformed with two MLP layers, and then is used to compute the mean and variance of the top-level latent variable. 4 Related Work VAE for text generation. VAEs trained under the neural variational inference (NVI) framework, has been widely used for generating text sequences: (Bowman et al., 2016; Yang et al., 2017; Semeniuta et al., 2017; Miao et al., 2016; Serban et al., 2017; Miao et al., 2017; Zhao et al., 2017; Shen et al., 2017; Guu et al., 2018; Kim et al., 2018; Yin et al., 2018; Kaiser et al., 2018; Bahuleyan et al., 2018; Chen et al., 2018b; Deng et al., 2018; Shah and Barber, 2018). By encouraging the latent feature space to match a prior distribution within an encoderdecoder architecture, the learned latent variable could potentially encode high-level semantic features and serve as a global representation during the decoding process (Bowman et al., 2016). The generated results are also endowed with better diversity due to the sampling procedure of the latent codes (Zhao et al., 2017). Generative Adversarial Networks (GANs) (Yu et al., 2017; Hu et al., 2017; Zhang et al., 2017; Fedus et al., 2018; Chen et al"
P19-1200,K16-1002,0,0.308159,"Missing"
P19-1200,J82-2003,0,0.737043,"Missing"
P19-1200,D18-1020,0,0.0459969,"In the double-variable model ml-VAE-D, the feature vector is transformed with two MLP layers, and then is used to compute the mean and variance of the top-level latent variable. 4 Related Work VAE for text generation. VAEs trained under the neural variational inference (NVI) framework, has been widely used for generating text sequences: (Bowman et al., 2016; Yang et al., 2017; Semeniuta et al., 2017; Miao et al., 2016; Serban et al., 2017; Miao et al., 2017; Zhao et al., 2017; Shen et al., 2017; Guu et al., 2018; Kim et al., 2018; Yin et al., 2018; Kaiser et al., 2018; Bahuleyan et al., 2018; Chen et al., 2018b; Deng et al., 2018; Shah and Barber, 2018). By encouraging the latent feature space to match a prior distribution within an encoderdecoder architecture, the learned latent variable could potentially encode high-level semantic features and serve as a global representation during the decoding process (Bowman et al., 2016). The generated results are also endowed with better diversity due to the sampling procedure of the latent codes (Zhao et al., 2017). Generative Adversarial Networks (GANs) (Yu et al., 2017; Hu et al., 2017; Zhang et al., 2017; Fedus et al., 2018; Chen et al., 2018a), is anoth"
P19-1200,N16-1012,0,0.0303612,"rious text processing tasks (Semeniuta et al., 2017; Zhao et al., 2017; Kim et al., 2018; Du et al., 2018; Hashimoto et al., 2018; Shen et al., 2018a; Xu and Durrett, 2018; Wang et al., 2019). While most recent work has focused on generating relatively short sequences (e.g., a single sentence or multiple sentences up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most text VAE models (Bowman et al., 2016). However, it may be difficult to scale RNNs for long-form text generation, as they tend to generate text that is repetitive, ungrammatical, selfcontradictory, overly generic and often lacking coherent long-term structure (Holtzman et al., 2018). Two samples of text generated using standard VAE with an RNN decoder is shown in Table 1. In this work, we propose various multi-level network structures for the VAE model (ml-VAE), to address coherency and repetitiveness challenges associated with long-form text generation. To gener"
P19-1200,D18-1354,0,0.0382184,"set. The standard model struggles with repetitions of the same context or words (in blue), yielding non-coherent text. A hierarhical decoder with multi-layered latent variables eliminates redundancy and yields more coherent text planned around focused concepts. Introduction The variational autoencoder (VAE) for text (Bowman et al., 2016) is a generative model in which a stochastic latent variable provides additional information to modulate the sequential text-generation process. VAEs have been used for various text processing tasks (Semeniuta et al., 2017; Zhao et al., 2017; Kim et al., 2018; Du et al., 2018; Hashimoto et al., 2018; Shen et al., 2018a; Xu and Durrett, 2018; Wang et al., 2019). While most recent work has focused on generating relatively short sequences (e.g., a single sentence or multiple sentences up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most text VAE models (Bowman et al., 2016). However, it ma"
P19-1200,N19-1021,1,0.837599,"ergence annealing and word dropout, however, none of them help to improve the perplexity compared to a plain neural language model. (Yang et al., 2017) argue that the small KL term relates to the strong autoregressive nature of an LSTM generative network, and they proposed to utilize a dilated CNN as a decoder to improve the informativeness of the latent variable. (Zhao et al., 2018b) proposed to augment the VAE training objective with an additional mutual information term. This yields an intractable integral in the case where the latent variables are continuous. Recent work (He et al., 2019; Fu et al., 2019) has shown that advanced scheduling can mitigate the posterior collapse issue. We instead introduce more flexible priors and hierarchical encoder and decoder structures to deal with posterior collapse. Hierarchical Structures. Natural language is inherently hierarchical (characters form a word, words form a sentence, sentences form a paragraph, paragraphs from a document, etc.). Previous work used multi-level LSTM encoders (Yang et al., 2016) or hierarchical autoencoders (Li et al., 2015a) to learn hierarchical representations for long text or defined a stochastic latent variable for each sent"
P19-1200,Q18-1031,0,0.032465,"aph feature vector into the linear layers to infer the mean and variance of the latent variable z. In the double-variable model ml-VAE-D, the feature vector is transformed with two MLP layers, and then is used to compute the mean and variance of the top-level latent variable. 4 Related Work VAE for text generation. VAEs trained under the neural variational inference (NVI) framework, has been widely used for generating text sequences: (Bowman et al., 2016; Yang et al., 2017; Semeniuta et al., 2017; Miao et al., 2016; Serban et al., 2017; Miao et al., 2017; Zhao et al., 2017; Shen et al., 2017; Guu et al., 2018; Kim et al., 2018; Yin et al., 2018; Kaiser et al., 2018; Bahuleyan et al., 2018; Chen et al., 2018b; Deng et al., 2018; Shah and Barber, 2018). By encouraging the latent feature space to match a prior distribution within an encoderdecoder architecture, the learned latent variable could potentially encode high-level semantic features and serve as a global representation during the decoding process (Bowman et al., 2016). The generated results are also endowed with better diversity due to the sampling procedure of the latent codes (Zhao et al., 2017). Generative Adversarial Networks (GANs) (Yu"
P19-1200,P15-1107,0,0.57083,"s demonstrated in (Bowman et al., 2016) that due to the autoregressive nature of the RNN, the decoder tends to ignore the information from z entirely, resulting in an extremely small KL term (see Section 4). 3 3.1 Multi-Level Generative Networks Single Latent Variable (ml-VAE-S:) Our first multi-level model improves upon standard VAE models by introducing a plan-ahead ability to sequence generation. Instead of directly making word-level predictions only conditioned on the semantic information from z, a series of plan vectors are first generated based upon z with a sentence-level LSTM decoder (Li et al., 2015b). Our hypothesis is that an explicit design of (inherently hierarchical) paragraph structure can capture sentence-level coherence and potentially mitigate repetitiveness. Intuitively, when predicting each token, the decoder can use information from 2080 Inference (Encoder) Network Higher Level CNN I love this place. ? ?? Word-Level LSTM Decoder ?? ? ? Lower Level CNN ? I love this place. KL Losses Sentence Level LSTM Decoder ?? ?? Lots of veggie options. Try veggie quesadilla. … … ? ? ?? … ?? ?? Generative (Decoder) Network Lots of veggie options. Try veggie quesadilla. Figure 1: The propose"
P19-1200,P02-1040,0,0.104322,"bution. As a result, the model is endowed with more flexibility to encode informative semantic features in the latent variables, yet matching their posterior distributions to the corresponding priors. ml-VAE-D achieves the best PPL results on both datasets (on the arXiv dataset, our hierarchical decoder outperforms the ml-LM by reducing the PPL from 58.1 down to 54.3). 5.3 Unconditional Text Generation We evaluate the quality of generated paragraphs as follows. We randomly sample 1000 latent codes and send them to all trained generative models to generate text. We use corpus-level BLEU score (Papineni et al., 2002) to quantitatively evaluate the generated paragraphs. Following strategy in (Yu et al., 2017; Zhang et al., 2017) we use the entire test set as the reference for each generated Figure 2: t-SNE visualization of the learned latent codes. text, and get the average BLEU scores6 over 1000 generated sentences for each model. The results are in Table 3. VAE tends to be a stronger baseline for paragraph generation, exhibiting higher corpus-level BLEU scores than both AAE and ARAE. This observation is consistent with the results in (C´ıfka et al., 2018) in Table 3. The VAE with multi-level decoder demo"
P19-1200,N18-1162,0,0.147556,"the corresponding plan vector via an MLP layer. V represents the weight matrix for computing distribution over words, and We are word embeddings to be learned. For each sentence, once the special END token is generated, the word-level 1 We use teacher-forcing during training and greedy decoding at test time. LSTM stops decoding 2 . LSTMword decoder parameters are shared for each generated sentence. 3.2 Double Latent Variables (ml-VAE-D): Similar architectures of our single latent variable ml-VAE-S model have been applied recently for multi-turn dialog response generation (Serban et al., 2017; Park et al., 2018), mainly focusing on short (one-sentence) response generation. Different from these works, our goal is to generate long text which introduces additional challenges to the hierarchical generative network. We hypothesize that with the two-level LSTM decoder embedded into the VAE framework, the load of capturing global and local semantics are handled differently than the flat-VAEs (Chen et al., 2016). While the multi-level LSTM decoder can capture relatively detailed information (e.g., word-level (local) coherence) via the word- and sentence-level LSTM networks, the latent codes of the VAE are en"
P19-1200,D17-1066,0,0.400135,"generated from two generative models on the Yelp reviews dataset. The standard model struggles with repetitions of the same context or words (in blue), yielding non-coherent text. A hierarhical decoder with multi-layered latent variables eliminates redundancy and yields more coherent text planned around focused concepts. Introduction The variational autoencoder (VAE) for text (Bowman et al., 2016) is a generative model in which a stochastic latent variable provides additional information to modulate the sequential text-generation process. VAEs have been used for various text processing tasks (Semeniuta et al., 2017; Zhao et al., 2017; Kim et al., 2018; Du et al., 2018; Hashimoto et al., 2018; Shen et al., 2018a; Xu and Durrett, 2018; Wang et al., 2019). While most recent work has focused on generating relatively short sequences (e.g., a single sentence or multiple sentences up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most"
P19-1200,P18-1152,0,0.0229683,"ces up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most text VAE models (Bowman et al., 2016). However, it may be difficult to scale RNNs for long-form text generation, as they tend to generate text that is repetitive, ungrammatical, selfcontradictory, overly generic and often lacking coherent long-term structure (Holtzman et al., 2018). Two samples of text generated using standard VAE with an RNN decoder is shown in Table 1. In this work, we propose various multi-level network structures for the VAE model (ml-VAE), to address coherency and repetitiveness challenges associated with long-form text generation. To generate globally-coherent long text sequences, it is desirable that both the higher-level abstract features (e.g., topic, sentiment, etc.) and lowerlevel fine-granularity details (e.g., specific word choices) of long text can be leveraged by the generative network. It’s difficult for a standard 2079 Proceedings of th"
P19-1200,H93-1035,0,0.58467,"Missing"
P19-1200,P18-1190,1,0.925113,"petitions of the same context or words (in blue), yielding non-coherent text. A hierarhical decoder with multi-layered latent variables eliminates redundancy and yields more coherent text planned around focused concepts. Introduction The variational autoencoder (VAE) for text (Bowman et al., 2016) is a generative model in which a stochastic latent variable provides additional information to modulate the sequential text-generation process. VAEs have been used for various text processing tasks (Semeniuta et al., 2017; Zhao et al., 2017; Kim et al., 2018; Du et al., 2018; Hashimoto et al., 2018; Shen et al., 2018a; Xu and Durrett, 2018; Wang et al., 2019). While most recent work has focused on generating relatively short sequences (e.g., a single sentence or multiple sentences up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most text VAE models (Bowman et al., 2016). However, it may be difficult to scale RNNs for long-form"
P19-1200,D18-1480,0,0.0721031,"e context or words (in blue), yielding non-coherent text. A hierarhical decoder with multi-layered latent variables eliminates redundancy and yields more coherent text planned around focused concepts. Introduction The variational autoencoder (VAE) for text (Bowman et al., 2016) is a generative model in which a stochastic latent variable provides additional information to modulate the sequential text-generation process. VAEs have been used for various text processing tasks (Semeniuta et al., 2017; Zhao et al., 2017; Kim et al., 2018; Du et al., 2018; Hashimoto et al., 2018; Shen et al., 2018a; Xu and Durrett, 2018; Wang et al., 2019). While most recent work has focused on generating relatively short sequences (e.g., a single sentence or multiple sentences up to around twenty words), generating long-form text (e.g., a single or multiple ⇤ This research was carried out during an internship at Microsoft Research. paragraphs) with deep latent-variable models has been less explored. Recurrent Neural Networks (RNNs) (Bahdanau et al., 2015; Chopra et al., 2016) have mainly been used for most text VAE models (Bowman et al., 2016). However, it may be difficult to scale RNNs for long-form text generation, as the"
P19-1200,N16-1174,0,0.0554587,"dditional mutual information term. This yields an intractable integral in the case where the latent variables are continuous. Recent work (He et al., 2019; Fu et al., 2019) has shown that advanced scheduling can mitigate the posterior collapse issue. We instead introduce more flexible priors and hierarchical encoder and decoder structures to deal with posterior collapse. Hierarchical Structures. Natural language is inherently hierarchical (characters form a word, words form a sentence, sentences form a paragraph, paragraphs from a document, etc.). Previous work used multi-level LSTM encoders (Yang et al., 2016) or hierarchical autoencoders (Li et al., 2015a) to learn hierarchical representations for long text or defined a stochastic latent variable for each sentence at decoding time (Serban et al., 2017). In contrast, our model encodes the entire paragraph into one single latent variable. The latent variable learned in our model relates more to the global semantic information of a paragraph, whereas those in (Serban et al., 2017) mainly contain the local information of a specific sentence. Park et al.(Park et al., 2018) introduced a variational hierarchical conversational model (VHCR) with global an"
P19-1200,P18-1070,0,0.0239578,"ayers to infer the mean and variance of the latent variable z. In the double-variable model ml-VAE-D, the feature vector is transformed with two MLP layers, and then is used to compute the mean and variance of the top-level latent variable. 4 Related Work VAE for text generation. VAEs trained under the neural variational inference (NVI) framework, has been widely used for generating text sequences: (Bowman et al., 2016; Yang et al., 2017; Semeniuta et al., 2017; Miao et al., 2016; Serban et al., 2017; Miao et al., 2017; Zhao et al., 2017; Shen et al., 2017; Guu et al., 2018; Kim et al., 2018; Yin et al., 2018; Kaiser et al., 2018; Bahuleyan et al., 2018; Chen et al., 2018b; Deng et al., 2018; Shah and Barber, 2018). By encouraging the latent feature space to match a prior distribution within an encoderdecoder architecture, the learned latent variable could potentially encode high-level semantic features and serve as a global representation during the decoding process (Bowman et al., 2016). The generated results are also endowed with better diversity due to the sampling procedure of the latent codes (Zhao et al., 2017). Generative Adversarial Networks (GANs) (Yu et al., 2017; Hu et al., 2017; Zhang"
P19-1200,P18-1101,0,0.117326,"ng relatively longer units of text has been less explored. Optimization Challenges. The “posterior collapse” issue associated with training text-VAEs was first outlined by (Bowman et al., 2016). They 2082 used two strategies, KL divergence annealing and word dropout, however, none of them help to improve the perplexity compared to a plain neural language model. (Yang et al., 2017) argue that the small KL term relates to the strong autoregressive nature of an LSTM generative network, and they proposed to utilize a dilated CNN as a decoder to improve the informativeness of the latent variable. (Zhao et al., 2018b) proposed to augment the VAE training objective with an additional mutual information term. This yields an intractable integral in the case where the latent variables are continuous. Recent work (He et al., 2019; Fu et al., 2019) has shown that advanced scheduling can mitigate the posterior collapse issue. We instead introduce more flexible priors and hierarchical encoder and decoder structures to deal with posterior collapse. Hierarchical Structures. Natural language is inherently hierarchical (characters form a word, words form a sentence, sentences form a paragraph, paragraphs from a docu"
P19-1200,P17-1061,0,0.115931,"Missing"
P19-1441,D15-1075,0,0.694245,"in 1, 2, ..., T do Pack the dataset t into mini-batch: Dt . end for epoch in 1, 2, ..., epochmax do 1. Merge all the datasets: D = D1 ∪ D2 ... ∪ DT 2. Shuffle D for bt in D do //bt is a mini-batch of task t. 3. Compute loss : L(Θ) L(Θ) = Eq. 6 for classification L(Θ) = Eq. 7 for regression L(Θ) = Eq. 8 for ranking 4. Compute gradient: ∇(Θ) 5. Update model: Θ = Θ − ∇(Θ) end end SNLI The Stanford Natural Language Inference (SNLI) dataset contains 570k human annotated sentence pairs, in which the premises are drawn from the captions of the Flickr30 corpus and hypotheses are manually annotated (Bowman et al., 2015b). This is the most widely used entailment dataset for NLI. The dataset is used only for domain adaptation in this study. and |A |− 1 negative examples. We then minimize the negative log likelihood of the positive example given queries across the training data X − Pr (A+ |Q), (8) (Q,A+ ) SciTail This is a textual entailment dataset derived from a science question answering (SciQ) dataset (Khot et al., 2018). The task involves assessing whether a given premise entails a given hypothesis. In contrast to other entailment datasets mentioned previously, the hypotheses in SciTail are created from s"
P19-1441,P18-2103,0,0.0508588,"Missing"
P19-1441,P18-1064,0,0.0268376,"om previous tasks to help learn a new task (Caruana, 1997; Zhang and Yang, 2017). For example, it is easier for a person who knows how to ski to learn skating than the one who ∗ 1 Equal Contribution. As of February 25, 2019 on the latest GLUE test set. does not. Similarly, it is useful for multiple (related) tasks to be learned jointly so that the knowledge learned in one task can benefit other tasks. Recently, there is a growing interest in applying MTL to representation learning using deep neural networks (DNNs) (Collobert et al., 2011; Liu et al., 2015; Luong et al., 2015; Xu et al., 2018; Guo et al., 2018; Ruder12 et al., 2019) for two reasons. First, supervised learning of DNNs requires large amounts of task-specific labeled data, which is not always available. MTL provides an effective way of leveraging supervised data from many related tasks. Second, the use of multi-task learning profits from a regularization effect via alleviating overfitting to a specific task, thus making the learned representations universal across tasks. In contrast to MTL, language model pretraining has shown to be effective for learning universal language representations by leveraging large amounts of unlabeled data"
P19-1441,N15-1092,1,0.453968,"ities where people often apply the knowledge learned from previous tasks to help learn a new task (Caruana, 1997; Zhang and Yang, 2017). For example, it is easier for a person who knows how to ski to learn skating than the one who ∗ 1 Equal Contribution. As of February 25, 2019 on the latest GLUE test set. does not. Similarly, it is useful for multiple (related) tasks to be learned jointly so that the knowledge learned in one task can benefit other tasks. Recently, there is a growing interest in applying MTL to representation learning using deep neural networks (DNNs) (Collobert et al., 2011; Liu et al., 2015; Luong et al., 2015; Xu et al., 2018; Guo et al., 2018; Ruder12 et al., 2019) for two reasons. First, supervised learning of DNNs requires large amounts of task-specific labeled data, which is not always available. MTL provides an effective way of leveraging supervised data from many related tasks. Second, the use of multi-task learning profits from a regularization effect via alleviating overfitting to a specific task, thus making the learned representations universal across tasks. In contrast to MTL, language model pretraining has shown to be effective for learning universal language repres"
P19-1441,P18-1157,1,0.906088,"ation of the input sentence pair (X1 , X2 ). We introduce a task-specific parameter vector wST S to compute the similarity score as: 4489 &gt; Sim(X1 , X2 ) = wST S · x, (2) where Sim(X1 , X2 ) is a real value of the range (∞, ∞). Pairwise Text Classification Output: Take natural language inference (NLI) as an example. The NLI task defined here involves a premise P = (p1 , ..., pm ) of m words and a hypothesis H = (h1 , ..., hn ) of n words, and aims to find a logical relationship R between P and H. The design of the output module follows the answer module of the stochastic answer network (SAN) (Liu et al., 2018a), a state-of-the-art neural NLI model. SAN’s answer module uses multi-step reasoning. Rather than directly predicting the entailment given the input, it maintains a state and iteratively refines its predictions. The SAN answer module works as follows. We first construct the working memory of premise P by concatenating the contextual embeddings of the words in P , which are the output of the transformer encoder, denoted as Mp ∈ Rd×m , and similarly the working memory of hypothesis H, denoted as Mh ∈ Rd×n . Then, we perform K-step reasoning on the memory to output the relation label, where K i"
P19-1441,N18-1202,0,0.181058,"of task-specific labeled data, which is not always available. MTL provides an effective way of leveraging supervised data from many related tasks. Second, the use of multi-task learning profits from a regularization effect via alleviating overfitting to a specific task, thus making the learned representations universal across tasks. In contrast to MTL, language model pretraining has shown to be effective for learning universal language representations by leveraging large amounts of unlabeled data. A recent survey is included in Gao et al. (2018). Some of the most prominent examples are ELMo (Peters et al., 2018), GPT (Radford et al., 2018) and BERT (Devlin et al., 2018). These are neural network language models trained on text data using unsupervised objectives. For example, BERT is based on a multi-layer bidirectional Transformer, and is trained on plain text for masked word prediction and next sentence prediction tasks. To apply a pre-trained model to specific NLU tasks, we often need to fine-tune, for each task, the model with additional task-specific layers using task-specific training data. For example, Devlin et al. (2018) shows that BERT can be fine-tuned this way to create state-of-the-art mo"
P19-1441,D16-1264,0,0.0923474,"onship of the two sentences based on a set of pre-defined labels. For example, both RTE and MNLI are language inference tasks, where the goal is to predict whether a sentence is an entailment, contradiction, or neutral with respect to the other. QQP and MRPC are paraphrase datasets that consist of sentence pairs. The task is to predict whether the sentences in the pair are semantically equivalent. Relevance Ranking: Given a query and a list of candidate answers, the model ranks all the candidates in the order of relevance to the query. QNLI is a version of Stanford Question Answering Dataset (Rajpurkar et al., 2016). The task involves assessing whether a sentence contains the correct answer to a given query. Although QNLI is defined as a binary classification task in GLUE, in this study we formulate it as a pairwise ranking task, where the model is expected to rank the candidate that contains the correct answer higher than the candidate that does not. We will show that this formulation leads to a significant improvement in accuracy over binary classification. 3 The MT-DNN model combines four types of NLU tasks: single-sentence classification, pairwise text classification, text similarity scoring, and rel"
P19-1441,W19-4810,0,0.0350359,"Missing"
P19-1441,W18-5446,0,0.121984,"Missing"
P19-1539,W18-5709,0,0.13562,"round responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse abo"
P19-1539,D18-1241,1,0.822774,"ent for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018). These models differ in how they fuse information between questions and documents. We chose SAN (Liu et al., 2018b) because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018), can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these “single-turn” MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018)) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work—like much of the prior work in end-to-end dialogue—models free-form dialogue, which also encompasses chitchat and non-factual responses. 8 Conclusions We have demonstrated that the machine reading comprehension approach offers a promising step to generating, on the fly, contentful conversation exchanges that are grounded in extended text corpora. The functional combination of MRC"
P19-1539,D18-1045,0,0.0185929,"model training, with an initial learning rate of 0.0005. Batch size was set to 32. During training, all responses were truncated to have a maximum length of 30, and maximum query length and document length were set to 30, 500, respectively. we used regular teacher-forcing decoding during training. For inference, we found that top-k random sample decoding (Fan et al., 2018) provides the best results for all the systems. That is, at each decoding step, a token was drawn from the k most likely candidates according to the distribution over the vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history by using standard Machine Translation (MT) metrics, comparing generated outputs to ground-truth responses. These metrics include B LEU-4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007). and N IST (Doddington, 2002). The latter metric i"
P19-1539,P18-1082,0,0.0333813,"used the pretrained GloVe8 for initialization. We set hidden dimensions to 512 and dropout rate to 0.4. GRU cells are used for S EQ 2S EQ and M EM N ET (we also tested LSTM cells and obtained similar results). We used the Adam optimizer for model training, with an initial learning rate of 0.0005. Batch size was set to 32. During training, all responses were truncated to have a maximum length of 30, and maximum query length and document length were set to 30, 500, respectively. we used regular teacher-forcing decoding during training. For inference, we found that top-k random sample decoding (Fan et al., 2018) provides the best results for all the systems. That is, at each decoding step, a token was drawn from the k most likely candidates according to the distribution over the vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history b"
P19-1539,N19-1125,1,0.868164,"Missing"
P19-1539,W07-0734,0,0.0141127,"vocabulary. Similar to recent work (Fan et al., 2018; Edunov et al., 2018), we set k = 20 (other common k values like 10 gave similar results). We selected key hyperparameter configurations on the validation set. 6.1 Evaluation Setup Table 2 shows automatic metrics for quantitative evaluation over three qualities of generated texts. We measure the overall relevance of the generated responses given the conversational history by using standard Machine Translation (MT) metrics, comparing generated outputs to ground-truth responses. These metrics include B LEU-4 (Papineni et al., 2002), M ETEOR (Lavie and Agarwal, 2007). and N IST (Doddington, 2002). The latter metric is a variant of B LEU that weights n-gram matches by their information gain by effectively penalizing uninformative n-grams (such as “I don’t know”), which makes it a relevant metric for evaluating systems aiming diverse and informative responses. MT metrics may not be particularly adequate for our task (Liu et al., 2016), given its focus on the informativeness of responses, and for that reason we also use two other types of metrics to measure the level of grounding and diversity. As a diversity metric, we count all n-grams in the system output"
P19-1539,N16-1014,1,0.92711,"s recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning"
P19-1539,P16-1094,1,0.958274,"s recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning"
P19-1539,D16-1230,0,0.0886793,"Missing"
P19-1539,P18-1138,0,0.391323,"tributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where"
P19-1539,P18-1157,1,0.938959,"tributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where"
P19-1539,D15-1166,0,0.0725271,"n layer is applied to further ingest and capture the most salient information. The output memory, M ∈ Rd×n , is obtained by applying another BiLSTM layer for final information rearrangement. Note that d is the hidden size of the memory and n is the length of the document. 3.2 Response Generation Having read and processed both the conversation history and the extra knowledge in the document, the model then produces a free-form response y = (y1 , . . . , yT ) instead of generating a span or performing answer classification as in MRC tasks. We use an attentional recurrent neural network decoder (Luong et al., 2015) to generate response tokens while attending to the memory. At the beginning, the initial hidden state h0 is the weighted sum of the representation of the history X. For each decoding step t with a hidden state ht , we generate a token yt based on the distribution: p(yt ) = softmax((W1 ht + b)/τ ), (1) where τ > 0 is the softmax temperature. The hidden state ht is defined as follows: ht = W2 [zt ++fattention (zt , M )]. (2) Here, [· ++·] indicates a concatenation of two vectors; fattention is a dot-product attention (Vaswani et al., 2017); and zt is a state generated by GRU(et−1 , ht−1 ) with"
P19-1539,D18-1255,0,0.363735,"d-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where people often search and acquire external information as needed to"
P19-1539,I17-1047,1,0.933114,"Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al., 2017). However, empirical results suggest that conditioning the decoder on rich and complex contexts, while helpful, does not on its own provide sufficient inductive bias for these systems to learn how to achieve deep and accurate integration between external knowledge and response generation. We posit that this ongoing challenge demands a more effective mechanism to support on-demand knowledge integration. We draw inspiration from how humans converse about a topic, where people often search and acquire external information as needed to continue a meaningful and informative conversation. Figure 1 i"
P19-1539,P02-1040,0,0.106889,"e decoding to draw yt from the above distribution p(yt ). Section 5 provides more details about the experimental configuration. 3.3 Data Weighting Scheme We further propose a simple data weighting scheme to encourage the generation of grounded responses. The idea is to bias the model training to fit better to those training instances where the ground-truth response is more closely relevant to the document. More specifically, given a training instance (X, D, y), we measure the closeness score c ∈ R between the document D and the gold response y (e.g., with the NIST (Doddington, 2002) or B LEU (Papineni et al., 2002) metrics). In each training data batch, we normalize the closeness scores of all the instances to have a sum of 1, and weight each of the instances with its corresponding normalized score when evaluating the 5429 # dialogues # utterances # documents # document sentences Train Valid Test 28.4k 2.36M 28.4k 15.18M 1.2k 0.12M 1.2k 0.58M 3.1k 0.34M 3.1k 1.68M 18.84 14.17 18.48 14.15 Average length (# words): utterances 18.74 document sentences 13.72 Table 1: Our grounded conversational dataset. training loss. This training regime promotes instances with grounded responses and thus encourages the mo"
P19-1539,D16-1264,0,0.230968,"ble at https://github.com/qkaren/ converse_reading_cmr. of turns X = (x1 , . . . , xM ) and a web document D = (s1 , . . . , sN ) as the knowledge source, where si is the ith sentence in the document. With the pair (X, D), the system needs to generate a natural language response y that is both conversationally appropriate and reflective of the contents of the web document. 3 Approach Our approach integrates conversation generation with on-demand MRC. Specifically, we use an MRC model to effectively encode the conversation history by treating it as a question in a typical QA task (e.g., SQuAD (Rajpurkar et al., 2016)), and encode the web document as the context. We then replace the output component of the MRC model (which is usually an answer classification module) with an attentional sequence generator that generates a free-form response. We refer to our approach as CMR (Conversation with on-demand Machine Reading). In general, any off-the-shelf MRC model could be applied here for knowledge comprehension. We use Stochastic Answer Networks (SAN)2 (Liu et al., 2018b), a performant machine reading model that until very recently held state-of-the-art performance on the SQuAD benchmark. We also employ a simpl"
P19-1539,Q19-1016,0,0.0224858,"ng meaning. from a given document for a given question (Seo et al., 2017; Liu et al., 2018b; Yu et al., 2018). These models differ in how they fuse information between questions and documents. We chose SAN (Liu et al., 2018b) because of its representative architecture and competitive performance on existing MRC tasks. We note that other off-theshelf MRC models, such as BERT (Devlin et al., 2018), can also be plugged in. We leave the study of different MRC architectures for future work. Questions are treated as entirely independent in these “single-turn” MRC models, so recent work (e.g., CoQA (Reddy et al., 2019) and QuAC (Choi et al., 2018)) focuses on multi-turn MRC, modeling sequences of questions and answers in a conversation. While multi-turn MRC aims to answer complex questions, that body of work is restricted to factual questions, whereas our work—like much of the prior work in end-to-end dialogue—models free-form dialogue, which also encompasses chitchat and non-factual responses. 8 Conclusions We have demonstrated that the machine reading comprehension approach offers a promising step to generating, on the fly, contentful conversation exchanges that are grounded in extended text corpora. The"
P19-1539,P15-1152,0,0.0338416,"fall without a parachute: 10,160 metres (33,330 ft). …… …… In 2005, Vulović‘s fall was recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017;"
P19-1539,P18-1205,0,0.0465991,"compared to machine translation, it is common for the generator to retain focus on the key information in the external document to produce semantically relevant responses. 7 Related Work Dialogue: Traditional dialogue systems (see (Jurafsky and Martin, 2009) for an historical perspective) are typically grounded, enabling these systems to be reflective of the user’s environment. The lack of grounding has been a stumbling block for the earliest end-to-end dialogue systems, as various researchers have noted that their outputs tend to be bland (Li et al., 2016a; Gao et al., 2019b), inconsistent (Zhang et al., 2018a; Li et al., Figure 3: Attention weights between words of the documents and words of the response. Dark (blue) cells represent probabilities closer to 1. 2016b; Zhang et al., 2019), and lacking in factual content (Ghazvininejad et al., 2018; Agarwal et al., 2018). Recently there has been growing interest in exploring different forms of grounding, including images, knowledge bases, and plain texts (Das et al., 2017; Mostafazadeh et al., 2017; Agarwal et al., 2018; Yang et al., 2019). A recent survey is included in Gao et al. (2019a). Prior work, e.g, (Ghazvininejad et al., 2018; Zhang et al.,"
P19-1539,N15-1020,1,0.77954,"hute: 10,160 metres (33,330 ft). …… …… In 2005, Vulović‘s fall was recreated by the American television MythBusters. Four years later, […] two Praguebased journalists, claimed that Flight 367 had been mistaken for an enemy aircraft and shot down by the Czechoslovak Air Force at an altitude of 800 metres (2,600 ft). Figure 1: Users discussing a topic defined by a Wikipedia article. In this real-world example from our Reddit dataset, information needed to ground responses is distributed throughout the source document. Introduction While end-to-end neural conversation models (Shang et al., 2015; Sordoni et al., 2015; Vinyals and Le, 2015; Serban et al., 2016; Li et al., 2016a; Gao et al., 2019a, etc.) are effective in learning how to be fluent, their responses are often vacuous and uninformative. A primary challenge thus lies in modeling what to say to make the conversation contentful. Several recent approaches have attempted to address this difficulty by conditioning the language decoder on external information sources, such as knowledge bases (Agarwal et al., 2018; Liu et al., 2018a), review posts (Ghazvininejad et al., 2018; Moghe et al., 2018), and even images (Das et al., 2017; Mostafazadeh et al.,"
P19-1539,N19-1423,0,\N,Missing
P19-1648,D16-1044,0,0.0341009,"tation dt . Now, we use Multimodal Factorized Bilinear pooling (MFB) (Yu et al., 2017c) to fuse v t and dt together. Specifically, z t = SumPooling(Uv v Tt ◦ Ud dTt , k) , (6) 0.5 (7) z t = sign(z t )|z t | , zt = z Tt /||z t ||, where Uv ∈ Rnh k×nh , Ud ∈ Rnh k×nh . The function SumPooling(x, k) in (6) means using a onedimensional non-overlapped window with the size k to perform sum pooling over x. (7) performs power normalization and `2 normalization. The whole process is denoted in short as: z t = MFB(v t , dt ) ∈ R1×nh . (8) There are also other methods for multimodal fusion, such as MCB (Fukui et al., 2016) and MLB (Kim et al., 2017). We use MFB in this paper due to its superior performance in VQA. Image and History Updating RNN State The initial state s0 is set to q, which represents the initial understanding of the question. The question representation is then updated based on the current dialogue history and the image, via an RNN with Gated Recurrent Unit (GRU) (Cho et al., 2014): st+1 = GRU(st , z t ) . (9) This process forms a cycle completing one reasoning step. After performing T steps of reasoning, multimodal fusion is then used to obtain the final context vector: (4) where β ∈ R1×M , pβ"
P19-1648,P18-5002,1,0.846335,"Missing"
P19-1648,D18-1241,0,0.0322098,"ring questions, question sequence generation is also investigated in Jain et al. (2018); Massiceti et al. (2018). For the GuessWhat?! task, various methods (such as RL) have been proposed to improve the performance of dialog agents, measured by task completion rate as in goal-oriented dialog system (Strub et al., 2017; Shekhar et al., 2018; Strub et al., 2018; Lee et al., 2018; Zhang et al., 2018). Other related work includes imagegrounded chitchat (Mostafazadeh et al., 2017), dialog-based image retrieval (Guo et al., 2018), and text-only conversational question answering (Reddy et al., 2018; Choi et al., 2018). A recent survey on neural approaches to dialog modeling can be found in Gao et al. (2018). In this work, we focus on the VisDial task. Different from previous approaches to visual dialog, which all used a single-step reasoning strategy, we propose a novel multi-step reasoning framework that can boost the performance of visual dialog systems by inferring context-relevant information from the image and the dialog history iteratively. Multi-step Reasoning The idea of multi-step reasoning has been explored in many tasks, including image classification (Mnih et al., 2014), text classification (Yu"
P19-1648,P17-1055,0,0.0329599,"pose a novel multi-step reasoning framework that can boost the performance of visual dialog systems by inferring context-relevant information from the image and the dialog history iteratively. Multi-step Reasoning The idea of multi-step reasoning has been explored in many tasks, including image classification (Mnih et al., 2014), text classification (Yu et al., 2017a), image generation (Gregor et al., 2015), language-based image editing (Chen et al., 2018), Visual Question Answering (VQA) (Yang et al., 2016; Nam et al., 2017; Hudson and Manning, 2018), and Machine Reading Comprehension (MRC) (Cui et al., 2017; Dhingra et al., 2017; Hill et al., 2016; Sordoni et al., 2016; Shen et al., 2017; Liu et al., 2018). Specifically, Mnih et al. (2014) introduced an RNN for image classification, by selecting a sequence of regions adaptively and only processing the selected regions. Yu et al. (2017a) used an RNN for text classification, by learning to skip irrelevant information when reading the text input. A recurrent variational autoencoder termed DRAW was proposed in Gregor et al. (2015) for multi-step image generation. A recurrent attentive model for image editing was also proposed in Chen et al. (2018) t"
P19-1648,D19-1209,0,0.0425862,"between image and text for a better understanding of both: on the one hand, the attended image regions can provide additional information for better dialog interpretation; on the other hand, the attended history snippets can be used for better image understanding (see the dotted red lines in Figure 2). Concurrent Work We also include some concurrent work for visual dialog that has not been discussed above, including image-questionanswer synergistic network (Guo et al., 2019), recursive visual attention (Niu et al., 2018), factor graph attention (Schwartz et al., 2019), dual attention network (Kang et al., 2019), graph neural network (Zheng et al., 2019), history-advantage sequence training (Yang et al., 2019), and weighted likelihood estimation (Zhang et al., 2019). 3 Recurrent Dual Attention Network The visual dialog task (Das et al., 2017a) is formulated as follows: given a question Q` grounded in an image I, and previous dialog history (including the image caption C) H` = {C, (Q1 , A1 ), · · · , (Q`−1 , A`−1 )} (` is the current dialog turn) as additional context, the goal is to generate an answer by ranking a list of N candi(1) (N ) date answers A` = {A` , . . . , A` }. Figure 2 provides an over"
P19-1648,P17-1168,0,0.0199444,"-step reasoning framework that can boost the performance of visual dialog systems by inferring context-relevant information from the image and the dialog history iteratively. Multi-step Reasoning The idea of multi-step reasoning has been explored in many tasks, including image classification (Mnih et al., 2014), text classification (Yu et al., 2017a), image generation (Gregor et al., 2015), language-based image editing (Chen et al., 2018), Visual Question Answering (VQA) (Yang et al., 2016; Nam et al., 2017; Hudson and Manning, 2018), and Machine Reading Comprehension (MRC) (Cui et al., 2017; Dhingra et al., 2017; Hill et al., 2016; Sordoni et al., 2016; Shen et al., 2017; Liu et al., 2018). Specifically, Mnih et al. (2014) introduced an RNN for image classification, by selecting a sequence of regions adaptively and only processing the selected regions. Yu et al. (2017a) used an RNN for text classification, by learning to skip irrelevant information when reading the text input. A recurrent variational autoencoder termed DRAW was proposed in Gregor et al. (2015) for multi-step image generation. A recurrent attentive model for image editing was also proposed in Chen et al. (2018) to fuse image and langu"
P19-1648,D17-1230,0,0.030702,"r with a Recurrent Neural Network (RNN) (Das et al., 2017a); and (ii) discriminative decoder to rank answer candidates via a softmaxbased cross-entropy loss (Das et al., 2017a) or a ranking-based multi-class N-pair loss (Lu et al., 2017). Reinforcement Learning (RL) was used in Das et al. (2017b); Chattopadhyay et al. (2017) to train two agents to play image guessing games. Lu et al. (2017) proposed a training schema to effectively transfer knowledge from a pre-trained discriminative model to a generative dialog model. Generative Adversarial Network (Goodfellow et al., 2014; Yu et al., 2017b; Li et al., 2017) was also 6464 used in Wu et al. (2018) to generate answers indistinguishable from human-generated answers, and a conditional variational autoencoder (Kingma and Welling, 2014; Sohn et al., 2015) was developed in Massiceti et al. (2018) to promote answer diversity. There were also studies investigating visual coreference resolution, either via attention memory implicitly (Seo et al., 2017) or using a more explicit reasoning procedure (Kottur et al., 2018) based on neural module networks (Andreas et al., 2016). In addition to answering questions, question sequence generation is also investigate"
P19-1648,P18-1157,1,0.790268,"by inferring context-relevant information from the image and the dialog history iteratively. Multi-step Reasoning The idea of multi-step reasoning has been explored in many tasks, including image classification (Mnih et al., 2014), text classification (Yu et al., 2017a), image generation (Gregor et al., 2015), language-based image editing (Chen et al., 2018), Visual Question Answering (VQA) (Yang et al., 2016; Nam et al., 2017; Hudson and Manning, 2018), and Machine Reading Comprehension (MRC) (Cui et al., 2017; Dhingra et al., 2017; Hill et al., 2016; Sordoni et al., 2016; Shen et al., 2017; Liu et al., 2018). Specifically, Mnih et al. (2014) introduced an RNN for image classification, by selecting a sequence of regions adaptively and only processing the selected regions. Yu et al. (2017a) used an RNN for text classification, by learning to skip irrelevant information when reading the text input. A recurrent variational autoencoder termed DRAW was proposed in Gregor et al. (2015) for multi-step image generation. A recurrent attentive model for image editing was also proposed in Chen et al. (2018) to fuse image and language features via multiple steps. For VQA, Stacked Attention Network (SAN) (Yang"
P19-1648,I17-1047,1,0.767024,"7) or using a more explicit reasoning procedure (Kottur et al., 2018) based on neural module networks (Andreas et al., 2016). In addition to answering questions, question sequence generation is also investigated in Jain et al. (2018); Massiceti et al. (2018). For the GuessWhat?! task, various methods (such as RL) have been proposed to improve the performance of dialog agents, measured by task completion rate as in goal-oriented dialog system (Strub et al., 2017; Shekhar et al., 2018; Strub et al., 2018; Lee et al., 2018; Zhang et al., 2018). Other related work includes imagegrounded chitchat (Mostafazadeh et al., 2017), dialog-based image retrieval (Guo et al., 2018), and text-only conversational question answering (Reddy et al., 2018; Choi et al., 2018). A recent survey on neural approaches to dialog modeling can be found in Gao et al. (2018). In this work, we focus on the VisDial task. Different from previous approaches to visual dialog, which all used a single-step reasoning strategy, we propose a novel multi-step reasoning framework that can boost the performance of visual dialog systems by inferring context-relevant information from the image and the dialog history iteratively. Multi-step Reasoning The"
P19-1648,D14-1162,0,0.0834941,"the val v1.0 split is associated with a 10-turn dialog, while a dialog with a flexible number of turns is provided for each image in teststd v1.0. Each question-answer pair in the VisDial dataset is accompanied by a list of 100 answer candidates, and the goal is to find the correct answer among all the candidates. Preprocessing We truncate captions/questions/ answers that are longer than 40/20/20 words, respectively. And we build a vocabulary of words that occur at least 5 times in train v1.0, resulting in 11, 319 words in the vocabulary. For word embeddings, we use pre-trained GloVe vectors (Pennington et al., 2014) for all the captions, questions and answers, concatenated with the learned word embedding from the BiLSTM encoders to further boost the performance. For image representation, we use bottom-up-attention features (Anderson et al., 2018) extracted from Faster R-CNN (Ren et al., 2015) pre-trained on Visual Genome (Krishna et al., 2017). A set of 36 features is created for each image. Each feature is a 2048dimentional vector. Evaluation Following Das et al. (2017a), we use a set of ranking metrics (Recall@k for k = {1, 5, 10}, mean rank, and mean reciprocal rank (MRR)), to measure the performance"
P19-1648,C18-1104,0,0.0251949,"versity. There were also studies investigating visual coreference resolution, either via attention memory implicitly (Seo et al., 2017) or using a more explicit reasoning procedure (Kottur et al., 2018) based on neural module networks (Andreas et al., 2016). In addition to answering questions, question sequence generation is also investigated in Jain et al. (2018); Massiceti et al. (2018). For the GuessWhat?! task, various methods (such as RL) have been proposed to improve the performance of dialog agents, measured by task completion rate as in goal-oriented dialog system (Strub et al., 2017; Shekhar et al., 2018; Strub et al., 2018; Lee et al., 2018; Zhang et al., 2018). Other related work includes imagegrounded chitchat (Mostafazadeh et al., 2017), dialog-based image retrieval (Guo et al., 2018), and text-only conversational question answering (Reddy et al., 2018; Choi et al., 2018). A recent survey on neural approaches to dialog modeling can be found in Gao et al. (2018). In this work, we focus on the VisDial task. Different from previous approaches to visual dialog, which all used a single-step reasoning strategy, we propose a novel multi-step reasoning framework that can boost the performance of"
P19-1648,P17-1172,0,0.387345,"nthesize the answer with a Recurrent Neural Network (RNN) (Das et al., 2017a); and (ii) discriminative decoder to rank answer candidates via a softmaxbased cross-entropy loss (Das et al., 2017a) or a ranking-based multi-class N-pair loss (Lu et al., 2017). Reinforcement Learning (RL) was used in Das et al. (2017b); Chattopadhyay et al. (2017) to train two agents to play image guessing games. Lu et al. (2017) proposed a training schema to effectively transfer knowledge from a pre-trained discriminative model to a generative dialog model. Generative Adversarial Network (Goodfellow et al., 2014; Yu et al., 2017b; Li et al., 2017) was also 6464 used in Wu et al. (2018) to generate answers indistinguishable from human-generated answers, and a conditional variational autoencoder (Kingma and Welling, 2014; Sohn et al., 2015) was developed in Massiceti et al. (2018) to promote answer diversity. There were also studies investigating visual coreference resolution, either via attention memory implicitly (Seo et al., 2017) or using a more explicit reasoning procedure (Kottur et al., 2018) based on neural module networks (Andreas et al., 2016). In addition to answering questions, question sequence generation"
P19-3011,D18-1547,0,0.0887227,"Missing"
P19-3011,P17-1163,0,0.058692,"Missing"
P19-3011,D17-1237,1,0.931587,"prior studies, it is been impractical to perform a rigorous comparative study under the same condition. ConvLab is the first dialog research platform that covers a full range of trainable statistical models with fully annotated datasets, differing from previous toolkits whose focus is largely concentrated on the system policy component while other components are mostly limited to pre-fixed baseline models (Ultes, 2017; Miller et al., 2017; Li et al., 2018). There is also an increasing interest in building bots that seamlessly intertwine multiple subdomains to accomplish high-level user goals (Peng et al., 2017; Budzianowski et al., 2018). The development of multi-domain dialog system adds additional complexities to both data collection and annotation, and the models for dialog system components. For the former, Budzianowski et al. (2018) collected the MultiWOZ dataset, a dialog corpus with dialogs ranging over multiple domains for the trip information setting, whereas there is no open-platform yet that is designed to handle multi-domain, multi-intent phenomena. To foster multi-domain dialog research, ConvLab features the MultiWOZ task and offers a complete set of reference models ranging from indiv"
P19-3011,P18-2069,0,0.0841428,"Missing"
P19-3011,P07-2045,0,0.00643389,"Missing"
P19-3011,N07-2038,0,0.54107,"Understanding For natural language understanding, ConvLab provides three reference models: Semantic Tuple Classifier (STC) (Mairesse et al., 2009), OneNet (Kim et al., 2017) and Multi-intent LU (MILU). STC can handle multi-domain, multi-intent dialog acts but cannot detect out-of-vocabulary (OOV) values. While OneNet can capture OOVs, it cannot handle multi-intent dialog acts. Thus, ConvLab offers a new MILU model which extends OneNet to process multi-intent dialog acts. For more details on MILU, please refer to the ConvLab site. User Policy For user policy, ConvLab provides an agenda-based (Schatzmann et al., 2007) user model and data-driven approaches such as HUS and its variational variants (Gur et al., 2018). Similar to the system side, each model works at the dialog act level, and can be pipelined with NLU and NLG modules to construct a whole user simulator. Dialog State Tracking The dialog state tracker is responsible for updating the belief state. ConvLab provides a rule-based tracker similar to the baselines in DSTCs (Williams et al., 2013) that are adapted to handle multi-domain interactions. End-to-end Model ConvLab makes available two end-to-end dialog system models: Mem2Seq (Madotto et al., 2"
P19-3011,W18-5007,0,0.0124046,"slight modifications in the configuration file. Some example configuration files are presented in Section 4. 2.2 Figure 3: An environment configuration view. As shown in Figure 3, there are also many different ways of combining components to build an environment. For example, the top layer corresponds to a user simulator operating at the dialog act level which is the typical setting of prior works focusing on developing reinforcement learning methods for dialog policy optimization. As with dialog agent, there are recent attempts on end-toend approaches to avoid requiring expensive annotation (Kreyssig et al., 2018). For human evaluation, ConvLab also provides an integration of crowd source platform such as Amazon Mechanical Turk3 as shown in the bottom layer. Dialog Agent Configuration 2.4 Reference Models This section describes a set of reference models for each component that are available in the initial release. As we will keep adding new state-of-theart models, the set of reference models available in ConvLab will be extended. Figure 2: A dialog system configuration view. In Figure 2, each layer represents a different way of constructing a dialog system. The top 2 Environment Configuration 3 ConvLab"
P19-3011,P18-1133,0,0.0586765,"-driven approaches such as HUS and its variational variants (Gur et al., 2018). Similar to the system side, each model works at the dialog act level, and can be pipelined with NLU and NLG modules to construct a whole user simulator. Dialog State Tracking The dialog state tracker is responsible for updating the belief state. ConvLab provides a rule-based tracker similar to the baselines in DSTCs (Williams et al., 2013) that are adapted to handle multi-domain interactions. End-to-end Model ConvLab makes available two end-to-end dialog system models: Mem2Seq (Madotto et al., 2018) and Sequicity (Lei et al., 2018). To support multi-domain intents, Sequicity resets the belief span when the model predicts a topic shift between domains. Word-level Dialog State Tracking Word-level DSTs directly take system and user natural language as inputs and update dialog state. ConvLab imports MDBT (Ramadan et al., 2018) model which jointly identifies the domain and tracks the belief states by utilizing the semantic similarity between dialog utterances and ontology terms. 3 Domains The initial release of ConvLab offers two domains of differing complexity: MultiWOZ and Movie. MultiWOZ The main task of the MultiWOZ doma"
P19-3011,P17-4013,0,0.21599,"Missing"
P19-3011,P18-1136,0,0.0410831,"ann et al., 2007) user model and data-driven approaches such as HUS and its variational variants (Gur et al., 2018). Similar to the system side, each model works at the dialog act level, and can be pipelined with NLU and NLG modules to construct a whole user simulator. Dialog State Tracking The dialog state tracker is responsible for updating the belief state. ConvLab provides a rule-based tracker similar to the baselines in DSTCs (Williams et al., 2013) that are adapted to handle multi-domain interactions. End-to-end Model ConvLab makes available two end-to-end dialog system models: Mem2Seq (Madotto et al., 2018) and Sequicity (Lei et al., 2018). To support multi-domain intents, Sequicity resets the belief span when the model predicts a topic shift between domains. Word-level Dialog State Tracking Word-level DSTs directly take system and user natural language as inputs and update dialog state. ConvLab imports MDBT (Ramadan et al., 2018) model which jointly identifies the domain and tracks the belief states by utilizing the semantic similarity between dialog utterances and ontology terms. 3 Domains The initial release of ConvLab offers two domains of differing complexity: MultiWOZ and Movie. MultiWOZ T"
P19-3011,D15-1199,0,0.116547,"Missing"
P19-3011,P14-5010,0,0.00241576,"m who is new to the area to quickly develop a reasonable baseline system for task-oriented dialog due to the lack of a well-structured, easy-to-use open-source system that allows researchers to build and evaluate dialog bots. ConvLab is aimed to fill the gap. ConvLab is an open-source multi-domain end-toend dialog system that allows researchers to automatically train dialog models, build and evaluate task-completion dialog bots. Such open-source systems have been instrumental in many AI research breakthroughs. For example, among many, Moses (Koehn, 2007), HTK (Young et al., 2002) and CoreNLP (Manning et al., 2014) have been widely used to facilitate subsequent research in machine translation, speech recognition and natural language processing, respectively. ConvLab consists of a rich set of modeling tools and runtime engines for building task-oriented bots of different types, and an end-to-end evaluation platform. There are roughly two architectures 64 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics: System Demonstrations, pages 64–69 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Agents-Environments-Bodies (AEB) desig"
P19-3011,W13-4065,0,0.021667,"process multi-intent dialog acts. For more details on MILU, please refer to the ConvLab site. User Policy For user policy, ConvLab provides an agenda-based (Schatzmann et al., 2007) user model and data-driven approaches such as HUS and its variational variants (Gur et al., 2018). Similar to the system side, each model works at the dialog act level, and can be pipelined with NLU and NLG modules to construct a whole user simulator. Dialog State Tracking The dialog state tracker is responsible for updating the belief state. ConvLab provides a rule-based tracker similar to the baselines in DSTCs (Williams et al., 2013) that are adapted to handle multi-domain interactions. End-to-end Model ConvLab makes available two end-to-end dialog system models: Mem2Seq (Madotto et al., 2018) and Sequicity (Lei et al., 2018). To support multi-domain intents, Sequicity resets the belief span when the model predicts a topic shift between domains. Word-level Dialog State Tracking Word-level DSTs directly take system and user natural language as inputs and update dialog state. ConvLab imports MDBT (Ramadan et al., 2018) model which jointly identifies the domain and tracks the belief states by utilizing the semantic similarit"
P19-3011,D17-2014,0,0.0321439,"n down the pipeline. There also have emerged some models in-between (Wen et al., 2016; Mrkˇsi´c et al., 2016). Due to the wide range of approaches and different metrics used in prior studies, it is been impractical to perform a rigorous comparative study under the same condition. ConvLab is the first dialog research platform that covers a full range of trainable statistical models with fully annotated datasets, differing from previous toolkits whose focus is largely concentrated on the system policy component while other components are mostly limited to pre-fixed baseline models (Ultes, 2017; Miller et al., 2017; Li et al., 2018). There is also an increasing interest in building bots that seamlessly intertwine multiple subdomains to accomplish high-level user goals (Peng et al., 2017; Budzianowski et al., 2018). The development of multi-domain dialog system adds additional complexities to both data collection and annotation, and the models for dialog system components. For the former, Budzianowski et al. (2018) collected the MultiWOZ dataset, a dialog corpus with dialogs ranging over multiple domains for the trip information setting, whereas there is no open-platform yet that is designed to handle mu"
P19-3021,P19-1441,1,0.838229,"nt. Figure from (Gao et al., 2019b). Figure 1: An example of a basic multi-task configuration. Two encoder-decoder chains share a common decoder, alternately trained on separate datasets. coders. This architecture consists of additional personality embeddings, which are provided to the decoder alongside token embeddings at each timestep of decoding. Grounding generated responses helps condition outputs based on a given personality embedding: for the same query, the system learns to generate responses in different styles, all while preserving the underlying context. tations (Gao et al., 2019b; Liu et al., 2019). By unifying a conversational sequence-to-sequence model and an autoencoder with a shared decoder, multi-task learning can personalize the conversational model (Luan et al., 2017). Multi-task learning has potentially many other powerful applications for inducing biases in conversational systems. I CECAPS allows users to build arrays of models with arbitrary sharing of components, and place them in a multi-task learning environment. Users can construct arbitrary multi-task training schedules, assigning different tasks or balances among tasks per training step. 3 3.2 SpaceFusion (Gao et al., 20"
P19-3021,P18-1157,1,0.844079,"general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable during response generation; probabilities associated with these tokens are clamped to zero. The start-token censor-list is similar, but only masks the response’s first token. We also support infrequency filters; users may restrict the decoder from generating responses with rare words. provide informed responses with context about the real world, without needing comprehensive paired conversational data to embody that information. We provide an extension of stochastic answer networks (Liu et al., 2018), a machine reading comprehension system, that acts as a full knowledgegrounded conversation model (Qin et al., 2019), hybridizing machine reading comprehension with a response generation model. At a high level, this model consists of two deep biLSTMs in parallel that encode conversational context and knowledge, respectively. The information from these encoders is then combined using cross-attention, the output of which forms the basis of a memory cell that powers a response generator. 4 4.3 The standard beam-search implementation in TensorFlow works by iteratively generating tokens, generatin"
P19-3021,P18-4021,0,0.146101,"onversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can be chained together to form a single, end-to-end functional model. This chaining paradigm allows users"
P19-3021,I17-1061,1,0.82046,"atasets. coders. This architecture consists of additional personality embeddings, which are provided to the decoder alongside token embeddings at each timestep of decoding. Grounding generated responses helps condition outputs based on a given personality embedding: for the same query, the system learns to generate responses in different styles, all while preserving the underlying context. tations (Gao et al., 2019b; Liu et al., 2019). By unifying a conversational sequence-to-sequence model and an autoencoder with a shared decoder, multi-task learning can personalize the conversational model (Luan et al., 2017). Multi-task learning has potentially many other powerful applications for inducing biases in conversational systems. I CECAPS allows users to build arrays of models with arbitrary sharing of components, and place them in a multi-task learning environment. Users can construct arbitrary multi-task training schedules, assigning different tasks or balances among tasks per training step. 3 3.2 SpaceFusion (Gao et al., 2019b) is a learning paradigm that aligns latent spaces learned by different models trained over different datasets. Of particular interest is its application to neural conversation"
P19-3021,D15-1166,0,0.0461808,"me context to be placed nearby in latent space and aligning semantically related responses along straight lines in latent space. This induces a structure in the latent space such that distance and direction from a predicted response vector roughly correspond to relevance and diversity, respectively, as in Figure 2. Built-in modules and configurations I CECAPS provides several built-in modules and configurations. Most standard NLP architectures are available, including transformers (Vaswani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A cri"
P19-3021,D17-2014,0,0.0618856,"Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can be chained together to form a single, end-to-end functional model. This chainin"
P19-3021,N18-1202,0,0.0294649,"ons with their agents and directly observe their responses. Response generation is powered by the custom decoder described in Section 4. While the commandline session is useful for quick testing, for conveTraining configurations I CECAPS training configurations follow a basic five-phase pattern. We include example training scripts that users may use as templates. 126 learning models. It places a strong emphasis on sequence modeling baselines. AllenNLP (Gardner et al., 2018) is a PyTorch library developed by AI2 for natural language processing tasks, notable for an open-source release of ELMo (Peters et al., 2018). OpenNMT (Klein et al., 2017) is a popular neural machine translation toolkit originally developed for LuaTorch that now has implementations in PyTorch and TensorFlow. MarianNMT (Junczys-Dowmunt et al., 2018) is another framework for neural machine translation developed between the Adam Mickiewicz University in Pozna and the University of Edinburgh. It is built in C++ and designed for fast training in multi-GPU systems. Texar (Hu et al., 2018) is a text generation toolkit affiliated with Carnegie Mellon University, featuring a similar emphasis on modularity to I CE CAPS . It includes reinforc"
P19-3021,N19-1125,1,0.84795,"sed directly or loaded for fine-tuning or bootstrapping other models; these models power an online demo of our framework. 1 2 Architecture I CECAPS is designed for modularity, flexibility, and ease of use. Modules are built on top of TensorFlow Estimators, making them easy for developers to use and extend flexibly. I CECAPS supports arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-tu"
P19-3021,P19-1539,1,0.727317,"g response generation; probabilities associated with these tokens are clamped to zero. The start-token censor-list is similar, but only masks the response’s first token. We also support infrequency filters; users may restrict the decoder from generating responses with rare words. provide informed responses with context about the real world, without needing comprehensive paired conversational data to embody that information. We provide an extension of stochastic answer networks (Liu et al., 2018), a machine reading comprehension system, that acts as a full knowledgegrounded conversation model (Qin et al., 2019), hybridizing machine reading comprehension with a response generation model. At a high level, this model consists of two deep biLSTMs in parallel that encode conversational context and knowledge, respectively. The information from these encoders is then combined using cross-attention, the output of which forms the basis of a memory cell that powers a response generator. 4 4.3 The standard beam-search implementation in TensorFlow works by iteratively generating tokens, generating a constant number of hypotheses at the end of the decoding phase. I CECAPS implements a modified beam search decode"
P19-3021,W18-2501,0,0.128523,"rts arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and"
P19-3021,P16-1162,0,0.198835,"ke building complex dialogue systems intuitive for the end user. 5.1 Text data processing TensorFlow estimators expect to read data from TFRecord binary files for efficient processing. We provide a script TEXT DATA PROCESSING . PY for converting text data into TFRecords, equipped with several useful preprocessing transformations. Our script can sort data within local windows so that batches fed during training have minimal padding inefficiency. These batches can be shuffled amongst each other to mitigate any biases induced by sorting. We provide token preprocessing through byte pair encoding (Sennrich et al., 2016), which builds a token set at a level of abstraction between characters and words. This often allows for faster training and improved generalization. Another feature focused on conversational scenarios is fixed-length context extraction. Conversational data often contains large, potentially unwieldy multi-turn contexts; we can limit our data samples to a desired context length. We also provide an option for annotating datasets with topic grounding information, by analyzing the data for unique tokens to use as topic markers. 5.2 Training the system. The system is now ready to train: a single ca"
P19-3021,W18-2503,0,0.111568,"NLP (Gardner et al., 2018) is a PyTorch library developed by AI2 for natural language processing tasks, notable for an open-source release of ELMo (Peters et al., 2018). OpenNMT (Klein et al., 2017) is a popular neural machine translation toolkit originally developed for LuaTorch that now has implementations in PyTorch and TensorFlow. MarianNMT (Junczys-Dowmunt et al., 2018) is another framework for neural machine translation developed between the Adam Mickiewicz University in Pozna and the University of Edinburgh. It is built in C++ and designed for fast training in multi-GPU systems. Texar (Hu et al., 2018) is a text generation toolkit affiliated with Carnegie Mellon University, featuring a similar emphasis on modularity to I CE CAPS . It includes reinforcement learning capabilities alongside its sequence modelling tools. A few other toolkits have a dialog emphasis. DeepPavlov (Burtsev et al., 2018) is a deep learning library with a focus on task-oriented dialogue. It provides demos and pre-trained models for tasks such as question answering and sentiment classification. Affiliated with DeepPavlov is the ConvAI2 challenge (Dinan et al., 2019), a general dialogue competition featuring a synthetic"
P19-3021,P18-4020,0,0.0368469,"Missing"
P19-3021,W18-1819,0,0.0298992,"exibly. I CECAPS supports arbitrary architectures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements"
P19-3021,P17-4012,0,0.176011,"tures of modules chained together within versatile multi-task configurations. Introduction Neural conversational systems have seen great improvements over the past several years, with current models able to generate surprisingly coherent dialogs (Gao et al., 2019a). Business applications, games, and potentially other settings can benefit from intelligent conversational agents, inviting users to interact intuitively with complex systems. Although a range of open-source tools is available to train neural network models for natural language processing (Vaswani et al., 2018; Gardner et al., 2018; Klein et al., 2017), only a few emphasize multi-turn conversational settings (Miller et al., 2017; Burtsev et al., 2018). Conversations present distinct challenges. They generally consist of many turns, and agents need to contextualize responses in these multi-turn contexts. Agents may also need to contextualize their responses in other cues, such as style, intent, and external knowledge, while retaining a conversational flow. 2.1 Component chaining Sequence-to-sequence models can be abstracted as chains of sequence encoders and sequence decoders. Our library implements various encoders and decoders, which can b"
P19-3021,N16-1014,1,0.717728,"ani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A critical task in building intelligent conversational agents is grounding their responses in an external knowledge base. This allows agents to 124 agent to avoid profanities or other offensive language. Likewise, the system should avoid obvious ungrammatical outputs, such as broken abbreviations or nonsensical punctuation marks. I CECAPS supports several filters, including a general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable d"
P19-3021,P16-1094,1,0.920476,"ani et al., 2017), LSTM-based seq2seq models (Sutskever et al., 2014) with attention (Bahdanau et al., 2015; Luong et al., 2015), n-gram convolutional language models, and deep convolutional networks for baseline image grounding. Where applicable, these are implemented as chains of simpler components as per our design philosophy. We also provide features that target conversational scenarios, from individual chainable components to custom multi-task learning presets. 3.1 SpaceFusion Personality grounding 3.3 Inspired by recent work on modeling personality differences in conversational systems (Li et al., 2016b), I CECAPS provides implementations of personality-grounded seq2seq and transformer deKnowledge grounding A critical task in building intelligent conversational agents is grounding their responses in an external knowledge base. This allows agents to 124 agent to avoid profanities or other offensive language. Likewise, the system should avoid obvious ungrammatical outputs, such as broken abbreviations or nonsensical punctuation marks. I CECAPS supports several filters, including a general censor-list and a start-token censor-list. The general censor-list contains a list of tokens to disable d"
W00-1219,O93-1009,0,0.0323252,"Missing"
W02-1032,J92-4003,0,0.0626433,"Missing"
W02-1032,P01-1017,0,0.0381344,"red language modeling (SLM, Chelba and Jelinek, 2000). SLM uses a statistical parser trained on an annotated corpus in order to identify the headword of each constituent, which are then used as conditioning words in the trigram context. Though SLMs have been shown to significantly improve the performance of the LM measured in perplexity, they also pose practical problems. First, the performance of SLM is contingent on the amount and quality of syntactically annotated training data, but such data may not always be available. Second, SLMs are very time-intensive, both in their training and use. Charniak (2001) and Roark (2001) also present language models based on syntactic dependency structure, which use lexicalized PCFGs that sum over the derivation probabilities. They both report improvements in perplexity over Chelba and Jelinek (2000) on the Wall Street Journal section of the Penn Treebank data, suggesting that syntactic structure can be further exploited for language modeling. The kind of linguistic structure used in our models is significantly more modest than that provided by parser-based models, yet offers practical benefits for realistic applications, as shown in the next section. 5 Evalu"
W02-1032,O01-2002,1,0.808596,"-PHTM, and the models using unigram for category probability estimation of Equation (7) as U-PHTM. 3 Using Clusters 3.1 Principle Clustering techniques attempt to make use of similarities between words to produce a better estimate of the probability of word strings (Goodman, 2001). We have mentioned in Section 2.2 that the headword trigram model can be thought of as a cluster-based model with two clusters, the headword and the function word. In this section, we describe a method of clustering automatically similar words and headwords. We followed the techniques described in Goodman (2001) and Gao et al. (2001), and performed experiments using predictive clustering along with headword trigram models. 3.2 Predictive clustering model Consider a trigram probability P(w3|w1w2), where w3 is the word to be predicted, called the predicted word, and w1 and w2 are context words used to predict w3, called the conditional words. Gao et al. (2001) presents a thorough comparative study on various clustering models for Asian languages, concluding that a model that uses clusters for predicted words, called the predictive clustering model, performed the best in most cases. Let wi be the cluster which word wi belong"
W02-1032,H94-1015,0,0.0799061,"Missing"
W02-1032,J01-2004,0,0.0501106,"g (SLM, Chelba and Jelinek, 2000). SLM uses a statistical parser trained on an annotated corpus in order to identify the headword of each constituent, which are then used as conditioning words in the trigram context. Though SLMs have been shown to significantly improve the performance of the LM measured in perplexity, they also pose practical problems. First, the performance of SLM is contingent on the amount and quality of syntactically annotated training data, but such data may not always be available. Second, SLMs are very time-intensive, both in their training and use. Charniak (2001) and Roark (2001) also present language models based on syntactic dependency structure, which use lexicalized PCFGs that sum over the derivation probabilities. They both report improvements in perplexity over Chelba and Jelinek (2000) on the Wall Street Journal section of the Penn Treebank data, suggesting that syntactic structure can be further exploited for language modeling. The kind of linguistic structure used in our models is significantly more modest than that provided by parser-based models, yet offers practical benefits for realistic applications, as shown in the next section. 5 Evaluation Methodology"
W03-1701,P98-1029,0,0.0380793,"Missing"
W03-1701,A00-2009,0,0.0296688,"Missing"
W03-1701,C98-1029,0,\N,Missing
W03-1718,J96-1002,0,0.00689166,"ork to integrate various features from different knowledge sources. Each feature is typically represented as a binary constraint f. All features are then combined using a log-linear model shown in Eq. 5. Pλ ( y |x ) = 1 exp( Z λ ( x) λ i f i ( x , y )) i (5) where i is a weight of the feature fi , and Z(x) is a normalization factor. Weights ( ) are estimated using the maximum entropy principle: to satisfy constraints on observed data and assume a uniform distribution (with the maximum entropy) on unseen data. The training algorithm we used is the improved iterative scaling (IIS) described in (Berger et al, 1996)3. The context features include six characters: three on the left of the SCNE, and three on the right. Given the context features, the ME classifier would estimate the probability of the candidate being a SCNE. In our example, we treat candidates with the probability larger than 0.5 as SCNEs. To get the precision-recall curve, we can vary the probability threshold from 0.1 to 0.9. 4.2 Vector Space Model VSM is another model we used to detect SCNE. Similar to ME, we use six surrounding characters as the features, as shown in Figure 2. Figure 2. Context window In this approach, we apply the stan"
W03-1718,P03-1035,1,0.899904,"Missing"
W03-1718,C02-1054,0,0.0644216,"Missing"
W03-1718,C02-1012,1,0.900718,"Missing"
W03-1718,M98-1018,0,\N,Missing
W03-1718,P02-1060,0,\N,Missing
W03-1718,M98-1017,0,\N,Missing
W03-1718,P02-1062,0,\N,Missing
W04-1107,W00-0726,0,0.393223,"Missing"
W04-1107,P00-1015,1,0.803515,"ons. For example, one inputs a POS pattern: ‘a_n_n’, and an expected annotation result: ‘B-NP_I-NP_E-NP 3 ’, the tool will list all the consistent and inconsistent sentences in the annotated text respectively. Based on the output one can revise those inconsistent results one by one, and finally the consistency of the chunked text will be improved step by step. 5 i =1 Chunking Model After annotating the corpus, we could use various learning algorithms to build the chunking model. In this paper, HMM is selected because not only its training speed is fast, but also it has comparable performance (Xun and Huang, 2000). Automatic chunking with HMM should conduct the following two steps. 1) Identify boundaries of each chunk. It is to assign each word a chunk mark, named M, which contains 5 classes: B, I, E, S (a single word chunk) and O (outside all chunks). 2) Tag the chunk type, named X, which contains 11 types defined in Section 3. So each word will be tagged with two tags: M and X (the words excluding from any chunk only have M). So the result after chunking is a sequence of triples (t, m, x), where t, m, x represent POS tag, chunk mark and chunk type respectively. All the triples of a chunk are combined"
W04-1107,P02-1055,0,0.41431,"Missing"
W04-1107,W00-0730,0,0.220739,"Missing"
W04-1107,W01-0706,0,0.35348,"Missing"
W04-1107,P03-1063,0,0.271011,"Missing"
W04-1107,brants-2000-inter,0,0.0743744,"Missing"
W04-1107,C02-1145,0,0.139467,"Missing"
W04-1107,P01-1043,0,\N,Missing
W04-1119,P03-1035,1,0.896513,"Missing"
W04-1119,Y98-1021,0,0.0447688,"Missing"
W04-1119,P97-1041,0,0.0894923,"Missing"
W04-1119,C02-1012,1,0.888397,"Missing"
W04-1119,W03-1727,0,0.0142352,"gmented corpus. Their basic assumption is that Chinese words are usually 1 to 4 characters long. They however did not take into account a large amount of named entities (e.g. Chinese organization name, transliterate name and some person names) most of which are longer than 4 characters (e.g., 微软亚洲研究院 Microsoft Research Asia, 加 利 福 尼 亚 California, 陈 欧 阳 晓 彤 a woman’s name which puts her husband’s surname ahead). An and Wong used Hidden Markov Models (HMM) for segmentation. Their system is solely trained on a corpus which has been manually annotated with word boundaries and Part-of-Speech tags. Wu (2003) also used the training data to tune the segmentation parameters of their MSR-NLP Chinese system. He used the annotated training data to deal with the morphologically derived words. In this paper we present a semi-supervised training method where we use both an auto-segmented training corpus and a small hand-annotated subset of it. Comparing to unsupervised approaches, our approach leads to a better segmenter that can identify much more named entities which are not in the dictionary. Comparing to supervised approaches, our method requires much less human effort for data annotation. The Chinese"
W04-1119,P04-1059,1,\N,Missing
W17-2608,D13-1160,0,0.0245633,"Missing"
W17-2608,P15-1067,0,0.0406751,"POSITION) 1. Forward-center 2. Swingman 3. Cabinet of the United States 5 get entity. Hence, the whole inference process can be thought as the model iteratively reformulates the representations in order to minimize its distance to the target entity in neural space. Related Work Link Prediction and Knowledge Base Completion Given that R is a relation, h is the head entity, and t is the tail entity, most of the embedding models for link prediction focus on finding the scoring function fr (h, t) that represents the implausibility of a triple. (Bordes et al., 2011, 2014, 2013; Wang et al., 2014; Ji et al., 2015; Nguyen et al., 2016). In many studies, the scoring function fr (h, t) is linear or bi-linear. For example, in TransE (Bordes et al., 2013), the function is implemented as fr (h, t) = kh + r − tk, where h, r and t are the corresponding vector representations. Recently, different studies (Guu et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Das et al., 2016; Toutanova et al., 2016) demonstrate the importance for models to also learn from multi-step relations. Learning from multi-step relations injects the structured relationships between triples into the model. However, this also pos"
W17-2608,D15-1082,0,0.433431,"us, the knowledge base completion (KBC) task has emerged an important open research problem (Nickel et al., 2011). Neural-network based methods have been very popular for solving the KBC task. Following Bordes et al. (2013), one of the most popular approaches for KBC is to learn vector-space representations of entities and relations during training, and then apply linear or bi-linear operations to infer the missing relations at test time. However, several recent papers demonstrate limitations of prior approaches relying upon vector-space models alone (Guu et al., 2015; Toutanova et al., 2016; Lin et al., 2015a). By themselves, there is no straightforward way to capture the structured relationships between multiple triples adequately. For example, assume that we want to fill in the missing relation for the triple (Obama, NATIONALITY, ?), a multi-step search procedure might be needed to discover the evidence in the observed triples such as (Obama, B ORN I N, Hawaii) and (Hawaii, PART O F, U.S.A). To address this issue, Guu et al. (2015); Toutanova et al. (2016); Neelakantan et al. (2015); Das et al. (2016); Lin et al. (2015a) propose different approaches of injecting structured information based on"
W17-2608,D16-1147,0,0.0100396,"Missing"
W17-2608,P09-1113,0,0.0126757,"Missing"
W17-2608,P15-1016,0,0.0372826,"a relation, h is the head entity, and t is the tail entity, most of the embedding models for link prediction focus on finding the scoring function fr (h, t) that represents the implausibility of a triple. (Bordes et al., 2011, 2014, 2013; Wang et al., 2014; Ji et al., 2015; Nguyen et al., 2016). In many studies, the scoring function fr (h, t) is linear or bi-linear. For example, in TransE (Bordes et al., 2013), the function is implemented as fr (h, t) = kh + r − tk, where h, r and t are the corresponding vector representations. Recently, different studies (Guu et al., 2015; Lin et al., 2015a; Neelakantan et al., 2015; Das et al., 2016; Toutanova et al., 2016) demonstrate the importance for models to also learn from multi-step relations. Learning from multi-step relations injects the structured relationships between triples into the model. However, this also poses a technical challenge of considering exponential numbers of multi-step relationships. Prior approaches address this issue by designing path-mining algorithms (Lin et al., 2015a) or considering all possible paths using a dynamic programming algorithm with the restriction of using linear or bi-linear models only (Toutanova et al., 2016). Neelakanta"
W17-2608,N16-1054,0,0.312463,"controller. We set the maximum inference step Tmax of the IRN to 5. We randomly initialize all model parameters, and use SGD as the training algorithm with mini-batch size of 64. We set the learning rate to a constant number, 0.01. To prevent the model from learning a trivial solution by increasing entity embeddings norms, we follow Bordes et al. (2013) to enforce the L2 -norm of the entity embeddings as 1. We use hits@10 as the validation metric for the IRN. Following the work (Lin et al., 2015a), we add reverse relations into the training triplet set to increase the training data. Following Nguyen et al. (2016), we divide the results of previous work into two groups. The first group contains the models that directly optimize a scoring function for the triples in a knowledge base without using extra information. The second group of models make uses of additional information from multi-step relations. For example, RTransE (García-Durán et al., 2015) and PTransE Experimental Results In this section, we evaluate the performance of our model on the benchmark FB15k and WN18 datasets for KBC (Bordes et al., 2013). These datasets contain multi-relations between head and tail entities. Given a head entity an"
W17-2608,N16-3020,0,0.00355877,"Missing"
W17-2608,D16-1145,0,0.289258,"Missing"
W17-2608,N13-1008,0,0.0189883,"ference depending on the complexity of the instance. MemNN and NTM explicitly store inputs (such as graph definition, supporting facts) in the memory. In contrast, in IRNs, we do not explicitly store all the observed inputs in the shared memory. Instead, we directly operate on the shared memory, which modeling the structured relationships implicitly. During training, we randomly initialize the memory and update the memory jointly with the controller with respect to task-specific objectives via back-propagation, instead of explicitly defining memory write operations as in NTM. Studies such as (Riedel et al., 2013) show that incorporating textual information can further improve the KBC tasks. It would be interesting to incorporate the information outside the knowledge bases in our model in the future. 6 Neural Frameworks Sequence-to-sequence models (Sutskever et al., 2014; Cho et al., 2014) have shown to be successful in many applications such as machine translation and conversation modeling (Sordoni et al., 2015). While sequence-tosequence models are powerful, recent work has shown the necessity of incorporating an external memory to perform inference in simple algorithmic tasks (Graves et al., 2014, 2"
W17-2608,D10-1106,0,0.0300331,"Missing"
W17-2608,P15-1128,1,0.0252863,"Missing"
W17-2608,N15-1020,1,0.0245097,"Missing"
W17-2608,W15-4007,0,0.0843574,"ses disease/symptoms “sports” sports-team-roster/team basketball-roster-position/player basketball-roster-position/player baseball-player/position-s appointment/appointed-by batting-statistics/team basketball-player-stats/team person/profession “tv program” tv-producer-term/program tv-producer-term/producer-type tv-guest-role/episodes-appeared-in tv-program/languages tv-guest-role/actor tv-program/spin-offs award-honor/honored-for tv-program/country-of-origin (2015) and Das et al. (2016) use an RNN to model the multi-step relationships over a set of random walk paths on the observed triplets. Toutanova and Chen (2015) shows the effectiveness of using simple node and link features that encode structured information on FB15k and WN18. In our work, the IRN outperforms prior results and shows that similar information can be captured by the model without explicitly designing inference procedures on the observed triplets. Our model can be regarded as a recursive function that iteratively update the representation in such a way that its distance to the target entity in the neural space is minimized, i.e., kfIRN (h, r) − tk. biggest difference between our model and the existing frameworks is the controller and the"
W17-2608,D15-1174,0,0.0202694,"Missing"
W17-2608,P16-1136,0,0.406423,"tities and relations. Thus, the knowledge base completion (KBC) task has emerged an important open research problem (Nickel et al., 2011). Neural-network based methods have been very popular for solving the KBC task. Following Bordes et al. (2013), one of the most popular approaches for KBC is to learn vector-space representations of entities and relations during training, and then apply linear or bi-linear operations to infer the missing relations at test time. However, several recent papers demonstrate limitations of prior approaches relying upon vector-space models alone (Guu et al., 2015; Toutanova et al., 2016; Lin et al., 2015a). By themselves, there is no straightforward way to capture the structured relationships between multiple triples adequately. For example, assume that we want to fill in the missing relation for the triple (Obama, NATIONALITY, ?), a multi-step search procedure might be needed to discover the evidence in the observed triples such as (Obama, B ORN I N, Hawaii) and (Hawaii, PART O F, U.S.A). To address this issue, Guu et al. (2015); Toutanova et al. (2016); Neelakantan et al. (2015); Das et al. (2016); Lin et al. (2015a) propose different approaches of injecting structured inf"
W17-2608,D15-1034,0,\N,Missing
W19-2401,P18-5002,1,0.877282,"Missing"
W19-2401,D14-1002,1,0.771046,"at local connections between any two neighboring sentences can be overlooked. One can easily distinguish a generated sentence from a real one by judging whether it is semantically cohesive with its neighboring sentences. We strive to embody these two different yet important concepts by developing coherence and cohesion discriminators, operating on the sentence level and word level, respectively. Our design of these two discriminators is inspired by the Deep Structured Semantic Model (DSSM) which was originally developed to measure the semantic similarity between two texts (Huang et al., 2013; Gao et al., 2014; Palangi et al., 2016; Xu et al., 2017). In this study, we extend ‘semantic similarity’ to coherence and cohesion in a long-form text. 3.1 Figure 1: Illustration of coherence and cohesion discriminators. Dcoherence takes in bag-of-words sentence embeddings as inputs, and Dcohesion takes in the raw word embeddings of consecutive sentences as inputs. The source encoder f (or u) is different from the target encoder g (or v). (CNN)1 or RNN2 , denoted as f , takes as input the BOW vectors of the source text chunk S and encodes it into a single vector f (S). Similarly, g encodes the target text chu"
W19-2401,J08-1001,0,0.0220925,"ohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated word at time t, wt , can be viewed as an action to be chosen by the policy from a large discrete space, or vocabulary, conditioned on state st−1 = w≤t−1 . Let rt be the reward"
W19-2401,P85-1007,0,0.24846,"nerations. Furthermore, we model cohesion between consecutive sentence pairs using word-level features. Related work Coherence and cohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated word at time t, wt , can be viewed as an a"
W19-2401,P18-1152,0,0.020626,"anguage model that generates more coherent and cohesive long-form texts, and empirically validate its effectiveness using the TripAdvisor and Yelp English reviews datasets. 2 et al. (2015) and Paulus et al. (2017). These works directly optimize for specific metrics, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003), using REINFORCE (Williams, 1992). However, these metrics do not give a complete picture of the text generation quality. Only recently have there been efforts to provide more relevant objectives, such as consistency and repetition in a text (Li et al., 2015, 2016a; Holtzman et al., 2018). But these works use the objectives to re-rank candidate outputs, not to reward or penalize them. Li et al. (2016b) constructed a set of reward models for the dialogue task, such as information flow and semantic coherence, to tune the generator, yet they do not provide an ablation study on the relative contribution of these reward models individually. It is not clear that these reward models can be generalized to other tasks, in particular, long-form text generation tasks. The most relevant to our work is Bosselut et al. (2018), which promotes text generation in the correct order, and discour"
W19-2401,P98-1044,0,0.0287386,"ion between consecutive sentence pairs using word-level features. Related work Coherence and cohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated word at time t, wt , can be viewed as an action to be chosen by the policy f"
W19-2401,P88-1020,0,0.329802,"the relative quality of generations. Furthermore, we model cohesion between consecutive sentence pairs using word-level features. Related work Coherence and cohesion. Coherence and cohesion have been extensively studied in the computational linguistics community, particularly in the ‘pre-deep-learning’ era. Lack of formal specifications for coherence and cohesion (Mani et al., 1998), resulted in many different formalisms, such as Rhetorical Structure Theory (Mann and Thompson, 1988), and other forms of coherence and cohesion relations and their quantification (Mani et al., 1998; Hobbs, 1985; Hovy, 1988; McKeown, 1985; Cohen and Levesque, 1985; Hovy, 1991; Cristea et al., 1998; Halliday and Hasan, 1996; Liddy, 1991; Van Dijk, 2013; Edmundson, 1969; Barzilay and Lapata, 2008). This list is not exhaustive. However, prior work jointly exploring coherence and cohesion using neural models in the context of long-form text generation has not come to our attention. Reinforcement learning for text generation. The text generation task can be framed as a reinforcement learning (RL) problem (Daum´e et al., 2009), in which the generator G is acting as a policy π, with parameters θπ , and each generated w"
W19-2401,D17-1153,0,0.0304854,"Missing"
W19-2401,D14-1181,0,0.00294367,"l encoder. Given source text chunk S and target text chunk T , the coherence discriminator Dcoherence computes the coherence score in three steps, as illustrated in Figure 1 (upper). First, each sentence is encoded by the bag-of-words (BOW) embedding, i.e., the average of its word vectors from a pre-trained word embedding (Pennington et al., 2014). Secondly, an encoder which can be implemented using a convolutional neural network 1 We explored with deeper networks. However, the performance difference was marginal. For simplicity, we decided to use a 1-layer convolutional network architecture (Kim, 2014; Collobert et al., 2011). 2 For clarity in our model description, we omit RNN hereafter. We present results using both CNN and RNN encoders in Table 2. 3 ing data, negative (incoherent) pairs need to be artificially constructed. The next section describes the way these negative pairs are generated. form acohesive pairof consecutive sentences. Let sk := s1k , s2k , ..., snk be the k th sentence that con- sists of n words, sk+1 := s1k+1 , s2k+1 , ..., sm k+1 be the real next sentence of m  1 that2 consistsm e words, and sek+1 := sek+1 , sek+1 , ..., sek+1 be the artificially constructed i"
W19-2401,D17-1259,0,0.0347691,"Missing"
W19-2401,P02-1040,0,0.10418,"hat uses negative samples to estimate its reward baseline and therefore eliminates the need for a sepa1 Proceedings of the First Workshop on Narrative Understanding, pages 1–11 c Minneapolis, Minnesota, June 7, 2019. 2019 Association for Computational Linguistics rate critic function; and (4) we develop a new neural language model that generates more coherent and cohesive long-form texts, and empirically validate its effectiveness using the TripAdvisor and Yelp English reviews datasets. 2 et al. (2015) and Paulus et al. (2017). These works directly optimize for specific metrics, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003), using REINFORCE (Williams, 1992). However, these metrics do not give a complete picture of the text generation quality. Only recently have there been efforts to provide more relevant objectives, such as consistency and repetition in a text (Li et al., 2015, 2016a; Holtzman et al., 2018). But these works use the objectives to re-rank candidate outputs, not to reward or penalize them. Li et al. (2016b) constructed a set of reward models for the dialogue task, such as information flow and semantic coherence, to tune the generator, yet they do not provide an ablatio"
W19-2401,P16-1094,1,0.872387,"Missing"
W19-2401,D14-1162,0,0.0821075,"that consists of m e sentences. Dcoherence is designed to distinguish a positive (coherent) pair (S, T ) from a negative (incoherent) pair (S, Te) by assigning different scores, i.e., Dcoherence (S, T ) &gt; Dcoherence (S, Te). Model architecture. The model takes a form of dual encoder. Given source text chunk S and target text chunk T , the coherence discriminator Dcoherence computes the coherence score in three steps, as illustrated in Figure 1 (upper). First, each sentence is encoded by the bag-of-words (BOW) embedding, i.e., the average of its word vectors from a pre-trained word embedding (Pennington et al., 2014). Secondly, an encoder which can be implemented using a convolutional neural network 1 We explored with deeper networks. However, the performance difference was marginal. For simplicity, we decided to use a 1-layer convolutional network architecture (Kim, 2014; Collobert et al., 2011). 2 For clarity in our model description, we omit RNN hereafter. We present results using both CNN and RNN encoders in Table 2. 3 ing data, negative (incoherent) pairs need to be artificially constructed. The next section describes the way these negative pairs are generated. form acohesive pairof consecutive sen"
W19-2401,D16-1127,1,0.823404,"the TripAdvisor and Yelp English reviews datasets. 2 et al. (2015) and Paulus et al. (2017). These works directly optimize for specific metrics, such as BLEU (Papineni et al., 2002) or ROUGE (Lin and Hovy, 2003), using REINFORCE (Williams, 1992). However, these metrics do not give a complete picture of the text generation quality. Only recently have there been efforts to provide more relevant objectives, such as consistency and repetition in a text (Li et al., 2015, 2016a; Holtzman et al., 2018). But these works use the objectives to re-rank candidate outputs, not to reward or penalize them. Li et al. (2016b) constructed a set of reward models for the dialogue task, such as information flow and semantic coherence, to tune the generator, yet they do not provide an ablation study on the relative contribution of these reward models individually. It is not clear that these reward models can be generalized to other tasks, in particular, long-form text generation tasks. The most relevant to our work is Bosselut et al. (2018), which promotes text generation in the correct order, and discourages in its reverse order using rewards. However, this may not be sufficient in capturing coherence since there ar"
W19-2401,D17-1230,0,0.0520846,"Missing"
W19-2401,P15-1152,0,0.0590307,"Missing"
W19-2401,N03-1020,0,0.21645,"Missing"
W19-2401,N15-1020,1,0.830371,"al approaches to natural language generation rely on a large amount of human-generated text to train language models (Cho et al., 2014; Graves, 2013; Sutskever et al., 2014). Although these models can generate sentences that, if judged individually, are similar to human-generated ones, they often fail to capture the local and global dependencies among sentences, resulting in a text that is neither coherent nor cohesive. For example, neural language models based on Recurrent Neural Networks (RNNs) are widely applied to response generation for dialogue (Vinyals and Le, 2015; Shang et al., 2015; Sordoni et al., 2015; Li et al., 2015). Although the responses by themselves look reasonable, they are detached from the whole dialogue session. See Gao et al. (2018) for a comprehensive survey. In this paper, we address the challenge in a principled manner, employing a pair of discriminators to score whether and to what extent a text is coherent or cohesive. The coherence discriminator measures the compatibility among all sentences in a paragraph. The cohesion discriminator measures the compatibility of each pair of consecutive sentences. These models, given a conditional input text and multiple candidate output"
W19-2401,1983.tc-1.13,0,0.528181,"Missing"
W19-2401,C98-1044,0,\N,Missing
W19-2401,N18-1016,1,\N,Missing
W19-2401,N16-1014,1,\N,Missing
W19-5042,W19-5039,0,0.171139,"earning fine-tuning, we use multi-task learning on NLI, RQE and QA tasks on general and medical domains to improve performance. The proposed methods are proved effective for natural language understanding in the medical domain, and we rank the first place on the QA task. 1 • General NLU embeddings: We use MT-DNN (Liu et al., 2019b) trained on GLUE benchmark(Wang et al., 2019). MT-DNN is trained on 10 tasks including NLI, question equivalence, and machine comprehension. These tasks correspond well to the target MEDIQA tasks but in different domains. Background The MEDIQA 2019 shared tasks (Ben Abacha et al., 2019) aim to improve the current state-ofthe-art systems for textual inference, question entailment and question answering in the medical domain. This ACL-BioNLP 2019 shared task is motivated by a need to develop relevant methods, techniques and gold standards for inference and entailment in the medical domain and their application to improve domain-specific information retrieval and question answering systems. The shared task consists of three parts: i) natural language inference (NLI) on MedNLI, ii) Recognizing Question Entailment (RQE), and iii) Question Answering (QA). Recent advancement in NLP"
W19-5042,N15-1092,1,0.700511,"cScholar scientific papers. Although SciBERT obtained state-of-the-art results on several singlesentence tasks, it lacks knowledge from other NLU tasks such as GLUE. In this paper, we investigate different methods to combine and transfer the knowledge from the two different sources and illustrate our results on the MEDIQA shared task. We name our method as DoubleTransfer, since it transfers knowledge from two different sources. Our method is based on fine-tuning both MT-DNN and SciBERT using multi-task learning, which has demonstrated the efficiency of knowledge transformation (Caruana, 1997; Liu et al., 2015; Xu et al., 2018; Liu et al., 2019b), and integrating models from both domains with ensembles. Related Works. Transfer learning has been widely used in training models in the medical do399 Proceedings of the BioNLP 2019 workshop, pages 399–405 c Florence, Italy, August 1, 2019. 2019 Association for Computational Linguistics Algorithm 1 Multi-task Fine-tuning with External Datasets Require: In-domain datasets D1 , ..., DK1 , External domain datasets DK1 +1 , ..., DK2 , max epoch, mixture ratio α 1: Initialize the model M 2: for epoch= 1, 2, ..., max epoch do 3: Divide each dataset Dk into Nk m"
W19-5042,P19-1441,1,0.934066,".com Abstract large-scale corpus and transfer it to downstream tasks. We investigate NLU in the medical (scientific) domain. From BERT, we need to adapt to i) The change from general domain corpus to scientific language; ii) The change from low-level language model tasks to complex NLU tasks. Although there is limited training data in NLU in the medical domain, we fortunately have pre-trained models from two intermediate steps: This paper describes our competing system to enter the MEDIQA-2019 competition. We use a multi-source transfer learning approach to transfer the knowledge from MT-DNN (Liu et al., 2019b) and SciBERT (Beltagy et al., 2019) to natural language understanding tasks in the medical domain. For transfer learning fine-tuning, we use multi-task learning on NLI, RQE and QA tasks on general and medical domains to improve performance. The proposed methods are proved effective for natural language understanding in the medical domain, and we rank the first place on the QA task. 1 • General NLU embeddings: We use MT-DNN (Liu et al., 2019b) trained on GLUE benchmark(Wang et al., 2019). MT-DNN is trained on 10 tasks including NLI, question equivalence, and machine comprehension. These tasks"
W19-5042,D18-1187,0,0.133695,"Missing"
W19-5042,N18-1101,0,0.0280719,"sting embeddings from SciBERT or MT-DNN. Transfer learning is also widely used in other tasks of NLP, such as machine translation (Bahdanau et al., 2014) and machine reading comprehension (Xu et al., 2018). 2 Fine-tuning details Algorithm. We fine-tune the two types of pretrained models on all the three tasks using multitask learning. As suggested by MEDIQA paper, we also fine-tune our model on MedQuAD (Abacha and Demner-Fushman, 2019), a medical QA dataset. We will provide details for fine-tuning on these datasets in Section 2.3. We additionally regularize the model by also training on MNLI (Williams et al., 2018). To prevent the negative transfer from MNLI, we put a larger weight on MEDIQA data by sampling MNLI data with less probability. Our algorithm is presented in Algorithm 1 and illustrated as Figure 1, which is a mixture ratio method for multitask learning inspired by Xu et al. (2018). We start with in-domain datasets D1 , ...DK1 (i.e., the MEDIQA tasks, K1 = 3) and external datasets DK1 +1 , ..., DK2 (in this case MNLI). We cast all the training samples as sentence pairs (s1 , s2 ) ∈ Dk , k = 1, 2, ..., K2 . In each epoch of training, we use all mini-batches from in-domain data, while only a sm"
W19-5042,N19-1271,1,\N,Missing
