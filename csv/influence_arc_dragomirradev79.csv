2020.acl-main.706,D19-1052,0,0.0451164,"Missing"
2020.acl-main.706,2020.acl-main.708,0,0.20003,"e bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions and Future Directi"
2020.acl-main.706,2020.acl-main.18,0,0.0282211,"e bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions and Future Directi"
2020.acl-main.706,P17-1025,0,0.0299591,"for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a classifier and significantly improve Commonse"
2020.acl-main.706,W18-6505,0,0.0214463,"Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained"
2020.acl-main.706,P16-1154,0,0.0267129,"sts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be men"
2020.acl-main.706,P16-1014,0,0.0262209,", 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generate"
2020.acl-main.706,P16-1195,0,0.0197135,"generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale"
2020.acl-main.706,P04-1050,0,0.0480877,"Missing"
2020.acl-main.706,N12-1093,0,0.0316404,"e to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have show"
2020.acl-main.706,D16-1128,0,0.0238773,"ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 20"
2020.acl-main.706,D16-1011,0,0.0343869,"which are combined with natural language for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a"
2020.acl-main.706,P19-1195,0,0.115119,"f variable diameters) in the simulator to achieve a given goal. Figure 1 shows an example of a task with a specified goal. The input to ESPRIT is a sequence of frames from a physics simulation and the output is a natural language narrative that reflects the locations of the objects in the initial scene and a description of the sequence of physical events that would lead to the desired goal state, as shown in Figure 2. The first phase of the framework uses a neural network classifier to identify salient frames from the simulation. For the second phase we experimented with table-to-text models (Puduppully et al., 2019a,b) as well as pre-trained language models (Radford et al., 2018). We evaluated our framework for natural language generated reasoning using several automated and human evaluations with a focus on the understanding of qualitative physics and the ordering of a natural sequence of physical events. We found that our model achieves very high performance for phase one (identifying frames with salient physical events) and that, for phase two, the table-to-text models outperform pre-trained language models on qualitative physics reasoning. 2 2.1 Dataset PHYRE Benchmark We build our dataset by extend"
2020.acl-main.706,P09-1011,0,0.0603867,"et al. (2020) propose to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulceh"
2020.acl-main.706,P19-1487,1,0.734658,"(Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al., 2019; Rajani et al., 2019; DeYoung et al., 2020). Lei et al. (2016), for example, generate textual rationales for sentiment analysis by highlighting phrases in the input. Forbes and Choi (2017) learn the physical knowledge of actions and objects from natural language. Camburu et al. (2018) propose e-SNLI by generating explanations for the natural language inference problem at a cost of performance. Rajani et al. (2019) propose to use LMs to generate explanations that can be used during training and inference in a classifier and significantly improve CommonsenseQA performance. Bisk et al. (2020) propose to use a questi"
2020.acl-main.706,D15-1166,0,0.0153733,"; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that"
2020.acl-main.706,N16-1086,0,0.015485,"ing task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results"
2020.acl-main.706,W00-1418,0,0.219091,"explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While tra"
2020.acl-main.706,N18-1137,0,0.0145506,"entifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al.,"
2020.acl-main.706,P17-2034,0,0.0279544,"hang et al., 2016). Recent papers use neural networks over visual inputs to predict future pixels (Finn et al., 2016; Lerer et al., 2016; Mirza et al., 2016; Du and Narasimhan, 2019) or make qualitative predictions (Groth et al., 2018; Li et al., 2016, 2017; Janner et al., 2019; Wu et al., 2015; Mao et al., 2019). Furthermore, several frameworks and benchmarks have been introduced to test visual reasoning such as PHYRE (Bakhtin et al., 2019), Mujoco (Todorov et al., 2012), and Intphys (Riochet et al., 2018), some of which are combined with natural language for question answering such as NLVR (Suhr et al., 2017), CLEVR (Johnson et al., 2017), and VQA (Antol et al., 2015). In a parallel work, Yi et al. (2020) introduced the CLEVRER dataset for reasoning about collision events from videos with different types of questions. In contrast, we develop the ability to reason and explain the behavior of dynamic physical systems by generating natural language. Natural language explanations and Commonsense reasoning. Several recent papers propose 7913 to use natural language for explanation and commonsense reasoning (Lei et al., 2016; Camburu et al., 2018; Forbes and Choi, 2017; Chai et al., 2018; Forbes et al.,"
2020.acl-main.706,P98-2209,0,0.135936,"ining and inference in a classifier and significantly improve CommonsenseQA performance. Bisk et al. (2020) propose to use a question answering task to test the model’s physical commonsense and reasoning ability. In contrast to the previous work, we focus on identifying pivotal physical events and then generating natural language explanations for them. We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention ("
2020.acl-main.706,P18-1151,1,0.861331,"We find that this two-step approach works more effectively. Table-to-text generation. Table-to-text generation aims to produce natural language output from structured input. Applications include generating sports commentaries from game records (Tanaka-Ishii et al., 1998; Chen and Mooney, 2008; Taniguchi et al., 2019), weather forecasts (Liang et al., 2009; Konstas and Lapata, 2012; Mei et al., 2016), biographical texts from Wikipedia infoboxes (Lebret et al., 2016; Sha et al., 2018; Liu et al., 2018; Perez-Beltrachini and Lapata, 2018), descriptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use di"
2020.acl-main.706,D15-1199,0,0.0419796,"Missing"
2020.acl-main.706,D17-1239,0,0.0932855,"alls fall. The red ball lands on the ground and the green ball lands on the red ball and rolls to the right over the black vertical bar. Generation (AVG) The red ball lands in the cubby and the green ball lands on top and a little to the right, sending the green ball right. It rolls over the short black wall of the cage and onto the floor, where it keeps rolling right towards the purple goal... The red ball falls and knocks the green ball off of its curved black platform and to the left. It rolls leftwards and continues falling until it lands on the purple floor... Generation (BiLSTM) maries (Wiseman et al., 2017). Second, the output generated by the BiLSTM model predicts the incorrect direction of motion for the green ball, an error that is occasionally seen across generation descriptions of both models. This indicates that a table-to-text paradigm for generating such solution explanations is not adequate for learning the direction of motion for the physical reasoning required for these explanations. Table 6: Example input records, gold annotation, and generated simulation description from the AVG and BiLSTM models, taken from example 00014:394. We show only a short segment of the actual input records"
2020.acl-main.706,2020.acl-main.224,0,0.0218279,"iptions of knowledge bases (ODonnell et al., 2000; Trisedya et al., 2018; Zhu et al., 2019; Yu et al., 2019) and source code (Iyer et al., 2016), and dialog response generation from slot-value pairs (Wen et al., 2015). Recently, neural encoder-decoder models (Sutskever et al., 2014; Cho et al., 2014) based on attention (Bahdanau et al., 2015; Luong et al., 2015) and copy mechanisms (Gu et al., 2016; Gulcehre et al., 2016) have shown promising results on table-to-text tasks (Wiseman et al., 2017; Gehrmann et al., 2018; Puduppully et al., 2019a,b; Iso et al., 2019; Castro Ferreira et al., 2019; Zhao et al., 2020; Chen et al., 2020a). While traditional methods use different modules for each generation stage in a pipeline (Reiter and Dale, 2000), neural table-to-text models are trained on large-scale datasets, relying on representation learning for generating coherent and grammatical texts. Puduppully et al. (2019a) propose a neural network approach that first selects data records to be mentioned and then generates a summary from the selected data, in an end-to-end fashion. Chen et al. (2020b) use pre-trained language models to generate descriptions for tabular data in a few shot setting. 7 Conclusions"
2020.acl-main.706,D14-1179,0,\N,Missing
2020.acl-main.706,P19-1202,0,\N,Missing
2020.acl-main.706,D19-1204,1,\N,Missing
2020.acl-main.706,C98-2204,0,\N,Missing
2020.coling-main.99,D18-1399,0,0.0308436,"vised: the model has access to concept-resource edges Acr and resource-resource edges Ar , as well as a percentage of the available concept-concept edges Ac , described later. 4.4 Node Features X Sparse Embeddings We used TF-IDF (term frequency–inverse document frequency) to get sparse embeddings for all nodes. We restricted the global vocabulary to be the 322 concept terms only, which means that the dimension of the node features is 322, as we aim to model keywords. Dense Embeddings As the concepts in our corpus often consist of phrases such as dynamic programming, we made use of Phrase2vec (Artetxe et al., 2018). Phrase2vec (P2V) is a generalization of skip-gram models (Mikolov et al., 2013) which learns n-gram embeddings during training, and here we aim to infer the embeddings of the concepts in our corpus. We trained the P2V model using only our corpus by treating each slide file as a short document as a sequence of tokens. For each resource node, we take an element-wise average of the P2V embeddings of each single token and phrases that resource covered. Similarly, for each concept node, we took element-wise average of the embeddings of each individual token and the concept phrase. In addition, we"
2020.coling-main.99,C16-1091,0,0.0274866,"e of the embeddings of each individual token and the concept phrase. In addition, we then utilized the BERT model (Devlin et al., 2018) as another type of dense embedding by extracting the representation of the [CLS] token of each resource. We fine-tuned the masked language modeling of BERT using our corpus. 4.5 Adjacency Matrix A To construct the adjacency matrix A, for each node pair (vi , vj ), we applied cosine similarity based on enriched TFIDF features3 as the value Aij . Previous work has applied cosine similarity for vector space models (Garc´ıa-Pablos et al., 2018; Zuin et al., 2018; Bhatia et al., 2016), so we believe it is a suitable method in our case. This way we were able to generate concept-resource edge values (Acr ) and resourceresource edge values (Ar ). Note that for concept-concept edge values Ac : 1 if ci is a prerequisite of cj , 0 otherwise. These values are not computed in the unsupervised setting. 3 This means that the TFIDF features are calculated on an extended vocabulary that includes all possible tokens appeared in the corpus. 1152 Method Concept embedding + classifier P2V (lb1) P2V (lb2) BERT (lb1) BERT (lb2) BERT (original) Graph-based methods DeepWalk (Perozzi et al., 2"
2020.coling-main.99,P18-1057,1,0.852054,"). Some research integrates feature engineering to represent a concept, inputting these features to a classic classifier to predict relationship of a given concept pair (Liang et al., 2017; Liang et al., 2018). The resources to learn those concept features include university course descriptions and materials as well as online educational data (Liu et al., 2016; Liang et al., 2017). Recently, Li et al. (2019) introduced a dataset containing 1,352 English lecture files collected from university-level lectures as well as 208 manually-labeled prerequisite relation topics, initially introduced in (Fabbri et al., 2018). To avoid feature engineering, they applied graph-based methods including GAE and VGAE (Kipf and Welling, 2017) which treat each concept as a node thus building a concept graph. They pretrained a Doc2vec model (Le and Mikolov, 2014) to infer each concept as a dense vector, and then trained the concept graph in a semi-supervised way. Finally, the model was able to recover unseen edges of a concept graph. Different from their work, we wish to do the prerequisite chain learning in an unsupervised manner, while in training, no concept relations will be provided to the model. 3 3.1 Dataset Resourc"
2020.coling-main.99,P16-1082,0,0.196517,"nsupervised prerequisite learning. We also expand an existing corpus which totals 1, 717 English Natural Language Processing (NLP)-related lecture slide files and manual concept pair annotations over 322 topics. 1 Introduction With the increasing amount of information available online, there is a rising need for structuring how one should process that information and learn knowledge efficiently in a reasonable order. As a result, recent work has tried to learn prerequisite relations among concepts, or which concept is needed to learn another concept within a concept graph (Liang et al., 2017; Gordon et al., 2016; AlSaad et al., 2018). Figure 1 shows an illustration of prerequisite chains as a directed graph. In such a graph, each node is a concept, and the direction of each edge indicates the prerequisite relation. Consider two concepts p and q, we define p → q as p is a prerequisite concept of q. For example, the concept Variational Autoencoders is a prerequisite concept of the concept Variational Graph Autoencoders. If someone wants to learn about the concept Variational Graph Autoencoders, the prerequisite concept Variation Autoencoder should appear in the prerequisite concept graph in order to cr"
2020.coling-main.99,P17-1133,0,0.225358,"s p and q, we define p → q as p is a prerequisite concept of q. For example, the concept Variational Autoencoders is a prerequisite concept of the concept Variational Graph Autoencoders. If someone wants to learn about the concept Variational Graph Autoencoders, the prerequisite concept Variation Autoencoder should appear in the prerequisite concept graph in order to create a proper study plan. Recent work has attempted to extract such prerequisite relationships from various types of materials including Wikipedia articles, university course dependencies or MOOCs (Massive Open Online Courses) (Pan et al., 2017; Gordon et al., 2016; Liang et al., 2017). However, these materials either need additional steps for pre-processing and cleaning, or contain too many noisy free-texts, bringing more challenges to prerequisite relation learning or extracting. Recently, Li et al. (2019) presented a collection of university lecture slide files mainly in NLP lectures with related prerequisite concept annotations. We expanded this dataset as we believe these lecture slides offer a concise yet comprehensive description of advanced topics. Deep models such as word embeddings (Mikolov et al., 2013) and more recently"
2020.emnlp-main.660,D15-1075,0,0.334807,"way. By a forward-looking perspective, instead, a single machine that can handle diverse (seen and unseen) tasks is desired. The reason is that we cannot always rely on expensive human resources to annotate large-scale task-specific labeled data, especially considering the inestimable number of tasks to be explored. Therefore, a reasonable attempt is to map diverse NLP tasks into a common learning problem—solving this common problem equals to solving any downstream NLP tasks, even some tasks that are new or have insufficient annotations. Textual entailment (aka. natural language inference in Bowman et al. (2015)) is the task of studying the relation of two assertive sentences, Premise (P) and Hypothesis (H): whether H is true given P. Textual entailment (TE) was originally brought up as a unified framework for modeling diverse NLP tasks (Dagan et al., 2005; Poliak et al., 2018). The research on TE dates back more than two decades and has made significant progress. Particularly, with the advances of deep neural networks and the availability of large-scale human annotated datasets, fine-tuned systems often claim surpassing human performance on certain benchmarks. Nevertheless, two open problems remain."
2020.emnlp-main.660,N19-1300,0,0.0152841,"AIL is to not only learn the matching function, but also map the instances in S and T to the same space. 8233 UFO-E NTAIL vs. STILTS. Given the source data S and a couple of labeled examples from the target T , STILTS (Phang et al., 2018) first trains RoBERTa on S, then fine-tune on the labeled examples of T . Both the pretraining and fine-tuning use the same RoBERTa system in Figure 1. It has been widely used as the state of the art technique for making use of related tasks to improve target tasks, especially when the target tasks have limited annotations (Liu et al., 2019; Sap et al., 2019; Clark et al., 2019). By the architecture, STILTS relies on the standard RoBERTa classifier which consists of a RoBERTa encoder and a logistic regression on the top; UFO-E NTAIL instead has a cross-task nearest neighbor block on the top of the RoBERTa encoder. STILTS tries to learn the target-specific parameters by tuning on the k labeled examples. However, this is very challenging if k is over small, like values {1, 3, 5, 10} we will use in our problems. We can also think STILTS learns class prototypical representations implicitly (i.e., the weights in the logistic regression layer), however, the bias term in th"
2020.emnlp-main.660,P07-1033,0,0.367983,"Missing"
2020.emnlp-main.660,N19-1423,0,0.0427111,"w years, the research on textual entailment has been driven by the creation of large-scale datasets, such as SNLI (Bowman et al., 2015), science domain SciTail (Khot et al., 2018), and multi-genre MNLI (Williams et al., 2018). Representative work includes the first attentive recurrent neural network (Rockt¨aschel et al., 2016) and its followers (Wang and Jiang, 2016; Wang et al., 2017), as well as the attentive convolutional networks such as attentive pooling (dos Santos et al., 2016) and attentive convolution (Yin and Sch¨utze, 2018), and self-attentive large-scale language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). All these studies result in systems that are overly tailored to the datasets. Our work differs in that we care more about fewshot applications of textual entailment, assuming that a new domain or an NLP task is not provided with rich annotated data. Generalization via domain adaptation. Two main types of domain adaptation (DA) problems have been studied in literature: supervised DA and semi-supervised DA. In the supervised case, we have access to a large annotated data in the source domain and a small-scale annotated data in the target domain (Daum´e III, 2007;"
2020.emnlp-main.660,N19-1039,0,0.0214007,"r work differs in that we care more about fewshot applications of textual entailment, assuming that a new domain or an NLP task is not provided with rich annotated data. Generalization via domain adaptation. Two main types of domain adaptation (DA) problems have been studied in literature: supervised DA and semi-supervised DA. In the supervised case, we have access to a large annotated data in the source domain and a small-scale annotated data in the target domain (Daum´e III, 2007; Kang and Feng, 2018). In the semi-supervised case, we have a large but unannotated corpus in the target domain (Miller, 2019). In contrast to semi-supervised DA, our work does not assume the availability of a large unlabeled data from the target domain or task. We also build more ambitious missions than the supervised DA since our work aims to adapt the model to new domains as well as new NLP tasks. Generalization via few-shot learning. Fewshot problems are studied typically in the image domain (Koch et al., 2015; Vinyals et al., 2016; Snell et al., 2017; Ren et al., 2018; Sung et al., 2018). The core idea in metric-based few-shot 8230 learning is similar to nearest neighbors. The predicted probability of a test ins"
2020.emnlp-main.660,W18-5441,0,0.0444137,"Missing"
2020.emnlp-main.660,N18-2017,0,0.0191469,"so think STILTS learns class prototypical representations implicitly (i.e., the weights in the logistic regression layer), however, the bias term in the logistic regression layer reflect mainly the distribution in the source S, which is less optimal for predicting in the target T . 4 Experiments We apply UFO-E NTAIL to entailment tasks of open domain and open NLP tasks. Experimental setup. Our system is implemented with Pytorch on the transformers package released by Huggingface2 . We use “RoBERTalarge” initialized by the pretrained language model. To mitigate the potential bias or artifacts (Gururangan et al., 2018) in sampling, all numbers of k-shot are average of five runs in seeds {42, 16, 32, 64, 128}. Due to GPU memory constraints, we only update the nearest neighbor block, the hidden layer and top-5 layers in RoBERTa. For other training configurations, please refer to our released code. Baselines. The following baselines are shared by experiments on open entailment tasks and open NLP tasks. • 0-shot. We assume zero examples from target domains. We train a RoBERTa classifier3 on 2 https://github.com/huggingface/ transformers 3 Specifically, the “RobertaForSequenceClassification” classifier in the Hu"
2020.emnlp-main.660,D13-1020,0,0.03641,"onverted to be textual entailment. Our work provides a new perspective to tackle these NLP issues, especially given only a couple of labeled examples. Question Answering. We attempt to handle the QA setting in which only a couple of labeled examples are provided. A QA problem can be formulated as a textual entailment problem—the document acts as the premise, and the (question, answer candidate), after converting into a natural sentence, acts as the hypothesis. Then a true (resp. false) hypothesis can be translated into a correct (resp. incorrect) answer. We choose the QA benchmark MCTest-500 (Richardson et al., 2013) which releases an entailment-formatted corpus. MCTest500 is a set of 500 items (split into 300 train, 50 dev and 150 test). Each item consists of a document, four questions followed by one correct answer, and three incorrect answers. Deep learning has not achieved significant success on it because of the limited training data (Trischler et al., 2016)—this is exactly our motivation that applying few-shot textual entailment to handle annotation-scarce NLP problems. For MCTest benchmark, we treat one question as one example. K-shot means we randomly sample k annotated questions (each corresponds"
2020.emnlp-main.660,D18-1514,0,0.0520493,"a method named Matching Networks. Snell et al. (2017) propose Prototypical Networks which first build prototypical representations for each class by summing up representations of supporting examples, then compare classes with test instances by squared Euclidean distances. Unlike fixed metric measures, the Relation Network (Sung et al., 2018) implements the comparison through learning a matching metric in a multi-layer architecture. In the language domain, Yu et al. (2018) combine multiple metrics learned from diverse clusters of training tasks for an unseen few-shot text classification task. Han et al. (2018) release a few-shot relation classification dataset “FewRel” and compare a couple of representative methods on it. These few-shot studies assume that, in the same domain, a part of the classes have limited samples, while other classes have adequate examples. In this work, we make a more challenging assumption that all classes in the target domain have only a couple of examples, and the training classes and testing classes are from different domains. Unified natural language processing. McCann et al. (2018) cast a group of NLP tasks as question answering over context, such as machine translatio"
2020.emnlp-main.660,D19-1454,0,0.0395338,"Missing"
2020.emnlp-main.660,2021.ccl-1.108,0,0.306573,"Missing"
2020.emnlp-main.660,P16-1041,0,0.012281,"emise, and the (question, answer candidate), after converting into a natural sentence, acts as the hypothesis. Then a true (resp. false) hypothesis can be translated into a correct (resp. incorrect) answer. We choose the QA benchmark MCTest-500 (Richardson et al., 2013) which releases an entailment-formatted corpus. MCTest500 is a set of 500 items (split into 300 train, 50 dev and 150 test). Each item consists of a document, four questions followed by one correct answer, and three incorrect answers. Deep learning has not achieved significant success on it because of the limited training data (Trischler et al., 2016)—this is exactly our motivation that applying few-shot textual entailment to handle annotation-scarce NLP problems. For MCTest benchmark, we treat one question as one example. K-shot means we randomly sample k annotated questions (each corresponds to a short article and has four answer candidates). We obtain k entailment pairs for the class “entailment” and 3k pairs for the class “non-entailment”. The official evaluation metrics in MCTest include accuracy and NDCG4 . Here, we report accuracy. Coreference Resolution. Coreference resolution aims to cluster the entities and pronouns that 8235 ref"
2020.emnlp-main.660,N16-1170,0,0.0186353,"not guarantee the accessibility of rich annotations. 2 Related Work Textual Entailment. Textual entailment was first studied in Dagan et al. (2005) and the main focus in the early stages was to study lexical and some syntactic features. In the past few years, the research on textual entailment has been driven by the creation of large-scale datasets, such as SNLI (Bowman et al., 2015), science domain SciTail (Khot et al., 2018), and multi-genre MNLI (Williams et al., 2018). Representative work includes the first attentive recurrent neural network (Rockt¨aschel et al., 2016) and its followers (Wang and Jiang, 2016; Wang et al., 2017), as well as the attentive convolutional networks such as attentive pooling (dos Santos et al., 2016) and attentive convolution (Yin and Sch¨utze, 2018), and self-attentive large-scale language models like BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019). All these studies result in systems that are overly tailored to the datasets. Our work differs in that we care more about fewshot applications of textual entailment, assuming that a new domain or an NLP task is not provided with rich annotated data. Generalization via domain adaptation. Two main types of domain ad"
2020.emnlp-main.660,Q18-1042,0,0.0190498,"we randomly sample k annotated questions (each corresponds to a short article and has four answer candidates). We obtain k entailment pairs for the class “entailment” and 3k pairs for the class “non-entailment”. The official evaluation metrics in MCTest include accuracy and NDCG4 . Here, we report accuracy. Coreference Resolution. Coreference resolution aims to cluster the entities and pronouns that 8235 refer to the same object. This is a challenging task in NLP, and greatly influences the capability of machines in understanding the text. We test on the coreference resolution benchmark GAP (Webster et al., 2018), a human-labeled corpus from Wikipedia for recognizing ambiguous pronoun-name coreference. An example from the GAP dataset is shown here: “McFerran’s horse farm was named Glen View. After his death in 1885, John E. Green acquired the farm.” For a specific pronoun in the sentence, GAP provides two entity candidates for it to link. To correctly understand the meaning of this sentence, a machine must know which person (“McFerran” or “John E. Green”) the pronoun “his” refers to. GAP has such kind of annotated examples of sizes split as 2k/454/2k in train/dev/test. Please note that some examples h"
2020.emnlp-main.660,N18-1101,0,0.652655,", c November 16–20, 2020. 2020 Association for Computational Linguistics entailment. We argue that textual entailment particularly matters when the target NLP task has insufficient annotations; in this way, some NLP tasks that share the same inference pattern and annotations are insufficient to build a task-specific model can be handled by a unified entailment system. Motivated by the two issues, we build UFOE NTAIL—the first ever generalized few-shot textual entailment system with the following setting. We first assume that we can access a largescale generic purpose TE dataset, such as MNLI (Williams et al., 2018); this dataset enables us to build a base entailment system with acceptable performance. To get even better performance in any new domain or new task, we combine the generic purpose TE dataset with a couple of domain/taskspecific examples to learn a better-performing entailment for that new domain/task. This is a reasonable assumption because in the real-world, any new domain or new task does not typically have large annotated data, but obtaining a couple of examples is usually feasible. Technically, our UFO-E NTAIL is inspired by the Prototypical Network (Snell et al., 2017), a popular metric"
2020.emnlp-main.660,D19-1404,1,0.883773,"Missing"
2020.emnlp-main.660,Q18-1047,1,0.890372,"Missing"
2020.emnlp-main.660,N18-1109,0,0.0760185,"ses for those supporting samples. Vinyals et al. (2016) compare each test instance with those supporting examples by the cosine distance in a method named Matching Networks. Snell et al. (2017) propose Prototypical Networks which first build prototypical representations for each class by summing up representations of supporting examples, then compare classes with test instances by squared Euclidean distances. Unlike fixed metric measures, the Relation Network (Sung et al., 2018) implements the comparison through learning a matching metric in a multi-layer architecture. In the language domain, Yu et al. (2018) combine multiple metrics learned from diverse clusters of training tasks for an unseen few-shot text classification task. Han et al. (2018) release a few-shot relation classification dataset “FewRel” and compare a couple of representative methods on it. These few-shot studies assume that, in the same domain, a part of the classes have limited samples, while other classes have adequate examples. In this work, we make a more challenging assumption that all classes in the target domain have only a couple of examples, and the training classes and testing classes are from different domains. Unifie"
2021.acl-long.535,N19-1071,0,0.0128072,"ve text; the graph is built by connecting claim and premise argumentative discourse units. We build on this framework for modeling discourse in conversational data. Few-Shot Summarization As the datasets we introduce are not on a scale with larger datasets, we focus on few-shot and domain transfer summarization techniques. Wang et al. (2019) examine domain adaptation in extractive summarization, while Hua and Wang (2017) examine domain adaptation between opinion and news summarization. Within unsupervised abstractive summarization, several approaches have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Braˇzinskas et al., 2020) and pretrained language models (Zhou and Rush, 2019; Laban et al., 2020). Recent work in abstractive (Zhang et al., 2019; Fabbri et al., 2020a) and extractive-compressive summarization (Desai et al., 2020) has shown the power of pretrained models for a few-shot transfer. The quality of models trained on several hundred examples in these papers is comparable to that of models trained on the equivalent full datasets. Thus, we believe that introducing curated validation and testing datasets consisting of a few hundred examples is a valuable contribut"
2021.acl-long.535,2020.acl-main.461,0,0.024727,"Missing"
2021.acl-long.535,D19-1291,0,0.248492,"onversation datasets: dialogue summarization from SAMSum (Gliwa et al., 2019b), heuristic-generated community question answering from CQASumm (Chowdhury and Chakraborty, 2018), meeting summarization data from AMI and ICSI, and smaller test sets in the news comments, discussion forum, and email domains. We believe that such benchmarking will facilitate a more straightforward comparison of conversation summarization models across domains. To unify modeling across these conversational domains, we propose to use recent work in end-toend argument mining (Lenz et al., 2020; Stab and Gurevych, 2014; Chakrabarty et al., 2019) to instantiate the theoretical graph framework which motivated our annotation protocol, proposed by Barker and Gaizauskas (2016a) for conversation summarization. This protocol is employed to both identify and use the “issues–viewpoints–assertions” argument structure (discussed in Related Work) for summarizing news comments. We construct this argument graph using entailment relations, linearize the graph, train a graph-to-text model (Ribeiro et al., 2020), and experiment with argument mining as a way to reduce noise in long-text input. Our contributions are the following: (1) we crowdsource da"
2021.acl-long.535,2020.emnlp-main.336,0,0.247035,"chieved stateof-the-art performance across summarization tasks and strong performance in zero and few-shot settings (Fabbri et al., 2020a). However, less work has focused on summarizing online conversations. Unlike documents, articles, and scientific papers, which contain specific linguistic structures and conventions such as topic sentences and abstracts, conversational text scatters main points across multiple utterances and between numerous writers. As a result, the text summarization task in the conversational data domain offers a challenging research field to test newly-developed models (Chen and Yang, 2020). Recently, Gliwa et al. (2019a) introduced a dataset for chat-dialogue conversation summarization consisting of 16k examples, the first largescale dataset of its kind. Previous work in conversation summarization was limited by the data available and focused primarily on meeting summarization, such as the AMI (Kraaij et al., 2005) and ICSI (Janin et al., 2003) datasets. The datasets 6866 Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 6866–6880 August 1–6, 2021. ©2021 Assoc"
2021.acl-long.535,2020.acl-main.460,0,0.0142711,"modeling discourse in conversational data. Few-Shot Summarization As the datasets we introduce are not on a scale with larger datasets, we focus on few-shot and domain transfer summarization techniques. Wang et al. (2019) examine domain adaptation in extractive summarization, while Hua and Wang (2017) examine domain adaptation between opinion and news summarization. Within unsupervised abstractive summarization, several approaches have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Braˇzinskas et al., 2020) and pretrained language models (Zhou and Rush, 2019; Laban et al., 2020). Recent work in abstractive (Zhang et al., 2019; Fabbri et al., 2020a) and extractive-compressive summarization (Desai et al., 2020) has shown the power of pretrained models for a few-shot transfer. The quality of models trained on several hundred examples in these papers is comparable to that of models trained on the equivalent full datasets. Thus, we believe that introducing curated validation and testing datasets consisting of a few hundred examples is a valuable contribution within the current paradigm, which was confirmed by the poor performance of models transferred from other domains c"
2021.acl-long.535,W17-4513,0,0.0129361,"s. Lenz et al. (2020) are the first to propose an end-to-end approach for constructing an argument graph (Stede et al., 2016), a structured representation of claims and premises in an argumentative text; the graph is built by connecting claim and premise argumentative discourse units. We build on this framework for modeling discourse in conversational data. Few-Shot Summarization As the datasets we introduce are not on a scale with larger datasets, we focus on few-shot and domain transfer summarization techniques. Wang et al. (2019) examine domain adaptation in extractive summarization, while Hua and Wang (2017) examine domain adaptation between opinion and news summarization. Within unsupervised abstractive summarization, several approaches have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Braˇzinskas et al., 2020) and pretrained language models (Zhou and Rush, 2019; Laban et al., 2020). Recent work in abstractive (Zhang et al., 2019; Fabbri et al., 2020a) and extractive-compressive summarization (Desai et al., 2020) has shown the power of pretrained models for a few-shot transfer. The quality of models trained on several hundred examples in these papers is compara"
2021.acl-long.535,W17-4218,0,0.0218394,"le contribution within the current paradigm, which was confirmed by the poor performance of models transferred from other domains compared to that trained on this validation data. 3 ConvoSumm In this section, we introduce our dataset selection, our annotation protocol, and the characteristics of our crowdsourced dataset. Data Selection For the news comments subdomain, we use the NYT Comments dataset, which consists of 2 million comments made on 9,000 New York Times articles published between 2017 and 2018. It is publicly available and has been used in work for news-comment relevance modeling (Kolhatkar and Taboada, 2017); it also contains metadata that may be of use in summarization modeling. For the discussion forums and debate subdomain, we select Reddit data from CoarseDiscourse (Zhang et al., 2017), which contains annotations about the discourse structure of the threads. For the community question answering subdomain, we use StackExchange (Stack), which provides access to all forums and has been used in modeling for answer relevance and question deduplication (Hoogeveen et al., 2015). We chose StackExchange over the commonly-used Yahoo! Answers data due to licensing reasons. For the email threads subdomai"
2021.acl-long.535,2020.acl-main.703,0,0.0439697,"of comments from a New York Times article discussing people’s favorite parts of the Super Bowl. The summary is an analysis of the comments and quantifies the viewpoints present. Introduction Automatic text summarization is the process of outputting the most salient parts of an input in a concise and readable form. Recent work in summarization has made significant progress due to introducing large-scale datasets such as the CNNDailyMail dataset (Nallapati et al., 2016) and the New York Times dataset (Sandhaus, 2008). Furthermore, the use of large self-supervised pretrained models such as BART (Lewis et al., 2020) and Pegasus (Zhang et al., 2019) has achieved stateof-the-art performance across summarization tasks and strong performance in zero and few-shot settings (Fabbri et al., 2020a). However, less work has focused on summarizing online conversations. Unlike documents, articles, and scientific papers, which contain specific linguistic structures and conventions such as topic sentences and abstracts, conversational text scatters main points across multiple utterances and between numerous writers. As a result, the text summarization task in the conversational data domain offers a challenging research"
2021.acl-long.535,W04-1013,0,0.0255664,"Missing"
2021.acl-long.535,2021.ccl-1.108,0,0.0300145,"Missing"
2021.acl-long.535,N15-1046,0,0.395525,"et has lower inter-document similarity, which presents challenges for models which rely strictly on redundancy in the input, and our datasets generally exhibit less layout bias, when compared to the analysis done in Dey et al. (2020b). Comparison to Existing Datasets Although previous work on conversation summarization, before the introduction of SAMSum (Gliwa et al., 2019b), has largely featured unsupervised or fewshot methods, there exist several datasets with reference summaries. These include SENSEI (Barker et al., 2016b) for news comments, the Argumentative Dialogue Summary Corpus (ADS) (Misra et al., 2015) for discussion forums, and the BC3 (Ulrich et al., 2009) dataset for email data. However, much of the existing datasets are not wide in scope. For example, SENSEI only covers six topics and the ADS Corpus covers one topic and only has 45 dialogues. Furthermore, they each pertain to one subdomain of conversation. Our dataset avoids these issues by covering four diverse subdomains of conversation and having approximately 500 annotated summaries for each subdomain. Additionally, since neural abstractive summarization baselines do not exist for these datasets, we benchmark our models on these dat"
2021.acl-long.535,K16-1028,0,0.0331568,"Missing"
2021.acl-long.535,D18-1206,0,0.0689261,"Missing"
2021.acl-long.535,N19-4009,0,0.0132363,"Summaries Recent work has shown the strength of text-based pretrained models on graph-to-text problems (Ribeiro et al., 2020). Following that work, we linearize the graph by following a depth-first approach starting from the Conversation Node. We found that inserting special tokens to signify edge types did not improve performance, likely due to the size of our data, and simply make use of an arrow → to signify the relationship between sentences. We train a sequence-to-sequence model on our linearized graph input, which we call -arg-graph. 5 Experimental Settings We use the fairseq codebase (Ott et al., 2019) for our experiments. Our base abstractive text summarization model is BART-large (Lewis et al., 2020), a pretrained denoising autoencoder with 336M parameters that builds on the sequence-to-sequence transformer of Vaswani et al. (2017). We finetune BART using a polynomial decay learning rate scheduler with Adam optimizer (Kingma and Ba, 2015). We used a learning rate of 3e-5 and warmup and total updates of 20 and 200, following previous few-shot transfer work (Fabbri et al., 2020a). We could have equally fine-tuned other pretrained models such as Pegasus (Zhang et al., 2019) or T5 (Raffel et"
2021.acl-long.535,W14-4407,1,0.80133,"s of our proposed datasets; (2) we benchmark state-of-theart models on these datasets as well as previous widely-used conversation summarization datasets to provide a clear baseline for future work; and (3) we apply argument mining to model the structure of our conversational data better as well as reduce noise in long-text input, showing comparable or improved results in both automatic and human evaluations.1 2 Related Work Modeling Conversation Summarization Early approaches to conversation summarization consisted of feature engineering (Shasha Xie et al., 2008), template selection methods (Oya et al., 2014), and statistical machine learning approaches (Galley, 2006; Wang and Cardie, 2013). More recent modeling approaches for dialogue summarization have attempted to take advantage of conversation structures found within the data through dialogue act classification (Goo and Chen, 2018b), discourse labeling (Ganesh and Dingliwal, 2019), topic segmentation (Liu et al., 2019c), and keypoint analysis (Liu et al., 2019a). Chen and Yang (2020) utilize multiple conversational structures from different perspectives in its sequence-tosequence model. However, such approaches focus exclusively on dialogue su"
2021.acl-long.535,D14-1006,0,0.369633,"active model on several conversation datasets: dialogue summarization from SAMSum (Gliwa et al., 2019b), heuristic-generated community question answering from CQASumm (Chowdhury and Chakraborty, 2018), meeting summarization data from AMI and ICSI, and smaller test sets in the news comments, discussion forum, and email domains. We believe that such benchmarking will facilitate a more straightforward comparison of conversation summarization models across domains. To unify modeling across these conversational domains, we propose to use recent work in end-toend argument mining (Lenz et al., 2020; Stab and Gurevych, 2014; Chakrabarty et al., 2019) to instantiate the theoretical graph framework which motivated our annotation protocol, proposed by Barker and Gaizauskas (2016a) for conversation summarization. This protocol is employed to both identify and use the “issues–viewpoints–assertions” argument structure (discussed in Related Work) for summarizing news comments. We construct this argument graph using entailment relations, linearize the graph, train a graph-to-text model (Ribeiro et al., 2020), and experiment with argument mining as a way to reduce noise in long-text input. Our contributions are the follo"
2021.acl-long.535,L16-1167,0,0.0240666,"his framework and advances in argument mining for end-to-end training for summarization. Argument Mining Work in argument mining (Stab and Gurevych, 2014) has aimed to identify these argumentative units and classify them into claims, premises, and major claims, or claims describing the key concept in a text. More recently, Chakrabarty et al. (2019) propose to finetune BERT (Devlin et al., 2019) for identifying argumentative units and relationships between them within a text and across texts. Lenz et al. (2020) are the first to propose an end-to-end approach for constructing an argument graph (Stede et al., 2016), a structured representation of claims and premises in an argumentative text; the graph is built by connecting claim and premise argumentative discourse units. We build on this framework for modeling discourse in conversational data. Few-Shot Summarization As the datasets we introduce are not on a scale with larger datasets, we focus on few-shot and domain transfer summarization techniques. Wang et al. (2019) examine domain adaptation in extractive summarization, while Hua and Wang (2017) examine domain adaptation between opinion and news summarization. Within unsupervised abstractive summari"
2021.acl-long.535,P10-1078,0,0.0476971,"subdomain, we use StackExchange (Stack), which provides access to all forums and has been used in modeling for answer relevance and question deduplication (Hoogeveen et al., 2015). We chose StackExchange over the commonly-used Yahoo! Answers data due to licensing reasons. For the email threads subdomain, we use the publicly-available W3C corpus (Craswell et al., 2005). Previous work also made use of this dataset for email summarization (Ulrich et al., 2008) but provided only a small sample of 40 email threads, for which we provide transfer testing results. We generally follow the guidance of Tomasoni and Huang (2010), from summarizing community question answering forums, for determining which subsets of data to select from the above datasets. We remove an example if (1) there were less than five posts (four in the case of email threads; “post” refers to any answer, comment, or email); (2) the longest post was over 400 words; (3) the sum of all post lengths was outside of [100, 1400] words (although we extended this maximum length for NYT comments); or (4) the average length of the posts was outside of the [50, 300] words interval. For Stack data, we first filtered answers which received a negative communi"
2021.acl-long.535,P13-1137,0,0.0384572,"Missing"
2021.acl-long.535,N18-1101,0,0.0180917,"ction, following Chakrabarty et al. (2019) and making use of data for argument mining from that paper and from Stab and Gurevych (2014). The output of this step can also simply be used without further graph construction as a less noisy version of the input, which we call -arg-filtered. Relationship Type Classification We follow the procedure in Lenz et al. (2020) and use entailment to determine the relationship between argumentative units within a document. However, rather than using the classifier provided, we make use of RoBERTa (Liu et al., 2019b) fine-tuned on the MNLI entailment dataset (Williams et al., 2018). Rather than using both support and contradiction edges between claims and premises, we make the simplification that all relationships can be captured with support edges, as we are dealing with a single document in this step. Within a single text, the 6871 Dataset/Method NYT Reddit Stack Email Lexrank 22.30/3.87/19.14 22.71/4.52/19.38 26.30/5.62/22.27 16.04/3.68/13.38 Textrank 25.11/3.75/20.61 24.38/4.54/19.84 25.43/4.40/20.58 19.50/3.90/16.18 BERT-ext 25.88/3.81/22.00 24.51/4.18/20.95 26.84/4.63/22.85 25.46/6.17/21.73 Table 4: ROUGE-1/2/L results for extractive LexRank (Erkan and Radev, 2004"
2021.acl-long.535,P19-1503,0,0.0134024,"on this framework for modeling discourse in conversational data. Few-Shot Summarization As the datasets we introduce are not on a scale with larger datasets, we focus on few-shot and domain transfer summarization techniques. Wang et al. (2019) examine domain adaptation in extractive summarization, while Hua and Wang (2017) examine domain adaptation between opinion and news summarization. Within unsupervised abstractive summarization, several approaches have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Braˇzinskas et al., 2020) and pretrained language models (Zhou and Rush, 2019; Laban et al., 2020). Recent work in abstractive (Zhang et al., 2019; Fabbri et al., 2020a) and extractive-compressive summarization (Desai et al., 2020) has shown the power of pretrained models for a few-shot transfer. The quality of models trained on several hundred examples in these papers is comparable to that of models trained on the equivalent full datasets. Thus, we believe that introducing curated validation and testing datasets consisting of a few hundred examples is a valuable contribution within the current paradigm, which was confirmed by the poor performance of models transferred"
2021.acl-long.535,2020.findings-emnlp.19,0,0.0196521,"in sequences over the typical 1024 max encoder length with which BART is trained, we copied the encoder positional embeddings to allow sequences up to length 2048. To address the input-length of meeting summaries, which range from 6k to 12k tokens, we use the Longformer (Beltagy et al., 2020), which allows for sequences up to length 16k to6872 Method/Dataset HMNet DDA-GCN Longformer-BART Longformer-BART-arg AMI 53.02/18.57/53.15/22.32/54.20/20.72/51.36 54.47/20.83/51.74 ICSI 46.28/10.60/43.03/12.14/40.26 44.17/11.69/41.33 Table 6: ROUGE-1/2/L results for DDA-GCN (Feng et al., 2020) and HMNet (Zhu et al., 2020) on the AMI and ICSI meeting summarization dataset along with our Longformer and Longformer-arg models. kens. We initialize the Longformer model with BART parameters trained on the CNN-DailyMail dataset, as the meeting summarization datasets contain fewer than 100 data points. We otherwise fine-tune models from vanilla BART, following intuition in few-shot summarization (Fabbri et al., 2020a) and based on initial experiments. In the tables which follow, ”-arg” refers to any model trained with argument-mining-based input, and we specify which -arg-graph or -arg-filtered settings were used for e"
2021.acl-short.127,2020.acl-main.366,0,0.0321302,"9) or Phrase2Vec (Artetxe et al., 2018) embeddings. We consider four edge types to build the adjacency matrix A: Ac,s : edges between source concept nodes; Arc : edges between all resource nodes and concept nodes; Ar : edges between resource nodes only; and Ac,t : edges between target concept nodes. In unsupervised prerequisite chain learning, Ac,s —concept relations of the source domain—are known, and the task is to predict Ac,t —concept relations of the target domain. For Arc and Ar , we calculate cosine similarities based on node embeddings, consistent with previous works (Li et al., 2019; Chiu et al., 2020). Cross-Domain Graph Encoder VGAE (Kipf and Welling, 2016) contains a graph neural network (GCN) encoder (Kipf and Welling, 2017) and an inner product decoder. In a GCN, the hidden representation of a node i in the next layer is computed using only the information of direct neighbours and the node itself. To account for cross-domain knowledge, we additionally consider the domain neighbours for each node i. These domain neighbours are a set of common or semantically similar 1006 concepts from the other domain.1 We define the cross-domain graph encoder as: (l+1) hi =σ X (l) (l) W (l) hj + W (l)"
2021.acl-short.127,P16-1082,0,0.022844,"proach of inferring the relations within a concept graph (Liang et al., 2018; Li et al., 2019, 2020). In a concept graph, we define p → q as the notion that learning concept p is a prerequisite to learning concept q. Existing methods formulate this question as a classification task. A typical method is to encode concept pairs and train a classifier to predict if there is a prerequisite relation (Alzetta et al., 2019; Yu et al., 2020). However, this method requires annotated prerequisite pairs during training. Alternatively, others have used graphbased models to predict prerequisite relations. Gordon et al. (2016) proposed information-theoretic approaches to infer concept dependencies. Li et al. (2019) modeled a concept graph using Variational Graph Autoencoders (VGAE) (Kipf and Welling, 2016), training their model to infer unseen prerequisite relations in a semi-supervised way. While most of the previous methods were supervised or semisupervised, Li et al. (2020) introduced RelationalVGAE, which enabled unsupervised learning on prerequisite relations. Existing work mainly focuses on prerequisite relations within a single domain. In this paper, we tackle the task of cross-domain prerequisite chain lear"
2021.acl-short.127,2020.coling-main.99,1,0.877257,"Missing"
2021.acl-short.127,2020.acl-main.285,0,0.0189224,"rich domain (source domain) to an information-poor domain (target domain), substantially surpassing other baseline models. Also, we expand an existing dataset by introducing two new domains––CV and Bioinformatics (BIO). The annotated data and resources, as well as the code, will be made publicly available. 1 Figure 1: Cross-domain prerequisite chains. Introduction With the rapid growth of online educational resources in diverse fields, people need an efficient way to acquire new knowledge. Building a concept graph can help people design a correct and efficient study path (ALSaad et al., 2018; Yu et al., 2020). There are mainly two approaches to learning prerequisite relations between concepts: one is to extract the relations directly from course content, video sequences, textbooks, or Wikipedia articles (Yang et al., 2015b; Pan et al., 2017; Alzetta et al., 2019), but this approach requires extra work on feature engineering and keyword extraction. Our method follows a different approach of inferring the relations within a concept graph (Liang et al., 2018; Li et al., 2019, 2020). In a concept graph, we define p → q as the notion that learning concept p is a prerequisite to learning concept q. Exis"
2021.acl-short.127,P17-1133,0,0.0186871,"ta and resources, as well as the code, will be made publicly available. 1 Figure 1: Cross-domain prerequisite chains. Introduction With the rapid growth of online educational resources in diverse fields, people need an efficient way to acquire new knowledge. Building a concept graph can help people design a correct and efficient study path (ALSaad et al., 2018; Yu et al., 2020). There are mainly two approaches to learning prerequisite relations between concepts: one is to extract the relations directly from course content, video sequences, textbooks, or Wikipedia articles (Yang et al., 2015b; Pan et al., 2017; Alzetta et al., 2019), but this approach requires extra work on feature engineering and keyword extraction. Our method follows a different approach of inferring the relations within a concept graph (Liang et al., 2018; Li et al., 2019, 2020). In a concept graph, we define p → q as the notion that learning concept p is a prerequisite to learning concept q. Existing methods formulate this question as a classification task. A typical method is to encode concept pairs and train a classifier to predict if there is a prerequisite relation (Alzetta et al., 2019; Yu et al., 2020). However, this meth"
2021.emnlp-demo.37,N18-1150,0,0.0194848,"mat that can be directly used by SummVis and its UI. Other systems also exist for text summarization. Text summarization has been a long-standing task for natural language processing. Early systems for summarization had been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014), the task of abstractive summarization has been receiving more attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://huggingface.co/datasets MEAD3 is a platform for multi-lingual summarization. Sumy4 can produce extractive summaries from HTML pages or plain texts, using several traditional summarization methods including Mihalcea and Tarau (2004) and Erkan and Radev (2004). OpenNMT 5 is mostly for machine translation, but it also hosts several summarizat"
2021.emnlp-demo.37,2020.emnlp-main.336,0,0.02927,"rfriendly and easy-to-use APIs. 2 2.1 Related Work Text Summarization Jia et al., 2020). Moreover, the field of text summarization has also been broadening into several subcategories, such as multi-document summarization (McKeown and Radev, 1995; Carbonell and Goldstein, 1998; Ganesan et al., 2010; Fabbri et al., 2019), query-based summarization (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Kulkarni et al., 2020) and dialogue summarization (Zhong et al., 2021; Chen et al., 2021a,b; Gliwa et al., 2019; Chen and Yang, 2020; Zhu et al., 2020). The proposed tasks, along with the datasets can also be classified by domain, such as news (Hermann et al., 2015; Fabbri et al., 2019; Narayan et al., 2018), meetings (Zhong et al., 2021; Carletta et al., 2005; Janin et al., 2003), scientifc literature (Cohan et al., 2018; Yasunaga et al., 2019), and medical records (DeYoung et al., 2021; Zhang et al., 2019; Portet et al., 2009). 2.2 Existing Systems for Summarization Transformers (Wolf et al., 2020) includes a large number of transformer-based models in its Modelhub1 , including BART (Lewis et al., 2020) and Pegasus (Zhan"
2021.emnlp-demo.37,P18-1063,0,0.0172102,"sed by SummVis and its UI. Other systems also exist for text summarization. Text summarization has been a long-standing task for natural language processing. Early systems for summarization had been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014), the task of abstractive summarization has been receiving more attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://huggingface.co/datasets MEAD3 is a platform for multi-lingual summarization. Sumy4 can produce extractive summaries from HTML pages or plain texts, using several traditional summarization methods including Mihalcea and Tarau (2004) and Erkan and Radev (2004). OpenNMT 5 is mostly for machine translation, but it also hosts several summarization models such as Gehr"
2021.emnlp-demo.37,2021.emnlp-main.594,0,0.0779975,"Missing"
2021.emnlp-demo.37,P19-1102,1,0.883087,"Datasets Usercreated SAMSum Evaluation Existing Datasets HMNet HMNet HMNet x Models Multidoc Dialogue -based … ROUGE ROUGE ROUGE Visualization Evaluation Metrics ROUGE BERT-score … Architecture User select HMNet Workflow Figure 1: SummerTime is a toolkit for helping nonexpert users to find the best summarization models for their own data and use cases. has broadened its scope with the introduction of more summarization tasks, such as query-based summarization (Dang, 2005; Zhong et al., 2021), long-document summarization (Cohan et al., 2018), multi-document summarization (Ganesan et al., 2010; Fabbri et al., 2019), dialogue summarization (Gliwa et al., 2019; Zhong et al., 2021). Such summarization tasks can also be from different do1 Introduction mains (Hermann et al., 2015; Zhang et al., 2019; Cohan et al., 2018). The goal of text summarization is to generate short and fluent summaries from longer textual sources, However, as the field rapidly grows, it is often while preserving the most salient information in hard for NLP non-experts to follow all relevant new them. Benefiting from recent advances of deep models, datasets, and evaluation metrics. Moreover, neural networks, in particular sequence to s"
2021.emnlp-demo.37,2020.emnlp-main.751,0,0.0772031,"Missing"
2021.emnlp-demo.37,2021.findings-acl.449,0,0.138628,"nded for expert users, maintaining the userfriendly and easy-to-use APIs. 2 2.1 Related Work Text Summarization Jia et al., 2020). Moreover, the field of text summarization has also been broadening into several subcategories, such as multi-document summarization (McKeown and Radev, 1995; Carbonell and Goldstein, 1998; Ganesan et al., 2010; Fabbri et al., 2019), query-based summarization (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Kulkarni et al., 2020) and dialogue summarization (Zhong et al., 2021; Chen et al., 2021a,b; Gliwa et al., 2019; Chen and Yang, 2020; Zhu et al., 2020). The proposed tasks, along with the datasets can also be classified by domain, such as news (Hermann et al., 2015; Fabbri et al., 2019; Narayan et al., 2018), meetings (Zhong et al., 2021; Carletta et al., 2005; Janin et al., 2003), scientifc literature (Cohan et al., 2018; Yasunaga et al., 2019), and medical records (DeYoung et al., 2021; Zhang et al., 2019; Portet et al., 2009). 2.2 Existing Systems for Summarization Transformers (Wolf et al., 2020) includes a large number of transformer-based models in its Modelhub1 , including"
2021.emnlp-demo.37,C10-1039,0,0.30761,"ale-LILY/ SummerTime. Datasets Usercreated SAMSum Evaluation Existing Datasets HMNet HMNet HMNet x Models Multidoc Dialogue -based … ROUGE ROUGE ROUGE Visualization Evaluation Metrics ROUGE BERT-score … Architecture User select HMNet Workflow Figure 1: SummerTime is a toolkit for helping nonexpert users to find the best summarization models for their own data and use cases. has broadened its scope with the introduction of more summarization tasks, such as query-based summarization (Dang, 2005; Zhong et al., 2021), long-document summarization (Cohan et al., 2018), multi-document summarization (Ganesan et al., 2010; Fabbri et al., 2019), dialogue summarization (Gliwa et al., 2019; Zhong et al., 2021). Such summarization tasks can also be from different do1 Introduction mains (Hermann et al., 2015; Zhang et al., 2019; Cohan et al., 2018). The goal of text summarization is to generate short and fluent summaries from longer textual sources, However, as the field rapidly grows, it is often while preserving the most salient information in hard for NLP non-experts to follow all relevant new them. Benefiting from recent advances of deep models, datasets, and evaluation metrics. Moreover, neural networks, in pa"
2021.emnlp-demo.37,P19-1098,0,0.0163965,"or summarization had been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014), the task of abstractive summarization has been receiving more attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://huggingface.co/datasets MEAD3 is a platform for multi-lingual summarization. Sumy4 can produce extractive summaries from HTML pages or plain texts, using several traditional summarization methods including Mihalcea and Tarau (2004) and Erkan and Radev (2004). OpenNMT 5 is mostly for machine translation, but it also hosts several summarization models such as Gehrmann et al. (2018). 3 SummerTime The main purpose of SummerTime is to help nonexpert users navigate through various summarization models, datasets and evaluation metric"
2021.emnlp-demo.37,D18-1443,0,0.0131555,"2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://huggingface.co/datasets MEAD3 is a platform for multi-lingual summarization. Sumy4 can produce extractive summaries from HTML pages or plain texts, using several traditional summarization methods including Mihalcea and Tarau (2004) and Erkan and Radev (2004). OpenNMT 5 is mostly for machine translation, but it also hosts several summarization models such as Gehrmann et al. (2018). 3 SummerTime The main purpose of SummerTime is to help nonexpert users navigate through various summarization models, datasets and evaluation metrics, and provide simple yet comprehensive information for them to select the models that best suit their needs. Figure 1 shows how SummerTime is split into different modules to help users achieve such goal. We will describe in detail each component of SummerTime in the following sections. With Section 3.1, we introduce the models we support in all subcategories of summarization; in Section 3.2 we list all the existing datasets we support and how us"
2021.emnlp-demo.37,N16-1012,0,0.0369574,"lso allow SummerTime to store output in a format that can be directly used by SummVis and its UI. Other systems also exist for text summarization. Text summarization has been a long-standing task for natural language processing. Early systems for summarization had been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014), the task of abstractive summarization has been receiving more attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://huggingface.co/datasets MEAD3 is a platform for multi-lingual summarization. Sumy4 can produce extractive summaries from HTML pages or plain texts, using several traditional summarization methods including Mihalcea and Tarau (2004) and Erkan and Radev (2004). OpenNMT 5 is mostly for machine"
2021.emnlp-demo.37,D19-5409,0,0.237805,"g Datasets HMNet HMNet HMNet x Models Multidoc Dialogue -based … ROUGE ROUGE ROUGE Visualization Evaluation Metrics ROUGE BERT-score … Architecture User select HMNet Workflow Figure 1: SummerTime is a toolkit for helping nonexpert users to find the best summarization models for their own data and use cases. has broadened its scope with the introduction of more summarization tasks, such as query-based summarization (Dang, 2005; Zhong et al., 2021), long-document summarization (Cohan et al., 2018), multi-document summarization (Ganesan et al., 2010; Fabbri et al., 2019), dialogue summarization (Gliwa et al., 2019; Zhong et al., 2021). Such summarization tasks can also be from different do1 Introduction mains (Hermann et al., 2015; Zhang et al., 2019; Cohan et al., 2018). The goal of text summarization is to generate short and fluent summaries from longer textual sources, However, as the field rapidly grows, it is often while preserving the most salient information in hard for NLP non-experts to follow all relevant new them. Benefiting from recent advances of deep models, datasets, and evaluation metrics. Moreover, neural networks, in particular sequence to sequence those models and datasets are often"
2021.emnlp-demo.37,N18-2097,0,0.338502,"notebook demo, is available at https://github.com/Yale-LILY/ SummerTime. Datasets Usercreated SAMSum Evaluation Existing Datasets HMNet HMNet HMNet x Models Multidoc Dialogue -based … ROUGE ROUGE ROUGE Visualization Evaluation Metrics ROUGE BERT-score … Architecture User select HMNet Workflow Figure 1: SummerTime is a toolkit for helping nonexpert users to find the best summarization models for their own data and use cases. has broadened its scope with the introduction of more summarization tasks, such as query-based summarization (Dang, 2005; Zhong et al., 2021), long-document summarization (Cohan et al., 2018), multi-document summarization (Ganesan et al., 2010; Fabbri et al., 2019), dialogue summarization (Gliwa et al., 2019; Zhong et al., 2021). Such summarization tasks can also be from different do1 Introduction mains (Hermann et al., 2015; Zhang et al., 2019; Cohan et al., 2018). The goal of text summarization is to generate short and fluent summaries from longer textual sources, However, as the field rapidly grows, it is often while preserving the most salient information in hard for NLP non-experts to follow all relevant new them. Benefiting from recent advances of deep models, datasets, and"
2021.emnlp-demo.37,P06-1039,0,0.254744,"Missing"
2021.emnlp-demo.37,2020.emnlp-main.295,0,0.0194715,"r each model, and provide simple explanations for all the evaluation metrics we support. Moreover, we go beyond pure numbers and provide visualization of the performance and output of different models, to facilitate users in making decisions about which models or pipelines to finally adopt. The purpose of SummerTime is not to replace any previous work, on the contrary, we integrate existing libraries and place them in the same framework. We provide wrappers around such libraries intended for expert users, maintaining the userfriendly and easy-to-use APIs. 2 2.1 Related Work Text Summarization Jia et al., 2020). Moreover, the field of text summarization has also been broadening into several subcategories, such as multi-document summarization (McKeown and Radev, 1995; Carbonell and Goldstein, 1998; Ganesan et al., 2010; Fabbri et al., 2019), query-based summarization (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Kulkarni et al., 2020) and dialogue summarization (Zhong et al., 2021; Chen et al., 2021a,b; Gliwa et al., 2019; Chen and Yang, 2020; Zhu et al., 2020). The proposed tasks, along with the datasets can"
2021.emnlp-demo.37,D19-1259,0,0.0161296,"nthology network as well as their citation networks and their manually labeled summaries. QMSum (Zhong et al., 2021) is designed for querybased multi-domain meeting summarization. It collects the meetings from AMI and ICSI dataset, as well as the committee meetings of the Welsh Parliament and Parliament of Canada. Experts manually wrote summaries for each meeting. ArXiv (Cohan et al., 2018) is a dataset extracted from research papers for abstractive summarization of single, longer-form documents. For each research paper from arxiv.org, its abstract is used as ground-truth summaries. PubMedQA (Jin et al., 2019) is a question answering dataset on the biomedical domain. Every QA instance contains a short answer and a long answer, latter of which can also be used for query-based summarization. SummScreen (Chen et al., 2021a) consists of community contributed transcripts of television show episodes from The TVMegaSite, Inc. (TMS) and ForeverDream (FD). The summary of each tranWith SummerTime, users can easily create or convert their own summarization datasets and evaluate all the supporting models within the framework. However, in the case that no such datasets are available, SummerTime also provides ac"
2021.emnlp-demo.37,W17-1004,0,0.0188427,"rk, on the contrary, we integrate existing libraries and place them in the same framework. We provide wrappers around such libraries intended for expert users, maintaining the userfriendly and easy-to-use APIs. 2 2.1 Related Work Text Summarization Jia et al., 2020). Moreover, the field of text summarization has also been broadening into several subcategories, such as multi-document summarization (McKeown and Radev, 1995; Carbonell and Goldstein, 1998; Ganesan et al., 2010; Fabbri et al., 2019), query-based summarization (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Kulkarni et al., 2020) and dialogue summarization (Zhong et al., 2021; Chen et al., 2021a,b; Gliwa et al., 2019; Chen and Yang, 2020; Zhu et al., 2020). The proposed tasks, along with the datasets can also be classified by domain, such as news (Hermann et al., 2015; Fabbri et al., 2019; Narayan et al., 2018), meetings (Zhong et al., 2021; Carletta et al., 2005; Janin et al., 2003), scientifc literature (Cohan et al., 2018; Yasunaga et al., 2019), and medical records (DeYoung et al., 2021; Zhang et al., 2019; Portet et al., 2009). 2.2 Existing Systems f"
2021.emnlp-demo.37,W04-3252,0,0.268655,"ion metrics for text summarization. SummerTime adopts a subset of such metrics in SummEval that are more popular and easier to understand. SummerTime also works well with SummVis (Vig et al., 2021), which provides an interactive way of analysing summarization results on the token-level. We also allow SummerTime to store output in a format that can be directly used by SummVis and its UI. Other systems also exist for text summarization. Text summarization has been a long-standing task for natural language processing. Early systems for summarization had been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014), the task of abstractive summarization has been receiving more attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://hugg"
2021.emnlp-demo.37,W07-0734,0,0.0848569,"datasets for future work, for which we describe in more detail in Section 5. 3.3 QMSum is_query_base=True is_dialogue_based=True is_multi_doc=False … pipeline construction Query-based Dialogue-based TF-IDF Flatten-then -summarize BM25 HMNet Total # solutions: 2*1 + 2*1*5 = 12 Single-doc Summarization TextRank BART Pegasus … (5 in total) Figure 3: An illustration of how SummerTime finds solutions to a specific tasks defined by a dataset. The red star denotes that an ending point is reached. Evaluation Metrics relation with human judgements; To evaluate the performance of each supported METEOR (Lavie and Agarwal, 2007) is based model on certain dataset, SummerTime integrates on word-to-word matches between generated and with SummEval (Fabbri et al., 2020) and provides reference summaries, it consider two words as the following evaluation metrics for the users to “aligned” based on a Porter stemmer (Porter, 2001) understand model performance: or synonyms in WordNet (Miller, 1995); ROUGE (Lin, 2004) is a recall-oriented method based on overlapping n-grams, word sequences, BERTScore (Zhang et al., 2020b) computes tokenlevel similarity between sentences with the contexand word pairs between the generated output"
2021.emnlp-demo.37,P19-1209,0,0.0123922,"UI. Other systems also exist for text summarization. Text summarization has been a long-standing task for natural language processing. Early systems for summarization had been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014), the task of abstractive summarization has been receiving more attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://huggingface.co/datasets MEAD3 is a platform for multi-lingual summarization. Sumy4 can produce extractive summaries from HTML pages or plain texts, using several traditional summarization methods including Mihalcea and Tarau (2004) and Erkan and Radev (2004). OpenNMT 5 is mostly for machine translation, but it also hosts several summarization models such as Gehrmann et al. (2018). 3 Su"
2021.emnlp-demo.37,2020.acl-main.703,0,0.192007,"iwa et al., 2019; Chen and Yang, 2020; Zhu et al., 2020). The proposed tasks, along with the datasets can also be classified by domain, such as news (Hermann et al., 2015; Fabbri et al., 2019; Narayan et al., 2018), meetings (Zhong et al., 2021; Carletta et al., 2005; Janin et al., 2003), scientifc literature (Cohan et al., 2018; Yasunaga et al., 2019), and medical records (DeYoung et al., 2021; Zhang et al., 2019; Portet et al., 2009). 2.2 Existing Systems for Summarization Transformers (Wolf et al., 2020) includes a large number of transformer-based models in its Modelhub1 , including BART (Lewis et al., 2020) and Pegasus (Zhang et al., 2020a), two strong neural summarizers we also use in SummerTime. It also hosts datasets for various NLP tasks in its Datasets2 library (Lhoest et al., 2021). Despite the wide coverage in transformer-based models, Transformers do not natively support models or pipelines that can handle aforementioned subcategories of summarization tasks. Moreover, it assumes certain NLP proficiency in its users, thus is harder for nonexpert users to use. We integrate with Transformers and Datasets to import the state-of-the-art models, as well as summarization datasets into SummerTim"
2021.emnlp-demo.37,D18-1206,0,0.130842,"egories, such as multi-document summarization (McKeown and Radev, 1995; Carbonell and Goldstein, 1998; Ganesan et al., 2010; Fabbri et al., 2019), query-based summarization (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Kulkarni et al., 2020) and dialogue summarization (Zhong et al., 2021; Chen et al., 2021a,b; Gliwa et al., 2019; Chen and Yang, 2020; Zhu et al., 2020). The proposed tasks, along with the datasets can also be classified by domain, such as news (Hermann et al., 2015; Fabbri et al., 2019; Narayan et al., 2018), meetings (Zhong et al., 2021; Carletta et al., 2005; Janin et al., 2003), scientifc literature (Cohan et al., 2018; Yasunaga et al., 2019), and medical records (DeYoung et al., 2021; Zhang et al., 2019; Portet et al., 2009). 2.2 Existing Systems for Summarization Transformers (Wolf et al., 2020) includes a large number of transformer-based models in its Modelhub1 , including BART (Lewis et al., 2020) and Pegasus (Zhang et al., 2020a), two strong neural summarizers we also use in SummerTime. It also hosts datasets for various NLP tasks in its Datasets2 library (Lhoest et al., 2021). Despite t"
2021.emnlp-demo.37,P17-1098,0,0.0213957,"tegrate existing libraries and place them in the same framework. We provide wrappers around such libraries intended for expert users, maintaining the userfriendly and easy-to-use APIs. 2 2.1 Related Work Text Summarization Jia et al., 2020). Moreover, the field of text summarization has also been broadening into several subcategories, such as multi-document summarization (McKeown and Radev, 1995; Carbonell and Goldstein, 1998; Ganesan et al., 2010; Fabbri et al., 2019), query-based summarization (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Kulkarni et al., 2020) and dialogue summarization (Zhong et al., 2021; Chen et al., 2021a,b; Gliwa et al., 2019; Chen and Yang, 2020; Zhu et al., 2020). The proposed tasks, along with the datasets can also be classified by domain, such as news (Hermann et al., 2015; Fabbri et al., 2019; Narayan et al., 2018), meetings (Zhong et al., 2021; Carletta et al., 2005; Janin et al., 2003), scientifc literature (Cohan et al., 2018; Yasunaga et al., 2019), and medical records (DeYoung et al., 2021; Zhang et al., 2019; Portet et al., 2009). 2.2 Existing Systems for Summarization Tr"
2021.emnlp-demo.37,D15-1222,0,0.0294279,"ROUGE (Lin, 2004) is a recall-oriented method based on overlapping n-grams, word sequences, BERTScore (Zhang et al., 2020b) computes tokenlevel similarity between sentences with the contexand word pairs between the generated output and tualized embeddings of each tokens. the gold summary; BLEU (Papineni et al., 2002) measures n-gram precision and employs a penalty for brevity, BLEU Since we assume no NLP background from our is often used as an evaluation metric for machine target users, we make sure that SummerTime protranslation; vides a short explanation for each evaluation metric ROUGE-WE (Ng and Abrecht, 2015) aims to as well as a clarification whether high or low scores go beyond surface lexical similarity and uses pre- are better for a given evaluation metric, to help the trained word embeddings to measure the similarity non-expert users understand the meaning of the between different words and presents a better cor- metrics and use them to make decisions. 333 Algorithm 1 S ELECT(M, D, E) Input: M: a pool of models to choose from, D: a set of examples from a dataset, T : a set of evaluation metrics, d: initial resource number, k: increase resource factor Output: M ⊆ M: a subset of models; 1: Init"
2021.emnlp-demo.37,P02-1040,0,0.111277,"-to-word matches between generated and with SummEval (Fabbri et al., 2020) and provides reference summaries, it consider two words as the following evaluation metrics for the users to “aligned” based on a Porter stemmer (Porter, 2001) understand model performance: or synonyms in WordNet (Miller, 1995); ROUGE (Lin, 2004) is a recall-oriented method based on overlapping n-grams, word sequences, BERTScore (Zhang et al., 2020b) computes tokenlevel similarity between sentences with the contexand word pairs between the generated output and tualized embeddings of each tokens. the gold summary; BLEU (Papineni et al., 2002) measures n-gram precision and employs a penalty for brevity, BLEU Since we assume no NLP background from our is often used as an evaluation metric for machine target users, we make sure that SummerTime protranslation; vides a short explanation for each evaluation metric ROUGE-WE (Ng and Abrecht, 2015) aims to as well as a clarification whether high or low scores go beyond surface lexical similarity and uses pre- are better for a given evaluation metric, to help the trained word embeddings to measure the similarity non-expert users understand the meaning of the between different words and pres"
2021.emnlp-demo.37,D15-1044,0,0.0471218,"e token-level. We also allow SummerTime to store output in a format that can be directly used by SummVis and its UI. Other systems also exist for text summarization. Text summarization has been a long-standing task for natural language processing. Early systems for summarization had been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014), the task of abstractive summarization has been receiving more attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://huggingface.co/datasets MEAD3 is a platform for multi-lingual summarization. Sumy4 can produce extractive summaries from HTML pages or plain texts, using several traditional summarization methods including Mihalcea and Tarau (2004) and Erkan and Radev (2004). OpenNMT 5"
2021.emnlp-demo.37,2020.emnlp-main.647,0,0.0173155,"-doc Dial. Lang. News News Open-domain News Scientific articles Meetings Scientific papers Biomedial TV shows 300k 56k 16k 226k 1k 1k 215k 273.5k 26.9k 781 2.1k 94 431 4.7k 9.0k 4.9k 239 6.6k 56 263.8 20 23.3 150 69.6 220 43 337.4 7 7 7 7 7 3 7 3 7 7 3 7 7 7 7 7 7 7 7 7 3 7 7 3 7 7 3 News 1.5M 635 31.8 7 7 7 En En En En En En En En En Fr, De, Es, Ru, Tr Table 1: The summarization datasets included in SummerTime. “Dial.” is short for “Dialogue” while “Lang.” denotes the languages of each of the datasets. script is the recap from TMS, or a recap of the FD shows from Wikipedia and TVMaze. MLSum (Scialom et al., 2020) is a large-scale multilingual summarization dataset. It contains over 1.5M news articles in five languages, namely French, German, Spanish, Russian, and Turkish. A summary of all datasets included in SummerTime is shown as Table 1, it is worth noticing that the fields in this table (i.e., domain, query-based, multi-doc, etc) are also incorporated in each of the dataset classes (e.g., SAMSumDataset) as class variables, so that such labels can later be used to identify applicable models. Similar with the models classes, we include a short description for each of the datasets. Note that the data"
2021.emnlp-demo.37,2021.acl-demo.18,0,0.361066,"d subcategories of summarization tasks. Moreover, it assumes certain NLP proficiency in its users, thus is harder for nonexpert users to use. We integrate with Transformers and Datasets to import the state-of-the-art models, as well as summarization datasets into SummerTime, under the same easy-to-use framework. Another library that we integrate with is SummEval (Fabbri et al., 2020), which is a collection of evaluation metrics for text summarization. SummerTime adopts a subset of such metrics in SummEval that are more popular and easier to understand. SummerTime also works well with SummVis (Vig et al., 2021), which provides an interactive way of analysing summarization results on the token-level. We also allow SummerTime to store output in a format that can be directly used by SummVis and its UI. Other systems also exist for text summarization. Text summarization has been a long-standing task for natural language processing. Early systems for summarization had been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014"
2021.emnlp-demo.37,2020.acl-main.552,0,0.0112616,"ad been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014), the task of abstractive summarization has been receiving more attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://huggingface.co/datasets MEAD3 is a platform for multi-lingual summarization. Sumy4 can produce extractive summaries from HTML pages or plain texts, using several traditional summarization methods including Mihalcea and Tarau (2004) and Erkan and Radev (2004). OpenNMT 5 is mostly for machine translation, but it also hosts several summarization models such as Gehrmann et al. (2018). 3 SummerTime The main purpose of SummerTime is to help nonexpert users navigate through various summarization models, datasets and evaluation metrics, and provide simpl"
2021.emnlp-demo.37,P19-1100,0,0.015216,"natural language processing. Early systems for summarization had been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014), the task of abstractive summarization has been receiving more attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://huggingface.co/datasets MEAD3 is a platform for multi-lingual summarization. Sumy4 can produce extractive summaries from HTML pages or plain texts, using several traditional summarization methods including Mihalcea and Tarau (2004) and Erkan and Radev (2004). OpenNMT 5 is mostly for machine translation, but it also hosts several summarization models such as Gehrmann et al. (2018). 3 SummerTime The main purpose of SummerTime is to help nonexpert users navigate through various summariza"
2021.emnlp-demo.37,D19-5410,0,0.0347652,"Missing"
2021.emnlp-demo.37,D19-1324,0,0.0147696,"ssing. Early systems for summarization had been focusing on extractive summarization (Mihalcea and Tarau, 2004; Erkan and Radev, 2004), by finding the most salient sentences from source documents. With the advancement of neural networks (Bahdanau et al., 2014; Sutskever et al., 2014), the task of abstractive summarization has been receiving more attention (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Lebanoff et al., 2019) while neural-based methods have also been developed for extractive summarization (Zhong et al., 2019b,a; Xu and Durrett, 2019; Cho et al., 2019; Zhong et al., 2020; 330 1 2 https://huggingface.co/models https://huggingface.co/datasets MEAD3 is a platform for multi-lingual summarization. Sumy4 can produce extractive summaries from HTML pages or plain texts, using several traditional summarization methods including Mihalcea and Tarau (2004) and Erkan and Radev (2004). OpenNMT 5 is mostly for machine translation, but it also hosts several summarization models such as Gehrmann et al. (2018). 3 SummerTime The main purpose of SummerTime is to help nonexpert users navigate through various summarization models, datasets and"
2021.emnlp-demo.37,2020.findings-emnlp.19,0,0.12154,"-use APIs. 2 2.1 Related Work Text Summarization Jia et al., 2020). Moreover, the field of text summarization has also been broadening into several subcategories, such as multi-document summarization (McKeown and Radev, 1995; Carbonell and Goldstein, 1998; Ganesan et al., 2010; Fabbri et al., 2019), query-based summarization (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Kulkarni et al., 2020) and dialogue summarization (Zhong et al., 2021; Chen et al., 2021a,b; Gliwa et al., 2019; Chen and Yang, 2020; Zhu et al., 2020). The proposed tasks, along with the datasets can also be classified by domain, such as news (Hermann et al., 2015; Fabbri et al., 2019; Narayan et al., 2018), meetings (Zhong et al., 2021; Carletta et al., 2005; Janin et al., 2003), scientifc literature (Cohan et al., 2018; Yasunaga et al., 2019), and medical records (DeYoung et al., 2021; Zhang et al., 2019; Portet et al., 2009). 2.2 Existing Systems for Summarization Transformers (Wolf et al., 2020) includes a large number of transformer-based models in its Modelhub1 , including BART (Lewis et al., 2020) and Pegasus (Zhang et al., 2020a), t"
2021.eval4nlp-1.8,P17-1089,0,0.0122453,"iple columns and properly producing conjunctions). While existing studies show that the models tend to fail on challenging cases that involve novel user expression (Suhr et al., 2020) and SQL structures (Suhr et al., 2020; Shaw et al., 2021), our diagnosis exposes more robustness issues in their surface form understanding (even with seemingly simple inputs), and highlights the importance of addressing such issues in the modeling foundation (Bommasani et al., 2021). Our dataset and code are released as an extensible test suite. seek to quickly create data for creating a domain specific parser. Iyer et al. (2017) also demonstrated that crowdsourced annotations from such approaches, as in turn user feedback in an online setting, can be used improve parses and detect incorrect queries. Although originally designed in the context of transfer-based machine translation to generate translation pairs (Chiang, 2005), SCFG’s have also been adapted in previous semantic parsing work (Wong and Mooney, 2006, 2007) for generating new sentence-parse pairs. More recent utilization’s of SCFG’s for semantic parsing induce the grammar and use the resulting data for additional training and pre-training (Jia and Liang, 20"
2021.eval4nlp-1.8,N18-1202,0,0.0424064,"Missing"
2021.eval4nlp-1.8,P16-1002,0,0.0184244,"yer et al. (2017) also demonstrated that crowdsourced annotations from such approaches, as in turn user feedback in an online setting, can be used improve parses and detect incorrect queries. Although originally designed in the context of transfer-based machine translation to generate translation pairs (Chiang, 2005), SCFG’s have also been adapted in previous semantic parsing work (Wong and Mooney, 2006, 2007) for generating new sentence-parse pairs. More recent utilization’s of SCFG’s for semantic parsing induce the grammar and use the resulting data for additional training and pre-training (Jia and Liang, 2016; Yu et al., 2020). 2 3 Robustness Testing Finally, Ribeiro et al. (2020) has demonstrated the efficacy of handcrafting templates for generating data points to “unit test” the models. We design synchronous context-free grammar (SCFG) production rules to generate test data for specific cross-database semantic parsing capabilities. Other NLP evaluation frameworks that look beyond accuracy and target a more general set of NLP tasks have also been proposed (Goel et al., 2021; Liu et al., 2021; Kiela et al., 2021). Related Work Paraphrasing A number of augmentation methods have been made to create"
2021.eval4nlp-1.8,2020.intexsempar-1.5,1,0.693521,"ed the method for synthesizing the corresponding test data. We evaluated a variety of high performing models using the proposed approach, and identified several non-obvious weaknesses across models (e.g. unable to correctly select many columns). Our dataset and code are released as a test suite at http://github.com/hclent/ BehaviorCheckingSemPar. 1 Figure 1: The database (top) is applied to our SCFG production rule (middle) to produce a new example for the DISTINCT category (bottom). See Appendix B for production rules of other categories. sented with novel user utterances (Suhr et al., 2020; Radhakrishnan et al., 2020; Shaw et al., 2021), databases (Suhr et al., 2020) and SQL query structures (Finegan-Dollak et al., 2018; Suhr et al., 2020; Shaw et al., 2021). As baseline performance climbs ever upward, at what point can we confidently deploy our models to end users, and how will we know we have reached this point? Inspired by Ribeiro et al. (2020), which has shown the effectiveness of simple, systematic, and heuristic behavior checking strategies for evaluating the robustness of NLP models, we propose a controllable, non-adversarial unit testing approach to shed more light on the capabilities of crossdata"
2021.eval4nlp-1.8,2020.acl-main.442,0,0.063231,"dragomir.radev@yale.edu, victorialin@fb.com Abstract The benchmark performance of cross-database semantic parsing has climbed steadily in recent years, catalyzed by the wide adoption of pre-trained language models. Yet existing work have shown that state-of-the-art crossdatabase semantic parsers struggle to generalize to novel user utterances, databases and query structures. To obtain transparent details on the strengths and limitation of these models, we propose a diagnostic testing approach based on controlled synthesis of canonical natural language and SQL pairs. Inspired by the CheckList (Ribeiro et al., 2020), we characterize a set of essential capabilities for cross-database semantic parsing models, and detailed the method for synthesizing the corresponding test data. We evaluated a variety of high performing models using the proposed approach, and identified several non-obvious weaknesses across models (e.g. unable to correctly select many columns). Our dataset and code are released as a test suite at http://github.com/hclent/ BehaviorCheckingSemPar. 1 Figure 1: The database (top) is applied to our SCFG production rule (middle) to produce a new example for the DISTINCT category (bottom). See App"
2021.eval4nlp-1.8,2021.acl-long.75,0,0.0618911,"ng the corresponding test data. We evaluated a variety of high performing models using the proposed approach, and identified several non-obvious weaknesses across models (e.g. unable to correctly select many columns). Our dataset and code are released as a test suite at http://github.com/hclent/ BehaviorCheckingSemPar. 1 Figure 1: The database (top) is applied to our SCFG production rule (middle) to produce a new example for the DISTINCT category (bottom). See Appendix B for production rules of other categories. sented with novel user utterances (Suhr et al., 2020; Radhakrishnan et al., 2020; Shaw et al., 2021), databases (Suhr et al., 2020) and SQL query structures (Finegan-Dollak et al., 2018; Suhr et al., 2020; Shaw et al., 2021). As baseline performance climbs ever upward, at what point can we confidently deploy our models to end users, and how will we know we have reached this point? Inspired by Ribeiro et al. (2020), which has shown the effectiveness of simple, systematic, and heuristic behavior checking strategies for evaluating the robustness of NLP models, we propose a controllable, non-adversarial unit testing approach to shed more light on the capabilities of crossdatabase semantic parser"
2021.eval4nlp-1.8,D13-1161,0,0.0376,"has demonstrated the efficacy of handcrafting templates for generating data points to “unit test” the models. We design synchronous context-free grammar (SCFG) production rules to generate test data for specific cross-database semantic parsing capabilities. Other NLP evaluation frameworks that look beyond accuracy and target a more general set of NLP tasks have also been proposed (Goel et al., 2021; Liu et al., 2021; Kiela et al., 2021). Related Work Paraphrasing A number of augmentation methods have been made to create paraphrases of the input query, with methods such as synonym replacement (Kwiatkowski et al., 2013), use of a paraphrase model (Berant and Liang, 2014), and backwards utterance generation (Zhong et al., 2020). While these approaches ensure the creation of additional examples with more variation on the natural language side, they can be vulnerable to error, when a wrong synonym or paraphrase is chosen by a model. Although such errors may amount to just noise when used as additional training data in conjunction with a benchmark dataset, they make evaluation on such generated sets impossible, unless examples with errors are manually removed from the dataset. Generating Canonical Natural Langua"
2021.eval4nlp-1.8,2020.acl-main.703,0,0.0155145,"uage questions based on SQL queries (Figure 1). This grammar features production rules that evaluate important categories of SQL element types such as clauses (e.g. SELECT and WHERE), as well as commonly used operators including aggreIntroduction Cross-database semantic parsing, the task of mapping natural language utterances to SQL queries for any database, has attracted increasing attention since the introduction of benchmarks like WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018). The advent of pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020) has further accelerated the progress in this area (Lin et al., 2020; Yu et al., 2020; Shi et al., 2020; Wang et al., 2020; Choi et al., 2020). Despite impressive gains on standard benchmarks, studies on cross-database semantic parsing models show that they still suffer from outof-distribution (OOD) generalization when pre˚ Work done during an internship at Salesforce Research. 73 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 73–83 c November 10, 2021. 2021 Association for Computational Linguistics gators (MAX), conditionals (BETWEEN), and l"
2021.eval4nlp-1.8,2020.acl-main.742,0,0.147771,"models, and detailed the method for synthesizing the corresponding test data. We evaluated a variety of high performing models using the proposed approach, and identified several non-obvious weaknesses across models (e.g. unable to correctly select many columns). Our dataset and code are released as a test suite at http://github.com/hclent/ BehaviorCheckingSemPar. 1 Figure 1: The database (top) is applied to our SCFG production rule (middle) to produce a new example for the DISTINCT category (bottom). See Appendix B for production rules of other categories. sented with novel user utterances (Suhr et al., 2020; Radhakrishnan et al., 2020; Shaw et al., 2021), databases (Suhr et al., 2020) and SQL query structures (Finegan-Dollak et al., 2018; Suhr et al., 2020; Shaw et al., 2021). As baseline performance climbs ever upward, at what point can we confidently deploy our models to end users, and how will we know we have reached this point? Inspired by Ribeiro et al. (2020), which has shown the effectiveness of simple, systematic, and heuristic behavior checking strategies for evaluating the robustness of NLP models, we propose a controllable, non-adversarial unit testing approach to shed more light on t"
2021.eval4nlp-1.8,2020.findings-emnlp.438,1,0.870372,"roduction rules that evaluate important categories of SQL element types such as clauses (e.g. SELECT and WHERE), as well as commonly used operators including aggreIntroduction Cross-database semantic parsing, the task of mapping natural language utterances to SQL queries for any database, has attracted increasing attention since the introduction of benchmarks like WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018). The advent of pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020) has further accelerated the progress in this area (Lin et al., 2020; Yu et al., 2020; Shi et al., 2020; Wang et al., 2020; Choi et al., 2020). Despite impressive gains on standard benchmarks, studies on cross-database semantic parsing models show that they still suffer from outof-distribution (OOD) generalization when pre˚ Work done during an internship at Salesforce Research. 73 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 73–83 c November 10, 2021. 2021 Association for Computational Linguistics gators (MAX), conditionals (BETWEEN), and logical operators (OR). We handcraft the rules for these categories t"
2021.eval4nlp-1.8,2020.acl-main.677,0,0.215153,"SQL element types such as clauses (e.g. SELECT and WHERE), as well as commonly used operators including aggreIntroduction Cross-database semantic parsing, the task of mapping natural language utterances to SQL queries for any database, has attracted increasing attention since the introduction of benchmarks like WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018). The advent of pre-trained language models (Peters et al., 2018; Devlin et al., 2019; Liu et al., 2019; Lewis et al., 2020) has further accelerated the progress in this area (Lin et al., 2020; Yu et al., 2020; Shi et al., 2020; Wang et al., 2020; Choi et al., 2020). Despite impressive gains on standard benchmarks, studies on cross-database semantic parsing models show that they still suffer from outof-distribution (OOD) generalization when pre˚ Work done during an internship at Salesforce Research. 73 Proceedings of the 2nd Workshop on Evaluation and Comparison of NLP Systems (Eval4NLP 2021), pages 73–83 c November 10, 2021. 2021 Association for Computational Linguistics gators (MAX), conditionals (BETWEEN), and logical operators (OR). We handcraft the rules for these categories to ensure that the generated question-query pairs are s"
2021.eval4nlp-1.8,2021.acl-demo.34,0,0.014339,"emantic parsing induce the grammar and use the resulting data for additional training and pre-training (Jia and Liang, 2016; Yu et al., 2020). 2 3 Robustness Testing Finally, Ribeiro et al. (2020) has demonstrated the efficacy of handcrafting templates for generating data points to “unit test” the models. We design synchronous context-free grammar (SCFG) production rules to generate test data for specific cross-database semantic parsing capabilities. Other NLP evaluation frameworks that look beyond accuracy and target a more general set of NLP tasks have also been proposed (Goel et al., 2021; Liu et al., 2021; Kiela et al., 2021). Related Work Paraphrasing A number of augmentation methods have been made to create paraphrases of the input query, with methods such as synonym replacement (Kwiatkowski et al., 2013), use of a paraphrase model (Berant and Liang, 2014), and backwards utterance generation (Zhong et al., 2020). While these approaches ensure the creation of additional examples with more variation on the natural language side, they can be vulnerable to error, when a wrong synonym or paraphrase is chosen by a model. Although such errors may amount to just noise when used as additional trainin"
2021.eval4nlp-1.8,P15-1129,0,0.0643565,"Missing"
2021.eval4nlp-1.8,N06-1056,0,0.128664,"ance of addressing such issues in the modeling foundation (Bommasani et al., 2021). Our dataset and code are released as an extensible test suite. seek to quickly create data for creating a domain specific parser. Iyer et al. (2017) also demonstrated that crowdsourced annotations from such approaches, as in turn user feedback in an online setting, can be used improve parses and detect incorrect queries. Although originally designed in the context of transfer-based machine translation to generate translation pairs (Chiang, 2005), SCFG’s have also been adapted in previous semantic parsing work (Wong and Mooney, 2006, 2007) for generating new sentence-parse pairs. More recent utilization’s of SCFG’s for semantic parsing induce the grammar and use the resulting data for additional training and pre-training (Jia and Liang, 2016; Yu et al., 2020). 2 3 Robustness Testing Finally, Ribeiro et al. (2020) has demonstrated the efficacy of handcrafting templates for generating data points to “unit test” the models. We design synchronous context-free grammar (SCFG) production rules to generate test data for specific cross-database semantic parsing capabilities. Other NLP evaluation frameworks that look beyond accura"
2021.findings-acl.435,N19-1423,0,0.0379345,"52 words) doc. Curation summarization news (229∼842 words) hypothesis length single sentence (4∼18 words) single sentence (6∼22 words) multi-sent (80 ∼100 words) 3∼4 sent. (40∼50 words) multi-sent (64∼279 words) Table 1: Data resources that are reformatted into D OC NLI. even a document, and the hypotheses cover a large range of granularity: from a single sentence to a longer paragraph (e.g., 250 words); (ii) Diverse domains; (iii) No severe artifacts; for example, we do not include the hypotheses that can be easily found “grammatically incorrect” by well-trained language models such as BERT (Devlin et al., 2019). 3.1 Data Preprocessing Table 1 lists all the resources that we use to create D OC NLI. Briefly, D OC NLI combines and reformats five existing NLP benchmarks: adversarial NLI (ANLI) (Nie et al., 2020), the question answering benchmark SQuAD (Rajpurkar et al., 2016) and three summarization benchmarks (DUC20013 , CNN/DailyMail (Nallapati et al., 2016), and Curation4 (Curation, 2020)). Next, we describe how each data resource is integrated into D OC NLI. ANLI to D OC NLI. ANLI is a large-scale NLI dataset collected via an iterative, adversarial human-and-model-in-the-loop procedure. In each roun"
2021.findings-acl.435,N18-2017,0,0.0191109,"skar et al., 2019), a state-ofthe-art controllable text generator, to generate a new sentence which is used to replace the selected sentence. This operation generates a new “fake” summary. Table 2 illustrates a (document, real summary) pair in the Curation dataset, and the three types of “fake” summaries we generated. 3.2 Mitigating Artifacts in D OC NLI In Section 3.1, we transformed these NLI, QA and summarization datasets to satisfy the format of D OC NLI. We refer this resulting dataset as rawD OC NLI. In consideration of the common artifacts in some popular sentence-level NLI benchmarks (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018), we tried a “hypothesis-only” baseline based on RoBERTa on this raw-D OC NLI. Sur4916 D OC NLI raw D OC NLI added pairs entail ANLI SQuAD {(D, R)} {(Fi+ , Fi )} not entail ANLI SQuAD {(D, Fi )} {(Fi , R)} train 466,653 475,661 942,314 entail. not entail sum dev 28,890 205,368 234,258 test 33,128 233,958 267,086 Table 4: Data sizes of D OC NLI. Table 3: D: a document in summarization benchmarks; R: a real summary; Fi : a fake summary derived from R (i=1· · · n); Fi+ : Using CTRL to insert a generated sentence between a random pair of consecutive sentences"
2021.findings-acl.435,2021.ccl-1.108,0,0.0736248,"Missing"
2021.findings-acl.435,D15-1075,0,0.0303937,"amework to study those NLP problems by casting the background text as a premise and the text of target meaning as a hypothesis. Then, a good NLI recognizer can be considerably translated to a well-performing system regarding respective NLP tasks. NLI was first studied in (Dagan et al., 2005). Research in the early stages was mostly driven by the PASCAL Recognizing Textual Entailment (RTE) challenges which are annual competitions with benchmark datasets released. In the past few years, the study of NLI has moved forward rapidly along with the construction of large-scale datasets, such as SNLI (Bowman et al., 2015), the science domain SciTail (Khot et al., 2018) and multi-genre MNLI (Williams et al., 2018), etc. However, some NLI datasets may not be suitable any more for solving downstream NLP problems since they were commonly crowdsourced in isolation from any end task 2 (Khot et al., 2018). In addition, most NLI datasets and studies paid attention merely to sentence-level inference — both the premises and hypotheses are single (and usually short) sentences. This makes them unsuitable for other open-ended NLP problems. For example, to verify the factual correctness of a document summary, sentence-level"
2021.findings-acl.435,P17-1152,0,0.0474086,"Missing"
2021.findings-acl.435,2020.acl-main.441,0,0.404372,"and answer span as inputs. Kry´sci´nski et al. (2019) created a (document, sentence) pair data “FactCC” to train a classifier for checking the factual correctness of single sentences in automatically generated summaries. FactCC is specific to the target summarization benchmark dataset, so it is unclear how well FactCC can generalize to other summarization benchmarks and other NLP problems. In addition, only single sentences act as hypotheses. Nevertheless, that literature exactly showed that document-level NLI, especially the inference of document-level hypotheses, is highly desirable. ANLI (Nie et al., 2020) also gather multi-sentence as premises. However, the sentence sizes in ANLI premises are pretty limited and the hypotheses in ANLI are single sentences consistently. To our knowledge, our D OC NLI is the first dataset that uses hypotheses longer than single sentences, and stays closely with end NLP tasks. 3 Data Creation What kind of document-level NLI dataset is preferred? (i) We want the premise is a paragraph or 4914 original task domain premise length various multi-sentence ANLI NLI (wiki, news, etc.) (20∼94 words) paragraph SQuAD QA wiki (27∼237 words) DUC doc. summarization news (2001)"
2021.findings-acl.435,S18-2023,0,0.0401762,"Missing"
2021.findings-acl.435,D13-1020,0,0.308729,"cessitate document-level inference. Task-specific finetuning can further improve the performance and achieve new state of the art for some end tasks. 2 Related Work To our knowledge, document-level NLI has attracted very little ink in the community, possibly because of the lack of labeled datasets. In this section, we mainly describe some prior NLI datasets that share some spirits with our D OC NLI. End-task driven. As mentioned in Section 1, the RTE series were driven by downstream NLP tasks such as information retrieval, information extraction, question answering, and summarization. MCTest (Richardson et al., 2013) is a question answering task in which a paragraph is given as background knowledge, then each question is paired with a positive answer and some negative answers. The MCTest benchmark released an NLI version of this corpus by treating the whole paragraph as a premise and combining the question and answer candidates as hypotheses. SciTail (Khot et al., 2018) is also derived from the end QA task of answering multiple-choice school-level science questions. Unlike MCTest, the premises in SciTail are single sentences selected by an information retrieval approach. By casting an end NLP task as NLI,"
2021.findings-acl.435,N18-1074,0,0.236391,", some NLI datasets may not be suitable any more for solving downstream NLP problems since they were commonly crowdsourced in isolation from any end task 2 (Khot et al., 2018). In addition, most NLI datasets and studies paid attention merely to sentence-level inference — both the premises and hypotheses are single (and usually short) sentences. This makes them unsuitable for other open-ended NLP problems. For example, to verify the factual correctness of a document summary, sentence-level NLI systems cannot be of much help (Kry´sci´nski et al., 2019). Considering the fact-checking task FEVER (Thorne et al., 2018) as another example, in order to figure out the truth value of a claim against a Wikipedia article, NLI 2 Except for RTE and SciTail 4913 Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pages 4913–4922 August 1–6, 2021. ©2021 Association for Computational Linguistics has to be done on individual sentences instead of using the whole article as the premise. In short, some NLP tasks require the reasoning of NLI to go beyond the sentence granularity, regarding both the premise and the hypothesis. In this work, we introduce D OC NLI, a largescale dataset for document-lev"
2021.findings-acl.435,L18-1239,0,0.0242261,"able text generator, to generate a new sentence which is used to replace the selected sentence. This operation generates a new “fake” summary. Table 2 illustrates a (document, real summary) pair in the Curation dataset, and the three types of “fake” summaries we generated. 3.2 Mitigating Artifacts in D OC NLI In Section 3.1, we transformed these NLI, QA and summarization datasets to satisfy the format of D OC NLI. We refer this resulting dataset as rawD OC NLI. In consideration of the common artifacts in some popular sentence-level NLI benchmarks (Gururangan et al., 2018; Poliak et al., 2018; Tsuchiya, 2018), we tried a “hypothesis-only” baseline based on RoBERTa on this raw-D OC NLI. Sur4916 D OC NLI raw D OC NLI added pairs entail ANLI SQuAD {(D, R)} {(Fi+ , Fi )} not entail ANLI SQuAD {(D, Fi )} {(Fi , R)} train 466,653 475,661 942,314 entail. not entail sum dev 28,890 205,368 234,258 test 33,128 233,958 267,086 Table 4: Data sizes of D OC NLI. Table 3: D: a document in summarization benchmarks; R: a real summary; Fi : a fake summary derived from R (i=1· · · n); Fi+ : Using CTRL to insert a generated sentence between a random pair of consecutive sentences in the Fi , in a way similar to what w"
2021.findings-acl.435,N18-1101,0,0.0271823,"t of target meaning as a hypothesis. Then, a good NLI recognizer can be considerably translated to a well-performing system regarding respective NLP tasks. NLI was first studied in (Dagan et al., 2005). Research in the early stages was mostly driven by the PASCAL Recognizing Textual Entailment (RTE) challenges which are annual competitions with benchmark datasets released. In the past few years, the study of NLI has moved forward rapidly along with the construction of large-scale datasets, such as SNLI (Bowman et al., 2015), the science domain SciTail (Khot et al., 2018) and multi-genre MNLI (Williams et al., 2018), etc. However, some NLI datasets may not be suitable any more for solving downstream NLP problems since they were commonly crowdsourced in isolation from any end task 2 (Khot et al., 2018). In addition, most NLI datasets and studies paid attention merely to sentence-level inference — both the premises and hypotheses are single (and usually short) sentences. This makes them unsuitable for other open-ended NLP problems. For example, to verify the factual correctness of a document summary, sentence-level NLI systems cannot be of much help (Kry´sci´nski et al., 2019). Considering the fact-checkin"
2021.findings-acl.435,P19-1217,0,0.0221376,"NLI +finetune Prior state-of-the-art FEVER binary 50.00 86.64 87.51 88.84 89.44 – SciTail b-MNLI majority 60.33 66.66 ESIM (Chen et al., 2017) 70.60 – De-Att (Parikh et al., 2016) 72.30 – DGEM (Khot et al., 2018) 77.30 – BERT-large 89.71 90.55 Longformer-base 92.23 92.03 RoBERTa-large 95.13 93.95 D OC NLI (pretrain) 78.17 91.13 +finetune 96.04 94.07 Prior state-of-the-art 97.70 – MCTest v160 v500 25.00 25.00 75.41 70.66 82.50 78.66 90.00 85.83 90.83 90.66 80.00 75.50 Table 6: Train on D OC NLI, test on NLP tasks that are out-of-domain and require document-level NLI. SOTA of MCTest comes from (Yu et al., 2019). ous SOTA fact-checking system. We combine “refute” and “not-enough-info” as a single class “not entail”, and rename this data as “FEVERbinary”. We randomly split FEVER-binary by 203,152/8,209/10,000 for train/dev/test respectively.7 MCTest (Richardson et al., 2013). In Related Work, we have introduced MCTest. Briefly, it is a multi-choice QA benchmark in the domain of fictional story. The authors of MCTest released an NLI-version MCTest by combining the question and the positive (resp. negative) answer candidate as a positive (resp. negative) hypothesis. MCTest consists of two subsets. MCTes"
2021.findings-emnlp.377,2021.naacl-main.109,0,0.0578886,"Missing"
2021.findings-emnlp.377,N18-2097,0,0.0178644,"equence Summarization Recent summarization models are based on Transformer (Vaswani et al., 2017) that has a quadratic time and memory complexity with respect to the input length, preventing it from being used for longer sequences. To address this issue, Beltagy et al. (2020) used the sliding window and global attention, while Zaheer et al. (2020) used a combination of random, sliding window and global attention mechanism to reduce the quadratic complexity to close-linear. Previous benchmarks for long sequence summarization mostly focus on documents instead of dialogues: P UB M ED and A RXIV (Cohan et al., 2018) consists of scientific papers which are typically very long; B ILL S UM (Kornilova and Eidelman, 2019) is a corpus of U.S. Congressional bills and their summaries; B IG PATENT (Sharma et al., 2019) contains 1.3 million U.S. patent files and human-written summaries. Methodology In this section, we will introduce the dataset used to evaluate and pretrain the model, two types of summary models, and the details of the experiment setup. 3.1 Datasets To explore the problems in long dialogue summarization, we leverage three different long dialogue summarization tasks as main datasets: QMSum (Zhong e"
2021.findings-emnlp.377,N19-1423,0,0.0170129,"to sliding window attention + global attention, which is more memory efficient. Longformer can accept up to 16K tokens and has shown improvement over long document summarization using its long-encoder-decoder (LED) variant. We allow the maximum input of 4,096 tokens for Longformer and cutoff the rest of the input, as we found further increasing such limit yields no improvements. To incorporate queries in QMSum for these endto-end models, we simply append the queries to the front of the meeting transcripts, as it is a standard practice for query-based summarization and also question answering (Devlin et al., 2019). 3.3 Experiment Setup For a fair comparison between all models, we fit all of the models into the same RTX 8000 GPU with 48 GiB of GPU memory. We adopt the fairseq3 implementation for BART, and the original code base for both Longformer4 and HMNet5 . We inherit the hyperparameters for all those models for fine-tuning in our experiments.6 Our most expensive experiments are fine-tuning for HMNet and Longformer, which take around 8 hours, while the runtime for BART model is less than one hour. We use ROUGE (Lin, 2004) as our main evaluation metric and pyrouge library7 as the ROUGE implementation"
2021.findings-emnlp.377,P19-1102,1,0.852432,"eries at the beginning of the input. For the input to the two models, we use the gold relevant text spans given a query in QMSum to avoid the influences of retrieval models. The results show that encoding queries has a large impact on both types of models, especially for BART, even if the gold utterances are given. 4.4 Transfer Ability between Different Tasks Pretraining has been shown effective for document summarization by introducing external knowledge As we discussed, some dialogues (e.g., QMSum) from other similar tasks (Hermann et al., 2015; contain more than 20k tokens. They exceed the Fabbri et al., 2019). We hypothesize that it is input limitation of most existing summarization especially important for dialogue summarization models. In this section, we further analyze the per- because the dataset size is usually small. Thereformance of summarization models as the input fore, we study the transfer learning between diflength changes. To compare the robustness be- ferent dialogue summarization tasks via pretraintween two types of models (mainly BART and HM- ing. Tab. 3 shows the performance of BART-large Net), we divide the test dialogues by the number of models that are pretrained using differe"
2021.findings-emnlp.377,D19-5409,0,0.102612,"ong et al., 2021; Zhu et al., 2021). Dia- els such as Longformer (Beltagy et al., 2020), and logue summarization aims to generate a short sum- several dialogue utterance retrieval methods for a mary for long dialogues to help the readers capture retrieve-then-summarize pipeline model, as well important information more efficiently. as hierarchical dialogue encoding models. For the A number of existing works on dialogue sum- specific challenges in dialogues, we explore difmarization focus on extracting the main events of ferent datasets for pretraining to test the transfera short conversation (Gliwa et al., 2019; Rohde ability between similar summarization tasks. We et al., 2021). However, unlike the short dialogues evaluate these models on three recent long dia∗ logue summarization datasets: QMSum for meetEqual Contribution. ‡ The work was done when Asli was at MSR. ings (Zhong et al., 2021), MediaSum for inter4426 Findings of the Association for Computational Linguistics: EMNLP 2021, pages 4426–4433 November 7–11, 2021. ©2021 Association for Computational Linguistics views (Zhu et al., 2021), SummScreen for TV series transcripts (Chen et al., 2021). In our experiments, we find that the pipeline met"
2021.findings-emnlp.377,D19-5406,0,0.0131131,"017) that has a quadratic time and memory complexity with respect to the input length, preventing it from being used for longer sequences. To address this issue, Beltagy et al. (2020) used the sliding window and global attention, while Zaheer et al. (2020) used a combination of random, sliding window and global attention mechanism to reduce the quadratic complexity to close-linear. Previous benchmarks for long sequence summarization mostly focus on documents instead of dialogues: P UB M ED and A RXIV (Cohan et al., 2018) consists of scientific papers which are typically very long; B ILL S UM (Kornilova and Eidelman, 2019) is a corpus of U.S. Congressional bills and their summaries; B IG PATENT (Sharma et al., 2019) contains 1.3 million U.S. patent files and human-written summaries. Methodology In this section, we will introduce the dataset used to evaluate and pretrain the model, two types of summary models, and the details of the experiment setup. 3.1 Datasets To explore the problems in long dialogue summarization, we leverage three different long dialogue summarization tasks as main datasets: QMSum (Zhong et al., 2021) is a query-based multi-domain meeting summarization dataset annotated by humans. It contai"
2021.findings-emnlp.377,2020.acl-main.703,0,0.0305473,"contains less than 20 utterances, some tasks Abstract for summarizing much longer dialogues have been proposed recently (Chen et al., 2021; Zhong et al., Dialogue summarization helps readers capture 2021). These datasets are usually derived from salient information from long conversations in meetings and interviews, with hundreds of turns meetings, interviews, and TV series. However, in each dialogue. The length of such dialogues real-world dialogues pose a great challenge typically exceeds the input limits imposed by reto current summarization models, as the diacent transformer-based models (Lewis et al., 2020), logue length typically exceeds the input limmaking it difficult to train an end-to-end summaits imposed by recent transformer-based pretrained models, and the interactive nature of rization model for such tasks. This poses the chaldialogues makes relevant information more lenge: How can we effectively use the current neucontext-dependent and sparsely distributed ral summarization models on dialogues that greatly than news articles. In this work, we perexceed their length limits? form a comprehensive study on long dialogue Additionally, compared with document summasummarization by investigati"
2021.findings-emnlp.377,W04-1013,0,0.0552362,"practice for query-based summarization and also question answering (Devlin et al., 2019). 3.3 Experiment Setup For a fair comparison between all models, we fit all of the models into the same RTX 8000 GPU with 48 GiB of GPU memory. We adopt the fairseq3 implementation for BART, and the original code base for both Longformer4 and HMNet5 . We inherit the hyperparameters for all those models for fine-tuning in our experiments.6 Our most expensive experiments are fine-tuning for HMNet and Longformer, which take around 8 hours, while the runtime for BART model is less than one hour. We use ROUGE (Lin, 2004) as our main evaluation metric and pyrouge library7 as the ROUGE implementation throughout all experiments. 4 Result and Analysis Here we demonstrate our findings in four corresponding subsections. We also show some concrete examples and perform qualitative analysis in § 4.5 4.1 Dealing with Long Dialogues We compare several methods for addressing the long input issue for dialogue summarization, including different utterance retrieval methods describe in § 3.2.1 for a retrieve-then-summarize framework, heuristics for shortening the dialogue 3 https://github.com/pytorch/fairseq https://github.c"
2021.findings-emnlp.377,D18-1206,0,0.0283295,"his work, we choose one of them,i.e. “Forever et al., 2021; Nema et al., 2017), while others only need to summarize whole dialogues (Chen et al., Dreaming”, for which we call SummScreen-FD as 2021; Gliwa et al., 2019; Hermann et al., 2015). our benchmark. As for dialogue summarization models, Zhu et al. Tab. 1 shows the statistics for these three (2020b) described a hierarchical model for both long dialogue datasets. Additionally, we also inner- and cross-utterance attention, while Chen consider CNN/Dailymail (Hermann et al., 2015) and Yang (2020) proposed a multi-view decoder (CNN/DM), XSum (Narayan et al., 2018), and to leverage different extracted views of dialogues, SAMSum (Gliwa et al., 2019) as datasets for presuch as topic view and stage view. training in our experiments. 4427 3.2 Models 3.2.1 Retrieve-then-summarize Pipeline Dialogues tend to be relatively long, and most existing summarization models cannot process such long inputs. The two-stage retrieve-thensummarize pipeline first retrieves the most relevant subtext in the dialogue and then feeds to a summarizer. We experiment with the following retrievers: • TF-IDF (Jones, 1972) Based on bag-of-words representation, TF-IDF measuers term fre"
2021.findings-emnlp.377,P17-1098,0,0.0268736,"., 2020a), TV series (Chen et al., this dataset, 20k samples are randomly extracted 2021), interviews (Zhu et al., 2021), and chit- for pretraining; SummScreen (Chen et al., 2021) is a dialogue chat (Gliwa et al., 2019; Zhao et al., 2020; Chen summarization dataset consisting of 26.9k pairs and Yang, 2021). Some summarization datasets of TV series transcripts and human-annotated sum(not limited to dialogues) contain queries asking maries. It comes with two sources for recaps, and for summarizing specific parts of dialogues (Zhong in this work, we choose one of them,i.e. “Forever et al., 2021; Nema et al., 2017), while others only need to summarize whole dialogues (Chen et al., Dreaming”, for which we call SummScreen-FD as 2021; Gliwa et al., 2019; Hermann et al., 2015). our benchmark. As for dialogue summarization models, Zhu et al. Tab. 1 shows the statistics for these three (2020b) described a hierarchical model for both long dialogue datasets. Additionally, we also inner- and cross-utterance attention, while Chen consider CNN/Dailymail (Hermann et al., 2015) and Yang (2020) proposed a multi-view decoder (CNN/DM), XSum (Narayan et al., 2018), and to leverage different extracted views of dialogues,"
2021.findings-emnlp.377,P18-1062,0,0.0550762,"Missing"
2021.findings-emnlp.377,P19-1212,0,0.0228442,"m being used for longer sequences. To address this issue, Beltagy et al. (2020) used the sliding window and global attention, while Zaheer et al. (2020) used a combination of random, sliding window and global attention mechanism to reduce the quadratic complexity to close-linear. Previous benchmarks for long sequence summarization mostly focus on documents instead of dialogues: P UB M ED and A RXIV (Cohan et al., 2018) consists of scientific papers which are typically very long; B ILL S UM (Kornilova and Eidelman, 2019) is a corpus of U.S. Congressional bills and their summaries; B IG PATENT (Sharma et al., 2019) contains 1.3 million U.S. patent files and human-written summaries. Methodology In this section, we will introduce the dataset used to evaluate and pretrain the model, two types of summary models, and the details of the experiment setup. 3.1 Datasets To explore the problems in long dialogue summarization, we leverage three different long dialogue summarization tasks as main datasets: QMSum (Zhong et al., 2021) is a query-based multi-domain meeting summarization dataset annotated by humans. It contains 1,808 queries together with 232 long meeting transcripts, with topics as software product, a"
2021.findings-emnlp.377,2020.findings-emnlp.19,1,0.927658,"ontains annotated gold spans which could be used as the gold labels for training the retrievers; MediaSum (Zhu et al., 2021) is a large-scale media interview dataset consisting of 463.6K transcripts Dialogue Summarization Dialogue summariza- collected from NPR and CNN. Because MediaSum tion aims to generate concise summaries for dia- contains short summaries, i.e. only a short sentence logues, such as meetings (McCowan et al., 2005; representing the topic, we only use this dataset for Janin et al., 2003; Zhong et al., 2021; Shang et al., pretraining and analysis. Due to the huge size of 2018; Zhu et al., 2020a), TV series (Chen et al., this dataset, 20k samples are randomly extracted 2021), interviews (Zhu et al., 2021), and chit- for pretraining; SummScreen (Chen et al., 2021) is a dialogue chat (Gliwa et al., 2019; Zhao et al., 2020; Chen summarization dataset consisting of 26.9k pairs and Yang, 2021). Some summarization datasets of TV series transcripts and human-annotated sum(not limited to dialogues) contain queries asking maries. It comes with two sources for recaps, and for summarizing specific parts of dialogues (Zhong in this work, we choose one of them,i.e. “Forever et al., 2021; Nema et"
2021.findings-emnlp.377,2020.coling-main.39,0,0.0374974,"e summariza- collected from NPR and CNN. Because MediaSum tion aims to generate concise summaries for dia- contains short summaries, i.e. only a short sentence logues, such as meetings (McCowan et al., 2005; representing the topic, we only use this dataset for Janin et al., 2003; Zhong et al., 2021; Shang et al., pretraining and analysis. Due to the huge size of 2018; Zhu et al., 2020a), TV series (Chen et al., this dataset, 20k samples are randomly extracted 2021), interviews (Zhu et al., 2021), and chit- for pretraining; SummScreen (Chen et al., 2021) is a dialogue chat (Gliwa et al., 2019; Zhao et al., 2020; Chen summarization dataset consisting of 26.9k pairs and Yang, 2021). Some summarization datasets of TV series transcripts and human-annotated sum(not limited to dialogues) contain queries asking maries. It comes with two sources for recaps, and for summarizing specific parts of dialogues (Zhong in this work, we choose one of them,i.e. “Forever et al., 2021; Nema et al., 2017), while others only need to summarize whole dialogues (Chen et al., Dreaming”, for which we call SummScreen-FD as 2021; Gliwa et al., 2019; Hermann et al., 2015). our benchmark. As for dialogue summarization models, Zhu"
2021.findings-emnlp.377,2021.naacl-main.474,1,0.874688,"ty can be further improved with In this paper, we systematically investigate these a stronger retrieval model and pretraining on issues on dialog summarization: we first explore proper external summarization datasets. the various solutions to the lengthy input problem. Then, we analyze and compare the methods to im1 Introduction prove generic summarization models on challengLarge amount of dialogue data have been produced ing dialogue datasets. To address the long input in meetings, TV series, and interviews (Chen et al., issue, we investigate extended transformer mod2021; Zhong et al., 2021; Zhu et al., 2021). Dia- els such as Longformer (Beltagy et al., 2020), and logue summarization aims to generate a short sum- several dialogue utterance retrieval methods for a mary for long dialogues to help the readers capture retrieve-then-summarize pipeline model, as well important information more efficiently. as hierarchical dialogue encoding models. For the A number of existing works on dialogue sum- specific challenges in dialogues, we explore difmarization focus on extracting the main events of ferent datasets for pretraining to test the transfera short conversation (Gliwa et al., 2019; Rohde ability b"
2021.naacl-main.37,D19-1052,0,0.0377301,"Missing"
2021.naacl-main.37,2020.acl-main.708,0,0.0149657,"construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and s"
2021.naacl-main.37,2020.findings-emnlp.190,0,0.0356274,"Missing"
2021.naacl-main.37,2020.acl-main.18,0,0.0285505,"construction framework effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and s"
2021.naacl-main.37,P19-1483,0,0.0424379,"Missing"
2021.naacl-main.37,W19-8652,0,0.0305433,"Missing"
2021.naacl-main.37,L18-1544,0,0.0274201,"e structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia tables, which is significantly richer than those defined in the domain specific ontologies E2E and WebNLG were built on (Table 2). We also intro1 Introduction duce a novel tree ontology annotation approach on Automatically generating textual descriptions from tables, which converts a flat table schema into a"
2021.naacl-main.37,D19-1428,0,0.0202842,"generating textual descriptions from tables, which converts a flat table schema into a structured data improves the accessibility of knowl- tree structured semantic frame. The tree ontology edge bases to lay users. Such applications include reflects the core and auxiliary relations in the table explaining data records to non-experts (Cawsey schema, and naturally occurs across many domains. et al., 1997), writing sports news (Chen and As a result, DART provides high-quality sentence Mooney, 2008), summarizing information in mul- annotations to tree structured semantic frames extiple documents (Fan et al., 2019), and generating tracted from various data sources, including Wikdialogue responses (Wen et al., 2015). iSQL (Zhong et al., 2017) and WikiTableQuestions While significant progress has been made in this (Pasupat and Liang, 2015), two open-domain quesfield, there are still several issues with existing tion answering datasets, as well as E2E (Novikova Data-to-Text datasets. First, they adopt a flat ontol- et al., 2017b) and WebNLG (Gardent et al., 2017) ogy structure of the data, such as slot-value pairs (Figure 1). We evaluated several state-of-the-art for data records (Lebret et al., 2016; Novi"
2021.naacl-main.37,W17-3518,0,0.177339,"itates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia tables, which is significantly richer than those defi"
2021.naacl-main.37,2020.acl-main.703,0,0.0345274,"Missing"
2021.naacl-main.37,P09-1011,0,0.106929,"Missing"
2021.naacl-main.37,2020.findings-emnlp.165,0,0.0746934,"Missing"
2021.naacl-main.37,N19-1236,0,0.0269971,"Missing"
2021.naacl-main.37,D17-1238,0,0.0117771,"ges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia ta"
2021.naacl-main.37,P19-1195,0,0.0491104,"Missing"
2021.naacl-main.37,W07-2315,0,0.135635,"Missing"
2021.naacl-main.37,D19-1314,0,0.0450819,"Missing"
2021.naacl-main.37,2020.acl-main.704,0,0.020086,"Missing"
2021.naacl-main.37,W17-5525,0,0.0286406,"ges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the nature of the task (Wiseman et al., 2017) and the automatic generation procedure (Vougiouklis et al., 2018; Elsahar et al., 2018). To address some of these issues and to encourage further research in natural language generation from structured data, we introduce DART, a large and open-domain structured DAta-Record-to-Text generation corpus. The goal of DART is to harvest the diverse predicates occurred in Wikipedia ta"
2021.naacl-main.37,P17-2002,0,0.0678008,"Missing"
2021.naacl-main.37,2020.emnlp-main.89,0,0.0169662,"ork effectively merged heterogeneous sources from open domain semantic parsing and spoken dialogue systems by utilizing techniques including tree ontology annotation, question-answer pair to declarative sentence conversion and predicate unification, all with minimum post-editing. We present systematic evaluation on DART as well as new state-of-the-art results on WebNLG 2017 to show that DART (1) poses new challenges to existing data-to-text datasets and (2) facilitates out-of-domain generalization. Our data and code can be found at https://github. com/Yale-LILY/dart. 2017; Chen et al., 2020a; Parikh et al., 2020). This flat structure is not powerful enough to encode rich semantic relationships in the ontology of the structured data, especially tables, whose representation can be further improved with these semantic knowledge. Second, some of the datasets only focus on a small number of domains or knowledge graphs, therefore providing limited number of predicates and data ontologies. For example, E2E (Novikova et al., 2017b) on restaurants and WebNLG (Gardent et al., 2017) on 15 categories from DBPedia. Furthermore, some of them only have loose alignments between data input and sentence due to the natu"
2021.naacl-main.37,T87-1019,0,0.795136,"Missing"
2021.naacl-main.472,N18-1150,1,0.850619,"ting models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of"
2021.naacl-main.472,P18-1063,0,0.0367563,"lving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting sum"
2021.naacl-main.472,2020.findings-emnlp.329,1,0.691909,"nsive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the meeting transcripts to extract templates and generate summaries with the guidance of the templates. Shang et al. (2018) utilize multi-sentence compression techniques to generate summaries under an unsupervised setting. Li et al. (2019) attempt to incorporate multi-modal information to facilitate the meeting summarization. Zhu et al. (2020) propose a model which builds a hierarchical structure on word-level and turn-level information and uses news summary data to alleviate the inadequacy of meeting data. Unlike previous works, instead of merely generating summaries for the complete meeting, we propose a novel task where we focus on summarizing multi-granularity contents which cater to different people’s need for the entire meetings, and help people comprehensively understand meetings. 3 Data Construction In this section, we show how we collected meeting data from three different domains: academic meetings, product meetings, and"
2021.naacl-main.472,N12-1041,0,0.66189,"2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meet"
2021.naacl-main.472,P19-1098,0,0.0151867,"are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2"
2021.naacl-main.472,N16-1012,0,0.0336034,"s and analysis from different perspectives reveal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a mor"
2021.naacl-main.472,N18-2097,0,0.0198273,". 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006;"
2021.naacl-main.472,W06-0707,0,0.0593589,"Missing"
2021.naacl-main.472,P06-1039,0,0.208583,"Missing"
2021.naacl-main.472,N18-1065,0,0.039943,"Missing"
2021.naacl-main.472,2020.emnlp-main.295,0,0.0149003,"queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch o"
2021.naacl-main.472,W17-1004,0,0.108697,"of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921 June 6–11, 2021. ©2021 Association for Computational Linguistics standing out, while others may be more interested in what other attendees thought about different elements of the design. It is challenging to compress or compose a short summary that contains all the salient information. Alternatively, summarization systems should adopt a more flexible and interactive approach that allows people to express their interests and caters to their diverse intents when generating summaries (Dang, 2005, 2006; Litvak and Vanetik, 2017; Baumel et al., 2018). With comprehensive consideration of the multigranularity meeting contents, we propose a new task, query-based meeting summarization. To enable research in this area, we also create a highquality multi-domain summarization dataset. In this task, as shown in Figure 1, given a query and a meeting transcript, a model is required to generate the corresponding summary. The query-based approach is a flexible setup that enables the system to satisfy different intents and different levels of granularity. Besides the annotated queries and corresponding gold summaries at different"
2021.naacl-main.472,D18-1208,0,0.0649839,"Missing"
2021.naacl-main.472,D19-1387,1,0.849152,"generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from acade"
2021.naacl-main.472,D14-1181,0,0.00351051,", Pointer Network will point to the start turn and the end turn for each query. It is worth noting that one query can correspond to multiple spans in our dataset, so we always extract three spans as the corresponding text for each query when we use Pointer Network as Locator in the experiments. In addition, we design a hierarchical rankingbased model structure as the Locator. As shown in Figure 3, we first input the tokens in each turn to a feature-based BERT to obtain the word embedding, where feature-based means we fix the parameters of BERT, so it is actually an embedding layer. Next, CNN (Kim, 2014) is applied as a turn-level encoder to capture the local features such as bigram, 5 Experiments trigram and so on in each turn. Here we do not use Transformer because previous work (Kedzie et al., In this section, we introduce the implementation details, effectiveness of Locator, experimental results 2018) shows that this component does not matter and multi-domain experiments on QMSum. too much for the final performance. We combine different features to represent the utterance ui in 5.1 Implementation Details each turn, and concatenate the speaker embedding si as the turn-level representation:"
2021.naacl-main.472,2020.coling-main.499,0,0.0176046,"dia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the meeting transcripts to extract templates and generate summaries with the guidance of the templates. Shang et al. (2018) utilize multi-sentence compression techniques to generate summaries u"
2021.naacl-main.472,P19-1209,0,0.0161457,"meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2"
2021.naacl-main.472,2020.acl-main.703,0,0.0293968,"our goal in the second stage is to summarize the selected text spans based on the query. We instantiate our Summarizer with the current powerful abstractive models to explore whether the query-based meeting summarization task on our dataset is challenging. To be more specific, we choose the following three models: Pointer-Generator Network (See et al., 2017) is a popular sequence-to-sequence model with copy mechanism and coverage loss, and it acts as a baseline system in many generation tasks. The input to Pointer-Generator Network (PGNet) is: “&lt;s&gt; Query &lt;/s&gt; Relevant Text Spans &lt;/s&gt;”. BART (Lewis et al., 2020) is a denoising pretrained model for language generation, translation and comprehension. It has achieved new state-ofthe-art results on many generation tasks, including summarization and abstractive question answering. The input to BART is the same as PGNet. HMNet (Zhu et al., 2020) is the state-of-the-art meeting summarization model. It contains a hierarchical structure to process long meeting transcripts and a role vector to depict the difference among speakers. Besides, a cross-domain pretraining process is also included in this strong model. We add a turn representing the query at the begi"
2021.naacl-main.472,P19-1210,0,0.211278,"cussing user interface? User Interface Designer said the remote should perform standard features right out-of-the-box ...... Turn 121: User Interface Designer: The idea of having a remote is you have different keys and different structures. ...... Turn 162: Project Manager: Sure. Let&apos;s push forward the interface design. ...... Turn 316: Project Manager: Thanks. Have a nice day! Figure 1: Examples of query-based meeting summarization task. Users are interested in different facets of the meeting. In this task, a model is required to summarize the contents that users are interested in and query. Li et al., 2019; Zhu et al., 2020) is a task where summarization models are leveraged to generate summaries of entire meetings based on meeting transcripts. The resulting summaries distill the core contents of a meeting that helps people efficiently catch up to meetings. Most existing work and datasets on meeting summarization (Janin et al., 2003; Carletta et al., 2005) 1 Introduction pose the problem as a single document summarizaMeetings remain the go-to tool for collaboration, tion task where a single summary is generated for the whole meeting. Unlike news articles where with 11 million meetings taking pl"
2021.naacl-main.472,W13-2117,0,0.0344771,"2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehdad et al., 2013; Oya et al., domain meeting summarization, and build a new 2014; Shang et al., 2018; Li et al., 2019; Zhu et al., benchmark QMSum with a hierarchical annotation 2020; Koay et al., 2020). Specifically, Mehdad structure. 2) We design a locate-then-summarize et al. (2013) leverage entailment graphs and rankmodel and conduct comprehensive experiments on ing strategy to generate meeting summaries. Wang 5906 and Cardie (2013) attempt to make use of decisions, action items and progress to generate the whole meeting summaries. Oya et al. (2014) leverages the relationship between summaries and the mee"
2021.naacl-main.472,P17-1098,0,0.0183837,"meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meeting is also a genre of discourses where query-based summarization could be applied, but to our best knowledge, there are no works studying this direction. 2.3 Meeting Summarization Meeting summarization has attracted a lot of interOverall, our contributions are listed as follows: est recently (Chen and Metze, 2012; Wang and 1) We propose a new task, query-based multi- Cardie, 2013; Mehd"
2021.naacl-main.472,W14-4407,0,0.133484,"et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 2020) and becomes an emerging branch of text summarization area. 2.2 Query-based Summarization Query-based summarization aims to generate a brief summary according to a source document and a given query. There are works studying this task (Daumé III and Marcu, 2006; Otterbacher et al., 2009; Wang et al., 2016; Litvak and Vanetik, 2017; Nema et al., 2017; Baumel et al., 2018; Ishigaki et al., 2020; Kulkarni et al., 2020; Laskar et al., 2020). However, the models focus on news (Dang, 2005, 2006), debate (Nema et al., 2017), and Wikipedia (Zhu et al., 2019). Meet"
2021.naacl-main.472,D15-1044,0,0.0238088,"n QMSum. Our results and analysis from different perspectives reveal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting"
2021.naacl-main.472,P17-1099,0,0.400999,"veal that the existing models struggle in solving this task, highlighting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summar"
2021.naacl-main.472,D19-1324,0,0.0115444,"the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and mor"
2021.naacl-main.472,P18-1062,0,0.35554,"ing a toll on our productivity and well- decisions (Wang and Cardie, 2013). This poses the being (Spataro, 2020). The proliferation of meet- question of whether a single paragraph is enough to summarize the content of an entire meeting? ings makes it hard to stay on top of this sheer Figure 1 shows an example of a meeting about volume of information and increases the need for automated methods for accessing key informa- “remote control design”. The discussions in the tion exchanged during them. Meeting summariza- meeting are multi-faceted and hence different users tion (Wang and Cardie, 2013; Shang et al., 2018; might be interested in different facets. For exam∗ ple, someone may be interested in learning about These two authors contributed equally. The order of authorship decided by the flip of a coin. the new trends that may lead to the new product 5905 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 5905–5921 June 6–11, 2021. ©2021 Association for Computational Linguistics standing out, while others may be more interested in what other attendees thought about different elements of the design. It i"
2021.naacl-main.472,2020.acl-main.553,1,0.826971,"s to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2018; Zhu et al., 20"
2021.naacl-main.472,P13-1137,0,0.0731651,"Missing"
2021.naacl-main.472,2020.acl-main.552,1,0.832276,"dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this task attracts more and more interests from academia (Wang and Cardie, 2013; Oya et al., 2014; Shang et al., 2"
2021.naacl-main.472,P19-1100,1,0.888302,"ghting the challenges the models face when generating query-based meeting summaries. We are releasing our dataset and baselines to support additional research in queryfocused meeting summarization. its strong variants and different training settings. 3) By human evaluation, we further pose the challenges of the new task, including the impact of different query types and factuality errors. 2 2.1 Related Work Text Summarization Most prior work in text summarization (Rush et al., 2015; Chopra et al., 2016; Nallapati et al., 2016; See et al., 2017; Celikyilmaz et al., 2018; Chen and Bansal, 2018; Zhong et al., 2019a; Xu and Durrett, 2019; Liu and Lapata, 2019; Lebanoff et al., 2019; Cho et al., 2019; Zhong et al., 2020; Wang et al., 2020; Xu et al., 2019; Jia et al., 2020) investigate how to generate better summaries on news article data, such as CNN/DailyMail (Hermann et al., 2015), Newsroom (Grusky et al., 2018), etc. Scientific paper summarization is another important branch (Cohan et al., 2018; Yasunaga et al., 2019; An et al., 2021). Our paper mainly focuses on meeting summarization, a more challenging task compared to news summarization. With the burst of demand for meeting summarization, this tas"
2021.naacl-main.472,D19-5410,1,0.872427,"Missing"
2021.naacl-main.472,2020.findings-emnlp.19,0,0.764092,"rface? User Interface Designer said the remote should perform standard features right out-of-the-box ...... Turn 121: User Interface Designer: The idea of having a remote is you have different keys and different structures. ...... Turn 162: Project Manager: Sure. Let&apos;s push forward the interface design. ...... Turn 316: Project Manager: Thanks. Have a nice day! Figure 1: Examples of query-based meeting summarization task. Users are interested in different facets of the meeting. In this task, a model is required to summarize the contents that users are interested in and query. Li et al., 2019; Zhu et al., 2020) is a task where summarization models are leveraged to generate summaries of entire meetings based on meeting transcripts. The resulting summaries distill the core contents of a meeting that helps people efficiently catch up to meetings. Most existing work and datasets on meeting summarization (Janin et al., 2003; Carletta et al., 2005) 1 Introduction pose the problem as a single document summarizaMeetings remain the go-to tool for collaboration, tion task where a single summary is generated for the whole meeting. Unlike news articles where with 11 million meetings taking place each day people"
2021.naacl-main.57,N19-1071,0,0.0212821,"etworks, small finetune-able layers that aim to reproduce characteristics of the target dataset as seen in a small set of labeled examples. In contrast, we aim to encode the characteristics of our target dataset, such as level of extraction and compression, a priori in the intermediate training phase. In other work, Lebanoff et al. (2018) adapt a single-document summarization model to multi-document settings, while Zhu et al. (2019) use Wikipedia reference data for downstream query-based summarization Several approaches for unsupervised summarization have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Bražinskas et al., 2020b). Zhou and Rush (2019) makes use of pretrained language models for unsupervised text summarization by aligning the coverage of the generated summary to the source document. Laban et al. (2020) train an unsupervised summarization model with reinforcement learning rewards. In another line of work, extractive models such as TextRank, (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and more recently PacSum (Zheng and Lapata, 2019), make use of graph centrality for modeling salience. The power of pretrained models for few-shot transfer was"
2021.naacl-main.57,D18-1045,0,0.0306921,"f that given target dataset. Unless otherwise stated, all results reported are ROUGE1/2/L. We run all few-shot transfer experiments on five subsets of supervised data, and the reported numbers, unless zero-shot, are the average of the top three results of the five runs following previous work (Gunel et al., 2020). The 10 data point sets are subsets of the 100 data point sets. Data Augmentation Parameters: For data augmentation via round-trip translation, we use a beam size of 10 and k of 10 on German and Russian translation models; fairseq provides bidirectional pretrained translation models (Edunov et al., 2018) from WMT19 (Ng et al., 2019) for these language pairs. For both 10 and 100 data points, this resulted in 2010 and 20100 total data points. For consistency loss, we use the same augmented data. Model Hyperparameters: We use the fairseq codebase (Ott et al., 2019) for our experiments. Our base abstractive text summarization model is BART-large (Lewis et al., 2020), a pretrained denoising autoencoder with 336M parameters that builds off of the sequence-to-sequence transformer 4 Experimental Settings of Vaswani et al. (2017). We fine-tune BART usDatasets: We experiment with four datasets, CN- ing"
2021.naacl-main.57,2020.acl-main.703,1,0.88821,"ine-tuning an already-pretrained model specifically for summarization on a downstream dataset 2 Related Work by leveraging a generic text corpus (Wikipedia) While advances have been made in neural tech- to create auxiliary fine-tuning data that transfers niques for summarization due in part to large across domains, allowing for more fine-grained datasets, less work has focused on domain adap- control over the transfer process. We show the tation of such methods in the zero and few-shot generalizability of such fine-tuning across domains. settings. Wang et al. (2019) examine domain adap- BART (Lewis et al., 2020) is a pretrained denoising tation, but in extractive summarization. Hua and autoencoder and achieved state-of-the-art perforWang (2017) examine domain adaptation between mance when fine-tuned on summarization tasks at opinion and news summarization, observing that the time. In this work, we use BART as our base models trained on one domain and applied to an- pretrained model but in future work will experiother domain can capture relevant content but differ ment with other pretrained models. 705 3 Methods Data Augmentation via Round-Trip Translation: In addition to fine-tuning on WikiTransfer d"
2021.naacl-main.57,W04-1013,0,0.0207062,"ning pretrained models using unsupervised Wikipedia data. We create dataset-specific unsupervised data for this intermediate fine-tuning, by making use of characteristics of the target dataset such as the average length of input documents, the average summary length, and the general bin of whether the summaries desired are very abstractive or very extractive, as discussed above. Assume that we want a summary of M sentences from source documents of N sentences on average, and that we know approximately how extractive the summaries are in the target dataset, as defined as the upper bound ROUGE (Lin, 2004) performance of an extractive model, the extractive oracle, on that dataset. We bin the level of extraction of the target summaries into extremely abstractive (ROUGE oracle 10-30), more abstractive (ROUGE oracle 20-30), more extractive (ROUGE oracle 30-50), and extremely extractive (ROUGE oracle 40-60). We then iterate the following procedure on all Wikipedia articles available in a Wikipedia dump: We remove the first M sentences from the Wikipedia article for use as a summary and the following N sentences for use as a source document. Then, we want to check whether this pseudo data point matc"
2021.naacl-main.57,W04-3252,0,0.010523,"t settings, while Zhu et al. (2019) use Wikipedia reference data for downstream query-based summarization Several approaches for unsupervised summarization have made use of variational autoencoders (Baziotis et al., 2019; Chu and Liu, 2019; Bražinskas et al., 2020b). Zhou and Rush (2019) makes use of pretrained language models for unsupervised text summarization by aligning the coverage of the generated summary to the source document. Laban et al. (2020) train an unsupervised summarization model with reinforcement learning rewards. In another line of work, extractive models such as TextRank, (Mihalcea and Tarau, 2004), LexRank (Erkan and Radev, 2004), and more recently PacSum (Zheng and Lapata, 2019), make use of graph centrality for modeling salience. The power of pretrained models for few-shot transfer was shown for abstractive summarization in Zhang et al. (2019) and extractive summarization in Desai et al. (2020). Our work focuses on the zero-shot abstractive summarization setting and the transferability of models fine-tuned on taskspecific data from a generic corpus, rather than just the transferability of a single pretrained model. The closest work to ours for zero-shot transfer is Yang et al. (2020)"
2021.naacl-main.57,K16-1028,0,0.0325103,"of subfunctions of the input, called subaspects, which 1 Introduction determine the output form. Jung et al. (2019) deAutomatic text summarization aims to distill the fine three subaspects for summarization: position, most salient content of a given text in a compact importance, and diversity, and study how these form. Recent advances in summarization have been subaspects manifest themselves in summarization driven by the availability of large-scale datasets corpora and model outputs. For example, a comsuch as the CNN-DailyMail (CNNDM) corpus mon subaspect for the CNNDM dataset is position; (Nallapati et al., 2016) and the New York Times earlier sentences tend to constitute a good sum704 Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 704–717 June 6–11, 2021. ©2021 Association for Computational Linguistics mary. Inspired by this view of summarization as subaspects, we aim to encode subaspects of a target dataset into unlabeled data to allow a model finetuned on this data to learn characteristics of the target dataset to improve zero-shot and few-shot transfer of the model. In our work, we focus on the s"
2021.naacl-main.57,P19-1212,0,0.0365573,"Missing"
2021.repl4nlp-1.1,2020.emnlp-main.195,0,0.0711365,"Missing"
2021.repl4nlp-1.1,P19-1299,0,0.0823978,"ws the results of our comparison study. Sentiment Classification Finally, we evaluate sentiment classification task on Amazon multilingual reviews dataset (Prettenhofer and Stein, 2010). It contains positive and negative reviews from 3 domains, including DVD, Music and Books, in four languages: English (en), French (fr), German (de), and Japanese (ja). For each domain, there are 1,000 positive samples and 1,000 negative samples in each language for both training and testing. We choose the following baselines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara and Cimiano (2019) Agerri and Rigau (2019)H Evaluation We te"
2021.repl4nlp-1.1,Q18-1039,0,0.0591951,"Missing"
2021.repl4nlp-1.1,S16-1049,0,0.058756,"Missing"
2021.repl4nlp-1.1,2020.acl-main.747,0,0.091198,"Missing"
2021.repl4nlp-1.1,D18-1269,0,0.0663312,"Missing"
2021.repl4nlp-1.1,J82-2005,0,0.700201,"Missing"
2021.repl4nlp-1.1,N19-1423,0,0.0205863,"during training. Ideally, when deciding an instance weight, we should compare it with all instances from the target language. But doing so would incur prohibitively excessive computational resources. We thus approximate in small batches and calculate the weights by comparing how similar the instances are to the target ones within a small batch in each training step. Instance Weighting-based Gradient Descent Vanilla mini-batch gradient descent is defined as: 2.1 Pre-trained Models We compare two multilingual versions of pretrained models for the pre-trained models: multilingual BERT (mBERT)1 (Devlin et al., 2019) and XLM-Roberta (XLMR)2 (Conneau et al., 2020). We evaluate on multiple tasks in Section 3, so there are different ways to utilize the pre-trained models. For the sentiment and document classification task, we train a fully-connected layer on top of the output of the [CLS] token, which is considered to be the representation of the input sequence. For the opinion target extraction task, we formulate it as sequence labeling task (Agerri and Rigau, 2019; Jebbara and Cimiano, 2019). To extract such opinion target tokens is to classify each token into one of the following: Beginning, Inside and Ou"
2021.repl4nlp-1.1,N16-1083,0,0.0633704,"Missing"
2021.repl4nlp-1.1,D18-1498,0,0.0393052,"Missing"
2021.repl4nlp-1.1,N19-1257,0,0.0164374,"d Models We compare two multilingual versions of pretrained models for the pre-trained models: multilingual BERT (mBERT)1 (Devlin et al., 2019) and XLM-Roberta (XLMR)2 (Conneau et al., 2020). We evaluate on multiple tasks in Section 3, so there are different ways to utilize the pre-trained models. For the sentiment and document classification task, we train a fully-connected layer on top of the output of the [CLS] token, which is considered to be the representation of the input sequence. For the opinion target extraction task, we formulate it as sequence labeling task (Agerri and Rigau, 2019; Jebbara and Cimiano, 2019). To extract such opinion target tokens is to classify each token into one of the following: Beginning, Inside and Outside of an aspect. We follow a typical IOB scheme for the task (Toh and Wang, 2014; San Vicente ´ et al., 2015; Alvarez-L´ opez et al., 2016). In this case, each token should have a label, so we have a fully-connected layer that is shared for each token. We note that it may be possible to improve all the results even further by employing more powerful task layers and modules such as conditional random fields (Lafferty et al., 2001), but keep things relatively simple since our m"
2021.repl4nlp-1.1,S14-2004,0,0.0338226,"selines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara and Cimiano (2019) Agerri and Rigau (2019)H Evaluation We test on three tasks: opinion target extraction, document classification, and sentiment classification 3 . English is the source language for all the experiments. We evaluate four settings: 1) direct adaptation with mBERT-base (mBERT), 2) mBERT with Instance Weighting (mBERT+IW), 3) direct adaption of XLMR-base (XLMR), and 4) XLMR with Instance Weighting (XLMR+IW). Opinion Target Extraction We choose SemEval 2016 Workshop Task 5 (Pontiki et al., 2016) for opinion target extraction. It includes restaurant re"
2021.repl4nlp-1.1,P10-1114,0,0.0559969,"a strong baseline (Schwenk and Li, 2018), which applies pre-trained MultiCCA word embeddings (Ammar et al., 2016) and then trained in a supervised way. Another baseline is a zero-shot method proposed by Artetxe and Schwenk (2019), which applies a single BiLSTM encoder with a shared vocabulary among all languages, and a decoder trained with parallel corpora. Artetxe and Schwenk (2019) apply mBERT as a zero-shot language transfer. Table 2 shows the results of our comparison study. Sentiment Classification Finally, we evaluate sentiment classification task on Amazon multilingual reviews dataset (Prettenhofer and Stein, 2010). It contains positive and negative reviews from 3 domains, including DVD, Music and Books, in four languages: English (en), French (fr), German (de), and Japanese (ja). For each domain, there are 1,000 positive samples and 1,000 negative samples in each language for both training and testing. We choose the following baselines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the"
2021.repl4nlp-1.1,S16-1174,0,0.024419,"seline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara and Cimiano (2019) Agerri and Rigau (2019)H Evaluation We test on three tasks: opinion target extraction, document classification, and sentiment classification 3 . English is the source language for all the experiments. We evaluate four settings: 1) direct adaptation with mBERT-base (mBERT), 2) mBERT with Instance Weighting (mBERT+IW), 3) direct adaption of XLMR-base (XLMR), and 4) XLMR with Instance Weighting (XLMR+IW). Opinion Target Extraction We choose SemEval 2016 Workshop Task 5 (Pontiki et al., 2016) for opinion target extraction. It includes restaurant reviews in five languag"
2021.repl4nlp-1.1,S15-2127,0,0.0643988,"Missing"
2021.repl4nlp-1.1,C16-1038,0,0.0188132,"omain, there are 1,000 positive samples and 1,000 negative samples in each language for both training and testing. We choose the following baselines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara and Cimiano (2019) Agerri and Rigau (2019)H Evaluation We test on three tasks: opinion target extraction, document classification, and sentiment classification 3 . English is the source language for all the experiments. We evaluate four settings: 1) direct adaptation with mBERT-base (mBERT), 2) mBERT with Instance Weighting (mBERT+IW), 3) direct adaption of XLMR-base (XLMR), and 4) XLMR with Instance Weighting (XLMR+IW)."
2021.repl4nlp-1.1,S14-2038,0,0.0182931,"ultiple tasks in Section 3, so there are different ways to utilize the pre-trained models. For the sentiment and document classification task, we train a fully-connected layer on top of the output of the [CLS] token, which is considered to be the representation of the input sequence. For the opinion target extraction task, we formulate it as sequence labeling task (Agerri and Rigau, 2019; Jebbara and Cimiano, 2019). To extract such opinion target tokens is to classify each token into one of the following: Beginning, Inside and Outside of an aspect. We follow a typical IOB scheme for the task (Toh and Wang, 2014; San Vicente ´ et al., 2015; Alvarez-L´ opez et al., 2016). In this case, each token should have a label, so we have a fully-connected layer that is shared for each token. We note that it may be possible to improve all the results even further by employing more powerful task layers and modules such as conditional random fields (Lafferty et al., 2001), but keep things relatively simple since our main goal is to evaluate instance weighting with zero-shot CLTC. θ ←θ−α k X ∇θ f (yi , gθ (xi )) (1) i=1 where α is the learning rate, θ is the parameter that we want to update, gθ (xi ) is the model p"
2021.repl4nlp-1.1,D17-1155,0,0.0653529,"Missing"
2021.repl4nlp-1.1,D19-1077,0,0.0466328,"Missing"
2021.repl4nlp-1.1,D17-1053,0,0.166397,"apply mBERT as a zero-shot language transfer. Table 2 shows the results of our comparison study. Sentiment Classification Finally, we evaluate sentiment classification task on Amazon multilingual reviews dataset (Prettenhofer and Stein, 2010). It contains positive and negative reviews from 3 domains, including DVD, Music and Books, in four languages: English (en), French (fr), German (de), and Japanese (ja). For each domain, there are 1,000 positive samples and 1,000 negative samples in each language for both training and testing. We choose the following baselines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara a"
2021.repl4nlp-1.1,P17-1130,0,0.123322,"t language transfer. Table 2 shows the results of our comparison study. Sentiment Classification Finally, we evaluate sentiment classification task on Amazon multilingual reviews dataset (Prettenhofer and Stein, 2010). It contains positive and negative reviews from 3 domains, including DVD, Music and Books, in four languages: English (en), French (fr), German (de), and Japanese (ja). For each domain, there are 1,000 positive samples and 1,000 negative samples in each language for both training and testing. We choose the following baselines: translation baseline, UMM (Xu and Wan, 2017), CLDFA (Xu and Yang, 2017) and MAN-MoE (Chen et al., 2019). For the translation baseline, we translate the training and testing data for each target language into English using Watson Language Translator5 , and trained on the mBERT model, which is more 1 hi · hj + 1). score(i, j) = ( 2 khi k khj k We also investigate two other ways for scoring function: Euclidean-Distance based and the CORAL Function (Sun et al., 2016). While Cosine scoring function performs the best, so we report it in our main experiments and ignoring the other two. 3 es Pontiki et al. (2014)H Kumar et al. (2016)H Jebbara and Cimiano (2019) Agerri an"
A00-1021,A97-1033,1,0.650168,"200 questions in the evaluation set, we were asked to provide a list of 50-byte and 250-byte extracts from a 2-GB corpus. The results are shown in Section 7. Some techniques used by other participants in the T R E C evaluation are paragraph indexing, followed by abductive inference (Harabagiu and Maiorano, 1999) and knowledge-representation combined with information retrieval (Breck et al., 1999). Some earlier systems related to our work are FaqFinder (Kulyukin et al., 1998), MURAX (Kupiec, 1993), which uses an encyclopedia as a knowledge base from which to extract answers, and P R O F I L E (Radev and McKeown, 1997) which identifies named entities and noun phrases that describe them in text. 2 System description Our system (Figure 1) consists of two pieces: an IR component (GuruQA) that which returns matching texts, and an answer selection componeat (AnSel/Werlect) that extracts and ranks potential answers from these texts. This paper focuses on the process of ranking potential answers selected by the IR engine, which is itself described in (Prager et al., 1999). ~ lndexer ~ Searc~x'~ GuruQA  Rankcd ~ HtLst [ AnSel/ ~ [Werlect to ""PLACES"", but ""How long "" goes to ""@SYN(LENGTH$, DURATIONS)"". Some templat"
A00-1021,A97-1030,0,0.013199,"componeat (AnSel/Werlect) that extracts and ranks potential answers from these texts. This paper focuses on the process of ranking potential answers selected by the IR engine, which is itself described in (Prager et al., 1999). ~ lndexer ~ Searc~x'~ GuruQA  Rankcd ~ HtLst [ AnSel/ ~ [Werlect to ""PLACES"", but ""How long "" goes to ""@SYN(LENGTH$, DURATIONS)"". Some templates do not cause complete replacement of the matched string. For example, the pattern ""What is the population"" gets replaced by ""NUMBERS population'. • Before indexing the text, we process it with Textract (Byrd and Ravin, 1998; Wacholder et al., 1997), which performs lemmatization, and discovers proper names and technical terms. We added a new module (Resporator) which annotates text segments with QA-Tokens using pattern matching. Thus the text ""for 5 centuries"" matches the DURATIONS pattern ""for :CARDINAL _timeperiod"", where :CARDINAL is the label for cardinal numbers, and _timeperiod marks a time expression.  HitList I Answerselection • G u r u Q A scores text passages instead of documents. We use a simple documentand collection-independent weighting scheme: QA-Tokens get a weight of 400, proper nouns get 200 and any other word - 100 (s"
A97-1033,M92-1024,0,0.0328184,"d for the message understanding conferences (MUC, 1992), and use of extracted information for question answering. Techniques for proper noun extraction include the use of regular g r a m m a r s to delimit and identify proper nouns (Mani et al., 1993; Paik et al., 1994), the use of extensive n a m e lists, place names, titles and &quot;gazetteers&quot; in conjunction with partial g r a m m a r s in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers t h a t use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald, 1993; Coates-Stephens, 1991) and the larger discourse context (Mani et al., 1993) to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists. We u"
A97-1033,A88-1019,0,0.0154053,"Missing"
A97-1033,M92-1031,0,0.308897,"Missing"
A97-1033,M92-1032,0,0.03486,"use in summarization. In contrast with previous work on information extraction, our work has the following features: • It builds a database of profiles for entities by storing descriptions from a collected corpus of • past news. • It operates in real time, allowing for connections with the latest breaking, online news to extract information about the most recently mentioned individuals and organizations. Introduction In our work to date on news summarization at Columbia University (McKeown and Radev, 1995; Radev, 1996), information is extracted from a series of input news articles (MUC, 1992; Grishman et al., 1992) and is analyzed by a generation component to produce a s u m m a r y that shows how perception of the event has changed over time. In this s u m m a r i z a t i o n paradigm, problems arise when information needed for the s u m m a r y is either missing from the input article(s) or not extracted by the information extraction system. In such cases, the information m a y be readily available in other current news stories, in past news, or in online databases. If the summarization system can find the needed information in other online sources, then it can produce an improved s u m m a r y by mer"
A97-1033,M93-1023,0,0.0839026,"a m m a r s to delimit and identify proper nouns (Mani et al., 1993; Paik et al., 1994), the use of extensive n a m e lists, place names, titles and &quot;gazetteers&quot; in conjunction with partial g r a m m a r s in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers t h a t use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald, 1993; Coates-Stephens, 1991) and the larger discourse context (Mani et al., 1993) to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists. We use a regular g r a m m a r encoding part-ofspeech categories to extract certain text patterns and we use WordNet (Miller et al., 1990) to provide semantic filtering. Our work on extract"
A97-1033,W93-0105,0,0.269583,"construction of knowledge sources for generation. 2.1 Information Extraction Work on information extraction is quite broad and covers far more topics and problems than the information extraction problem we address. We restrict our comparison here to work on proper noun extraction, extraction of people descriptions in various information extraction systems developed for the message understanding conferences (MUC, 1992), and use of extracted information for question answering. Techniques for proper noun extraction include the use of regular g r a m m a r s to delimit and identify proper nouns (Mani et al., 1993; Paik et al., 1994), the use of extensive n a m e lists, place names, titles and &quot;gazetteers&quot; in conjunction with partial g r a m m a r s in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers t h a t use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored"
A97-1033,W93-0104,0,0.0851769,"tles and &quot;gazetteers&quot; in conjunction with partial g r a m m a r s in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers t h a t use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald, 1993; Coates-Stephens, 1991) and the larger discourse context (Mani et al., 1993) to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists. We use a regular g r a m m a r encoding part-ofspeech categories to extract certain text patterns and we use WordNet (Miller et al., 1990) to provide semantic filtering. Our work on extracting descriptions is quite similar to the work carried out under the D A R P A message understanding p r o g r a m for extracting desc"
A97-1033,M93-1010,0,0.0165286,"e the use of regular g r a m m a r s to delimit and identify proper nouns (Mani et al., 1993; Paik et al., 1994), the use of extensive n a m e lists, place names, titles and &quot;gazetteers&quot; in conjunction with partial g r a m m a r s in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al., 1992; Aberdeen et al., 1992), statistical training to learn, for example, Spanish names, from online corpora (Ayuso et al., 1992), and the use of concept based pattern matchers t h a t use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al., 1993; Lehnert et al., 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald, 1993; Coates-Stephens, 1991) and the larger discourse context (Mani et al., 1993) to improve the accuracy of proper noun extraction when large known word lists are not available. Like this research, our work also aims at extracting proper nouns without the aid of large word lists. We use a regular g r a m m a r encoding part-ofspeech categories to extract certain text patterns and we use WordNet (Miller et al., 1990) to provide semantic filteri"
A97-1033,W96-0512,1,0.716907,"L E that tracks prior references to a given entity by extracting descriptions for later use in summarization. In contrast with previous work on information extraction, our work has the following features: • It builds a database of profiles for entities by storing descriptions from a collected corpus of • past news. • It operates in real time, allowing for connections with the latest breaking, online news to extract information about the most recently mentioned individuals and organizations. Introduction In our work to date on news summarization at Columbia University (McKeown and Radev, 1995; Radev, 1996), information is extracted from a series of input news articles (MUC, 1992; Grishman et al., 1992) and is analyzed by a generation component to produce a s u m m a r y that shows how perception of the event has changed over time. In this s u m m a r i z a t i o n paradigm, problems arise when information needed for the s u m m a r y is either missing from the input article(s) or not extracted by the information extraction system. In such cases, the information m a y be readily available in other current news stories, in past news, or in online databases. If the summarization system can find th"
A97-1033,M91-1028,0,\N,Missing
A97-1033,H93-1062,0,\N,Missing
A97-1033,M92-1030,0,\N,Missing
bird-etal-2008-acl,D07-1089,0,\N,Missing
bird-etal-2008-acl,W06-1613,0,\N,Missing
bird-etal-2008-acl,N04-1042,0,\N,Missing
bird-etal-2008-acl,radev-etal-2004-mead,1,\N,Missing
C02-1073,J93-1004,0,\N,Missing
C02-1073,A00-2024,0,\N,Missing
C02-1073,saggion-etal-2002-developing,1,\N,Missing
C02-1073,A00-2035,0,\N,Missing
C02-1073,W00-0401,1,\N,Missing
C02-1073,W00-0408,0,\N,Missing
C02-1073,E99-1011,0,\N,Missing
C02-1073,W97-0704,0,\N,Missing
C02-1073,W00-0403,1,\N,Missing
C02-1073,grover-etal-2000-lt,0,\N,Missing
C04-1184,radev-etal-2004-cst,1,\N,Missing
C04-1184,W03-1609,0,\N,Missing
C04-1184,P02-1040,0,\N,Missing
C04-1184,P01-1008,0,\N,Missing
C04-1184,N03-1024,0,\N,Missing
C04-1184,N03-1003,0,\N,Missing
C08-1040,N06-1061,0,0.0152327,"ach topic, rather than to all speeches. We preferred to use topic-specific idf values because the relative importance of words may vary from one topic to the other. The tf-idf cosine similarity measure is computed as the cosine of the angle between the tf-idf vectors. It is defined as follows: √P P 2 w∈u,v tf u (w) tf v (w) idf(w) , P (tf (w) idf(w))2 v w∈v 2 w∈u (tf u (w) idf(w)) √ (2) The choice of tf-idf scores to measure speech similarity is an arbitrary choice. Some other possible similarity measures are edit distance, language models (Kurland and Lee, 2005), or generation probabilities (Erkan, 2006). The recursive definition of the score of any speech s in the speeches network is given by X p(t) p(s) = (3) deg(t) where deg(t) is the degree of node t, and adj[s] is the set of all speeches adjacent to s in the network. This can be rewritten in matrix notation as: p = pB (4) where p = (p(s1 ), p(s2 ), . . . , p(sN )) and the matrix B is the row normalized similarity matrix of the graph S(i, j) B(i, j) = P k S(i, k) (5) where S(i, j) = sim(si , sj ). Equation (4) shows that the vector of salience scores p is the left eigenvector of B with eigenvalue 1. The matrix B can be thought of as a sto"
C08-1040,C04-1162,0,0.0268609,"Missing"
C08-1040,W06-1639,0,0.0155848,"2004). The interest of applying natural language processing techniques in the area of political science has been recently increasing. (Quinn et al., 2006) introduce a multinomial mixture model to cluster political speeches into topics or related categories. In (Porter et al., 2005), a network analysis of the members and committees of the US House of Representatives is performed. The authors prove that there are connections linking some political positions to certain committees. This suggests that there are factors affecting committee membership and that they are not determined at random. In (Thomas et al., 2006), the authors try to automatically classify speeches, from the US Congress debates, as supporting or opposing a given topic by taking advantage of the voting records of the speakers. (Fader et al., 2007) introduce MavenRank , which is a method based on lexical centrality that identifies the most influential members of the US Senate. It computes a single salience score for each speaker that is constant over time. In this paper, we introduce a new method for tracking the evolution of the salience of participants in a discussion over time. Our method is based on the ones described in (Erkan and R"
C08-1040,D07-1069,1,0.281648,"political speeches into topics or related categories. In (Porter et al., 2005), a network analysis of the members and committees of the US House of Representatives is performed. The authors prove that there are connections linking some political positions to certain committees. This suggests that there are factors affecting committee membership and that they are not determined at random. In (Thomas et al., 2006), the authors try to automatically classify speeches, from the US Congress debates, as supporting or opposing a given topic by taking advantage of the voting records of the speakers. (Fader et al., 2007) introduce MavenRank , which is a method based on lexical centrality that identifies the most influential members of the US Senate. It computes a single salience score for each speaker that is constant over time. In this paper, we introduce a new method for tracking the evolution of the salience of participants in a discussion over time. Our method is based on the ones described in (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007), The objective of this paper is to dynamically rank speakers or participants in a discussion. The proposed method is dynamic in the sense that th"
C08-1040,W04-3252,0,\N,Missing
C08-1077,W03-1028,0,0.0258279,". This algorithm outputs a set of ngrams for each topic whereas our algorithm models each subtopic using a single n-gram. Due to limitations of time we were not able to compare this approach with ours. We plan to have this comparison in our future work. To reduce the complexity of this task, a candidate set of subtopics needs to be generated that cover the document collection. We choose to use a keyphrase detection algorithm to generate topic labels. Several keyphrase extraction algorithms have been discussed in the literature, including ones based on machine learning methods (Turney, 2000), (Hulth, 2003) and tf-idf ((Frank et al., 1999)). Our method uses language models and pointwise mutual information expressed as the Kullback-Leibler divergence. Kullback-Leibler divergence has been found to be an effective method of finding keyphrases in text collections. But identification of keyphrases is not enough to find topics in document. The keyphrases identified may describe the entire collection, or aspects of the collection. We wish to summarize subtopics within these collections. The problem of subtopic detection is also related to novelty detection in (Allan et al., ). In this problem, given a"
C08-1077,W03-1805,0,0.0257959,"ess of “gun control”, we would find the pointwise KL divergence of “gun control” between the foreground bigram language model and the foreground unigram language model. ϕp = δw (LMf g N ||LMf g 1 ) (3) The informativeness of a phrase can be found by finding the pointwise KL divergence of the foreground model against the background model. 2. The algorithm generates keyphrases for a single document, but for our purposes we need keyphrases for a corpus. Another method is using Kullback-Leibler divergence to find informative keyphrases. We found that KL divergence generated good candidate topics. Tomokiyo and Hurst (2003) developed a method of extracting keyphrases using statistical language models. They considered keyphrases as consisting of two features, phraseness and informativeness. Phraseness is described by them as the “degree to which a given word sequence is considered to be a phrase.” For example, collocations could be considered sequences with a high phraseness. Informativeness is the extent to which a phrase captures the key idea or main topic in a set of documents. To find keyphrases, they compared two language models, the target document set and a background corpus. Pointwise KL divergence was ch"
C08-1087,N06-1061,0,0.00630791,"irs) in the top 100, 200 and 300 highly similar ones out of total 2, 862 pairs. Table 4 shows the number of fact sharing pairs among the top highest similar pairs. Table 4 shows how cosine similarity that uses a tf-idf measure outperforms the others. We tried three different policies for computing IDF values to compute cosine similarity: a general IDF, an AAN-specific IDF where IDF values are calculated only using the documents of AAN, and finally DP-specific IDF in which we only used all-DP data set. Table 4 also shows the results for an asymmetric similarity measure, generation probability (Erkan, 2006) as well as two string edit distances: the longest common substring and the Levenshtein distance (Levenshtein, 1966). 4 Methodology In this section we discuss our graph clustering method for article summarization, as well as other baseline methods used for comparisons. 4.1 Network-Based Clustering The Citation Summary Network of an article A is a network in which each sentence in the citation summary of A is a node. This network is a complete undirected weighted graph where the weight of an edge between two nodes shows the similarity of the two corresponding sentences of those nodes. The simil"
C08-1087,kan-etal-2002-using,0,0.173624,"n” (Elkiss et al., 2008) of a citation summary of an article is consistently higher than the that of its abstract. (Elkiss et al., 2008) also conclude that citation summaries are more focused than abstracts, and that they contain additional information that does not appear in abstracts. (Kupiec et al., 1995) use the abstracts of scientific articles as a target summary, where they use 188 Engineering Information summaries that are mostly indicative in na1 http://www.pubmedcentral.gov ture. Abstracts tend to summarize the documents topics well, however, they don’t include much use of metadata. (Kan et al., 2002) use annotated bibliographies to cover certain aspects of summarization and suggest guidelines that summaries should also include metadata and critical document features as well as the prominent content-based features. Siddharthan and Teufel describe a new task to decide the scientific attribution of an article (Siddharthan and Teufel, 2007) and show high human agreement as well as an improvement in the performance of Argumentative Zoning (Teufel, 2005). Argumentative Zoning is a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basi"
C08-1087,N06-1048,0,0.00872684,"plify part-of-speech tags since the rich tags used by Czech would have led to a large but rarely seen set of POS features. Table 6: System Summaries for P99-1065. (a) Using C-RR, (b) using C-Lexrank with length of 5 sentences nodes are sentences and a weighted edge between two nodes shows the lexical similarity. Once this network is built, Lexrank performs a random walk to find the most central nodes in the graph and reports them as summary sentences. 5 5.1 Experimental Setup Evaluation Method Fact-based evaluation systems have been used in several past projects (Lin and Demner-Fushman, 2006; Marton and Radul, 2006), especially in the TREC question answering track. (Lin and Demner-Fushman, 2006) use stemmed unigram similarity of responses with nugget descriptions to produce the evaluation results, whereas (Marton and Radul, 2006) uses both human judgments and human descriptions to evaluate a response. An ideal summary in our model is one that covers more facts and more important facts. Our definition for the properties of a “good” summary of a paper is one that is relatively short and consists of the main contributions of that paper. From this viewpoint, there are two criteria for our evaluation metric."
C08-1087,N04-1019,0,0.0218698,"s one that covers more facts and more important facts. Our definition for the properties of a “good” summary of a paper is one that is relatively short and consists of the main contributions of that paper. From this viewpoint, there are two criteria for our evaluation metric. First, summaries that contain more important facts are preferred over summaries that cover fewer relevant facts. Second, facts should not be equally weighted in this model, as some of them may show more important contributions of a paper, while others may not. To evaluate our system, we use the pyramid evaluation method (Nenkova and Passonneau, 2004) at sentence level. Each fact in the citation summary of a paper is a summarization content unit (SCU) (Nenkova and Passonneau, 2004), and the fact distribution matrix, created by annotation, provides the information about the importance of each fact in the citation summary. The score given by the pyramid method for a summary is a ratio of the sum of the weights of its facts to the sum of the weights of an optimal summary. This score ranges from 0 to 1, and high scores show the summary content contain more heavily weighted facts. We believe that if a fact appears in more sentences of the citat"
C08-1087,N07-1040,0,0.102729,"scientific articles as a target summary, where they use 188 Engineering Information summaries that are mostly indicative in na1 http://www.pubmedcentral.gov ture. Abstracts tend to summarize the documents topics well, however, they don’t include much use of metadata. (Kan et al., 2002) use annotated bibliographies to cover certain aspects of summarization and suggest guidelines that summaries should also include metadata and critical document features as well as the prominent content-based features. Siddharthan and Teufel describe a new task to decide the scientific attribution of an article (Siddharthan and Teufel, 2007) and show high human agreement as well as an improvement in the performance of Argumentative Zoning (Teufel, 2005). Argumentative Zoning is a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, Contrast according to their role in the author’s argument. These all show the importance of citation summaries and the vast area for new work to analyze them to produce a summary for a given topic. 2 Data The ACL Anthology is a collection of papers from the Computational Linguistics journal, and proceedings from ACL conferences and worksh"
C08-1087,J02-4002,0,0.620794,"y article usually has more than a few sentences, the main challenge of this task is to find a subset of these sentences that will lead to a better and shorter summary. 689 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 689–696 Manchester, August 2008 Cluster DP PBMT Summ QA TE Nodes 167 186 839 238 56 Edges 323 516 1425 202 44 Table 1: Clusters and their citation network size 1.1 Related Work Although there has been work on analyzing citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002), to the knowledge of the author there is no previous work that study the text of the citation summaries to produce a summary. (Bradshaw, 2003; Bradshaw, 2002) get benefit from citations to determine the content of articles and introduce “Reference Directed Indexing” to improve the results of a search engine. In other work, (Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize citations into three groups using 160 pre-defined phrase-based rules. This categorization is then used to build a tool for survey generation. (Nanba and Okumura, 1999) also di"
C08-1087,W06-1613,0,0.352301,"re research on topic summarization. Given that the citation summary of any article usually has more than a few sentences, the main challenge of this task is to find a subset of these sentences that will lead to a better and shorter summary. 689 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 689–696 Manchester, August 2008 Cluster DP PBMT Summ QA TE Nodes 167 186 839 238 56 Edges 323 516 1425 202 44 Table 1: Clusters and their citation network size 1.1 Related Work Although there has been work on analyzing citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002), to the knowledge of the author there is no previous work that study the text of the citation summaries to produce a summary. (Bradshaw, 2003; Bradshaw, 2002) get benefit from citations to determine the content of articles and introduce “Reference Directed Indexing” to improve the results of a search engine. In other work, (Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize citations into three groups using 160 pre-defined phrase-based rules. This categorization is then"
C08-1087,radev-etal-2004-mead,1,\N,Missing
C08-1087,P99-1071,0,\N,Missing
C10-1101,A00-1043,0,0.0440531,"re-written as G(Di , Sk−1 ) = |Di ∩ (∪j6=i Dj ) − Sk−1 | Let’s denote Di ∩ (∪j6=i Dj ) by ∩i . The following equations prove the theorem. Experimental Setup We use the annotated data described in Section 2. In summary, the annotation consisted of two parts: nugget extraction and nugget distribution analysis. Five annotators were employed to annotate the sentences in each of the 25 citation summaries and write down the nuggets (non-overlapping contributions) of the target paper. Then using these 899 Summary generated using bigram-based keyphrases Sentence Ziff-Davis Corpus Most previous work (Jing 2000; Knight and Marcu 2002; Riezler et al 2003; Nguyen et al 2004a; Turner and Charniak 2005; McDonald 2006) has relied on automatically constructed parallel corpora for training and evaluation purposes. J05-4004:18 Between these two extremes, there has been a relatively modest amount of work in sentence simplification (Chandrasekar, Doran, and Bangalore 1996; Mahesh 1997; Carroll et al 1998; Grefenstette 1998; Jing 2000; Knight and Marcu 2002) and document compression (Daume III and Marcu 2002; Daume III and Marcu 2004; Zajic, Dorr, and Schwartz 2004) in which words, phrases, and sentences are s"
C10-1101,kan-etal-2002-using,0,0.0250531,"gorize citations into three groups using 160 predefined phrase-based rules to support a system for writing a survey. Previous research has shown the importance of the citation summaries in understanding what a paper contributes. In particular, (Elkiss et al., 2008) performed a large-scale study on citation summaries and their importance. Results from this experiment confirmed that the “Self Cohesion” (Elkiss et al., 2008) of a citation summary of an article is consistently higher than the that of its abstract and that citations contain additional information that does not appear in abstracts. Kan et al. (2002) use annotated bibliographies to cover certain aspects of summarization and suggest using metadata and critical document features as well as the prominent content-based features to summarize documents. Kupiec et al. (1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization. Siddharthan and Teufel describe a new task to decide the scientific attribution of an article (Siddharthan and Teufel, 2007) and show high human agreement as well as an improvement in the performance of Argumentative Zoning (Teufel, 2005). Argumentat"
C10-1101,N04-1019,0,0.00865802,"as many nuggets as possible. Each sentence in a citation summary may contain 0 or more nuggets and not all nuggets are mentioned an equal number of times. Covering some nuggets (contributions) is therefore more important than others and should be weighted highly. P where j = maxi ( nt=i |Tt |≥ X). The pyramid score for a summary is then calculated as follows. Q P = M ax This score ranges from 0 to 1, and a high score shows the summary contains more heavily weighted facts. To capture this property, the pyramid score seems the best evaluation metric to use. We use the pyramid evaluation method (Nenkova and Passonneau, 2004) at the sentence level to evaluate the summary created for each set. We benefit from the list of annotated nuggets provided by the annotators as the ground truth of the summarization evaluation. These annotations give the list of nuggets covered by each sentence in each citation summary, which are equivalent to the summarization content unit (SCU) as described in (Nenkova To evaluate the quality of the summaries generated by the greedy algorithm, we compare its pyramid score in each of the 25 citation summaries with those of a gold standard, a random summary, and four other methods. The gold s"
C10-1101,C08-1087,1,0.960171,"apers, resulting in a collective system. The focus of this work is on the corpora created based on citation sentences. A citation sentence is a sentence in an article containing a citation and can contain zero or more nuggets (i.e., non-overlapping contributions) about the cited article. For example the following sentences are a The set of citations is important to analyze because human summarizers have put their effort collectively but independently to read the target article and cite its important contributions. This has been shown in other work too (Elkiss et al., 2008; Nanba et al., 2004; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). In this work, we introduce a technique to summarize the set of citation sentences and cover the major contributions of the target paper. Our methodology first finds the set of keyphrases that represent important information units (i.e., nuggets), and then finds the best set of k sentences to cover more, and more important nuggets. Our results confirm the effectiveness of the method and show that it outperforms other state of the art summarization techniques. Moreover, as shown in the paper, this method does not need to calculate the full cosine sim"
C10-1101,radev-etal-2004-mead,1,0.703345,"Missing"
C10-1101,W09-3607,1,0.830442,"thod as one of our Fact f1 : “ Supervised Learning” f2 : “ instance/concept relations” f3 : “Part-of-Speech tagging” f4 : “filtering QA results” f5 : “lexico-semantic information” f6 : “hyponym relations” Occurrences 5 3 3 2 2 2 Table 2: Nuggets of P03-1001 extracted by annotators. baseline methods, which we have explained in more details in Section 4. 2 Data In order to evaluate our method, we use the ACL Anthology Network (AAN), which is a collection of papers from the Computational Linguistics journal and proceedings from ACL conferences and workshops and includes more than 13, 000 papers (Radev et al., 2009). We use 25 manually annotated papers from (Qazvinian and Radev, 2008), which are highly cited articles in AAN. Table 1 shows the ACL ID, title, and the number of citation sentences for these papers. The annotation guidelines asked a number of annotators to read the citation summary of each paper and extract a list of the main contributions of that paper. Each item on the list is a non-overlapping contribution (nugget) perceived by reading the citation summary. The annotation strictly instructed the annotators to focus on the citing sentences to do the task and not their own background on the"
C10-1101,N07-1040,0,0.04967,"is consistently higher than the that of its abstract and that citations contain additional information that does not appear in abstracts. Kan et al. (2002) use annotated bibliographies to cover certain aspects of summarization and suggest using metadata and critical document features as well as the prominent content-based features to summarize documents. Kupiec et al. (1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization. Siddharthan and Teufel describe a new task to decide the scientific attribution of an article (Siddharthan and Teufel, 2007) and show high human agreement as well as an improvement in the performance of Argumentative Zoning (Teufel, 2005). Argumentative Zoning is a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, Contrast according to their role in the author’s argument. These all show the importance of citation summaries and the vast area for new work to analyze them to produce a summary for a given topic. The Maximal Marginal Relevance (MMR) summarization method, which is based on a greedy algorithm, is described in (Carbonell and Goldstein, 199"
C10-1101,W03-1805,0,0.379153,"sentences that have a larger number of important and non-redundant keyphrases. In order to take the first step, we extract statistically significantly frequent N -grams (up to N = 4) from each citing sentence and use them as the set of representative keyphrases for that citing sentence. 3.1 Automatic Keyphrase Extraction A list of keyphrases for each citation sentence can be generated by extracting N -grams that occur significantly frequently in that sentence compared to a large corpus of such N -grams. Our method for such an extraction is inspired by the previous work by Tomokiyo and Hurst (Tomokiyo and Hurst, 2003). A language model, M, is a statistical model that assigns probabilities to a sequence of N grams. Every language model is a probability distribution over all N -grams and thus the probabilities of all N -grams of the same length sum up to 1. In order to extract keyphrases from a text using statistical significance we need two language models. The first model is referred to as the Background Model (BM) and is built using a large text corpus. Here we build the BM using the text of all the paper abstracts provided in AAN 1 . The second language model is called the Foreground Model (FM) and is th"
C10-1101,P08-1093,0,0.250599,"ctive system. The focus of this work is on the corpora created based on citation sentences. A citation sentence is a sentence in an article containing a citation and can contain zero or more nuggets (i.e., non-overlapping contributions) about the cited article. For example the following sentences are a The set of citations is important to analyze because human summarizers have put their effort collectively but independently to read the target article and cite its important contributions. This has been shown in other work too (Elkiss et al., 2008; Nanba et al., 2004; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). In this work, we introduce a technique to summarize the set of citation sentences and cover the major contributions of the target paper. Our methodology first finds the set of keyphrases that represent important information units (i.e., nuggets), and then finds the best set of k sentences to cover more, and more important nuggets. Our results confirm the effectiveness of the method and show that it outperforms other state of the art summarization techniques. Moreover, as shown in the paper, this method does not need to calculate the full cosine similarity matrix for a"
C10-1101,N09-1066,1,0.800494,"cus of this work is on the corpora created based on citation sentences. A citation sentence is a sentence in an article containing a citation and can contain zero or more nuggets (i.e., non-overlapping contributions) about the cited article. For example the following sentences are a The set of citations is important to analyze because human summarizers have put their effort collectively but independently to read the target article and cite its important contributions. This has been shown in other work too (Elkiss et al., 2008; Nanba et al., 2004; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). In this work, we introduce a technique to summarize the set of citation sentences and cover the major contributions of the target paper. Our methodology first finds the set of keyphrases that represent important information units (i.e., nuggets), and then finds the best set of k sentences to cover more, and more important nuggets. Our results confirm the effectiveness of the method and show that it outperforms other state of the art summarization techniques. Moreover, as shown in the paper, this method does not need to calculate the full cosine similarity matrix for a document cluster, which"
C98-2171,A97-1033,1,0.834779,"lues across the corpus. Notice that a large number of entities (9,053 out of the 11,504) have a single description. These are not as interesting for our analysis as the remaining 2,451 entities that have D D P E values between 2 and 24. Figure 1: Sample sentence containing two entity-description pairs. Each entity appearing in a text can have multiple descriptions (up to several dozen) associated with it. We call the set of all descriptions related to the same entity in a corpus, a profile of that entity. Profiles for a large number of entities were compiled using our earlier system, PROFILE (Radev and McKeown, 1997). It turns out that there is a large variety in the size of the profile (number of distinct descriptions) for different entities. Table 1 shows a subset of the profile for Ung Huot, the former foreign minister of Cambodia, who was elected prime minister at some point of time during the run of our experiment. A few sample semantic features of the descriptions in Table 1 are shown as separate columns. We used information extraction techniques to collect entities a~d descriptions from a corpus and analyzed their lexical and semantic properties. We have processed 178 MB 1 of newswire and analyzed"
C98-2171,J98-3005,1,0.780648,"incorrect P P attachment. We have also had problems from the part-of-speech tagger and, as a result, we occasionally incorrectly extract word sequences that do not represent descriptions. 6 Applications and Future Work We should note that P R O F I L E is part of a large system for information retrieval and summarization of news through information extraction and symbolic text generation (McKeown and Radev, 1995). We intend to use P R O F I L E to improve lexical choice in the s u m m a r y gem eration component, especially when producing user-centered summaries or s u m m a r y updates 1077 (Radev and McKeown, 1998 to appear). There are two particularly appealing cases - (1) when the extraction component has failed to extract a description and (2) when the user model (user&apos;s interests, knowledge of the entity and personal preferences for sources of information and for either conciseness or verbosity) dictates that a description should be used even when one doesn&apos;t appear in the texts being summarized. A second potentially interesting application involves using the d a t a and rules extracted by P R O F I L E for language regeneration. In (Radev and McKeown, 1998 to appear) we show how tile conversion of"
C98-2171,J93-1007,0,0.0150207,", which includes techniques of extracting shallow structure from a corpus and applying that structure to computer-generated texts. Language reuse involves two components: a source text written by a human and a target text, that is to be automatically generated by a computer, partially making use of structures reused from the source text. The source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which they are used. Some examples of language reuse include collocation analysis (Smadja, 1993), the use of entire factual sentences extracted from corpora (e.g., &quot;&apos;Toy Story&apos; is the Academy Award winning animated film developed by Pixa~&apos;), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995). In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques that can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993). Stydying the concept"
D07-1024,H05-1091,0,0.727069,"Similarity Based on Dependency Parsing In order to apply the semi-supervised harmonic functions and its supervised counterpart kNN, and the kernel based TSVM and SVM methods, we need to define a similarity measure between two sentences. For this purpose, we use the dependency parse trees of the sentences. Unlike a syntactic parse (which describes the syntactic constituent structure of a sentence), the dependency parse of a sentence captures the semantic predicate-argument relationships among its words. The idea of using dependency parse trees for relation extraction in general was studied by Bunescu and Mooney (2005a). To extract the relationship between two entities, they design a kernel function that uses the shortest path in the dependency tree between them. The motivation is based on the observation that the shortest path between the entities usually captures the necessary information to identify their relationship. They show that their approach outperforms the dependency tree kernel of Culotta and Sorensen (2004), which is based on the subtree that contains the two entities. We adapt the idea of Bunescu and Mooney (2005a) to the task of identifying protein-protein interaction sentences. We define th"
D07-1024,P04-1054,0,0.0703078,"ependency parse of a sentence captures the semantic predicate-argument relationships among its words. The idea of using dependency parse trees for relation extraction in general was studied by Bunescu and Mooney (2005a). To extract the relationship between two entities, they design a kernel function that uses the shortest path in the dependency tree between them. The motivation is based on the observation that the shortest path between the entities usually captures the necessary information to identify their relationship. They show that their approach outperforms the dependency tree kernel of Culotta and Sorensen (2004), which is based on the subtree that contains the two entities. We adapt the idea of Bunescu and Mooney (2005a) to the task of identifying protein-protein interaction sentences. We define the similarity between two sentences based on the paths between two proteins in the dependency parse trees of the sentences. In this study we assume that the protein names have already been annotated and focus instead on the task of extracting protein-protein interaction sentences for a given protein pair. We parse the sentences with the Stanford Parser1 (de Marneffe et al., 2006). From the dependency parse t"
D07-1024,de-marneffe-etal-2006-generating,0,0.0965333,"Missing"
D07-1024,W04-1204,0,0.0216441,"path between two protein names in a dependency tree is a good description of the semantic relation between them in the corresponding sentence. We consider two similarity functions; one based on the cosine similarity and the other based on the edit distance among such paths. 2 Related Work There have been many approaches to extract protein interactions from free text. One of them is based on matching pre-specified patterns and rules (Blaschke et al., 1999; Ono et al., 2001). However, complex cases that are not covered by the pre-defined patterns and rules cannot be extracted by these methods. Huang et al. (2004) proposed a method where patterns are discovered automatically from a set of sentences by dynamic programming. Bunescu et al. (2005) have studied the performance of rule learning algorithms. They propose two methods for protein interaction extraction. One is based on the rule learning method Rapier and the other on longest common subsequences. They show that these methods outperform hand-written rules. Another class of approaches is using more syntaxaware natural language processing (NLP) techniques. Both full and partial (shallow) parsing strategies have been applied in the literature. In par"
D07-1069,N06-1061,0,0.309544,"Missing"
D07-1069,W07-0204,0,0.0176735,"gnificant research has been done in the area of identifying central nodes in a network. Various methods exist for measuring centrality, including degree centrality, closeness, betweenness (Freeman, 1977; Newman, 2003), and eigenvector centrality. Eigenvector centrality in particular has been successfully applied to many different types of networks, including hyperlinked web pages (Brin and Page, 1998; Kleinberg, 1998), lexical networks (Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Kurland and Lee, 2005; Kurland and Lee, 2006), and semantic networks (Mihalcea et al., 2004). The authors of (Lin and Kan, 2007) extended these methods to include timestamped graphs where nodes are added over time and applied it to multidocument summarization. In (Tong and Faloutsos, 2006), the authors use random walks on a graph as a method for finding a subgraph that best connects some or all of a set of query nodes. In our paper, we introduce a new application of eigenvector centrality for identifying the central speakers in the type of debate or conversation network described above. Our method is based on the one described in (Erkan 658 Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Langua"
D07-1069,C04-1162,0,0.0816027,"Missing"
D07-1069,W06-1639,0,0.108682,"hip: are the central speakers on a given issue ranking members of a related committee? Is there a type of importance captured through speaker centrality that isn’t obvious in the natural committee rankings? There has been growing interest in using techniques from natural language processing in the area of political science. In (Porter et al., 2005) the authors performed a network analysis of members and committees of the US House of Representatives. They found connections between certain committees and political positions that suggest that committee membership is not determined at random. In (Thomas et al., 2006), the authors use the transcripts of debates from the US Congress to automatically classify speeches as supporting or opposing a given topic by taking advantage of the voting records of the speakers. In (Wang et al., 2005), the authors use a generative model to simultaneously discover groups of voters and topics using the voting records and the text from bills of the US Senate and the United Nations. The authors of (Quinn et al., 2006) introduce a multinomial mixture model to perform unsupervised clustering of Congressional speech documents into topically related categories. We rely on the out"
D07-1069,W04-3252,0,\N,Missing
D09-1145,E99-1043,0,0.551419,"Missing"
D09-1145,de-marneffe-etal-2006-generating,0,0.0663152,"Missing"
D09-1145,P08-2008,0,0.0296897,"Missing"
D09-1145,W08-0607,0,0.818758,"speculation keywords, we use the syntactic structures of the sentences to determine their scopes. 1 The first sentence is definite, whereas the second one contains speculative information, which is conveyed by the use of the word “suggest”. While speculative information might still be useful for biomedical scientists, it is important that it is distinguished from the factual information. Recognizing speculations in scientific text has gained interest in the recent years. Previous studies focus on identifying speculative sentences (Light et al., 2004; Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). However, in many cases, not the entire sentence, but fragments of a sentence are speculative. Consider the following example sentences. 1. The mature mitochondrial forms of the erythroid and housekeeping ALAS isozymes are predicted to have molecular weights of 59.5 kd and 64.6 kd, respectively. (PMID: 2050125) Introduction 2. Like RAD9, RAD9B associates with HUS1, RAD1, and Speculation, also known as hedging, is a frequently used language phenomenon in scientific articles, especially in experimental studies, which are common in the biomedical domain. When researchers are not completely certa"
D09-1145,W04-3103,0,0.487853,"esent the contexts of the keywords. After detecting the actual speculation keywords, we use the syntactic structures of the sentences to determine their scopes. 1 The first sentence is definite, whereas the second one contains speculative information, which is conveyed by the use of the word “suggest”. While speculative information might still be useful for biomedical scientists, it is important that it is distinguished from the factual information. Recognizing speculations in scientific text has gained interest in the recent years. Previous studies focus on identifying speculative sentences (Light et al., 2004; Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). However, in many cases, not the entire sentence, but fragments of a sentence are speculative. Consider the following example sentences. 1. The mature mitochondrial forms of the erythroid and housekeeping ALAS isozymes are predicted to have molecular weights of 59.5 kd and 64.6 kd, respectively. (PMID: 2050125) Introduction 2. Like RAD9, RAD9B associates with HUS1, RAD1, and Speculation, also known as hedging, is a frequently used language phenomenon in scientific articles, especially in experimental studies, which are co"
D09-1145,P07-1125,0,0.702771,"f the keywords. After detecting the actual speculation keywords, we use the syntactic structures of the sentences to determine their scopes. 1 The first sentence is definite, whereas the second one contains speculative information, which is conveyed by the use of the word “suggest”. While speculative information might still be useful for biomedical scientists, it is important that it is distinguished from the factual information. Recognizing speculations in scientific text has gained interest in the recent years. Previous studies focus on identifying speculative sentences (Light et al., 2004; Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). However, in many cases, not the entire sentence, but fragments of a sentence are speculative. Consider the following example sentences. 1. The mature mitochondrial forms of the erythroid and housekeeping ALAS isozymes are predicted to have molecular weights of 59.5 kd and 64.6 kd, respectively. (PMID: 2050125) Introduction 2. Like RAD9, RAD9B associates with HUS1, RAD1, and Speculation, also known as hedging, is a frequently used language phenomenon in scientific articles, especially in experimental studies, which are common in the biomedical doma"
D09-1145,P96-1006,0,0.0224951,"Missing"
D09-1145,radev-etal-2004-mead,1,0.763034,"Missing"
D09-1145,P08-1033,0,0.748166,"ting the actual speculation keywords, we use the syntactic structures of the sentences to determine their scopes. 1 The first sentence is definite, whereas the second one contains speculative information, which is conveyed by the use of the word “suggest”. While speculative information might still be useful for biomedical scientists, it is important that it is distinguished from the factual information. Recognizing speculations in scientific text has gained interest in the recent years. Previous studies focus on identifying speculative sentences (Light et al., 2004; Medlock and Briscoe, 2007; Szarvas, 2008; Kilicoglu and Bergler, 2008). However, in many cases, not the entire sentence, but fragments of a sentence are speculative. Consider the following example sentences. 1. The mature mitochondrial forms of the erythroid and housekeeping ALAS isozymes are predicted to have molecular weights of 59.5 kd and 64.6 kd, respectively. (PMID: 2050125) Introduction 2. Like RAD9, RAD9B associates with HUS1, RAD1, and Speculation, also known as hedging, is a frequently used language phenomenon in scientific articles, especially in experimental studies, which are common in the biomedical domain. When resear"
D09-1145,W08-0606,0,0.673412,"blem of identifying the portions of sentences which are speculative. In other words, we allow a sentence to include both speculative and non-speculative parts. We introduce and evaluate a diverse set of features that represent the context of a keyword and use these features in a supervised machine learning setting to classify 1399 the keywords as real speculation keywords or not. Then, we develop a rule-based method to determine their linguistic scopes by considering the keyword-specific features and the syntactic structures of the sentences. To the best of our knowledge, the BioScope corpus (Vincze et al., 2008) is the only available data set that has been annotated for speculative sentence fragments and we report the first results on this corpus. 3 Corpus The BioScope corpus2 has been annotated at the token level for speculation keywords and at the sentence level for their linguistic scopes (Vincze et al., 2008). The corpus consists of three subcorpora: medical free texts (radiology reports), biomedical article abstracts, and biomedical full text articles. In this paper we focus on identifying speculations in scientific text. Therefore, we use the biomedical article abstracts and the biomedical full"
D10-1121,E06-1027,0,0.0368055,"d Liu (2004) use WordNet synonyms and antonyms to predict the polarity of any given word with unknown polarity. They label each word with the polarity of its synonyms and the opposite polarity of its antonyms. They continue in a bootstrapping manner to label all unlabeled instances. This work is very similar to (Kamps et al., 2004) in which a network of WordNet synonyms is used to find the shortest path between any given word, and the words “good” and “bad”. Kim and Hovy (Kim and Hovy, 2004) used WordNet synonyms and antonyms to expand two lists of positive and negative seed words. Similarly, Andreevskaia and Bergler (2006) used WordNet to expand seed lists with fuzzy sentiment categories, in which words could be more central to one category than the other. Finally, Kanayama and Nasukawa (2006) used syntactic features and context coherency, defined as the tendency for same polarities to appear successively, to acquire polar atoms. All the work mentioned above focus on the task of identifying the polarity of individual words. Our proposed work is identifying attitudes in sentences that appear in online discussions. Perhaps the most similar work to ours is the prior work on subjectivity analysis, which is to ident"
D10-1121,banea-etal-2008-bootstrapping,0,0.195534,"rity of individual words. Our proposed work is identifying attitudes in sentences that appear in online discussions. Perhaps the most similar work to ours is the prior work on subjectivity analysis, which is to identify text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: The first category is concerned with identifying the subjectivity of individual phrases and words regardless of the sentence and context they appear in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, subjectivity of a phrase or word is analyzed within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, ). A good study of the applications of subjectivity analysis from review mining to email classification is given in (Wiebe, 2000). Somasundaran et al. (2007) develop genre-speci.c lexicons using interesting function word combinations for detecting opinions in meetings. Despite similarities, our work is different from subjectivity analysis because the later only discriminates between opinions and facts. A di"
D10-1121,H05-1091,0,0.0308323,"aced with their POS tags. Second person pronouns are left as is. Polarized words are replaced with their polarity tags and their POS tags. • Dependency grammar patterns: the shortest path connecting every second person pronoun to the closed polarized word is extracted. The second person pronoun, the polarized word tag, and the types of the dependency relations along the path connecting them are used as a pattern. It has been shown in previous work on relation extraction that the shortest path between any two entities captures the the information required to assert a relationship between them (Bunescu and Mooney, 2005). Every polarized word is assigned to the closest second person pronoun in the dependency tree. This is only useful for sentences that have polarized words. Table 1 shows the different kinds of representations for a particular sentence. We use text, partof-speech tags, polarity tags, and dependency relations. The corresponding patterns for this sentence are shown in Table 2. 4.4 Building the Models Given a set of patterns representing a set of sentences, we can build a graph G = V, E, w where V is the set of all possible token that may appear in the patterns. E = V × V is the set of possible t"
D10-1121,P08-1081,0,0.0131029,"ding-based model simultaneously model semantics and structure of threaded discussions. Shen et al (2006) proposes three clustering methods for exploiting the temporal information in the streams, as well as an algorithm based on linguistic features to analyze the discourse structure information. Huang et al (2007) used an SVM classifier to extract (thread-title, reply) pairs as chat knowledge from online discussion forums to support the construction of a chatbot for a certain domain. Other work has focused on the structure of questions and questionanswer pairs in online forums and discussions (Ding et al., 2008; Cong et al., 2008). 3 Problem Definition Assume we have a set of sentences exchanged between participants in an online discussion. Our objective is to identify sentences that display an attitude from the text writer to the text recepient from those that do not. An attitude is the mental position of one particpant with regard to another participant. An attitude may not be directly observable, but rather inferred from what particpants say to one another. The attitude could be either positive or negative. Strategies for showing a positive attitude may include agreement, and praise, while strate"
D10-1121,P97-1023,0,0.593668,"d dependency relations. We use all those patterns to build several pairs of models that represent sentences with and without attitude. The rest of the paper is organized as follows. In Section 2 we review some of the related prior work on identifying polarized words and subjectivity analysis. We explain the problem definition and discuss our approach in Sections 3 & 4. Finally, in Sections 5 & 6 we introduce our dataset and discuss the experimental setup. Finally, we conclude in Section 7. 2 Related Work Identifying the polarity of individual words is a well studied problem. In previous work, Hatzivassiloglou and McKeown (1997) propose a method to identify the polarity of adjectives. They use a manually labeled corpus to classify each conjunction of an adjective as “the same orientation” as the adjective or “different orientation”. Their method can label simple in “simple and well-received” as the same orientation and simplistic in “simplistic but well-received” as the opposite orientation of wellreceived. Although the results look promising, the method would only be applicable to adjectives since noun conjunctions may collocate regardless of their semantic orientations (e.g., “rise and fall”). In other work, Turney"
D10-1121,C00-1044,0,0.678105,"n the task of identifying the polarity of individual words. Our proposed work is identifying attitudes in sentences that appear in online discussions. Perhaps the most similar work to ours is the prior work on subjectivity analysis, which is to identify text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: The first category is concerned with identifying the subjectivity of individual phrases and words regardless of the sentence and context they appear in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, subjectivity of a phrase or word is analyzed within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, ). A good study of the applications of subjectivity analysis from review mining to email classification is given in (Wiebe, 2000). Somasundaran et al. (2007) develop genre-speci.c lexicons using interesting function word combinations for detecting opinions in meetings. Despite similarities, our work is different from subjectivity analysis because the later only discriminates between opi"
D10-1121,ruppenhofer-etal-2008-finding,0,0.00848567,"ven in (Wiebe, 2000). Somasundaran et al. (2007) develop genre-speci.c lexicons using interesting function word combinations for detecting opinions in meetings. Despite similarities, our work is different from subjectivity analysis because the later only discriminates between opinions and facts. A discussion sentence may display an opinion about some topic yet no attitude. The language constituents considered in opinion detection may be different from those used to detect attitude. Moreover, extracting attitudes from online discussions is different from targeting subjective expressions (Josef Ruppenhofer and Wiebe, 2008; Kim and Hovy, 2004). The later usually has a limited set of targets that compete for the subjective expressions (for example in movie review, targets could be: director, actors, plot, and so forth). We cannot use similar methods because we are working on an open domain where anything could be a target. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). There is also some related work on mining online discussions. Lin et al (2009) proposes a sparse coding-based model simultaneously model semantics and s"
D10-1121,kamps-etal-2004-using,0,0.0285691,"Missing"
D10-1121,W06-1642,0,0.035235,"posite polarity of its antonyms. They continue in a bootstrapping manner to label all unlabeled instances. This work is very similar to (Kamps et al., 2004) in which a network of WordNet synonyms is used to find the shortest path between any given word, and the words “good” and “bad”. Kim and Hovy (Kim and Hovy, 2004) used WordNet synonyms and antonyms to expand two lists of positive and negative seed words. Similarly, Andreevskaia and Bergler (2006) used WordNet to expand seed lists with fuzzy sentiment categories, in which words could be more central to one category than the other. Finally, Kanayama and Nasukawa (2006) used syntactic features and context coherency, defined as the tendency for same polarities to appear successively, to acquire polar atoms. All the work mentioned above focus on the task of identifying the polarity of individual words. Our proposed work is identifying attitudes in sentences that appear in online discussions. Perhaps the most similar work to ours is the prior work on subjectivity analysis, which is to identify text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main ca"
D10-1121,C04-1200,0,0.479311,"ns. 1246 Previous work has also used WordNet, a lexical database of English, to identify word polarity. Specifically, Hu and Liu (2004) use WordNet synonyms and antonyms to predict the polarity of any given word with unknown polarity. They label each word with the polarity of its synonyms and the opposite polarity of its antonyms. They continue in a bootstrapping manner to label all unlabeled instances. This work is very similar to (Kamps et al., 2004) in which a network of WordNet synonyms is used to find the shortest path between any given word, and the words “good” and “bad”. Kim and Hovy (Kim and Hovy, 2004) used WordNet synonyms and antonyms to expand two lists of positive and negative seed words. Similarly, Andreevskaia and Bergler (2006) used WordNet to expand seed lists with fuzzy sentiment categories, in which words could be more central to one category than the other. Finally, Kanayama and Nasukawa (2006) used syntactic features and context coherency, defined as the tendency for same polarities to appear successively, to acquire polar atoms. All the work mentioned above focus on the task of identifying the polarity of individual words. Our proposed work is identifying attitudes in sentences"
D10-1121,P03-1054,0,0.00721898,"esenting the grammatical structure of a particular sentence. If we closely examine the sentence, we will notice that we are only interested in a part of the sentence that includes the second person pronoun ”you“. We extract this part, by starting at the word of interest , in this case ”you“, and go up in the hierarchy till we hit the first sentence clause. Once, we reach a sentence clause, we extract the corre1248 sponding text if it is grammatical, otherwise we go up one more level to the closest sentence clause. We used the Stanford parser to generate the grammatical structure of sentences (Klein and Manning, 2003). Figure 1: An example showing how to identify the relevant part of a sentence. 4.3 Sentences as Patterns The fragments we extracted earlier are more relevant to our task and are more suitable for further analysis. However, these fragments are completely lexicalized and consequently the performance of any analysis based on them will be limited by data sparsity. We can alleviate this by using more general representations of words. Those general representations can be used a long with words to generate a set of patterns that represent each fragment. Each pattern consists of a sequence of tokens."
D10-1121,W03-1014,0,0.58015,"sions. Perhaps the most similar work to ours is the prior work on subjectivity analysis, which is to identify text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: The first category is concerned with identifying the subjectivity of individual phrases and words regardless of the sentence and context they appear in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, subjectivity of a phrase or word is analyzed within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, ). A good study of the applications of subjectivity analysis from review mining to email classification is given in (Wiebe, 2000). Somasundaran et al. (2007) develop genre-speci.c lexicons using interesting function word combinations for detecting opinions in meetings. Despite similarities, our work is different from subjectivity analysis because the later only discriminates between opinions and facts. A discussion sentence may display an opinion about some topic yet no attitude. The language constituents considered i"
D10-1121,2007.sigdial-1.5,0,0.0111659,"bjectivity analysis mainly consists of two main categories: The first category is concerned with identifying the subjectivity of individual phrases and words regardless of the sentence and context they appear in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, subjectivity of a phrase or word is analyzed within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, ). A good study of the applications of subjectivity analysis from review mining to email classification is given in (Wiebe, 2000). Somasundaran et al. (2007) develop genre-speci.c lexicons using interesting function word combinations for detecting opinions in meetings. Despite similarities, our work is different from subjectivity analysis because the later only discriminates between opinions and facts. A discussion sentence may display an opinion about some topic yet no attitude. The language constituents considered in opinion detection may be different from those used to detect attitude. Moreover, extracting attitudes from online discussions is different from targeting subjective expressions (Josef Ruppenhofer and Wiebe, 2008; Kim and Hovy, 2004)"
D10-1121,P05-1017,0,0.145255,"nd simplistic in “simplistic but well-received” as the opposite orientation of wellreceived. Although the results look promising, the method would only be applicable to adjectives since noun conjunctions may collocate regardless of their semantic orientations (e.g., “rise and fall”). In other work, Turney and Littman (2003) use statistical measures to find the association between a given word and a set of positive/negative seed words. In order to get word co-occurrence statistics they use the “near” operator from a commercial search engine on a given word and a seed word. In more recent work, Takamura et al. (2005) used the spin model to extract word semantic orientation. First, they construct a network of words using definitions, thesaurus, and co-occurrence statistics. In this network, each word is regarded as an electron, which has a spin and each spin has a direction taking one of two values: up or down. Then, they use the energy point of view to propose that neighboring electrons tend to have the same spin direction, and therefore neighboring words tend to have the same polarity orientations. Finally, they use the mean field method to find the optimal solution for electron spin directions. 1246 Pre"
D10-1121,W06-1639,0,0.231746,"positive or negative. This is different from most of the research on social networks that has focused almost exclusively on positive links. The method is experimentally tested using a manually labeled set of discussion posts. The results show that the proposed method is capable of identifying attitudinal sentences, and their signs, with high accuracy and that it outperforms several other baselines. 1 Introduction Mining sentiment from text has a wide range of applications from mining product reviews on the Web (Morinaga et al., 2002; Turney and Littman, 2003) to analyzing political speeches (Thomas et al., 2006). Automatic methods for sentiment mining are very important because manual extraction of them is very costly, and inefficient. A new application of sentiment mining is to automatically identify attitudes between participants in an online discussion. An automatic tool to identify attitudes will enable Alice: “You know what, he turned out to be a great disappointment” Bob: “You are completely unqualified to judge this great person” However, Bob shows strong attitude toward Alice. In this work, we look at ways to predict whether a sentence displays an attitude toward the text recipient. An attitu"
D10-1121,H05-1044,0,0.123638,"already labeled as either positive or negative respectively. We used the list of labeled seeds from (Hatzivassiloglou and McKeown, 1997) and (Stone et al., 1966). For any given word w, we calculate the mean hitting time between w, and the two seed sets h(w|S+), and h(w|S−). The mean hitting time h(i|k) is defined as the average number of steps a random walker, starting in state i 6= k, will take to enter state k for the first time (Norris, 1997). If h(w|S+) is greater than h(w|S−), the word is classified as negative, otherwise it is classified as positive. We also use the method described in (Wilson et al., 2005) to determine the contextual polarity of the identified words. The set of features used to predict contextual polarity include word, sentence, polarity, structure, and other features. 4.2 Identifying Relevant Parts of Sentences The writing style in online discussion forums is very informal. Some of the sentence are very long, and punctuation marks are not always properly used. To solve this problem, we decided to use the grammatical structure of sentences to identify the most relevant part of sentences that would be the subject of further analysis. Figure 1 shows a parse tree representing the"
D10-1121,W03-1017,0,0.511599,"similar work to ours is the prior work on subjectivity analysis, which is to identify text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: The first category is concerned with identifying the subjectivity of individual phrases and words regardless of the sentence and context they appear in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, subjectivity of a phrase or word is analyzed within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, ). A good study of the applications of subjectivity analysis from review mining to email classification is given in (Wiebe, 2000). Somasundaran et al. (2007) develop genre-speci.c lexicons using interesting function word combinations for detecting opinions in meetings. Despite similarities, our work is different from subjectivity analysis because the later only discriminates between opinions and facts. A discussion sentence may display an opinion about some topic yet no attitude. The language constituents considered in opinion detection may be diff"
D10-1121,J93-2004,0,\N,Missing
D10-1121,W06-2920,0,\N,Missing
D10-1121,N09-1009,0,\N,Missing
D10-1121,H05-2017,0,\N,Missing
D10-1121,H05-1043,0,\N,Missing
D10-1121,W09-0106,0,\N,Missing
D10-1121,N09-1012,0,\N,Missing
D10-1121,J03-4003,0,\N,Missing
D10-1121,P09-1009,0,\N,Missing
D10-1121,P07-1009,0,\N,Missing
D10-1121,P10-1131,0,\N,Missing
D10-1121,P04-1060,0,\N,Missing
D10-1121,P07-1036,0,\N,Missing
D10-1121,P06-1111,0,\N,Missing
D10-1121,P09-1041,0,\N,Missing
D10-1121,P09-1042,0,\N,Missing
D10-1121,P04-1061,0,\N,Missing
D10-1121,D07-1072,0,\N,Missing
D10-1121,D08-1092,0,\N,Missing
D10-1121,P07-1035,0,\N,Missing
D10-1121,P09-2001,0,\N,Missing
D11-1147,J96-2004,0,0.0362874,"annotated data task rumor retrieval belief classification κ 0.954 0.853 Table 3: Inter-judge agreement in two annotation tasks in terms of κ-statistic 4.2 Inter-Judge Agreement To calculate the annotation accuracy, we annotated 500 instances twice. These annotations were compared with each other, and the Kappa coefficient (κ) was calculated. The κ statistic is formulated as κ= Pr(a) − Pr(e) 1 − Pr(e) where P r(a) is the relative observed agreement among raters, and P r(e) is the probability that annotators agree by chance if each annotator is randomly assigning categories (Krippendorff, 1980; Carletta, 1996). Table 3 shows that annotators can reach a high agreement in both extracting rumors (κ = 0.95) and identifying believers (κ = 0.85). 5 Approach In this section, we describe a general framework, which given a tweet, predicts (1) whether it is a rumor-related statement, and if so (2) whether the user believes the rumor or not. We describe 3 sets of features, and explain why these are intuitive to use for identification of rumors. We process the tweets as they appear in the user timeline, and do not perform any pre-processing. Specially, we think that capitalization might be an important propert"
D11-1147,P07-1104,0,0.0594877,"e values. Here, the feature vector is the vector of coefficients corresponding to different network, content, and twitterbased properties, and the parameter vector θ ∈ RD (D ≤ 9 in our experiments) assigns a real-valued weight to each feature. This estimator chooses θ to minimize the sum of least squares and a regularization term R. 1X θˆ = arg min{ ||hθ, xi i − yi ||22 + R(θ)} (6) θ 2 i where the regularizer term R(θ) is the weighted L1 norm of the parameters. X R(θ) = α |θj | (7) j Here, α is a parameter that controls the amount of regularization (set to 0.1 in our experiments). Gao et. al (Gao et al., 2007) argue that optimizing L1 -regularized objective function is challenging since its gradient is discontinuous whenever some parameters equal zero. In this work, we use the orthant-wise limited-memory quasi-Newton algorithm (OWL-QN), which is a modification of LBFGS that allows it to effectively handle the discontinuity of the gradient (Andrew and Gao, 2007). OWL-QN is based on the fact that when restricted to a single orthant, the L1 regularizer is differentiable, and is in fact a linear function of θ. Thus, as long as each coordinate of any two consecutive search points does not pass through z"
D11-1147,D10-1121,1,0.258531,"et al., ). They analyze the re-tweet network topology and find that the patterns of propagation in rumors differ from news because rumors tend to be questioned more than news by the Twitter community. 2.2 Sentiment Analysis The automated detection of rumors is similar to traditional NLP sentiment analysis tasks. Previous work has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). Hassan et al. use a supervised Markov model, part of speech, and dependency patterns to identify attitudinal polarities in threads posted to Usenet discussion posts (Hassan et al., 2010). Others have designated sentiment scores for news stories and blog posts based on algorithmically generated lexicons of positive and negative words (Godbole et al., 2007). Pang and Lee provide a detailed overview of current techniques and practices in sentiment analysis and opinion mining (Pang and Lee, 2008; Pang and Lee, 2004). Though rumor classification is closely related to 1590 opinion mining and sentiment analysis, it presents a different class of problem because we are concerned not just with the opinion of the person posting a tweet, but with whether the statements they post appear c"
D11-1147,P04-1035,0,0.015121,"used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). Hassan et al. use a supervised Markov model, part of speech, and dependency patterns to identify attitudinal polarities in threads posted to Usenet discussion posts (Hassan et al., 2010). Others have designated sentiment scores for news stories and blog posts based on algorithmically generated lexicons of positive and negative words (Godbole et al., 2007). Pang and Lee provide a detailed overview of current techniques and practices in sentiment analysis and opinion mining (Pang and Lee, 2008; Pang and Lee, 2004). Though rumor classification is closely related to 1590 opinion mining and sentiment analysis, it presents a different class of problem because we are concerned not just with the opinion of the person posting a tweet, but with whether the statements they post appear controversial. The automatic identification of rumors from a corpus is most closely related to the identification of memes done in (Leskovec et al., 2009), but presents new challenges since we seek to highlight a certain type of recurring phrases. Our work presents one of the first attempts at automatic rumor analysis. 2.3 Mining"
D11-1147,W02-1011,0,0.0255641,"and simply talk about rumors of interest. Mendoza et al. explore Twitter data to analyze the behavior of Twitter users under the emergency situation of 2010 earthquake in Chile (Mendoza et al., ). They analyze the re-tweet network topology and find that the patterns of propagation in rumors differ from news because rumors tend to be questioned more than news by the Twitter community. 2.2 Sentiment Analysis The automated detection of rumors is similar to traditional NLP sentiment analysis tasks. Previous work has used machine learning techniques to identify positive and negative movie reviews (Pang et al., 2002). Hassan et al. use a supervised Markov model, part of speech, and dependency patterns to identify attitudinal polarities in threads posted to Usenet discussion posts (Hassan et al., 2010). Others have designated sentiment scores for news stories and blog posts based on algorithmically generated lexicons of positive and negative words (Godbole et al., 2007). Pang and Lee provide a detailed overview of current techniques and practices in sentiment analysis and opinion mining (Pang and Lee, 2008; Pang and Lee, 2004). Though rumor classification is closely related to 1590 opinion mining and senti"
D11-1147,pak-paroubek-2010-twitter,0,\N,Missing
D12-1006,W10-0731,0,0.020077,"Missing"
D12-1006,banea-etal-2008-bootstrapping,0,0.0177042,"toward another makes our work related to a huge body of work on sentiment analysis. One such line of research is the well-studied problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005). Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008; Riloff and Wiebe, 2003). Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. Our method is not restricted to phrases or words, rather it generalizes this to identifying the polarity of an interaction between two individuals based on several posts they exchange. 2.2 Stance Classification Perhaps the closest work to this paper is the work on stance classification. We notice that most of these methods focus on the polarity of th"
D12-1006,C08-2004,0,0.0403918,"Missing"
D12-1006,P11-1078,0,0.0132737,"ting social relations between individuals from text. Elson et al. (2010) present a method for extracting social networks from nineteenth-century British novels and serials. They link two characters based on whether they are in conversation or not. McCallum et al. (2007) explored the use of structured data such as email headers for social network construction. Gruzd and Hyrthonthwaite (2008) explored the use of post text in discussions to study interaction patterns in e-learning communities. Extracting social power relations from natural language (i.e. who influences whom) has been studied in (Bramsen et al., 2011; Danescu-Niculescu-Mizil et al., 2011). Our work is related to this line of research because we employ natural language processing techniques to reveal embedded social structures. Despite similarities, our work is uniquely characterized by the fact that we extract signed social networks with both positive and negative links from text. 2.4 Signed Social Networks Most of the work on social networks analysis has only focused on positive interactions. A few recent papers have taken the signs of edges into account. Brzozowski et al. (2008) study the positive and negative relationships between user"
D12-1006,H05-1091,0,0.0225828,"nnecting the two entities offers a very condensed representation of the information needed to assess whether they are related or not. For example the two sentences “you are completely unqualified” and “you know what, he is unqualified ...” show two different ways the words “you”, and “unqualified” could appear in a sentence. In the first case the polarized word “unqualified” refers to the word “you”. In the second case, the two words are not related. The information in the shortest path between two entities in a dependency tree can be used to assert whether a relationship exists between them (Bunescu and Mooney, 2005). The sequence of words connecting the two entities is a very good predictor of whether they are related or not. However, these paths are completely lexicalized and consequently their performance will be limited by data sparseness. To alleviate this problem, we use higher levels of generalization to represent the path connecting the two tokens. These representations are the part-of-speech tags, and the shortest path in a dependency graph connecting the two tokens. We represent every sentence with several representations at different levels of generalization. For example, the sentence “your ide"
D12-1006,D09-1030,0,0.0198989,"Missing"
D12-1006,P10-1015,0,0.0121164,"entation of the discussion. Research on debate stance recognition attempts to perform classification under the “supporting vs. opposing” paradigm. However such simple view might not always be accurate for discussions on more complex topics with many aspects. After building the signed network representation of discussions, we present a method that can detect how the large group could split into many subgroups (not necessarily two) with coherent opinions. 2.3 Extracting Social Networks from Text Little work has been done on the front of extracting social relations between individuals from text. Elson et al. (2010) present a method for extracting social networks from nineteenth-century British novels and serials. They link two characters based on whether they are in conversation or not. McCallum et al. (2007) explored the use of structured data such as email headers for social network construction. Gruzd and Hyrthonthwaite (2008) explored the use of post text in discussions to study interaction patterns in e-learning communities. Extracting social power relations from natural language (i.e. who influences whom) has been studied in (Bramsen et al., 2011; Danescu-Niculescu-Mizil et al., 2011). Our work is"
D12-1006,P04-1085,0,0.223232,"umber of hand crafted rules to identify agreement and disagreement interactions. Hand crafted rules usually result in systems with very low recall causing them to miss many agreement/disagreement instances (they report 0.26 recall at the 0.56 precision level). We present a machine learning system to solve this problem and achieve much better performance. Park et al. (2011) propose a method for finding news articles with different views on contentious issues. Mohit et al. (2008) present a set of heuristics for including disagreement information in a minimum cut stance classification framework. Galley et al. (2004) show the value of using durational and structural features for identifying agreement and disagreement in spoken conver61 sational speech. They use features like duration of spurts, speech rate, speaker overlap, etc. which are not applicable to written language. Our approach is different from agreement/disagreement identification because we not only study sentiment at the local sentiment level but also at the global level that takes into consideration many posts exchanged between participants to build a signed network representation of the discussion. Research on debate stance recognition atte"
D12-1006,D10-1121,1,0.947832,"ote, then every reference connecting them is an agreement and vice versa. We believe this will result in a very noisy training/testing set and hence we decided to recruit human annotators to create a training set. We found out that many instances with references to other discussants were labeled as neither agreement nor disagreement regardless of whether the discussants have similar or opposing positions. We will use this system as a baseline and will show that the existence of positive/negative words close to a person name does not necessarily show agreement or disagreement with that person. Hassan et al. (2010) use a language model based approach for identifying agreement and disagreement sentences in discussions. This work is limited to sentences. It does not consider the overall relation between participants. It also does not consider subgroup detection. We will use this method as a baseline for one of our components and will show that the proposed method outperforms it. Murakami and Raymond (2010) present another method for stance recognition. They use a small number of hand crafted rules to identify agreement and disagreement interactions. Hand crafted rules usually result in systems with very l"
D12-1006,P97-1023,0,0.230025,"explain our approach in Section 3. Section 4 describes our dataset. Results and discussion are presented in Section 5. We present a method for identifying subgroups in online discussions in Section 3.3. We conclude in Section 6. 2 Related Work In this section, we survey several lines of research that are related to our work. 60 Mining Sentiment from Text Our general goal of mining attitude from one individual toward another makes our work related to a huge body of work on sentiment analysis. One such line of research is the well-studied problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005). Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008; Riloff and Wiebe, 2003). Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the p"
D12-1006,C00-1044,0,0.110654,"ning attitude from one individual toward another makes our work related to a huge body of work on sentiment analysis. One such line of research is the well-studied problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005). Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008; Riloff and Wiebe, 2003). Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. Our method is not restricted to phrases or words, rather it generalizes this to identifying the polarity of an interaction between two individuals based on several posts they exchange. 2.2 Stance Classification Perhaps the closest work to this paper is the work on stance classification. We notice that most of these methods focus o"
D12-1006,C04-1200,0,0.0377175,"aset. Results and discussion are presented in Section 5. We present a method for identifying subgroups in online discussions in Section 3.3. We conclude in Section 6. 2 Related Work In this section, we survey several lines of research that are related to our work. 60 Mining Sentiment from Text Our general goal of mining attitude from one individual toward another makes our work related to a huge body of work on sentiment analysis. One such line of research is the well-studied problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005). Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008; Riloff and Wiebe, 2003). Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. O"
D12-1006,P03-1054,0,0.00704206,"Missing"
D12-1006,C10-2100,0,0.0359173,"pposing positions. We will use this system as a baseline and will show that the existence of positive/negative words close to a person name does not necessarily show agreement or disagreement with that person. Hassan et al. (2010) use a language model based approach for identifying agreement and disagreement sentences in discussions. This work is limited to sentences. It does not consider the overall relation between participants. It also does not consider subgroup detection. We will use this method as a baseline for one of our components and will show that the proposed method outperforms it. Murakami and Raymond (2010) present another method for stance recognition. They use a small number of hand crafted rules to identify agreement and disagreement interactions. Hand crafted rules usually result in systems with very low recall causing them to miss many agreement/disagreement instances (they report 0.26 recall at the 0.56 precision level). We present a machine learning system to solve this problem and achieve much better performance. Park et al. (2011) propose a method for finding news articles with different views on contentious issues. Mohit et al. (2008) present a set of heuristics for including disagreem"
D12-1006,P11-1035,0,0.0277168,"t consider subgroup detection. We will use this method as a baseline for one of our components and will show that the proposed method outperforms it. Murakami and Raymond (2010) present another method for stance recognition. They use a small number of hand crafted rules to identify agreement and disagreement interactions. Hand crafted rules usually result in systems with very low recall causing them to miss many agreement/disagreement instances (they report 0.26 recall at the 0.56 precision level). We present a machine learning system to solve this problem and achieve much better performance. Park et al. (2011) propose a method for finding news articles with different views on contentious issues. Mohit et al. (2008) present a set of heuristics for including disagreement information in a minimum cut stance classification framework. Galley et al. (2004) show the value of using durational and structural features for identifying agreement and disagreement in spoken conver61 sational speech. They use features like duration of spurts, speech rate, speaker overlap, etc. which are not applicable to written language. Our approach is different from agreement/disagreement identification because we not only stu"
D12-1006,W03-1014,0,0.416012,"our work related to a huge body of work on sentiment analysis. One such line of research is the well-studied problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005). Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008; Riloff and Wiebe, 2003). Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. Our method is not restricted to phrases or words, rather it generalizes this to identifying the polarity of an interaction between two individuals based on several posts they exchange. 2.2 Stance Classification Perhaps the closest work to this paper is the work on stance classification. We notice that most of these methods focus on the polarity of the written text assuming t"
D12-1006,P05-1017,0,0.0367642,"scussion are presented in Section 5. We present a method for identifying subgroups in online discussions in Section 3.3. We conclude in Section 6. 2 Related Work In this section, we survey several lines of research that are related to our work. 60 Mining Sentiment from Text Our general goal of mining attitude from one individual toward another makes our work related to a huge body of work on sentiment analysis. One such line of research is the well-studied problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003; Kim and Hovy, 2004; Takamura et al., 2005). Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008; Riloff and Wiebe, 2003). Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. Our method is not restric"
D12-1006,W06-1639,0,0.596735,"identify groups. Tan et al. (2011) studied how twitter following relations can be used to improve stance classification. Their main hypothesis is that connected users are more likely to hold similar opinions. This may be correct for the twitter following relations, but it is not necessarily correct for open discussions where no such relations exist. The only criterion that can be used to connect discussants is how often they reply to each other’s posts. We will show later that while many people reply to people with similar opinions, many others reply to people with different opinions as well. Thomas et al. (2006) address the same problem of determining support and opposition as applied to congressional floor-debates. They assess the agreement/disagreement between different speakers by training a text classifier and applying it to a window surrounding the names of other speakers. They construct their training data by assuming that if two speaker have the same vote, then every reference connecting them is an agreement and vice versa. We believe this will result in a very noisy training/testing set and hence we decided to recruit human annotators to create a training set. We found out that many instances"
D12-1006,H05-2018,0,0.0328097,"globally splits the community involved in the discussion by utilizing the dynamics of the local interactions between participants. 3 3.1 Approach Identifying Attitude from Text To build a signed network representation of discussants, we start by trying to identify sentences that show positive or negative attitude from the writer to the addressee. The first step toward identifying attitude is to identify words with positive/negative semantic orientation. The semantic orientation or polarity of a word indicates the direction the word deviates from the norm (Lehrer, 1974). We use OpinionFinder (Wilson et al., 2005a) to identify words with positive or negative semantic orientation. The polarity of a word is also affected by the context where the word appears. For example, a positive word that appears in a negated context should have a negative polarity. Other polarized words sometimes appear as neutral words in some contexts. To identify contex62 tual polarity of words, a large set of features is used including words, sentences, structure, and other features similar to the method described in (Wilson et al., 2005b). Our overall objective is to find the direct attitude between participants. Hence after i"
D12-1006,H05-1044,0,0.0126188,"globally splits the community involved in the discussion by utilizing the dynamics of the local interactions between participants. 3 3.1 Approach Identifying Attitude from Text To build a signed network representation of discussants, we start by trying to identify sentences that show positive or negative attitude from the writer to the addressee. The first step toward identifying attitude is to identify words with positive/negative semantic orientation. The semantic orientation or polarity of a word indicates the direction the word deviates from the norm (Lehrer, 1974). We use OpinionFinder (Wilson et al., 2005a) to identify words with positive or negative semantic orientation. The polarity of a word is also affected by the context where the word appears. For example, a positive word that appears in a negated context should have a negative polarity. Other polarized words sometimes appear as neutral words in some contexts. To identify contex62 tual polarity of words, a large set of features is used including words, sentences, structure, and other features similar to the method described in (Wilson et al., 2005b). Our overall objective is to find the direct attitude between participants. Hence after i"
D16-1006,W12-3010,0,0.0397389,"Missing"
D16-1006,P15-1034,0,0.181507,"unstructured text, the problem of the extractions being uninformative and incomplete has surfaced. A recent paper (Bast and Haussmann, 2014) pointed out that a significant fraction of the extracted propositions is not informative. A simple inference algorithm was proposed that uses generic rules for each semantic class of predicate to derive new triples from extracted triples. Though it improved the informativeness of extracted triples, it did not alleviate the problem of lost context in complex sentences. We, therefore, create our own extractions. Some recent works (Bast and Haussmann, 2013; Angeli et al., 2015) have tried to address the problem of long and uninformative extractions in opendomain information extraction by finding short entailment or clusters of semantically related constituents from a longer utterance. These clusters are reduced to triples using schema mapping to known relation types or using a set of hand-crafted rules. N EST IE shares similar objectives but uses bootstrapping to learn extraction patterns. Bootstrapping and pattern learning has a long history in traditional information extraction. Systems like DIPRE (Brin, 1998), S NOWBALL (Agichtein and Gravano, 2000), NELL (Mitche"
D16-1006,P98-1013,0,0.0620378,"the fact phrases. There is another body of work in natural language understanding that shares tasks with OIE. AMR parsing (Banarescu et al., ), semantic role labeling (SRL) (Toutanova et al., 2008; Punyakanok et al., 2008) and frame-semantic parsing (Das et al., 2014). In these tasks, verbs or nouns are analyzed to identify their arguments. The verb or noun is then mapped to a semantic frame and roles of each argument in the frame are identified. These techniques have gained interest with the advent of hand-constructed semantic resources like PropBank and FrameNet (Kingsbury and Palmer, 2002; Baker et al., 1998). Generally, the verb/noun and the semantically labeled arguments correspond to OIE propositions and, therefore, the two tasks are considered similar. Systems like SRL-IE (Christensen et al., 2010) explore if these techniques can be used for OIE. However, while OIE aims to identify the relation/predicate between a pair of arguments, frame-based techniques aim to identify arguments and their roles with respect to a predicate. Hence, the frames won’t correspond to propositions when both the arguments cannot be identified for a binary relation or when the correct argument is buried in long argume"
D16-1006,N07-4013,0,0.0174812,"Missing"
D16-1006,W10-0907,0,0.0553025,"2008; Punyakanok et al., 2008) and frame-semantic parsing (Das et al., 2014). In these tasks, verbs or nouns are analyzed to identify their arguments. The verb or noun is then mapped to a semantic frame and roles of each argument in the frame are identified. These techniques have gained interest with the advent of hand-constructed semantic resources like PropBank and FrameNet (Kingsbury and Palmer, 2002; Baker et al., 1998). Generally, the verb/noun and the semantically labeled arguments correspond to OIE propositions and, therefore, the two tasks are considered similar. Systems like SRL-IE (Christensen et al., 2010) explore if these techniques can be used for OIE. However, while OIE aims to identify the relation/predicate between a pair of arguments, frame-based techniques aim to identify arguments and their roles with respect to a predicate. Hence, the frames won’t correspond to propositions when both the arguments cannot be identified for a binary relation or when the correct argument is buried in long argument phrases. 57 Dataset Syntactic paraphrases Seed Templates Bootstrapping Pattern Representation Pattern Learning Pattern Representation Syntactic Patterns Fact Extraction Propositions Seed Extract"
D16-1006,W99-0613,0,0.024514,"were ignored while constructing the seed set. Example 1 Consider a statement-hypothesis pair, Statement: Paul Bremer, the top U.S. civilian administrator in Iraq, and Iraq’s new president, Ghazi alYawar, visited the northern Iraqi city of Kirkuk. Hypothesis: Ghazi al-Yawar is the president of Iraq. The hypothesis is entailed in the statement. The seed templates extract propositions from the hypothesis: (al-Yawar,is,president, (al-Yawar,is,president of Iraq), and (al-Yawar,is president of,Iraq). Bootstrapping is a popular technique to generate positive training data for information extraction (Collins and Singer, 1999; Hoffmann et al., 2011). We extend the bootstrapping techniques employed in O LLIE and R E N OUN, for n-ary and complex relations. First, instead of learning dependency parsetree patterns connecting the heads of the argument phrases and the relation phrase connecting them, we learn the dependency parse-tree patterns connecting the heads of all argument and relation phrases in the template. This allows greater coverage of context for the propositions and prevents the arguments/relations from being over-specified and/or uninformative. Second, some of the relations in the representation are deri"
D16-1006,de-marneffe-etal-2006-generating,0,0.0329068,"Missing"
D16-1006,D11-1142,0,0.301122,"r-order relations, and complex, interdependent assertions. Nesting the extracted propositions allows N EST IE to more accurately reflect the meaning of the original sentence. Our experimental study on real-world datasets suggests that N EST IE obtains comparable precision with better minimality and informativeness than existing approaches. N EST IE produces 1.7-1.8 times more minimal extractions and achieves 1.1-1.2 times higher informativeness than C LAUS IE. 1 Dragomir Radev Department of EECS University of Michigan Ann Arbor radev@umich.edu However, state-of-the-art OIE systems, R E V ERB (Fader et al., 2011) and O LLIE (Schmitz et al., 2012) focus on extracting binary assertions and suffer from three key drawbacks. First, lack of expressivity of representation leads to significant information loss for higher-order relations and complex assertions. This results in incomplete, uniformative and incoherent prepositions. Consider Example 1 in Figure 1. Important contextual information is either ignored or is subsumed in over-specified argument and relation phrases. It is not possible to fix such nuances by post-processing the propositions. This affects downstream applications like Question Answering ("
D16-1006,P11-1055,0,0.0352489,"cting the seed set. Example 1 Consider a statement-hypothesis pair, Statement: Paul Bremer, the top U.S. civilian administrator in Iraq, and Iraq’s new president, Ghazi alYawar, visited the northern Iraqi city of Kirkuk. Hypothesis: Ghazi al-Yawar is the president of Iraq. The hypothesis is entailed in the statement. The seed templates extract propositions from the hypothesis: (al-Yawar,is,president, (al-Yawar,is,president of Iraq), and (al-Yawar,is president of,Iraq). Bootstrapping is a popular technique to generate positive training data for information extraction (Collins and Singer, 1999; Hoffmann et al., 2011). We extend the bootstrapping techniques employed in O LLIE and R E N OUN, for n-ary and complex relations. First, instead of learning dependency parsetree patterns connecting the heads of the argument phrases and the relation phrase connecting them, we learn the dependency parse-tree patterns connecting the heads of all argument and relation phrases in the template. This allows greater coverage of context for the propositions and prevents the arguments/relations from being over-specified and/or uninformative. Second, some of the relations in the representation are derived from the type of dep"
D16-1006,kingsbury-palmer-2002-treebank,0,0.0389778,"es, it doesn’t canonicalize the fact phrases. There is another body of work in natural language understanding that shares tasks with OIE. AMR parsing (Banarescu et al., ), semantic role labeling (SRL) (Toutanova et al., 2008; Punyakanok et al., 2008) and frame-semantic parsing (Das et al., 2014). In these tasks, verbs or nouns are analyzed to identify their arguments. The verb or noun is then mapped to a semantic frame and roles of each argument in the frame are identified. These techniques have gained interest with the advent of hand-constructed semantic resources like PropBank and FrameNet (Kingsbury and Palmer, 2002; Baker et al., 1998). Generally, the verb/noun and the semantically labeled arguments correspond to OIE propositions and, therefore, the two tasks are considered similar. Systems like SRL-IE (Christensen et al., 2010) explore if these techniques can be used for OIE. However, while OIE aims to identify the relation/predicate between a pair of arguments, frame-based techniques aim to identify arguments and their roles with respect to a predicate. Hence, the frames won’t correspond to propositions when both the arguments cannot be identified for a binary relation or when the correct argument is"
D16-1006,P15-1036,0,0.0172066,"rn extraction patterns. Bootstrapping and pattern learning has a long history in traditional information extraction. Systems like DIPRE (Brin, 1998), S NOWBALL (Agichtein and Gravano, 2000), NELL (Mitchell, 2010), and O LLIE bootstrap based on seed instances of a relation and then learn patterns for extraction. We follow a similar bootstrapping algorithm to learn extraction patterns for n-ary and nested propositions. Using a nested representation to express complex and n-ary assertions has been studied in closeddomain or ontology-aided information extraction. Yago (Suchanek et al., 2008) and (Nakashole and Mitchell, 2015) extend binary relations to capture temporal, geospatial and prepositional context information. We study such a representation for opendomain information extraction. 7 Conclusions We presented N EST IE, a novel open information extractor that uses nested representation for expressing complex propositions and inter-propositional relations. It extends the bootstrapping techniques of previous approaches to learn syntactic extraction pat63 terns for the nested representation. This allows it to obtain higher informativeness and minimality scores for extractions at comparable precision. It produces"
D16-1006,J08-2005,0,0.0191597,"learning extraction patterns. Also, it doesn’t capture the relations between the clauses. There has been some work in open-domain information extraction to extract higher-order relations. KRAKEN (Akbik and L¨oser, 2012) uses a predefined set of rules based on dependency parse to identify fact phrases and argument heads within fact phrases. But unlike alternative approaches, it doesn’t canonicalize the fact phrases. There is another body of work in natural language understanding that shares tasks with OIE. AMR parsing (Banarescu et al., ), semantic role labeling (SRL) (Toutanova et al., 2008; Punyakanok et al., 2008) and frame-semantic parsing (Das et al., 2014). In these tasks, verbs or nouns are analyzed to identify their arguments. The verb or noun is then mapped to a semantic frame and roles of each argument in the frame are identified. These techniques have gained interest with the advent of hand-constructed semantic resources like PropBank and FrameNet (Kingsbury and Palmer, 2002; Baker et al., 1998). Generally, the verb/noun and the semantically labeled arguments correspond to OIE propositions and, therefore, the two tasks are considered similar. Systems like SRL-IE (Christensen et al., 2010) explo"
D16-1006,D12-1048,0,0.869984,"interdependent assertions. Nesting the extracted propositions allows N EST IE to more accurately reflect the meaning of the original sentence. Our experimental study on real-world datasets suggests that N EST IE obtains comparable precision with better minimality and informativeness than existing approaches. N EST IE produces 1.7-1.8 times more minimal extractions and achieves 1.1-1.2 times higher informativeness than C LAUS IE. 1 Dragomir Radev Department of EECS University of Michigan Ann Arbor radev@umich.edu However, state-of-the-art OIE systems, R E V ERB (Fader et al., 2011) and O LLIE (Schmitz et al., 2012) focus on extracting binary assertions and suffer from three key drawbacks. First, lack of expressivity of representation leads to significant information loss for higher-order relations and complex assertions. This results in incomplete, uniformative and incoherent prepositions. Consider Example 1 in Figure 1. Important contextual information is either ignored or is subsumed in over-specified argument and relation phrases. It is not possible to fix such nuances by post-processing the propositions. This affects downstream applications like Question Answering (Fader et al., 2014) which rely on"
D16-1006,J08-2002,0,0.0253963,"ct assertions instead of learning extraction patterns. Also, it doesn’t capture the relations between the clauses. There has been some work in open-domain information extraction to extract higher-order relations. KRAKEN (Akbik and L¨oser, 2012) uses a predefined set of rules based on dependency parse to identify fact phrases and argument heads within fact phrases. But unlike alternative approaches, it doesn’t canonicalize the fact phrases. There is another body of work in natural language understanding that shares tasks with OIE. AMR parsing (Banarescu et al., ), semantic role labeling (SRL) (Toutanova et al., 2008; Punyakanok et al., 2008) and frame-semantic parsing (Das et al., 2014). In these tasks, verbs or nouns are analyzed to identify their arguments. The verb or noun is then mapped to a semantic frame and roles of each argument in the frame are identified. These techniques have gained interest with the advent of hand-constructed semantic resources like PropBank and FrameNet (Kingsbury and Palmer, 2002; Baker et al., 1998). Generally, the verb/noun and the semantically labeled arguments correspond to OIE propositions and, therefore, the two tasks are considered similar. Systems like SRL-IE (Chris"
D16-1006,P10-1013,0,0.0550337,"patterns for nested representations are learned in Sec. 4. In Sec. 5, we compare N EST IE against alternative methods on two datasets: Wikipedia and News. In Sec. 6, we discuss related work on pattern-based information extraction. 2 Background The key goal of OIE is to obtain a shallow semantic representation of the text in the form of tuples consisting of argument phrases and a phrase that expresses the relation between the arguments. The phrases are identified automatically using domainindependent syntactic and lexical constraints. Some OIE systems are: TextRunner (Yates et al., 2007) WOE (Wu and Weld, 2010): They use a sequence-labeling graphical model on extractions labeled automatically using heuristics or distant supervision. Consequently, long-range dependencies, holistic and lexical aspects of relations tend to get ignored. ReVerb (Fader et al., 2011): Trained with shallow syntactic features, R E V ERB uses a logistic regression classifier to extract relations that begin with a verb and occur between argument phrases. Ollie (Schmitz et al., 2012): Bootstrapping from R E V ERB extractions, O LLIE learns syntactic and lexical dependency parse-tree patterns for extraction. Some patterns reduce"
D16-1006,C98-1013,0,\N,Missing
D16-1006,J14-1002,0,\N,Missing
D18-1193,Q13-1005,0,0.076636,"Missing"
D18-1193,W13-2322,0,0.0317059,"Missing"
D18-1193,P14-1133,0,0.030183,"edicts NONE instead, it will be pushed into the stack. The stack pops NONE at next step. For example, in Figure 2, the current popped token is SELECT, which is a instance of keyword (KW) type. It calls the COL module to predict a column name, which will be pushed to the stack. 4.6 Data Augmentation Even though Spider already has a significantly larger number of complex queries than existing datasets, the number of training examples for some complex SQL components is still limited. A widely used way is to conduct data augmentation to generate more training examples automatically. Many studies (Berant and Liang, 2014; Iyer et al., 2017; Su and Yan, 2017) have shown that data augmentation can bring significant improvement in performance. In prior work, data augmentation was typically performed within a single domain dataset. We propose a cross-domain data augmentation method to expand our training data for complex queries. Cross-domain data augmentation is more difficult than the in-domain setting because question-program pairs tend to have domain specific words and phrases. To tackle this issue, we first create a list of universal patterns for questionSQL pairs, based on the human labeled pairs from all t"
D18-1193,N10-1138,0,0.0462004,"Missing"
D18-1193,P16-1004,0,0.137291,"Berant and Liang, 2014; Pasupat and Liang, 2015). As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003a, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods proposed in the database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) tend to involve hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network-based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequence-to-sequence (seq2seq) approach to converting texts to logical forms. Most previous work focuses on a specific table schema. Zhong et al. (2017) publish the WikiSQL dataset and propose a seq2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employing a sequence-to-set model. Dong and Lapata (2018) propose a coarse-to-fine model which achieves the new state-of-the-art performances on several datasets including WikiSQL. Their model first generate a sketch of the"
D18-1193,P18-1068,0,0.531592,"ems. In this work, we focus on recent neural network-based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequence-to-sequence (seq2seq) approach to converting texts to logical forms. Most previous work focuses on a specific table schema. Zhong et al. (2017) publish the WikiSQL dataset and propose a seq2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employing a sequence-to-set model. Dong and Lapata (2018) propose a coarse-to-fine model which achieves the new state-of-the-art performances on several datasets including WikiSQL. Their model first generate a sketch of the target program. Then the model fills in missing details in the sketch. Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017; Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rule"
D18-1193,C12-2040,0,0.0386433,"th the previous best models. 2 Related Work Semantic parsing maps natural language to formal meaning representations. There are a range of representations, such as logic forms and executable programs (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003a, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods proposed in the database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) tend to involve hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network-based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequence-to-sequence (seq2seq) approach to converting texts to logical forms. Most previous work focuses on a specific table schema. Zhong et al. (2017) publish the WikiSQL dataset and propose a seq2seq"
D18-1193,P17-1089,0,0.406209,"Missing"
D18-1193,P17-1105,0,0.171224,"q2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employing a sequence-to-set model. Dong and Lapata (2018) propose a coarse-to-fine model which achieves the new state-of-the-art performances on several datasets including WikiSQL. Their model first generate a sketch of the target program. Then the model fills in missing details in the sketch. Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017; Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rules and tokens, our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decoding. Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based models have to predict many"
D18-1193,Q14-1030,0,0.0397021,"Missing"
D18-1193,D17-1127,0,0.0638207,"the stack. The stack pops NONE at next step. For example, in Figure 2, the current popped token is SELECT, which is a instance of keyword (KW) type. It calls the COL module to predict a column name, which will be pushed to the stack. 4.6 Data Augmentation Even though Spider already has a significantly larger number of complex queries than existing datasets, the number of training examples for some complex SQL components is still limited. A widely used way is to conduct data augmentation to generate more training examples automatically. Many studies (Berant and Liang, 2014; Iyer et al., 2017; Su and Yan, 2017) have shown that data augmentation can bring significant improvement in performance. In prior work, data augmentation was typically performed within a single domain dataset. We propose a cross-domain data augmentation method to expand our training data for complex queries. Cross-domain data augmentation is more difficult than the in-domain setting because question-program pairs tend to have domain specific words and phrases. To tackle this issue, we first create a list of universal patterns for questionSQL pairs, based on the human labeled pairs from all the different training databases in Spi"
D18-1193,P11-1060,0,0.113297,"Missing"
D18-1193,D15-1166,0,0.0322332,"ral previous state-of-the-art models in the text-to-SQL task. As the dataset and task definition used in this work are fundamentally different from prior work using datasets such as GeoQuery, WikiSQL, we adapted these models to our task in the same way as (Yu et al., 2018b). Specifically: Seq2Seq with Attention or Copying In order to make the models aware of the table schema information, Yu et al. (2018b) pass the models with a vocabulary that contains SQL keywords and column names of the given database. (Iyer et al., 2017) Iyer et al. (2017) apply an attention based seq2seq model similar to (Luong et al., 2015) to text-to-SQL tasks. Yu et al. (2018b) adapt their model without user interaction to the task. SQLNet & TypeSQL Xu et al. (2017) introduce SQLNet, which employs a column attention mechanism and a sketch-based method to generates SQL queries as a slot-filling task. Yu et al. (2018a) improves SQLNet by utilizing word types extracted from a knowledge graph or table content 1659 Method Seq2Seq Seq2Seq+Attention (Dong and Lapata, 2016) Seq2Seq+Copying Iyer et al. (2017) SQLNet (Xu et al., 2017) TypeSQL (Yu et al., 2018a) SyntaxSQLNet -augment -wikiSQL -augment -table -wikiSQL -augment -history -t"
D18-1193,P15-1142,0,0.255475,"Missing"
D18-1193,D14-1162,0,0.081044,"ve 5 clauses. Exact matching score is 1 if the model predicts all clauses correctly for a given example. To better understand model performance on different queries, (Yu et al., 2018b) divide SQL queries into 4 levels: easy, medium, hard, extra hard. The definition of difficulty is based on the number of SQL components, selections, and conditions. Queries that contain more SQL keywords are considered harder. 5.2 Experimental Settings Our model is implemented in PyTorch (Paszke et al., 2017). We build each module based on the TypeSQL (Yu et al., 2018a) implementation. We use pre-trained GloVe (Pennington et al., 2014) embeddings for all question, SQL history, and schema tokens. All word embeddings are fixed. For each experiment, the dimension and dropout rate of all hidden layers is set to 120 and 0.3 respectively. We use Adam (Kingma and Ba, 2015) with the default hyperparameters for optimization, with a batch size of 64. The same loss functions in (Xu et al., 2017) are used. • Training data: Spider (plus examples from 6 existing datasets) + WikiSQL + data augmentation • Model architecture: history path + tableaware column encoding We will conduct ablation studies to analyze the effect of each of the prop"
D18-1193,C04-1021,0,0.849862,"Missing"
D18-1193,J82-3002,0,0.499483,"Missing"
D18-1193,P07-1121,0,0.200758,"Missing"
D18-1193,W16-0105,0,0.0488845,"; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003a, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods proposed in the database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) tend to involve hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network-based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequence-to-sequence (seq2seq) approach to converting texts to logical forms. Most previous work focuses on a specific table schema. Zhong et al. (2017) publish the WikiSQL dataset and propose a seq2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employing a sequence-to-set model. Dong and Lapata (2018) propose a coarse-to-fine model which achieves the new state-of-t"
D18-1193,P17-1041,0,0.186096,"taset and propose a seq2seq model with reinforcement learning to generate SQL queries. Xu et al. (2017) further improve the results on the WikiSQL task by using a SQL-sketch based approach employing a sequence-to-set model. Dong and Lapata (2018) propose a coarse-to-fine model which achieves the new state-of-the-art performances on several datasets including WikiSQL. Their model first generate a sketch of the target program. Then the model fills in missing details in the sketch. Our syntax tree-based decoder is related to recent work that exploits syntax information for code generation tasks (Yin and Neubig, 2017; Rabinovich et al., 2017). Yin and Neubig (2017) introduce a neural model that transduces a natural language statement into an abstract syntax tree (AST). While they format the generation process as a seq2seq decoding of rules and tokens, our model uses a module for each grammar component, and calls them recursively to generate a SQL syntax tree. Similarly, Rabinovich et al. (2017) propose abstract syntax networks that use a collection of recursive modules for decoding. Our model differs from theirs in the following points. First, we exploit a SQL specific grammar instead of AST. AST-based mo"
D18-1193,N18-2093,1,0.222984,"split (Zettlemoyer and Collins, 2005), most queries in the test set also appear in the train set. The WikiSQL dataset recently developed by (Zhong et al., 2017) is much larger and does use different databases for training and testing, but it only contains very simple SQL queries and 1653 Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 1653–1663 c Brussels, Belgium, October 31 - November 4, 2018. 2018 Association for Computational Linguistics OP ROOT Action Modules database schemas. To address those issues in the current semantic parsing datasets, Yu et al. (2018b) have developed a large-scale human labeled text-to-SQL dataset consisting of about 6,000 complex SQL queries and 200 databases with multiple tables. This dataset defines a new complex and cross-domain text-to-SQL task that requires models to generalize well to both new SQL queries and databases. The task cannot be solved easily without truly understanding the semantic meanings of the input questions. In this paper, we propose SyntaxSQLNet, a SQL specific syntax tree network to address the aforementioned task. Specifically, to generate complex SQL queries with multiple clauses, selections an"
D18-1425,Q13-1005,0,0.0373234,"oreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2"
D18-1425,W13-2322,0,0.031249,"ins 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advis"
D18-1425,P14-1133,0,0.0534807,"x SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). Thes"
D18-1425,H94-1010,0,0.896485,"ry given the input question, models need to understand both the natural language question and relationships between tables and columns in the database schema. In addition, we also propose a new task for text-to-SQL problem. Since Spider contains 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATI"
D18-1425,N10-1138,0,0.0214231,"ext-to-SQL problem. Since Spider contains 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp"
D18-1425,P16-1004,0,0.159092,"hat no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). These datasets have been stu"
D18-1425,P18-1068,0,0.473951,"ets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). These datasets have been studied for decades in both the NLP community (Warren and Pereira, 1982; Popescu et al., 2003b, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017; Iyer et al., 2017; Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018; Huang et al., 2018; Wang et al., 2018; Dong and Lapata, 2018; McCann et al., 2018) and the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017b). We provide detailed statistics on these datasets in Table 1. Most of the previous work train their models without schemas as inputs because they use a sin3912 SQL Review 150 man-hours gle database for both training and testing. Thus, they do not need to generalize to new domains. Most importantly, these datasets have a limited number of labeled logic forms or SQL queries. In order to expand the size of these datasets and apply neural network approaches, each logic form or SQL query has about 4"
D18-1425,P11-1060,0,0.0682285,". Since Spider contains 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazad"
D18-1425,L18-1491,0,0.0935667,"Missing"
D18-1425,P16-1057,0,0.118169,"Missing"
D18-1425,D15-1166,0,0.0429896,"Missing"
D18-1425,C12-2040,0,0.0602249,"semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). These datasets have been studied for decades in both the NLP community (Warren and Pereira, 1982; Popescu et al., 2003b, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017; Iyer et al., 2017; Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018; Huang et al., 2018; Wang et al., 2018; Dong and Lapata, 2018; McCann et al., 2018) and the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017b). We provide detailed statistics on these datasets in Table 1. Most of the previous work train their models without schemas as inputs because they use a sin3912 SQL Review 150 man-hours gle database for both training and testing. Thus, they do not need to generalize to new domains. Most importantly, these datasets have a limited number of label"
D18-1425,C04-1021,0,0.615026,"Missing"
D18-1425,P17-1089,0,0.429303,"Missing"
D18-1425,P16-1002,0,0.0994049,"Missing"
D18-1425,H90-1020,0,0.075333,"e the SQL query given the input question, models need to understand both the natural language question and relationships between tables and columns in the database schema. In addition, we also propose a new task for text-to-SQL problem. Since Spider contains 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled"
D18-1425,P15-1085,0,0.0422304,"opment sets. Also, the task needs to take different table schemas as inputs. Therefore, the model has to generalize to new databases. However, in order to generate about 90,000 questions and SQL pairs for about 26,000 databases, Zhong et al. (2017) made simplified assumptions about the SQL queries and databases. Their SQL labels only cover single SELECT column and aggregation, and WHERE conditions. Moreover, all the databases only contain single tables. No JOIN, GROUP BY, and ORDER BY, etc. are included. Recently, researchers have constructed some datasets for code generation including IFTTT (Quirk et al., 2015), DJANGO (Oda et al., 2015), HEARTHSTONE (Ling et al., 2016), NL2Bash (Lin et al., 2018), and CoNaLa (Yin et al., 2018). Database Collection & Creation 200 databases (DB) 150 man-hours SQL Review 150 man-hours Question Review and Paraphrase 150 man-hours Question and SQL Annotation 20-50 examples per DB 500 man-hours Question Review & Paraphrase 150 man-hours Final Review & Processing 150 man-hours Figure 2: The annotation process of our Spider corpus. These tasks parse natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling e"
D18-1425,P17-1105,0,0.0897459,"et al., 2015), HEARTHSTONE (Ling et al., 2016), NL2Bash (Lin et al., 2018), and CoNaLa (Yin et al., 2018). Database Collection & Creation 200 databases (DB) 150 man-hours SQL Review 150 man-hours Question Review and Paraphrase 150 man-hours Question and SQL Annotation 20-50 examples per DB 500 man-hours Question Review & Paraphrase 150 man-hours Final Review & Processing 150 man-hours Figure 2: The annotation process of our Spider corpus. These tasks parse natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). 3 Corpus Construction All questions and SQL queries were written and reviewed by 11 computer science students who were native English speakers.As illustrated in Figure 2, we develop our dataset in five steps, spending around 1,000 hours of human labor in total: §3.1 Database Collection and Creation, §3.2 Question and SQL Annotation, §3.3 SQL Review, §3.4 Question Review and Paraphrase, §3.5 Final Question and SQL Review. 3.1 Database Collection and Creation Collecting databases with complex schemas is hard. Although relational databases are widely used in industry and"
D18-1425,Q14-1030,0,0.0204325,"dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Z"
D18-1425,P17-1041,0,0.248117,"NE (Ling et al., 2016), NL2Bash (Lin et al., 2018), and CoNaLa (Yin et al., 2018). Database Collection & Creation 200 databases (DB) 150 man-hours SQL Review 150 man-hours Question Review and Paraphrase 150 man-hours Question and SQL Annotation 20-50 examples per DB 500 man-hours Question Review & Paraphrase 150 man-hours Final Review & Processing 150 man-hours Figure 2: The annotation process of our Spider corpus. These tasks parse natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). 3 Corpus Construction All questions and SQL queries were written and reviewed by 11 computer science students who were native English speakers.As illustrated in Figure 2, we develop our dataset in five steps, spending around 1,000 hours of human labor in total: §3.1 Database Collection and Creation, §3.2 Question and SQL Annotation, §3.3 SQL Review, §3.4 Question Review and Paraphrase, §3.5 Final Question and SQL Review. 3.1 Database Collection and Creation Collecting databases with complex schemas is hard. Although relational databases are widely used in industry and academia, most of them"
D18-1425,N18-2093,1,0.771968,"(2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). These datasets have been studied for decades in both the NLP community (Warren and Pereira, 1982; Popescu et al., 2003b, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017; Iyer et al., 2017; Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018; Huang et al., 2018; Wang et al., 2018; Dong and Lapata, 2018; McCann et al., 2018) and the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017b). We provide detailed statistics on these datasets in Table 1. Most of the previous work train their models without schemas as inputs because they use a sin3912 SQL Review 150 man-hours gle database for both training and testing. Thus, they do not need to generalize to new domains. Most importantly, these datasets have a limited number of labeled logic forms or SQL queries. In order to expand the size of these datasets and apply neura"
D18-1425,J82-3002,0,0.411092,"e is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jagadish, 2014), Yelp and IMDB (Yaghmazadeh et al., 2017b), Advising (Finegan-Dollak et al., 2018), and WikiSQL (Zhong et al., 2017). These datasets have been studied for decades in both the NLP community (Warren and Pereira, 1982; Popescu et al., 2003b, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017; Iyer et al., 2017; Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018; Huang et al., 2018; Wang et al., 2018; Dong and Lapata, 2018; McCann et al., 2018) and the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017b). We provide detailed statistics on these datasets in Table 1. Most of the previous work train their models without schemas as inputs because they use a sin3912 SQL Review 150 man-hours gle database for both training and testing. Thus, they do not need to generalize to"
D18-1425,P07-1121,0,0.0807639,"ropose a new task for text-to-SQL problem. Since Spider contains 200 databases with foreign keys, we can split the dataset with complex SQL queries in a way that no database overlaps in train and test, which overRelated Work and Existing Datasets Several semantic parsing datasets with different queries have been created. The output can be in many formats, e.g., logic forms. These datasets include ATIS (Price, 1990; Dahl et al., 1994), GeoQuery (Zelle and Mooney, 1996), and JOBS (Tang and Mooney, 2001a). They have been studied extensively (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Dong and Lapata, 2016). However, they are domain specific and there is no standard label guidance for multiple SQL queries. Recently, more semantic parsing datasets using SQL as programs have been created. Iyer et al. (2017) and Popescu et al. (2003a) labeled SQL queries for ATIS and GeoQuery datasets. Other existing text-to-SQL datasets also include Restaurants (Tang and Mooney, 2001b; Popescu et al., 2003a), Scholar (Iyer et al., 2017), Academic (Li and Jag"
D19-1537,W10-2903,0,0.0605657,"lumn) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-drive"
D19-1537,H94-1010,0,0.712423,"xample. match accuracy over the previous state-of-the-art. Further analysis shows that our editing approach is more robust to error propagation than copying segments, and the improvement becomes more significant if the basic text-to-SQL generation accuracy (without editing) improves. 2 Cross-Domain Context-Depencent Semantic Parsing 2.1 Datasets We use SParC 1 (Yu et al., 2019b), a large-scale cross-domain context-dependent semantic parsing dataset with SQL labels, as our main evaluation benchmark. A SParC example is shown in Table 3. We also report performance on ATIS (Hemphill et al., 1990; Dahl et al., 1994a) for direct comparison to Suhr et al. (2018). In addition, we evaluate the cross-domain context-independent text-toSQL ability of our model on Spider2 (Yu et al., 1 2 https://yale-lily.github.io/sparc https://yale-lily.github.io/spider 2018c), which SParC is built on. We summarize and compare the data statistics in Table 1 and Table 2. While the ATIS dataset has been extensively studied, it is limited to a particular domain. By contrast, SParC is both context-dependent and cross-domain. Each interaction in SParC is constructed using a question in Spider as the interaction goal, where the ann"
D19-1537,N19-1423,0,0.0255756,"of user utterance and column headers. dorms have a TV louge (b) Utterance Encoder. Bi LSTM Concatennation Attention over Utterance Tokens Self-Attention Among Table Columns Bi LSTM Bi LSTM Bi LSTM Bi LSTM Bi LSTM dorm . id dorm . name has . dorm id has . amenity id amenity Bi LSTM . id amenity . name (c) Table Encoder. Figure 2: Utterance-Table Encoder for the example in (a). Utterance-Table BERT Embedding. We consider two options as the input to the first layer biLSTM. The first choice is the pretrained word embedding. Second, we also consider the contextualized word embedding based on BERT (Devlin et al., 2019). To be specific, we follow Hwang et al. (2019) to concatenate the user utterance and all the column headers in a single sequence separated by the [SEP] token: [CLS], Xi , [SEP], c1 , [SEP], . . . , cm , [SEP] This sequence is fed into the pretrained BERT model whose hidden states at the last layer is used as the input embedding. 3.2 The hidden state of this interaction encoder hI encodes the history as the interaction proceeds. Turn Attention When issuing the current utterance, the user may omit or explicitly refer to the previously mentioned information. To this end, we adopt the turn attent"
D19-1537,P16-1004,0,0.0241913,"2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Frie"
D19-1537,P18-1068,0,0.1126,"meaning of user utterances, the structure of table schema, and the relationship between the two. To this end, we build an utterance-table encoder with co-attention between the two as illustrated in Figure 2. Figure 2b shows the utterance encoder. For the user utterance at each turn, we first use a bi-LSTM to encode utterance tokens. The bi-LSTM hidden state is fed into a dot-product attention layer (Luong et al., 2015) over the column header embeddings. For each utterance token embedding, we get an attention weighted average of the column header embeddings to obtain the most relevant columns (Dong and Lapata, 2018). We then concatenate the bi-LSTM hidden state and the column attention vector, and use a second layer biLSTM to generate the utterance token embedding hE . Figure 2c shows the table encoder. For each column header, we concatenate its table name and its column name separated by a special dot token (i.e., table name . column name). Each column header is processed by a bi-LSTM layer. To better capture the internal structure of the table schemas (e.g., foreign key), we then employ a selfattention (Vaswani et al., 2017) among all column headers. We then use an attention layer to capture the relati"
D19-1537,P18-1033,1,0.853357,"L queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et a"
D19-1537,N18-1177,0,0.0515028,"Missing"
D19-1537,P16-1154,0,0.0390467,", we predict a switch pcopy to decide if we need copy from the previous query or insert a new token. pcopy = σ(ck Wcopy + bcopy ) pinsert = 1 − pcopy (5) Then, we use a separate layer to score the query tokens at turn t − 1, and the output distribution is modified as the following to take into account the editing probability: Pprev SQL = softmax(ok Wprev SQL hQ t−1 ) mSQL = ok WSQL + bSQL mcolumn = ok Wcolumn hC PSQL S column = softmax([mSQL ; mcolumn ]) P (yk ) = pcopy · Pprev SQL (yk ∈ prev SQL) [ +pinsert · PSQL S column (yk ∈ SQL column) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; A"
D19-1537,D18-1188,0,0.0130342,"r et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to"
D19-1537,P19-1444,0,0.46871,"Missing"
D19-1537,P18-1124,0,0.0623869,"Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) m"
D19-1537,P17-1097,0,0.0224668,"(Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an i"
D19-1537,H90-1021,0,0.51482,"unge’) Table 3: SParC example. match accuracy over the previous state-of-the-art. Further analysis shows that our editing approach is more robust to error propagation than copying segments, and the improvement becomes more significant if the basic text-to-SQL generation accuracy (without editing) improves. 2 Cross-Domain Context-Depencent Semantic Parsing 2.1 Datasets We use SParC 1 (Yu et al., 2019b), a large-scale cross-domain context-dependent semantic parsing dataset with SQL labels, as our main evaluation benchmark. A SParC example is shown in Table 3. We also report performance on ATIS (Hemphill et al., 1990; Dahl et al., 1994a) for direct comparison to Suhr et al. (2018). In addition, we evaluate the cross-domain context-independent text-toSQL ability of our model on Spider2 (Yu et al., 1 2 https://yale-lily.github.io/sparc https://yale-lily.github.io/spider 2018c), which SParC is built on. We summarize and compare the data statistics in Table 1 and Table 2. While the ATIS dataset has been extensively studied, it is limited to a particular domain. By contrast, SParC is both context-dependent and cross-domain. Each interaction in SParC is constructed using a question in Spider as the interaction"
D19-1537,N18-2115,0,0.0394036,"017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) an"
D19-1537,P17-1089,0,0.0436623,"sing executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018;"
D19-1537,D18-1192,0,0.0123607,"while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 201"
D19-1537,P17-1167,0,0.0294541,"al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE"
D19-1537,D19-1624,0,0.0439311,"r which takes bag-of-words representations of column headers as input. They also modify the decoder to select between a SQL keyword or a column header. (2) SyntaxSQL-con: This is adapted from the original context-agnostic SyntaxSQLNet (Yu et al., 2018b) by using bi-LSTMs to encode the interaction history including the utterance and the associated SQL query response. It also employs a column attention mechanism to compute representations of the previous question and SQL query. Spider. We compare with the results as reported in Yu et al. (2018b). Furthermore, we also include recent results from Lee (2019) who propose to use recursive decoding procedure, Bogin SQLNet (Xu et al., 2017) SyntaxSQLNet (Yu et al., 2018b) +data augmentation (Yu et al., 2018b) Lee (2019) GNN (Bogin et al., 2019) IRNet (Guo et al., 2019) IRNet (BERT) (Guo et al., 2019) Ours + utterance-table BERT Embedding Dev Set 10.9 18.9 24.8 28.5 40.7 53.2 61.9 36.4 57.6 Test Set 12.4 19.7 27.2 24.3 39.4 46.7 54.7 32.9 53.4 Table 4: Spider results on dev set and test set. et al. (2019) introducing graph neural networks for encoding schemas, and Guo et al. (2019) who achieve state-of-the-art performance by using an intermediate repr"
D19-1537,P16-1138,0,0.0348913,"promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating"
D19-1537,D15-1166,0,0.0400847,"), (X2 , Y2 ), . . . , (Xt−1 , Yt−1 )] Furthermore, in the cross-domain setting, each interaction is grounded to a different database. Therefore, the model is also given the schema of the current database as an input. We consider relational databases with multiple tables, and each table contains multiple column headers: T = [c1 , c2 , . . . , cl , . . . , cm ] where m is the number of column headers, and each cl consists of multiple words including its table name and column name (§ 3.1). 3 Methodology We employ an encoder-decoder architecture with attention mechanisms (Sutskever et al., 2014; Luong et al., 2015) as illustrated in Figure 1. The framework consists of (1) an utterance-table encoder to explicitly encode the user utterance and table schema at each turn, (2) A turn attention incorporating the recent history for decoding, (3) a table-aware decoder taking into account the context of the utterance, the table schema, and the previously generated query to make editing decisions. 3.1 Utterance-Table Encoder An effective encoder captures the meaning of user utterances, the structure of table schema, and the relationship between the two. To this end, we build an utterance-table encoder with co-att"
D19-1537,P96-1008,0,0.628325,"ations by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Y"
D19-1537,P15-1142,0,0.0267292,"ith context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE contains three domains using stack- or list-like elements and most queries include a single binary predicate. SequentialQA is created by decomposing some complicated questions in WikiTableQuestions (Pasupat and Liang, 2015). Since both SCONE and SequentialQA are annotated with only denotations but not query labels, they don’t include many questions with rich semantic and contextual types. For example, SequentialQA (Iyyer et al., 2017) requires that the answer to follow-up questions must be a subset of previous answers, and most of the questions can be answered by simple SQL queries with SELECT and WHERE clauses. Concurrent with our work, Yu et al. (2019a) introduced CoSQL, a large-scale cross-domain conversational text-to-SQL corpus collected under the Wizard-of-Oz setting. Each dialogue in CoSQL simulates a DB"
D19-1537,D17-1127,0,0.0360468,"oyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Su"
D19-1537,P18-1193,0,0.0113637,"17; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segme"
D19-1537,N18-1203,0,0.0305507,"Missing"
D19-1537,P18-1034,0,0.0408402,"languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Mill"
D19-1537,P15-1129,0,0.0350818,"tences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions int"
D19-1537,D14-1162,1,0.106791,"(Guo et al., 2019) IRNet (BERT) (Guo et al., 2019) Ours + utterance-table BERT Embedding Dev Set 10.9 18.9 24.8 28.5 40.7 53.2 61.9 36.4 57.6 Test Set 12.4 19.7 27.2 24.3 39.4 46.7 54.7 32.9 53.4 Table 4: Spider results on dev set and test set. et al. (2019) introducing graph neural networks for encoding schemas, and Guo et al. (2019) who achieve state-of-the-art performance by using an intermediate representation to bridge natural language questions and SQL queries. 5.3 Implementation Details Our model is implemented in PyTorch (Paszke et al., 2017). We use pretrained 300-dimensional GloVe (Pennington et al., 2014) word embedding. All LSTM layers have 300 hidden size, and we use 1 layer for encoder LSTMs, and 2 layers for decoder LSTMs. We use the ADAM optimizer (Kingma and Ba, 2015) to minimize the tokenlevel cross-entropy loss with a batch size of 16. Model parameters are randomly initialized from a uniform distribution U [−0.1, 0.1]. The main model has an initial learning rate of 0.001 and it will be multiplied by 0.8 if the validation loss increases compared with the previous epoch. When using BERT instead of GloVe, we use the pretrained small uncased BERT model with 768 hidden size5 , and we fine t"
D19-1537,P17-1099,0,0.0417505,"pcopy to decide if we need copy from the previous query or insert a new token. pcopy = σ(ck Wcopy + bcopy ) pinsert = 1 − pcopy (5) Then, we use a separate layer to score the query tokens at turn t − 1, and the output distribution is modified as the following to take into account the editing probability: Pprev SQL = softmax(ok Wprev SQL hQ t−1 ) mSQL = ok WSQL + bSQL mcolumn = ok Wcolumn hC PSQL S column = softmax([mSQL ; mcolumn ]) P (yk ) = pcopy · Pprev SQL (yk ∈ prev SQL) [ +pinsert · PSQL S column (yk ∈ SQL column) (6) While the copy mechanism has been introduced by Gu et al. (2016) and See et al. (2017), they focus on summarization or response generation applications by copying from the source sentences. By contrast, our focus is on editing the previously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer,"
D19-1537,D18-1197,0,0.0120536,"ost of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames whic"
D19-1537,P17-1041,0,0.060978,"iously generated query while incorporating the context of user utterances and table schemas. 4 Related Work Semantic parsing is the task of mapping natural language sentences into formal representations. It has been studied for decades including using linguistically-motivated compositional representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2"
D19-1537,N18-2093,1,0.932754,"representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets includin"
D19-1537,P09-1110,0,0.0185736,"still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets including ATIS (Hemphill et al., 1990; Dahl et al., 1994b), SpaceBook (Vlachos and Clark, 2014), SCONE (Long et al., 2016; Guu et al., 2017; Fried et al., 2018; Suhr and Artzi, 2018; Huang et al., 2019), SequentialQA (Iyyer et al., 2017), SParC (Yu et al., 2019b) and CoSQL (Yu et al., 2019a). On ATIS, Miller et al. (1996) maps utterances to semantic frames which are then mapped to SQL queries; Zettlemoyer and Collins (2009) starts with context-independent Combinatory Categorial Grammar (CCG) parsing and then resolves references to generate lambda-calculus logical forms for sequences of sentences. The most relevant to our work is Suhr et al. (2018), who generate ATIS SQL queries from interactions by incorporating history with an interaction-level encoder and copying segments of previously generated queries. Furthermore, SCONE contains three domains using stack- or list-like elements and most queries include a single binary predicate. SequentialQA is created by decomposing some complicated questions in WikiTableQu"
D19-1537,D18-1193,1,0.950706,"representations, such as logical forms (Zelle and Mooney, 1996; Clarke et al., 2010) and lambda calculus (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2011), and using executable programs, such as SQL queries (Miller et al., 1996; Zhong et al., 2017) and other general-purpose programming languages (Yin and Neubig, 2017; Iyer et al., 2018). Most of the early studies worked on a few domains and small datasets such as GeoQuery (Zelle and Mooney, 1996) and Overnight (Wang et al., 2015). Recently, large and cross-domain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c) have received an increasing amount of attention as many data-driven neural approaches achieve promising results (Dong and Lapata, 2016; Su and Yan, 2017; Iyer et al., 2017; Xu et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a; Huang et al., 2018; Dong and Lapata, 2018; Sun et al., 2018; Gur et al., 2018; Guo et al., 2018; Yavuz et al., 2018; Shi et al., 2018). Most of them still focus on context-independent semantic parsing by converting single-turn questions into executable queries. Relatively less effort has been devoted to context-dependent semantic parsing on datasets includin"
D19-1537,D18-1425,1,0.89856,"Missing"
D19-1537,P19-1443,1,0.875363,"Missing"
H01-1006,P99-1020,0,0.0186725,"Missing"
H01-1006,A00-1021,1,0.690357,"Missing"
H01-1056,W00-1009,1,0.878413,"Missing"
H05-1115,J96-2004,0,0.0177241,"e 11, 3 and 6, respectively. Next, we assigned each cluster of articles to an annotator, who was asked to read all articles in the cluster. He or she then generated a list of factual questions key to understanding the story. Once we collected the questions for each cluster, two judges independently annotated nine of the training clusters. For each sentence and question pair in a given cluster, the judges were asked to indicate whether or not the sentence contained a complete answer to the question. Once an acceptable rate of interjudge agreement was verified on the first nine clusters (Kappa (Carletta, 1996) of 0.68), the remaining 11 clusters were annotated by one judge each. In some cases, the judges did not find any sentences containing the answer for a given question. Such questions were removed from the corpus. The final number of questions annotated for answers over the entire corpus was 341, and the distributions of questions per cluster can be found in Table 1. 4.2 Evaluation metrics and methods To evaluate our sentence retrieval mechanism, we produced extract files, which contain a list of sentences deemed to be relevant to the question, for the system and from human judgment. To compare"
H05-1115,P04-1027,0,0.0190167,"Missing"
H05-1115,P04-1035,0,0.0218464,"e answer to a user’s question may be updated and reworded over time by journalists in order to keep a running story fresh, or because the facts themselves change. Therefore, there is often more than one correct answer to a question. We aim to develop a method for sentence retrieval that goes beyond finding sentences that are similar to a single query. To this end, we propose to use a stochastic, graph-based method. Recently, graph-based methods have proved useful for a number of NLP and IR tasks such as document re-ranking in ad hoc IR (Kurland and Lee, 2005) and analyzing sentiments in text (Pang and Lee, 2004). In (Erkan and Radev, 2004), we introduced the LexRank method and successfully applied it to generic, multi-document summarization. Presently, we introduce topic-sensitive LexRank in creating a sentence retrieval system. We evaluate its performance against a competitive baseline, which considers the similarity between each sentence and the question (using IDF-weighed word overlap). We demonstrate that LexRank significantly improves question-focused sentence selection over the baseline. 2 Formal description of the problem Our goal is to build a question-focused sentence retrieval mechanism usi"
H05-1115,voorhees-tice-2000-trec,0,0.0168393,"t have? In which places had the hurricane landed? How many people were killed? When did the Kursk sink? How many people were injured? How many people were in the mall at the time of the bombing? What issue concerned British human rights groups? Table 1: Corpus of complex news stories. and sentence numbers of the top 20 sentences. The “gold standard” extracts list the sentences judged as containing answers to a given question by the annotators (and therefore have variable sizes) in no particular order.2 We evaluated the performance of the systems using two metrics - Mean Reciprocal Rank (MRR) (Voorhees and Tice, 2000) and Total Reciprocal Document Rank (TRDR) (Radev et al., 2005). MRR, used in the TREC Q&A evaluations, is the reciprocal rank of the first correct answer (or sentence, in our case) to a given question. This measure gives us an idea of how far down we must look in the ranked list in order to find a correct answer. To contrast, TRDR is the total of the reciprocal ranks of all answers found by the system. In the context of answering questions from complex stories, where there is often more than one correct answer to a question, and where answers are typically time-dependent, we should focus on m"
J02-4001,P99-1071,1,0.535805,"Missing"
J02-4001,J96-2004,0,0.0414546,"Missing"
J02-4001,W00-0408,0,0.00656006,"Missing"
J02-4001,J02-4006,0,0.00628879,"om the original document, Witbrock and Mittal (1999) extract a set of words from the input document and then order the words into sentences using a bigram language model. Jing and McKeown (1999) point out that human summaries are often constructed from the source document by a process of cutting and pasting document fragments that are then combined and regenerated as summary sentences. Hence a summarizer can be developed to extract sentences, reduce them by dropping unimportant fragments, and then use information fusion and generation to combine the remaining fragments. In this special issue, Jing (2002) reports on automated techniques to build a corpus representing the cut-and-paste process used by humans; such a corpus can then be used to train an automated summarizer. Other researchers focus on the reduction process. In an attempt to learn rules for reduction, Knight and Marcu (2000) use expectation maximization to train a system to compress the syntactic parse tree of a sentence in order to produce a shorter but 401 Computational Linguistics Volume 28, Number 4 still maximally grammatical version. Ultimately, this approach can likely be used for shortening two sentences into one, three in"
J02-4001,A97-1042,1,0.514115,"rely on machine learning to identify important features, on natural language analysis to identify key passages, or on relations between words rather than bags of words. The application of machine learning to summarization was pioneered by Kupiec, Pedersen, and Chen (1995), who developed a summarizer using a Bayesian classifier to combine features from a corpus of scientific articles and their abstracts. Aone et al. (1999) and Lin (1999) experimented with other forms of machine learning and its effectiveness. Machine learning has also been applied to learning individual features; for example, Lin and Hovy (1997) applied machine learning to the problem of determining how sentence position affects the selection of sentences, and Witbrock and Mittal (1999) used statistical approaches to choose important words and phrases and their syntactic context. 400 Radev, Hovy, and McKeown Summarization: Introduction Approaches involving more sophisticated natural language analysis to identify key passages rely on analysis either of word relatedness or of discourse structure. Some research uses the degree of lexical connectedness between potential passages and the remainder of the text; connectedness may be measure"
J02-4001,W01-0100,0,0.107473,"ms that can automatically summarize one or more documents become increasingly desirable. Recent research has investigated types of summaries, methods to create them, and methods to evaluate them. Several evaluation competitions (in the style of the National Institute of Standards and Technology’s [NIST’s] Text Retrieval Conference [TREC]) have helped determine baseline performance levels and provide a limited set of training material. Frequent workshops and symposia reflect the ongoing interest of researchers around the world. The volume of papers edited by Mani and Maybury (1999) and a book (Mani 2001) provide good introductions to the state of the art in this rapidly evolving subfield. A summary can be loosely defined as a text that is produced from one or more texts, that conveys important information in the original text(s), and that is no longer than half of the original text(s) and usually significantly less than that. Text here is used rather loosely and can refer to speech, multimedia documents, hypertext, etc. The main goal of a summary is to present the main ideas in a document in less space. If all sentences in a text document were of equal importance, producing a summary would no"
J02-4001,P99-1072,0,0.0527337,"Missing"
J02-4001,W97-0713,0,0.0944504,"y the number of shared words, synonyms, or anaphora (e.g., Salton et al. 1997; Mani and Bloedorn 1997; Barzilay and Elhadad 1999). Other research rewards passages that include topic words, that is, words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not possible in all genres. This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. An open-source summarization environment, MEAD, was recently developed at the Johns Hopkins summer workshop (Radev et al. 2002). MEAD allows researchers to experiment with different features and methods for combination. Some re"
J02-4001,C94-1056,0,0.0181083,"Missing"
J02-4001,W02-0404,1,0.419938,"Missing"
J02-4001,2001.mtsummit-papers.68,0,0.0198146,"Missing"
J02-4001,W00-0403,1,0.391204,"Missing"
J02-4001,J98-3005,1,0.16809,"ems from different source documents. In an early approach to multidocument summarization, information extraction was used to facilitate the identification of similarities and differences (McKeown and Radev 1995). As for single-document summarization, this approach produces more of a briefing than a summary, as it contains only preidentified information types. Identity of slot values are used to determine when information is reliable enough to include in the summary. Later work merged information extraction approaches with regeneration of extracted text to improve summary generation (Radev and McKeown 1998). Important differences (e.g., updates, trends, direct contradictions) are identified through a set of discourse rules. Recent work also follows this approach, using enhanced information extraction and additional forms of contrasts (White and Cardie 2002). To identify redundancy in text documents, various similarity measures are used. A common approach is to measure similarity between all pairs of sentences and then use clustering to identify themes of common information (McKeown et al. 1999; Radev, Jing, and Budzikowska 2000; Marcu and Gerber 2001). Alternatively, systems measure the similari"
J02-4001,J02-4005,0,0.0345746,"Missing"
J02-4001,J02-4004,0,0.00917955,"Missing"
J02-4001,J02-4002,0,0.0637939,", words that have been determined to correlate well with the topic of interest to the user (for topic-oriented summaries) or with the general theme of the source text (Buckley and Cardie 1997; Strzalkowski et al. 1999; Radev, Jing, and Budzikowska 2000). Alternatively, a summarizer may reward passages that occupy important positions in the discourse structure of the text (Ono, Sumita, and Miike 1994; Marcu 1997b). This method requires a system to compute discourse structure reliably, which is not possible in all genres. This technique is the focus of one of the articles in this special issue (Teufel and Moens 2002), which shows how particular types of rhetorical relations in the genre of scientific journal articles can be reliably identified through the use of classification. An open-source summarization environment, MEAD, was recently developed at the Johns Hopkins summer workshop (Radev et al. 2002). MEAD allows researchers to experiment with different features and methods for combination. Some recent work (Conroy and O’Leary 2001) has turned to the use of hidden Markov models (HMMs) and pivoted QR decomposition to reflect the fact that the probability of inclusion of a sentence in an extract depends"
J02-4001,W02-0402,0,0.0221134,"Missing"
J02-4001,J02-4003,0,0.00735213,"Missing"
J02-4001,W97-0703,0,\N,Missing
J02-4001,E99-1011,0,\N,Missing
J02-4001,P02-1058,1,\N,Missing
J02-4001,P02-1040,0,\N,Missing
J02-4001,W97-0704,1,\N,Missing
J02-4001,W02-0406,1,\N,Missing
J02-4001,X98-1024,0,\N,Missing
J02-4001,H01-1065,1,\N,Missing
J02-4001,C67-1037,0,\N,Missing
J14-3003,N09-1003,0,0.0603119,"Missing"
J14-3003,E06-1027,0,0.033505,"o build lexicons of polarized words. Esuli and Sebastiani (2005, 2006) use a textual representation of words by collating all the glosses of the word as found in some dictionary. Then, a binary text classifier is trained using the textual representation and applied to new words. Kim and Hovy (2004) start with two lists of positive and negative seed words. WordNet is used to expand these lists. Synonyms of positive words and antonyms of negative words are considered positive, and synonyms of negative words and antonyms of positive words are considered negative. A similar method is presented in Andreevskaia and Bergler (2006), where WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds. The sentiment classes are treated as fuzzy categories where some words are very central to one category, whereas others may be interpreted differently. Mohammad, Dunne, and Dorr (2009) utilize the marking theory, which states that overtly marked words such as dishonest, unhappy, and impure tend to have negative semantic orientations whereas their unmarked counterparts (honest, happy, and pure) tend to have positive semantic orientation. They use a set of 11 antonym-generating affix patterns to gener"
J14-3003,banea-etal-2008-bootstrapping,0,0.0486055,"Missing"
J14-3003,elkateb-etal-2006-building,0,0.0249823,"k G(V, E) where V = Ven ∪ Vfr is the union of the sets of English and Foreign words. E is a set of edges connecting nodes in V. There are three types of connections: English–English connections, Foreign–Foreign connections, and English–Foreign connections. For the English–English connections, we use the same methodology as in Section 3. Foreign–Foreign connections are created in a similar way to the English connections. Some foreign languages have lexical resources based on the design of the Princeton English WordNet. For example: Euro WordNet (Vossen 1997), Arabic WordNet (Black et al. 2006; Elkateb et al. 2006a, 2006b), and the Hindi WordNet (Jha et al. 2001; Narayan et al. 2002). We also use co-occurrence statistics similar to the work of Hatzivassiloglou and McKeown (1997). Finally, to connect foreign words to English words, we use a Foreign to English dictionary. For every word in a list of foreign words, we look up its meaning in a dictionary and add an edge between the foreign word and every other English word that appeared as a possible meaning for it. If there is no comprehensive enough dictionary available, constructing a multilingual word network like a translation graph (Etzioni et al. 20"
J14-3003,P10-1041,1,0.37299,"Missing"
J14-3003,P97-1023,0,0.8206,"r and does not need a large training corpus. The rest of the article is structured as follows. In Section 2, we review related work on word polarity and subjectivity classification and note applications of the random walk and hitting times framework. Section 3 presents our method for identifying word polarity. We describe how the proposed method can be extended to cover foreign languages in Section 4, and out-of-vocabulary words in Section 5. Section 6 describes our experimental set-up. We present our conclusions in Section 7. 2. Related Work 2.1 Identifying Word Polarity Hatzivassiloglou and McKeown (1997) proposed a method for identifying the word polarity of adjectives. They extract all conjunctions of adjectives from a given corpus 540 Hassan et al. A Random Walk–Based Model for Identifying Semantic Orientation and then they classify each conjunctive expression as either the same orientation such as “simple and well-received” or different orientation such as “simplistic but wellreceived.” The result is a graph that they cluster into two subsets of adjectives. They classify the cluster with the higher average frequency as positive. They created and labeled their own data set for experiments."
J14-3003,C00-1044,0,0.0822779,"tences will be in the same subjectivity class. All sentences to be classified are represented as unlabeled nodes and the only two labeled nodes represent the subjective and objective classes. A Mincut algorithm is then performed on the constructed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sen"
J14-3003,E09-1046,0,0.246618,"with two lists of positive and negative seed words. WordNet is used to expand these lists. Synonyms of positive words and antonyms of negative words are considered positive, and synonyms of negative words and antonyms of positive words are considered negative. A similar method is presented in Andreevskaia and Bergler (2006), where WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds. The sentiment classes are treated as fuzzy categories where some words are very central to one category, whereas others may be interpreted differently. Mohammad, Dunne, and Dorr (2009) utilize the marking theory, which states that overtly marked words such as dishonest, unhappy, and impure tend to have negative semantic orientations whereas their unmarked counterparts (honest, happy, and pure) tend to have positive semantic orientation. They use a set of 11 antonym-generating affix patterns to generate overtly marked words and their counterparts from the Macquarie Thesaurus. After obtaining a set of 2,600 seeds by the affix patterns, they expand the sentiment lexicon using a Roget-like thesaurus. Their method does not require seed sentiment words or WordNet, but still needs"
J14-3003,kamps-etal-2004-using,0,0.417031,"have the same spin direction, neighboring words tend to have similar polarity. They pose the problem as an optimization problem and use the mean field method to find the best solution. The analogy with electrons leads them to assume that each word should be either positive or negative. This assumption is not accurate because most of the words in the language do not have any semantic orientation. They report that their method could get misled by noise in the gloss definition and their computations sometimes get trapped in a local optimum because of its greedy optimization flavor. Kamps et al. (2004) construct a network based on WordNet (Miller 1995) synonyms and then use the shortest paths between any given word and the words “good” and “bad” to determine word polarity. They report that using shortest paths could be very noisy. For example, “good” and “bad” themselves are closely related in WordNet with a 5-long sequence “good, sound, heavy, big, bad.” A given word w may be more connected to one set of words (e.g., positive words); yet have a shorter path connecting it to one word in the other set. Restricting seed words to only two words affects their accuracy. Adding more seed words co"
J14-3003,W06-1642,0,0.0365433,"arked words and their counterparts from the Macquarie Thesaurus. After obtaining a set of 2,600 seeds by the affix patterns, they expand the sentiment lexicon using a Roget-like thesaurus. Their method does not require seed sentiment words or WordNet, but still needs a comprehensive thesaurus. The idea of the marking theory is language-dependent and cannot be applied from one language to another. Contrasting the dictionary based approaches that rely on resources such as WordNet, Velikovich et al. (2010) investigated the viability of learning sentiment lexicons semi-automatically from the Web. Kanayama and Nasukawa (2006) use syntactic features and context coherency (i.e., the tendency for same polarities to appear successively) to detect polar clauses. 2.3 Random Walk–Based Methods Closest to our work in its methodology is probably the line of research on semisupervised graphical methods for sentiment classification. Rao and Ravichandran (2009) build a lexical graph similar to ours. The graph is constructed of both unlabeled and labeled nodes, each node representing a word that can be either positive or negative, and each edge representing some semantic relatedness that can be constructed using resources like"
J14-3003,C04-1200,0,0.089308,"y check if any of the antonyms of the given word has known polarity. If so, they label it with the opposite label of the antonym. They continue in a bootstrapping manner until they label all possible words. 541 Computational Linguistics Volume 40, Number 3 2.2 Building Sentiment Lexicons A number of other methods try to build lexicons of polarized words. Esuli and Sebastiani (2005, 2006) use a textual representation of words by collating all the glosses of the word as found in some dictionary. Then, a binary text classifier is trained using the textual representation and applied to new words. Kim and Hovy (2004) start with two lists of positive and negative seed words. WordNet is used to expand these lists. Synonyms of positive words and antonyms of negative words are considered positive, and synonyms of negative words and antonyms of positive words are considered negative. A similar method is presented in Andreevskaia and Bergler (2006), where WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds. The sentiment classes are treated as fuzzy categories where some words are very central to one category, whereas others may be interpreted differently. Mohammad, Dunne, and"
J14-3003,N10-1017,0,0.0606132,"Missing"
J14-3003,D09-1063,0,0.0519769,"Missing"
J14-3003,P04-1035,0,0.075136,"labels. Our experiments showed that this achieves better results than methods that use label propagation. 2.4 Subjectivity Analysis Subjectivity analysis is another research line that is closely related to our work. The main task in subjectivity analysis is to identify text that presents opinion as opposed to objective text that present factual information (Wiebe 2000). Text could be either words, phrases, sentences, or other chunks. Wiebe et al. (2001) list a number of applications of subjectivity analysis such as classifying e-mails and mining reviews. For example, to analyze movie reviews, Pang and Lee (2004) apply Mincut to a graph constructed from individual sentences as nodes to determine whether a sentence is subjective or objective. Each node (sentence) has an individual subjectivity score obtained from a first-pass classifier using sentence features and linguistic knowledge. Edges are weighted by a similarity metric of how likely it is that the two sentences will be in the same subjectivity class. All sentences to be classified are represented as unlabeled nodes and the only two labeled nodes represent the subjective and objective classes. A Mincut algorithm is then performed on the construc"
J14-3003,H05-1043,0,0.10657,"performed on the constructed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity of WordNet words. Wiebe and Mihalcea (2006b) label word se"
J14-3003,E09-1077,0,0.36027,"nguage-dependent and cannot be applied from one language to another. Contrasting the dictionary based approaches that rely on resources such as WordNet, Velikovich et al. (2010) investigated the viability of learning sentiment lexicons semi-automatically from the Web. Kanayama and Nasukawa (2006) use syntactic features and context coherency (i.e., the tendency for same polarities to appear successively) to detect polar clauses. 2.3 Random Walk–Based Methods Closest to our work in its methodology is probably the line of research on semisupervised graphical methods for sentiment classification. Rao and Ravichandran (2009) build a lexical graph similar to ours. The graph is constructed of both unlabeled and labeled nodes, each node representing a word that can be either positive or negative, and each edge representing some semantic relatedness that can be constructed using resources like WordNet or other thesaurus. They evaluate two semi-supervised methods: Mincut (including its variant, Randomized Mincut) and label propagation. The general idea of label propagation is defining a probability distribution over the positive and negative classes for each node in the graph. A Markov random walk is performed on the"
J14-3003,W03-1014,0,0.117343,"ive and objective classes. A Mincut algorithm is then performed on the constructed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity o"
J14-3003,N09-1001,0,0.021424,"d Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity of WordNet words. Wiebe and Mihalcea (2006b) label word senses in WordNet as subjective or objective, utilizing the MPQA corpus. They show that subjectivity information for WordNet senses can improve word sense disambiguation tasks for subjectivity ambiguous words. Su and Markert (2009) propose a semi-supervised minimum cut framework to label word sense entries in WordNet with subjectivity information. Their method requires less training data other than the sense definitions and relational structure of WordNet. 2.5 Word Polarity Classification for Foreign Languages Word sentiment and subjectivity has also been studied for languages other than English. Jijkoun and Hofmann (2009) describe a method for creating a non-English subjectivity 543 Computational Linguistics Volume 40, Number 3 lexicon based on an English lexicon, an on-line translation service, and Wordnet. Mihalcea a"
J14-3003,P05-1017,0,0.0256095,"ists of the given word and one of the seed words. They use the search engine NEAR operator to look for instances where the given word is physically close to the seed word in the returned document. They present their method as an unsupervised method where a very small number of seed words are used to define semantic orientation rather than train the model. One of the limitations of their method is that it requires a large corpus of text to achieve good performance. They use several corpora; the size of the best performing data set is roughly one hundred billion words (Turney and Littman 2003). Takamura et al. (2005) propose using spin models for extracting semantic orientation of words. They construct a network of words using gloss definitions, thesaurus, and co-occurrence statistics. They regard each word as an electron. Each electron has a spin and each spin has a direction taking one of two values: up or down. Two neighboring spins tend to have the same orientation from an energy point of view. Their hypothesis is that as neighboring electrons tend to have the same spin direction, neighboring words tend to have similar polarity. They pose the problem as an optimization problem and use the mean field m"
J14-3003,P02-1053,0,0.230956,"ical Engineering & Computer Science and School of Information, University of Michigan, Ann Arbor, MI, USA. E-mail: radev@umich.edu. Submission received: 15 November 2011; revised submission received: 10 May 2013; accepted for publication: 14 July 2013. doi:10.1162/COLI a 00192 © 2014 Association for Computational Linguistics Computational Linguistics Volume 40, Number 3 1. Introduction Identifying emotions and attitudes from unstructured text has a variety of possible applications. For example, there has been a large body of work for mining product reputation on the Web (Morinaga et al. 2002; Turney 2002). Morinaga et al. (2002) have shown how product reputation mining helps with marketing and customer relation management. The Google products catalog and many on-line shopping sites like Amazon.com provide customers not only with comprehensive information and reviews about a product, but also with faceted sentiment summaries. Such systems are all supported by a sentiment lexicon, some even in multiple languages. Another interesting application is mining on-line discussions. An enormous number of discussion groups exist on the Web. Millions of users post content to these groups covering pretty m"
J14-3003,W01-1626,0,0.0473898,"walks to learn paraphrases. Our work is different from previous random walk methods in that it uses the mean hitting time as the criterion for assigning polarity labels. Our experiments showed that this achieves better results than methods that use label propagation. 2.4 Subjectivity Analysis Subjectivity analysis is another research line that is closely related to our work. The main task in subjectivity analysis is to identify text that presents opinion as opposed to objective text that present factual information (Wiebe 2000). Text could be either words, phrases, sentences, or other chunks. Wiebe et al. (2001) list a number of applications of subjectivity analysis such as classifying e-mails and mining reviews. For example, to analyze movie reviews, Pang and Lee (2004) apply Mincut to a graph constructed from individual sentences as nodes to determine whether a sentence is subjective or objective. Each node (sentence) has an individual subjectivity score obtained from a first-pass classifier using sentence features and linguistic knowledge. Edges are weighted by a similarity metric of how likely it is that the two sentences will be in the same subjectivity class. All sentences to be classified are"
J14-3003,P06-1134,0,0.103909,"ed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity of WordNet words. Wiebe and Mihalcea (2006b) label word senses in WordNet as subject"
J14-3003,W03-1017,0,0.0402044,"es. A Mincut algorithm is then performed on the constructed graph to obtain the subjectivity classes for individual sentences. The authors also integrate the subjectivity classification of isolated sentences to document level sentiment analysis. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Hatzivassiloglou and Wiebe 2000; Wiebe 2000; Banea, Mihalcea, and Wiebe 2008). In the second category, the context of subjective text is used (Nasukawa and Yi 2003; Riloff and Wiebe 2003; Yu and Hatzivassiloglou 2003; Popescu and Etzioni 2005). Wiebe and Mihalcea (2006a) studied the association of word subjectivity and word sense. They showed that different subjectivity labels can be assigned to different senses of the same word. Wiebe, Wilson, and Cardie (2005) described MPQA, a corpus of news articles from a wide variety of news sources manually annotated for opinions and other private states (i.e., beliefs, emotions, sentiments, speculations) directed for studying opinions and emotions in language. In addition, there has been a large body of work on labeling subjectivity of WordNet words. Wiebe and Mih"
J14-3003,H05-2017,0,\N,Missing
J14-3003,N10-1119,0,\N,Missing
J14-3003,N10-1122,0,\N,Missing
J14-3003,P07-1123,0,\N,Missing
J14-3003,esuli-sebastiani-2006-sentiwordnet,0,\N,Missing
J98-3005,M92-1024,0,0.0460828,"Missing"
J98-3005,W97-0703,0,0.146356,"that these sentences function together coherently as a full paragraph. As with the other statistical approaches, this work is aimed at summarization of single articles. Work presented at the 1997 ACL Workshop on Intelligent Scalable Text Summarization primarily focused on the use of sentence extraction. Alternatives to the use 473 Computational Linguistics Volume 24, Number 3 of frequency of key phrases included the identification and representation of lexical chains (Halliday and Hasan 1976) to find the major themes of an article followed by the extraction of one or two sentences per chain (Barzilay and Elhadad 1997), training over the position of summary sentences in the full article (Hovy and Lin 1997), and the construction of a graph of important topics to identify paragraphs that should be extracted (Mitra, Singhal, and Buckley 1997). While most of the work in this category focuses on summarization of single articles, early work is beginning to emerge on summarization across multiple documents. In ongoing work at Carnegie Mellon, Carbonell (personal communication) is developing statistical techniques to identify similar sentences and phrases across articles. The aim is to identify sentences that are r"
J98-3005,W97-0702,0,0.029309,"Sentence Extraction To allow summarization in arbitrary domains, researchers have traditionally applied statistical techniques (Luhn 1958; Paice 1990; Preston and Williams 1994; Rau, Brandow, and Mitze 1994). This approach can be better termed extraction rather than summarization, since it attempts to identify and extract key sentences from an article using statistical techniques that locate important phrases using various statistical measures. This has been successful in different domains (Preston and Williams 1994) and is, in fact, the approach used in recent commercial summarizers (Apple [Boguraev and Kennedy 1997], Microsoft, and inXight). Rau, Brandow, and Mitze (1994) report that statistical summaries of individual news articles were rated lower by evaluators than summaries formed by simply using the lead sentence or two from the article. This follows the principle of the ""inverted pyramid"" in news writing, which puts the most salient information in the beginning of the article and leaves elaborations for later paragraphs, allowing editors to cut from the end of the text without compromising the readability of the remaining text. Paice (1990) also notes that problems for this approach center around"
J98-3005,C90-3059,0,0.0199317,"kich et al. 1997) generates Web traffic summaries for advertisement management software. It makes use of an ontology over the domain to combine information at the conceptual level. All of these systems take tabular data as input. The research focus has been on linguistic summarization. SUMMONS, on the other hand, focuses on conceptual summarization of both structured and full-text data. At least four previous systems developed elsewhere use natural language to sum475 Computational Linguistics Volume 24, Number 3 marize quantitative data, including ANA (Kukich 1983), SEMTEX (R6sner 1987), FOG (Bourbeau et al. 1990), and LFS (Iordanskaja et al. 1994). All of these use some forms of conceptual and linguistic summarization and the techniques can be adapted for our current work on summarization of multiple articles. In related work, Dalianis and Hovy (1993) have also looked at the problem of summarization, identifying eight aggregation operators (e.g., conjunction around noun phrases) that apply during generation to create more concise text. 3. System Overview The overall architecture of our summarization system given earlier in Figure 1 draws on research in software agents (Genesereth and Ketchpel 1994) to"
J98-3005,A88-1019,0,0.024822,"Missing"
J98-3005,M92-1031,0,0.0190995,"fic information extraction systems, there has also been a large body of work on identifying people and organizations in text through proper noun extraction. These are domain-independent techniques that can also be used to extract information for a summary. Techniques for proper noun extraction include the use of regular grammars to delimit and identify proper nouns (Mani et al. 1993; Paik et al. 1994), the use of extensive name lists, place names, titles and ""gazetteers"" in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al. 1992; Aberdeen et al. 1992), statistical training to learn, for example, Spanish names, from on-line corpora (Ayuso 474 Radev and McKeown Generating Natural Language Summaries et al. 1992), and the use of concept-based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al. 1993; Lehnert et al. 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald 1993; Coates-Stephens 1991) and the larger discourse context (Mani et al. 1993) to improve the accuracy of p"
J98-3005,M95-1011,0,0.0032534,". By relying on these systems, the task we have addressed to date is happily more restricted than direct summarization of full text. This has allowed us to focus on issues related to the combination of information in the templates and the generation of text to express them. In order to port our system to other domains, we would need to develop new templates and the information extraction rules required for them. While this is a task we leave to those working in the information extraction field, we note that there do exist tools for semi-automatically acquiring such rules (Lehnert et al. 1993; Fisher et al. 1995). This helps to alleviate the otherwise knowledge-intensive nature of the task. We are working on the development of tools for domain-independent types of information extraction. For example, our work on extracting descriptions of individuals and organizations and representing them in a formalism that facilitates reuse of the descriptions in summaries can be used in any domain. In the remainder of this section, we highlight the novel techniques of SUMMONS and explain why they are important for our work. 1.1 Summarization of Multiple Articles With a few exceptions (cf. Section 2), all existing"
J98-3005,P88-1020,0,0.00800527,"Missing"
J98-3005,W97-0704,0,0.251839,"ical approaches, this work is aimed at summarization of single articles. Work presented at the 1997 ACL Workshop on Intelligent Scalable Text Summarization primarily focused on the use of sentence extraction. Alternatives to the use 473 Computational Linguistics Volume 24, Number 3 of frequency of key phrases included the identification and representation of lexical chains (Halliday and Hasan 1976) to find the major themes of an article followed by the extraction of one or two sentences per chain (Barzilay and Elhadad 1997), training over the position of summary sentences in the full article (Hovy and Lin 1997), and the construction of a graph of important topics to identify paragraphs that should be extracted (Mitra, Singhal, and Buckley 1997). While most of the work in this category focuses on summarization of single articles, early work is beginning to emerge on summarization across multiple documents. In ongoing work at Carnegie Mellon, Carbonell (personal communication) is developing statistical techniques to identify similar sentences and phrases across articles. The aim is to identify sentences that are representative of more than one article. Mani and Bloedorn (1997) link similar words and p"
J98-3005,P83-1022,0,0.0414662,"1995). ZEDDoc (Passonneau et al. 1997; Kukich et al. 1997) generates Web traffic summaries for advertisement management software. It makes use of an ontology over the domain to combine information at the conceptual level. All of these systems take tabular data as input. The research focus has been on linguistic summarization. SUMMONS, on the other hand, focuses on conceptual summarization of both structured and full-text data. At least four previous systems developed elsewhere use natural language to sum475 Computational Linguistics Volume 24, Number 3 marize quantitative data, including ANA (Kukich 1983), SEMTEX (R6sner 1987), FOG (Bourbeau et al. 1990), and LFS (Iordanskaja et al. 1994). All of these use some forms of conceptual and linguistic summarization and the techniques can be adapted for our current work on summarization of multiple articles. In related work, Dalianis and Hovy (1993) have also looked at the problem of summarization, identifying eight aggregation operators (e.g., conjunction around noun phrases) that apply during generation to create more concise text. 3. System Overview The overall architecture of our summarization system given earlier in Figure 1 draws on research in"
J98-3005,W97-0903,1,0.472801,"rwise appear as separate sentences gets added in as modifiers of the existing sentences, or new words that can simultaneously convey both pieces of information are selected. PLANDoc (McKeown, Kukich, and Shaw 1994a; McKeown, Robin, and Kukich 1995; Shaw 1995) generates summaries of the activities of telephone planning engineers, using linguistic summarization both to order its input messages and to combine them into single sentences. Focus has been on the combined use of conjunction, ellipsis, and paraphrase to result in concise, yet fluent reports (Shaw 1995). ZEDDoc (Passonneau et al. 1997; Kukich et al. 1997) generates Web traffic summaries for advertisement management software. It makes use of an ontology over the domain to combine information at the conceptual level. All of these systems take tabular data as input. The research focus has been on linguistic summarization. SUMMONS, on the other hand, focuses on conceptual summarization of both structured and full-text data. At least four previous systems developed elsewhere use natural language to sum475 Computational Linguistics Volume 24, Number 3 marize quantitative data, including ANA (Kukich 1983), SEMTEX (R6sner 1987), FOG (Bourbeau et al. 1"
J98-3005,M93-1023,0,0.0907063,"on linguistic summarization. SUMMONS, on the other hand, focuses on conceptual summarization of both structured and full-text data. At least four previous systems developed elsewhere use natural language to sum475 Computational Linguistics Volume 24, Number 3 marize quantitative data, including ANA (Kukich 1983), SEMTEX (R6sner 1987), FOG (Bourbeau et al. 1990), and LFS (Iordanskaja et al. 1994). All of these use some forms of conceptual and linguistic summarization and the techniques can be adapted for our current work on summarization of multiple articles. In related work, Dalianis and Hovy (1993) have also looked at the problem of summarization, identifying eight aggregation operators (e.g., conjunction around noun phrases) that apply during generation to create more concise text. 3. System Overview The overall architecture of our summarization system given earlier in Figure 1 draws on research in software agents (Genesereth and Ketchpel 1994) to allow connections to a variety of different types of data sources. Facilities are used to provide a transparent interface to heterogeneous data sources that run on several machines and may be written in different programming languages. Curren"
J98-3005,W93-0105,0,0.0248138,"future work. Marcu (1997) uses a rhetorical parser to build rhetorical structure trees for arbitrary texts and produces a summary by extracting sentences that span the major rhetorical nodes of the tree. In addition to domain-specific information extraction systems, there has also been a large body of work on identifying people and organizations in text through proper noun extraction. These are domain-independent techniques that can also be used to extract information for a summary. Techniques for proper noun extraction include the use of regular grammars to delimit and identify proper nouns (Mani et al. 1993; Paik et al. 1994), the use of extensive name lists, place names, titles and ""gazetteers"" in conjunction with partial grammars in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al. 1992; Aberdeen et al. 1992), statistical training to learn, for example, Spanish names, from on-line corpora (Ayuso 474 Radev and McKeown Generating Natural Language Summaries et al. 1992), and the use of concept-based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al. 1993; Lehnert et al. 1993). In"
J98-3005,W97-0713,0,0.105914,"ey do not address the task of summarization since they do not combine and rephrase extracted information as part of a textual summary. A recent approach to symbolic summarization is being carried out at Cambridge University on identifying strategies for summarization (Sparck Jones 1993). This work studies how various discourse processing techniques (e.g., rhetorical structure relations) can be used to both identify important information and form the actual summary. While promising, this work does not involve an implementation as of yet, but provides a framework and strategies for future work. Marcu (1997) uses a rhetorical parser to build rhetorical structure trees for arbitrary texts and produces a summary by extracting sentences that span the major rhetorical nodes of the tree. In addition to domain-specific information extraction systems, there has also been a large body of work on identifying people and organizations in text through proper noun extraction. These are domain-independent techniques that can also be used to extract information for a summary. Techniques for proper noun extraction include the use of regular grammars to delimit and identify proper nouns (Mani et al. 1993; Paik et"
J98-3005,W93-0104,0,0.0103133,"ial grammars in order to recognize proper nouns as unknown words in close proximity to known words (Cowie et al. 1992; Aberdeen et al. 1992), statistical training to learn, for example, Spanish names, from on-line corpora (Ayuso 474 Radev and McKeown Generating Natural Language Summaries et al. 1992), and the use of concept-based pattern matchers that use semantic concepts as pattern categories as well as part-of-speech information (Weischedel et al. 1993; Lehnert et al. 1993). In addition, some researchers have explored the use of both local context surrounding the hypothesized proper nouns (McDonald 1993; Coates-Stephens 1991) and the larger discourse context (Mani et al. 1993) to improve the accuracy of proper noun extraction when large known-word lists are not available. In a way similar to this research, our work also aims at extracting proper nouns without the aid of large word lists. We use a regular grammar encoding part-of-speech categories to extract certain text patterns (descriptions) and we use WordNet (Miller et al. 1990) to provide semantic filtering. Another system, called MURAX (Kupiec 1993), is similar to ours from a different perspective. MURAX also extracts information from"
J98-3005,A94-1002,1,0.277667,"Missing"
J98-3005,P93-1031,1,0.514309,"Missing"
J98-3005,W97-0707,0,0.0473643,"Missing"
J98-3005,W96-0512,1,0.456172,"iate historical references and the context of prior news. Briefings focus on certain types of information that are present in the source text in which the reader has expressed interest. They deliberately ignore facts that are tangential to the user&apos;s interests, whether or not these facts are the focus of the article. In other words, briefings are more user-centered than general summaries; the latter convey information that the writer has considered important, whereas briefings are based on information that the user is looking for. We present a system, called SUMMONS 1 (McKeown and Radev 1995; Radev 1996; Radev and McKeown 1997), shown in Figure 1, which introduces novel techniques in the following areas: • It briefs the user on information of interest using tools related to information extraction, conceptual combination, and text generation. • It combines information from multiple news articles into a coherent summary using symbolic techniques. • It augments the resulting summaries using descriptions of entities obtained from on-line sources. As can be expected from a knowledge-based summarization system, SUMMONS works in a restricted domain. We have chosen the domain of news on terrorism fo"
J98-3005,A97-1033,1,0.832566,"nces and the context of prior news. Briefings focus on certain types of information that are present in the source text in which the reader has expressed interest. They deliberately ignore facts that are tangential to the user&apos;s interests, whether or not these facts are the focus of the article. In other words, briefings are more user-centered than general summaries; the latter convey information that the writer has considered important, whereas briefings are based on information that the user is looking for. We present a system, called SUMMONS 1 (McKeown and Radev 1995; Radev 1996; Radev and McKeown 1997), shown in Figure 1, which introduces novel techniques in the following areas: • It briefs the user on information of interest using tools related to information extraction, conceptual combination, and text generation. • It combines information from multiple news articles into a coherent summary using symbolic techniques. • It augments the resulting summaries using descriptions of entities obtained from on-line sources. As can be expected from a knowledge-based summarization system, SUMMONS works in a restricted domain. We have chosen the domain of news on terrorism for several reasons. First,"
J98-3005,P95-1053,0,0.0436206,"Missing"
J98-3005,C90-1021,0,\N,Missing
J98-3005,H93-1062,0,\N,Missing
J98-3005,M92-1030,0,\N,Missing
J98-3005,A97-1030,0,\N,Missing
J98-3005,C92-3158,0,\N,Missing
J98-3005,X98-1026,0,\N,Missing
J98-3005,M95-1006,0,\N,Missing
K17-1045,W09-1802,0,0.254656,"Missing"
K17-1045,H01-1065,0,0.128073,"Dragomir Radev1 1 Department of Computer Science, Yale University 2 The LNM Institute of Information Technology {michihiro.yasunaga,r.zhang,kshitijh.meelu}@yale.edu {ayush.original}@gmail.com {krishnan.srinivasan,dragomir.radev}@yale.edu Abstract salience. Then, they select summary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (McDonald, 2007; Gillick and Favre, 2009; Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003; Barzilay et al., 2001) can follow to improve coherence of the summary. Recently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all the sentences in the same document cluster are processed independently. Hence, the relationships between sentences and thus the re"
K17-1045,N09-1041,0,0.62727,"Missing"
K17-1045,C16-1053,0,0.0291462,"summarizers produce the summary in two steps: sentence ranking and sentence selection. First, they utilize humanengineered features such as sentence position and length (Radev et al., 2004a), word frequency and importance (Nenkova et al., 2006; Hong and Nenkova, 2014), among others, to rank sentence 452 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 452–462, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics employ encoder-decoder RNNs to effectively produce short abstractive summaries for opinions. Cao et al. (2016) develop a query-focused summarization system called AttSum which deals with saliency ranking and relevance ranking using query-attention-weighted CNNs. Very recently, thanks to the large scale news article datasets (Hermann et al., 2015), Cheng and Lapata (2016) train an extractive summarization system with attention-based encoder-decoder RNNs to sequentially label summary-worth sentences in single documents. See et al. (2017), adopting an abstractive approach, augment the standard attention-based encoder-decoder RNNs with the ability to copy words from the source text via pointing and to kee"
K17-1045,hong-etal-2014-repository,0,0.102031,"repeated trials. Note that the axis is only displaying the interval - 7.0. thevertical four experiments. The vertical axis 4.0 displays the validation costs in the interval 4.0 - 7.0. art systems related to our regression method. We 1 Introduction compute ROUGE scores from the actual output Cosine PDG ADG Similarity summary of each system. We run the G-Flow code released by Christensen et al. (2013) to get Number of nodes 265 265 265 the output summary of the G-Flow system. The Number of edges 1023 1050 884 output summary of other systems are compiled in Average edge weight 0.075 0.295 0.359 Hong et al. (2014). To ensure fair comparison, we Average node degree 0.171 5.136 2.260 use ROUGE-1.5.5 with the same parameters as in Hong et al. (2014) across all methods: -n 2 -m -l ρ of degree and salience 0.136 0.113 0.093 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0. Table 5: Characteristics of the three graph repreFrom Table 3, we observe that our GCN syssentations, averaged over the clusters (i.e. graphs) tem significantly outperforms the commonly used in DUC 2004. Note that max edge weight in all baselines and traditional graph approaches such three representations is 1.0 due to rescaling for as Centroid, Lex"
K17-1045,P16-1046,0,0.319592,"mmary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (McDonald, 2007; Gillick and Favre, 2009; Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003; Barzilay et al., 2001) can follow to improve coherence of the summary. Recently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all the sentences in the same document cluster are processed independently. Hence, the relationships between sentences and thus the relationships between different documents are ignored. However, Christensen et al. (2013) demonstrates the importance of considering discourse relations among sentences in multi-document summarization. This work proposes a multi-document summarization system that exploits the rep"
K17-1045,E14-1075,0,0.456827,"idocument summarization systems. 1 Introduction Document summarization aims to produce fluent and coherent summaries covering salient information in the documents. Many previous summarization systems employ an extractive approach by identifying and concatenating the most salient text units (often whole sentences) in the document. Traditional extractive summarizers produce the summary in two steps: sentence ranking and sentence selection. First, they utilize humanengineered features such as sentence position and length (Radev et al., 2004a), word frequency and importance (Nenkova et al., 2006; Hong and Nenkova, 2014), among others, to rank sentence 452 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 452–462, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics employ encoder-decoder RNNs to effectively produce short abstractive summaries for opinions. Cao et al. (2016) develop a query-focused summarization system called AttSum which deals with saliency ranking and relevance ranking using query-attention-weighted CNNs. Very recently, thanks to the large scale news article datasets (Hermann et al., 2015), Cheng and L"
K17-1045,W14-1504,0,0.0931152,"Missing"
K17-1045,N13-1136,0,0.641372,"nks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all the sentences in the same document cluster are processed independently. Hence, the relationships between sentences and thus the relationships between different documents are ignored. However, Christensen et al. (2013) demonstrates the importance of considering discourse relations among sentences in multi-document summarization. This work proposes a multi-document summarization system that exploits the representational power of deep neural networks and the sentence relation information encoded in graph representations of document clusters. Specifically, we apply Graph Convolutional Networks (Kipf and Welling, 2017) on sentence relation graphs. First, we discuss three different techniques to produce sentence relation graphs, where nodes represent sentences in a cluster and edges capture the connections betwe"
K17-1045,P03-1069,0,0.0387029,"n Srinivasan1 Dragomir Radev1 1 Department of Computer Science, Yale University 2 The LNM Institute of Information Technology {michihiro.yasunaga,r.zhang,kshitijh.meelu}@yale.edu {ayush.original}@gmail.com {krishnan.srinivasan,dragomir.radev}@yale.edu Abstract salience. Then, they select summary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (McDonald, 2007; Gillick and Favre, 2009; Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003; Barzilay et al., 2001) can follow to improve coherence of the summary. Recently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all the sentences in the same document cluster are processed independently. Hence, the relationships between s"
K17-1045,P13-1099,0,0.0286741,"based Neural Multi-Document Summarization Michihiro Yasunaga1 Rui Zhang1 Kshitijh Meelu1 Ayush Pareek2 Krishnan Srinivasan1 Dragomir Radev1 1 Department of Computer Science, Yale University 2 The LNM Institute of Information Technology {michihiro.yasunaga,r.zhang,kshitijh.meelu}@yale.edu {ayush.original}@gmail.com {krishnan.srinivasan,dragomir.radev}@yale.edu Abstract salience. Then, they select summary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (McDonald, 2007; Gillick and Favre, 2009; Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003; Barzilay et al., 2001) can follow to improve coherence of the summary. Recently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all"
K17-1045,P06-2020,0,0.0973184,"Missing"
K17-1045,I05-2004,0,0.0184367,", and extract salient sentences in a greedy manner while avoiding redundancy. We evaluate our model on the DUC 2004 multidocument summarization (MDS) task. Our model shows a clear advantage over traditional graphbased extractive summarizers, as well as a baseline GRU model that does not use any graph, and achieves competitive results with other state-ofthe-art MDS systems. This work provides a new gateway to incorporating graph-based techniques into neural summarization. 2 2.1 Related Work Graph-based MDS Graph-based MDS models have traditionally employed surface level (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006) or deep level (Pardo et al., 2006; Antiqueira et al., 2009) approaches based on topological features and the number of nodes (Albert and Barab´asi, 2002). Efforts have been made to improve decision making of these systems by using discourse relationships between sentences (Radev, 2000; Radev et al., 2001). Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Mei et al. (2010) propose DivRank to balance the prestige and diversity of the top ranked vertices"
K17-1045,D15-1044,0,0.497344,"adev}@yale.edu Abstract salience. Then, they select summary-worthy sentences using a range of algorithms, such as graph centrality (Erkan and Radev, 2004), constraint optimization via Integer Linear Programming (McDonald, 2007; Gillick and Favre, 2009; Li et al., 2013), or Support Vector Regression (Li et al., 2007) algorithms. Optionally, sentence reordering (Lapata, 2003; Barzilay et al., 2001) can follow to improve coherence of the summary. Recently, thanks to their strong representation power, neural approaches have become popular in text summarization, especially in sentence compression (Rush et al., 2015) and single-document summarization (Cheng and Lapata, 2016). Despite their popularity, neural networks still have issues when dealing with multi-document summarization (MDS). In previous neural multi-document summarizers (Cao et al., 2015, 2017), all the sentences in the same document cluster are processed independently. Hence, the relationships between sentences and thus the relationships between different documents are ignored. However, Christensen et al. (2013) demonstrates the importance of considering discourse relations among sentences in multi-document summarization. This work proposes"
K17-1045,P17-1099,0,0.0387102,", August 3 - August 4, 2017. 2017 Association for Computational Linguistics employ encoder-decoder RNNs to effectively produce short abstractive summaries for opinions. Cao et al. (2016) develop a query-focused summarization system called AttSum which deals with saliency ranking and relevance ranking using query-attention-weighted CNNs. Very recently, thanks to the large scale news article datasets (Hermann et al., 2015), Cheng and Lapata (2016) train an extractive summarization system with attention-based encoder-decoder RNNs to sequentially label summary-worth sentences in single documents. See et al. (2017), adopting an abstractive approach, augment the standard attention-based encoder-decoder RNNs with the ability to copy words from the source text via pointing and to keep track of what has been summarized. These models (Cheng and Lapata, 2016; See et al., 2017) achieve state-of-the-art performance on the DUC 2002 single-document summarization task. However, scaling up these RNN sequence-to-sequence approaches to the multidocument summarization task has not been successful, 1) due to the lack of large multi-document summarization datasets needed to train the computationally expensive sequence-t"
K17-1045,N06-2046,0,0.0602558,"ences in a greedy manner while avoiding redundancy. We evaluate our model on the DUC 2004 multidocument summarization (MDS) task. Our model shows a clear advantage over traditional graphbased extractive summarizers, as well as a baseline GRU model that does not use any graph, and achieves competitive results with other state-ofthe-art MDS systems. This work provides a new gateway to incorporating graph-based techniques into neural summarization. 2 2.1 Related Work Graph-based MDS Graph-based MDS models have traditionally employed surface level (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006) or deep level (Pardo et al., 2006; Antiqueira et al., 2009) approaches based on topological features and the number of nodes (Albert and Barab´asi, 2002). Efforts have been made to improve decision making of these systems by using discourse relationships between sentences (Radev, 2000; Radev et al., 2001). Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Mei et al. (2010) propose DivRank to balance the prestige and diversity of the top ranked vertices in information networ"
K17-1045,N16-1007,0,0.0128408,"nt ) (Cho et al., 2014; Chung et al., 2014) and extract the last hidden state as the sentence embedding. We then apply Graph Convolutional Networks (Kipf and Welling, 2017) on the sentence relation graph with the sentence embeddings as the input node features, to produce final sentence embeddings that reflect the graph representation. Thereafter, a second level GRU (GRUdoc ) produces the entire cluster embedding Summarization Using Neural Networks Neural networks have recently been popular for text summarization (K˚ageb¨ack et al., 2014; Rush et al., 2015; Yin and Pei, 2015; Cao et al., 2016; Wang and Ling, 2016; Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017). For example, Rush et al. (2015) introduce a neural attention feed-forward network-based model for sentence compression. Wang and Ling (2016) 453 Sentences d1s1 doc1 GRUdoc Sentence Relation Graph h0 h1 GRUdoc h2 h0 h1 h2 d1s2 Cluster doc2 d2s2 C Sentence Embedding d2s1 h0 h1 h2 h3 h4 w1 w2 w3 . GRUsent Cluster Embedding Estimated Scores Graph Convolutional Networks Salience Estimation Figure 1: Illustration of our architecture for sentence salience estimation. In this example, there are two documents in the cluster and"
K17-1045,K16-1028,0,0.0389677,"nd extract the last hidden state as the sentence embedding. We then apply Graph Convolutional Networks (Kipf and Welling, 2017) on the sentence relation graph with the sentence embeddings as the input node features, to produce final sentence embeddings that reflect the graph representation. Thereafter, a second level GRU (GRUdoc ) produces the entire cluster embedding Summarization Using Neural Networks Neural networks have recently been popular for text summarization (K˚ageb¨ack et al., 2014; Rush et al., 2015; Yin and Pei, 2015; Cao et al., 2016; Wang and Ling, 2016; Cheng and Lapata, 2016; Nallapati et al., 2016, 2017; See et al., 2017). For example, Rush et al. (2015) introduce a neural attention feed-forward network-based model for sentence compression. Wang and Ling (2016) 453 Sentences d1s1 doc1 GRUdoc Sentence Relation Graph h0 h1 GRUdoc h2 h0 h1 h2 d1s2 Cluster doc2 d2s2 C Sentence Embedding d2s1 h0 h1 h2 h3 h4 w1 w2 w3 . GRUsent Cluster Embedding Estimated Scores Graph Convolutional Networks Salience Estimation Figure 1: Illustration of our architecture for sentence salience estimation. In this example, there are two documents in the cluster and each document has two sentences. Sentences are p"
K17-1045,W12-2601,0,0.0111739,"how mean (and standard deviation for R-1) over 10 repeated trials for each of our experiments. We use the benchmark data sets from the Document Understanding Conferences (DUC) containing clusters of English news articles and human reference summaries. Table 2 shows the statistics of the data sets. We use DUC 2001, 2002, 2003 and 2004 containing 30, 59, 30 and 50 clusters of nearly 10 documents each respectively. Our model is trained on DUC 2001 and 2002, validated on 2003, and tested on 2004. For evaluation, we use the ROUGE-1,2 metric, with stemming and stop words not removed as suggested by Owczarzak et al. (2012). 4.2 38.57 GRU layers (L = 3). The hidden states in GRUsent , GCN hidden layers, and GRUdoc are all 300dimensional vectors (D = F = 300). The rescaling factor α in the objective function (Eq 13) is chosen as 40 from {10, 20, 30, 40, 50, 100} based on the validation performance. The objective function is optimized using Adam (Kingma and Ba, 2015) stochastic gradient descent with a learning rate of 0.001 and a batch size of 1. We use gradient clipping with a maximum gradient norm of 1.0. The model is validated every 10 iterations, and the training is stopped early if the validation performance"
K17-1045,W00-1009,1,0.606346,"etitive results with other state-ofthe-art MDS systems. This work provides a new gateway to incorporating graph-based techniques into neural summarization. 2 2.1 Related Work Graph-based MDS Graph-based MDS models have traditionally employed surface level (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006) or deep level (Pardo et al., 2006; Antiqueira et al., 2009) approaches based on topological features and the number of nodes (Albert and Barab´asi, 2002). Efforts have been made to improve decision making of these systems by using discourse relationships between sentences (Radev, 2000; Radev et al., 2001). Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Mei et al. (2010) propose DivRank to balance the prestige and diversity of the top ranked vertices in information networks and achieve improved results on MDS. Christensen et al. (2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences (Mann and Thompson, 1988). In our work, we build on the Approximate Dis"
K17-1045,radev-etal-2004-mead,1,0.810013,"ph, and it achieves competitive results against other state-of-the-art multidocument summarization systems. 1 Introduction Document summarization aims to produce fluent and coherent summaries covering salient information in the documents. Many previous summarization systems employ an extractive approach by identifying and concatenating the most salient text units (often whole sentences) in the document. Traditional extractive summarizers produce the summary in two steps: sentence ranking and sentence selection. First, they utilize humanengineered features such as sentence position and length (Radev et al., 2004a), word frequency and importance (Nenkova et al., 2006; Hong and Nenkova, 2014), among others, to rank sentence 452 Proceedings of the 21st Conference on Computational Natural Language Learning (CoNLL 2017), pages 452–462, c Vancouver, Canada, August 3 - August 4, 2017. 2017 Association for Computational Linguistics employ encoder-decoder RNNs to effectively produce short abstractive summaries for opinions. Cao et al. (2016) develop a query-focused summarization system called AttSum which deals with saliency ranking and relevance ranking using query-attention-weighted CNNs. Very recently, tha"
K17-1045,H01-1056,1,0.539887,"ts with other state-ofthe-art MDS systems. This work provides a new gateway to incorporating graph-based techniques into neural summarization. 2 2.1 Related Work Graph-based MDS Graph-based MDS models have traditionally employed surface level (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wan and Yang, 2006) or deep level (Pardo et al., 2006; Antiqueira et al., 2009) approaches based on topological features and the number of nodes (Albert and Barab´asi, 2002). Efforts have been made to improve decision making of these systems by using discourse relationships between sentences (Radev, 2000; Radev et al., 2001). Erkan and Radev (2004) introduce LexRank to compute sentence importance based on the eigenvector centrality in the connectivity graph of inter-sentence cosine similarity. Mei et al. (2010) propose DivRank to balance the prestige and diversity of the top ranked vertices in information networks and achieve improved results on MDS. Christensen et al. (2013) build multi-document graphs to identify pairwise ordering constraints over the sentences by accounting for discourse relationships between sentences (Mann and Thompson, 1988). In our work, we build on the Approximate Discourse Graph (ADG) mo"
L16-1076,P05-1022,0,0.259903,"Missing"
L16-1076,P12-2071,0,0.0384176,"Missing"
L16-1076,P13-2045,1,0.879947,"Missing"
L16-1076,P14-5010,0,0.00506755,"Missing"
L16-1076,H05-1067,0,0.613636,"to gain insights into what differentiates funny captions from the rest. We developed a set of unsupervised methods for ranking captions based on features such as originality, centrality, sentiment, concreteness, grammaticality, humancenteredness, etc. We used each of these methods to independently rank all captions from our corpus and selected the top captions for each method. Then, we performed Amazon Mechanical Turk experiments in which we asked Turkers to judge which of the selected captions is funnier. Figure 1: Cartoon number 31 Figure 2: Cartoon number 32 2. Related Work In early work, Mihalcea and Strapparava (2005) investigate whether classification techniques can distinguish between humorous and non-humorous text. Training data consisted of humorous one-liners (15 words or less), and non-humorous one-liners, which are derived from Reuters news titles, proverbs, and sentences from the British National Corpus. They looked at features such as alliteration, antonymy and adult slang. Mihalcea and Pullman (2007) took this work further. They looked at four semantic classes relevant to humancenteredness: persons, social groups, social relationships, and personal pronouns. They showed that social relationships"
L16-1076,N12-2012,0,0.0309884,"Missing"
L16-1452,J05-3002,0,0.0660722,"ed to be important. Like most of the other graph-based studies, LexRank uses cosine similarity based on the tf-idf metric to measure the similarities among the nodes in a sentence graph. Yet, these methods treat sentences as bags of words. This representation may fail to capture some of the semantically related information, which in turn may affect the summary quality negatively. We propose utilizing dependency grammars for sentence similarity computation in MDS. In the literature, dependency parsing has been used to find common information among sentences in order to perform sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008) and to detect uninformative parts of sentences for the task of sentence compression (Yousfi-Monod et al., 2008; Blake et al., 2007). Dependency grammars have also been used for identifying concepts in specific domain terminologies by matching noun phrases to domain specific vocabularies (Fiszman et al., 2004), and for opinion summa2833 rization (Zhuang et al., 2006; Somprasertsri and Lalitrojwong, 2010). In addition, dependency parsing has been used to align sentences in documents with their human generated summaries in (Hirao et al., 2004) and to generate a depen"
L16-1452,P04-1054,0,0.706863,"of words model is sometimes inadequate for capturing the syntactic and semantic similarities among the sentences, which may affect the qualities of the summaries. To address this problem, we propose to employ dependency grammars, which represent the syntactic dependencies among the words in a sentence, for sentence similarity computation in MDS. Using this approach, concepts in multiple documents and relations between similar contents can be captured. In this study, we first adapt two dependency tree based sentence similarity kernels in order to use them in MDS. These kernels are proposed by Culotta and Sorensen (2004) and Choi and Kim (2013) respectively, for relation extraction. They do not take the dependency relation types into account while calculating sentence similarity. We then propose a series of new sentence similarity kernels based on typed dependency grammars and test these kernels on LexRank (Erkan and Radev, 2004), a well-known and publicly available MDS system, by replacing its tf-idf based cosine similarity method with each of the kernels. The proposed similarity kernels make use of the binary dependency relations in the sentences. We conduct experiments on DUC 2003 and DUC 2004 Task 2 data"
L16-1452,D08-1019,0,0.0227746,"t of the other graph-based studies, LexRank uses cosine similarity based on the tf-idf metric to measure the similarities among the nodes in a sentence graph. Yet, these methods treat sentences as bags of words. This representation may fail to capture some of the semantically related information, which in turn may affect the summary quality negatively. We propose utilizing dependency grammars for sentence similarity computation in MDS. In the literature, dependency parsing has been used to find common information among sentences in order to perform sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008) and to detect uninformative parts of sentences for the task of sentence compression (Yousfi-Monod et al., 2008; Blake et al., 2007). Dependency grammars have also been used for identifying concepts in specific domain terminologies by matching noun phrases to domain specific vocabularies (Fiszman et al., 2004), and for opinion summa2833 rization (Zhuang et al., 2006; Somprasertsri and Lalitrojwong, 2010). In addition, dependency parsing has been used to align sentences in documents with their human generated summaries in (Hirao et al., 2004) and to generate a dependency-based language model fo"
L16-1452,W04-2611,0,0.0253086,"ffect the summary quality negatively. We propose utilizing dependency grammars for sentence similarity computation in MDS. In the literature, dependency parsing has been used to find common information among sentences in order to perform sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008) and to detect uninformative parts of sentences for the task of sentence compression (Yousfi-Monod et al., 2008; Blake et al., 2007). Dependency grammars have also been used for identifying concepts in specific domain terminologies by matching noun phrases to domain specific vocabularies (Fiszman et al., 2004), and for opinion summa2833 rization (Zhuang et al., 2006; Somprasertsri and Lalitrojwong, 2010). In addition, dependency parsing has been used to align sentences in documents with their human generated summaries in (Hirao et al., 2004) and to generate a dependency-based language model for Information Retrieval as in (Gao et al., 2004). To the best of our knowledge, none of the previous studies have used the dependency grammar concept to compute sentence similarity in a text summarization approach. 3. Methodology Dependency tree representations of sentences allow us to utilize the syntactic de"
L16-1452,R09-1028,0,0.0283165,"nk (Erkan and Radev, 2004), a well-known and publicly available MDS system, by replacing its tf-idf based cosine similarity method with each of the kernels. The proposed similarity kernels make use of the binary dependency relations in the sentences. We conduct experiments on DUC 2003 and DUC 2004 Task 2 data sets. The results show that the best one of the proposed methods outperforms the other untyped tree kernels and LexRank’s own sentence similarity 2. Related Work Several methods including supervised approaches (Das and Martins, 2007; Pei et al., 2012), topic driven models (Nastase, 2008; Hennig and Labor, 2009; Wang et al., 2009), and clustering based models (Radev et al., 2004; Aliguliyev, 2010) have been proposed in the literature for MDS. Recently, graph-based summarization methods have attracted the increasing attention of researchers (Erkan and Radev, 2004; Wan and Yang, 2008; Shen and Li, 2010) and have been successful when compared to the other state of the art summarization approaches (Mihalcea, 2004). Graph-based methods represent documents as a graph, where vertices are sentences and edges denote the similarity between the correponding pairs of sentences. LexRank (Erkan and Radev, 2004) i"
L16-1452,C04-1064,0,0.01159,"sentence fusion (Barzilay and McKeown, 2005; Filippova and Strube, 2008) and to detect uninformative parts of sentences for the task of sentence compression (Yousfi-Monod et al., 2008; Blake et al., 2007). Dependency grammars have also been used for identifying concepts in specific domain terminologies by matching noun phrases to domain specific vocabularies (Fiszman et al., 2004), and for opinion summa2833 rization (Zhuang et al., 2006; Somprasertsri and Lalitrojwong, 2010). In addition, dependency parsing has been used to align sentences in documents with their human generated summaries in (Hirao et al., 2004) and to generate a dependency-based language model for Information Retrieval as in (Gao et al., 2004). To the best of our knowledge, none of the previous studies have used the dependency grammar concept to compute sentence similarity in a text summarization approach. 3. Methodology Dependency tree representations of sentences allow us to utilize the syntactic dependency relations among words. Therefore, it is a more powerful approach than the bag of words representation for modeling the syntactic and semantic information in sentences. Considering this strength of dependency grammars, we first"
L16-1452,P03-1054,0,0.00580853,"Rank method with Position and Length features, and used the Cross-Sentence Informational Subsumption (CSIS) reranker (Radev et al., 2004). Since we tested our similarity kernels in the LexRank system by replacing its own sentence similarity method, we used their environment in our experiments. We used the Dragon Toolkit4 , which is a development package for Information Retrieval and Text Mining (Zhou et al., 2007), to develop the dependency grammar kernels. At the preprocessing phase, we first generated the dependency parse trees of the sentences in our data sets by using the Stanford Parser (Klein and Manning, 2003). Then, the feature set given in (Culotta and Sorensen, 2004) was created for each node in the tree. The word, POS tag, and general POS tag features were generated using the Stanford Parser. The entity type feature was created using the Stanford Named Entity Recognizer (NER) tool which is available in the Stanford CoreNLP Package5 .The WordNet hypernyms feature was generated using JAWS (Java API for WordNet Searching) (Spell, 2009). All words were stemmed using the Porter Stemmer (Porter, 1980). For the bigram kernels, we filtered the dependency relation types in Table 1 as they did not improv"
L16-1452,P11-1052,0,0.0127091,"the tf-idf measure is effective at highlighting the importance of a word. The experimental results show that representing sentences as a set of their dependency bigram relations is a more effective approach than the bag-of-words representation model for sentence similarity computation in the MDS task. We compared the ROUGE-1 scores of our methods with the best system on DUC 2004 (Conroy et al., 2004) and one of the recent state-of-the-art methods, the submodular func6 ROUGE version 1.5.5 with following options: -n 2 -m -w 1.2 -b 665 -c 95 -r 1000 -f A -p 0.5 -t 0 -2 4 -u -a -d tions approach (Lin and Bilmes, 2011). Our MSK and CK kernels achieve similar performances with the submodular functions method and perform better than the best system on DUC 2004. DTK and Tri-K obtain better performances than the leadbased method on both data sets. However, these untyped dependency tree based approaches failed to achieve higher scores than the original similarity method of the LexRank system. 5. Conclusion We presented sentence similarity computation methods for MDS based on the dependency parse trees of the sentences. We adapted two different dependency tree based sentence similarity kernels, which have origina"
L16-1452,I05-2004,0,0.0460271,"milarity for multi-document summarization. Keywords: multi-document summarization, sentence similarity, dependency grammars 1. Introduction method in terms of ROUGE-1 and ROUGE-2 scores. Multi-document summarization (MDS), which refers to the task of automatically generating a summary of multiple documents about the same topic without losing the most important information, is one of the most promising solutions proposed to overcome the information overload problem (Li et al., 2007). Sentence similarity calculation is a crucial task for many extractive approaches to MDS (Erkan and Radev, 2004; Mihalcea and Tarau, 2005; Wang et al., 2008; Aliguliyev, 2009). Most of them use a bag of words model to compute sentence similarity. However, the bag of words model is sometimes inadequate for capturing the syntactic and semantic similarities among the sentences, which may affect the qualities of the summaries. To address this problem, we propose to employ dependency grammars, which represent the syntactic dependencies among the words in a sentence, for sentence similarity computation in MDS. Using this approach, concepts in multiple documents and relations between similar contents can be captured. In this study, we"
L16-1452,P04-3020,0,0.0494271,"nels and LexRank’s own sentence similarity 2. Related Work Several methods including supervised approaches (Das and Martins, 2007; Pei et al., 2012), topic driven models (Nastase, 2008; Hennig and Labor, 2009; Wang et al., 2009), and clustering based models (Radev et al., 2004; Aliguliyev, 2010) have been proposed in the literature for MDS. Recently, graph-based summarization methods have attracted the increasing attention of researchers (Erkan and Radev, 2004; Wan and Yang, 2008; Shen and Li, 2010) and have been successful when compared to the other state of the art summarization approaches (Mihalcea, 2004). Graph-based methods represent documents as a graph, where vertices are sentences and edges denote the similarity between the correponding pairs of sentences. LexRank (Erkan and Radev, 2004) is one of the most salient graph-based methods for MDS. Here the general idea is that sentences that have connections to many other significant sentences are considered to be important. Like most of the other graph-based studies, LexRank uses cosine similarity based on the tf-idf metric to measure the similarities among the nodes in a sentence graph. Yet, these methods treat sentences as bags of words. Th"
L16-1452,D08-1080,0,0.0185809,"ernels on LexRank (Erkan and Radev, 2004), a well-known and publicly available MDS system, by replacing its tf-idf based cosine similarity method with each of the kernels. The proposed similarity kernels make use of the binary dependency relations in the sentences. We conduct experiments on DUC 2003 and DUC 2004 Task 2 data sets. The results show that the best one of the proposed methods outperforms the other untyped tree kernels and LexRank’s own sentence similarity 2. Related Work Several methods including supervised approaches (Das and Martins, 2007; Pei et al., 2012), topic driven models (Nastase, 2008; Hennig and Labor, 2009; Wang et al., 2009), and clustering based models (Radev et al., 2004; Aliguliyev, 2010) have been proposed in the literature for MDS. Recently, graph-based summarization methods have attracted the increasing attention of researchers (Erkan and Radev, 2004; Wan and Yang, 2008; Shen and Li, 2010) and have been successful when compared to the other state of the art summarization approaches (Mihalcea, 2004). Graph-based methods represent documents as a graph, where vertices are sentences and edges denote the similarity between the correponding pairs of sentences. LexRank ("
L16-1452,C12-1136,0,0.0198896,"ped dependency grammars and test these kernels on LexRank (Erkan and Radev, 2004), a well-known and publicly available MDS system, by replacing its tf-idf based cosine similarity method with each of the kernels. The proposed similarity kernels make use of the binary dependency relations in the sentences. We conduct experiments on DUC 2003 and DUC 2004 Task 2 data sets. The results show that the best one of the proposed methods outperforms the other untyped tree kernels and LexRank’s own sentence similarity 2. Related Work Several methods including supervised approaches (Das and Martins, 2007; Pei et al., 2012), topic driven models (Nastase, 2008; Hennig and Labor, 2009; Wang et al., 2009), and clustering based models (Radev et al., 2004; Aliguliyev, 2010) have been proposed in the literature for MDS. Recently, graph-based summarization methods have attracted the increasing attention of researchers (Erkan and Radev, 2004; Wan and Yang, 2008; Shen and Li, 2010) and have been successful when compared to the other state of the art summarization approaches (Mihalcea, 2004). Graph-based methods represent documents as a graph, where vertices are sentences and edges denote the similarity between the correp"
L16-1452,C10-1111,0,0.0235686,"ask 2 data sets. The results show that the best one of the proposed methods outperforms the other untyped tree kernels and LexRank’s own sentence similarity 2. Related Work Several methods including supervised approaches (Das and Martins, 2007; Pei et al., 2012), topic driven models (Nastase, 2008; Hennig and Labor, 2009; Wang et al., 2009), and clustering based models (Radev et al., 2004; Aliguliyev, 2010) have been proposed in the literature for MDS. Recently, graph-based summarization methods have attracted the increasing attention of researchers (Erkan and Radev, 2004; Wan and Yang, 2008; Shen and Li, 2010) and have been successful when compared to the other state of the art summarization approaches (Mihalcea, 2004). Graph-based methods represent documents as a graph, where vertices are sentences and edges denote the similarity between the correponding pairs of sentences. LexRank (Erkan and Radev, 2004) is one of the most salient graph-based methods for MDS. Here the general idea is that sentences that have connections to many other significant sentences are considered to be important. Like most of the other graph-based studies, LexRank uses cosine similarity based on the tf-idf metric to measur"
L16-1452,P09-2075,0,0.0356771,"4), a well-known and publicly available MDS system, by replacing its tf-idf based cosine similarity method with each of the kernels. The proposed similarity kernels make use of the binary dependency relations in the sentences. We conduct experiments on DUC 2003 and DUC 2004 Task 2 data sets. The results show that the best one of the proposed methods outperforms the other untyped tree kernels and LexRank’s own sentence similarity 2. Related Work Several methods including supervised approaches (Das and Martins, 2007; Pei et al., 2012), topic driven models (Nastase, 2008; Hennig and Labor, 2009; Wang et al., 2009), and clustering based models (Radev et al., 2004; Aliguliyev, 2010) have been proposed in the literature for MDS. Recently, graph-based summarization methods have attracted the increasing attention of researchers (Erkan and Radev, 2004; Wan and Yang, 2008; Shen and Li, 2010) and have been successful when compared to the other state of the art summarization approaches (Mihalcea, 2004). Graph-based methods represent documents as a graph, where vertices are sentences and edges denote the similarity between the correponding pairs of sentences. LexRank (Erkan and Radev, 2004) is one of the most sa"
L16-1452,C08-2035,0,0.0321339,"Missing"
L16-1493,E09-1005,0,0.0698683,"Missing"
L16-1493,P05-1018,0,0.0189283,"nted as a set {x1 , . . . , xn } of n sentences. An extractive summary S is composed of sentences from this document, i.e., S ⊆ D. For any document, we seek to recover the highest-scoring summary Sb which can fits within a pre-determined budget b. Sb = arg max score(S, D) (1) S⊆D s.t. cost(S) &lt; b The tractability of this inference formulation depends on the factorization of the function score over the summary. In this work, we forego exact solutions to (1) in order to accommodate richer scoring functions that can model phenomena such as diversity (Carbonell and Goldstein, 1998) and coherence (Barzilay and Lapata, 2005; Christensen et al., 2013). Summaries are scored using a linear model score(S, D) = w> Φ(S, D) (2) where Φ is a feature map for summaries and w is a vector of learned parameters. In order to train the parameters w, we assume the existence of a training dataset D comprised of instances hD, S ∗ i Algorithm 1 Structured perceptron (Collins, 2002) Input: training dataset D, feature map Φ, learning rate η Output: vector of learned parameters w 1: w0 ← 0|Φ| 2: k ← 0 3: while not converged do 4: for instance hD, S ∗ i ∈ D do 5: Sb ← arg maxS wk> Φ(D, S) 6: if Sb 6= S ∗ then   b 7: wk+1 ← wk + η Φ("
L16-1493,N13-1136,0,0.0202871,"xn } of n sentences. An extractive summary S is composed of sentences from this document, i.e., S ⊆ D. For any document, we seek to recover the highest-scoring summary Sb which can fits within a pre-determined budget b. Sb = arg max score(S, D) (1) S⊆D s.t. cost(S) &lt; b The tractability of this inference formulation depends on the factorization of the function score over the summary. In this work, we forego exact solutions to (1) in order to accommodate richer scoring functions that can model phenomena such as diversity (Carbonell and Goldstein, 1998) and coherence (Barzilay and Lapata, 2005; Christensen et al., 2013). Summaries are scored using a linear model score(S, D) = w> Φ(S, D) (2) where Φ is a feature map for summaries and w is a vector of learned parameters. In order to train the parameters w, we assume the existence of a training dataset D comprised of instances hD, S ∗ i Algorithm 1 Structured perceptron (Collins, 2002) Input: training dataset D, feature map Φ, learning rate η Output: vector of learned parameters w 1: w0 ← 0|Φ| 2: k ← 0 3: while not converged do 4: for instance hD, S ∗ i ∈ D do 5: Sb ← arg maxS wk> Φ(D, S) 6: if Sb 6= S ∗ then   b 7: wk+1 ← wk + η Φ(D, S ∗ ) − Φ(D, S) 8: k ←k+"
L16-1493,W02-1001,0,0.295561,"information (Callan, 1994). Although this approach is simple and scalable, document style and structural differences when changing domains or publishers can significantly affect snippet quality. 3. System Our system takes as input an HTML document. We automatically extract the article text from the HTML, and then automatically preprocess the text to obtain sentences, tokens and part of speech tags. Then, we compute various features over the preprocessed document. Each sentence is scored using a combination of feature values and feature weights, which are learned using a structured perceptron (Collins, 2002). Finally, sentences are extracted in a greedy fashion based on their scores while respecting the length constraint. These steps are illustrated in Figure 2. 3.1. Features We implemented various features drawn from the summarization literature that capture aspects of salience, diversity, coverage, content and readability. Table 1 presents features from each category implemented in our framework. 3089 Category Position Length Content Lexical cues Syntactic cues Examples sentence position, paragraph position, in-paragraph position word length, character length, summary length similarity to query"
L16-1493,P06-1039,0,0.0790576,"Missing"
L16-1493,hong-etal-2014-repository,0,0.022483,"cument summarization has focused on scoring, ranking, and extracting the most “informative” sentences from a document using various supervised (e.g. (Conroy et al., 2004; Daum´e and Marcu, 2006; Lin, 1999; Svore et al., 2007)) and unsupervised (e.g. (Erkan and Radev, 2004; Mei et al., 2010; Mihalcea and Radev, 2011)) methods. Innovations fall into two broad categories: (a) finding ways to assess whether a sentence should be included in a summary; and (b) efficient algorithms for exploring the space of possible summaries. A recent study compared a number of extractive summarization algorithms (Hong et al., 2014). The best performing algorithm performed global optimization over the input sentence set. However, these algorithms were compared using the DUC 2004 task, (a) which is a multi-document summarization task; and (b) for which the reference summaries were abstractive. † Equal contribution. Figure 1: Our summarization system More related to our framework, snippet extraction is a popular approach for search engines and news aggregators to show some content related to the query and the original document (Li et al., 2008). A simple way to identify snippets is to extract a passage from specific areas"
L16-1493,N12-1015,0,0.0159613,"∗ i ∈ D do 5: Sb ← arg maxS wk> Φ(D, S) 6: if Sb 6= S ∗ then   b 7: wk+1 ← wk + η Φ(D, S ∗ ) − Φ(D, S) 8: k ←k+1 P return average weights k1 j wj where S ∗ represents a reference summary for document D. The parameters are estimated using the structured perceptron (Collins, 2002) which minimizes a 0/1 loss over D and incorporates parameter averaging for generalization. The basic learning procedure is described in Algorithm 1. When inference is inexact and carried out via search—as in the case of our framework—convergence and performance can be improved using violation-fixing weight updates (Huang and Feyong, 2012). In addition to greedy search, we also experimented with beams of various sizes to reduce search errors but did not observe performance improvements1 . 4. 4.1. Experiments - Systems Compared Baselines and Other Methods We compare the extractive summaries generated by our framework to three simple baselines (Lead-based and two variations of Greedy) as well as to an array of standard summarization methods from the literature. Lead-based The baseline, lead-based, algorithm takes the first sentences in the document that fit within the budget. Greedy The greedy algorithm is exactly the one describ"
L16-1493,W04-1013,0,0.0124222,"ntence to the document, computed using cosine similarity over term frequency vectors) and a binary feature— computed on the fly—to identify when a candidate sentence is adjacent to a sentence already present in the summary. The scoring function we used for the greedy and knapsack algorithms uses the same features. 4.3. Automatic Evaluation For evaluation, we produced 300-character extractive summaries from the input documents by passing them through the systems described earlier. We evaluate the performance of all systems against manually creative extractive reference summaries. We use ROUGE (Lin, 2004), a well-established automatic evaluation metric based on lexical overlap which has been widely used in the scientific community and has been shown to correlate well with human evaluations. We follow the standards suggested by Owczarzak et al. (2012). As is standard, we report ROUGE R-1, R-2 and R-4. Table 2 shows evaluation results. The lead-based baseline outperforms all the methods from the literature that we included. However, our framework outperforms this baseline. Analyzing the reference summaries, we observed that they are primarily lead-based unless one of the first sentences in the i"
L16-1493,W12-2601,0,0.0322969,"unction we used for the greedy and knapsack algorithms uses the same features. 4.3. Automatic Evaluation For evaluation, we produced 300-character extractive summaries from the input documents by passing them through the systems described earlier. We evaluate the performance of all systems against manually creative extractive reference summaries. We use ROUGE (Lin, 2004), a well-established automatic evaluation metric based on lexical overlap which has been widely used in the scientific community and has been shown to correlate well with human evaluations. We follow the standards suggested by Owczarzak et al. (2012). As is standard, we report ROUGE R-1, R-2 and R-4. Table 2 shows evaluation results. The lead-based baseline outperforms all the methods from the literature that we included. However, our framework outperforms this baseline. Analyzing the reference summaries, we observed that they are primarily lead-based unless one of the first sentences in the input document is a result of errors in article extraction from HTML (e.g. a byline is extracted as part of the article, or a photo caption is included as part of the article) or a repetition of the article title. 5. Experiments - Side by Side Editori"
L16-1493,D07-1047,0,0.0250912,"s without sacrificing meaning or grammaticality. We present evaluation results for the single-document news summarization use case, comparing performance on well written articles as well as on a sampling of news articles of random quality. 2. Related Work Research on single-document, extractive summarization has been conducted since the 1950s (Luhn, 1958). Traditionally, extractive single document summarization has focused on scoring, ranking, and extracting the most “informative” sentences from a document using various supervised (e.g. (Conroy et al., 2004; Daum´e and Marcu, 2006; Lin, 1999; Svore et al., 2007)) and unsupervised (e.g. (Erkan and Radev, 2004; Mei et al., 2010; Mihalcea and Radev, 2011)) methods. Innovations fall into two broad categories: (a) finding ways to assess whether a sentence should be included in a summary; and (b) efficient algorithms for exploring the space of possible summaries. A recent study compared a number of extractive summarization algorithms (Hong et al., 2014). The best performing algorithm performed global optimization over the input sentence set. However, these algorithms were compared using the DUC 2004 task, (a) which is a multi-document summarization task; a"
L16-1493,E09-1089,0,0.0268594,"ard MEAD, we implemented a variation in which the first sentence in the document is required to be in the summary (S1 + MEAD). Divrank (Mei et al., 2010) is based on a reinforced random walk over a lexical similarity graph. This model automatically balances the prestige and the diversity of the top ranked vertices in a principled way. Lexrank (Erkan and Radev, 2004) is based on eigenvector similarity over a lexical similarity graph. The nodes of the graph correspond to input sentences and the edges to weighted cosine similarity. The most central sentences are selected for the summary. MaxCov (Takamura and Okumura, 2009) applies approximation algorithms for the budgeted maximum coverage problem (Khuller et al., 1999) to document summarization. It assumes the existence of a vocabulary in which each word is associated with some positive profit (word score in the summary). Given a collection of subsets of this vocabulary (sentences), each associated with some cost (number of characters), the budgeted maximum coverage problem identifies a summary whose total cost remains within the budget and whose union maximizes the summary score. Personalized PageRank (PPR) (Agirre and Soroa, 2009) is a variation of the pagera"
L16-1493,J08-1001,0,\N,Missing
N04-1021,J00-1004,0,0.0540744,"Missing"
N04-1021,J93-2003,0,0.0210066,"e different types of features, including features based on syntactic analyses of the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear"
N04-1021,P03-1011,1,0.218398,"of words in tree fragment and k for maximum height of with some probability, to transform one tree into another. tree fragment. We proceed from left to right in the ChiHowever, when training the model, trees for both the nese sentence and incrementally grow a pair of subtrees, source and target languages are provided, in our case one subtree in Chinese and the other in English, such that from the Chinese and English parsers. each word in the Chinese subtree is aligned to a word in We began with the tree-to-tree alignment model prethe English subtree. We grow this pair of subtrees unsented by Gildea (2003). The model was extended to hantil we can no longer grow either subtree without violatdle dependency trees, and to make use of the word-level ing the two parameter values n and k. Note that these alignments produced by the baseline MT system. The aligned subtree pairs have properties similar to alignment probability assigned by the tree-to-tree alignment model, templates. They can rearrange in complex ways between given the word-level alignment with which the candidate source and target. Figure 2 shows how subtree-pairs for translation was generated, was used as a feature in our parameters n ="
N04-1021,P03-1021,1,0.129951,"nsisting of S sentence pairs {(fs , es ) : s = 1, . . . , S}. However, this does not guarantee optimal performance on the metric of translation quality by which our system will ultimately be evaluated. For this reason, we optimize the parameters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment te"
N04-1021,P02-1038,1,0.203592,"the source and target sentences, which we hope will address the grammaticality of the translations, as well as lower-level features. As we work on n-best lists, we can easily use global sentence-level features. We begin by describing our baseline system and the n-best rescoring framework within which we conducted our experiments. We then present a selection of new features, progressing from word-level features to those based Anoop Sarkar Simon Fraser U. As an alternative to the often used source-channel approach (Brown et al., 1993), we directly model the posterior probability P r(eI1 |f1J ) (Och and Ney, 2002) using a log-linear combination of feature functions. In this framework, we have a set of M feature functions hm (eI1 , f1J ), m = 1, . . . , M . For each feature function, there exists a model parameter λm , m = 1, . . . , M . The direct translation probability is given by: PM exp[ m=1 λm hm (eI1 , f1J )] I J P r(e1 |f1 ) = P (2) PM J 0I e0I exp[ m=1 λm hm (e 1 , f1 )] 1 We obtain the following decision rule: eˆI1 = argmax eI1 M nX o λm hm (eI1 , f1J ) (3) m=1 The standard criterion for training such a log-linear model is to maximize the probability of the parallel training corpus consisting"
N04-1021,J04-4002,1,0.260908,"ters directly against the BLEU metric on held-out data. This is a more difficult optimization problem, as the search space is no longer convex. Figure 1: Example segmentation of Chinese sentence and its English translation into alignment templates. However, certain properties of the BLEU metric can be exploited to speed up search, as described in detail by Och (2003). We use this method of optimizing feature weights throughout this paper. 2.1 Baseline MT System: Alignment Templates Our baseline MT system is the alignment template system described in detail by Och, Tillmann, and Ney (1999) and Och and Ney (2004). In the following, we give a short description of this baseline model. The probability model of the alignment template system for translating a sentence can be thought of in distinct stages. First, the source sentence words f1J are grouped to phrases f˜1K . For each phrase f˜ an alignment template z is chosen and the sequence of chosen alignment templates is reordered (according to π1K ). Then, every phrase f˜ produces its translation e˜ (using the corresponding alignment template z). Finally, the sequence of phrases e˜K 1 constitutes the sequence of words eI1 . Our baseline system incorporat"
N04-1021,W99-0604,1,0.236444,"Missing"
N04-1021,W03-1002,0,0.0153043,"of the AT touches the upper right corner of the previous AT and the first word in the current AT immediately follows the last word in the previous AT. The total probability is the product over all alignment templates i, either P (ATi is right-continuous) or 1 − P (ATi is right-continuous). In both models, the probabilities P have been estimated from the full training data (train). 5 Shallow Syntactic Feature Functions By shallow syntax, we mean the output of the part-ofspeech tagger and chunkers. We hope that such features can combine the strengths of tag- and chunk-based translation systems (Schafer and Yarowsky, 2003) with our baseline system. 5.1 Projected POS Language Model This feature uses Chinese POS tag sequences as surrogates for Chinese words to model movement. Chinese words are too sparse to model movement, but an attempt to model movement using Chinese POS may be more successful. We hope that this feature will compensate for a weak model of word movement in the baseline system. Chinese POS sequences are projected to English using the word alignment. Relative positions are indicated for each Chinese tag. The feature function was also tried without the relative positions: CD +0 M +1 NN +3 NN -1 NN"
N04-1021,C96-2141,0,0.254737,"Missing"
N04-1021,P98-2230,0,0.0760244,"Missing"
N04-1021,P01-1067,1,0.0551165,"ical human translations. One reason for that is that the MT output uses fewer unseen words and typically more frequent words which lead to a higher language model probability. We also performed experiments to balance this effect by dividing the parser probability by the word unigram probability and using this ’normalized parser probability’ as a feature function, but also this did not yield improvements. 6.2 Tree-to-String Alignment A tree-to-string model is one of several syntaxbased translation models used. The model is a conditional probability p(f |T (e)). Here, we used a model defined by Yamada and Knight (2001) and Yamada and Knight (2002). Internally, the model performs three types of operations on each node of a parse tree. First, it reorders the child nodes, such as changing VP → VB NP PP into VP → NP PP VB. Second, it inserts an optional word at each node. Third, it translates the leaf English words into Chinese words. These operations are stochastic and their probabilities are assumed to depend only on the node, and are independent of other operations on the node, or other nodes. The probability of each operation is automatically obtained by a training algorithm, using about 780,000 English par"
N04-1021,C98-2225,0,\N,Missing
N04-3007,J98-3005,1,\N,Missing
N04-3007,W00-0403,1,\N,Missing
N04-3007,H01-1056,1,\N,Missing
N04-4031,C00-1027,0,\N,Missing
N09-1066,kan-etal-2002-using,0,0.273112,"citing article. Elkiss et al. (2008b) conducted several experiments on a set of 2, 497 articles from the free PubMed Central (PMC) repository.1 Results from this experiment confirmed that the cohesion of a citation text of an article is consistently higher than the that of its abstract. They also concluded that citation texts contain additional information are more focused than abstracts. Nakov et al. (2004) use sentences surrounding citations to create training and testing data for semantic analysis, synonym set creation, database curation, document summarization, and information retrieval. Kan et al. (2002) use annotated bibliographies to cover certain aspects of summarization and suggest using metadata and critical document features as well as the prominent content-based features to summarize documents. Kupiec et al. (1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization. Siddharthan and Teufel (2007) describe a new reference task and show high human agreement as well as an improvement in the performance of argumentative zoning (Teufel, 2005). In argumentative zoning—a rhetorical classification task—seven 1 http://www"
N09-1066,P03-1054,0,0.00253884,"rization system by generating multiple alternative sentence compressions of the most important sentences in target documents (Zajic et al., 2007). Trimmer compressions are generated by applying linguistically-motivated rules to mask syntactic components of a parse of a source sentence. The rules can be applied iteratively to compress sentences below a configurable length threshold, or can be applied in all combinations to generate the full space of compressions. Trimmer can leverage the output of any constituency parser that uses the Penn Treebank conventions. At present, the Stanford Parser (Klein and Manning, 2003) is used. The set of compressions is ranked according to a set of features that may include metadata about the source sentences, details of the compression process that generated the compression, and externally calculated features of the compression. Summaries are constructed from the highest scoring compressions, using the metadata and maximal marginal relevance (Carbonell and Goldstein, 1998) to avoid redundancy and over-representation of a single source. 587 LexRank We also used LexRank (Erkan and Radev, 2004), a state-of-the-art multidocument summarization system, to generate summaries. Le"
N09-1066,W04-1013,0,0.0666638,"mentative zoning—a rhetorical classification task—seven 1 http://www.pubmedcentral.gov classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to their role in the author’s argument. Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts—comparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Lin, 2004). 4.2 4 The salience of a node is recursively defined on the salience of adjacent nodes. This is similar to the concept of prestige in social networks, where the prestige of a person is dependent on the prestige of the people he/she knows. However, since random walk may get caught in cycles or in disconnected components, we reserve a low probability to jump to random nodes instead of neighbors (a technique suggested by Langville and Meyer (2006)). Summarization systems We used four summarization systems for our survey-creation approach: Trimmer, LexRank, CLexRank, and C-RR. Trimmer is a syntac"
N09-1066,P08-1093,0,0.371637,"ribing our experiments with technical papers, abstracts, and citation texts, we first summarize relevant prior work that used these sources of information as input. 3 Related work Previous work has focused on the analysis of citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002). Bradshaw (2003) used citation texts to determine the content of articles and improve the results of a search engine. Citation 586 texts have also been used to create summaries of single scientific articles in Qazvinian and Radev (2008) and Mei and Zhai (2008). However, there is no previous work that uses the text of the citations to produce a multi-document survey of scientific articles. Furthermore, there is no study contrasting the quality of surveys generated from citation summaries— both automatically and manually produced—to surveys generated from other forms of input such as the abstracts or full texts of the source articles. Nanba and Okumura (1999) discuss citation categorization to support a system for writing a survey. Nanba et al. (2004a) automatically categorize citation sentences into three groups using pre-defined phrase-based rules."
N09-1066,N04-1019,0,0.386146,"zoning (Teufel, 2005). In argumentative zoning—a rhetorical classification task—seven 1 http://www.pubmedcentral.gov classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to their role in the author’s argument. Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts—comparing the results to human-generated surveys using both automatic and nugget-based pyramid evaluation (Lin and Demner-Fushman, 2006; Nenkova and Passonneau, 2004; Lin, 2004). 4.2 4 The salience of a node is recursively defined on the salience of adjacent nodes. This is similar to the concept of prestige in social networks, where the prestige of a person is dependent on the prestige of the people he/she knows. However, since random walk may get caught in cycles or in disconnected components, we reserve a low probability to jump to random nodes instead of neighbors (a technique suggested by Langville and Meyer (2006)). Summarization systems We used four summarization systems for our survey-creation approach: Trimmer, LexRank, CLexRank, and C-RR. Trimmer"
N09-1066,C08-1087,1,0.846023,"as, 584 some of which may be unfamiliar to panelists. Thus, they must learn about a new discipline “on the fly” in order to relate their own expertise to the proposal. Our goal is to effectively serve these needs by combining two currently available technologies: (1) bibliometric lexical link mining that exploits the structure of citations and relations among citations; and (2) summarization techniques that exploit the content of the material in both the citing and cited papers. It is generally agreed upon that manually written abstracts are good summaries of individual papers. More recently, Qazvinian and Radev (2008) argue that citation texts are useful in creating a summary of the important contributions of a research paper. The citation text of a target paper is the set of sentences in other technical papers that explicitly refer to it (Elkiss et al., 2008a). However, Teufel (2005) argues that using citation text directly is not suitable for document summarization. In this paper, we compare and contrast the usefulness of abstracts and of citation text in automatically generating a technical survey on a given topic from multiple research papers. The next section provides the background for this work, inc"
N09-1066,N07-1040,0,0.503915,"e focused than abstracts. Nakov et al. (2004) use sentences surrounding citations to create training and testing data for semantic analysis, synonym set creation, database curation, document summarization, and information retrieval. Kan et al. (2002) use annotated bibliographies to cover certain aspects of summarization and suggest using metadata and critical document features as well as the prominent content-based features to summarize documents. Kupiec et al. (1995) use a statistical method and show how extracts can be used to create summaries but use no annotated metadata in summarization. Siddharthan and Teufel (2007) describe a new reference task and show high human agreement as well as an improvement in the performance of argumentative zoning (Teufel, 2005). In argumentative zoning—a rhetorical classification task—seven 1 http://www.pubmedcentral.gov classes (Own, Other, Background, Textual, Aim, Basis, and Contrast) are used to label sentences according to their role in the author’s argument. Our aim is not only to determine the utility of citation texts for survey creation, but also to examine the quality distinctions between this form of input and others such as abstracts and full texts—comparing the"
N09-1066,J02-4002,0,0.390372,"ons of the target paper. Our goal is to test the hypothesis that an effective technical survey will reflect information on research not only from the perspective of its authors but also from the perspective of others who use/commend/discredit/add to it. Before describing our experiments with technical papers, abstracts, and citation texts, we first summarize relevant prior work that used these sources of information as input. 3 Related work Previous work has focused on the analysis of citation and collaboration networks (Teufel et al., 2006; Newman, 2001) and scientific article summarization (Teufel and Moens, 2002). Bradshaw (2003) used citation texts to determine the content of articles and improve the results of a search engine. Citation 586 texts have also been used to create summaries of single scientific articles in Qazvinian and Radev (2008) and Mei and Zhai (2008). However, there is no previous work that uses the text of the citations to produce a multi-document survey of scientific articles. Furthermore, there is no study contrasting the quality of surveys generated from citation summaries— both automatically and manually produced—to surveys generated from other forms of input such as the abstra"
N09-1066,W06-1613,0,0.814172,"eering and Computer Scienceφ School of Information§ , University of Michigan. {hassanam,mpradeep,vahed,radev}@umich.edu Abstract The number of research publications in various disciplines is growing exponentially. Researchers and scientists are increasingly finding themselves in the position of having to quickly understand large amounts of technical material. In this paper we present the first steps in producing an automatically generated, readily consumable, technical survey. Specifically we explore the combination of citation information and summarization techniques. Even though prior work (Teufel et al., 2006) argues that citation text is unsuitable for summarization, we show that in the framework of multi-document survey creation, citation texts can play a crucial role. 1 Introduction In today’s rapidly expanding disciplines, scientists and scholars are constantly faced with the daunting task of keeping up with knowledge in their field. In addition, the increasingly interconnected nature of real-world tasks often requires experts in one discipline to rapidly learn about other areas in a short amount of time. Cross-disciplinary research requires scientists in areas such as linguistics, biology, and"
N12-1009,P11-1051,1,0.637938,"nce), our work is different in two ways. First, previous work mostly ignored the fact that the citing sentence itself might be citing multiple references. Second, it defined the citing area (Nanba and Okumura, 1999) or the citation context (Qazvinian and Radev, 2010) as a set of whole contiguous sentences. In our work, we address the case where one citing sentence cites multiple papers, and define what we call the reference scope to be the fragments (not necessarily contiguous) of the citing sentence that are related to the target reference. In a recent work on citation-based summarization by Abu-Jbara and Radev (2011), the authors noticed the issue of having multiple references in one sentence. They raised this issue when they discussed 82 the factors that impede the coherence and the readability of citation-based summaries. They suggested that removing the fragments of a citing sentence that are not relevant to the summarized paper will significantly improve the quality of the produced summaries. In their work, they defined the scope of a given reference as the shortest fragment of the citing sentence that contains the reference and could form a grammatical sentence if the rest of the sentence was removed"
N12-1009,P11-3015,0,0.125845,"d summaries. In their work, they defined the scope of a given reference as the shortest fragment of the citing sentence that contains the reference and could form a grammatical sentence if the rest of the sentence was removed. They identify the scope by generating the syntactic parse tree of the sentence and then finding the text that corresponds to the smallest subtree rooted at an S node and contains the target reference node as one of its leaf nodes. They admitted that their method was very basic and works only when the scope forms one grammatical fragment, which is not true in many cases. Athar (2011) noticed the same issue with citing sentences that cite multiple references, but this time in the context of sentiment analysis in citations. He showed experimentally that identifying what he termed the scope of citation influence improves sentiment classification accuracy. He adapted the same basic method proposed by AbuJbara and Radev (2011). We use this method as a baseline in our evaluation below. In addition to this related work, there is a large body of research that used citing sentences in different applications. For example, citing sentences have been used to summarize the contributio"
N12-1009,H05-1091,0,0.117069,"Missing"
N12-1009,grover-etal-2000-lt,0,0.0503445,"n task. This allows us to use the popular classification agreement measure, the Kappa coefficient (Cohen, 1968). The Kappa coefficient is defined as follows: K= P (A) − P (E) 1 − P (E) (3) where P(A) is the relative observed agreement among raters and P(E) is the hypothetical probability of chance agreement. The agreement between the two annotators on the scope identification task was K = 0.61. On Landis and Kochs (Landis and Koch, 1977) scale, this value indicates substantial agreement. 5.3 Experimental Setup Method We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for text tokenization, part-of-speech tagging, chunking, and noun phrase head identification. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We use LibSVM (Chang and Lin, 2011) for Support Vector Machines (SVM) classification. Our SVM model uses a linear kernel. We use Weka (Hall et al., 2009) for logistic regression classification. We use the Machine Learning for Language Toolkit (MALLET) (McCallum, 2002) for CRF-based sequence labeling. In all the scope identification experiments and results below, we use 10-fold cross validation for training/test"
N12-1009,P03-1054,0,0.0298391,"performance of these methods in Section 5. The following three subsections describe the methods. 84 Word Classification In this method we define reference scope identification as a classification task of the individual words of the citing sentence. Each word is classified as inside or outside the scope of a given target reference. We use a number of linguistic and structural features to train a classification model on a set of labeled sentences. The trained model is then used to label new sentences. The features that we use to train the model are listed in Table 1. We use the Stanford parser (Klein and Manning, 2003) for syntactic and dependency parsing. We experiment with two classification algorithms: Support Vector Machines (SVM) and logistic regression. 4.2.2 Sequence Labeling In the method described in Section 4.2.1 above, we classify each word independently from the labels of the nearby words. The nature of our task, however, suggests that the accuracy of word classification can be improved by considering the labels of the words surrounding the word being classified. It is very likely that the word takes the same label as the word before and after it if they all belong to the same clause in the sent"
N12-1009,P08-1093,0,0.0372316,"aper, we present and compare three different approaches for identifying the fragments of a citing sentence that are related to a given target reference. Our methods are: word classification, sequence labeling, and segment classification. Our experiments show that segment classification achieves the best results. 1 (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below con"
N12-1009,N09-1066,1,0.915639,"ence. Our methods are: word classification, sequence labeling, and segment classification. Our experiments show that segment classification achieves the best results. 1 (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. Introduction Citation plays an important role in science. It makes the accumulation of knowledge possible. When a reference app"
N12-1009,C08-1087,1,0.939302,"d in the summary. In this paper, we present and compare three different approaches for identifying the fragments of a citing sentence that are related to a given target reference. Our methods are: word classification, sequence labeling, and segment classification. Our experiments show that segment classification achieves the best results. 1 (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, se"
N12-1009,P10-1057,1,0.893341,"approaches for identifying the fragments of a citing sentence that are related to a given target reference. Our methods are: word classification, sequence labeling, and segment classification. Our experiments show that segment classification achieves the best results. 1 (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. Introduction Citation plays"
N12-1009,C10-1101,1,0.628422,"fine the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in the citing area. Then, they use a set of cue words and hand-crafted rules to determine whether the surrounding sentences should be added to the citing area or not. In (Nanba et al., 2000) they use their citing area identification algorithm to improve citation type classification and automatic survey generation. Qazvinian and Radev (2010) addressed a similar problem. They proposed a method based on probabilistic inference to extract non-explicit citing sentences; i.e., sentences that appear around the sentence that contains the target reference and are related to it. They showed experimentally that citation-based survey generation produces better results when using both explicit and non-explicit citing sentences rather than using the explicit ones alone. Although this work shares the same general goal with ours (i.e identifying the pieces of text that are relevant to a given target reference), our work is different in two ways"
N12-1009,W09-3607,1,0.919083,"Missing"
N12-1009,D07-1089,0,0.0648417,"Missing"
N12-1009,N07-1040,0,0.0157842,"riments show that segment classification achieves the best results. 1 (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. Introduction Citation plays an important role in science. It makes the accumulation of knowledge possible. When a reference appears in a scientific article, it is usually accompanied by a span of text that highlights the important c"
N12-1009,W06-1613,0,0.8007,"ssification. Our experiments show that segment classification achieves the best results. 1 (1) Resnik (1999) addressed the issue of language identification for finding Web pages in the languages of interest. Previous work has studied and used citation sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; AbuJbara and Radev, 2011), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007), and paraphrase recognition (Nakov et al., 2004; Schwartz et al., 2007). Sentence (1) above contains one reference, and the whole sentence is talking about that reference. This is not always the case in scientific writing. Sentences that contain references to multiple papers are very common. For example, sentence (2) below contains three references. Introduction Citation plays an important role in science. It makes the accumulation of knowledge possible. When a reference appears in a scientific article, it is usually accompanied by a span of text t"
N12-3009,P11-4021,1,0.819382,"e the signs of all the posts exchanged by interacting participants to assign a sign for their relation. • Aggregate the signs of attitudinal sentences to assign a sign to the post. Figure 1: Overview of the system processing pipeline mance evaluation. 2 System Overview Figure 1 shows a block diagram of the system components and the processing pipeline. The first component in the system is the thread parsing component which takes as input a discussion thread and parses it to identify the posts, the participants, and the reply structure of the thread. This component uses a module from CLAIRLib (Abu-Jbara and Radev, 2011) to tokenize the posts and split them into sentences. The second component in the pipeline processes the text of the posts to identify polarized words and tag them with their polarity. This component uses the publicly available tool, opinionfinder (Wilson et al., 2005a), as a framework for polarity identification. This component uses an extended polarity lexicon created by applying a random walk model to WordNet (Miller, 1995) and a set of seed polarized words. This approach is described in detail in our previous work (Hassan and Radev, 2010). The context of words is taken into consideration b"
N12-3009,P10-1015,0,0.0241597,"individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Opinionfinder (Wilson et al., 2005a) is a system for mining opinions from text. Another research line focused on analyzing online discussions. For example, Lin et al. (2009) proposed a sparse codingbased model that simultaneously models the semantics and the structure of threaded discussions and Shen et al. (2006) proposed a method for exploiting the temporal information in discussion streams to identify the reply structure of the dialog. Many systems addressed the problem of extracting social networks from data (Elson et al., 2010; McCallum et al., 2007), but none of them considered both positive and negative relations. In the rest of the paper, we describe the system architecture, implementation, usage, and its perfor1 http://clair.eecs.umich.edu/AttitudeMiner/ 33 Proceedings of the NAACL-HLT 2012: Demonstration Session, pages 33–36, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics Thread Parsing Discussion Thread ….……. ….……. ….……. Subgroups Text Polarity Identification • Identify posts • Identify discussants • Identify the reply structure • Tokenize text • Split posts into sentences"
N12-3009,P10-1041,1,0.923634,"identify the sign of an interaction, and finally on the entire thread level to identify the overall polarity of the relation. Once the polarity of the pairwise relations that develop between interacting discussants is identified, this information is then used to construct a signed network representation of the discussion thread. The system also implements two signed network partitioning techniques that can be used to detect how the discussants split into subgroups regarding the discussion topic. The functionality of the system is based on our previous research on word polarity identification (Hassan and Radev, 2010) and attitude identification (Hassan et al., 2010). The system is publicly available for download and has a web interface to try online1 . This work is related to previous work in the areas of sentiment analysis and online discussion mining. Many previous systems studied the problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Opinionfinder (Wilson et al., 2005a) is a system for mining opinions from text. Another research line focused on analyzing online discussions. For example, Lin et al. (2009) proposed a sparse codingbased"
N12-3009,D10-1121,1,0.813319,"he entire thread level to identify the overall polarity of the relation. Once the polarity of the pairwise relations that develop between interacting discussants is identified, this information is then used to construct a signed network representation of the discussion thread. The system also implements two signed network partitioning techniques that can be used to detect how the discussants split into subgroups regarding the discussion topic. The functionality of the system is based on our previous research on word polarity identification (Hassan and Radev, 2010) and attitude identification (Hassan et al., 2010). The system is publicly available for download and has a web interface to try online1 . This work is related to previous work in the areas of sentiment analysis and online discussion mining. Many previous systems studied the problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Opinionfinder (Wilson et al., 2005a) is a system for mining opinions from text. Another research line focused on analyzing online discussions. For example, Lin et al. (2009) proposed a sparse codingbased model that simultaneously models the semantics and"
N12-3009,P97-1023,0,0.044913,"s two signed network partitioning techniques that can be used to detect how the discussants split into subgroups regarding the discussion topic. The functionality of the system is based on our previous research on word polarity identification (Hassan and Radev, 2010) and attitude identification (Hassan et al., 2010). The system is publicly available for download and has a web interface to try online1 . This work is related to previous work in the areas of sentiment analysis and online discussion mining. Many previous systems studied the problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Opinionfinder (Wilson et al., 2005a) is a system for mining opinions from text. Another research line focused on analyzing online discussions. For example, Lin et al. (2009) proposed a sparse codingbased model that simultaneously models the semantics and the structure of threaded discussions and Shen et al. (2006) proposed a method for exploiting the temporal information in discussion streams to identify the reply structure of the dialog. Many systems addressed the problem of extracting social networks from data (Elson et al., 2010; McCallum et al., 2007), but none"
N12-3009,H05-2018,0,0.237173,"e discussants split into subgroups regarding the discussion topic. The functionality of the system is based on our previous research on word polarity identification (Hassan and Radev, 2010) and attitude identification (Hassan et al., 2010). The system is publicly available for download and has a web interface to try online1 . This work is related to previous work in the areas of sentiment analysis and online discussion mining. Many previous systems studied the problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Opinionfinder (Wilson et al., 2005a) is a system for mining opinions from text. Another research line focused on analyzing online discussions. For example, Lin et al. (2009) proposed a sparse codingbased model that simultaneously models the semantics and the structure of threaded discussions and Shen et al. (2006) proposed a method for exploiting the temporal information in discussion streams to identify the reply structure of the dialog. Many systems addressed the problem of extracting social networks from data (Elson et al., 2010; McCallum et al., 2007), but none of them considered both positive and negative relations. In th"
N12-3009,H05-1044,0,0.172895,"e discussants split into subgroups regarding the discussion topic. The functionality of the system is based on our previous research on word polarity identification (Hassan and Radev, 2010) and attitude identification (Hassan et al., 2010). The system is publicly available for download and has a web interface to try online1 . This work is related to previous work in the areas of sentiment analysis and online discussion mining. Many previous systems studied the problem of identifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Opinionfinder (Wilson et al., 2005a) is a system for mining opinions from text. Another research line focused on analyzing online discussions. For example, Lin et al. (2009) proposed a sparse codingbased model that simultaneously models the semantics and the structure of threaded discussions and Shen et al. (2006) proposed a method for exploiting the temporal information in discussion streams to identify the reply structure of the dialog. Many systems addressed the problem of extracting social networks from data (Elson et al., 2010; McCallum et al., 2007), but none of them considered both positive and negative relations. In th"
N13-1067,P11-1051,1,0.888244,"Missing"
N13-1067,N12-1009,1,0.902792,"eholder. The reference to the target paper is replaced by the placeholder TREF. Each other reference is replaced by REF. b. Reference Grouping: In this step, we identify grouped references (i.e. multiple references listed between one pair of parentheses separated by semicolons). Each such group is replaced by a placeholder, GREF. If the target reference is a member of the group, we use a different placeholder: GTREF. c. Non-syntactic Reference Removal: A reference or a group of references could either be a syntactic constituent and has a semantic role in the sentence or not (Whidby, 2012; Abu Jbara and Radev, 2012). If the reference is not a syntactic compoFeature Demonstrative determiners Conjunctive adverbs Position Contains Closest Noun Phrase 2-3 grams Contains Other references Contains a Mention of target reference Multiple references Description Takes a value of 1 if the current sentence contains contains a demonstrative determiner (this, these, etc.), and 0 otherwise. Takes a value of 1 if the current sentence starts with a conjunctive adverb (However, Furthermore, Accordingly, etc.), and 0 otherwise. Position of the current sentence with respect to the citing sentence. This feature takes one of"
N13-1067,N12-1073,0,0.205026,"larity Classification The polarity (or sentiment) of a citation has also been studied previously. Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (Ziman, 1968; MacRoberts and MacRoberts, 1984; THOMPSON and YIYUN, 1991). Athar (2011) addressed the problem of identifying sentiment in citing sentences. He used a set of structure-based features to train a machine learning classifier using annotated data. This work uses the citing sentence only to predict sentiment. Context sentences were ignored. Athar and Teufel (2012a) observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3. They proposed two methods for utilizing the context. In the first method, they treat the citing sentence and a fixed context (a window of four sentences around the citing sentence) as if they were a single sentence. They extract features from the merged text and train a classifier similar to what they did in their 2011 paper. In the second method, they use a four-class annotation scheme. Each sentence in a window of four sentences around the ci"
N13-1067,W12-4303,0,0.0363825,"larity Classification The polarity (or sentiment) of a citation has also been studied previously. Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (Ziman, 1968; MacRoberts and MacRoberts, 1984; THOMPSON and YIYUN, 1991). Athar (2011) addressed the problem of identifying sentiment in citing sentences. He used a set of structure-based features to train a machine learning classifier using annotated data. This work uses the citing sentence only to predict sentiment. Context sentences were ignored. Athar and Teufel (2012a) observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3. They proposed two methods for utilizing the context. In the first method, they treat the citing sentence and a fixed context (a window of four sentences around the citing sentence) as if they were a single sentence. They extract features from the merged text and train a classifier similar to what they did in their 2011 paper. In the second method, they use a four-class annotation scheme. Each sentence in a window of four sentences around the ci"
N13-1067,P11-3015,0,0.0528888,"ries. We selected the six categories after studying all the previously used citation taxonomies. We included the ones we believed are important for improving bibliometric measures and for the applications that we are planning to pursue in the future (Section 5). 2.3 Citation Polarity Classification The polarity (or sentiment) of a citation has also been studied previously. Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (Ziman, 1968; MacRoberts and MacRoberts, 1984; THOMPSON and YIYUN, 1991). Athar (2011) addressed the problem of identifying sentiment in citing sentences. He used a set of structure-based features to train a machine learning classifier using annotated data. This work uses the citing sentence only to predict sentiment. Context sentences were ignored. Athar and Teufel (2012a) observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3. They proposed two methods for utilizing the context. In the first method, they treat the citing sentence and a fixed context (a window of four sentences around"
N13-1067,N09-1066,1,0.464876,"f a citation; i.e. the author’s intention behind choosing a published article and citing it. This analysis of citation purpose and polarity can be useful for many applications. For example, it can be used to build systems that help funding agencies and hiring committees at universities and research institutions evaluate researchers’ work more accurately. It can also be used as a preprocessing step in systems that process scholarly data. For example, citation-based summarization systems (Qazvinian and Radev, 2008; Qazvinian et al., 2010; AbuJbara and Radev, 2011) and survey generation systems (Mohammad et al., 2009; Qazvinian et al., 2013) can benefit from citation purpose and polarity analysis to improve paper and content selection. In this paper, we investigate the use of linguistic analysis techniques to automatically identify the purpose of citing a paper and the polarity of this citation. We first present a sequence labeling method for extracting the text that cites a given target reference; i.e. the text that appears in a scientific article and refers to another article and comments on it. We use the term citation context to refer to this text. Next, 596 Proceedings of NAACL-HLT 2013, pages 596–60"
N13-1067,S12-1035,0,0.0165246,"get reference appears within a group of references or separate (i.e. single reference). The lemmatized form of the closest verb/adjective/adverb to the target reference or its representative or any mention of it. Distance is measure based on the shortest path in the dependency tree. Whether the citation from the source paper to the target reference is a self citation. Whether the citation context contains a first/third person pronoun. Whether the citation context contains a negation cue. The list of negation cues is taken from the training data of the *SEM 2012 negation detection shared task (Morante and Blanco, 2012). Whether the citation context contains a speculation cue. The list is taken from Quirk et al. (1985) The closest subjectivity cue to the target reference or its representative or any anaphoric mention of it. The list of cues is taken from OpinionFinder (Wilson et al., 2005) Whether the citation context contains a contrary expression. The list is taken from Biber (1988) The headline of the section in which the citation appears. We identify five title categorizes: 1) Introduction, Motivation, etc. 2) Background, Prior Work, Previous Work, etc. 3) Experiments, Data, Results, Evaluation, etc. 4)"
N13-1067,P02-1040,0,0.115682,"Missing"
N13-1067,C08-1087,1,0.514208,"to automatically distinguish between positive, negative, and neutral citations and to identify the purpose of a citation; i.e. the author’s intention behind choosing a published article and citing it. This analysis of citation purpose and polarity can be useful for many applications. For example, it can be used to build systems that help funding agencies and hiring committees at universities and research institutions evaluate researchers’ work more accurately. It can also be used as a preprocessing step in systems that process scholarly data. For example, citation-based summarization systems (Qazvinian and Radev, 2008; Qazvinian et al., 2010; AbuJbara and Radev, 2011) and survey generation systems (Mohammad et al., 2009; Qazvinian et al., 2013) can benefit from citation purpose and polarity analysis to improve paper and content selection. In this paper, we investigate the use of linguistic analysis techniques to automatically identify the purpose of citing a paper and the polarity of this citation. We first present a sequence labeling method for extracting the text that cites a given target reference; i.e. the text that appears in a scientific article and refers to another article and comments on it. We us"
N13-1067,P10-1057,1,0.748247,"ing area to refer to the same concept. They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in the citing area. Then, they use a set of cue words and hand-crafted rules to determine whether the surrounding sentences should be added to the citing area or not. In (Nanba et al., 2000), they use their algorithm to improve citation type classification and automatic survey generation. Qazvinian and Radev (2010) addressed a similar problem. They proposed a method based on probabilistic inference to extract non-explicit citing sentences; i.e., sentences that appear around the sentence that contains the target reference and are related to it. They showed experimentally that citation-based survey generation produces better results when using both explicit and non-explicit citing sentences rather than using the explicit ones alone. 597 In previous work, we addressed the issue of identifying the scope of a given target reference in citing sentences that contain multiple references (2012). Our definition o"
N13-1067,C10-1101,1,0.480733,"the same concept. They define the citing area as the succession of sentences that appear around the location of a given reference in a scientific paper and have connection to it. Their algorithm starts by adding the sentence that contains the target reference as the first member sentence in the citing area. Then, they use a set of cue words and hand-crafted rules to determine whether the surrounding sentences should be added to the citing area or not. In (Nanba et al., 2000), they use their algorithm to improve citation type classification and automatic survey generation. Qazvinian and Radev (2010) addressed a similar problem. They proposed a method based on probabilistic inference to extract non-explicit citing sentences; i.e., sentences that appear around the sentence that contains the target reference and are related to it. They showed experimentally that citation-based survey generation produces better results when using both explicit and non-explicit citing sentences rather than using the explicit ones alone. 597 In previous work, we addressed the issue of identifying the scope of a given target reference in citing sentences that contain multiple references (2012). Our definition o"
N13-1067,W09-3607,1,0.838211,"hm outperforms the one proposed by...”. The arguments of the dependency relation are replaced by their lemmatized forms. This type of features has been shown to give good results in similar tasks (Athar and Teufel, 2012a). Table 3: The features used for citation purpose and polarity classification 4 Evaluation In this section, we describe the data that we used for evaluation and the experiments that we conducted. 4.1 size of the citation context window, and to develop the feature sets used in the three tasks described in Section 3 above. 4.2 Data We use the ACL Anthology Network corpus (AAN) (Radev et al., 2009; Radev et al., 2013) in our evaluation. AAN is a publicly available collection of more than 19,000 NLP papers. It includes a manually curated citation network of its papers as well as the full text of the papers and the citing sentences associated with each edge in the citation network. From this set, we selected 30 papers that have different numbers of incoming citations and that were consistently cited since they were published. These 30 papers received a total of about 3,500 citations from within AAN (average = 115 citation/paper, Min = 30, and Max = 338). These citations come from 1,493 u"
N13-1067,W06-1613,0,0.49205,"our volumes of Science Studies. Some of them are: Cited source is the specific point of departure for the research question investigated, Cited source contains the concepts, definitions, interpretations used, Cited source contains the data used by the citing paper. Nanba and Okumura (1999) came up with a simple schema composed of only three categories: Basis, Comparison, and other Other. They proposed a rule-based method that uses a set of statistically selected cue words to determine the category of a citation. They used this classification as a first step for scientific paper summarization. Teufel et al. (2006), in their work on citation function classification, adopted 12 categories from Spiegel-Rosing’s taxonomy. They trained an SVM classifier and used it to label each citing sentence with exactly one category. Further, they mapped the twelve categories to four top level categories namely: weakness, contrast (4 categories), positive (6 categories) and neutral. The taxonomy that we use in this work is based on previous work. We adopt a scheme that contains six categories. We selected the six categories after studying all the previously used citation taxonomies. We included the ones we believed are"
N13-1067,H05-2018,0,0.00828693,"ee. Whether the citation from the source paper to the target reference is a self citation. Whether the citation context contains a first/third person pronoun. Whether the citation context contains a negation cue. The list of negation cues is taken from the training data of the *SEM 2012 negation detection shared task (Morante and Blanco, 2012). Whether the citation context contains a speculation cue. The list is taken from Quirk et al. (1985) The closest subjectivity cue to the target reference or its representative or any anaphoric mention of it. The list of cues is taken from OpinionFinder (Wilson et al., 2005) Whether the citation context contains a contrary expression. The list is taken from Biber (1988) The headline of the section in which the citation appears. We identify five title categorizes: 1) Introduction, Motivation, etc. 2) Background, Prior Work, Previous Work, etc. 3) Experiments, Data, Results, Evaluation, etc. 4) Discussion, Conclusion, Future work, etc.. 5) All other section headlines. Headlines are identified using regular expressions. All the dependency relations that appear in the citation context. For example, nsubj(outperf orm, algorithm) is one of the relations extracted from"
N13-1067,P84-1044,0,0.664644,"ork is based on previous work. We adopt a scheme that contains six categories. We selected the six categories after studying all the previously used citation taxonomies. We included the ones we believed are important for improving bibliometric measures and for the applications that we are planning to pursue in the future (Section 5). 2.3 Citation Polarity Classification The polarity (or sentiment) of a citation has also been studied previously. Previous work showed that positive and negative citations are common, although negative citations might be expressed indirectly or in an implicit way (Ziman, 1968; MacRoberts and MacRoberts, 1984; THOMPSON and YIYUN, 1991). Athar (2011) addressed the problem of identifying sentiment in citing sentences. He used a set of structure-based features to train a machine learning classifier using annotated data. This work uses the citing sentence only to predict sentiment. Context sentences were ignored. Athar and Teufel (2012a) observed that taking the context into consideration when judging sentiment in citations increases the number of negative citations by a factor of 3. They proposed two methods for utilizing the context. In the first method, they treat t"
N16-1008,D07-1069,1,0.692207,"aselines on two publicly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. ("
N16-1008,C08-1040,1,0.865783,"icly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the"
N16-1008,D13-1068,0,0.0170224,"ls for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable models, even though it is difficult to incorporate features and high-dimensional latent states in a HMM-based model. Ahmed et al. (2011) proposed a hierarchical nonparametric model that integrates a Recurrent Chinese Restaurant Process with Latent Dirichlet Allocation to cluster words over time. The main issues with this approach are that it does not generate human-readable sentences, and that scaling nonparametric Bayesian models is often challenging. Similarly, Huang and Huang (2013) introduced a joint mixture-event-aspect model using a generative method. Navarro-Colorado and Saquete (2015) combined temporal information with topic modeling, and obtained the best performance in the crossdocument event ordering task of SemEval 2015. There has been prior work (Wang et al., 2008; Lee et al., 2009) using matrix factorization to perform sentence clustering. A key distinction between our work and this previous work is that our method requires no additional sentence selection steps after sentence clustering, so we avoid error cascades. 60 Zhu and Chen (2007) were among the first"
N16-1008,W04-1013,0,0.0596697,"tent factor models have had huge success in recommender systems. These latent factors, often in the form of low-rank embeddings, capture not only explicit information but also implicit context from the input data. In this work, we propose a novel matrix factorization framework to “recommend” key sentences to a timeline. Figure 2 shows an overview of the framework. More specifically, we formulate this task as a matrix completion problem. Given a news corpus, we assume that there are m total sentences, which are the rows in the matrix. The first column is the metric section, where we use ROUGE (Lin, 2004) as the metric to pre-compute a sentence importance 61 score between a candidate sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we r"
N16-1008,S15-2138,0,0.0165672,"e first to consider latent variable models, even though it is difficult to incorporate features and high-dimensional latent states in a HMM-based model. Ahmed et al. (2011) proposed a hierarchical nonparametric model that integrates a Recurrent Chinese Restaurant Process with Latent Dirichlet Allocation to cluster words over time. The main issues with this approach are that it does not generate human-readable sentences, and that scaling nonparametric Bayesian models is often challenging. Similarly, Huang and Huang (2013) introduced a joint mixture-event-aspect model using a generative method. Navarro-Colorado and Saquete (2015) combined temporal information with topic modeling, and obtained the best performance in the crossdocument event ordering task of SemEval 2015. There has been prior work (Wang et al., 2008; Lee et al., 2009) using matrix factorization to perform sentence clustering. A key distinction between our work and this previous work is that our method requires no additional sentence selection steps after sentence clustering, so we avoid error cascades. 60 Zhu and Chen (2007) were among the first to consider multimodal timeline summarization, but they focus on visualization, and do not make use of images"
N16-1008,P14-1087,0,0.0300454,"example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable models, even though it is difficult to incorporate feature"
N16-1008,nivre-etal-2006-maltparser,0,0.156814,"e sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we run the Stanford part-of-speech tagger (Toutanova et al., 2003) and MaltParser (Nivre et al., 2006) to generate SVO events based on dependency parses. Additional features can easily be incorporated into this framework; we leave the consideration of additional features for future work. Finally, for each sentence, we use an image search engine to retrieve a top-ranked relevant image, and then we use a convolutional neural network (CNN) architecture to extract visual features in an unsupervised fashion. We use a CNN model from Simonyan and Zisserman (2015), which is trained on the ImageNet Challenge 2014 dataset (Russakovsky et al., 2014). In our work, we keep the 16 convolutional layers and m"
N16-1008,radev-etal-2004-mead,1,0.359708,"date sentence in a news article, we take advantage of top-ranked relevant images from the Web and model the image using a convolutional neural network architecture. Finally, we propose a scalable low-rank approximation approach for learning joint embeddings of news stories and images. In experiments, we compare our model to various competitive baselines, and demonstrate the stateof-the-art performance of the proposed textbased and multimodal approaches. 1 To distill key insights from news reports, prior work in summarization often relies on feature engineering, and uses clustering techniques (Radev et al., 2004b) to select important events to be included in the final summary. While this approach is unsupervised, the process of feature engineering is always expensive, and the number of clusters is not easy to estimate. To present a complete summary, researchers from the natural language processing (NLP) community often solely rely on the textual information, while studies in the computer vision (CV) community rely solely on the image and video information. However, even though news images are abundantly available together with news stories, approaches that jointly learn textual and visual representat"
N16-1008,N03-1033,0,0.0186037,"e importance 61 score between a candidate sentence and a humangenerated summary. During training, we use these scores to tune model parameters, and during testing, we predict the sentence importance scores given the features in other columns. That is, we learn the embedding of important sentences. The second set of columns is the text feature section. In our experiments, this includes word observations, subject-verb-object (SVO) events, and the publication date of the document from which the candidate sentence is extracted. In our preprocessing step, we run the Stanford part-of-speech tagger (Toutanova et al., 2003) and MaltParser (Nivre et al., 2006) to generate SVO events based on dependency parses. Additional features can easily be incorporated into this framework; we leave the consideration of additional features for future work. Finally, for each sentence, we use an image search engine to retrieve a top-ranked relevant image, and then we use a convolutional neural network (CNN) architecture to extract visual features in an unsupervised fashion. We use a CNN model from Simonyan and Zisserman (2015), which is trained on the ImageNet Challenge 2014 dataset (Russakovsky et al., 2014). In our work, we ke"
N16-1008,N15-1112,0,0.147479,"rmance. Our main contributions are three-fold: • We propose a novel matrix factorization approach for extractive summarization, leveraging the success of collaborative filtering; • We are among the first to consider representation learning of a joint embedding for text and images in timeline summarization; • Our model significantly outperforms various competitive baselines on two publicly available datasets. 2 Related Work Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popul"
N16-1008,D11-1040,0,0.599874,"Supervised learning is widely used in summarization. For example, the seminal study by Kupiec et al. (1995) used a Naive Bayes classifier for selecting sentences. Recently, Wang et al. (2015) proposed a regression method that uses a joint loss function, combining news articles and comments. Additionally, unsupervised techniques such as language modeling (Allan et al., 2001) have been used for temporal summarization. In recent years, ranking and graph-based methods (Radev et al., 2004b; Erkan and Radev, 2004; Mihalcea and Tarau, 2004; Fader et al., 2007; Hassan et al., 2008; Mei et al., 2010; Yan et al., 2011b; Yan et al., 2011a; Zhao et al., 2013; Ng et al., 2014; Zhou et al., 2014; Glavaˇs ˇ and Snajder, 2014; Tran et al., 2015; Dehghani and Asadpour, 2015) have also proved popular for extractive timeline summarization, often in an unsupervised setting. Dynamic programming (Kiernan and Terzi, 2009) and greedy algorithms (Althoff et al., 2015) have also been considered for constructing summaries over time. Our work aligns with recent studies on latent variable models for multi-document summarization and storyline clustering. Conroy et al. (2001) were among the first to consider latent variable mo"
N16-1008,W04-3252,0,\N,Missing
N16-1177,P15-1162,0,0.0761047,"Missing"
N16-1177,P14-1062,0,0.794263,"e models and Convolutional Neural Networks (CNN) models. Recursive models can be considered as generalizations of traditional sequence-modeling neural networks to tree structures. For example, (Socher et al., 2013) uses Recursive Neural Networks to build representations of phrases and sentences by combining neighboring constituents based on the parse tree. In their model, the composition is performed in a bottom-up way from leaf nodes of tokens until the root node of the parsing tree is reached. CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors. Therefore, the model actually splits the sentence locally into n-grams by sliding windows. Introduction Sentence and document modeling systems are important for many Natural Language Processing (NLP) applications. The challenge for textual modeling is to capture features for different text units and to perform compositions over variable-length sequences (e.g., phrases, sentences, documents). As a traditional method, the bag-of-words model treats However, despite their ability to account for word orders, order-sensitive"
N16-1177,D14-1181,0,0.412854,"Neural Networks (CNN) models. Recursive models can be considered as generalizations of traditional sequence-modeling neural networks to tree structures. For example, (Socher et al., 2013) uses Recursive Neural Networks to build representations of phrases and sentences by combining neighboring constituents based on the parse tree. In their model, the composition is performed in a bottom-up way from leaf nodes of tokens until the root node of the parsing tree is reached. CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors. Therefore, the model actually splits the sentence locally into n-grams by sliding windows. Introduction Sentence and document modeling systems are important for many Natural Language Processing (NLP) applications. The challenge for textual modeling is to capture features for different text units and to perform compositions over variable-length sequences (e.g., phrases, sentences, documents). As a traditional method, the bag-of-words model treats However, despite their ability to account for word orders, order-sensitive models base"
N16-1177,C02-1150,0,0.891809,"st applies independent LSTM networks to each subsentence. Then a second LSTM layer is added between the first LSTM layer and the convolutional layer to encode the dependency across different sentences. We evaluate DSCNN on several sentence-level and document-level tasks including sentiment analysis, question type classification, and subjectivity classification. Experimental results demonstrate the effectiveness of our approach comparable with the state-of-the-art. In particular, our method achieves highest accuracies on MR sentiment analysis (Pang and Lee, 2005), TREC question classification (Li and Roth, 2002), and subjectivity classification task SUBJ (Pang and Lee, 2004) compared with several competitive baselines. The remaining part of this paper is the following. Section 2 discusses related work. Section 3 presents the background including LSTM networks and convolution operators. We then describe our architec1513 tures for sentence modeling and document modeling in Section 4, and report experimental results in Section 5. 2 Related Work The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio e"
N16-1177,P15-1107,0,0.0409797,"Missing"
N16-1177,P15-2029,0,0.368428,"stems are important for many Natural Language Processing (NLP) applications. The challenge for textual modeling is to capture features for different text units and to perform compositions over variable-length sequences (e.g., phrases, sentences, documents). As a traditional method, the bag-of-words model treats However, despite their ability to account for word orders, order-sensitive models based on neural networks still suffer from several disadvantages. First, recursive models depend on well-performing parsers, which can be difficult for many languages or noisy domains (Iyyer et al., 2015; Ma et al., 2015). Besides, since tree-structured neural networks are vulnerable to the vanishing gradient problem (Iyyer et al., 2015), recursive models require heavy label1512 Proceedings of NAACL-HLT 2016, pages 1512–1521, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics ing on phrases to add supervisions on internal nodes. Furthermore, parsing is restricted to sentences and it is unclear how to model paragraphs and documents using recursive neural networks. In CNN models, convolutional operators process word vectors sequentially using small windows. Thus sentences a"
N16-1177,P11-1015,0,0.707441,"013) NB (Socher et al., 2013) NBSVM-bi (Wang and Manning, 2012) SVMS (Silva et al., 2011) Standard-RNN (Socher et al., 2013) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DRNN (Irsoy and Cardie, 2014) Standard-LSTM (Tai et al., 2015) bi-LSTM (Tai et al., 2015) Tree-LSTM (Tai et al., 2015) SA-LSTM (Dai and Le, 2015) DCNN (Kalchbrenner et al., 2014) CNN-MC (Kim, 2014) MVCNN (Yin and Sch¨utze, 2015) Dep-CNN (Ma et al., 2015) Neural-BoW (Kalchbrenner et al., 2014) DAN (Iyyer et al., 2015) Paragraph-Vector (Le and Mikolov, 2014) WRRBM+BoW(bnc) (Dahl et al., 2012) Full+Unlabeled+BoW(bnc) (Maas et al., 2011) DSCNN DSCNN-Pretrain MR — — 79.4 — — 79.0 — — — — — 80.7 — 81.1 — 81.9 — 80.3 — — — 81.5 82.2 SST-2 79.4 81.8 — — 82.4 82.9 85.4 86.6 86.7 86.8 88.0 — 86.8 88.1 89.4 — 80.5 86.3 87.8 — — 89.1 88.7 SST-5 40.7 41.0 — — 43.2 44.4 45.7 49.8 45.8 49.1 51.0 — 48.5 47.4 49.6 49.5 42.4 47.7 48.7 — — 49.7 50.6 TREC — — — 95.0 — — — — — — — — 93.0 92.2 — 95.4 88.2 — — — — 95.4 95.6 SUBJ — — 93.2 — — — — — — — — — — 93.2 93.9 — — — — — 88.2 93.2 93.9 IMDB — — 91.2 — — — — — — — — 92.8 — — — — — 89.4 92.6 89.2 88.9 90.2 90.7 Table 1: Experiment results of DSCNN compared with other models. Performance is"
N16-1177,P04-1035,0,0.656454,"a second LSTM layer is added between the first LSTM layer and the convolutional layer to encode the dependency across different sentences. We evaluate DSCNN on several sentence-level and document-level tasks including sentiment analysis, question type classification, and subjectivity classification. Experimental results demonstrate the effectiveness of our approach comparable with the state-of-the-art. In particular, our method achieves highest accuracies on MR sentiment analysis (Pang and Lee, 2005), TREC question classification (Li and Roth, 2002), and subjectivity classification task SUBJ (Pang and Lee, 2004) compared with several competitive baselines. The remaining part of this paper is the following. Section 2 discusses related work. Section 3 presents the background including LSTM networks and convolution operators. We then describe our architec1513 tures for sentence modeling and document modeling in Section 4, and report experimental results in Section 5. 2 Related Work The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), whe"
N16-1177,P05-1015,0,0.769896,"ses. As for document modeling (Figure 2), DSCNN first applies independent LSTM networks to each subsentence. Then a second LSTM layer is added between the first LSTM layer and the convolutional layer to encode the dependency across different sentences. We evaluate DSCNN on several sentence-level and document-level tasks including sentiment analysis, question type classification, and subjectivity classification. Experimental results demonstrate the effectiveness of our approach comparable with the state-of-the-art. In particular, our method achieves highest accuracies on MR sentiment analysis (Pang and Lee, 2005), TREC question classification (Li and Roth, 2002), and subjectivity classification task SUBJ (Pang and Lee, 2004) compared with several competitive baselines. The remaining part of this paper is the following. Section 2 discusses related work. Section 3 presents the background including LSTM networks and convolution operators. We then describe our architec1513 tures for sentence modeling and document modeling in Section 4, and report experimental results in Section 5. 2 Related Work The success of deep learning architectures for NLP is first based on the progress in learning distributed word"
N16-1177,D14-1162,0,0.0996574,"ask SUBJ (Pang and Lee, 2004) compared with several competitive baselines. The remaining part of this paper is the following. Section 2 discusses related work. Section 3 presents the background including LSTM networks and convolution operators. We then describe our architec1513 tures for sentence modeling and document modeling in Section 4, and report experimental results in Section 5. 2 Related Work The success of deep learning architectures for NLP is first based on the progress in learning distributed word representations in semantic vector space (Bengio et al., 2003; Mikolov et al., 2013; Pennington et al., 2014), where each word is modeled with a realvalued vector called a word embedding. In this formulation, instead of using one-hot vectors by indexing words into a vocabulary, word embeddings are learned by projecting words onto a low dimensional and dense vector space that encodes both semantic and syntactic features of words. Given word embeddings, different models have been proposed to learn the composition of words to build up phrase and sentence representations. Most methods fall into three types: unordered models, sequence models, and Convolutional Neural Networks models. In unordered models,"
N16-1177,D12-1110,0,0.485927,"ives the schematic for the hierarchy. 5 5.1 Experiments Datasets Movie Review Data (MR) proposed by (Pang and Lee, 2005) is a dataset for sentiment analysis of movie reviews. The dataset consists of 5,331 positive and 5,331 negative reviews, mostly in one sentence. We follow the practice of using 10-fold cross validation to report results. Stanford Sentiment Treebank (SST) is another popular sentiment classification dataset introduced Method SVM (Socher et al., 2013) NB (Socher et al., 2013) NBSVM-bi (Wang and Manning, 2012) SVMS (Silva et al., 2011) Standard-RNN (Socher et al., 2013) MV-RNN (Socher et al., 2012) RNTN (Socher et al., 2013) DRNN (Irsoy and Cardie, 2014) Standard-LSTM (Tai et al., 2015) bi-LSTM (Tai et al., 2015) Tree-LSTM (Tai et al., 2015) SA-LSTM (Dai and Le, 2015) DCNN (Kalchbrenner et al., 2014) CNN-MC (Kim, 2014) MVCNN (Yin and Sch¨utze, 2015) Dep-CNN (Ma et al., 2015) Neural-BoW (Kalchbrenner et al., 2014) DAN (Iyyer et al., 2015) Paragraph-Vector (Le and Mikolov, 2014) WRRBM+BoW(bnc) (Dahl et al., 2012) Full+Unlabeled+BoW(bnc) (Maas et al., 2011) DSCNN DSCNN-Pretrain MR — — 79.4 — — 79.0 — — — — — 80.7 — 81.1 — 81.9 — 80.3 — — — 81.5 82.2 SST-2 79.4 81.8 — — 82.4 82.9 85.4 86.6"
N16-1177,D13-1170,0,0.538442,"pproach is achieving state-ofthe-art performance on several tasks, including sentiment analysis, question type classification, and subjectivity classification. 1 By contrast, order-sensitive models based on neural networks are becoming increasingly popular thanks to their ability to capture word order information. Many prevalent order-sensitive neural models can be categorized into two classes: Recursive models and Convolutional Neural Networks (CNN) models. Recursive models can be considered as generalizations of traditional sequence-modeling neural networks to tree structures. For example, (Socher et al., 2013) uses Recursive Neural Networks to build representations of phrases and sentences by combining neighboring constituents based on the parse tree. In their model, the composition is performed in a bottom-up way from leaf nodes of tokens until the root node of the parsing tree is reached. CNN based models, as the second category, utilize convolutional filters to extract local features (Kalchbrenner et al., 2014; Kim, 2014) over embedding matrices consisting of pretrained word vectors. Therefore, the model actually splits the sentence locally into n-grams by sliding windows. Introduction Sentence"
N16-1177,P15-1150,0,0.216506,"ns in an order-sensitive way. For example, thanks to its ability to capture longdistance dependencies, LSTM has re-emerged as a popular choice for many sequence-modeling tasks, including machine translation (Bahdanau et al., 2014), image caption generation (Vinyals et al., 2014), and natural language generation (Wen et al., 2015). Besides, RNN and LSTM can be both converted to tree-structured networks by using parsing information. For example, (Socher et al., 2013) applied Recursive Neural Networks as a variant of the standard RNN structured by syntactic trees to the sentiment analysis task. (Tai et al., 2015) also generalizes LSTM to Tree-LSTM where each LSTM unit combines information from its children units. Recently, CNN-based models have demonstrated remarkable performances on sentence modeling and classification tasks. Leveraging convolution operators, these models can extract features from variablelength phrases corresponding to different filters. For example, DCNN in (Kalchbrenner et al., 2014) constructs hierarchical features of sentences by onedimensional convolution and dynamic k-max pooling. (Yin and Sch¨utze, 2015) further utilizes multichannel embeddings and unsupervised pretraining to"
N16-1177,P12-2018,0,0.224402,"Missing"
N16-1177,D15-1199,0,0.0120628,"4) adds an additional hidden layer on top of the averaged word embeddings before the softmax layer for classification purposes. In contrast, sequence models, such as standard Recurrent Neural Networks (RNN) and Long ShortTerm Memory networks, construct phrase and sentence representations in an order-sensitive way. For example, thanks to its ability to capture longdistance dependencies, LSTM has re-emerged as a popular choice for many sequence-modeling tasks, including machine translation (Bahdanau et al., 2014), image caption generation (Vinyals et al., 2014), and natural language generation (Wen et al., 2015). Besides, RNN and LSTM can be both converted to tree-structured networks by using parsing information. For example, (Socher et al., 2013) applied Recursive Neural Networks as a variant of the standard RNN structured by syntactic trees to the sentiment analysis task. (Tai et al., 2015) also generalizes LSTM to Tree-LSTM where each LSTM unit combines information from its children units. Recently, CNN-based models have demonstrated remarkable performances on sentence modeling and classification tasks. Leveraging convolution operators, these models can extract features from variablelength phrases"
N16-1177,K15-1021,0,0.120126,"Missing"
N18-1089,W13-3520,0,0.250475,"t embeddings in our baseline model. 4.1 Datasets As a standard English dataset, we use the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993), containing 45 different POS tags. We adopt the standard split: sections 0-18 for training, 19-21 for development and 22-24 for testing (Collins, 2002; Manning, 2011). For multilingual POS tagging experiments, to compare with prior work, we use treebanks from Universal Dependencies (UD) v1.2 (Nivre et al., 2015) (17 POS) with the given data splits. We experiment on languages for which pre-trained Polyglot word embeddings (Al-Rfou et al., 2013) are available, resulting in 27 languages listed in Table 2. We regard languages with less than 60k tokens of training data as low-resource (Table 2, bottom), as in Plank et al. (2016). While Miyato et al. (2017) set the norm of a perturbation  (Eq 2) to be a fixed value for all input sentences, to generate adversarial examples for an entire sentence of a variable length and to include character embeddings besides word embeddings, we make the perturbation size  adaptive to the dimension of the concatenated √ input embedD ding s ∈√R . We set  to be α D (i.e., proportional to D), as the expec"
N18-1089,P16-1231,0,0.0579204,"Missing"
N18-1089,D17-1215,0,0.0350295,"ile these methods produce random noise, AT generates perturbations that the current model is particularly vulnerable to, and thus is claimed to be effective (Goodfellow et al., 2015). It should be noted that while related in name, adversarial training (AT) differs from Generative Adversarial Networks (GANs) (Goodfellow et al., 2014). GANs have already been applied to NLP tasks such as dialogue generation (Li et al., 2017) and transfer learning (Kim et al., 2017; Gui et al., 2017). Adversarial training also differs from adversarial evaluation, recently proposed for reading comprehension tasks (Jia and Liang, 2017). 3 CRF. In sequence labeling tasks it is beneficial to consider the correlations between neighboring labels and jointly decode the best chain of labels for a given sentence. With this motivation, we apply a conditional random field (CRF) (Lafferty et al., 2001) on top of the word-level BiLSTM to perform POS tag inference with global normalization, addressing the “label bias” problem. Specifically, given an input sentence, we pass the output sequence of the word-level BiLSTM to a firstorder chain CRF to compute the conditional probability of the target label sequence: p(y |s; θ) where θ repres"
N18-1089,Q17-1018,0,0.0393289,"Missing"
N18-1089,D17-1302,0,0.0399066,") and its variant for NLP tasks, word dropout (Iyyer et al., 2015). Xie et al. (2017) discuss various data noising techniques for language modeling. While these methods produce random noise, AT generates perturbations that the current model is particularly vulnerable to, and thus is claimed to be effective (Goodfellow et al., 2015). It should be noted that while related in name, adversarial training (AT) differs from Generative Adversarial Networks (GANs) (Goodfellow et al., 2014). GANs have already been applied to NLP tasks such as dialogue generation (Li et al., 2017) and transfer learning (Kim et al., 2017; Gui et al., 2017). Adversarial training also differs from adversarial evaluation, recently proposed for reading comprehension tasks (Jia and Liang, 2017). 3 CRF. In sequence labeling tasks it is beneficial to consider the correlations between neighboring labels and jointly decode the best chain of labels for a given sentence. With this motivation, we apply a conditional random field (CRF) (Lafferty et al., 2001) on top of the word-level BiLSTM to perform POS tag inference with global normalization, addressing the “label bias” problem. Specifically, given an input sentence, we pass the output"
N18-1089,D14-1082,0,0.191976,"ial POS tagging. Given a sentence, we input the normalized word embeddings (w1 , w2 , w3 ) and character embeddings (showing c1 , c2 , c3 for w1 ). Each word is represented by concatenating its word embedding and its character-level BiLSTM output. They are fed into the main BiLSTM-CRF network for POS tagging. In adversarial training, we compute and add the worstcase perturbation η to all the input embeddings for regularization. Introduction Recently, neural network-based approaches have become popular in many natural language processing (NLP) tasks including tagging, parsing, and translation (Chen and Manning, 2014; Bahdanau et al., 2015; Ma and Hovy, 2016). However, it has been shown that neural networks tend to be locally unstable and even tiny perturbations to the original inputs can mislead the models (Szegedy et al., 2014). Such maliciously perturbed inputs are called adversarial examples. Adversarial training (Goodfellow et al., 2015) aims to improve the robustness of a model to input perturbations by training on both unmodified examples and adversarial examples. Previous work (Goodfellow et al., 2015; Shaham et al., 2015) on image recognition has demonstrated the enhanced robustness of their mode"
N18-1089,Q16-1026,0,0.0670659,"Missing"
N18-1089,N16-1030,0,0.107158,"te the conditional probability of the target label sequence: p(y |s; θ) where θ represents all of the model parameters (in the BiLSTMs and CRF), s and y denote the input embeddings and the target POS tag sequence, respectively, for the given sentence. For training, we minimize the negative loglikelihood (loss function) Method In this section, we introduce our baseline POS tagging model and explain how we implement adversarial training on top. 3.1 Baseline POS Tagging Model L(θ; s, y) = − log p(y |s; θ) Following the recent top-performing models for sequence labeling tasks (Plank et al., 2016; Lample et al., 2016; Ma and Hovy, 2016), we employ a Bi-directional LSTM-CRF model as our baseline (see Figure 1 for an illustration). (1) with respect to the model parameters. Decoding searches for the POS tag sequence y ∗ with the highest conditional probability using the Viterbi algorithm. For more detail about the BiLSTMCRF formulation, refer to Ma and Hovy (2016). Character-level BiLSTM. Prior work has shown that incorporating character-level representations of words can boost POS tagging accuracy by capturing morphological information present in each language. Major neural character-level models include th"
N18-1089,D17-1230,0,0.02753,"such as dropout (Srivastava et al., 2014) and its variant for NLP tasks, word dropout (Iyyer et al., 2015). Xie et al. (2017) discuss various data noising techniques for language modeling. While these methods produce random noise, AT generates perturbations that the current model is particularly vulnerable to, and thus is claimed to be effective (Goodfellow et al., 2015). It should be noted that while related in name, adversarial training (AT) differs from Generative Adversarial Networks (GANs) (Goodfellow et al., 2014). GANs have already been applied to NLP tasks such as dialogue generation (Li et al., 2017) and transfer learning (Kim et al., 2017; Gui et al., 2017). Adversarial training also differs from adversarial evaluation, recently proposed for reading comprehension tasks (Jia and Liang, 2017). 3 CRF. In sequence labeling tasks it is beneficial to consider the correlations between neighboring labels and jointly decode the best chain of labels for a given sentence. With this motivation, we apply a conditional random field (CRF) (Lafferty et al., 2001) on top of the word-level BiLSTM to perform POS tag inference with global normalization, addressing the “label bias” problem. Specifically, giv"
N18-1089,K17-3002,0,0.0174538,"level accuracy and downstream dependency parsing performance by our baseline / adversarial POS taggers. 5.2 Sentence-level & Downstream Analysis In the word-level analysis, we showed that AT can boost tagging accuracy on rare words and the neighbors of unseen words, enhancing overall robustness on rare/unseen words. In this section, we discuss the benefit of our improved POS tagger in a major downstream task, dependency parsing. Most of the recent state-of-the-art dependency parsers take predicted POS tags as input (e.g. Chen and Manning (2014); Andor et al. (2016); Dozat and Manning (2017)). Dozat et al. (2017) empirically show that their dependency parser gains significant improvements by using POS tags predicted by a Bi-LSTM POS tagger, while POS tags predicted by the UDPipe tagger (Straka et al., 2016) do not contribute to parsing performance as much. This observation illustrates that POS tagging performance has a great influence on dependency parsing, motivating the hypothesis that the POS tagging improvements gained from our adversarial training help dependency parsing. To test the hypothesis, we consider three settings in dependency parsing of English and French: using POS tags predicted by th"
N18-1089,D15-1104,0,0.0943394,"Missing"
N18-1089,P16-1101,0,0.306196,"normalized word embeddings (w1 , w2 , w3 ) and character embeddings (showing c1 , c2 , c3 for w1 ). Each word is represented by concatenating its word embedding and its character-level BiLSTM output. They are fed into the main BiLSTM-CRF network for POS tagging. In adversarial training, we compute and add the worstcase perturbation η to all the input embeddings for regularization. Introduction Recently, neural network-based approaches have become popular in many natural language processing (NLP) tasks including tagging, parsing, and translation (Chen and Manning, 2014; Bahdanau et al., 2015; Ma and Hovy, 2016). However, it has been shown that neural networks tend to be locally unstable and even tiny perturbations to the original inputs can mislead the models (Szegedy et al., 2014). Such maliciously perturbed inputs are called adversarial examples. Adversarial training (Goodfellow et al., 2015) aims to improve the robustness of a model to input perturbations by training on both unmodified examples and adversarial examples. Previous work (Goodfellow et al., 2015; Shaham et al., 2015) on image recognition has demonstrated the enhanced robustness of their models to unseen images via adversarial trainin"
N18-1089,P14-5010,0,0.00302959,"contribute to parsing performance as much. This observation illustrates that POS tagging performance has a great influence on dependency parsing, motivating the hypothesis that the POS tagging improvements gained from our adversarial training help dependency parsing. To test the hypothesis, we consider three settings in dependency parsing of English and French: using POS tags predicted by the baseline model, using POS tags predicted by the AT model, and using gold POS tags. For English (PTB-WSJ), we first convert the treebank into Stanford Dependencies (SD) using Stanford CoreNLP (ver 3.8.0) (Manning et al., 2014), and then apply two wellknown dependency parsers: Stanford Parser (ver 3.5.0) (Chen and Manning, 2014) and Parsey McParseface (SyntaxNet) (Andor et al., 2016). For French (UD), we use Parsey Universal from SyntaxNet. The three parsers are all publicly available and pre-trained on corresponding treebanks. Table 5 shows the results of the experiments. We can observe improvements in both languages by using the POS tags predicted by our AT POS tagger. As Manning (2011) points out, when prethe effects of AT on rare / unseen words, we analyze tagging performance at the word level, considering vocab"
N18-1089,D17-1206,0,0.0858057,"ank et al. (2016). While Miyato et al. (2017) set the norm of a perturbation  (Eq 2) to be a fixed value for all input sentences, to generate adversarial examples for an entire sentence of a variable length and to include character embeddings besides word embeddings, we make the perturbation size  adaptive to the dimension of the concatenated √ input embedD ding s ∈√R . We set  to be α D (i.e., proportional to D), as the expected squared norm of s 979 Model Toutanova et al. (2003) Manning (2011) Collobert et al. (2011) Søgaard (2011) Ling et al. (2015) Ma and Hovy (2016) Yang et al. (2017) Hashimoto et al. (2017) Ours – Baseline (BiLSTM-CRF) Ours – Adversarial Accuracy 97.27 97.28 97.29 97.50 97.78 97.55 97.55 97.55 97.54 97.58 Our Models Plank et al. (2016) Baseline Adversarial BiLSTM TNT CRF Table 1: POS tagging accuracy on the PTB-WSJ test set, with other top-performing systems. 4.2 Training & Evaluation Details Model settings. We initialize word embeddings with 100-dimensional GloVe (Pennington et al., 2014) for English, and with 64-dimensional Polyglot (Al-Rfou et al., 2013) for other languages. We use 30-dimensional character embeddings, and set the state sizes of character/word-level BiLSTM to"
N18-1089,J93-2004,0,0.0620789,"of word / character embeddings, the model could trivially learn embeddings of large norms to make the perturbations insignificant. To prevent this issue, we normalize word/character embeddings so that they have mean 0 and variance 1 for every entry, as in Miyato et al. (2017). The normalization is performed every time we feed input embeddings into the LSTMs and generate adversarial examples. To ensure a fair comparison, we also normalize input embeddings in our baseline model. 4.1 Datasets As a standard English dataset, we use the Wall Street Journal (WSJ) portion of the Penn Treebank (PTB) (Marcus et al., 1993), containing 45 different POS tags. We adopt the standard split: sections 0-18 for training, 19-21 for development and 22-24 for testing (Collins, 2002; Manning, 2011). For multilingual POS tagging experiments, to compare with prior work, we use treebanks from Universal Dependencies (UD) v1.2 (Nivre et al., 2015) (17 POS) with the given data splits. We experiment on languages for which pre-trained Polyglot word embeddings (Al-Rfou et al., 2013) are available, resulting in 27 languages listed in Table 2. We regard languages with less than 60k tokens of training data as low-resource (Table 2, bo"
N18-1089,P82-1020,0,0.783233,"Missing"
N18-1089,K17-3014,0,0.0965117,"– – – – – – – – Avg 91.20 91.55 – – Berend Nguyen et (2017) al. (2017) 95.63 95.83 93.32 90.73 93.47 94.69 90.63 96.11 89.19 94.96 95.28 96.09 93.53 92.02 96.28 85.10 95.67 93.95 95.50 92.70 94.62 97.4 – 95.8 92.7 94.7 95.9 93.7 96.8 94.6 96.0 – 96.4 – 93.1 97.5 91.4 97.4 96.3 97.5 97.1 – 94.11 93.59 95.55 – – – – – – 97.12 86.30 88.82 89.47 88.99 81.80 – – – – – – – 88.41 – Table 2: POS tagging (test) for 27 Table 1: POS taggingaccuracy accuracy (test) forUD 27 v1.2 UD treebanks, with other recent works, Plank et al. (2016), v1.2 treebanks. The first column shows languages Berend (2017) and Nguyen et al. (2017). For Plank and the rest show tagging accuracy of different et al. (2016), we include the traditional baselines TNT models. Plank et al. (2016), we that include the and CRF, For and their state-of-the-art model employs • their statetraditional baselines TNT and CRF, and a multi-task BiLSTM. Languages with are morphoof-the-art model that employs a multi-task BiLlogically rich, and those at the bottom (‘el’ to ‘ta’) are low-resource, less than 60k tokens in their STM. Berendcontaining (2017) and Nguyen et al. (2017) are training sets.works reporting POS tagging perfortwo recent mance on UD v1.2."
N18-1089,K17-3001,0,0.0626974,"mples. Adversarial training (Goodfellow et al., 2015) aims to improve the robustness of a model to input perturbations by training on both unmodified examples and adversarial examples. Previous work (Goodfellow et al., 2015; Shaham et al., 2015) on image recognition has demonstrated the enhanced robustness of their models to unseen images via adversarial training and has provided theoretical explanations of the regularization effects. Despite its potential as a powerful regularizer, adversarial training (AT) has yet to be explored extensively in natural language tasks. Recently, Miyato et al. (2017) applied AT on text classification, achieving state-of-the-art accuracy. Yet, the specific effects of the robustness obtained from AT are still unclear in the context of NLP. For example, research studies have yet to answer questions such as 1) how can we interpret perturbations or robustness on natural language inputs? 2) how are they related to linguistic factors like vocabulary statis1 We distinguish AT from Generative Adversarial Networks (GANs). 976 Proceedings of NAACL-HLT 2018, pages 976–986 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics 2 tic"
N18-1089,P11-2009,0,0.0672206,"Missing"
N18-1089,P16-2038,0,0.0542865,"Missing"
N18-1089,L16-1680,0,0.0516701,"Missing"
N18-1089,P08-1076,0,0.0979331,"Missing"
N18-1089,N03-1033,0,0.440746,"Missing"
N18-1089,W11-0328,0,0.0392656,"Missing"
N18-1089,D14-1162,0,0.082016,"rtional to D), as the expected squared norm of s 979 Model Toutanova et al. (2003) Manning (2011) Collobert et al. (2011) Søgaard (2011) Ling et al. (2015) Ma and Hovy (2016) Yang et al. (2017) Hashimoto et al. (2017) Ours – Baseline (BiLSTM-CRF) Ours – Adversarial Accuracy 97.27 97.28 97.29 97.50 97.78 97.55 97.55 97.55 97.54 97.58 Our Models Plank et al. (2016) Baseline Adversarial BiLSTM TNT CRF Table 1: POS tagging accuracy on the PTB-WSJ test set, with other top-performing systems. 4.2 Training & Evaluation Details Model settings. We initialize word embeddings with 100-dimensional GloVe (Pennington et al., 2014) for English, and with 64-dimensional Polyglot (Al-Rfou et al., 2013) for other languages. We use 30-dimensional character embeddings, and set the state sizes of character/word-level BiLSTM to be 50, 200 for English, 50, 100 for low resource languages, and 50, 150 for other languages. The model parameters and character embeddings are randomly initialized, as in Ma and Hovy (2016). We apply dropout (Srivastava et al., 2014) to input embeddings and BiLSTM outputs for both baseline and adversarial training, with dropout rate 0.5. Optimization. We train the model parameters and word/character embe"
N18-1089,D17-1187,0,0.306141,"y et al., 2014; Goodfellow et al., 2015) was originally introduced in the context of image classification to improve the robustness of a model by training on input images with malicious perturbations. Previous work (Goodfellow et al., 2015; Shaham et al., 2015; Wang et al., 2017) has provided a theoretical framework to understand adversarial examples and the regularization effects of adversarial training (AT) in image recognition. Recently, Miyato et al. (2017) applied AT to a natural language task (text classification) by extending the concept of adversarial perturbations to word embeddings. Wu et al. (2017) further explored the possibility of AT in relation extraction. Both report improved performance on their tasks via AT, but the specific effects of AT have yet to be analyzed. In our work, we aim to address this issue by providing detailed analyses on the effects of AT from the perspective of NLP, such as different languages, vocabulary statistics, word embedding distribution, and aim to motivate future research • AT can boost the tagging performance for rare/ unseen words and increase the sentence-level accuracy. This positively affects the performance of down-stream tasks such as dependency"
N18-1089,P17-1161,0,0.0875793,"diction of a word cannot be influenced by a small perturbation in the input embedding. To verify this hypothesis, we cluster all words in the test set based on their correct POS tags4 and evaluate the tightness of the word vector distribution within each cluster. We compare this clustering quality among the three settings: 1) beginning (initialized with GloVe or Polyglot), 2) after base4 We excluded words with multiple tags in the test text. 983 Model Tsuruoka et al. (2011) Collobert et al. (2011) Yang et al. (2017) Suzuki and Isozaki (2008) Søgaard and Goldberg (2016) Hashimoto et al. (2017) Peters et al. (2017) Ours – Baseline (BiLSTM-CRF) Ours – Adversarial F1 93.81 94.32 94.66 95.15 95.56 95.77 96.37 95.18 95.25 AT enhanced F1 score from the baseline BiLSTMCRF model’s 95.18 to 95.25 for chunking, and from 91.22 to 91.56 for NER, also significantly outperforming Ma and Hovy (2016). These improvements made by AT are bigger than that for English POS tagging, most likely due to the larger room for improvement in chunking and NER. The improvements are again statistically significant, with p-value &lt; 0.05 on the t-test. The experimental results suggest that the proposed adversarial training scheme is gen"
N18-1089,P16-2067,0,0.436445,"r chain CRF to compute the conditional probability of the target label sequence: p(y |s; θ) where θ represents all of the model parameters (in the BiLSTMs and CRF), s and y denote the input embeddings and the target POS tag sequence, respectively, for the given sentence. For training, we minimize the negative loglikelihood (loss function) Method In this section, we introduce our baseline POS tagging model and explain how we implement adversarial training on top. 3.1 Baseline POS Tagging Model L(θ; s, y) = − log p(y |s; θ) Following the recent top-performing models for sequence labeling tasks (Plank et al., 2016; Lample et al., 2016; Ma and Hovy, 2016), we employ a Bi-directional LSTM-CRF model as our baseline (see Figure 1 for an illustration). (1) with respect to the model parameters. Decoding searches for the POS tag sequence y ∗ with the highest conditional probability using the Viterbi algorithm. For more detail about the BiLSTMCRF formulation, refer to Ma and Hovy (2016). Character-level BiLSTM. Prior work has shown that incorporating character-level representations of words can boost POS tagging accuracy by capturing morphological information present in each language. Major neural character-le"
N18-2093,Q13-1005,0,0.0901207,"k Semantic parsing maps natural language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012;"
N18-2093,W13-2322,0,0.0388136,"in a reason2 Related Work Semantic parsing maps natural language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; G"
N18-2093,P14-1133,0,0.0450292,"ngful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Data"
N18-2093,N10-1138,0,0.0312634,"igure 2). By grouping different slots in a reason2 Related Work Semantic parsing maps natural language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Po"
N18-2093,P17-1089,0,0.160066,"Missing"
N18-2093,P17-1105,0,0.105436,". We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) involve more hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al.,"
N18-2093,Q14-1030,0,0.0326972,"al language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b)."
N18-2093,P11-1060,0,0.0463556,"ing different slots in a reason2 Related Work Semantic parsing maps natural language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003,"
N18-2093,P16-1057,0,0.112214,"Missing"
N18-2093,J82-3002,0,0.779252,"oney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) involve more hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequenceto-sequence approach to converting text to logical forms. Most of previous work focus on specific table schemas, which means they us"
N18-2093,P15-1142,0,0.269664,"s. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Database community (Li and Jag"
N18-2093,P07-1121,0,0.157012,"slot filling problem (Figure 2). By grouping different slots in a reason2 Related Work Semantic parsing maps natural language to meaningful executable programs. The programs could 588 Proceedings of NAACL-HLT 2018, pages 588–594 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Figure 1: T YPE SQL consists of three slot-filling models on the right. We only show MODEL COL on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and"
N18-2093,D14-1162,0,0.0805578,"Missing"
N18-2093,C04-1021,0,0.729361,"Missing"
N18-2093,W16-0105,0,0.0419702,"2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) involve more hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 2017a; Iyer et al., 2017). Dong and Lapata (2016) introduce a sequenceto-sequence approach to converting text to logical forms. Most of previous work focus on specific table schemas, which means they use a single database in both train and test. Thus, they don’t generalize to new databases. Zhong et al. (2017) publish the WikiSQL dataset and propose a sequence-to-sequence model with reinforcement learning to generate SQL queries. In the problem definition of the WikiSQL task, the databases in the test set do not appear in the train and develo"
N18-2093,P17-1041,0,0.245392,"on the left for brevity. MODEL AGG and MODEL OPVAL have the similar pipelines. be a range of representations such as logic forms (Zelle and Mooney, 1996; Zettlemoyer and Collins, 2005; Wong and Mooney, 2007; Das et al., 2010; Liang et al., 2011; Banarescu et al., 2013; Artzi and Zettlemoyer, 2013; Reddy et al., 2014; Berant and Liang, 2014; Pasupat and Liang, 2015). Another area close to our task is code generation. This task parses natural language descriptions into a more general-purpose programming language such as Python (Allamanis et al., 2015; Ling et al., 2016; Rabinovich et al., 2017; Yin and Neubig, 2017). SELECT $AGG $SELECT COL WHERE $COND COL $OP $COND VAL (AND $COND COL $OP $COND VAL)* As a sub-task of semantic parsing, the text-toSQL problem has been studied for decades (Warren and Pereira, 1982; Popescu et al., 2003, 2004; Li et al., 2006; Giordani and Moschitti, 2012; Wang et al., 2017b). The methods of the Database community (Li and Jagadish, 2014; Yaghmazadeh et al., 2017) involve more hand feature engineering and user interactions with the systems. In this work, we focus on recent neural network based approaches (Yin et al., 2016; Zhong et al., 2017; Xu et al., 2017; Wang et al., 201"
N18-2093,P16-1004,0,\N,Missing
N18-2093,C12-2040,0,\N,Missing
N19-1075,W18-2501,0,0.0386863,"Missing"
N19-1075,J02-3001,0,0.552126,"combines the strengths of earlier models that performed SRL on the basis of a full dependency parse with more recent models that use no syntactic information at all. Our local and non-ensemble model achieves state-of-the-art performance on the CoNLL 09 English and Spanish datasets. SRL models benefit from syntactic information, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system. 1 Introduction Semantic role labeling (SRL) is the task of identifying the semantic relationships between each predicate in a sentence and its arguments (Gildea and Jurafsky, 2002). While early research assumed that SRL models required syntactic information to perform well (Punyakanok et al., 2008), recent work has demonstrated that neural networks can achieve competitive and even state-of-the-art performance without any syntactic information at all (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017). These systems have the benefits of being simpler to implement and performing more robustly on foreign languages and outof-domain data, cases where syntactic parsing is more difficult (Marcheggiani et al., 2017). In this paper, we show that using supertags is an"
N19-1075,N09-2047,1,0.753656,"ith Supertags Jungo Kasai♣∗ Dan Friedman♠∗ Robert Frank♦ Dragomir Radev♦ Owen Rambow♥ ♣ University of Washington ♠ Google ♦ Yale University ♥ Elemental Cognition, LLP jkasai@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015)."
N19-1075,P17-1044,0,0.0928139,"supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system. 1 Introduction Semantic role labeling (SRL) is the task of identifying the semantic relationships between each predicate in a sentence and its arguments (Gildea and Jurafsky, 2002). While early research assumed that SRL models required syntactic information to perform well (Punyakanok et al., 2008), recent work has demonstrated that neural networks can achieve competitive and even state-of-the-art performance without any syntactic information at all (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017). These systems have the benefits of being simpler to implement and performing more robustly on foreign languages and outof-domain data, cases where syntactic parsing is more difficult (Marcheggiani et al., 2017). In this paper, we show that using supertags is an effective middle ground between using full syntactic parses and using no syntactic information ∗ Work partially done at Yale University. 701 Proceedings of NAACL-HLT 2019, pages 701–709 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Token No , it was n’t black Monday Model 1 DEP/R P/R S"
N19-1075,J99-2004,0,0.686404,"nformation ∗ Work partially done at Yale University. 701 Proceedings of NAACL-HLT 2019, pages 701–709 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Token No , it was n’t black Monday Model 1 DEP/R P/R SBJ/R ROOT+L R ADV/L NAME/R PRD/L+L Model TAG DEP/R P/R ROOT+SBJ/L PRD/R ADV/L NAME/R - lacks such obligatory children, we encode whether it possesses non-obligatory dependents to the left (L) or right (R) as in Model 1. Model TAG. We propose Model TAG supertags that represent syntactic information analogously to TAG supertags (elementary trees) (Bangalore and Joshi, 1999). A Model TAG supertag encodes the dependency relation and the direction of the head of a word similarly to Model 0 if the dependency relation is non-obligatory (corresponding to adjunction nodes), and the information about obligatory dependents of verbs if any similarly to Model 2 (corresponding to substitution nodes). Table 1: Supertags for the sentence “No, it wasn’t black Monday.” Position Obligatory Parent Optional Parent Obligatory Dep. Optional Dep. Feature Direction Relation Direction Relation Direction Relation Direction 0 ·· ·· 1 ·· ·· · · 2 ·· ·· ·· · TAG ·· ·· 2.2 Motivated by rece"
N19-1075,P18-1192,0,0.376754,"al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM for semantic role labeling. Predicted supertags are represented by real-valued vectors, contrasting with approaches based on syntactic paths (Roth and Lapata, 2016; He et al., 2018) and syntactic edges (Marcheggiani and Titov, 2017; Strubell et al., 2018). This way of incorporating information alleviates the issue of error propagation from parsing. Supertagging has many advantages as part of a natural language processing pipeline. First, as a straightforward sequence-labeling task, the supertagging architecture is much simpler than comparable systems for structured parsing. Second, it is simple to extract different forms of supertags from a dependency corpus to test different hypotheses about which kinds of syntactic information are most useful for downstream tasks. Our"
N19-1075,W09-1206,0,0.273579,"Missing"
N19-1075,D17-1180,1,0.946368,"rt Frank♦ Dragomir Radev♦ Owen Rambow♥ ♣ University of Washington ♠ Google ♦ Yale University ♥ Elemental Cognition, LLP jkasai@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to"
N19-1075,Q17-1010,0,0.219706,"tenation of a dense vector representation of the word, a vector embedding of a predicted PTBstyle POS tag (only for English),2 and a vector output by character-level Convolutional Neural Networks (CNNs) for morphological information. For POS tagging before English supertagging, we use the same hyperparameters as in Ma and Hovy (2016). For supertagging, we follow the hyperparameters chosen in Kasai et al. (2018) regardless of the supertag model that is employed. We initialize the word embeddings by the pretrained 100 dimensional GloVe (Pennington et al., 2014) and the 300 dimensional FastText (Bojanowski et al., 2017) vectors for English and Spanish respectively. Table 2: Supertag models for SRL. Models 1 and 2 are from Ouchi et al. (2014) and Model 0 is from Nguyen and Nguyen (2016). sults show that supertags, by encoding just enough information, can improve SRL performance even compared to systems that incorporate complete dependency parses. 2 Our Models 2.1 Supertagger Model Supertag Design We experiment with four supertag models, two from Ouchi et al. (2014), one from Nguyen and Nguyen (2016), and one of our own design inspired by Tree Adjoining Grammar supertags (Bangalore and Joshi, 1999). Each model"
N19-1075,N18-1107,1,0.861608,"the-art supertaggers (TAG: Kasai et al. (2017, 2018); CCG: Lewis et al. (2016); Xu (2016)), we employ a bi-directional LSTM (BiLSTM) architecture for our supertagging. The input for each word is the conncatenation of a dense vector representation of the word, a vector embedding of a predicted PTBstyle POS tag (only for English),2 and a vector output by character-level Convolutional Neural Networks (CNNs) for morphological information. For POS tagging before English supertagging, we use the same hyperparameters as in Ma and Hovy (2016). For supertagging, we follow the hyperparameters chosen in Kasai et al. (2018) regardless of the supertag model that is employed. We initialize the word embeddings by the pretrained 100 dimensional GloVe (Pennington et al., 2014) and the 300 dimensional FastText (Bojanowski et al., 2017) vectors for English and Spanish respectively. Table 2: Supertag models for SRL. Models 1 and 2 are from Ouchi et al. (2014) and Model 0 is from Nguyen and Nguyen (2016). sults show that supertags, by encoding just enough information, can improve SRL performance even compared to systems that incorporate complete dependency parses. 2 Our Models 2.1 Supertagger Model Supertag Design We exp"
N19-1075,W03-1006,1,0.633442,"ammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM for semantic role labeling. Predicted supertags are represented by real-valued vectors, contrasting with approaches based on syntactic paths (Roth and Lapata, 2016; He et al., 2018) and syntactic edges (Marcheggiani and Titov, 2017; Strubell et al., 2018). This way of incorporating information alleviates the issue of error propagation from parsing. Supertagging has many advantages as part of a nat"
N19-1075,J07-4004,0,0.0694181,"Yale University ♥ Elemental Cognition, LLP jkasai@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and"
N19-1075,P10-1036,0,0.0328416,"ntal Cognition, LLP jkasai@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech"
N19-1075,D15-1112,0,0.203749,"Missing"
N19-1075,D15-1169,0,0.072174,"ngalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM for semantic role labeling. Predicted supertags are represented by real-valued vectors, contrasting with approaches based on syntactic paths (Roth and Lapata, 2016; He et al., 2018) and syntactic edges (Marcheggiani and Titov, 2017; Strubell et al., 2018). This way of incorporating information alleviates the issue of error propagation from parsing. Supertagging has many advantages as part of a natural language process"
N19-1075,W17-6213,1,0.876586,"i♣∗ Dan Friedman♠∗ Robert Frank♦ Dragomir Radev♦ Owen Rambow♥ ♣ University of Washington ♠ Google ♦ Yale University ♥ Elemental Cognition, LLP jkasai@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirection"
N19-1075,N16-1026,0,0.133999,"i@cs.washington.edu danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to anoth"
N19-1075,P16-1101,0,0.0220744,"n 0 ·· ·· 1 ·· ·· · · 2 ·· ·· ·· · TAG ·· ·· 2.2 Motivated by recent state-of-the-art supertaggers (TAG: Kasai et al. (2017, 2018); CCG: Lewis et al. (2016); Xu (2016)), we employ a bi-directional LSTM (BiLSTM) architecture for our supertagging. The input for each word is the conncatenation of a dense vector representation of the word, a vector embedding of a predicted PTBstyle POS tag (only for English),2 and a vector output by character-level Convolutional Neural Networks (CNNs) for morphological information. For POS tagging before English supertagging, we use the same hyperparameters as in Ma and Hovy (2016). For supertagging, we follow the hyperparameters chosen in Kasai et al. (2018) regardless of the supertag model that is employed. We initialize the word embeddings by the pretrained 100 dimensional GloVe (Pennington et al., 2014) and the 300 dimensional FastText (Bojanowski et al., 2017) vectors for English and Spanish respectively. Table 2: Supertag models for SRL. Models 1 and 2 are from Ouchi et al. (2014) and Model 0 is from Nguyen and Nguyen (2016). sults show that supertags, by encoding just enough information, can improve SRL performance even compared to systems that incorporate comple"
N19-1075,D16-1181,0,0.106279,"danfriedman@google.com {robert.frank,dragomir.radev}@yale.edu owenr@elementalcognition.com Abstract at all. A supertag is a linguistically rich description assigned to a lexical item. Supertags impose complex constraints on their local context, so supertagging can be thought of as “almost parsing” (Bangalore and Joshi, 1999). Supertagging has been shown to facilitate Tree-Adjoining Grammar (TAG) parsing (Bangalore et al., 2009; Friedman et al., 2017; Kasai et al., 2017, 2018) and Combinatory Categorial Grammar (CCG) parsing (Clark and Curran, 2007; Kummerfeld et al., 2010; Lewis et al., 2016; Xu, 2016). We propose that supertags can serve as a rich source of syntactic information for downstream tasks without the need for full syntactic parsing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM f"
N19-1075,K17-1041,0,0.545632,"ormation, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system. 1 Introduction Semantic role labeling (SRL) is the task of identifying the semantic relationships between each predicate in a sentence and its arguments (Gildea and Jurafsky, 2002). While early research assumed that SRL models required syntactic information to perform well (Punyakanok et al., 2008), recent work has demonstrated that neural networks can achieve competitive and even state-of-the-art performance without any syntactic information at all (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017). These systems have the benefits of being simpler to implement and performing more robustly on foreign languages and outof-domain data, cases where syntactic parsing is more difficult (Marcheggiani et al., 2017). In this paper, we show that using supertags is an effective middle ground between using full syntactic parses and using no syntactic information ∗ Work partially done at Yale University. 701 Proceedings of NAACL-HLT 2019, pages 701–709 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Token No , it was n’t black Monday M"
N19-1075,W09-1209,0,0.691839,"# Stags 88 220 503 317 Spanish Dev 92.97 90.63 90.08 92.33 ID 92.67 90.37 89.84 92.18 Table 3: Supertagging accuracies for English and Spanish. ID and OOD indicate the in-domain and out-of-domain evaluation data respectively. The # Stags columns show the number of supertags in the corresponding training set. For pre-trained word embeddings, we use the same word embeddings as the ones in Marcheggiani et al. (2017) for English and the 300-dimensional FastText vectors (Bojanowski et al., 2017) for Spanish. We use the predicates predicted by the mate-tools (Bj¨orkelund et al., 2009) (English) and Zhao et al. (2009) (Spanish) system in our models, again following Marcheggiani et al. (2017) to facilitate comparison. Our code is available online for easy replication of our results.4 BiLSTM model, which is our implementation of the syntax-agnostic model in Marcheggiani et al. (2017). We also present results for a BiLSTM model with dropout and highway connections but without supertags (BDH model), to distinguish the effects of supertags from the effects of better LSTM regularization. In every experiment we train the model five times, and present the mean score. Table 4 shows that Model 1 yields the best perf"
N19-1075,D17-1159,0,0.549148,"rove dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM for semantic role labeling. Predicted supertags are represented by real-valued vectors, contrasting with approaches based on syntactic paths (Roth and Lapata, 2016; He et al., 2018) and syntactic edges (Marcheggiani and Titov, 2017; Strubell et al., 2018). This way of incorporating information alleviates the issue of error propagation from parsing. Supertagging has many advantages as part of a natural language processing pipeline. First, as a straightforward sequence-labeling task, the supertagging architecture is much simpler than comparable systems for structured parsing. Second, it is simple to extract different forms of supertags from a dependency corpus to test different hypotheses about which kinds of syntactic information are most useful for downstream tasks. Our reWe introduce a new syntax-aware model for depend"
N19-1075,P15-1109,0,0.0684731,"from syntactic information, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system. 1 Introduction Semantic role labeling (SRL) is the task of identifying the semantic relationships between each predicate in a sentence and its arguments (Gildea and Jurafsky, 2002). While early research assumed that SRL models required syntactic information to perform well (Punyakanok et al., 2008), recent work has demonstrated that neural networks can achieve competitive and even state-of-the-art performance without any syntactic information at all (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017). These systems have the benefits of being simpler to implement and performing more robustly on foreign languages and outof-domain data, cases where syntactic parsing is more difficult (Marcheggiani et al., 2017). In this paper, we show that using supertags is an effective middle ground between using full syntactic parses and using no syntactic information ∗ Work partially done at Yale University. 701 Proceedings of NAACL-HLT 2019, pages 701–709 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Token No"
N19-1075,N19-1392,1,0.881902,"Missing"
N19-1075,P18-2106,0,0.0578599,"Missing"
N19-1075,D14-1162,0,0.0869394,"for our supertagging. The input for each word is the conncatenation of a dense vector representation of the word, a vector embedding of a predicted PTBstyle POS tag (only for English),2 and a vector output by character-level Convolutional Neural Networks (CNNs) for morphological information. For POS tagging before English supertagging, we use the same hyperparameters as in Ma and Hovy (2016). For supertagging, we follow the hyperparameters chosen in Kasai et al. (2018) regardless of the supertag model that is employed. We initialize the word embeddings by the pretrained 100 dimensional GloVe (Pennington et al., 2014) and the 300 dimensional FastText (Bojanowski et al., 2017) vectors for English and Spanish respectively. Table 2: Supertag models for SRL. Models 1 and 2 are from Ouchi et al. (2014) and Model 0 is from Nguyen and Nguyen (2016). sults show that supertags, by encoding just enough information, can improve SRL performance even compared to systems that incorporate complete dependency parses. 2 Our Models 2.1 Supertagger Model Supertag Design We experiment with four supertag models, two from Ouchi et al. (2014), one from Nguyen and Nguyen (2016), and one of our own design inspired by Tree Adjoinin"
N19-1075,N18-1202,0,0.0492232,"We also present results for a BiLSTM model with dropout and highway connections but without supertags (BDH model), to distinguish the effects of supertags from the effects of better LSTM regularization. In every experiment we train the model five times, and present the mean score. Table 4 shows that Model 1 yields the best performance in the English dev set, and thus we only use Model 1 supertags for test evaluation. We primarily show results only with word type embeddings to conduct fair comparisons with prior work, but we also provide results with deep contextual word representations, ELMo (Peters et al., 2018), and compare our results with recent work that utilizes ELMo (He et al., 2018). 5 English in-domain. Table 5 summarizes the results on the English in-domain test set. First, we were able to approximately replicate the results from Marcheggiani et al. (2017). Adding dropout and highway connections to our BiLSTM model improves performance by 0.5 points, to 88.1, and adding supertags improves results even further to 88.6. Our supertag model performs even better than the non-ensemble model in Marcheggiani and Titov (2017), in which the model is given the complete dependency parse of the sentence."
N19-1075,J08-2005,0,0.333326,"ls that use no syntactic information at all. Our local and non-ensemble model achieves state-of-the-art performance on the CoNLL 09 English and Spanish datasets. SRL models benefit from syntactic information, and we show that supertagging is a simple, powerful, and robust way to incorporate syntax into a neural SRL system. 1 Introduction Semantic role labeling (SRL) is the task of identifying the semantic relationships between each predicate in a sentence and its arguments (Gildea and Jurafsky, 2002). While early research assumed that SRL models required syntactic information to perform well (Punyakanok et al., 2008), recent work has demonstrated that neural networks can achieve competitive and even state-of-the-art performance without any syntactic information at all (Zhou and Xu, 2015; Marcheggiani et al., 2017; He et al., 2017). These systems have the benefits of being simpler to implement and performing more robustly on foreign languages and outof-domain data, cases where syntactic parsing is more difficult (Marcheggiani et al., 2017). In this paper, we show that using supertags is an effective middle ground between using full syntactic parses and using no syntactic information ∗ Work partially done a"
N19-1075,P16-1113,0,0.592626,"ing. Following Ouchi et al. (2014), who used supertags to improve dependency parsing, we extract various forms of supertags from the dependencyannotated CoNNL 09 corpus. This contrasts with prior SRL work that uses TAG or CCG supertags (Chen and Rambow, 2003; Lewis et al., 2015). We train a bidirectional LSTM (BiLSTM) to predict supertags and feed the predicted supertag embedding, along with word and predicted part-ofspeech embeddings, to another BiLSTM for semantic role labeling. Predicted supertags are represented by real-valued vectors, contrasting with approaches based on syntactic paths (Roth and Lapata, 2016; He et al., 2018) and syntactic edges (Marcheggiani and Titov, 2017; Strubell et al., 2018). This way of incorporating information alleviates the issue of error propagation from parsing. Supertagging has many advantages as part of a natural language processing pipeline. First, as a straightforward sequence-labeling task, the supertagging architecture is much simpler than comparable systems for structured parsing. Second, it is simple to extract different forms of supertags from a dependency corpus to test different hypotheses about which kinds of syntactic information are most useful for down"
N19-1075,E14-4030,0,\N,Missing
N19-1075,D18-1548,0,\N,Missing
otterbacher-radev-2004-revisionbank,J98-3005,1,\N,Missing
otterbacher-radev-2004-revisionbank,A00-2024,0,\N,Missing
otterbacher-radev-2004-revisionbank,W02-0404,1,\N,Missing
otterbacher-radev-2004-revisionbank,J95-4004,0,\N,Missing
otterbacher-radev-2004-revisionbank,W03-0507,0,\N,Missing
otterbacher-radev-2004-revisionbank,W00-0403,1,\N,Missing
otterbacher-radev-2004-revisionbank,J96-2004,0,\N,Missing
otterbacher-radev-2008-modeling,P02-1020,0,\N,Missing
P03-1048,J93-1004,0,0.0343641,"Missing"
P03-1048,W00-0403,1,0.724039,". The Kappa coefficient controls agreement P (A) by taking into account agreement by chance P (E) : K= P (A) − P (E) 1 − P (E) No matter how many items or annotators, or how the categories are distributed, K = 0 when there is no agreement other than what would be expected by chance, and K = 1 when agreement is perfect. If two annotators agree less than expected by chance, Kappa can also be negative. We report Kappa between three annotators in the case of human agreement, and between three humans and a system (i.e. four judges) in the next section. 3.1.3 Relative Utility Relative Utility (RU) (Radev et al., 2000) is tested on a large corpus for the first time in this project. RU takes into account chance agreement as a lower bound and interjudge agreement as an upper bound of performance. RU allows judges and summarizers to pick different sentences with similar content in their summaries without penalizing them for doing so. Each judge is asked to indicate the importance of each sentence in a cluster on a scale from 0 to 10. Judges also specify which sentences subsume or paraphrase each other. In relative utility, the score of an automatic summary increases with the importance of the sentences that it"
P03-1048,W97-0710,1,0.831436,"Missing"
P03-1048,E99-1011,0,\N,Missing
P03-1048,W97-0704,0,\N,Missing
P03-1048,J96-2004,0,\N,Missing
P03-1048,I05-2047,0,\N,Missing
P06-2096,N03-1024,0,0.344193,"Missing"
P06-2096,2001.mtsummit-papers.68,0,0.0611246,"Missing"
P06-2096,W04-3219,0,0.245318,"much. We shall explain syntactic features and their usages later. In this small example, our syntax-based alignment will align nothing (the bottom FSA in Figure 1) since “Milan” is the only lexically common word in both sentences. For much larger clusters in our experiments, we are able to produce a significant number of novel sentences from our alignment with such tightened syntactic conditions. Figure 2 shows one of the actual clusters used in our work that has 18 unique sentences. Two of the many automatically generated grammatical sentences are also shown. Another piece of related work, (Quirk et al., 2004), starts off with parallel inputs and uses monolingual Statistical Machine Translation techniques to align them and generate novel sentences. In our work, the input text does not need to be nearly as parallel. The main contribution of this paper is a syntaxbased alignment technique for generating novel paraphrases of sentences that describe a particular fact. Such techniques can be potentially useful in multi-document summarizers such as Newsblaster (http://newsblaster.cs. columbia.edu) and NewsInEssence (http: //www.newsinessence.com). Such systems are notorious for mostly reusing text from e"
P06-2096,E99-1023,0,0.00971403,"The relative position is defined to be the word’s absolute position divided by the length of the sentence it appears in (e.g. the 4th word of a 20-word sentence has a relative position of 0.2). If the difference between two relative positions is larger than 0.4 (empirically chosen before running the experiments), we consider the two words “unmatched”. Otherwise, they are syntactically matched.  “B-XP” meaning that the word is at the beginning of an XP chunk. From now on, we shall refer to the Chunk tag of a word as its IOB value (IOB was named by Tjong Kim Sang and Jorn Veeenstra (Tjong Kim Sang and Veenstra, 1999) after Ratnaparkhi (Ratnaparkhi, 1998)). For example, in the sentence “I visited Milan Theater”, the IOB value for “I” is B-NP since it marks the beginning of a nounphrase (NP). On the other hand, “Theater” has an IOB value of I-NP because it is inside a nounphrase (Milan Theater) and is not at the beginning of that constituent. Finally, the syntactic dependence trace of a word is the path of IOB values The pseudo-code of checking syntactic match is shown in Figure 4. 750 4 Evaluation 4.1 Experimental Setup 4.1.1 Data The data we use in our experiment come from a number of sentence clusters on"
P06-2096,W02-1022,0,0.0414419,"ce analysis to identify paraphrases from comparable texts (news from different sources on the same event). In summary, Pang et al. use syntactic alignment of parallel texts while Barzilay and Lee use comparable (not parallel) input but ignore syntax. Our work differs from the two in that we apply syntactic information on aligning comparable texts and that the syntactic clues we use are drawn from Chunklink ilk.uvt.nl/ ˜sabine/homepage/software.html output, which is further analysis from the syntactic parse trees. Another related paper using multiple sequence alignment for text generation was (Barzilay and Lee, 2002). In that work, the authors were able to automatically acquire different lexicalizations of the same concept from “multiple-parallel corpora”. We also draw some ideas from the FitchMargoliash method for building evolutionary trees 1. A police official said it was a Piper tourist plane and that the crash had set the top floors on fire. 2. According to ABCNEWS aviation expert John Nance, Piper planes have no history of mechanical troubles or other problems that would lead a pilot to lose control. 3. April 18, 2002 8212; A small Piper aircraft crashes into the 417-foot-tall Pirelli skyscraper in"
P06-2096,N03-1003,0,0.250184,"The main contribution of this paper is a syntaxbased alignment technique for generating novel paraphrases of sentences that describe a particular fact. Such techniques can be potentially useful in multi-document summarizers such as Newsblaster (http://newsblaster.cs. columbia.edu) and NewsInEssence (http: //www.newsinessence.com). Such systems are notorious for mostly reusing text from existing news stories. We believe that allowing them to use novel formulations of known facts will make these systems much more successful. 748 Our work is closest in spirit to the two papers that inspired us (Barzilay and Lee, 2003) and (Pang et al., 2003). Both of these papers describe how multiple sequence alignment can be used for extracting paraphrases from clustered texts. Pang et al. use as their input the multiple human English translations of Chinese documents provided by the LDC as part of the NIST machine translation evaluation. Their approach is to merge multiple parse trees into a single finite state automaton in which identical input subconstituents are merged while alternatives are converted to parallel paths in the output FSA. Barzilay and Lee, on the other hand, make use of classic techniques in biologica"
P06-2096,P02-1040,0,\N,Missing
P10-1041,E06-1027,0,0.122956,"uild lexicons of polarized words. Esuli and Sebastiani (2005; 2006) use a textual representation of words by collating all the glosses of the word as found in some dictionary. Then, a binary text classifier is trained using the textual representation and applied to new words. Kim and Hovy (2004) start with two lists of positive and negative seed words. WordNet is used to expand these lists. Synonyms of positive words and antonyms of negative words are considered positive, while synonyms of negative words and antonyms of positive words are considered negative. A similar method is presented in (Andreevskaia and Bergler, 2006) where WordNet synonyms, antonyms, and glosses are used to iteratively expand a list of seeds. The sentiment classes are treated as fuzzy categories where some words are very central to one category, while others may be interpreted differently. Kanayama and Nasukawa (2006) use syntactic features and context coherency, the tendency for same polarities to appear successively , to acquire polar atoms. Other related work is concerned with subjectivity analysis. Subjectivity analysis is the task of identifying text that present opinions as opposed to objective text that present factual information"
P10-1041,banea-etal-2008-bootstrapping,0,0.180918,"and context coherency, the tendency for same polarities to appear successively , to acquire polar atoms. Other related work is concerned with subjectivity analysis. Subjectivity analysis is the task of identifying text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Text could be either words, phrases, sentences, or any other chunks. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, the context of subjective text is used (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005) Wiebe et al. (2001) lists a lot of applications of subjectivity analysis such as classifying emails and mining reviews. Subjectivity analysis is related to the proposed method because identifying the polarity of text is the natural next step that should follow identifying subjective text. 3 of words, some of which are labeled as either positive or negative. In this network, two words are connecting if they are related. Differ"
P10-1041,esuli-sebastiani-2006-sentiwordnet,0,0.215383,"Missing"
P10-1041,P97-1023,0,0.937398,"have the same spin direction, neighboring words tend to have similar polarity. They pose the problem as an optimization problem and use the mean field method to find the best solution. The analogy with electrons leads them to assume that each word should be either positive or negative. This assumption is not accurate because most of the words in the language do not have any semantic orientation. They report that their method could get misled by noise in the gloss definition and their computations sometimes get trapped in a local optimum because of its greedy optimization flavor. Related Work Hatzivassiloglou and McKeown (1997) proposed a method for identifying word polarity of adjectives. They extract all conjunctions of adjectives from a given corpus and then they classify each conjunctive expression as either the same orientation such as “simple and well-received” or different orientation such as “simplistic but wellreceived”. The result is a graph that they cluster into two subsets of adjectives. They classify the cluster with the higher average frequency as positive. They created and labeled their own dataset for experiments. Their approach will probably works only with adjectives because there is nothing wrong"
P10-1041,C00-1044,0,0.447091,"awa (2006) use syntactic features and context coherency, the tendency for same polarities to appear successively , to acquire polar atoms. Other related work is concerned with subjectivity analysis. Subjectivity analysis is the task of identifying text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Text could be either words, phrases, sentences, or any other chunks. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, the context of subjective text is used (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005) Wiebe et al. (2001) lists a lot of applications of subjectivity analysis such as classifying emails and mining reviews. Subjectivity analysis is related to the proposed method because identifying the polarity of text is the natural next step that should follow identifying subjective text. 3 of words, some of which are labeled as either positive or negative. In this network, two words are connecting if the"
P10-1041,kamps-etal-2004-using,0,0.792509,"Missing"
P10-1041,W06-1642,0,0.27705,"ordNet and a list of seed labeled words to predict its polarity. They check if any of the synonyms of the given word has known polarity. If so, they label it with the label of its synonym. Otherwise, they check if any of the antonyms of the given word has known polarity. If so, they label it 396 with the opposite label of the antonym. They continue in a bootstrapping manner till they label all possible word. This method is quite similar to the shortest-path method proposed in (Kamps et al., 2004). There are some other methods that try to build lexicons of polarized words. Esuli and Sebastiani (2005; 2006) use a textual representation of words by collating all the glosses of the word as found in some dictionary. Then, a binary text classifier is trained using the textual representation and applied to new words. Kim and Hovy (2004) start with two lists of positive and negative seed words. WordNet is used to expand these lists. Synonyms of positive words and antonyms of negative words are considered positive, while synonyms of negative words and antonyms of positive words are considered negative. A similar method is presented in (Andreevskaia and Bergler, 2006) where WordNet synonyms, antonyms, a"
P10-1041,W03-1017,0,0.16155,"r related work is concerned with subjectivity analysis. Subjectivity analysis is the task of identifying text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Text could be either words, phrases, sentences, or any other chunks. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, the context of subjective text is used (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005) Wiebe et al. (2001) lists a lot of applications of subjectivity analysis such as classifying emails and mining reviews. Subjectivity analysis is related to the proposed method because identifying the polarity of text is the natural next step that should follow identifying subjective text. 3 of words, some of which are labeled as either positive or negative. In this network, two words are connecting if they are related. Different sources of information could be used to decide whether two words are related or not. For example, the synonyms of a"
P10-1041,H05-1043,0,0.305311,". Subjectivity analysis is the task of identifying text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Text could be either words, phrases, sentences, or any other chunks. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, the context of subjective text is used (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005) Wiebe et al. (2001) lists a lot of applications of subjectivity analysis such as classifying emails and mining reviews. Subjectivity analysis is related to the proposed method because identifying the polarity of text is the natural next step that should follow identifying subjective text. 3 of words, some of which are labeled as either positive or negative. In this network, two words are connecting if they are related. Different sources of information could be used to decide whether two words are related or not. For example, the synonyms of any word are semantically related to it. The intuiti"
P10-1041,W03-1014,0,0.354948,"cquire polar atoms. Other related work is concerned with subjectivity analysis. Subjectivity analysis is the task of identifying text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Text could be either words, phrases, sentences, or any other chunks. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, the context of subjective text is used (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005) Wiebe et al. (2001) lists a lot of applications of subjectivity analysis such as classifying emails and mining reviews. Subjectivity analysis is related to the proposed method because identifying the polarity of text is the natural next step that should follow identifying subjective text. 3 of words, some of which are labeled as either positive or negative. In this network, two words are connecting if they are related. Different sources of information could be used to decide whether two words are related or not."
P10-1041,P05-1017,0,0.920629,"study the problem of automatically identifying semantic orientation of any word by analyzing its relations to other words. Automatically classifying words as either positive or negative enables us to automatically identify the polarity of larger pieces of text. This could be a very useful building block for mining surveys, product reviews and online discussions. We apply a Markov random walk model to a large semantic word graph, producing a polarity estimate for any given word. Previous work on identifying the semantic orientation of words has addressed the problem as both a semi-supervised (Takamura et al., 2005) and an unsupervised (Turney and Littman, 2003) learning problem. In the semisupervised setting, a training set of labeled words Automatically identifying the polarity of words is a very important task in Natural Language Processing. It has applications in text classification, text filtering, analysis of product review, analysis of responses to surveys, and mining online discussions. We propose a method for identifying the polarity of words. We apply a Markov random walk model to a large word relatedness graph, producing a polarity estimate for any given word. A key advantage of the model is i"
P10-1041,P02-1053,0,0.352542,"ly tested using a manually labeled set of positive and negative words. It outperforms the state of the art methods in the semi-supervised setting. The results in the unsupervised setting is comparable to the best reported values. However, the proposed method is faster and does not need a large corpus. 1 Introduction Identifying emotions and attitudes from unstructured text is a very important task in Natural Language Processing. This problem has a variety of possible applications. For example, there has been a great body of work for mining product reputation on the Web (Morinaga et al., 2002; Turney, 2002). Knowing the reputation of a product is very important for marketing and customer relation management (Morinaga et al., 2002). Manually handling reviews to identify reputation is a very costly, and 395 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 395–403, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics is used to train the model. In the unsupervised setting, only a handful of seeds is used to define the two polarity classes. The proposed method could be used both in a semi-supervised and in an unsupervised se"
P10-1041,W01-1626,0,0.0127977,"the task of identifying text that present opinions as opposed to objective text that present factual information (Wiebe, 2000). Text could be either words, phrases, sentences, or any other chunks. There are two main categories of work on subjectivity analysis. In the first category, subjective words and phrases are identified without considering their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). In the second category, the context of subjective text is used (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005) Wiebe et al. (2001) lists a lot of applications of subjectivity analysis such as classifying emails and mining reviews. Subjectivity analysis is related to the proposed method because identifying the polarity of text is the natural next step that should follow identifying subjective text. 3 of words, some of which are labeled as either positive or negative. In this network, two words are connecting if they are related. Different sources of information could be used to decide whether two words are related or not. For example, the synonyms of any word are semantically related to it. The intuition behind that conne"
P10-1041,H05-2017,0,\N,Missing
P10-1041,C04-1200,0,\N,Missing
P10-1057,W09-3611,0,0.369956,"Missing"
P10-1057,N09-1066,1,0.933581,"s follows. Preceded by a review of prior work in Section 2, we explain the data collection and our annotation process in Section 3. Section 4 explains our methodology and is followed by experimental setup in Section 5. Introduction In scientific literature, scholars use citations to refer to external sources. These secondary sources are essential in comprehending the new research. Previous work has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information (Siddharthan and Teufel, 2007; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mohammad et al., 2009; Mei and Zhai, 2008). A citation to a paper in a scientific article may contain explicit information about the cited research. The following example is an excerpt from a CoNLL paper1 that contains information about Eisner’s work on bottom-up parsers and the notion of span in parsing: “Another use of bottom-up is due to Eisner (1996), who introduced the notion of a span.” 1 Buchholz and Marsi “CoNLL-X Shared Task On Multilingual Dependency Parsing”, CoNLL 2006 555 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555–564, c Uppsala, Sweden, 11-16 Ju"
P10-1057,C08-1087,1,0.814349,"f this paper is organized as follows. Preceded by a review of prior work in Section 2, we explain the data collection and our annotation process in Section 3. Section 4 explains our methodology and is followed by experimental setup in Section 5. Introduction In scientific literature, scholars use citations to refer to external sources. These secondary sources are essential in comprehending the new research. Previous work has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information (Siddharthan and Teufel, 2007; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mohammad et al., 2009; Mei and Zhai, 2008). A citation to a paper in a scientific article may contain explicit information about the cited research. The following example is an excerpt from a CoNLL paper1 that contains information about Eisner’s work on bottom-up parsers and the notion of span in parsing: “Another use of bottom-up is due to Eisner (1996), who introduced the notion of a span.” 1 Buchholz and Marsi “CoNLL-X Shared Task On Multilingual Dependency Parsing”, CoNLL 2006 555 Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics, pages 555–564, c Up"
P10-1057,W09-3607,1,0.833379,"Own, Other, Background, Textual, Aim, Basis, Contrast. Little work has been done on automatic citation extraction from research papers. Kaplan et al, (Kaplan et al., 2009) introduces “citation-site” as a block of text in which the cited text is discussed. The mentioned work uses a machine learning method for extracting citations from re3 Data The ACL Anthology Network (AAN)2 is a collection of papers from the ACL Anthology3 published in the Computational Linguistics journal and proceedings from ACL conferences and workshops and includes more than 14, 000 papers over a period of four decades (Radev et al., 2009). AAN includes the citation network of the papers in the ACL Anthology. The papers in AAN are publicly available in text format retrieved by an OCR process from the original pdf files, and are segmented into sentences. To build a corpus for our experiments we picked 10 recently published papers from various areas in NLP4 , each of which had references for a total of 203 candidate paper-reference pairs. Table 1 lists these papers together with their authors, titles, publication year, number of references, number of references within AAN, and the number of sen2 http://clair.si.umich.edu/clair/an"
P10-1057,W09-3610,0,0.0169474,"12 14 9 10 5 13 10 13 5 # Sents 102 153 74 138 231 84 262 185 203 92 Table 1: Papers chosen from AAN as source papers for the evaluation corpus, together with their publication year, number of references (in AAN) and number of sentences. Papers marked with ∗ are used to calculate inter-judge agreement. 2 Prior Work search papers and evaluates the result using 4 annotated articles. In our work we use graphical models to extract context sentences. Graphical models have a number of properties and corresponding techniques and have been used before on Information Retrieval tasks. Romanello et al, (Romanello et al., 2009) use Conditional Random Fields (CRF) to extract references from unstructured text in digital libraries of classic texts. Similar work include term dependency extraction (Metzler and Croft, 2005), query expansion (Metzler and Croft, 2007), and automatic feature selection (Metzler, 2007). Analyzing the structure of scientific articles and their relations has received a lot of attention recently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In additi"
P10-1057,N07-1040,0,0.654717,"roduce better surveys of research areas. The rest of this paper is organized as follows. Preceded by a review of prior work in Section 2, we explain the data collection and our annotation process in Section 3. Section 4 explains our methodology and is followed by experimental setup in Section 5. Introduction In scientific literature, scholars use citations to refer to external sources. These secondary sources are essential in comprehending the new research. Previous work has shown the importance of citations in scientific domains and indicated that citations include survey-worthy information (Siddharthan and Teufel, 2007; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mohammad et al., 2009; Mei and Zhai, 2008). A citation to a paper in a scientific article may contain explicit information about the cited research. The following example is an excerpt from a CoNLL paper1 that contains information about Eisner’s work on bottom-up parsers and the notion of span in parsing: “Another use of bottom-up is due to Eisner (1996), who introduced the notion of a span.” 1 Buchholz and Marsi “CoNLL-X Shared Task On Multilingual Dependency Parsing”, CoNLL 2006 555 Proceedings of the 48th Annual Meeting of the Association fo"
P10-1057,J02-4002,0,0.302734,"lo et al, (Romanello et al., 2009) use Conditional Random Fields (CRF) to extract references from unstructured text in digital libraries of classic texts. Similar work include term dependency extraction (Metzler and Croft, 2005), query expansion (Metzler and Croft, 2007), and automatic feature selection (Metzler, 2007). Analyzing the structure of scientific articles and their relations has received a lot of attention recently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific citations has been used in previous research. Bradshaw (Bradshaw, 2002; Bradshaw, 2003) uses citation"
P10-1057,W06-1613,0,0.0976086,"and corresponding techniques and have been used before on Information Retrieval tasks. Romanello et al, (Romanello et al., 2009) use Conditional Random Fields (CRF) to extract references from unstructured text in digital libraries of classic texts. Similar work include term dependency extraction (Metzler and Croft, 2005), query expansion (Metzler and Croft, 2007), and automatic feature selection (Metzler, 2007). Analyzing the structure of scientific articles and their relations has received a lot of attention recently. The structure of citation and collaboration networks has been studied in (Teufel et al., 2006; Newman, 2001), and summarization of scientific documents is discussed in (Teufel and Moens, 2002). In addition, there is some previous work on the importance of citation sentences. Elkiss et al, (Elkiss et al., 2008) perform a large-scale study on citations in the free PubMed Central (PMC) and show that they contain information that may not be present in abstracts. In other work, Nanba et al, (Nanba and Okumura, 1999; Nanba et al., 2004b; Nanba et al., 2004a) analyze citation sentences and automatically categorize them in order to build a tool for survey generation. The text of scientific ci"
P10-1057,P08-1093,0,\N,Missing
P11-1051,P08-1093,0,0.0991547,"er and summarize its important contributions. One way to make use of these sentences is creating a summary of the target paper. This summary is different from the abstract or a summary generated from the paper itself. While the abstract represents the author’s point of view, the citation summary is the summation of multiple scholars’ viewpoints. The task of summarizing a scientific paper using its set of citation sentences is called citationbased summarization. There has been previous work done on citationbased summarization (Nanba et al., 2000; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Previous work focused on the extraction aspect; i.e. analyzing the collection of citation sentences and selecting a representative subset that covers the main aspects of the paper. The cohesion and the readability of the produced summaries have been mostly ignored. This resulted in noisy and confusing summaries. In this work, we focus on the coherence and readability aspects of the problem. Our approach produces citation-based summaries in three stages: preprocessing, extraction, and postprocessing. Our experiments show that our approach produces better summaries than"
P11-1051,N09-1066,1,0.939078,"important contributions. One way to make use of these sentences is creating a summary of the target paper. This summary is different from the abstract or a summary generated from the paper itself. While the abstract represents the author’s point of view, the citation summary is the summation of multiple scholars’ viewpoints. The task of summarizing a scientific paper using its set of citation sentences is called citationbased summarization. There has been previous work done on citationbased summarization (Nanba et al., 2000; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Previous work focused on the extraction aspect; i.e. analyzing the collection of citation sentences and selecting a representative subset that covers the main aspects of the paper. The cohesion and the readability of the produced summaries have been mostly ignored. This resulted in noisy and confusing summaries. In this work, we focus on the coherence and readability aspects of the problem. Our approach produces citation-based summaries in three stages: preprocessing, extraction, and postprocessing. Our experiments show that our approach produces better summaries than several baseline summar"
P11-1051,C08-1087,1,0.261798,"eir efforts to read the paper and summarize its important contributions. One way to make use of these sentences is creating a summary of the target paper. This summary is different from the abstract or a summary generated from the paper itself. While the abstract represents the author’s point of view, the citation summary is the summation of multiple scholars’ viewpoints. The task of summarizing a scientific paper using its set of citation sentences is called citationbased summarization. There has been previous work done on citationbased summarization (Nanba et al., 2000; Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Mohammad et al., 2009). Previous work focused on the extraction aspect; i.e. analyzing the collection of citation sentences and selecting a representative subset that covers the main aspects of the paper. The cohesion and the readability of the produced summaries have been mostly ignored. This resulted in noisy and confusing summaries. In this work, we focus on the coherence and readability aspects of the problem. Our approach produces citation-based summaries in three stages: preprocessing, extraction, and postprocessing. Our experiments show that our approach produces b"
P11-1051,P10-1057,1,0.436605,"proposed a method for summarizing scientific articles by building a similarity network of the citation sentences that cite the target paper, and then applying network analysis techniques to find a set of sentences that covers as much of the summarized paper facts as possible. We use this method as one of the baselines when we evaluate our approach. Qazvinian et al. (2010) proposed a citation-based summarization method that first extracts a number of important keyphrases from the set of citation sentences, and then finds the best subset of sentences that covers as many keyphrases as possible. Qazvinian and Radev (2010) addressed the problem of identifying the non-explicit citing sentences to aid citation-based summarization. 501 Motivation The coherence and readability of citation-based summaries are impeded by several factors. First, many citation sentences cite multiple papers besides the target. For example, the following is a citation sentence that appeared in the NLP literature and talked about Resnik’s (1999) work. (1) Grefenstette and Nioche (2000) and Jones and Ghani (2000) use the web to generate corpora for languages where electronic resources are scarce, while Resnik (1999) describes a method for"
P11-1051,C10-1101,1,0.721627,"hey concluded that citation summaries are more focused and contain more information than abstracts. Mohammad et al. (2009) suggested using citation information to generate surveys of scientific paradigms. Qazvinian and Radev (2008) proposed a method for summarizing scientific articles by building a similarity network of the citation sentences that cite the target paper, and then applying network analysis techniques to find a set of sentences that covers as much of the summarized paper facts as possible. We use this method as one of the baselines when we evaluate our approach. Qazvinian et al. (2010) proposed a citation-based summarization method that first extracts a number of important keyphrases from the set of citation sentences, and then finds the best subset of sentences that covers as many keyphrases as possible. Qazvinian and Radev (2010) addressed the problem of identifying the non-explicit citing sentences to aid citation-based summarization. 501 Motivation The coherence and readability of citation-based summaries are impeded by several factors. First, many citation sentences cite multiple papers besides the target. For example, the following is a citation sentence that appeared"
P11-1051,radev-etal-2004-mead,1,0.802813,"ch of the 30 papers in dataset2. We also generate summaries for the papers using a number of baseline systems (described in Section 5.3.1). All the generated summaries were 5 sentences long. We use the Recall-Oriented Understudy for Gisting Evaluation (ROUGE) based on the longest common substrings (ROUGE-L) as our evaluation metric. 5.3.1 Baselines We evaluate the extraction quality of our system (FL) against 7 different baselines. In the first baseline, the sentences are selected randomly from the set of citation sentences and added to the summary. The second baseline is the MEAD summarizer (Radev et al., 2004) with all its settings set to default. The third baseline is LexRank (Erkan and Radev, 2004) run on the entire set of citation sentences of the target paper. The forth baseline is Qazvinian and Radev (2008) citation-based summarizer (QR08) in which the citation sentences are first clustered then the sentences within each cluster are ranked using LexRank. The remaining baselines are variations of our system produced by removing one component from the pipeline at a time. In one variation (FL-1), we remove the sentence filtering component. In another variation (FL-2), we remove the sentence class"
P11-1051,W09-3607,1,0.851661,"e reference with a suitable personal pronoun or remove it. The reference is replaced with a pronoun if it is part of the sentence and this replacement does not make the sentence ungrammatical. The reference is removed if it is not part of the sentence. If the sentence con505 Evaluation We provide three levels of evaluation. First, we evaluate each of the components in our system separately. Then we evaluate the summaries that our system generate in terms of extraction quality. Finally, we evaluate the coherence and readability of the summaries. 5.1 Data We use the ACL Anthology Network (AAN) (Radev et al., 2009) in our evaluation. AAN is a collection of more than 16000 papers from the Computational Linguistics journal, and the proceedings of the ACL conferences and workshops. AAN provides all citation information from within the network including the citation network, the citation sentences, and the citation context for each paper. We used 55 papers from AAN as our data. The papers have a variable number of citation sentences, ranging from 15 to 348. The total number of citation sentences in the dataset is 4,335. We split the data randomly into two different sets; one for evaluating the components of"
P11-1051,N07-1040,0,0.157777,"using information latent in citations has been explored tens of years back (Garfield et al., 1984; Hodges, 1972). Since then, there has been a large body of research done on citations. Nanba and Okumura (2000) analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrasebased rules. They also used citation categorization to support a system for writing surveys (Nanba and Okumura, 1999). Newman (2001) analyzed the structure of the citation networks. Teufel et al. (2006) addressed the problem of classifying citations based on their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analyzing citation sentences. Teufel (2007) described a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work. Elkiss et al. (2008) performed a study on citation summaries and their importance. They concluded that citation summaries are more focused and contain more information than abstracts. Mohammad et al. (2009) suggested using citati"
P11-1051,W06-1613,0,0.707131,"The idea of analyzing and utilizing citation information is far from new. The motivation for using information latent in citations has been explored tens of years back (Garfield et al., 1984; Hodges, 1972). Since then, there has been a large body of research done on citations. Nanba and Okumura (2000) analyzed citation sentences and automatically categorized citations into three groups using 160 pre-defined phrasebased rules. They also used citation categorization to support a system for writing surveys (Nanba and Okumura, 1999). Newman (2001) analyzed the structure of the citation networks. Teufel et al. (2006) addressed the problem of classifying citations based on their function. Siddharthan and Teufel (2007) proposed a method for determining the scientific attribution of an article by analyzing citation sentences. Teufel (2007) described a rhetorical classification task, in which sentences are labeled as one of Own, Other, Background, Textual, Aim, Basis, or Contrast according to their role in the authors argument. In parts of our approach, we were inspired by this work. Elkiss et al. (2008) performed a study on citation summaries and their importance. They concluded that citation summaries are m"
P11-1110,W02-1022,0,0.0147906,"lectively written summaries, and give evidence of 1098 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1098–1108, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics the diversity of perspectives and its cause. We believe that out experiments will give insight into new models of text generation, which is aimed at modeling the process of producing natural language texts, and is best characterized as the process of making choices between alternate linguistic realizations, also known as lexical choice (Elhadad, 1995; Barzilay and Lee, 2002; Stede, 1995). 2 Prior Work In summarization, a number of previous methods have focused on diversity. (Mei et al., 2010) introduce a diversity-focused ranking methodology based on reinforced random walks in information networks. Their random walk model introduces the rich-gets-richer mechanism to PageRank with reinforcements on transition probabilities between vertices. A similar ranking model is the Grasshopper ranking model (Zhu et al., 2007), which leverages an absorbing random walk. This model starts with a regular time-homogeneous random walk, and in each step the node with the highest w"
P11-1110,J05-3002,0,0.0327498,"luated based on their quality in performing a specific task (Sp¨arck-Jones, 1999) or intrinsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1 Document Understanding Conference 3 Data Annotation boston fans celebrate world series win; 37 arrests reported The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. The headlines datasets consist of 25 clusters of news hea"
P11-1110,J96-2004,0,0.0474714,"and showed that relevance judgments differ significantly between humans but relative rankings show high degrees of stability across annotators. However, perhaps the closest work to this paper is (van Halteren and Teufel, 2004) in which 40 Dutch students and 10 NLP researchers were asked to summarize a BBC news report, resulting in 50 different summaries. Teufel and van Halteren also used 6 DUC1 -provided summaries, and annotations from 10 student participants and 4 additional researchers, to create 20 summaries for another news article in the DUC datasets. They calculated the Kappa statistic (Carletta, 1996; Krippendorff, 1980) and observed high agreement, indicating that the task of atomic semantic unit (factoid) extraction can be robustly performed in naturally occurring text, without any copy-editing. The diversity of perspectives and the unprecedented growth of the factoid inventory also affects evaluation in text summarization. Evaluation methods are either extrinsic, in which the summaries are evaluated based on their quality in performing a specific task (Sp¨arck-Jones, 1999) or intrinsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren an"
P11-1110,J86-3001,0,0.205342,"ass activities that are not centrally coordinated (Blumer, 1951). Collective behavior is different from group behavior in the following ways: (a) it involves limited social interaction, (b) membership is fluid, and (c) it generates weak and unconventional norms (Smelser, 1963). In this paper, we focus on the computational analysis of collective discourse, a collective behavior seen in interactive content contribution and text summarization in online social media. In collective discourse each individual’s behavior is largely independent of that of other individuals. In social media, discourse (Grosz and Sidner, 1986) is often a collective reaction to an event. One scenario leading to collective reaction to a welldefined subject is when an event occurs (a movie is released, a story occurs, a paper is published) and people independently write about it (movie reviews, news headlines, citation sentences). This process of content generation happens over time, and each person chooses the aspects to cover. Each event has an onset and a time of death after which nothing is written about it. Tracing the generation of content over many instances will reveal temporal patterns that will allow us to make sense of the"
P11-1110,P99-1004,0,0.0162381,"ers exhibit similar results headlines citations 0.8 0.6 0.4 0.2 0 abortion amazon babies burger colombia england gervais google ireland maine mercury miss monkey mozart nobel priest ps3slim radiation redsox russian scientist soupy sweden typhoon yale A00_1023 A00_1043 A00_2024 C00_1072 C96_1058 D03_1017 D04_9907 H05_1047 H05_1079 J04_4002 N03_1017 N04_1033 P02_1006 P03_1001 P05_1012 P05_1013 P05_1014 P05_1033 P97_1003 P99_1065 W00_0403 W00_0603 W03_0301 W03_0510 W05_1203 Pyramid Score 1 Figure 3: The 25th to 75th percentile pyramid score range in individual clusters words that is inspired by (Lee, 1999). We represent each word by its context in the cluster and find the similarity of such contexts. Particularly, each word wi is represented by a bag of words, `i , that have a surface distance of 3 or smaller to wi anywhere in the cluster. In other words, `i contains any word that co-occurs with wi in a 4-gram in the cluster. This bag of words representation of words enables us to find the word-pair similarities. `~i · `~j sim(wi , wj ) = q |`~i ||`~j | hitting arrest police poor unhappy pitching fan celebrations poorer jump sox red 2nd sweeps second glory baseball dynasty victory title Pajek F"
P11-1110,W02-0406,0,0.0421576,".76 ± 0.4 0.80 ± 0.4 trigram 0.89 ± 0.3 Table 2: Agreement between different annotators in terms of average Kappa in 25 headline clusters. headlines 0 citations 0 10 10 Pr(X ≥ c) Pr(X ≥ c) Pr(X ≥ c) Pr(X ≥ c) Following these examples, we asked two annotators to annotate all 1, 390 headlines, and 926 citations. The annotators were asked to follow precise guidelines in nugget extraction. Our guidelines instructed annotators to extract non-overlapping phrases from each headline as nuggets. Therefore, each nugget should be a substring of the headline that represents a semantic unit4 . Previously (Lin and Hovy, 2002) had shown that information overlap judgment is a difficult task for human annotators. To avoid such a difficulty, we enforced our annotators to extract non-overlapping nuggets from a summary to make sure that they are mutually independent and that information overlap between them is minimized. Finding agreement between annotated welldefined nuggets is straightforward and can be calculated in terms of Kappa. However, when nuggets themselves are to be extracted by annotators, the task becomes less obvious. To calculate the agreement, we annotated 10 randomly selected headline clusters twice and"
P11-1110,N04-1019,0,0.700076,"1980) and observed high agreement, indicating that the task of atomic semantic unit (factoid) extraction can be robustly performed in naturally occurring text, without any copy-editing. The diversity of perspectives and the unprecedented growth of the factoid inventory also affects evaluation in text summarization. Evaluation methods are either extrinsic, in which the summaries are evaluated based on their quality in performing a specific task (Sp¨arck-Jones, 1999) or intrinsic where the quality of the summary itself is evaluated, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004;"
P11-1110,P04-1035,0,0.00697229,"ted, regardless of any applied task (van Halteren and Teufel, 2003; Nenkova and Passonneau, 2004). These evaluation methods assess the information content in the summaries that are generated automatically. Finally, recent research on analyzing online social media shown a growing interest in mining news stories and headlines because of its broad applications ranging from “meme” tracking and spike detection (Leskovec et al., 2009) to text summarization (Barzilay and McKeown, 2005). In similar work on blogs, it is shown that detecting topics (Kumar et al., 2003; Adar et al., 2007) and sentiment (Pang and Lee, 2004) in the blogosphere can help identify influential bloggers (Adar et al., 2004; Java et al., 2006) and mine opinions about products (Mishne and Glance, 2006). 1 Document Understanding Conference 3 Data Annotation boston fans celebrate world series win; 37 arrests reported The datasets used in our experiments represent two completely different categories: news headlines, and scientific citation sentences. The headlines datasets consist of 25 clusters of news headlines collected from Google News2 , and the citations datasets have 25 clusters of citations to specific scientific papers from the ACL"
P11-1110,D10-1007,0,0.0288059,"et al., 2010) introduce a diversity-focused ranking methodology based on reinforced random walks in information networks. Their random walk model introduces the rich-gets-richer mechanism to PageRank with reinforcements on transition probabilities between vertices. A similar ranking model is the Grasshopper ranking model (Zhu et al., 2007), which leverages an absorbing random walk. This model starts with a regular time-homogeneous random walk, and in each step the node with the highest weight is set as an absorbing state. The multi-view point summarization of opinionated text is discussed in (Paul et al., 2010). Paul et al. introduce Comparative LexRank, based on the LexRank ranking model (Erkan and Radev, 2004). Their random walk formulation is to score sentences and pairs of sentences from opposite viewpoints (clusters) based on both their representativeness of the collection as well as their contrastiveness with each other. Once a lexical similarity graph is built, they modify the graph based on cluster information and perform LexRank on the modified cosine similarity graph. The most well-known paper that address diversity in summarization is (Carbonell and Goldstein, 1998), which introduces Maxi"
P11-1110,C08-1087,1,0.951754,"representativeness of the collection as well as their contrastiveness with each other. Once a lexical similarity graph is built, they modify the graph based on cluster information and perform LexRank on the modified cosine similarity graph. The most well-known paper that address diversity in summarization is (Carbonell and Goldstein, 1998), which introduces Maximal Marginal Relevance (MMR). This method is based on a greedy algorithm that picks sentences in each step that are the least similar to the summary so far. There are a few other diversity-focused summarization systems like C-LexRank (Qazvinian and Radev, 2008), which employs document clustering. These papers try to increase diversity in summarizing documents, but do not explain the type of the diversity in their inputs. In this paper, we give an insightful discussion on the nature of the diversity seen in collective dis1099 course, and will explain why some of the mentioned methods may not work under such environments. In prior work on evaluating independent contributions in content generation, Voorhees (Voorhees, 1998) studied IR systems and showed that relevance judgments differ significantly between humans but relative rankings show high degrees"
P11-1110,P10-1057,1,0.906291,"Missing"
P11-1110,W03-0508,0,0.324668,"Missing"
P11-1110,W04-3254,0,0.0491764,"Missing"
P11-1110,N07-1013,0,0.0411983,"Missing"
P11-2104,banea-etal-2008-bootstrapping,0,0.614092,"n connection between words in the same language as well as multilingual connections. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results. 1 Introduction A great body of research work has focused on identifying the semantic orientation of words. Word polarity is a very important feature that has been used in several applications. For example, the problem of mining product reputation from Web reviews has been extensively studied (Turney, 2002; Morinaga et al., 2002; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Banea et al., 2008). This is a very 592 important task given the huge amount of product reviews written on the Web and the difficulty of manually handling them. Another interesting application is mining attitude in discussions (Hassan et al., 2010), where the attitude of participants in a discussion is inferred using the text they exchange. Due to its importance, several researchers have addressed the problem of identifying the semantic orientation of individual words. This work has almost exclusively focused on English. Most of this work used several language dependent resources. For example Turney and Littman"
P11-2104,elkateb-etal-2006-building,0,0.0835069,"tions, we use Wordnet (Miller, 1995). Wordnet is a large lexical database of English. Words are grouped in synsets to express distinct concepts. We add a link between two words if they occur in the same Wordnet synset. We also add a link between two words if they have a hypernym or a similar-to relation. Foreign-Foreign connections are created in a similar way to the English connections. Some other languages have lexical resources based on the design of the Princeton English Wordnet. For example: Euro Wordnet (EWN) (Vossen, 1997), Arabic Wordnet (AWN) (Elkateb, 2006; Black and Fellbaum, 2006; Elkateb and Fellbaum, 2006) and the Hindi Wordnet (Narayan et al., 2002; S. Jha, 2001). We also use co-occurrence statistics similar to the work of Hatzivassiloglou and McKeown (1997). Finally, to connect foreign words to English words, we use a foreign to English dictionary. For every word in a list of foreign words, we look up its meaning in a dictionary and add an edge between the foreign word and every other English word that appeared as a possible meaning for it. 3.2 Semantic Orientation Prediction We use the multilingual network we described above to predict the semantic orientation of words based on the mean hitt"
P11-2104,2006.bcs-1.2,0,0.0238124,"Missing"
P11-2104,P10-1041,1,0.643908,"inferred using the text they exchange. Due to its importance, several researchers have addressed the problem of identifying the semantic orientation of individual words. This work has almost exclusively focused on English. Most of this work used several language dependent resources. For example Turney and Littman (2003) use the entire English Web corpus by submitting queries consisting of the given word and a set of seeds to a search engine. In addition, several other methods have used Wordnet (Miller, 1995) for connecting semantically related words (Kamps et al., 2004; Takamura et al., 2005; Hassan and Radev, 2010). When we try to apply those methods to other languages, we run into the problem of the lack of resources in other languages when compared to English. For example, the General Inquirer lexicon (Stone et al., 1966) has thousands of English words labeled with semantic orientation. Most of the literature has used it as a source of labeled seeds or for evaluation. Such lexicons are not readily available in other languages. Another source that has been widely used for this task is Wordnet (Miller, 1995). Even though other Wordnets have been built for other languages, their coverage is very limited"
P11-2104,D10-1121,1,0.865028,"ction A great body of research work has focused on identifying the semantic orientation of words. Word polarity is a very important feature that has been used in several applications. For example, the problem of mining product reputation from Web reviews has been extensively studied (Turney, 2002; Morinaga et al., 2002; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Banea et al., 2008). This is a very 592 important task given the huge amount of product reviews written on the Web and the difficulty of manually handling them. Another interesting application is mining attitude in discussions (Hassan et al., 2010), where the attitude of participants in a discussion is inferred using the text they exchange. Due to its importance, several researchers have addressed the problem of identifying the semantic orientation of individual words. This work has almost exclusively focused on English. Most of this work used several language dependent resources. For example Turney and Littman (2003) use the entire English Web corpus by submitting queries consisting of the given word and a set of seeds to a search engine. In addition, several other methods have used Wordnet (Miller, 1995) for connecting semantically re"
P11-2104,P97-1023,0,0.810357,"ources such as English is useful for improving the performance on other languages with limited resources. The rest of the paper is structured as follows. In section 2, we review some of the related prior work. We define our problem and explain our approach in Section 3. Results and discussion are presented in Section 4. We conclude in Section 5. 2 Related Work The problem of identifying the polarity of individual words is a well-studied problem that attracted several research efforts in the past few years. In this 593 section, we survey several methods that addressed this problem. The work of Hatzivassiloglou and McKeown (1997) is among the earliest efforts that addressed this problem. They proposed a method for identifying the polarity of adjectives. Their method is based on extracting all conjunctions of adjectives from a given corpus and then they classify each conjunctive expression as either the same orientation such as “simple and well-received” or different orientation such as “simplistic but well-received”. Words are clustered into two sets and the cluster with the higher average word frequency is classified as positive. Turney and Littman (2003) identify word polarity by looking at its statistical associati"
P11-2104,C00-1044,0,0.135515,"urrence statistics. They measure the random walk mean hitting time of the given word to the positive set and the negative set. They show that their method outperforms other related methods and that it is more immune to noisy word connections. Identifying the semantic orientation of individual words is closely related to subjectivity analysis. Subjectivity analysis focused on identifying text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Some approaches to subjectivity analysis disregard the context phrases and words appear in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008), while others take it into consideration (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). 3 Approach The general goal of this work is to mine the semantic orientation of foreign words. We do this by creating a multilingual network of words. In this network two words are connected if we believe that they are semantically related. The network has EnglishEnglish, English-Foreign and Foreign-Foreign connections. Some of the English words will be used as seeds for which we know the semantic orientation. Given such a net"
P11-2104,kamps-etal-2004-using,0,0.644024,"Missing"
P11-2104,W06-1642,0,0.150722,"given word to the positive/negative seeds. Wordnet (Miller, 1995), thesaurus and cooccurrence statistics have been widely used to measure word relatedness by several semantic orientation prediction methods. Kamps et al. (2004) use the length of the shortest-path in Wordnet connecting any given word to positive/negative seeds to identify word polarity. Hu and Liu (2004) use Wordnet synonyms and antonyms to bootstrap from words with known polarity to words with unknown polarity. They assign any given word the label of its synonyms or the opposite label of its antonyms if any of them are known. Kanayama and Nasukawa (2006) used syntactic features and context coherency, defined as the tendency for same polarities to appear successively, to acquire polar atoms. Takamura et al. (2005) proposed using spin models for extracting semantic orientation of words. They construct a network of words using gloss definitions, thesaurus and cooccurrence statistics. They regard each word as an electron. Each electron has a spin and each spin has a direction taking one of two values: up or down. Two neighboring spins tend to have the same orientation from an energetic point of view. Their hypothesis is that as neighboring electr"
P11-2104,H05-1043,0,0.275924,"on of foreign words based on connection between words in the same language as well as multilingual connections. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results. 1 Introduction A great body of research work has focused on identifying the semantic orientation of words. Word polarity is a very important feature that has been used in several applications. For example, the problem of mining product reputation from Web reviews has been extensively studied (Turney, 2002; Morinaga et al., 2002; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Banea et al., 2008). This is a very 592 important task given the huge amount of product reviews written on the Web and the difficulty of manually handling them. Another interesting application is mining attitude in discussions (Hassan et al., 2010), where the attitude of participants in a discussion is inferred using the text they exchange. Due to its importance, several researchers have addressed the problem of identifying the semantic orientation of individual words. This work has almost exclusively focused on English. Most of this work used several language dependent resources. For exampl"
P11-2104,W03-1014,0,0.203605,"tive set and the negative set. They show that their method outperforms other related methods and that it is more immune to noisy word connections. Identifying the semantic orientation of individual words is closely related to subjectivity analysis. Subjectivity analysis focused on identifying text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Some approaches to subjectivity analysis disregard the context phrases and words appear in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008), while others take it into consideration (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). 3 Approach The general goal of this work is to mine the semantic orientation of foreign words. We do this by creating a multilingual network of words. In this network two words are connected if we believe that they are semantically related. The network has EnglishEnglish, English-Foreign and Foreign-Foreign connections. Some of the English words will be used as seeds for which we know the semantic orientation. Given such a network, we will measure the mean hitting time in a random walk starting at any given wor"
P11-2104,P05-1017,0,0.689186,"nts in a discussion is inferred using the text they exchange. Due to its importance, several researchers have addressed the problem of identifying the semantic orientation of individual words. This work has almost exclusively focused on English. Most of this work used several language dependent resources. For example Turney and Littman (2003) use the entire English Web corpus by submitting queries consisting of the given word and a set of seeds to a search engine. In addition, several other methods have used Wordnet (Miller, 1995) for connecting semantically related words (Kamps et al., 2004; Takamura et al., 2005; Hassan and Radev, 2010). When we try to apply those methods to other languages, we run into the problem of the lack of resources in other languages when compared to English. For example, the General Inquirer lexicon (Stone et al., 1966) has thousands of English words labeled with semantic orientation. Most of the literature has used it as a source of labeled seeds or for evaluation. Such lexicons are not readily available in other languages. Another source that has been widely used for this task is Wordnet (Miller, 1995). Even though other Wordnets have been built for other languages, their"
P11-2104,P02-1053,0,0.00590897,"ords. We use this network to identify the semantic orientation of foreign words based on connection between words in the same language as well as multilingual connections. The method is experimentally tested using a manually labeled set of positive and negative words and has shown very promising results. 1 Introduction A great body of research work has focused on identifying the semantic orientation of words. Word polarity is a very important feature that has been used in several applications. For example, the problem of mining product reputation from Web reviews has been extensively studied (Turney, 2002; Morinaga et al., 2002; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005; Banea et al., 2008). This is a very 592 important task given the huge amount of product reviews written on the Web and the difficulty of manually handling them. Another interesting application is mining attitude in discussions (Hassan et al., 2010), where the attitude of participants in a discussion is inferred using the text they exchange. Due to its importance, several researchers have addressed the problem of identifying the semantic orientation of individual words. This work has almost exclusively focused on English"
P11-2104,W03-1017,0,0.0590492,"e set. They show that their method outperforms other related methods and that it is more immune to noisy word connections. Identifying the semantic orientation of individual words is closely related to subjectivity analysis. Subjectivity analysis focused on identifying text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Some approaches to subjectivity analysis disregard the context phrases and words appear in (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008), while others take it into consideration (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). 3 Approach The general goal of this work is to mine the semantic orientation of foreign words. We do this by creating a multilingual network of words. In this network two words are connected if we believe that they are semantically related. The network has EnglishEnglish, English-Foreign and Foreign-Foreign connections. Some of the English words will be used as seeds for which we know the semantic orientation. Given such a network, we will measure the mean hitting time in a random walk starting at any given word to the positive set of seeds"
P11-2104,H05-2017,0,\N,Missing
P11-4021,W09-1416,1,0.861185,"Missing"
P11-4021,C08-1087,1,0.839133,"Missing"
P11-4021,A00-2018,0,0.00741184,"s and perform the basic processing tasks. For example, Clair::Document defines a data structure for holding textual data in various formats, and performs the basic text processing tasks such as tokenization, stemming, tag stripping, etc. Another set of modules perform more specific tasks in the three areas of focus (NLP, IR, and NA). For example, Clair::Bio::GIN::Interaction is devoted to protein-protein interaction extraction from biomedical text. A third set contains modules that interface Clairlib to external tools. For example, Clair::Utils::Parse provides an interface to Charniak parser (Charniak, 2000), Stanford parser (Klein and Manning, 2003), and Chunklink2 . Each module has a well-defined API. The API is oriented to developers to help them write applications and build systems on top of Clairlib modules; and to researchers to help them write applications and setup custom experiments for their research. 2 partition.pl -graph graph.net -method GirvanNewman -n 4 uses the GrivanNewman algorithm to divide a given graph into 4 partitions. 122 Graphical User Interface The graphical user interface (GUI) is an important feature that has been recently added to Clairlib and constituted a quantum le"
P11-4021,P03-1054,0,0.00686307,"tasks. For example, Clair::Document defines a data structure for holding textual data in various formats, and performs the basic text processing tasks such as tokenization, stemming, tag stripping, etc. Another set of modules perform more specific tasks in the three areas of focus (NLP, IR, and NA). For example, Clair::Bio::GIN::Interaction is devoted to protein-protein interaction extraction from biomedical text. A third set contains modules that interface Clairlib to external tools. For example, Clair::Utils::Parse provides an interface to Charniak parser (Charniak, 2000), Stanford parser (Klein and Manning, 2003), and Chunklink2 . Each module has a well-defined API. The API is oriented to developers to help them write applications and build systems on top of Clairlib modules; and to researchers to help them write applications and setup custom experiments for their research. 2 partition.pl -graph graph.net -method GirvanNewman -n 4 uses the GrivanNewman algorithm to divide a given graph into 4 partitions. 122 Graphical User Interface The graphical user interface (GUI) is an important feature that has been recently added to Clairlib and constituted a quantum leap in its development. The main purpose of"
P11-4021,P11-1051,1,0.84578,"Missing"
P11-4021,P04-3031,0,0.0488292,"se and conduct experiments to verify their hypotheses. Educators also find these tools useful in class demonstrations and for setting up practical programming assignments and projects for their students. A large number of systems have been developed over the years to solve problems and perform tasks in Natural Language Processing, Information Retrieval, or Network Analysis. Many of these systems perform specific tasks such as parsing, Graph Partitioning, co-reference resolution, web crawling etc. Some other systems are frameworks for performing generic tasks in one area of focus such as NLTK (Bird and Loper, 2004) and GATE (Cunningham et al., 2002) for Natural Language Processing; Pajek (Batagelj and Mrvar, 2003) and GUESS (Adar, 2006) for Network Analysis and Visualization; and Lemur1 for Language Modeling and Information Retrieval. This paper presents Clairlib, an open-source toolkit that contains a suit of modules for generic tasks in Natural Language Processing (NLP), Information Retrieval (IR), and Network Analysis (NA). While many systems have been developed to address tasks or subtasks in one of these areas as we have just mentioned, Clairlib provides one integrated environment that addresses ta"
P11-4021,C08-1040,1,\N,Missing
P11-4021,P06-4018,0,\N,Missing
P12-1042,P11-4021,1,0.804094,"Missing"
P12-1042,W11-1701,0,0.0370505,"lice officer asking you to prove your citizenship is Unconstitutional? As soon as you start trading Constitutional rights for ”security”, then you’ve lost. Table 1: Example posts from the Arizona Immigration Law thread pairing as shown in Section 3 below. 2.3 Community Mining Previous work also studied community mining in social media sites. Somasundaran and Wiebe (2009) presents an unsupervised opinion analysis method for debate-side classification. They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. This work is limited to dual sided debates and defines the problem as a classification task where the two debate sides are know beforehand. Our work is characterized by handling multi-side debates and by regarding the problem as a clustering problem where the number of sides is not known by the algorithm. This work also utilizes only discussant-to-topic attitud"
P12-1042,E06-1027,0,0.0057825,"granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and"
P12-1042,banea-etal-2008-bootstrapping,0,0.0134465,"each participant. The rest of this paper is organized as follows. Section 2 examines the previous work. We describe the data used in the paper in Section 2.4. Section 3 presents our approach. Experiments, results and analysis are presented in Section 4. We conclude in Section 5 2 Related Work mine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another"
P12-1042,C08-2004,0,0.153544,"n another work, Kim and Hovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; 401 Yessenalina et al., 2010). This work infers agreement between speakers based on cases where one speaker mentions another by name, and a simple algorithm for determining the polarity of the sentence in which the mention occurs. This work shows that even with the resulting sparsely connected agreement structure, the MinCut algorithm can improve over stance classification based on textual information alone. This work also requires that the debate sides be known by the algorithm and it only identifies discussant-to-discussant attitude. In our experiments below we show that identifying both d"
P12-1042,H05-1091,0,0.056093,"Stanford Parser (Klein and Manning, 2003) to generate the dependency parse tree of each sentence in the thread. An opinion word and a target form a pair if they stratify at least one of our dependency rules. Table 4 illustrates some 404 of these rules 5 . The rules basically examine the types of the dependencies on the shortest path that connect the opinion word and the target in the dependency parse tree. It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). If a sentence S in a post written by participant Pi contains an opinion word OPj and a target T Rk , and if the opinion-target pair satisfies one of our dependency rules, we say that Pi expresses an attitude towards T Rk . The polarity of the attitude is determined by the polarity of OPj . We represent this as + − Pi → T Rk if OPj is positive and Pi → T Rk if OPj is negative. It is likely that the same participant Pi express sentiment toward the same target T Rk multiple times in different sentences in different posts. We keep track of the counts of all the instances of positive/negative att"
P12-1042,P05-1045,0,0.00351808,". For example, the noun group Arizona immigration law is mentioned by Discussant 1 and Discussant 2 in snippets 1 and 2 above respectively. Therefore, we replace it with a placehold as illustrated in snippets (4) and (5) below. We only consider as entities noun groups that contain two words or more. We impose this requirement because individual nouns are very common and regarding all of them as entities will introduce significant noise. In addition to this shallow parsing method, we also use named entity recognition (NER) to identify more entities. We use the Stanford Named Entity Recognizer (Finkel et al., 2005) for this purpose. It recognizes three types of entities: person, location, and organization. We impose no restrictions on the entities identified using this method. Again, we replace each distinct entity with a unique placeholder. The final set of entities identified in a thread is the union of the entities identified by the two aforementioned methods. Table 3 Finally, a challenge that always arises when performing text mining tasks at this level of granularity is that entities are usually expressed by anaphorical pronouns. Previous work has shown that For example, the following snippet conta"
P12-1042,grover-etal-2000-lt,0,0.0397916,"s another discussant, either the discussant name is mentioned explicitly or a second person pronoun is used to indicate that the opinion is targeting the recipient of the post. For example, in snippet (2) above the second person pronoun you indicates that the opinion word disagree is targeting Discussant 1, the recipient of the post. The target of opinion can also be an entity mentioned in the discussion. We use two methods to identify such entities. The first method uses shallow parsing to identify noun groups (NG). We use the Edinburgh Language Technology Text Tokenization Toolkit (LT-TTT) (Grover et al., 2000) for this purpose. We consider as an entity any noun group that is mentioned by at least two different discussants. We replace each identified entity with a unique placeholder (EN T IT YID ). For example, the noun group Arizona immigration law is mentioned by Discussant 1 and Discussant 2 in snippets 1 and 2 above respectively. Therefore, we replace it with a placehold as illustrated in snippets (4) and (5) below. We only consider as entities noun groups that contain two words or more. We impose this requirement because individual nouns are very common and regarding all of them as entities wil"
P12-1042,P10-1041,1,0.514903,"identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et"
P12-1042,D10-1121,1,0.869877,"prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatziva"
P12-1042,P11-2104,1,0.551222,"n conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product compo"
P12-1042,P97-1023,0,0.182692,"(2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedne"
P12-1042,C00-1044,0,0.151386,"up and the subgroup membership of each participant. The rest of this paper is organized as follows. Section 2 examines the previous work. We describe the data used in the paper in Section 2.4. Section 3 presents our approach. Experiments, results and analysis are presented in Section 4. We conclude in Section 5 2 Related Work mine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant wi"
P12-1042,P10-2049,0,0.326,"components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appear in general English text. However, such statistics may not be reliable when the corpus size is small. In another related work, Jakob and Gurevych (2010) showed that resolving the anaphoric links in the text significantly improves opinion target extraction. In our work, we use anaphora resolution to improve opinion-target Participant A posted: I support Arizona because they have every right to do so. They are just upholding well-established federal law. All states should enact such a law. Participant B commented on A’s post: I support the law because the federal government is either afraid or indifferent to the issue. Arizona has the right and the responsibility to protect the people of the State of Arizona. If this requires a possible slight"
P12-1042,kamps-etal-2004-using,0,0.0841726,"Missing"
P12-1042,C04-1200,0,0.0270863,"different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context o"
P12-1042,P03-1054,0,0.00465615,"ult of applying this step to snippet (6) is: (6) It doesn’t matter whether you vote for Obama. Obama is unbeatable. Now, both mentions of Obama will be recognized by the Stanford NER system and will be identified as one entity. 3.4 Opinion-Target Pairing At this point, we have all the opinion words and the potential targets identified separately. The next step is to determine which opinion word is targeting which target. We propose a rule based approach for opinion-target pairing. Our rules are based on the dependency relations that connect the words in a sentence. We use the Stanford Parser (Klein and Manning, 2003) to generate the dependency parse tree of each sentence in the thread. An opinion word and a target form a pair if they stratify at least one of our dependency rules. Table 4 illustrates some 404 of these rules 5 . The rules basically examine the types of the dependencies on the shortest path that connect the opinion word and the target in the dependency parse tree. It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). If a sentence"
P12-1042,D07-1114,0,0.0193424,"Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appea"
P12-1042,D07-1113,0,0.0393162,"d punctuation, and opinion dependencies to build a stance classification model. This work is limited to dual sided debates and defines the problem as a classification task where the two debate sides are know beforehand. Our work is characterized by handling multi-side debates and by regarding the problem as a clustering problem where the number of sides is not known by the algorithm. This work also utilizes only discussant-to-topic attitude predictions for debate-side classification. Out work utilizes both discussant-to-topic and discussant-to-discussant attitude predictions. In another work, Kim and Hovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; 401 Yessenalin"
P12-1042,H05-1043,0,0.291514,"en word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granular"
P12-1042,W03-1014,0,0.0342432,"ented in Section 4. We conclude in Section 5 2 Related Work mine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment ana"
P12-1042,P09-1026,0,0.657995,"t the people of the State of Arizona. If this requires a possible slight inconvenience to any citizen so be it. Participant C commented on B’s post: That is such a sad thing to say. You do realize that under the 14th Amendment, the very interaction of a police officer asking you to prove your citizenship is Unconstitutional? As soon as you start trading Constitutional rights for ”security”, then you’ve lost. Table 1: Example posts from the Arizona Immigration Law thread pairing as shown in Section 3 below. 2.3 Community Mining Previous work also studied community mining in social media sites. Somasundaran and Wiebe (2009) presents an unsupervised opinion analysis method for debate-side classification. They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. This work is limited to dual sided debates and defines the problem as a classification task where the two debate side"
P12-1042,C08-1103,0,0.0207489,"lt on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work is to deter400 2.2 Opinion Target Extraction Several methods have been proposed to identify the target of an opinion expression. Most of the work have been done in the context of product reviews mining (Hu and Liu, 2004b; Kobayashi et al., 2007; Mei et al., 2007; Stoyanov and Cardie, 2008). In this context, opinion targets usually refer to product features (i.e. product components or attributes, as defined by Liu (2009)). In the work of Hu and Liu (2004b), they treat frequent nouns and noun phrases as product feature candidates. In our work, we extract as targets frequent noun phrases and named entities that are used by two or more different discussants. Scaffidi et al. (2007) propose a language model approach to product feature extraction. They assume that product features are mentioned more often in product reviews than they appear in general English text. However, such stati"
P12-1042,P05-1017,0,0.0277497,"ning could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studied sentiment in text at different levels of granularity. The first level is identifying the polarity of individual words. Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them. Turney and Littman (2003) used pointwise mutual information (PMI) and latent semantic analysis (LSA) to compute the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) proposed using a spin model to predict word polarity. Other studies used WordNet to improve word polarity prediction (Hu and Liu, 2004a; Kamps et al., 2004; Kim and Hovy, 2004; Andreevskaia and Bergler, 2006). Hassan and Radev (2010) used a random walk model built on top of a word relatedness network to predict the semantic orientation of English words. Hassan et al. (2011) proposed a method to extend their random walk model to assist word polarity identification in other languages including Arabic and Hindi. Other work focused on identifying the subjectivity of words. The goal of this work i"
P12-1042,W06-1639,0,0.413715,"titude predictions. In another work, Kim and Hovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; 401 Yessenalina et al., 2010). This work infers agreement between speakers based on cases where one speaker mentions another by name, and a simple algorithm for determining the polarity of the sentence in which the mention occurs. This work shows that even with the resulting sparsely connected agreement structure, the MinCut algorithm can improve over stance classification based on textual information alone. This work also requires that the debate sides be known by the algorithm and it only identifies discussant-to-discussant attitude. In our experiments below we show th"
P12-1042,P08-4003,0,0.0119738,"Split posts into sentences • Anaphora resolution • Identify named entities • Identify Frequent noun phrases. • Identify mentions of other discussants • Identify polarized words • Identify the contextual polarity of each word Subgroups Discussant Attitude Profiles (DAPs) Opinion-Target Pairing • Dependency Rules Clustering Figure 1: An overview of the subgroups detection system He is unbeatable. Jakob and Gurevych (2010) showed experimentally that resolving the anaphoric links in the text significantly improves opinion target extraction. We use the Beautiful Anaphora Resolution Toolkit (BART) (Versley et al., 2008) to resolve all the anaphoric links within the text of each post separately. The result of applying this step to snippet (6) is: (6) It doesn’t matter whether you vote for Obama. Obama is unbeatable. Now, both mentions of Obama will be recognized by the Stanford NER system and will be identified as one entity. 3.4 Opinion-Target Pairing At this point, we have all the opinion words and the potential targets identified separately. The next step is to determine which opinion word is targeting which target. We propose a rule based approach for opinion-target pairing. Our rules are based on the dep"
P12-1042,H05-2018,0,0.0556596,"rsing We start by parsing the thread to identify posts, participants, and the reply structure of the thread (i.e. who replies to whom). In the datasets described in Section 2.4, all this information was explicitly available in the thread. We tokenize the text of each post and split it into sentences using CLAIRLib (AbuJbara and Radev, 2011). 3.2 Opinion Word Identification The next step is to identify the words that express opinion and determine their polarity (positive or negative). Lehrer (1974) defines word polarity as the direction the word deviates to from the norm. We use OpinionFinder (Wilson et al., 2005a) to identify polarized words and their polarities. The polarity of a word is usally affected by the context in which it appears. For example, the word fine is positive when used as an adjective and negative when used as a noun. For another example, a positive word that appears in a negated context becomes negative. OpinionFinder uses a large set of features to identify the contextual polarity of a given polarized word given its isolated polarity and the sentence in which it appears (Wilson et al., 2005b). Snippet (3) below shows the result of applying this step to snippet (1) above (O means"
P12-1042,H05-1044,0,0.0436636,"rsing We start by parsing the thread to identify posts, participants, and the reply structure of the thread (i.e. who replies to whom). In the datasets described in Section 2.4, all this information was explicitly available in the thread. We tokenize the text of each post and split it into sentences using CLAIRLib (AbuJbara and Radev, 2011). 3.2 Opinion Word Identification The next step is to identify the words that express opinion and determine their polarity (positive or negative). Lehrer (1974) defines word polarity as the direction the word deviates to from the norm. We use OpinionFinder (Wilson et al., 2005a) to identify polarized words and their polarities. The polarity of a word is usally affected by the context in which it appears. For example, the word fine is positive when used as an adjective and negative when used as a noun. For another example, a positive word that appears in a negated context becomes negative. OpinionFinder uses a large set of features to identify the contextual polarity of a given polarized word given its isolated polarity and the sentence in which it appears (Wilson et al., 2005b). Snippet (3) below shows the result of applying this step to snippet (1) above (O means"
P12-1042,D10-1102,0,0.011553,"ovy (2007) predict the results of an election by analyzing discussion threads in online forums that discuss the elections. They use a supervised approach that uses unigrams, bigrams, and trigrams as features. In contrast, our work is unsupervised and uses different types information. Moreover, although this work is related to ours at the goal level, it does not involve any opinion analysis. Another related work classifies the speakers side in a corpus of congressional floor debates, using the speakers final vote on the bill as a labeling for side (Thomas et al., 2006; Bansal et al., 2008; 401 Yessenalina et al., 2010). This work infers agreement between speakers based on cases where one speaker mentions another by name, and a simple algorithm for determining the polarity of the sentence in which the mention occurs. This work shows that even with the resulting sparsely connected agreement structure, the MinCut algorithm can improve over stance classification based on textual information alone. This work also requires that the debate sides be known by the algorithm and it only identifies discussant-to-discussant attitude. In our experiments below we show that identifying both discussant-to-discussant and dis"
P12-1042,W03-1017,0,0.09027,"onclude in Section 5 2 Related Work mine whether a given word is factual or subjective. We use previous work on subjectivity and polarity prediction to identify opinion words in discussions. Some of the work on this problem classifies words as factual or subjective regardless of their context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008). Some other work noticed that the subjectivity of a given word depends on its context. Therefor, several studies proposed using contextual features to determine the subjectivity of a given word within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). The second level of granularity is the sentence level. Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. They define attitude as the mental position of one participant with regard to another participant. A very detailed survey that covers techniques and approaches in sentiment analysis and opinion mining could be found in (Pang and Lee, 2008). 2.1 Sentiment Analysis Our work is related to a huge body of work on sentiment analysis. Previous work has studie"
P12-1042,H05-2017,0,\N,Missing
P12-3023,H05-1091,0,0.0135376,"Our rules are based on the dependency relations that connect the words in a sentence. An opinion word and a target form a pair if the dependency path between them satisfies at least one of our dependency rules. Table 1 illustrates some of these rules. The rules basically examine the types of dependency relations on the shortest path that connect the opinion word and the target in the dependency parse tree. It has been shown in previous work on relation extraction that the shortest dependency path between any two entities captures the information required to assert a relationship between them (Bunescu and Mooney, 2005). If a sentence S in a post written by participant Pi contains an opinion word OPj and a target T Rk , and if the opinion-target pair satisfies one of our dependency rules, we say that Pi expresses an attitude towards T Rk . The polarity of the attitude is determined by + the polarity of OPj . We represent this as Pi → T Rk − if OPj is positive and Pi → T Rk if OPj is negative. Negation is handled in this step by reversing the polarity if the polarized expression is part of a 135 neg dependency relation. It is likely that the same participant Pi expresses sentiment towards the same target T Rk"
P12-3023,P11-4009,0,0.0139297,"f the Association for Computational Linguistics, pages 133–138, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Opinionfinder (Wilson et al., 2005) is a system for mining opinions from text. SENTIWORDNET (Esuli and Sebastiani, 2006) is a lexical resource in which each WordNet synset is associated to three numerical scores Obj(s), Pos(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. Dr Sentiment (Das and Bandyopadhyay, 2011) is an online interactive gaming technology used to crowd source human knowledge to build an extension of SentiWordNet. Another research line focused on analyzing online discussions. For example, Lin et al. (2009) proposed a sparse coding-based model that simultaneously models the semantics and the structure of threaded discussions. Shen et al. (2006) proposed a method for exploiting the temporal and lexical similarity information in discussion streams to identify the reply structure of the dialog. Many systems addressed the problem of extracting social networks from discussions (Elson et al.,"
P12-3023,P10-1015,0,0.0135054,"adhyay, 2011) is an online interactive gaming technology used to crowd source human knowledge to build an extension of SentiWordNet. Another research line focused on analyzing online discussions. For example, Lin et al. (2009) proposed a sparse coding-based model that simultaneously models the semantics and the structure of threaded discussions. Shen et al. (2006) proposed a method for exploiting the temporal and lexical similarity information in discussion streams to identify the reply structure of the dialog. Many systems addressed the problem of extracting social networks from discussions (Elson et al., 2010; McCallum et al., 2007). Other related sentiment analysis systems include MemeTube (Li et al., 2011), a sentiment-based system for analyzing and displaying microblog messages; and C-Feel-It (Joshi et al., 2011), a sentiment analyzer for micro-blogs. In the rest of this paper, we describe the system architecture, implementation, usage, and its evaluation. 2 System Overview Figure 1 shows a block diagram of the system components and the processing pipeline. The first component is the thread parsing component which takes as input a discussion thread and parses it to identify posts, participants,"
P12-3023,esuli-sebastiani-2006-sentiwordnet,0,0.0161487,"ining and subgroup detection in online discussions. This work is related to previous work in the areas of sentiment analysis and online discussion mining. Many previous systems studied the problem of iden133 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 133–138, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Opinionfinder (Wilson et al., 2005) is a system for mining opinions from text. SENTIWORDNET (Esuli and Sebastiani, 2006) is a lexical resource in which each WordNet synset is associated to three numerical scores Obj(s), Pos(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. Dr Sentiment (Das and Bandyopadhyay, 2011) is an online interactive gaming technology used to crowd source human knowledge to build an extension of SentiWordNet. Another research line focused on analyzing online discussions. For example, Lin et al. (2009) proposed a sparse coding-based model that simultaneously models the semantics and the structure of threaded discussions. Shen et al. (200"
P12-3023,P97-1023,0,0.213523,"opic. Opinion-target pairs are identified using a number of hand-crafted rules. The functionality of this system is based on our previous work on attitude mining and subgroup detection in online discussions. This work is related to previous work in the areas of sentiment analysis and online discussion mining. Many previous systems studied the problem of iden133 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 133–138, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Opinionfinder (Wilson et al., 2005) is a system for mining opinions from text. SENTIWORDNET (Esuli and Sebastiani, 2006) is a lexical resource in which each WordNet synset is associated to three numerical scores Obj(s), Pos(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. Dr Sentiment (Das and Bandyopadhyay, 2011) is an online interactive gaming technology used to crowd source human knowledge to build an extension of SentiWordNet. Another research line focused on analyzing online discussions. For example, Lin et"
P12-3023,P10-2049,0,0.0259475,"We impose this requirement because individual nouns are very common and considering all of them as candidate targets will introduce significant noise. In addition to this shallow parsing method, we also use named entity recognition (NER) to identify more targets. The named entity tool that we use recognizes three types of entities: person, location, and organization. We impose no restrictions on the entities identified using this method. A challenge that always arises when performing text mining tasks at this level of granularity is that entities are usually expressed by anaphorical pronouns. Jakob and Gurevych (2010) showed experimentally that resolving the anaphoric links Thread Parsing Discussion Thread ….……. ….……. ….……. Target Identification Opinion Identification • Identify posts • Identify discussants • Identify the reply structure • Tokenize text. • Split posts into sentences • Anaphora resolution • Identify named entities • Identify Frequent noun phrases. • Identify mentions of other discussants • Identify polarized words • Identify the contextual polarity of each word Subgroups Discussant Attitude Profiles (DAPs) Opinion-Target Pairing • Dependency Rules Clustering Figure 1: A block diagram illust"
P12-3023,P11-4022,0,0.0128137,"in et al. (2009) proposed a sparse coding-based model that simultaneously models the semantics and the structure of threaded discussions. Shen et al. (2006) proposed a method for exploiting the temporal and lexical similarity information in discussion streams to identify the reply structure of the dialog. Many systems addressed the problem of extracting social networks from discussions (Elson et al., 2010; McCallum et al., 2007). Other related sentiment analysis systems include MemeTube (Li et al., 2011), a sentiment-based system for analyzing and displaying microblog messages; and C-Feel-It (Joshi et al., 2011), a sentiment analyzer for micro-blogs. In the rest of this paper, we describe the system architecture, implementation, usage, and its evaluation. 2 System Overview Figure 1 shows a block diagram of the system components and the processing pipeline. The first component is the thread parsing component which takes as input a discussion thread and parses it to identify posts, participants, and the reply structure of the thread. The second component in the pipeline processes the text of posts to identify polarized words and tag them with their polarity. The list of polarity words that we use in th"
P12-3023,P11-4006,0,0.0565424,"Missing"
P12-3023,H05-2018,0,0.307845,". The functionality of this system is based on our previous work on attitude mining and subgroup detection in online discussions. This work is related to previous work in the areas of sentiment analysis and online discussion mining. Many previous systems studied the problem of iden133 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 133–138, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics tifying the polarity of individual words (Hatzivassiloglou and McKeown, 1997; Turney and Littman, 2003). Opinionfinder (Wilson et al., 2005) is a system for mining opinions from text. SENTIWORDNET (Esuli and Sebastiani, 2006) is a lexical resource in which each WordNet synset is associated to three numerical scores Obj(s), Pos(s) and Neg(s), describing how objective, positive, and negative the terms contained in the synset are. Dr Sentiment (Das and Bandyopadhyay, 2011) is an online interactive gaming technology used to crowd source human knowledge to build an extension of SentiWordNet. Another research line focused on analyzing online discussions. For example, Lin et al. (2009) proposed a sparse coding-based model that simultaneo"
P13-2045,P11-1061,0,0.0235379,"such as summarization. The Pyramid Evaluation method (Nenkova and Passonneau, 2004) for automatic summary evaluation depends on finding and annotating factoids in input sentences. Qazvinian and Radev (2011) also studied the properties of factoids present in collective human datasets and used it to create a summarization system. Hennig et al. (2010) describe an approach for automatically learning factoids for pyramid evaluation using a topic modeling approach. Our random-walk annotation technique is similar to the one used in (Hassan and Radev, 2010) to identify the semantic polarity of words. Das and Petrov (2011) also introduced a graph-based method for part-of-speech tagging in which edge weights are based on feature vectors similarity, which is like the corpus-based lexical similarity graph that we construct. 3 Table 1: Captions for contest #331. Finalists are listed in italics. this research project, we have acquired five cartoons along with all of the captions submitted in the corresponding contest. While the task of automatically identifying the funny captions would be quite useful, it is well beyond the current state of the art in NLP. A much more manageable task, and one that is quite important"
P13-2045,P10-1041,1,0.804354,"oids present in text collections is important for several NLP tasks such as summarization. The Pyramid Evaluation method (Nenkova and Passonneau, 2004) for automatic summary evaluation depends on finding and annotating factoids in input sentences. Qazvinian and Radev (2011) also studied the properties of factoids present in collective human datasets and used it to create a summarization system. Hennig et al. (2010) describe an approach for automatically learning factoids for pyramid evaluation using a topic modeling approach. Our random-walk annotation technique is similar to the one used in (Hassan and Radev, 2010) to identify the semantic polarity of words. Das and Petrov (2011) also introduced a graph-based method for part-of-speech tagging in which edge weights are based on feature vectors similarity, which is like the corpus-based lexical similarity graph that we construct. 3 Table 1: Captions for contest #331. Finalists are listed in italics. this research project, we have acquired five cartoons along with all of the captions submitted in the corresponding contest. While the task of automatically identifying the funny captions would be quite useful, it is well beyond the current state of the art in"
P13-2045,C10-2045,0,0.0290202,"Missing"
P13-2045,N04-1019,0,0.0388422,"they meant your mother’s house. They may be disappointed when they learn that “our leader” is your mother. You’d better call your mother and tell her to set a few extra place settings. If they ask for our leader, is it Obama or your mother? Which finger do I use for aliens? I guess the middle finger means the same thing to them. I sense somehow that flipping the bird was lost on them. What’s the Klingon gesture for “Go around us, jerk?” Related Work The distribution of factoids present in text collections is important for several NLP tasks such as summarization. The Pyramid Evaluation method (Nenkova and Passonneau, 2004) for automatic summary evaluation depends on finding and annotating factoids in input sentences. Qazvinian and Radev (2011) also studied the properties of factoids present in collective human datasets and used it to create a summarization system. Hennig et al. (2010) describe an approach for automatically learning factoids for pyramid evaluation using a topic modeling approach. Our random-walk annotation technique is similar to the one used in (Hassan and Radev, 2010) to identify the semantic polarity of words. Das and Petrov (2011) also introduced a graph-based method for part-of-speech taggi"
P13-2045,C08-1087,1,0.806732,"e LDA needs to know the number of topics a priori, we set the number of topics to be equal to the true number of factoids. We also use the average number of unique factoids in the held-out portion as the number of LDA topics. 5 Rec. 0.070 0.347 0.348 0.669 Table 4: Performance of various methods annotating factoids for cartoon captions. Clustering A simple baseline that can act as a surrogate for factoid annotation is clustering of discourse units, which is equivalent to assigning exactly one factoid (the name of its cluster) to each discourse unit. As our clustering method, we use C-Lexrank (Qazvinian and Radev, 2008), a method that has been well-tested on collective discourse. 4.3 Prec. 0.318 0.131 0.115 0.093 cartoon captions dataset. In some sense, the two datasets in this paper both represent difficult domains, ones in which authors are intentionally obscure. The good results acheived on the crossword clues dataset indicate that this obscurity can be overcome when discourse units are short. Future work in this vein includes applying these methods to domains, such as newswire, that are more typical for summarization, and if necessary, investigating how these methods can best be applied to domains with l"
P13-2045,P11-1110,1,0.85504,"Missing"
P13-2045,N03-1033,0,0.0112107,"ing feature vectors for each lemma and using the cosine similarity between these feature vectors as a lexical similarity function. We construct a word graph with edge weights proportional to the learned similarity of the respective word pairs. We use three types of features in these feature vectors: context word features, context part-ofspeech features, and spelling features. Context features are the presence of each word in a window of five words (two words on each side plus the word in question). Context part-of-speech features are the part-of-speech labels given by the Stanford POS tagger (Toutanova et al., 2003) within the same window. Spelling features are the counts of all character trigrams present in the word. Table 3 shows examples of similar word pairs from the set of crossword clues for “tea”. From Clues in crossword puzzles are typically obscure, requiring the reader to recognize double meanings or puns, which leads to a great deal of diversity. These clues can also refer to one or more of many different senses of the word. Table 2 shows examples of many different clues for the word “tea”. This table clearly illustrates the difference between factoids (the senses being referred to) and nugget"
P13-2045,W03-0508,0,0.106481,"Missing"
P13-2102,C10-2049,0,0.119611,"for generating and evaluating such surveys of scientific topics automatically starting from a phrase representing a topic area. We evaluate our system on a set of topics in the field of Natural Language Processing. In earlier work, (Teufel and Moens, 2002) have examined the problem of summarizing scientific articles using rhetorical analysis of sentences. Nanba and Okumura (1999) have also discussed the problem of generating surveys of multiple papers. Mohammad et al. (2009) presented experiments on generating surveys of scientific topics starting from papers to be summarized. More recently, Hoang and Kan (2010) have presented initial results on automatically generating related work section for a target paper by taking a hierarchical topic tree as an input. In this paper, we tackle the more challenging problem of summarizing a topic starting from a topic query. Our system takes as an input a string describing the topic area, selects the relevant papers from a corpus of papers, and then selects sentences from the citing sentences to these papers to generate a survey of the topic. A sample output of our system for the topic of “Word Sense Disambiguation” is shown in Figure 1. In this paper, we investig"
P13-2102,N09-1066,1,0.895741,"bstracts are available for individual papers, but no such information is available for scientific topics. In this paper, we explore strategies for generating and evaluating such surveys of scientific topics automatically starting from a phrase representing a topic area. We evaluate our system on a set of topics in the field of Natural Language Processing. In earlier work, (Teufel and Moens, 2002) have examined the problem of summarizing scientific articles using rhetorical analysis of sentences. Nanba and Okumura (1999) have also discussed the problem of generating surveys of multiple papers. Mohammad et al. (2009) presented experiments on generating surveys of scientific topics starting from papers to be summarized. More recently, Hoang and Kan (2010) have presented initial results on automatically generating related work section for a target paper by taking a hierarchical topic tree as an input. In this paper, we tackle the more challenging problem of summarizing a topic starting from a topic query. Our system takes as an input a string describing the topic area, selects the relevant papers from a corpus of papers, and then selects sentences from the citing sentences to these papers to generate a surv"
P13-2102,N04-1019,0,0.317018,"Missing"
P13-2102,C08-1087,1,0.908957,"Missing"
P13-2102,radev-etal-2004-mead,1,0.817996,"Missing"
P13-2102,J02-4002,0,0.160793,"edu Amjad Abu-Jbara Department of EECS University of Michigan Ann Arbor, MI, USA amjbara@umich.edu Abstract Thus, a system that can generate such surveys automatically would be a useful tool. Short summaries in the form of abstracts are available for individual papers, but no such information is available for scientific topics. In this paper, we explore strategies for generating and evaluating such surveys of scientific topics automatically starting from a phrase representing a topic area. We evaluate our system on a set of topics in the field of Natural Language Processing. In earlier work, (Teufel and Moens, 2002) have examined the problem of summarizing scientific articles using rhetorical analysis of sentences. Nanba and Okumura (1999) have also discussed the problem of generating surveys of multiple papers. Mohammad et al. (2009) presented experiments on generating surveys of scientific topics starting from papers to be summarized. More recently, Hoang and Kan (2010) have presented initial results on automatically generating related work section for a target paper by taking a hierarchical topic tree as an input. In this paper, we tackle the more challenging problem of summarizing a topic starting fr"
P13-2144,W12-3705,1,0.423233,"Missing"
P13-2144,P12-1042,1,0.79734,"They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. In previous work, we proposed a method that uses participantto-participant and participant-to-topic attitudes to identify subgroups in ideological discussions using attitude vector space clustering (Abu-Jbara and Radev, 2012). In this paper, we extend this method by adding latent similarity features to the attitude vectors and applying it to Arabic discussions. In another previous work, our group proposed a supervised method for extracting signed social networks from text (Hassan et al., 2012a). The signed networks constructed using this method were based only on participant-to-participant attitudes that are expressed explicitly in discussions. We used this method to extract signed networks from discussions and used a partitioning algorithm to detect opinion subgroups (Hassan et al., 2012b). In this paper, we exte"
P13-2144,P12-1091,1,0.0945698,"HTML file to identify the posts, the discussants, and the thread structure. We transform the Arabic content of the posts and the discussant names that are written in Arabic to the Buckwalter encoding (Buckwalter, 2004). We use AMIRAN (Diab, 2009), a system for processing Arabic text, to tokenize the text and identify noun phrases. 3.2 Identifying Opinion Targets 3.4 Latent Textual Similarity If two participants share the same opinion, they tend to focus on similar aspects of the discussion topic and emphasize similar points that support their opinion. To capture this, we follow previous work (Guo and Diab, 2012; Dasigi et al., 2012) and apply Latent Dirichelet Allocation (LDA) topic models to the text written by the different participants. We use an LDA model with 100 topics. So, we represent all the text written in the discussion by each participant as a vector of 100 dimensions. The vector of each participant contains the topic distribution of the participant, as produced by the LDA model. Identifying Opinionated Text To identify opinion-bearing text, we start from the word level. We identify the polarized words that appear in text by looking each word up in a lexicon of Arabic polarized words. In"
P13-2144,W11-1701,0,0.0227731,"gned network (the second representation). We evaluate this system using a data set of Arabic discussions collected from an Arabic debating site. We experiment with several variations of the system. The results show that the clustering the vector space representation achieves better results than partitioning the signed network representation. 2 Somasundaran and Wiebe (2009) present an unsupervised opinion analysis method for debateside classification. They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. In previous work, we proposed a method that uses participantto-participant and participant-to-topic attitudes to identify subgroups in ideological discussions using attitude vector space clustering (Abu-Jbara and Radev, 2012). In this paper, we extend this method by adding latent similarity features to the attitude vectors and applying it to Arabic discussions."
P13-2144,P10-1041,1,0.806507,"siloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One example of such applications is identifying perspectives (Grefenstette et al., 2004; Lin et al., 2006) which is most similar to the topic of this paper. For example, in (Lin et al., 2006), the authors experiment with several supervised and statistical models to capture how perspectives are expressed at the document and the sentence levels. Unfortun"
P13-2144,D10-1121,1,0.0599058,"Pang & Lee (2008) and Liu & Zhang (2012) wrote two recent comprehensive surveys about sentiment analysis and opinion mining techniques and applications. Previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One examp"
P13-2144,banea-etal-2008-bootstrapping,0,0.0115159,"n et al., 2012b). In this paper, we extend this method by using participant-to-topic attitudes to construct the signed network. Previous Work Our work is related to a large body of research on opinion mining and sentiment analysis. Pang & Lee (2008) and Liu & Zhang (2012) wrote two recent comprehensive surveys about sentiment analysis and opinion mining techniques and applications. Previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research effor"
P13-2144,P11-2104,1,0.701137,"s. One example of such applications is identifying perspectives (Grefenstette et al., 2004; Lin et al., 2006) which is most similar to the topic of this paper. For example, in (Lin et al., 2006), the authors experiment with several supervised and statistical models to capture how perspectives are expressed at the document and the sentence levels. Unfortunately, not much work has been done on Arabic sentiment analysis and opinion mining. Abbasi et al. (2008) applies sentiment analysis techniques to identify and classify documentlevel opinions in text crawled from English and Arabic web forums. Hassan et al. (2011) proposed a method for identifying the polarity of nonEnglish words using multilingual semantic graphs. They applied their method to Arabic and Hindi. Abdul-Mageed and Diab (2011) annotated a corpus of Modern Standard Arabic (MSA) news text for subjectivity at the sentence level. In a later work (2012a), they expanded their corpus by la830 beling data from more genres using Amazon Mechanical Turk. Abdul-Mageed et al. (2012a) developed SAMAR, a system for subjectivity and Sentiment Analysis for Arabic social media genres. We use this system as a component in our approach. 3 accuracy of SAMAR on"
P13-2144,W04-1606,0,0.012904,"s of five components. The input to the pipeline is a discussion thread in Arabic language crawled from a discussion forum. The output is the list of participants in the discussion and the subgroup membership of each discussant. We describe the components of the pipeline in the following subsections. 3.1 Preprocessing The input to this component is a discussion thread in HTML format. We parse the HTML file to identify the posts, the discussants, and the thread structure. We transform the Arabic content of the posts and the discussant names that are written in Arabic to the Buckwalter encoding (Buckwalter, 2004). We use AMIRAN (Diab, 2009), a system for processing Arabic text, to tokenize the text and identify noun phrases. 3.2 Identifying Opinion Targets 3.4 Latent Textual Similarity If two participants share the same opinion, they tend to focus on similar aspects of the discussion topic and emphasize similar points that support their opinion. To capture this, we follow previous work (Guo and Diab, 2012; Dasigi et al., 2012) and apply Latent Dirichelet Allocation (LDA) topic models to the text written by the different participants. We use an LDA model with 100 topics. So, we represent all the text w"
P13-2144,W12-4102,1,0.820433,"h as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. In previous work, we proposed a method that uses participantto-participant and participant-to-topic attitudes to identify subgroups in ideological discussions using attitude vector space clustering (Abu-Jbara and Radev, 2012). In this paper, we extend this method by adding latent similarity features to the attitude vectors and applying it to Arabic discussions. In another previous work, our group proposed a supervised method for extracting signed social networks from text (Hassan et al., 2012a). The signed networks constructed using this method were based only on participant-to-participant attitudes that are expressed explicitly in discussions. We used this method to extract signed networks from discussions and used a partitioning algorithm to detect opinion subgroups (Hassan et al., 2012b). In this paper, we extend this method by using participant-to-topic attitudes to construct the signed network. Previous Work Our work is related to a large body of research on opinion mining and sentiment analysis. Pang & Lee (2008) and Liu & Zhang (2012) wrote two recent comprehensive surveys"
P13-2144,W03-1014,0,0.0200975,"this paper, we extend this method by using participant-to-topic attitudes to construct the signed network. Previous Work Our work is related to a large body of research on opinion mining and sentiment analysis. Pang & Lee (2008) and Liu & Zhang (2012) wrote two recent comprehensive surveys about sentiment analysis and opinion mining techniques and applications. Previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying"
P13-2144,W06-1652,0,0.0303638,"Liu & Zhang (2012) wrote two recent comprehensive surveys about sentiment analysis and opinion mining techniques and applications. Previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One example of such application"
P13-2144,C00-1044,0,0.411253,"to detect opinion subgroups (Hassan et al., 2012b). In this paper, we extend this method by using participant-to-topic attitudes to construct the signed network. Previous Work Our work is related to a large body of research on opinion mining and sentiment analysis. Pang & Lee (2008) and Liu & Zhang (2012) wrote two recent comprehensive surveys about sentiment analysis and opinion mining techniques and applications. Previous work has proposed methods for identifying subjective text that expresses opinion and distinguishing it from objective text that presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010)."
P13-2144,P09-1026,0,0.499168,"used their method to estimate the policy positions of political parties in Britain and Ireland, on both economic and social policy dimensions. tive. To identify opinion subgroups, we cluster the vector space (the first representation) or partition the signed network (the second representation). We evaluate this system using a data set of Arabic discussions collected from an Arabic debating site. We experiment with several variations of the system. The results show that the clustering the vector space representation achieves better results than partitioning the signed network representation. 2 Somasundaran and Wiebe (2009) present an unsupervised opinion analysis method for debateside classification. They mine the web to learn associations that are indicative of opinion stances in debates and combine this knowledge with discourse information. Anand et al. (2011) present a supervised method for stance classification. They use a number of linguistic and structural features such as unigrams, bigrams, cue words, repeated punctuation, and opinion dependencies to build a stance classification model. In previous work, we proposed a method that uses participantto-participant and participant-to-topic attitudes to identi"
P13-2144,P05-1017,0,0.0115434,"(Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One example of such applications is identifying perspectives (Grefenstette et al., 2004; Lin et al., 2006) which is most similar to the topic of this paper. For example, in (Lin et al., 2006), the authors experiment with several supervised and statistical models to capture how perspectives are expressed at the document and the"
P13-2144,W06-1642,0,0.0162619,"presents factual information (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000a; Banea et al., 2008; Riloff and Wiebe, 2003). Subjective text may express positive, negative, or neutral opinion. Previous work addressed the problem of identifying the polarity of subjective text (Hatzivassiloglou and Wiebe, 2000b; Hassan et al., 2010; Riloff et al., 2006). Many of the proposed methods for text polarity identification depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One example of such applications is identifying perspectives (Grefenstette et al., 2004; Lin et al., 2006) which is most similar to the topic of this paper. For example, in (Lin et al., 2006), the authors experiment with several supervised and statistical models to capture how perspectives are expressed a"
P13-2144,W06-2915,0,0.0277713,"ation depend on the availability of polarity lexicons (i.e. lists of positive and negative words). Several approaches have been devised for building such lexicons (Turney and Littman, 2003; Kanayama and Nasukawa, 2006; Takamura et al., 2005; Hassan and Radev, 2010). Other research efforts focused on identifying the holders and the targets of opinion (Zhai et al., 2010; Popescu and Etzioni, 2007; Bethard et al., 2004). Opinion mining and sentiment analysis techniques have been used in various applications. One example of such applications is identifying perspectives (Grefenstette et al., 2004; Lin et al., 2006) which is most similar to the topic of this paper. For example, in (Lin et al., 2006), the authors experiment with several supervised and statistical models to capture how perspectives are expressed at the document and the sentence levels. Unfortunately, not much work has been done on Arabic sentiment analysis and opinion mining. Abbasi et al. (2008) applies sentiment analysis techniques to identify and classify documentlevel opinions in text crawled from English and Arabic web forums. Hassan et al. (2011) proposed a method for identifying the polarity of nonEnglish words using multilingual se"
P13-2144,H05-1044,0,0.0140321,"written in the discussion by each participant as a vector of 100 dimensions. The vector of each participant contains the topic distribution of the participant, as produced by the LDA model. Identifying Opinionated Text To identify opinion-bearing text, we start from the word level. We identify the polarized words that appear in text by looking each word up in a lexicon of Arabic polarized words. In our experiments, we use Sifat (Abdul-Mageed and Diab, 2012b), a lexicon of 3982 Arabic adjectives labeled as positive, negative, or neutral. The polarity of a word may be dependant on its context (Wilson et al., 2005). For example, a positive word that appears in a negated context should be treated as expressing negative opinion rather than positive. To identify the polarity of a word given the sentence it appears in, we use SAMAR (Abdul-Mageed et al., 2012b), a system for Subjectivity and Sentiment Analysis for Arabic social media genres. SAMAR labels a sentence that contains an opinion expression as positive, negative, or neutral taking into account the context of the opinion expression. The reported 3.5 Subgroup Detection At this point, we have for every discussant the targets towards which he/she expre"
P13-2144,C10-1143,0,0.00641845,"Missing"
P13-2144,W11-0413,1,\N,Missing
P13-2144,H05-2017,0,\N,Missing
P13-2144,H05-1043,0,\N,Missing
P13-2144,P12-2013,1,\N,Missing
P13-2144,abdul-mageed-diab-2012-awatif,1,\N,Missing
P15-1043,J07-1005,0,0.0263149,"Missing"
P15-1043,N04-1015,0,0.0569695,"Missing"
P15-1043,P06-1039,0,0.0509128,"Missing"
P15-1043,N09-1041,0,0.0861006,"Missing"
P15-1043,C08-1087,1,0.902209,"f automatically building informative surveys for scientific topics. Given the rapid growth of publications in scientific fields, the development of such systems is crucial as human-written surveys exist for a limited number of topics and get outdated quickly. In this paper, we investigate content models for extracting survey-worthy information from scientific papers. Such models are an essential component of any system for automatic survey article generation. Earlier work in the area of survey article generation has investigated content models based on lexical networks (Mohammad et al., 2009; Qazvinian and Radev, 2008). These models take as input citing sentences that describe important papers on the topic and assign them a salience score based on centrality in a lexical network formed by the input citing sentences. In this 441 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 441–450, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Topic dependency parsing named entity recognition question answering semantic role labeling sentiment analysis summarization"
P15-1043,kan-etal-2002-using,0,0.136377,"Missing"
P15-1043,J02-4002,0,0.100501,"Missing"
P15-1043,P08-1093,0,0.0578981,"Missing"
P15-1043,N09-1066,1,0.74697,"eneration is the task of automatically building informative surveys for scientific topics. Given the rapid growth of publications in scientific fields, the development of such systems is crucial as human-written surveys exist for a limited number of topics and get outdated quickly. In this paper, we investigate content models for extracting survey-worthy information from scientific papers. Such models are an essential component of any system for automatic survey article generation. Earlier work in the area of survey article generation has investigated content models based on lexical networks (Mohammad et al., 2009; Qazvinian and Radev, 2008). These models take as input citing sentences that describe important papers on the topic and assign them a salience score based on centrality in a lexical network formed by the input citing sentences. In this 441 Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing, pages 441–450, c Beijing, China, July 26-31, 2015. 2015 Association for Computational Linguistics Topic dependency parsing named entity recognition question answering semantic role labeling senti"
P15-1043,N04-1019,0,0.0749589,"on using Numpy and then optimized it using Scipy Weave. This code is available for use at https://github.com/rahuljha/ content-models. The repository also contains Python code for H IT S UM. 4 Experiments For evaluating our content models, we generated 2,000-character-long summaries using each of the systems (Lexrank, C-Lexrank, H IT S UM, and T OPIC S UM) for each of the topics. The summaries are generated by ranking the input sentences using each content model and picking the top sentences till the budget of 2,000 characters is reached. Each of these summaries is then given a pyramid score (Nenkova and Passonneau, 2004) computed using the factoids assigned to each sentence. For the pyramid evaluation, the factoids are organized in a pyramid of order n. The top tier in this pyramid contains the highest weighted factoids, the next tier contains the second highest weighted factoids, and so on. The score assigned to a summary is the ratio of the sum of the weights of the factoids it contains to the sum of weights of an optimal summary with the same number of factoids. Pyramid evaluation allows us to capture how each content model performs in terms of selecting sentences with the most highly weighted factoids. Si"
P15-1043,C10-2049,0,\N,Missing
P15-1043,W04-3252,0,\N,Missing
P15-1043,P13-2102,1,\N,Missing
P16-1062,P13-2045,1,0.926109,"arget” or “Sky light.” In contrast, people writing a descriptive caption for a photograph can adopt a less creative style. Corpora may also differ on how similar texts within a particular 654 Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics, pages 654–665, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics metric for short text clustering without varying the clustering method. Yan et al. (2012) proposed an alternative term weighting scheme to use in place of tf-idf when clustering using non-negative matrix factorization. King et al. (2013) used the cosine similarity between feature vectors that included context word and part-of-speech features and spelling features and applied Louvain clustering to the resulting graph. Xu et al. (2015) used a convolutional neural network to represent short texts and found that, when used with the k-means clustering algorithm, this deep semantic representation outperformed tf-idf, Laplacian eigenmaps, and average embeddings for clustering. Other papers focused on choosing the best clustering method for short texts, but kept the similarity metric constant. Rangrej et al. (2011) compared k-means,"
P16-1062,D15-1177,0,0.0149647,"a wide variety of ways, leading to data that can benefit from different similarity metrics than less creative texts. At the same time, we hypothesize that tightly clustered datasets—datasets where each text is much more similar to texts in its cluster than to texts from other clusters—can be clustered by powerful graph-based methods such as Markov Clustering (MCL) and Louvain, which may fail on more loosely clustered data. This paper explores the interaction of these effects. Recently, distributional semantics has been popular and successful for measuring text similarity (Socher et al., 2011; Cheng and Kartsaklis, 2015; He et al., 2015; Kenter and de Rijke, 2015; Kusner et al., 2015; Ma et al., 2015; Tai et al., 2015; Wang et al., 2015). Word embeddings represent similar words in similar locations in vector space: “cat” is closer to “feline” than to “bird.” It would be natural to expect such semantics-based approaches to be useful for clustering, particularly for corpora where authors have tried to express similar ideas in unique ways. And indeed, this paper will show that, depending on the choice of clustering method, semantics-based similarity Properties of corpora, such as the diversity of vocabulary and"
P16-1062,W05-0904,0,0.0523056,"ought vectors. In each case, we represent texts as vectors and find their cosine similarities; if cosine similarity can be negative, we add one and normalize by two to ensure similarity in the range [0, 1]. N -Gram Counts. First we consider n-gram count vectors. We use three variations: (1) unigrams, (2) unigrams and bigrams, and (3) unigrams, bigrams, and trigrams. N -Gram tf-idf. We also consider weighting n-grams by tf-idf, as calculated by sklearn (Pedregosa et al., 2011). Dependency Counts. Grammatical information has been found to be useful in text, particularly short text, similarity. (Liu and Gildea, 2005; Zhang et al., 2005; Wang et al., 2009b; Heilman ˇ c et al., and Smith, 2010; Tian et al., 2010; Sari´ 2012; Tai et al., 2015). To leverage this information, previous work has used dependency kernels (Tian et al., 2010), which measure similarity by the fraction of identical dependency parse segments between two sentences. Here, we accomplish the same effect using a count vector for each sentence, with the dependency parse segments as the vocabulary. We define the set of segments for a dependency parse to consist of, for each word, the word, its parent, and the dependency relation that connect"
P16-1062,W15-1505,0,0.0275372,"an less creative texts. At the same time, we hypothesize that tightly clustered datasets—datasets where each text is much more similar to texts in its cluster than to texts from other clusters—can be clustered by powerful graph-based methods such as Markov Clustering (MCL) and Louvain, which may fail on more loosely clustered data. This paper explores the interaction of these effects. Recently, distributional semantics has been popular and successful for measuring text similarity (Socher et al., 2011; Cheng and Kartsaklis, 2015; He et al., 2015; Kenter and de Rijke, 2015; Kusner et al., 2015; Ma et al., 2015; Tai et al., 2015; Wang et al., 2015). Word embeddings represent similar words in similar locations in vector space: “cat” is closer to “feline” than to “bird.” It would be natural to expect such semantics-based approaches to be useful for clustering, particularly for corpora where authors have tried to express similar ideas in unique ways. And indeed, this paper will show that, depending on the choice of clustering method, semantics-based similarity Properties of corpora, such as the diversity of vocabulary and how tightly related texts cluster together, impact the best way to cluster short"
P16-1062,D15-1181,0,0.0149005,"ing to data that can benefit from different similarity metrics than less creative texts. At the same time, we hypothesize that tightly clustered datasets—datasets where each text is much more similar to texts in its cluster than to texts from other clusters—can be clustered by powerful graph-based methods such as Markov Clustering (MCL) and Louvain, which may fail on more loosely clustered data. This paper explores the interaction of these effects. Recently, distributional semantics has been popular and successful for measuring text similarity (Socher et al., 2011; Cheng and Kartsaklis, 2015; He et al., 2015; Kenter and de Rijke, 2015; Kusner et al., 2015; Ma et al., 2015; Tai et al., 2015; Wang et al., 2015). Word embeddings represent similar words in similar locations in vector space: “cat” is closer to “feline” than to “bird.” It would be natural to expect such semantics-based approaches to be useful for clustering, particularly for corpora where authors have tried to express similar ideas in unique ways. And indeed, this paper will show that, depending on the choice of clustering method, semantics-based similarity Properties of corpora, such as the diversity of vocabulary and how tightly rela"
P16-1062,N10-1145,0,0.0603791,"Missing"
P16-1062,P82-1020,0,0.494212,"Missing"
P16-1062,P15-1150,0,0.0740451,"Missing"
P16-1062,P11-1110,1,0.883281,"Missing"
P16-1062,W10-0721,0,0.0173689,"ting texts in response to the same stimulus. In a corpus of texts relating to several stimuli, it may be desirable to cluster according to which stimulus each text relates to—for instance, grouping all of the news headlines about the same event together. Here, we consider texts triggered by several types of stimuli: photographs that need descriptive captions, cartoons that need humorous captions, and crossword answers that need original clues. Each need shapes the properties of the texts. Pascal and Flickr Captions. The Pascal Captions dataset (hereinafter PAS) and the 8K ImageFlickr dataset (Rashtchian et al., 2010) are sets of captions solicited from Mechanical Turkers for photographs from Flickr and from the PatRelated Work The most similar work to the present paper is Shrestha et al. (2012), which acknowledged that the similarity metric and the clustering method could both contribute to clustering results. It compared four similarity methods and also tested four clustering methods. Unlike the present work, it did not consider distributional semantics-based similarity measures or similarity measures that incorporated deep learning. In addition, it reported that the characteristics of the corpora “overs"
P16-1062,P15-2058,0,0.0308967,"time, we hypothesize that tightly clustered datasets—datasets where each text is much more similar to texts in its cluster than to texts from other clusters—can be clustered by powerful graph-based methods such as Markov Clustering (MCL) and Louvain, which may fail on more loosely clustered data. This paper explores the interaction of these effects. Recently, distributional semantics has been popular and successful for measuring text similarity (Socher et al., 2011; Cheng and Kartsaklis, 2015; He et al., 2015; Kenter and de Rijke, 2015; Kusner et al., 2015; Ma et al., 2015; Tai et al., 2015; Wang et al., 2015). Word embeddings represent similar words in similar locations in vector space: “cat” is closer to “feline” than to “bird.” It would be natural to expect such semantics-based approaches to be useful for clustering, particularly for corpora where authors have tried to express similar ideas in unique ways. And indeed, this paper will show that, depending on the choice of clustering method, semantics-based similarity Properties of corpora, such as the diversity of vocabulary and how tightly related texts cluster together, impact the best way to cluster short texts. We examine several such propert"
P16-1062,S12-1060,0,0.0737265,"Missing"
P16-1062,W15-1509,0,0.032151,"s of the 54th Annual Meeting of the Association for Computational Linguistics, pages 654–665, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics metric for short text clustering without varying the clustering method. Yan et al. (2012) proposed an alternative term weighting scheme to use in place of tf-idf when clustering using non-negative matrix factorization. King et al. (2013) used the cosine similarity between feature vectors that included context word and part-of-speech features and spelling features and applied Louvain clustering to the resulting graph. Xu et al. (2015) used a convolutional neural network to represent short texts and found that, when used with the k-means clustering algorithm, this deep semantic representation outperformed tf-idf, Laplacian eigenmaps, and average embeddings for clustering. Other papers focused on choosing the best clustering method for short texts, but kept the similarity metric constant. Rangrej et al. (2011) compared k-means, singular value decomposition, and affinity propagation for tweets, finding affinity propagation the most effective, using tf-idf with cosine similarity or Jaccard for a similarity measure. Errecalde e"
P16-1062,I05-1034,0,0.0378551,"case, we represent texts as vectors and find their cosine similarities; if cosine similarity can be negative, we add one and normalize by two to ensure similarity in the range [0, 1]. N -Gram Counts. First we consider n-gram count vectors. We use three variations: (1) unigrams, (2) unigrams and bigrams, and (3) unigrams, bigrams, and trigrams. N -Gram tf-idf. We also consider weighting n-grams by tf-idf, as calculated by sklearn (Pedregosa et al., 2011). Dependency Counts. Grammatical information has been found to be useful in text, particularly short text, similarity. (Liu and Gildea, 2005; Zhang et al., 2005; Wang et al., 2009b; Heilman ˇ c et al., and Smith, 2010; Tian et al., 2010; Sari´ 2012; Tai et al., 2015). To leverage this information, previous work has used dependency kernels (Tian et al., 2010), which measure similarity by the fraction of identical dependency parse segments between two sentences. Here, we accomplish the same effect using a count vector for each sentence, with the dependency parse segments as the vocabulary. We define the set of segments for a dependency parse to consist of, for each word, the word, its parent, and the dependency relation that connects them as shown in E"
P16-1062,W10-0707,0,\N,Missing
P16-1062,I13-1085,0,\N,Missing
P18-1033,W14-2402,0,0.0135844,"methodology apply broadly to the systems cited below. Within the DB community, systems commonly use pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from"
P18-1033,P14-1133,0,0.0222958,"dly to the systems cited below. Within the DB community, systems commonly use pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from logical parsing research"
P18-1033,D17-1151,0,0.0125685,"data. We call this a questionbased data split. However, many English questions may correspond to the same SQL query. If at least one copy of every SQL query appears in training, then the task evaluated is classification, not true semantic parsing, of the English questions. We can increase the number of distinct SQL queries by varying 5.1 Systems Recently, a great deal of work has used variations on the seq2seq model. We compare performance of a basic seq2seq model (Sutskever et al., 2014), and seq2seq with attention over the input (Bahdanau et al., 2015), implemented with TensorFlow seq2seq (Britz et al., 2017). We also extend that model to include an attention-based copying option, similar to Jia and Liang (2016). Our output vocabulary for the decoder includes a special token, COPY. If COPY has the highest probability at step t, we replace it with the input token with the 355 O O city0 O city1 Flight from Denver to Boston bidirectional LSTM provides a prediction for each word, either O if the word is not used in the final query, or a symbol such as city1 to indicate that it fills a slot. The hidden states of the LSTM at each end of the sentence are passed through a small feed-forward network to det"
P18-1033,P16-1004,0,0.431621,"aluation methodology for this task. In the process, we (1) introduce a new, challenging dataset, (2) standardize and fix many errors in existing datasets, and (3) propose a simple yet effective baseline system.1 ∗ The first two authors contributed equally to this work. Code and data is available at https://github. com/jkkummerfeld/text2sql-data/ 1 351 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Long Papers), pages 351–360 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics sumptions about the output structure (Dong and Lapata, 2016). One challenge for applying neural models to this task is annotating large enough datasets of question-query pairs. Recent work (Cai et al., 2017; Zhong et al., 2017) has automatically generated large datasets using templates to form random queries and corresponding natural-languagelike questions, and then having humans rephrase the question into English. Another option is to use feedback-based learning, where the system alternates between training and making predictions, which a user rates as correct or not (Iyer et al., 2017). Other work seeks to avoid the data bottleneck by using end-to-en"
P18-1033,C12-2040,0,0.151263,"Missing"
P18-1033,P17-1089,0,0.251203,"mputational Linguistics sumptions about the output structure (Dong and Lapata, 2016). One challenge for applying neural models to this task is annotating large enough datasets of question-query pairs. Recent work (Cai et al., 2017; Zhong et al., 2017) has automatically generated large datasets using templates to form random queries and corresponding natural-languagelike questions, and then having humans rephrase the question into English. Another option is to use feedback-based learning, where the system alternates between training and making predictions, which a user rates as correct or not (Iyer et al., 2017). Other work seeks to avoid the data bottleneck by using end-to-end approaches (Yin et al., 2016; Neelakantan et al., 2017), which we do not consider here. One key contribution of this paper is standardization of a range of datasets, to help address the challenge of limited data resources. challenging form of ambiguity from the task. In the process, we apply extensive effort to standardize datasets and fix a range of errors. Previous NLIDB work has led to impressive systems, but current evaluations provide an incomplete picture of their strengths and weaknesses. In this paper, we provide new a"
P18-1033,P16-1002,0,0.420886,"me SQL query. If at least one copy of every SQL query appears in training, then the task evaluated is classification, not true semantic parsing, of the English questions. We can increase the number of distinct SQL queries by varying 5.1 Systems Recently, a great deal of work has used variations on the seq2seq model. We compare performance of a basic seq2seq model (Sutskever et al., 2014), and seq2seq with attention over the input (Bahdanau et al., 2015), implemented with TensorFlow seq2seq (Britz et al., 2017). We also extend that model to include an attention-based copying option, similar to Jia and Liang (2016). Our output vocabulary for the decoder includes a special token, COPY. If COPY has the highest probability at step t, we replace it with the input token with the 355 O O city0 O city1 Flight from Denver to Boston bidirectional LSTM provides a prediction for each word, either O if the word is not used in the final query, or a symbol such as city1 to indicate that it fills a slot. The hidden states of the LSTM at each end of the sentence are passed through a small feed-forward network to determine the SQL template to use. This architecture is simple and enables a joint choice of the tags and th"
P18-1033,P17-2017,1,0.806873,"hers were written by CS students with knowledge of the database who were instructed to write questions they might ask in an academic advising appointment. The authors manually labeled the initial set of questions with SQL. To ensure high quality, at least two annotators scored each questionquery pair on a two-point scale for accuracy— did the query generate an accurate answer to the question?—and a three-point scale for helpfulness—did the answer provide the information the asker was probably seeking? Cases with low scores were fixed or removed from the dataset. We collected paraphrases using Jiang et al. (2017)’s method, with manual inspection to ensure accuracy. For a given sentence, this produced paraphrases with the same named entities (e.g. course number EECS 123). To add variation, we annotated entities in the questions and queries with their types—such as course name, department, or instructor—and substituted randomly-selected values of each type into each paraphrase and its corresponding query. This combination of paraphrasing and entity replacement means an original question of “For next semester, who is teaching EECS 123?” can give rise to “Who teaches MATH 456 next semester?” as well as “W"
P18-1033,W00-1317,0,0.498716,"text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking task, manually annotated. We use the modified SQL from Iyer et al. (2017), which follows the data split from the logical form version (Zettlemoyer and Collins, 2007). GeoQuery (Zelle and Mooney, 1996) User questions about US geography, manually annotated with Prolog. We use the SQL version (Popescu et al., 2003; Giordani and Moschitti, 2012; Iyer et al., 2017), which follows the logical form data split (Zettlemoyer and Collins, 2005). Restaurants (Tang and Mooney, 2000; Popescu et al., 2003) User questions about restaurants, their food types, and locations. Scholar (Iyer et al., 2017) User questions about academic publications, with automatically generated SQL that was checked by asking the user if the output was correct. Academic (Li and Jagadish, 2014) Questions about the Microsoft Academic Search (MAS) database, derived by enumerating every logical query that could be expressed using the search page of the MAS website and writing sentences to match them. The domain is similar to that of Scholar, but their schemas differ. 352 Yelp and IMDB (Yaghmazadeh et"
P18-1033,P11-1060,0,0.0827939,"ns about evaluation methodology apply broadly to the systems cited below. Within the DB community, systems commonly use pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and ap"
P18-1033,P15-1128,0,0.0259465,"pattern matching, grammar-based techniques, or intermediate representations of the query (Pazos Rangel et al., 2013). Recent work has explored incorporating user feedback to improve accuracy (Li and Jagadish, 2014). Unfortunately, none of these systems are publicly available, and many rely on domain-specific resources. In the NLP community, there has been extensive work on semantic parsing to logical representations that query a knowledge base (Zettlemoyer and Collins, 2005; Liang et al., 2011; Beltagy et al., 2014; Berant and Liang, 2014), while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from logical parsing research (Poon, 2013). However, none of these earlier systems are publicly avai"
P18-1033,W16-0105,0,0.0149045,"e for applying neural models to this task is annotating large enough datasets of question-query pairs. Recent work (Cai et al., 2017; Zhong et al., 2017) has automatically generated large datasets using templates to form random queries and corresponding natural-languagelike questions, and then having humans rephrase the question into English. Another option is to use feedback-based learning, where the system alternates between training and making predictions, which a user rates as correct or not (Iyer et al., 2017). Other work seeks to avoid the data bottleneck by using end-to-end approaches (Yin et al., 2016; Neelakantan et al., 2017), which we do not consider here. One key contribution of this paper is standardization of a range of datasets, to help address the challenge of limited data resources. challenging form of ambiguity from the task. In the process, we apply extensive effort to standardize datasets and fix a range of errors. Previous NLIDB work has led to impressive systems, but current evaluations provide an incomplete picture of their strengths and weaknesses. In this paper, we provide new and improved data, a new baseline, and guidelines that complement existing metrics, supporting fu"
P18-1033,D07-1071,0,0.0602937,"as the lexicon used by PRECISE. More recent work has produced general purpose systems that are competitive with previous results and are also available, such as Iyer et al. (2017). We also adapt a logical form parser with a sequence to tree approach that makes very few as3 Data For our analysis, we study a range of text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking task, manually annotated. We use the modified SQL from Iyer et al. (2017), which follows the data split from the logical form version (Zettlemoyer and Collins, 2007). GeoQuery (Zelle and Mooney, 1996) User questions about US geography, manually annotated with Prolog. We use the SQL version (Popescu et al., 2003; Giordani and Moschitti, 2012; Iyer et al., 2017), which follows the logical form data split (Zettlemoyer and Collins, 2005). Restaurants (Tang and Mooney, 2000; Popescu et al., 2003) User questions about restaurants, their food types, and locations. Scholar (Iyer et al., 2017) User questions about academic publications, with automatically generated SQL that was checked by asking the user if the output was correct. Academic (Li and Jagadish, 2014)"
P18-1033,P13-1092,0,0.0364262,"while work on mapping to SQL has recently increased (Yih et al., 2015; Iyer et al., 2017; Zhong et al., 2017). One of the earliest statistical models for mapping text to SQL was the PRECISE system (Popescu et al., 2003, 2004), which achieved high precision on queries that met constraints linking tokens and database values, attributes, and relations, but did not attempt to generate SQL for questions outside this class. Later work considered generating queries based on relations extracted by a syntactic parser (Giordani and Moschitti, 2012) and applying techniques from logical parsing research (Poon, 2013). However, none of these earlier systems are publicly available, and some required extensive engineering effort for each domain, such as the lexicon used by PRECISE. More recent work has produced general purpose systems that are competitive with previous results and are also available, such as Iyer et al. (2017). We also adapt a logical form parser with a sequence to tree approach that makes very few as3 Data For our analysis, we study a range of text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking ta"
P18-1033,C04-1021,0,0.784096,"Missing"
P18-1033,H90-1080,0,0.212278,"12) and applying techniques from logical parsing research (Poon, 2013). However, none of these earlier systems are publicly available, and some required extensive engineering effort for each domain, such as the lexicon used by PRECISE. More recent work has produced general purpose systems that are competitive with previous results and are also available, such as Iyer et al. (2017). We also adapt a logical form parser with a sequence to tree approach that makes very few as3 Data For our analysis, we study a range of text-to-SQL datasets, standardizing them to have a consistent SQL style. ATIS (Price, 1990; Dahl et al., 1994) User questions for a flight-booking task, manually annotated. We use the modified SQL from Iyer et al. (2017), which follows the data split from the logical form version (Zettlemoyer and Collins, 2007). GeoQuery (Zelle and Mooney, 1996) User questions about US geography, manually annotated with Prolog. We use the SQL version (Popescu et al., 2003; Giordani and Moschitti, 2012; Iyer et al., 2017), which follows the logical form data split (Zettlemoyer and Collins, 2005). Restaurants (Tang and Mooney, 2000; Popescu et al., 2003) User questions about restaurants, their food t"
P18-1057,P16-1082,0,0.383111,"Missing"
P18-1057,P13-2102,1,0.908376,"he educational aspect of mining text for presenting scientific topics. One goal has been to develop concept maps of topics, graphs showing which topics are prerequisites for learning a given topic (Gordon et al., 2016; Liu et al., 2016; Pan et al., 2017a,b; Liang et al., 2017). Another goal has been to automatically create reading lists for a subject either by building upon concept graphs (Gordon et al., 2017) or through an unstructured approach (Jardine, 2014). Additionally, other work has aimed to automatically summarize scientific topics, either by extractively summarizing academic papers (Jha et al., 2013, 2015; Jaidka et al., 2016) or by producing Wikipedia articles on these topics from multiple sources (Sauper and Barzilay, 2009; Liu et al., 2018). Scientific articles constitute primary texts which describe an author’s work on a particular subject, while Wikipedia articles can be viewed as tertiary sources which summarize both results from primary works as well as explanations from secondary sources. Tang and McCalla (2004, 2009) and Sheng et al. (2017) explore the pedagogical function among the types of sources. To address the problem of the scientific education of NLP more directly, we foc"
P18-1057,bird-etal-2008-acl,1,0.731779,"ng extractive and abstractive summarization methods. 3 3.1 Table 1: Top-level Taxonomy Topics Topic Category Introduction to Neural Networks and Deep Learning Tools for Deep Learning Miscellaneous Deep Learning Machine Learning Word Embeddings Recurrent Neural Networks Python Basics Reinforcement learning Convolutional Neural Networks Introduction to AI Dataset Collection Count 635 475 287 225 139 134 133 132 129 89 Table 2: Corpus count by taxonomy topic for the most frequent topics (excluding topic “Other”). An Overview of TutorialBank As opposed to other collections like the ACL Anthology (Bird et al., 2008; Radev et al., 2009, 2013, 2016), which contain solely academic papers, our corpus focuses mainly on resources other than academic papers. The main goal in our decision process of what to include in our corpus has been the quality-control of resources which can be used for an educational purpose. Initially, the resources collected were conference tutorials as well as surveys, books and longer papers on broader topics, as these genres contain an inherent amount of quality-control. Later on, other online resources were added to the corpus, as explained below. Student annotators, described later"
P18-1057,W17-5029,0,0.763787,"eferences are being published daily. As a result, the task of students, educators and researchers of tracking the changing landscape in this field has become increasingly difficult. Recent work has studied the educational aspect of mining text for presenting scientific topics. One goal has been to develop concept maps of topics, graphs showing which topics are prerequisites for learning a given topic (Gordon et al., 2016; Liu et al., 2016; Pan et al., 2017a,b; Liang et al., 2017). Another goal has been to automatically create reading lists for a subject either by building upon concept graphs (Gordon et al., 2017) or through an unstructured approach (Jardine, 2014). Additionally, other work has aimed to automatically summarize scientific topics, either by extractively summarizing academic papers (Jha et al., 2013, 2015; Jaidka et al., 2016) or by producing Wikipedia articles on these topics from multiple sources (Sauper and Barzilay, 2009; Liu et al., 2018). Scientific articles constitute primary texts which describe an author’s work on a particular subject, while Wikipedia articles can be viewed as tertiary sources which summarize both results from primary works as well as explanations from secondary"
P18-1057,P17-1133,0,0.374263,"ficial Intelligence and Deep Learning are strongly influencing current NLP research. With these developments, an increasing number of tutorials and online references are being published daily. As a result, the task of students, educators and researchers of tracking the changing landscape in this field has become increasingly difficult. Recent work has studied the educational aspect of mining text for presenting scientific topics. One goal has been to develop concept maps of topics, graphs showing which topics are prerequisites for learning a given topic (Gordon et al., 2016; Liu et al., 2016; Pan et al., 2017a,b; Liang et al., 2017). Another goal has been to automatically create reading lists for a subject either by building upon concept graphs (Gordon et al., 2017) or through an unstructured approach (Jardine, 2014). Additionally, other work has aimed to automatically summarize scientific topics, either by extractively summarizing academic papers (Jha et al., 2013, 2015; Jaidka et al., 2016) or by producing Wikipedia articles on these topics from multiple sources (Sauper and Barzilay, 2009; Liu et al., 2018). Scientific articles constitute primary texts which describe an author’s work on a partic"
P18-1057,I17-1088,0,0.231531,"ficial Intelligence and Deep Learning are strongly influencing current NLP research. With these developments, an increasing number of tutorials and online references are being published daily. As a result, the task of students, educators and researchers of tracking the changing landscape in this field has become increasingly difficult. Recent work has studied the educational aspect of mining text for presenting scientific topics. One goal has been to develop concept maps of topics, graphs showing which topics are prerequisites for learning a given topic (Gordon et al., 2016; Liu et al., 2016; Pan et al., 2017a,b; Liang et al., 2017). Another goal has been to automatically create reading lists for a subject either by building upon concept graphs (Gordon et al., 2017) or through an unstructured approach (Jardine, 2014). Additionally, other work has aimed to automatically summarize scientific topics, either by extractively summarizing academic papers (Jha et al., 2013, 2015; Jaidka et al., 2016) or by producing Wikipedia articles on these topics from multiple sources (Sauper and Barzilay, 2009; Liu et al., 2018). Scientific articles constitute primary texts which describe an author’s work on a partic"
P18-1057,W09-3607,1,0.7471,"bstractive summarization methods. 3 3.1 Table 1: Top-level Taxonomy Topics Topic Category Introduction to Neural Networks and Deep Learning Tools for Deep Learning Miscellaneous Deep Learning Machine Learning Word Embeddings Recurrent Neural Networks Python Basics Reinforcement learning Convolutional Neural Networks Introduction to AI Dataset Collection Count 635 475 287 225 139 134 133 132 129 89 Table 2: Corpus count by taxonomy topic for the most frequent topics (excluding topic “Other”). An Overview of TutorialBank As opposed to other collections like the ACL Anthology (Bird et al., 2008; Radev et al., 2009, 2013, 2016), which contain solely academic papers, our corpus focuses mainly on resources other than academic papers. The main goal in our decision process of what to include in our corpus has been the quality-control of resources which can be used for an educational purpose. Initially, the resources collected were conference tutorials as well as surveys, books and longer papers on broader topics, as these genres contain an inherent amount of quality-control. Later on, other online resources were added to the corpus, as explained below. Student annotators, described later on, as well as the"
P18-1057,P09-1024,0,0.216821,"ics, graphs showing which topics are prerequisites for learning a given topic (Gordon et al., 2016; Liu et al., 2016; Pan et al., 2017a,b; Liang et al., 2017). Another goal has been to automatically create reading lists for a subject either by building upon concept graphs (Gordon et al., 2017) or through an unstructured approach (Jardine, 2014). Additionally, other work has aimed to automatically summarize scientific topics, either by extractively summarizing academic papers (Jha et al., 2013, 2015; Jaidka et al., 2016) or by producing Wikipedia articles on these topics from multiple sources (Sauper and Barzilay, 2009; Liu et al., 2018). Scientific articles constitute primary texts which describe an author’s work on a particular subject, while Wikipedia articles can be viewed as tertiary sources which summarize both results from primary works as well as explanations from secondary sources. Tang and McCalla (2004, 2009) and Sheng et al. (2017) explore the pedagogical function among the types of sources. To address the problem of the scientific education of NLP more directly, we focus on the annotation and utilization of secondary sources presented in a manner immediately useful to the NLP community. We intr"
P18-1057,W17-5012,0,0.134535,"ne, 2014). Additionally, other work has aimed to automatically summarize scientific topics, either by extractively summarizing academic papers (Jha et al., 2013, 2015; Jaidka et al., 2016) or by producing Wikipedia articles on these topics from multiple sources (Sauper and Barzilay, 2009; Liu et al., 2018). Scientific articles constitute primary texts which describe an author’s work on a particular subject, while Wikipedia articles can be viewed as tertiary sources which summarize both results from primary works as well as explanations from secondary sources. Tang and McCalla (2004, 2009) and Sheng et al. (2017) explore the pedagogical function among the types of sources. To address the problem of the scientific education of NLP more directly, we focus on the annotation and utilization of secondary sources presented in a manner immediately useful to the NLP community. We introduce the TutorialBank corpus, a manually-collected dataset of links to over The field of Natural Language Processing (NLP) is growing rapidly, with new research published daily along with an abundance of tutorials, codebases and other online resources. In order to learn this dynamic field or stay up-to-date on the latest researc"
P18-1057,W16-1511,0,\N,Missing
P18-2017,P15-1136,0,0.125868,"three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achieved impressive gains (Wiseman et al., 2015, 2016; Clark and Manning, 2016b,a). They utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of ha"
P18-2017,D16-1245,0,0.116186,"chieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achie"
P18-2017,P16-1061,0,0.124566,"chieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achie"
P18-2017,H05-1013,0,0.138808,"Missing"
P18-2017,N07-1030,0,0.170921,"are considered as mentions. We show the mention detection accuracy breakdown by span widths in Figure 2. Our model indeed performs better thanks to the mention detection loss. The advantage is even clearer for longer spans which consist of 5 or more words. In addition, it is important to note that our model can detect mentions that do not exist in the training data. While Moosavi and Strube (2017) observe that there is a large overlap between the gold mentions of the training and dev (test) sets, we find that our model can correctly de105 the Entity Detection and Tracking task simultaneously. Denis and Baldridge (2007a) propose to use integer linear programming framework to model anaphoricity and coreference as a joint task. tect 1048 mentions which are not detected by Lee et al. (2017), consisting of 386 mentions existing in training data and 662 mentions not existing in training data. From those 662 mentions, some examples are (1) a suicide murder (2) Hong Kong Island (3) a US Airforce jet carrying robotic undersea vehicles (4) the investigation into who was behind the apparent suicide attack. This shows that our mention loss helps detection by generalizing to new mentions in test data rather than memori"
P18-2017,P10-1142,0,0.0608018,"S Airforce jet carrying robotic undersea vehicles (4) the investigation into who was behind the apparent suicide attack. This shows that our mention loss helps detection by generalizing to new mentions in test data rather than memorizing the existing mentions in training data. 5 6 Conclusion In this paper, we propose to use a biaffine attention model to jointly optimize mention detection and mention clustering in the end-to-end neural coreference resolver. Our model achieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is core"
P18-2017,P02-1014,0,0.0458906,"in test data rather than memorizing the existing mentions in training data. 5 6 Conclusion In this paper, we propose to use a biaffine attention model to jointly optimize mention detection and mention clustering in the end-to-end neural coreference resolver. Our model achieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj"
P18-2017,D13-1203,0,0.0437191,"Missing"
P18-2017,Q14-1037,0,0.0418307,"Missing"
P18-2017,D14-1162,0,0.0817454,"which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs have hidden size 200, and each FFNN has two hidden layers with 150 units and ReLU (Nair and Hinton, 2010) activations. We include (speaker ID, document FFNNanaphora and FFNNantecedent reduce span representation dimensions and only keep information relevant to coreference decisions. Compared with the traditional FFNN approach in Lee et al. (2017), biaffine attention directly models both the compatibility of si and sj by ˆs|j Ub"
P18-2017,W12-4501,0,0.2554,"ine attention model to produce scores c(i, j): where yˆi = sigmoid(m(i)), yi = 1 if and only if si is in one of the gold mention clusters. Our final loss combines mention detection and clustering: Lloss = −λdetect N X i=1 0 Ldetect (i) − N X Lcluster (i0 ) i0 =1 where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe"
P18-2017,D09-1120,0,0.0634763,"grouping mentions in a text such that all mentions in a cluster refer to the same entity. An example is given below (Bj¨orkelund and Kuhn, 2014) where mentions for two entities are labeled in two clusters: [Drug Emporium Inc.]a1 said [Gary Wilber]b1 was named CEO of [this drugstore chain]a2 . [He]b2 succeeds his father, Philip T. Wilber, who founded [the company]a3 and remains chairman. Robert E. Lyons III, who headed the [company]a4 ’s Philadelphia region, was appointed president and chief operating officer, succeeding [Gary Wilber]b3 . Many traditional coreference systems, either rulebased (Haghighi and Klein, 2009; Lee et al., 2011) ∗ Dragomir R. Radev Yale University dragomir.radev@yale.edu Work done during the internship at IBM Watson. 102 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 102–107 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Model architecture. We consider all text spans up to 10-word length as possible mentions. For brevity, we only show three candidate antecedent spans (“Drug Emporium Inc.”, “Gary Wilber”, “was named CEO”) for the current span “this drugstore chain”. 3"
P18-2017,J01-4004,0,0.634055,"ing to new mentions in test data rather than memorizing the existing mentions in training data. 5 6 Conclusion In this paper, we propose to use a biaffine attention model to jointly optimize mention detection and mention clustering in the end-to-end neural coreference resolver. Our model achieves the state-ofthe-art performance on the CoNLL-2012 Shared Task in English. Related Work Acknowledgments As summarized by Ng (2010), learning-based coreference models can be categorized into three types: (1) Mention-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Ferna"
P18-2017,P10-1040,0,0.0385321,"ns 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs have hidden size 200, and each FFNN has two hidden layers with 150 units and ReLU (Nair and Hinton, 2010) activations. We include (speaker ID, document FFNNanaphora and FFNNantecedent reduce span representation dimensions and only keep information relevant to coreference decisions. Compared with the traditional FFNN approach in Lee et al. (2017), biaffine attention directly models both the compatibility of si and sj by ˆs|j Ubiˆsi and the prior | ˆsi . likelihood of si having an antecedent"
P18-2017,N06-2015,0,0.0404729,"moid(m(i)), yi = 1 if and only if si is in one of the gold mention clusters. Our final loss combines mention detection and clustering: Lloss = −λdetect N X i=1 0 Ldetect (i) − N X Lcluster (i0 ) i0 =1 where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embed"
P18-2017,M95-1005,0,0.59745,"loss = −λdetect N X i=1 0 Ldetect (i) − N X Lcluster (i0 ) i0 =1 where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs ha"
P18-2017,W04-3250,0,0.110477,"Missing"
P18-2017,N16-1114,0,0.121226,"-pair models train binary classifiers to determine if a pair of mentions are coreferent (Soon et al., 2001; Ng and Cardie, 2002; Bengtson and Roth, 2008). (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achieved impressive gains (Wiseman et al., 2015, 2016; Clark and Manning, 2016b,a). They utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of hand-crafted features. F"
P18-2017,W11-1902,0,0.0332067,"t such that all mentions in a cluster refer to the same entity. An example is given below (Bj¨orkelund and Kuhn, 2014) where mentions for two entities are labeled in two clusters: [Drug Emporium Inc.]a1 said [Gary Wilber]b1 was named CEO of [this drugstore chain]a2 . [He]b2 succeeds his father, Philip T. Wilber, who founded [the company]a3 and remains chairman. Robert E. Lyons III, who headed the [company]a4 ’s Philadelphia region, was appointed president and chief operating officer, succeeding [Gary Wilber]b3 . Many traditional coreference systems, either rulebased (Haghighi and Klein, 2009; Lee et al., 2011) ∗ Dragomir R. Radev Yale University dragomir.radev@yale.edu Work done during the internship at IBM Watson. 102 Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 102–107 c Melbourne, Australia, July 15 - 20, 2018. 2018 Association for Computational Linguistics Figure 1: Model architecture. We consider all text spans up to 10-word length as possible mentions. For brevity, we only show three candidate antecedent spans (“Drug Emporium Inc.”, “Gary Wilber”, “was named CEO”) for the current span “this drugstore chain”. 3 tion model instead"
P18-2017,P15-1137,0,0.652723,"detection systems. In addition, thanks to the representation power of pre-trained word embeddings and deep neural networks, the model only uses a minimal set of hand-engineered features (speaker ID, document genre, span distance, span width). The core of the end-to-end neural coreference resolver is the scoring function to compute the mention scores for all possible spans and the antecedent scores for a pair of spans. Furthermore, one major challenge of coreference resolution is that most mentions in the document are singleton or non-anaphoric, i.e., not coreferent with any previous mention (Wiseman et al., 2015). Since the data set only have annotations for mention clusters, the end-to-end coreference resolution system needs to detect mentions, detect anaphoricity, and perform coreference linking. Therefore, research questions still remain on good designs of the scoring architecture and the learning strategy for both mention detection and antecedent scoring given only the gold cluster labels. To this end, we propose to use a biaffine attenCoreference resolution aims to identify in a text all mentions that refer to the same real-world entity. The state-of-the-art endto-end neural coreference model con"
P18-2017,D17-1018,0,0.2209,"tson and Roth, 2008; Fernandes et al., 2012; Durrett and Klein, 2013; Bj¨orkelund and Kuhn, 2014), usually solve the problem in two separate stages: (1) a mention detector to propose entity mentions from the text, and (2) a coreference resolver to cluster proposed mentions. At both stages, they rely heavily on complicated, fine-grained, conjoined features via heuristics. This pipeline approach can cause cascading errors, and in addition, since both stages rely on a syntactic parser and complicated handcraft features, it is difficult to generalize to new data sets and languages. Very recently, Lee et al. (2017) proposed the first state-of-the-art end-to-end neural coreference resolution system. They consider all text spans as potential mentions and therefore eliminate the need of carefully hand-engineered mention detection systems. In addition, thanks to the representation power of pre-trained word embeddings and deep neural networks, the model only uses a minimal set of hand-engineered features (speaker ID, document genre, span distance, span width). The core of the end-to-end neural coreference resolver is the scoring function to compute the mention scores for all possible spans and the antecedent"
P18-2017,H05-1004,0,0.153336,"where N is the number of all possible spans, N 0 is the number of unpruned spans, and λdetection controls weights of two terms. ˆsi = FFNNanaphora (si ) 4 ˆsj = FFNNantecedent (sj ), 1 ≤ j ≤ i − 1 (5) | ˆsi c(i, j) = ˆs|j Ubiˆsi + vbi Experiments Data Set and Evaluation We evaluate our model on the CoNLL-2012 Shared Task English data (Pradhan et al., 2012) which is based on the OntoNotes corpus (Hovy et al., 2006). It contains 2,802/343/348 train/development/test documents in different genres. We use three standard metrics: MUC (Vilain et al., 1995), B3 (Bagga and Baldwin, 1998), and CEAFφ4 (Luo, 2005). We report Precision, Recall, F1 for each metric and the average F1 as the final CoNLL score. Implementation Details For fair comparisons, we follow the same hyperparameters as in Lee et al. (2017). We consider all spans up to 10 words and up to 250 antecedents. λ = 0.4 is used for span pruning. We use fixed concatenations of 300-dimension GloVe (Pennington et al., 2014) embeddings and 50-dimension embeddings from Turian et al. (2010). Character CNNs use 8dimension learned embeddings and 50 kernels for each window size in {3,4,5}. LSTMs have hidden size 200, and each FFNN has two hidden layer"
P18-2017,Q15-1029,0,0.0391508,". (2) Mention-ranking models explicitly rank all previous candidate mentions for the current mention and select a single highest scoring antecedent for each anaphoric mention (Denis and Baldridge, 2007b; Wiseman et al., 2015; Clark and Manning, 2016a; Lee et al., 2017). (3) Entity-mention models learn classifiers to determine whether the current mention is coreferent with a preceding, partially-formed mention cluster (Clark and Manning, 2015; Wiseman et al., 2016; Clark and Manning, 2016b). In addition, we also note latent-antecedent models (Fernandes et al., 2012; Bj¨orkelund and Kuhn, 2014; Martschat and Strube, 2015). Fernandes et al. (2012) introduce coreference trees to represent mention clusters and learn to extract the maximum scoring tree in the graph of mentions. Recently, several neural coreference resolution systems have achieved impressive gains (Wiseman et al., 2015, 2016; Clark and Manning, 2016b,a). They utilize distributed representations of mention pairs or mention clusters to dramatically reduce the number of hand-crafted features. For example, Wiseman et al. (2015) propose the first neural coreference resolution system by training a deep feed-forward neural network for mention ranking. How"
P18-2017,P17-2003,0,0.0117705,"ts have contributions and when they work together the total gain is even higher. Mention Detection Subtask To further understand our model, we perform a mention detection subtask where spans with mention scores higher than 0 are considered as mentions. We show the mention detection accuracy breakdown by span widths in Figure 2. Our model indeed performs better thanks to the mention detection loss. The advantage is even clearer for longer spans which consist of 5 or more words. In addition, it is important to note that our model can detect mentions that do not exist in the training data. While Moosavi and Strube (2017) observe that there is a large overlap between the gold mentions of the training and dev (test) sets, we find that our model can correctly de105 the Entity Detection and Tracking task simultaneously. Denis and Baldridge (2007a) propose to use integer linear programming framework to model anaphoricity and coreference as a joint task. tect 1048 mentions which are not detected by Lee et al. (2017), consisting of 386 mentions existing in training data and 662 mentions not existing in training data. From those 662 mentions, some examples are (1) a suicide murder (2) Hong Kong Island (3) a US Airfor"
P18-2017,D08-1031,0,\N,Missing
P19-1102,J98-3005,1,\N,Missing
P19-1102,W04-3252,0,\N,Missing
P19-1102,D15-1044,0,\N,Missing
P19-1102,C10-1039,0,\N,Missing
P19-1102,N09-1041,0,\N,Missing
P19-1102,P99-1071,0,\N,Missing
P19-1102,W00-0403,1,\N,Missing
P19-1102,hong-etal-2014-repository,0,\N,Missing
P19-1102,W12-3018,0,\N,Missing
P19-1102,P16-1046,0,\N,Missing
P19-1102,N16-1012,0,\N,Missing
P19-1102,P17-1099,0,\N,Missing
P19-1102,K17-1045,1,\N,Missing
P19-1102,P17-2074,0,\N,Missing
P19-1102,N18-1150,0,\N,Missing
P19-1102,N18-2097,0,\N,Missing
P19-1102,D18-1206,0,\N,Missing
P19-1102,W18-6545,0,\N,Missing
P19-1306,D17-1110,0,0.0683054,"Missing"
P19-1306,P18-4020,0,0.0274498,"Missing"
P19-1306,P07-2045,0,0.00605898,"report the Actual Query Weighted Value (AQWV) (NIST, 2017), a set-based metric with penalty for both missing relevant and returning irrelevant documents. We use β = 40.0 and find the best global fixed cutoff over all queries. Baselines. For traditional CLIR approaches, we use query translation and document translation with the Indri system. For query translation, we use Dictionary-Based Query Translation (DBQT) and Probabilistic Structured Query (PSQ). For document translation, we use Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). For SMT, we use the moses system (Koehn et al., 2007) with word alignments using mGiza and 5-gram KenLM language model (Heafield, 2011). For NMT, we use sequence-to4 2 github.com/facebookresearch/MUSE 3 https://www.wiktionary.org/ www.iarpa.gov/index.php/ research-programs/material 5 https://trec.nist.gov/trec_eval/ 3176 sequence model with attention (Bahdanau et al., 2015; Miceli Barone et al., 2017) implemented in Marian (Junczys-Dowmunt et al., 2018). For deep relevance ranking baselines, we investigate recent state-of-the-art models including PACRR, PACRR-DRMM, and POSIT-DRMM. These models and our methods all use an SMTbased document transla"
P19-1306,W11-2123,0,0.0274541,"nalty for both missing relevant and returning irrelevant documents. We use β = 40.0 and find the best global fixed cutoff over all queries. Baselines. For traditional CLIR approaches, we use query translation and document translation with the Indri system. For query translation, we use Dictionary-Based Query Translation (DBQT) and Probabilistic Structured Query (PSQ). For document translation, we use Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). For SMT, we use the moses system (Koehn et al., 2007) with word alignments using mGiza and 5-gram KenLM language model (Heafield, 2011). For NMT, we use sequence-to4 2 github.com/facebookresearch/MUSE 3 https://www.wiktionary.org/ www.iarpa.gov/index.php/ research-programs/material 5 https://trec.nist.gov/trec_eval/ 3176 sequence model with attention (Bahdanau et al., 2015; Miceli Barone et al., 2017) implemented in Marian (Junczys-Dowmunt et al., 2018). For deep relevance ranking baselines, we investigate recent state-of-the-art models including PACRR, PACRR-DRMM, and POSIT-DRMM. These models and our methods all use an SMTbased document translation as input. Implementation Details. For POSIT-DRMM and Bilingual POSIT-DRMM, we"
P19-1306,P99-1027,0,0.908895,"s of query and documents independently, and the matching is performed at the final stage. The latter explicitly encodes the interaction between terms to direct capture word-level interaction patterns. For example, the DRMM (Guo et al., 2016) first compares the term embeddings of each pair of terms within the query and the document and then generates fixedlength matching histograms. 3 4 Related Work Cross-lingual Information Retrieval. Traditional CLIR approaches include document translation and query translation, and more research efforts are on the latter (Oard and Hackett, 1997; Oard, 1998; McCarley, 1999; Franz et al., 1999). Early methods use the dictionary to translate the user query (Hull and Grefenstette, 1996; Ballesteros and Croft, 1996; Pirkola, 1998). Other methods include the single best SMT query translation (Chin et al., 2014) and the weighted SMT translation alternatives known as the probabilistic structured query (PSQ) (Darwish and Oard, 2003; Ture et al., 2012). Recently, Bai et al. (2010) and Sokolov et al. (2013) propose methods to learn the sparse query-document associations from supervised ranking signals on cross-lingual Wikipedia and patent data, respectively. Furthermore,"
P19-1306,D18-1211,0,0.510747,"slation direction, it can be further categorized into the document translation and the query translation approaches (Nie, 2010). In both cases, we first solve the translation problem, and the task is transformed to the monolingual setting. However, while conceptually simple, the performance of this modular approach is fundamentally limited by the quality of machine translation. Recently, many deep neural IR models have shown promising results on monolingual data sets (Huang et al., 2013; Guo et al., 2016; Pang et al., 2016; Mitra et al., 2016, 2017; Xiong et al., 2017; Hui et al., 2017, 2018; McDonald et al., 2018). They learn a scoring function directly from the relevance label of query-document pairs. However, it is not clear how to use them when documents and queries are not in the same language. Furthermore, those deep neural networks need a large amount of training data. This is expensive to get for lowresource language pairs in our cross-lingual case. In this paper, we propose a cross-lingual deep relevance ranking architecture based on a bilingual view of queries and documents. As shown in Figure 1, our model first translates queries and documents and then uses four components to match them in bo"
P19-1306,W17-4710,0,0.0312461,"ry translation, we use Dictionary-Based Query Translation (DBQT) and Probabilistic Structured Query (PSQ). For document translation, we use Statistical Machine Translation (SMT) and Neural Machine Translation (NMT). For SMT, we use the moses system (Koehn et al., 2007) with word alignments using mGiza and 5-gram KenLM language model (Heafield, 2011). For NMT, we use sequence-to4 2 github.com/facebookresearch/MUSE 3 https://www.wiktionary.org/ www.iarpa.gov/index.php/ research-programs/material 5 https://trec.nist.gov/trec_eval/ 3176 sequence model with attention (Bahdanau et al., 2015; Miceli Barone et al., 2017) implemented in Marian (Junczys-Dowmunt et al., 2018). For deep relevance ranking baselines, we investigate recent state-of-the-art models including PACRR, PACRR-DRMM, and POSIT-DRMM. These models and our methods all use an SMTbased document translation as input. Implementation Details. For POSIT-DRMM and Bilingual POSIT-DRMM, we use the k-maxpooling with k = 5 and 0.3 dropout of the BiLSTM output. For PACRR, PACRR-DRMM and their bilingual counterparts, we use convolutional filter sizes with [1,2,3], and each filter size has 32 filters. We use k = 2 in the k-max-pooling. The loss function is m"
P19-1306,W17-2328,0,0.0243248,"rmore, Vuli´c Experiments Training and Inference. We first use the Indri1 system which uses query likelihood with Dirichlet Smoothing (Zhai and Lafferty, 2004) to preselect the documents from the collection. To build the training dataset, for each positive example in the returned list, we randomly sample one negative example from the documents returned by Indri. The model is then trained with a binary crossentropy loss. On validation or testing set, we use our prediction scores to rerank the documents returned by Indri. Extra Features. Following the previous work (Severyn and Moschitti, 2015; Mohan et al., 2017; McDonald et al., 2018), we compute the final relevance score by a linear model to combine the model output with the following set of extra fea3175 1 www.lemurproject.org/indri.php MAP P@20 Query Translation and Document Translation with Indri Dictionary-Based Query Translation (DBQT) 20.93 4.86 Probabilistic Structured Query (PSQ) 27.16 5.81 Statistical MT (SMT) 26.30 5.28 Neural MT (NMT) 26.54 5.26 Deep Relevance Ranking PACRR 24.69 5.24 PACRR-DRMM 22.15 5.14 POSIT-DRMM 23.91 6.04 Deep Relevance Ranking with Extra Features in Section 4 PACRR 27.03 5.34 PACRR-DRMM 25.46 5.50 POSIT-DRMM 26.10"
P19-1306,N18-2073,0,0.100832,"Missing"
P19-1306,D13-1175,0,0.0219708,"n Retrieval. Traditional CLIR approaches include document translation and query translation, and more research efforts are on the latter (Oard and Hackett, 1997; Oard, 1998; McCarley, 1999; Franz et al., 1999). Early methods use the dictionary to translate the user query (Hull and Grefenstette, 1996; Ballesteros and Croft, 1996; Pirkola, 1998). Other methods include the single best SMT query translation (Chin et al., 2014) and the weighted SMT translation alternatives known as the probabilistic structured query (PSQ) (Darwish and Oard, 2003; Ture et al., 2012). Recently, Bai et al. (2010) and Sokolov et al. (2013) propose methods to learn the sparse query-document associations from supervised ranking signals on cross-lingual Wikipedia and patent data, respectively. Furthermore, Vuli´c Experiments Training and Inference. We first use the Indri1 system which uses query likelihood with Dirichlet Smoothing (Zhai and Lafferty, 2004) to preselect the documents from the collection. To build the training dataset, for each positive example in the returned list, we randomly sample one negative example from the documents returned by Indri. The model is then trained with a binary crossentropy loss. On validation o"
P19-1306,C12-1164,0,0.385793,"Missing"
P19-1306,N15-1104,0,0.0863141,"Missing"
P19-1306,P14-2080,0,0.277727,"-gram matching features. We then use maxpooling and k-max-pooling to produce the feature matrix where each row is a document-aware encoding of a query term. The final step computes the relevance score: Bilingual PACRR uses an MLP on the whole feature matrix to get the relevance score, while Bilingual PACRR-DRMM first uses an MLP on individual rows to get query term scores and then use a second layer to combine them. and Moens (2015) and Litschko et al. (2018) use cross-lingual word embeddings to represent both queries and documents as vectors and perform IR by computing the cosine similarity. Schamoni et al. (2014) and Sasaki et al. (2018) also use an automatic process to build CLIR datasets from Wikipeida articles. Neural Learning to Rank. Most of neural learning to rank models can be categorized in two groups: representation based (Huang et al., 2013; Shen et al., 2014) and interaction based (Pang et al., 2016; Guo et al., 2016; Hui et al., 2017; Xiong et al., 2017; McDonald et al., 2018). The former builds representations of query and documents independently, and the matching is performed at the final stage. The latter explicitly encodes the interaction between terms to direct capture word-level inte"
P19-1306,oard-1998-comparative,0,\N,Missing
P19-1443,W06-3000,0,0.242624,"he development in dialogue (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018) uses predefined ontology and slot-value pairs with limited natural language meaning representation, whereas we focus on general SQL queries that enable more powerful semantic meaning representation. Recently, some conversational question answering datasets have been introduced, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018). They differ from SParC in that the answers are free-form text instead of SQL queries. On the other hand, Kato et al. (2004); Chai and Jin (2004); Bertomeu et al. (2006) conduct early studies of the contextual phenomena and thematic relations in database dialogue/QA systems, which we use as references when constructing SParC. 3 Data Collection We create the SParC dataset in four stages: selecting interaction goals, creating questions, annotating SQL representations, and reviewing. Interaction goal selection To ensure thematic relevance within each question sequence, we use questions in the original Spider dataset as the thematic guidance for constructing meaningful query interactions, i.e. the interaction goal. Each sequence is based on a question in Spider a"
P19-1443,W04-2504,0,0.426463,"atabase (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The users may explicitly refer to or omit previously mentioned entities and constraints, and may introduce refinements, additions or substitutions to what has already been said (Figure 1). This requires a practical text-to-SQL system to effectively process context information to synthesize the correct SQL logic. To enable modeling advances in contextdependent semantic parsing, we introduce SParC (cross-domain Semantic Parsing in Context), an expert-labeled dataset which contains 4,298 coherent question sequences (12k+ questions paired with SQL queries) querying 200 co"
P19-1443,H94-1010,0,0.882449,"ese work focus on precisely mapping stand-alone utterances to SQL queries, generating SQL queries in a context-dependent scenario (Miller et al., 1996; Zettlemoyer and 4511 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4511–4523 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Collins, 2009; Suhr et al., 2018) has been studied less often. The most prominent context-dependent text-to-SQL benchmark is ATIS1 , which is set in the flight-booking domain and contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The us"
P19-1443,P16-1004,0,0.0694107,"atabases have to address. In addition, it enables us to test the generalization of the trained systems to unseen databases and domains. We asked 15 college students with SQL experience to come up with question sequences over the Spider databases (§ 3). Questions in the original Spider dataset were used as guidance to the students for constructing meaningful interactions: each sequence is based on a question in Spider and the student has to ask inter-related questions to ob1 A subset of ATIS is also frequently used in contextindependent semantic parsing research (Zettlemoyer and Collins, 2007; Dong and Lapata, 2016). 2 The data is available at https://yale-lily. github.io/spider. tain information that answers the Spider question. At the same time, the students are encouraged to come up with related questions which do not directly contribute to the Spider question so as to increase data diversity. The questions were subsequently translated to complex SQL queries by the same student. Similar to Spider, the SQL Queries in SParC cover complex syntactic structures and most common SQL keywords. We split the dataset such that a database appears in only one of the train, development and test sets. We provide det"
P19-1443,P18-1068,0,0.0403454,"at there is plenty of room for advancement in modeling and learning on the SParC dataset. 2 Related Work Context-independent semantic parsing Early studies in semantic parsing (Zettlemoyer and Collins, 2005; Artzi and Zettlemoyer, 2013; Berant and Liang, 2014; Li and Jagadish, 2014; Pasupat and Liang, 2015; Dong and Lapata, 2016; Iyer et al., 2017) were based on small and singledomain datasets such as ATIS (Hemphill et al., 1990; Dahl et al., 1994) and GeoQuery (Zelle and Mooney, 1996). Recently, an increasing number of neural approaches (Zhong et al., 2017; Xu et al., 2017; Yu et al., 2018a; Dong and Lapata, 2018; Yu et al., 2018b) have started to use large and crossdomain text-to-SQL datasets such as WikiSQL (Zhong et al., 2017) and Spider (Yu et al., 2018c). Most of them focus on converting stand-alone natural language questions to executable queries. Table 1 compares SParC with other semantic parsing datasets. Context-dependent semantic parsing with SQL labels Only a few datasets have been constructed for the purpose of mapping contextdependent questions to structured queries. 3 Exact string match ignores ordering discrepancies of SQL components whose order does not matter. Exact set matching is ab"
P19-1443,H90-1021,0,0.896741,"Missing"
P19-1443,W14-4337,0,0.0264241,"Missing"
P19-1443,P17-1089,0,0.361004,"Missing"
P19-1443,P17-1167,0,0.14015,"Missing"
P19-1443,W04-2509,0,0.345013,"contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to achieve a complex goal. Previous studies have shown that by allowing questions to be constructed sequentially, users can explore the data in a more flexible manner, which reduces their cognitive burden (Hale, 2006; Levy, 2008; Frank, 2013; Iyyer et al., 2017) and increases their involvement when interacting with the system. The phrasing of such questions depends heavily on the interaction history (Kato et al., 2004; Chai and Jin, 2004; Bertomeu et al., 2006). The users may explicitly refer to or omit previously mentioned entities and constraints, and may introduce refinements, additions or substitutions to what has already been said (Figure 1). This requires a practical text-to-SQL system to effectively process context information to synthesize the correct SQL logic. To enable modeling advances in contextdependent semantic parsing, we introduce SParC (cross-domain Semantic Parsing in Context), an expert-labeled dataset which contains 4,298 coherent question sequences (12k+ questions paired with SQL quer"
P19-1443,P16-1138,0,0.462801,"rsing datasets. Context-dependent semantic parsing with SQL labels Only a few datasets have been constructed for the purpose of mapping contextdependent questions to structured queries. 3 Exact string match ignores ordering discrepancies of SQL components whose order does not matter. Exact set matching is able to consider ordering issues in SQL evaluation. See more evaluation details in section 6.1. 4512 Dataset SParC ATIS (Hemphill et al., 1990; Dahl et al., 1994) Spider (Yu et al., 2018c) WikiSQL (Zhong et al., 2017) GeoQuery (Zelle and Mooney, 1996) SequentialQA (Iyyer et al., 2017) SCONE (Long et al., 2016) Context X X 7 7 7 X X Resource database database database table database table environment Annotation SQL SQL SQL SQL SQL denotation denotation Cross-domain X 7 X X 7 X 7 Table 1: Comparison of SParC with existing semantic parsing datasets. Hemphill et al. (1990); Dahl et al. (1994) collected the contextualized version of ATIS that includes series of questions from users interacting with a flight database. Adopted by several works later on (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018), ATIS has only a single domain for flight planning which limits the possible SQL lo"
P19-1443,P96-1008,0,0.654289,"Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work focus on precisely mapping stand-alone utterances to SQL queries, generating SQL queries in a context-dependent scenario (Miller et al., 1996; Zettlemoyer and 4511 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4511–4523 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics Collins, 2009; Suhr et al., 2018) has been studied less often. The most prominent context-dependent text-to-SQL benchmark is ATIS1 , which is set in the flight-booking domain and contains only one database (Hemphill et al., 1990; Dahl et al., 1994). In a real-world setting, users tend to ask a sequence of thematically related questions to learn about a particular topic or to ac"
P19-1443,P17-1163,0,0.0488151,"Missing"
P19-1443,P15-1142,0,0.0459989,"asets used in recovering context-dependent meaning (including SCONE (Long et al., 2016) and SequentialQA (Iyyer et al., 2017)) contain no logical form annotations but only denotation (Berant and Liang, 2014) instead. SCONE (Long et al., 2016) contains some instructions in limited domains such as chemistry experiments. The formal representations in the dataset are world states representing state changes after each instruction instead of programs or logical forms. SequentialQA (Iyyer et al., 2017) was created by asking crowd workers to decompose some complicated questions in WikiTableQuestions (Pasupat and Liang, 2015) into sequences of inner-related simple questions. As shown in Table 1, neither of the two datasets were annotated with query labels. Thus, to make the tasks feasible, SCONE (Long et al., 2016) and SequentialQA (Iyyer et al., 2017) exclude many questions with rich semantic and contextual types. For example, (Iyyer et al., 2017) requires that the answers to the questions in SequentialQA must appear in the table, and most of them can be solved by simple SQL queries with SELECT and WHERE clauses. Such direct mapping without formal query labels becomes unfeasible for complex questions. Furthermore"
P19-1443,D14-1162,1,0.110555,"ts corresponding table name and column name separated by a special dot token (i.e., table name.column name), and use the average word embedding10 of tokens in this sequence as the column header embedding hC . Decoder The decoder is implemented with another LSTM (LSTMD ) with attention to the LSTME representations of the questions in η previous turns. At each decoding step, the decoder chooses to generate either a SQL keyword (e.g., select, where, group by) or a column header. To achieve this, we use separate layers to score SQL keywords and column headers, 10 We use the 300-dimensional GloVe (Pennington et al., 2014) pretrained word embeddings. 4517 Model and finally use the softmax operation to generate the output probability distribution over both categories. 5.2 SyntaxSQLNet with history input (SyntaxSQL-con) SyntaxSQLNet is a syntax tree based neural model for the complex and cross-domain contextindependent text-to-SQL task introduced by Yu et al. (2018b). The model consists of a table-aware column attention encoder and a SQL-specific syntax tree-based decoder. The decoder adopts a set of inter-connected neural modules to generate different SQL syntax components. We extend this model by providing the"
P19-1443,N18-1203,0,0.332817,"Missing"
P19-1443,D07-1071,0,0.146156,"atural language interfaces to databases have to address. In addition, it enables us to test the generalization of the trained systems to unseen databases and domains. We asked 15 college students with SQL experience to come up with question sequences over the Spider databases (§ 3). Questions in the original Spider dataset were used as guidance to the students for constructing meaningful interactions: each sequence is based on a question in Spider and the student has to ask inter-related questions to ob1 A subset of ATIS is also frequently used in contextindependent semantic parsing research (Zettlemoyer and Collins, 2007; Dong and Lapata, 2016). 2 The data is available at https://yale-lily. github.io/spider. tain information that answers the Spider question. At the same time, the students are encouraged to come up with related questions which do not directly contribute to the Spider question so as to increase data diversity. The questions were subsequently translated to complex SQL queries by the same student. Similar to Spider, the SQL Queries in SParC cover complex syntactic structures and most common SQL keywords. We split the dataset such that a database appears in only one of the train, development and t"
P19-1443,P09-1110,0,0.281655,"l., 2018c) WikiSQL (Zhong et al., 2017) GeoQuery (Zelle and Mooney, 1996) SequentialQA (Iyyer et al., 2017) SCONE (Long et al., 2016) Context X X 7 7 7 X X Resource database database database table database table environment Annotation SQL SQL SQL SQL SQL denotation denotation Cross-domain X 7 X X 7 X 7 Table 1: Comparison of SParC with existing semantic parsing datasets. Hemphill et al. (1990); Dahl et al. (1994) collected the contextualized version of ATIS that includes series of questions from users interacting with a flight database. Adopted by several works later on (Miller et al., 1996; Zettlemoyer and Collins, 2009; Suhr et al., 2018), ATIS has only a single domain for flight planning which limits the possible SQL logic it contains. In contrast to ATIS, SParC consists of a large number of complex SQL queries (with most SQL syntax components) inquiring 200 databases in 138 different domains, which contributes to its diversity in query semantics and contextual dependencies. Similar to Spider, the databases in the train, development and test sets of SParC do not overlap. Context-dependent semantic parsing with denotations Some datasets used in recovering context-dependent meaning (including SCONE (Long et"
P19-1443,P18-1135,1,0.81917,"uery labels becomes unfeasible for complex questions. Furthermore, SequentialQA contains questions based only on a single Wikipedia tables at a time. In contrast, SParC contains 200 significantly larger databases, and complex query labels with all common SQL key components. This requires a system developed for SParC to handle information needed over larger databases in different domains. Conversational QA and dialogue system Language understanding in context is also studied for dialogue and question answering systems. The development in dialogue (Henderson et al., 2014; Mrkˇsi´c et al., 2017; Zhong et al., 2018) uses predefined ontology and slot-value pairs with limited natural language meaning representation, whereas we focus on general SQL queries that enable more powerful semantic meaning representation. Recently, some conversational question answering datasets have been introduced, such as QuAC (Choi et al., 2018) and CoQA (Reddy et al., 2018). They differ from SParC in that the answers are free-form text instead of SQL queries. On the other hand, Kato et al. (2004); Chai and Jin (2004); Bertomeu et al. (2006) conduct early studies of the contextual phenomena and thematic relations in database di"
P19-1443,N18-2093,1,0.869956,"ORDER BY Introduction date_became_customer DESC LIMIT 1 Querying a relational database is often challenging and a natural language interface has long been regarded by many as the most powerful database interface (Popescu et al., 2003; Bertomeu et al., 2006; Li and Jagadish, 2014). The problem of mapping a natural language utterance into executable SQL queries (text-to-SQL) has attracted increasing attention from the semantic parsing community by virtue of a continuous effort of dataset creation (Zelle and Mooney, 1996; Iyyer et al., 2017; Zhong et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a) and the modeling innovation that follows it (Xu et al., 2017; Wang et al., 2018; Yu et al., 2018b; Shi et al., 2018). Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work f"
P19-1443,D18-1193,1,0.919435,"ORDER BY Introduction date_became_customer DESC LIMIT 1 Querying a relational database is often challenging and a natural language interface has long been regarded by many as the most powerful database interface (Popescu et al., 2003; Bertomeu et al., 2006; Li and Jagadish, 2014). The problem of mapping a natural language utterance into executable SQL queries (text-to-SQL) has attracted increasing attention from the semantic parsing community by virtue of a continuous effort of dataset creation (Zelle and Mooney, 1996; Iyyer et al., 2017; Zhong et al., 2017; Finegan-Dollak et al., 2018; Yu et al., 2018a) and the modeling innovation that follows it (Xu et al., 2017; Wang et al., 2018; Yu et al., 2018b; Shi et al., 2018). Q3 : How about for the first 5 customers? S 3 : SELECT customer_name FROM customers ORDER BY date_became_customer LIMIT 5 Figure 1: Two question sequences from the SParC dataset. Questions (Qi ) in each sequence query a database (Di ), obtaining information sufficient to complete the interaction goal (Ci ). Each question is annotated with a corresponding SQL query (Si ). SQL token sequences from the interaction context are underlined. While most of these work f"
P19-1443,W06-3001,0,\N,Missing
P19-1443,Q13-1005,0,\N,Missing
P19-1443,P14-1133,0,\N,Missing
P19-1443,P18-1033,1,\N,Missing
P19-1443,D18-1241,0,\N,Missing
P98-2176,A97-1033,1,0.834878,"2 and 24. 10' : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : : !F!il i!i i if! ii Figure 1: Sample sentence containing two entity-description pairs. Each entity appearing in a text can have multiple descriptions (up to several dozen) associated with it. We call the set of all descriptions related to the same entity in a corpus, a profile of that entity. Profiles for a large number of entities were compiled using our earlier system, PROFILE (Radev and McKeown, 1997). It turns out that there is a large variety in the size of the profile (number of distinct descriptions) for different entities. Table 1 shows a subset of the profile for Ung Huot, the former foreign minister of Cambodia, who was elected prime minister at some point of time during the run of our experiment. A few sample semantic features of the descriptions in Table 1 are shown as separate columns. We used information extraction techniques to collect entities and descriptions from a corpus and analyzed their lexical and semantic properties. We have processed 178 MB 1 of newswire and analyzed"
P98-2176,J98-3005,1,0.779805,"riptions, mostly due to incorrect P P attachment. We have also had problems from the part-of-speech tagger and, as a result, we occasionally incorrectly extract word sequences that do not represent descriptions. 6 Applications and Future Work We should note that P R O F I L E is part of a large system for information retrieval and summarization of news through information extraction and symbolic text generation (McKeown and Radev, 1995). We intend to use PROFILE to improve lexical choice in the summary generation component, especially when producing user-centered summaries or summary updates (Radev and McKeown, 1998 to appear). There are two particularly appealing cases - (1) when the extraction component has failed to extract a description and (2) when the user model (user's interests, knowledge of the entity and personal preferences for sources of information and for either conciseness or verbosity) dictates that a description should be used even when one doesn't appear in the texts being summarized. A second potentially interesting application involves using the data and rules extracted by PROFILE for language regeneration. In (Radev and McKeown, 1998 to appear) we show how the conversion of extracted"
P98-2176,J93-1007,0,0.014393,"udes techniques of extracting shallow structure from a corpus and applying t h a t structure to computer-generated texts. Language reuse involves two components: a source text written by a h u m a n and a target text, that is to be automatically generated by a computer, partially making use of structures reused from the source text. T h e source text is the one from which particular surface structures are extracted automatically, along with the appropriate syntactic, semantic, and pragmatic constraints under which t h e y are used. Some examples of language reuse include collocation analysis (Smadja, 1993), the use of entire factual sentences extracted from corpora (e.g., &quot;'Toy Story' is the Academy Award winning animated film developed by Pixar~'), and summarization using sentence extraction (Paice, 1990; Kupiec et al., 1995). In the case of summarization through sentence extraction, the target text has the additional property of being a subtext of the source text. Other techniques t h a t can be broadly categorized as language reuse are learning relations from on-line texts (Mitchell, 1997) and answering natural language questions using an on-line encyclopedia (Kupiec, 1993). Stydying the con"
patel-radev-2006-lexical,P02-1040,0,\N,Missing
patel-radev-2006-lexical,N03-1024,0,\N,Missing
patel-radev-2006-lexical,N03-1003,0,\N,Missing
Q14-1001,bird-etal-2008-acl,1,0.800631,"Missing"
Q14-1001,P11-1061,0,0.0414154,"Missing"
Q14-1001,D08-1038,0,0.0363006,"4 16 dialogue 4 10 knowledge 4 1 discourse Table 3: The entities of each type receiving the highest scores from the heterogeneous network Pagerank impact measure along with their respective changes in ranking when compared to a simple citation count measure. One possible way to address this is to use a narrower time window when creating the graph, such as only including edges from the previous five years. We apply this technique in the following section. 2.4 Entity impact evolution The heterogeneous graph formalism also provides a natural way to study the evolution of impact over time, as in (Hall et al., 2008), but at a much finer granularity. Hall et al. measured the year-by-year prominence of statistical topics, but we can measure year-by-year prominence for any entity in the graph. To measure the evolution of impacts over the years, we iteratively create year-by-year versions of the heterogeneous AAN. Each of these graphs contains all entities along with all edges occurring in a five year window. Due to space, we cannot comprehensively exhibit this technique and the data it produces, but as a brief example, in Figure 1, we show how the impacts of some major NLP conferences changes over time. The"
Q14-1001,W12-4102,1,0.689384,"ffect the relative rankings between different types of entities, this method is probably not appropriate for comparing entities of two different types against each other. But between nodes of the same type, this measure is an appropriate (and as we will show, accurate) way to compare impacts. We see this method as a first logical step in the direction of heterogeneous network-based scientometrics. This method could easily be extended to use a directed schema (Kurland and Lee, 2005) or a schema that is aware of the lexical content of citation sentences, such as sentiment-based signed networks (Hassan et al., 2012). Determining the intrinsic quality of scientific impact measures can be difficult since there is no way to collect gold standard measurements for realworld entities. Previous studies have attempted to show that their measures give high scores to a few known high-impact entities, e.g. Nobel prize winners (Hirsch, 2005), or have performed a statistical component analysis to find the most important measures in a group of related statistics (Bollen et al., 2009). Our approach, instead, is to generate realistic data from synthetic entities whose impacts are known. We had considered alternative for"
Q14-1001,N10-1017,0,0.0279166,"Missing"
Q14-1001,H05-1066,0,0.0598075,"Missing"
Q14-1001,H05-1052,0,0.0302469,"isambiguation, topic modeling, and the measurement of scientific impact to be easily solved using only this network and off-the-shelf graph algorithms. 1 1.1 Heterogeneous AAN schema We build a heterogeneous graph G(V, E) from AAN, where V is the set of vertices and E is the set of edges connecting vertices. A vertex can be one of five semantic types: {paper, author, venue, institution, term}. An edge can also be one of five types, each connecting different types of vertices: Introduction Graph-based methods have been used to great effect in NLP, on problems such as word sense disambiguation (Mihalcea, 2005), summarization (Erkan and Radev, 2004), and dependency parsing (McDonald et al., 2005). Most previous studies of networks consider networks with only a single type of node, and in some cases using a network with a single type of node can be an oversimplified view if it ignores other types of relationships. In this paper we will demonstrate heterogeneous networks, networks with multiple different types of nodes and edges, along with several applications of them. The applications in this paper are not presented so much as robust attempts to out-perform the current state-of-the-art, but rather a"
Q14-1001,D11-1024,0,0.0485264,"istribution of topic coherences for the four topic models. 4.3 Topic Model Evaluation We provide two separate evaluations in this section, one of the topics alone, and one extrinstic evaluation of the entire paper-topic model. The variants of random walk topic models are compared against LDA and the relational topic model (RTM), each with 100 topics (Chang and Blei, 2010). As RTM allows only a single type of relationship between documents, we use citations as the inter-document relationships. 4.3.1 Topic Coherence The coherence of a topic is evaluated using the coherence metric introduced in (Mimno et al., 2011). (t) (t) Given the top M words V (t) = (v1 , ..., vM ) for a topic t, the coherence of that topic can be calculated with the following formula: ! M m−1 (t) (t) X X D(v , v ) + 1 m l C(t; V (t) ) = log , (t) D(vl ) m=2 l=1 where D(v) is the number of documents containing v and D(v, v 0 ) is the number of documents containing both v and v 0 . This measure of coherence is highly correlated with manual annotations of topic quality, with a higher coherence score corresponding to a more coherent, higher quality topic. After calculating the coherence for each of the 100 topics for RTM and the random"
Q14-1001,W11-2207,0,0.0779684,"Missing"
Q14-1001,W04-3252,0,\N,Missing
radev-etal-2004-cst,J98-3005,1,\N,Missing
radev-etal-2004-cst,clough-etal-2002-building,0,\N,Missing
radev-etal-2004-cst,P02-1020,0,\N,Missing
radev-etal-2004-cst,W00-1009,1,\N,Missing
radev-etal-2004-cst,P02-1040,0,\N,Missing
radev-etal-2004-cst,P02-1047,0,\N,Missing
radev-etal-2004-cst,J96-2004,0,\N,Missing
radev-etal-2004-mead,W02-0404,1,\N,Missing
radev-etal-2004-mead,H01-1056,1,\N,Missing
radev-etal-2004-mead,W00-1009,1,\N,Missing
radev-etal-2004-mead,P02-1040,0,\N,Missing
S12-1043,W10-3110,0,0.146107,"at recognizes negation patterns in biomedical text. It consists of two components: a lexical scanner, lexer that uses regular expression rules to generate a finite state machine, and a parser. Morante (2008b) proposed a supervised approach for detecting negation cues and their scopes in biomedical text. Their system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another one that finds the full scope of these negation signals. Negation has been also studied in the context of sentiment analysis (Wilson et al., 2005; Jia et al., 2009; Councill et al., 2010; Heerschop et al., 2011; Hogenboom et al., 2011). Wiegand et al. (2010) surveyed the recent work on negation scope detection for sentiment analysis. Wilson et al. (2005) studied 329 the contextual features that affect text polarity. They used a machine learning approach in which negation is encoded using several features. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. Another feature accounts for a polar predicate having a negated subject. They also have disambiguation features to handle negation words that do not functi"
S12-1043,W09-1304,0,0.0254937,"ntics (*SEM), pages 328–334, c Montr´eal, Canada, June 7-8, 2012. 2012 Association for Computational Linguistics Token She would not have said ‘ Godspeed ’ had it not been so . Lemma She would not have say ‘ Godspeed ’ have it not be so . POS PRP MD RB VB VBD “ NNP ” VBD PRP RB VBN RB . Syntax (S(NP*) (VP* * (VP* (VP* (SBAR(S(NP* * *) (VP* (ADVP* *) (VP* (ADVP*)))))))) *) Cue 1 not - Scope 1 She would have said ’ Godspeed ’ had it not been so - Event 1 - Cue 2 not - Scope 2 had it been so - Event 2 - Table 1: Example sentence annotated for negation following sem shared task 2012 format 2008a; Morante and Daelemans, 2009; Agarwal and Yu, 2010; Morante, 2010; Read et al., 2011), mostly on clinical reports. The reason is that most NLP research in the biomedical domain is interested in automatically extracting factual relations and pieces of information from unstructured data. Negation detection is important here because information that falls in the scope of a negation cue cannot be treated as facts. Chapman et al. (2001) proposed a rule-based algorithm called NegEx for determining whether a finding or disease mentioned within narrative medical reports is present or absent. The algorithm uses regular-expression"
S12-1043,D08-1075,0,0.14325,"Missing"
S12-1043,W10-3111,0,0.0212128,"omponents: a lexical scanner, lexer that uses regular expression rules to generate a finite state machine, and a parser. Morante (2008b) proposed a supervised approach for detecting negation cues and their scopes in biomedical text. Their system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another one that finds the full scope of these negation signals. Negation has been also studied in the context of sentiment analysis (Wilson et al., 2005; Jia et al., 2009; Councill et al., 2010; Heerschop et al., 2011; Hogenboom et al., 2011). Wiegand et al. (2010) surveyed the recent work on negation scope detection for sentiment analysis. Wilson et al. (2005) studied 329 the contextual features that affect text polarity. They used a machine learning approach in which negation is encoded using several features. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. Another feature accounts for a polar predicate having a negated subject. They also have disambiguation features to handle negation words that do not function as negation cues in certain contexts, e.g. not to mention and not jus"
S12-1043,H05-1044,0,0.0180474,"r rule based system called Negfinder that recognizes negation patterns in biomedical text. It consists of two components: a lexical scanner, lexer that uses regular expression rules to generate a finite state machine, and a parser. Morante (2008b) proposed a supervised approach for detecting negation cues and their scopes in biomedical text. Their system consists of two memory-based engines, one that decides if the tokens in a sentence are negation signals, and another one that finds the full scope of these negation signals. Negation has been also studied in the context of sentiment analysis (Wilson et al., 2005; Jia et al., 2009; Councill et al., 2010; Heerschop et al., 2011; Hogenboom et al., 2011). Wiegand et al. (2010) surveyed the recent work on negation scope detection for sentiment analysis. Wilson et al. (2005) studied 329 the contextual features that affect text polarity. They used a machine learning approach in which negation is encoded using several features. One feature checks whether a negation expression occurs in a fixed window of four words preceding the polar expression. Another feature accounts for a polar predicate having a negated subject. They also have disambiguation features to"
S12-1043,morante-2010-descriptive,0,\N,Missing
S12-1043,W11-0141,0,\N,Missing
saggion-etal-2002-developing,J93-1004,0,\N,Missing
saggion-etal-2002-developing,W97-0703,0,\N,Missing
saggion-etal-2002-developing,A00-2035,0,\N,Missing
saggion-etal-2002-developing,W00-0408,0,\N,Missing
saggion-etal-2002-developing,E99-1011,0,\N,Missing
saggion-etal-2002-developing,W97-0704,0,\N,Missing
saggion-etal-2002-developing,W00-0403,1,\N,Missing
saggion-etal-2002-developing,grover-etal-2000-lt,0,\N,Missing
saggion-etal-2002-developing,J96-2004,0,\N,Missing
saggion-etal-2002-developing,I05-2047,0,\N,Missing
saggion-etal-2002-developing,W01-0100,0,\N,Missing
W00-0403,W97-0706,0,0.0541186,"Missing"
W00-0403,J98-3005,1,\N,Missing
W00-1009,M95-1011,0,0.012718,"Missing"
W00-1009,P94-1002,0,0.0760336,"Missing"
W00-1009,J98-3005,1,0.74799,"Missing"
W00-1009,W97-0703,0,\N,Missing
W00-1009,P99-1071,0,\N,Missing
W00-1009,M95-1006,0,\N,Missing
W00-1110,E99-1011,0,\N,Missing
W00-1110,W00-0403,1,\N,Missing
W02-0404,A00-2024,0,\N,Missing
W02-0404,W00-1009,1,\N,Missing
W02-0404,W01-1313,0,\N,Missing
W02-0404,P99-1072,0,\N,Missing
W02-0404,W00-0403,1,\N,Missing
W02-0404,H01-1065,0,\N,Missing
W03-0502,W00-0403,1,0.802245,"Missing"
W03-0502,W01-0100,0,\N,Missing
W03-0503,W97-0703,0,\N,Missing
W03-0503,W97-0704,0,\N,Missing
W03-0503,W00-0403,1,\N,Missing
W03-0503,I05-2047,0,\N,Missing
W03-0503,W01-0100,0,\N,Missing
W04-3247,W00-1009,1,\N,Missing
W04-3247,W00-0403,1,\N,Missing
W08-0211,P97-1023,0,0.0520935,"Missing"
W09-1416,D07-1024,1,\N,Missing
W09-1416,W09-1401,0,\N,Missing
W09-3607,C08-1087,1,0.419914,"164:209 KCC08 labeled is the labeled dependency parser from (Koo et al., 2008); here we only evaluate the unlabeled accuracy. Figure 5: Sample citation summary 59 Figure 6: Snapshot of the citation summary for a paper of the author. We have been able to annotate 8,578 authors this way: 6,396 male and 2,182 female. The citation text that we have extracted for each paper is a good resource to generate summaries of the contributions of that paper. We have previously developed systems using clustering the similarity networks to generate short, and yet informative, summaries of individual papers (Qazvinian and Radev 2008), and more general scientific topics, such as Dependency Parsing, and Machine Translation (Radev et al. 2009) . 7 8 Downloads The following files can be downloaded: Text files of the paper: The raw text files of the papers after converting them from pdf to text is available for all papers. The files are named by the corresponding ACL ID. Metadata: This file contains all the metadata associated with each paper. The metadata associated with every paper consists of the paper id, title, year, venue. Citations: The paper citation network indicating which paper cites which other paper. Figure 7 incl"
W11-1107,P91-1023,0,0.384984,"is an important keyword for the second publication since a similar keyword (”‘Machine Translation”’) is an important keyword for similar publications. Let documents Di and Dj contain keywords Kik and Kjl . Then intuitively, the similarity between two documents should be jointly proportional to 2 The major contributions of the paper are given below, Motivation First, we explain how similarity learning and feature weight learning can mutually benefit from each other using an example. For example, consider the following three publications in the field of Machine Translation, (Brown et al., 1990; Gale and Church, 1991; Marcu and Wong, 2002) Clearly, all the papers belong to the field of Machine Translation but (Gale and Church, 1991) contains the phrase ”‘Machine Translation”’ only once in the entire text. However, we can learn to attribute some similarity between (Brown et al., 1990) and the second publication using the text in (Marcu and Wong, 2002). The keywords ”‘Bilingual Corpora”’ and ”‘Machine Translation”’ co-occur in the text in (Marcu and Wong, 2002) which makes the keywords themselves similar. Now we can attribute some similarity between the (Brown et al., 1990) and (Marcu 43 • The similarity be"
W11-1107,W02-1018,0,0.024769,"for the second publication since a similar keyword (”‘Machine Translation”’) is an important keyword for similar publications. Let documents Di and Dj contain keywords Kik and Kjl . Then intuitively, the similarity between two documents should be jointly proportional to 2 The major contributions of the paper are given below, Motivation First, we explain how similarity learning and feature weight learning can mutually benefit from each other using an example. For example, consider the following three publications in the field of Machine Translation, (Brown et al., 1990; Gale and Church, 1991; Marcu and Wong, 2002) Clearly, all the papers belong to the field of Machine Translation but (Gale and Church, 1991) contains the phrase ”‘Machine Translation”’ only once in the entire text. However, we can learn to attribute some similarity between (Brown et al., 1990) and the second publication using the text in (Marcu and Wong, 2002). The keywords ”‘Bilingual Corpora”’ and ”‘Machine Translation”’ co-occur in the text in (Marcu and Wong, 2002) which makes the keywords themselves similar. Now we can attribute some similarity between the (Brown et al., 1990) and (Marcu 43 • The similarity between keywords Kik and"
W11-1107,N04-3012,0,0.0579825,"ompute similarity between documents. However, suppose publications X and Y mention the keyword ”‘Machine Learning”’ only once. Although, they are mentioned only once in the text of the document, for the purposes of computing semantic similarity between the docu42 Proceedings of the TextGraphs-6 Workshop, pages 42–50, c Portland, Oregon, USA, 19-24 June 2011. 2011 Association for Computational Linguistics ments, it would be beneficial to give it a high keyword weight. A commonly used approach to estimate semantic similarity between documents is to use an external knowledge source like WordNet (Pedersen et al., 2004). However, these approaches are domain dependent and language dependent. If document similarity can not be estimated accurately using just the text, there have been approaches incorporating multiple sources of similarity like link similarity, authorship similarity between publications (Bach et al., 2004; Cortes et al., 2009). (Muthukrishnan et al., 2010) also uses multiple sources of similarity. In addition to improving similarity estimates between documents, it also improves similarity estimates between keywords. Co-clustering (Dhillon et al., 2003) based approaches are used to alleviate prob"
W11-1107,W09-3607,1,0.834015,"roved performance of our similarity measure in the context of clustering. We compare our similarity measure against the three similarity baselines mentioned above. We use a spectral graph clustering algorithm proposed in (Dhillon et al., 2007) to perform the clustering. We performed our experiments on three different data sets. The three data sets are explained below. • AAN Data: The ACL Anthology is a collection of papers from the Computational Linguistics journal as well as proceedings from ACL conferences and workshops and includes 15, 160 papers. To build the ACL Anthology Network (AAN), (Radev et al., 2009) manually performed some preprocessing tasks including parsing references and building the network metadata, the citation, and the author collaboration networks. The full AAN includes the raw text of all the papers in addition to full citation and collaboration networks. We chose a subset of papers in 3 topics (Ma0.9 0.8 0.85 0.75 0.8 0.7 0.75 0.65 0.7 0.65 0.6 0.6 0.55 0.55 Content Link Linear SC-MV Unified Unified-binary Unified-TFIDF 0.5 0.45 0 10 20 30 40 50 60 Content Link Linear SC-MV Unified Unified-binary Unified-TFIDF 0.5 0.45 70 10 15 20 (a) AAN 25 30 35 40 (b) Cornell 0.85 0.85 0.8"
W12-3201,P11-1051,1,0.933125,"identify the purpose of citation (i.e. the author’s reason for citing a given paper.) Nakov et al. (2004) use the term citances to refer to citing sentences. They explored several different uses of citances including the creation of training and testing data for semantic analysis, synonym set creation, database curation, summarization, and information retrieval. Other previous studies have used citing sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011a), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), and citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007). 1 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 1–12, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics Number of papers Number of authors Number of venues Number of paper citations Citation network diameter Collaboration network diameter Number of citing sentences 18,290 14,799 341 84,237 22 15 77,753 Comparison"
W12-3201,P11-3015,0,0.110209,"Missing"
W12-3201,P87-1022,0,0.635436,"onsiderations only. Grosz et al. (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse and the choices of referring expressions. Karttunen (1984) provides examples of feature structures in which a negation operator might be useful. Shieber (1985) proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley’s algorithm by using restriction to do top-down filtering. Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. Brennan et al. (1987) propose a default ordering on transitions which correlates with discourse coherence. Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Church and Hanks (1989) explored tile use of mutual information statistics in ranking co-occurrences within five-word windows. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Gale and Church (1991) extract pairs of anchor words, such as numbers, prope"
W12-3201,P79-1002,0,0.47051,"d the most cited papers in each year and then manually picked a citing sentence that cites a top cited and describes it contribution. It should be noted here that the citation counts we used for ranking papers reflect the number of incoming citations the paper received only from the venues included in AAN. To create the summary, we used citing sentences that has the reference to the cited paper in the beginning of the sentence. This is 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 Carbonell (1979) discusses inferring the meaning of new words. Weischedel and Black (1980) discuss techniques for interacting with the linguist/developer to identify insufficiencies in the grammar. Moore (1981) observed that determiners rarely have a direct correlation with the existential and universal quantifiers of first-order logic. Heidorn (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisions based on syntactic considerations only. Grosz et al. (1983) proposed the centering model which is concerned with the interactions betwee"
W12-3201,P05-1033,0,0.0373327,"arse trees and proposed a tree to string model for alignment. BLEU (Papineni et al., 2002) was devised to provide automatic evaluation of MT output. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative. Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Liu et al. (2006) experimented with tree-to-string translation models that utilize source side parse trees. Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. Huang (2008) improves the re-ranking work of Charniak and Johnson (2005) by re-ranking on packed forest, which could potentially incorporate exponential number of k-best list. Mintz et al. (2009) uses Freebase to provide distant supervision for relation extraction. Chiang"
W12-3201,P10-1146,0,0.0127497,"(2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Liu et al. (2006) experimented with tree-to-string translation models that utilize source side parse trees. Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. Huang (2008) improves the re-ranking work of Charniak and Johnson (2005) by re-ranking on packed forest, which could potentially incorporate exponential number of k-best list. Mintz et al. (2009) uses Freebase to provide distant supervision for relation extraction. Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical phrase-based system. Table 6: A citation-based summary of the important contributions published in ACL conference proceedings since 1979. The top cited paper in each year is found and one citation sentence is manually picked to represent it in the summary. 8 because such citing sentences are often high-quality, concise summaries of the cited work. Table 6 shows the summary of the ACL conference contributions that we created using citing sentences. 12 Conclusion We motivated and di"
W12-3201,P89-1010,0,0.13401,"proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley’s algorithm by using restriction to do top-down filtering. Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. Brennan et al. (1987) propose a default ordering on transitions which correlates with discourse coherence. Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Church and Hanks (1989) explored tile use of mutual information statistics in ranking co-occurrences within five-word windows. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Pereira and Schabes (1992) establish that evaluation according to the bracketing accuracy and evaluation according to perplexity or crossentropy are very different. Pereira et al. (1993) proposed a soft clustering scheme, in which membership"
W12-3201,A88-1019,0,0.842378,"Missing"
W12-3201,P96-1025,0,0.116683,"nd evaluation according to perplexity or crossentropy are very different. Pereira et al. (1993) proposed a soft clustering scheme, in which membership of a word in a class is probabilistic. Hearst (1994) presented two implemented segmentation algorithms based on term repetition, and compared the boundaries produced to the boundaries marked by at least 3 of 7 subjects, using information retrieval metrics. Yarowsky (1995) describes a ’semi-unsupervised’ approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Collins (1997)’s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaur"
W12-3201,P97-1003,0,0.171313,"in which membership of a word in a class is probabilistic. Hearst (1994) presented two implemented segmentation algorithms based on term repetition, and compared the boundaries produced to the boundaries marked by at least 3 of 7 subjects, using information retrieval metrics. Yarowsky (1995) describes a ’semi-unsupervised’ approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Collins (1997)’s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which"
W12-3201,P91-1023,0,0.316436,"nces E.g., in Bruno was the bully of the neighborhood. Brennan et al. (1987) propose a default ordering on transitions which correlates with discourse coherence. Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Church and Hanks (1989) explored tile use of mutual information statistics in ranking co-occurrences within five-word windows. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Pereira and Schabes (1992) establish that evaluation according to the bracketing accuracy and evaluation according to perplexity or crossentropy are very different. Pereira et al. (1993) proposed a soft clustering scheme, in which membership of a word in a class is probabilistic. Hearst (1994) presented two implemented segmentation algorithms based on term repetition, and compared the boundaries produced to the boundaries marked by at least 3 of 7 subjects, using inform"
W12-3201,P07-1094,0,0.0119614,"rious MT evaluation criteria in the training procedure of log-linear MT models. Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative. Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Liu et al. (2006) experimented with tree-to-string translation models that utilize source side parse trees. Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. Huang (2008) improves the re-ranking work of Charniak and Johnson (2005) by re-ranking on packed forest, which could potentially incorporate exponential number of k-best list. Mintz et al. (2009) uses Freebase to provide distant supervision for relation extraction. Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical phrase-based system. Table 6: A citation-based summary of the important contributions published in ACL conf"
W12-3201,P83-1007,0,0.818437,"1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 Carbonell (1979) discusses inferring the meaning of new words. Weischedel and Black (1980) discuss techniques for interacting with the linguist/developer to identify insufficiencies in the grammar. Moore (1981) observed that determiners rarely have a direct correlation with the existential and universal quantifiers of first-order logic. Heidorn (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisions based on syntactic considerations only. Grosz et al. (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse and the choices of referring expressions. Karttunen (1984) provides examples of feature structures in which a negation operator might be useful. Shieber (1985) proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley’s algorithm by using restriction to do top-down filtering. Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. Brennan et al. (1987) propose a default"
W12-3201,I11-1001,0,0.332818,"ted to receive more negative feedback. This probably can be explained by the emergence of better statistical models for part-of-speech (POS) tagging (e.g. Conditional Random Fields (Lafferty et al., 2001)) that outperformed Church’s approach. However, as indicated by the neutral citation curve, Church’s work continued to be cited as a classical pioneering research on the POS tagging task, but not as the state-of-the-art approach. Similar analysis can be applied to the change in citation purpose of Shieber (1985) as illustrated in Figure 1 2.4 Study the Dynamics of Research In recent research, Gupta and Manning (2011) conducted a study that tries to understand the dynamics of research in computational linguistics (CL). They analyzed the abstracts of CL papers included in the ACL Anthology Reference Corpus. They extracted the contributions, the domain of application, and the apply propose extend system Abstracts 1368 2856 425 5065 Citing Sentences 2534 3902 917 6633 word grammar model rules statistical syntax summarization Table 3: Comparison of trigger word occurrences in abstracts vs citing sentences. techniques and tools used in each paper. They combined this information with pre-calculated article-tocom"
W12-3201,P94-1002,0,0.3072,"0*49 = 2450 pairs. As a proof of concept, we annotated 25 papers from AAN using the annotation method described above. This data set consisted of 33,683 sentence pairs of which 8,704 are paraphrases. The idea of using citing sentences to create data sets for paraphrase extraction was initially suggested by Nakov et al. (2004) who proposed an algorithm that extracts paraphrases from citing sentences using rules based on automatic named entity annotation and the dependency paths between them. ofrec´ıa mejores resultados que, la introducci´on de vocabulario (Hearst, 1997) o las cadenas l´exicas (Hearst, 1994) y, por tanto, es la que se ha utilizado en la segunda fase del algoritmo. (11) English: This can be done either by analyzing the number 8 Scientific Article Classification of overlapping lexical chains (Hearst, 1994) or by building a Automatic classification of scientific articles is one of the important tasks for creating publication databases. A variety of machine learning algorithms have been proposed for this task. Many of these methods perform the classification based on the title, the abstract, or the full text of the article. Some other methods used citation links in addition to conten"
W12-3201,P82-1015,0,0.355726,"citing sentences that has the reference to the cited paper in the beginning of the sentence. This is 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 Carbonell (1979) discusses inferring the meaning of new words. Weischedel and Black (1980) discuss techniques for interacting with the linguist/developer to identify insufficiencies in the grammar. Moore (1981) observed that determiners rarely have a direct correlation with the existential and universal quantifiers of first-order logic. Heidorn (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisions based on syntactic considerations only. Grosz et al. (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse and the choices of referring expressions. Karttunen (1984) provides examples of feature structures in which a negation operator might be useful. Shieber (1985) proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley’s algorithm by using restriction to do top-down filterin"
W12-3201,P90-1034,0,0.363962,"p-down filtering. Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. Brennan et al. (1987) propose a default ordering on transitions which correlates with discourse coherence. Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Church and Hanks (1989) explored tile use of mutual information statistics in ranking co-occurrences within five-word windows. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Pereira and Schabes (1992) establish that evaluation according to the bracketing accuracy and evaluation according to perplexity or crossentropy are very different. Pereira et al. (1993) proposed a soft clustering scheme, in which membership of a word in a class is probabilistic. Hearst (1994) presented two implemented segmentation algorithms based on term"
W12-3201,P08-1067,0,0.0318276,"iers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative. Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Liu et al. (2006) experimented with tree-to-string translation models that utilize source side parse trees. Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. Huang (2008) improves the re-ranking work of Charniak and Johnson (2005) by re-ranking on packed forest, which could potentially incorporate exponential number of k-best list. Mintz et al. (2009) uses Freebase to provide distant supervision for relation extraction. Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical phrase-based system. Table 6: A citation-based summary of the important contributions published in ACL conference proceedings since 1979. The top cited paper in each year is found and one citation sentence is manually"
W12-3201,P86-1031,0,0.632164,"ovides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisions based on syntactic considerations only. Grosz et al. (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse and the choices of referring expressions. Karttunen (1984) provides examples of feature structures in which a negation operator might be useful. Shieber (1985) proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley’s algorithm by using restriction to do top-down filtering. Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. Brennan et al. (1987) propose a default ordering on transitions which correlates with discourse coherence. Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Church and Hanks (1989) explored tile use of mutual information statistics in ranking co-occurrences within five-word windows. Hindle (1990) classified nouns on"
W12-3201,P84-1008,0,0.668497,"scuss techniques for interacting with the linguist/developer to identify insufficiencies in the grammar. Moore (1981) observed that determiners rarely have a direct correlation with the existential and universal quantifiers of first-order logic. Heidorn (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisions based on syntactic considerations only. Grosz et al. (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse and the choices of referring expressions. Karttunen (1984) provides examples of feature structures in which a negation operator might be useful. Shieber (1985) proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley’s algorithm by using restriction to do top-down filtering. Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. Brennan et al. (1987) propose a default ordering on transitions which correlates with discourse coherence. Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for exam"
W12-3201,P98-2127,0,0.174172,"disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Collins (1997)’s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which are translations of each other. Och and Ney (2000) introduce a NULL-alignment capability to HMM alignment models. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. BLEU (Papineni et al., 2002) was devised to provide automatic eva"
W12-3201,P06-1077,0,0.0133625,"to provide automatic evaluation of MT output. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative. Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Liu et al. (2006) experimented with tree-to-string translation models that utilize source side parse trees. Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. Huang (2008) improves the re-ranking work of Charniak and Johnson (2005) by re-ranking on packed forest, which could potentially incorporate exponential number of k-best list. Mintz et al. (2009) uses Freebase to provide distant supervision for relation extraction. Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a"
W12-3201,P08-1093,0,0.0291061,"ba et al., 2000) they use their citing area identification algorithm to identify the purpose of citation (i.e. the author’s reason for citing a given paper.) Nakov et al. (2004) use the term citances to refer to citing sentences. They explored several different uses of citances including the creation of training and testing data for semantic analysis, synonym set creation, database curation, summarization, and information retrieval. Other previous studies have used citing sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011a), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), and citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007). 1 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 1–12, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics Number of papers Number of authors Number of venues Number of paper citations Citation network diameter Collaboration network diamet"
W12-3201,P09-1113,0,0.00813247,"the second classifier to classify the subjective texts into positive and negative. Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Liu et al. (2006) experimented with tree-to-string translation models that utilize source side parse trees. Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. Huang (2008) improves the re-ranking work of Charniak and Johnson (2005) by re-ranking on packed forest, which could potentially incorporate exponential number of k-best list. Mintz et al. (2009) uses Freebase to provide distant supervision for relation extraction. Chiang (2010) proposes a method for learning to translate with both source and target syntax in the framework of a hierarchical phrase-based system. Table 6: A citation-based summary of the important contributions published in ACL conference proceedings since 1979. The top cited paper in each year is found and one citation sentence is manually picked to represent it in the summary. 8 because such citing sentences are often high-quality, concise summaries of the cited work. Table 6 shows the summary of the ACL conference con"
W12-3201,N09-1066,1,0.854174,"paper.) Nakov et al. (2004) use the term citances to refer to citing sentences. They explored several different uses of citances including the creation of training and testing data for semantic analysis, synonym set creation, database curation, summarization, and information retrieval. Other previous studies have used citing sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011a), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), and citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007). 1 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 1–12, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics Number of papers Number of authors Number of venues Number of paper citations Citation network diameter Collaboration network diameter Number of citing sentences 18,290 14,799 341 84,237 22 15 77,753 Comparison Basis Use Description Weakness Table 2: Annotation scheme for citation pur"
W12-3201,P81-1028,0,0.588596,"king papers reflect the number of incoming citations the paper received only from the venues included in AAN. To create the summary, we used citing sentences that has the reference to the cited paper in the beginning of the sentence. This is 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 Carbonell (1979) discusses inferring the meaning of new words. Weischedel and Black (1980) discuss techniques for interacting with the linguist/developer to identify insufficiencies in the grammar. Moore (1981) observed that determiners rarely have a direct correlation with the existential and universal quantifiers of first-order logic. Heidorn (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisions based on syntactic considerations only. Grosz et al. (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse and the choices of referring expressions. Karttunen (1984) provides examples of feature structures in which a negation operator might be useful. Shieber (1985)"
W12-3201,P00-1056,0,0.0116211,"on and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which are translations of each other. Och and Ney (2000) introduce a NULL-alignment capability to HMM alignment models. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. BLEU (Papineni et al., 2002) was devised to provide automatic evaluation of MT output. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier se"
W12-3201,P03-1021,0,0.00486782,"arity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which are translations of each other. Och and Ney (2000) introduce a NULL-alignment capability to HMM alignment models. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. BLEU (Papineni et al., 2002) was devised to provide automatic evaluation of MT output. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative. Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Liu et al. (2006) experimented with tree-to-string trans"
W12-3201,P04-1035,0,0.0172835,"hat in any language there is a correlation between the cooccurrences of words which are translations of each other. Och and Ney (2000) introduce a NULL-alignment capability to HMM alignment models. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. BLEU (Papineni et al., 2002) was devised to provide automatic evaluation of MT output. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative. Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Liu et al. (2006) experimented with tree-to-string translation models that utilize source side parse trees. Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet p"
W12-3201,P02-1040,0,0.0859422,") and, according to Dan Bikels web page, Arabic. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which are translations of each other. Och and Ney (2000) introduce a NULL-alignment capability to HMM alignment models. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. BLEU (Papineni et al., 2002) was devised to provide automatic evaluation of MT output. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they used the second classifier to classify the subjective texts into positive and negative. Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine"
W12-3201,P92-1017,0,0.0966488,"se coherence. Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Church and Hanks (1989) explored tile use of mutual information statistics in ranking co-occurrences within five-word windows. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Pereira and Schabes (1992) establish that evaluation according to the bracketing accuracy and evaluation according to perplexity or crossentropy are very different. Pereira et al. (1993) proposed a soft clustering scheme, in which membership of a word in a class is probabilistic. Hearst (1994) presented two implemented segmentation algorithms based on term repetition, and compared the boundaries produced to the boundaries marked by at least 3 of 7 subjects, using information retrieval metrics. Yarowsky (1995) describes a ’semi-unsupervised’ approach to the problem of sense disambiguation of words, also using a set of i"
W12-3201,P93-1024,0,0.0273124,"itiative, while answers and acknowledgements do not. Church and Hanks (1989) explored tile use of mutual information statistics in ranking co-occurrences within five-word windows. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Pereira and Schabes (1992) establish that evaluation according to the bracketing accuracy and evaluation according to perplexity or crossentropy are very different. Pereira et al. (1993) proposed a soft clustering scheme, in which membership of a word in a class is probabilistic. Hearst (1994) presented two implemented segmentation algorithms based on term repetition, and compared the boundaries produced to the boundaries marked by at least 3 of 7 subjects, using information retrieval metrics. Yarowsky (1995) describes a ’semi-unsupervised’ approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies be"
W12-3201,C08-1087,1,0.88097,"a given reference. In (Nanba et al., 2000) they use their citing area identification algorithm to identify the purpose of citation (i.e. the author’s reason for citing a given paper.) Nakov et al. (2004) use the term citances to refer to citing sentences. They explored several different uses of citances including the creation of training and testing data for semantic analysis, synonym set creation, database curation, summarization, and information retrieval. Other previous studies have used citing sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011a), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), and citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007). 1 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 1–12, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics Number of papers Number of authors Number of venues Number of paper citations Citation network diameter Collabor"
W12-3201,P10-1057,1,0.886115,"identification algorithm to identify the purpose of citation (i.e. the author’s reason for citing a given paper.) Nakov et al. (2004) use the term citances to refer to citing sentences. They explored several different uses of citances including the creation of training and testing data for semantic analysis, synonym set creation, database curation, summarization, and information retrieval. Other previous studies have used citing sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011a), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), and citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007). 1 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 1–12, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics Number of papers Number of authors Number of venues Number of paper citations Citation network diameter Collaboration network diameter Number of citing sentences 18,290 14,799 341 84,"
W12-3201,C10-1101,1,0.943104,"selected sentences that cite it in AAN. We notice that citing sentences contain additional facts that are not in the abstract, not only ones that summarize the paper contributions, but also those that criticize it (e.g., the last citing sentence in the Table). Previous work has explored this research direction. Qazvinian and Radev (2008) proposed a method for summarizing scientific articles by building a similarity network of the sentences that cite it, and then applying network analysis techniques to find a set of sentences that covers as much of the paper facts as possible. Qazvinian et al. (2010) proposed another summarization method that first extracts a number of important key phrases from the set of citing sentences, and then finds the best subset of sentences that covers as many key phrases as possible. These works focused on analyzing the citing sentences and selecting a representative subset that covers the different aspects of the summarized article. In recent work, Abu-Jbara and Radev (2011b) raised the issue of coherence and readability in summaries generated from citing sentences. They added a preprocessing and postprocessing steps to the summarization pipeline. In the prepr"
W12-3201,W09-3607,1,0.646797,"d contributions. We also propose and motivate several different uses and applications of citing sentences. 1 Introduction The ACL Anthology2 is one of the most successful initiatives of the Association for Computational Linguistics (ACL). It was initiated by Steven Bird in 2001 and is now maintained by Min-Yen Kan. It includes all papers published by ACL and related organizations as well as the Computational Linguistics journal over a period of four decades. The ACL Anthology Network (AAN) is another successful initiative built on top of the ACL Anthology. It was started in 2007 by our group (Radev et al., 2009) at the University of Michigan. AAN provides citation and collaboration networks of the articles included in the ACL Anthology (excluding book reviews). AAN also includes rankings of papers and authors based on their centrality statistics 1 2 http://clair.si.umich.edu/anthology/ http://www.aclweb.org/anthology-new/ in the citation and collaboration networks. It also includes the citing sentences associated with each citation link. These sentences were extracted automatically using pattern matching and then cleaned manually. Table 1 shows some statistics of the current release of AAN. The text"
W12-3201,P99-1067,0,0.0218426,"rser which is based on probabilities of dependencies between head-words in the parse tree. Collins (1997)’s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which are translations of each other. Och and Ney (2000) introduce a NULL-alignment capability to HMM alignment models. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. BLEU (Papineni et al., 2002) was devised to provide automatic evaluation of MT output. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear"
W12-3201,P99-1068,0,0.626374,"cantly in the 2000s. 3 Scientific Literature Summarization Using Citing Sentences The fact that citing sentences cover different aspects of the cited paper and highlight its most important contributions motivates the idea of using citing sentences to summarize research. The comparison that Elkiss et al. (2008) performed between abstracts and citing sentences suggests that a summary generated from citing sentences will be different and probably more concise and informative than the paper abstract or a summary generated from the full text of the paper. For example, Table 5 shows the abstract of Resnik (1999) and 5 selected sentences that cite it in AAN. We notice that citing sentences contain additional facts that are not in the abstract, not only ones that summarize the paper contributions, but also those that criticize it (e.g., the last citing sentence in the Table). Previous work has explored this research direction. Qazvinian and Radev (2008) proposed a method for summarizing scientific articles by building a similarity network of the sentences that cite it, and then applying network analysis techniques to find a set of sentences that covers as much of the paper facts as possible. Qazvinian"
W12-3201,P85-1018,0,0.303884,"ging received significant positive feedback during the 1990s and until early 2000s before it started to receive more negative feedback. This probably can be explained by the emergence of better statistical models for part-of-speech (POS) tagging (e.g. Conditional Random Fields (Lafferty et al., 2001)) that outperformed Church’s approach. However, as indicated by the neutral citation curve, Church’s work continued to be cited as a classical pioneering research on the POS tagging task, but not as the state-of-the-art approach. Similar analysis can be applied to the change in citation purpose of Shieber (1985) as illustrated in Figure 1 2.4 Study the Dynamics of Research In recent research, Gupta and Manning (2011) conducted a study that tries to understand the dynamics of research in computational linguistics (CL). They analyzed the abstracts of CL papers included in the ACL Anthology Reference Corpus. They extracted the contributions, the domain of application, and the apply propose extend system Abstracts 1368 2856 425 5065 Citing Sentences 2534 3902 917 6633 word grammar model rules statistical syntax summarization Table 3: Comparison of trigger word occurrences in abstracts vs citing sentences"
W12-3201,N07-1040,0,0.0258675,"different uses of citances including the creation of training and testing data for semantic analysis, synonym set creation, database curation, summarization, and information retrieval. Other previous studies have used citing sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011a), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), and citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007). 1 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 1–12, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics Number of papers Number of authors Number of venues Number of paper citations Citation network diameter Collaboration network diameter Number of citing sentences 18,290 14,799 341 84,237 22 15 77,753 Comparison Basis Use Description Weakness Table 2: Annotation scheme for citation purpose Table 1: Statistics of AAN 2011 release 2.1 In this paper, we focus on the usefulness of the citing sent"
W12-3201,W06-1613,0,0.472057,"hey explored several different uses of citances including the creation of training and testing data for semantic analysis, synonym set creation, database curation, summarization, and information retrieval. Other previous studies have used citing sentences in various applications such as: scientific paper summarization (Elkiss et al., 2008; Qazvinian and Radev, 2008; Mei and Zhai, 2008; Qazvinian et al., 2010; Qazvinian and Radev, 2010; Abu-Jbara and Radev, 2011a), automatic survey generation (Nanba et al., 2000; Mohammad et al., 2009), and citation function classification (Nanba et al., 2000; Teufel et al., 2006; Siddharthan and Teufel, 2007; Teufel, 2007). 1 Proceedings of the ACL-2012 Special Workshop on Rediscovering 50 Years of Discoveries, pages 1–12, c Jeju, Republic of Korea, 10 July 2012. 2012 Association for Computational Linguistics Number of papers Number of authors Number of venues Number of paper citations Citation network diameter Collaboration network diameter Number of citing sentences 18,290 14,799 341 84,237 22 15 77,753 Comparison Basis Use Description Weakness Table 2: Annotation scheme for citation purpose Table 1: Statistics of AAN 2011 release 2.1 In this paper, we focus on the"
W12-3201,P80-1025,0,0.757343,"citing sentence that cites a top cited and describes it contribution. It should be noted here that the citation counts we used for ranking papers reflect the number of incoming citations the paper received only from the venues included in AAN. To create the summary, we used citing sentences that has the reference to the cited paper in the beginning of the sentence. This is 1979 1980 1981 1982 1983 1984 1985 1986 1987 1988 1989 1990 1991 1992 1993 1994 1995 1996 1997 1998 1999 2000 2001 2002 2003 2004 2005 2006 2007 2008 2009 2010 Carbonell (1979) discusses inferring the meaning of new words. Weischedel and Black (1980) discuss techniques for interacting with the linguist/developer to identify insufficiencies in the grammar. Moore (1981) observed that determiners rarely have a direct correlation with the existential and universal quantifiers of first-order logic. Heidorn (1982) provides a good summary of early work in weight-based analysis, as well as a weight-oriented approach to attachment decisions based on syntactic considerations only. Grosz et al. (1983) proposed the centering model which is concerned with the interactions between the local coherence of discourse and the choices of referring expression"
W12-3201,P88-1015,0,0.0502551,"ns between the local coherence of discourse and the choices of referring expressions. Karttunen (1984) provides examples of feature structures in which a negation operator might be useful. Shieber (1985) proposes a more efficient approach to gaps in the PATR-II formalism, extending Earley’s algorithm by using restriction to do top-down filtering. Kameyama (1986) proposed a fourth transition type, Center Establishment (EST), for utterances E.g., in Bruno was the bully of the neighborhood. Brennan et al. (1987) propose a default ordering on transitions which correlates with discourse coherence. Whittaker and Stenton (1988) proposed rules for tracking initiative based on utterance types; for example, statements, proposals, and questions show initiative, while answers and acknowledgements do not. Church and Hanks (1989) explored tile use of mutual information statistics in ranking co-occurrences within five-word windows. Hindle (1990) classified nouns on the basis of co-occurring patterns of subjectverb and verb-object pairs. Gale and Church (1991) extract pairs of anchor words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Pereira and Schabes (1992) establish that"
W12-3201,P01-1067,0,0.0844022,"s: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chinese (Bikel, 2002) and, according to Dan Bikels web page, Arabic. Lin (1998) proposed a word similarity measure based on the distributional pattern of words which allows to construct a thesaurus using a parsed corpus. Rapp (1999) proposed that in any language there is a correlation between the cooccurrences of words which are translations of each other. Och and Ney (2000) introduce a NULL-alignment capability to HMM alignment models. Yamada and Knight (2001) used a statistical parser trained using a Treebank in the source language to produce parse trees and proposed a tree to string model for alignment. BLEU (Papineni et al., 2002) was devised to provide automatic evaluation of MT output. Och (2003) developed a training procedure that incorporates various MT evaluation criteria in the training procedure of log-linear MT models. Pang and Lee (2004) applied two different classifiers to perform sentiment annotation in two sequential steps: the first classifier separated subjective (sentiment-laden) texts from objective (neutral) ones and then they u"
W12-3201,P95-1026,0,0.0561577,"words, such as numbers, proper nouns (organization, person, title), dates, and monetary information. Pereira and Schabes (1992) establish that evaluation according to the bracketing accuracy and evaluation according to perplexity or crossentropy are very different. Pereira et al. (1993) proposed a soft clustering scheme, in which membership of a word in a class is probabilistic. Hearst (1994) presented two implemented segmentation algorithms based on term repetition, and compared the boundaries produced to the boundaries marked by at least 3 of 7 subjects, using information retrieval metrics. Yarowsky (1995) describes a ’semi-unsupervised’ approach to the problem of sense disambiguation of words, also using a set of initial seeds, in this case a few high quality sense annotations. Collins (1996) proposed a statistical parser which is based on probabilities of dependencies between head-words in the parse tree. Collins (1997)’s parser and its re-implementation and extension by Bikel (2002) have by now been applied to a variety of languages: English (Collins, 1999), Czech (Collins et al. , 1999), German (Dubey and Keller, 2003), Spanish (Cowan and Collins, 2005), French (Arun and Keller, 2005), Chin"
W12-3201,J93-1004,0,\N,Missing
W12-3201,J90-1003,0,\N,Missing
W12-3201,C98-2122,0,\N,Missing
W12-4102,W10-0731,0,0.0440135,"Missing"
W12-4102,banea-etal-2008-bootstrapping,0,0.0552789,"n and Radev (2010) use a random walk model defined over a word relatedness graph to classify words as either positive or negative. Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: subjectivity of a phrase or word is analyzed regardless of the context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008), or within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. Our method is not restricted to phrases or words, rather it generalizes this to identifying the polarity of an"
W12-4102,D09-1030,0,0.0812356,"Missing"
W12-4102,P10-1015,0,0.104984,"ures to analyze discourse structure information. negative edges into consideration (Yang et al., 2007; Doreian and Mrvar, 2009). All this work has been limited to analyzing a handful of datasets for which an explicit notion of both positive and negative relations exists. Our work goes beyond this limitation by leveraging the power of natural language processing to automate the discovery of signed social networks using the text embedded in the network. 2.3 3 Extracting Social Networks from Text Little work has been done on the front of extracting social relations between individuals from text. Elson et al. (2010) present a method for extracting social networks from nineteenth-century British novels and serials. They link two characters based on whether they are in conversation or not. McCallum et al. (2007) explored the use of structured data such as email headers for social network construction. Gruzd and Hyrthonthwaite (2008) explored the use of post text in discussions to study interaction patterns in e-learning communities. Our work is related to this line of research because we employ natural language processing techniques to reveal embedded social structures. Despite similarities, our work is un"
W12-4102,P10-1041,1,0.892717,"titude from one individual toward another makes our work related to a huge body of work on sentiment analysis. One such line of research is the well-studied problem of identifying the of individual words. In previous work, Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them in a large corpus. Turney and Littman (2003) used statistical measures to find the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) used the spin model to extract word semantic orientation. Finally, Hassan and Radev (2010) use a random walk model defined over a word relatedness graph to classify words as either positive or negative. Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: subjectivity of a phrase or word is analyzed regardless of the context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008"
W12-4102,D10-1121,1,0.938055,"is is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: subjectivity of a phrase or word is analyzed regardless of the context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008), or within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. Our method is not restricted to phrases or words, rather it generalizes this to identifying the polarity of an interaction between two individuals based on several posts they exchange. 2.2 Mining Online Discussions Our use of discussion threads as a source of d"
W12-4102,P97-1023,0,0.23794,"plain our approach in Section 3. Section 4 describes our dataset. Results and discussion are presented in Section 5. We present a possible application for the proposed approach in Section 6. We conclude in Section 7. 2 Related Work In this section, we survey several lines of research that are related to our work. 7 2.1 Mining Sentiment from Text Our general goal of mining attitude from one individual toward another makes our work related to a huge body of work on sentiment analysis. One such line of research is the well-studied problem of identifying the of individual words. In previous work, Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them in a large corpus. Turney and Littman (2003) used statistical measures to find the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) used the spin model to extract word semantic orientation. Finally, Hassan and Radev (2010) use a random walk model defined over a word relatedness graph to classify words as either positive or negative. Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The obj"
W12-4102,C00-1044,0,0.762418,"mantic orientation. Finally, Hassan and Radev (2010) use a random walk model defined over a word relatedness graph to classify words as either positive or negative. Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: subjectivity of a phrase or word is analyzed regardless of the context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008), or within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. Our method is not restricted to phrases or words, rather it generalizes this to identifyi"
W12-4102,P03-1054,0,0.0230232,"Missing"
W12-4102,H05-1043,0,0.213934,"egative. Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: subjectivity of a phrase or word is analyzed regardless of the context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008), or within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. Our method is not restricted to phrases or words, rather it generalizes this to identifying the polarity of an interaction between two individuals based on several posts they exchange. 2.2 Mining Online Discussions Our use of discussion th"
W12-4102,W03-1014,0,0.538944,"efined over a word relatedness graph to classify words as either positive or negative. Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: subjectivity of a phrase or word is analyzed regardless of the context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008), or within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. Our method is not restricted to phrases or words, rather it generalizes this to identifying the polarity of an interaction between two individuals based on s"
W12-4102,P05-1017,0,0.0728181,"at are related to our work. 7 2.1 Mining Sentiment from Text Our general goal of mining attitude from one individual toward another makes our work related to a huge body of work on sentiment analysis. One such line of research is the well-studied problem of identifying the of individual words. In previous work, Hatzivassiloglou and McKeown (1997) proposed a method to identify the polarity of adjectives based on conjunctions linking them in a large corpus. Turney and Littman (2003) used statistical measures to find the association between a given word and a set of positive/negative seed words. Takamura et al. (2005) used the spin model to extract word semantic orientation. Finally, Hassan and Radev (2010) use a random walk model defined over a word relatedness graph to classify words as either positive or negative. Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: subjectivity of a phrase or word is analyzed r"
W12-4102,W01-1626,0,0.0384897,"s with attitude. Finally, we build a network connecting participants based on their interactions. We use the predictions we made both at the word and sentence levels to associate a sign to every edge. 3.1 Identified Positive/Negative Words The first step toward identifying attitude is to identify words with positive/negative semantic orientation. The semantic orientation or polarity of a word indicates the direction the word deviates from the norm (Lehrer, 1974). Past work has demonstrated that polarized words are very good indicators of subjective sentences (Hatzivassiloglou and Wiebe, 2000; Wiebe et al., 2001). We use a Random Walk based method to identify the semantic orientation of words (Hassan and Radev, 2010). We construct a graph where each node represents a word/partof-speech pair. We connect nodes based on synonyms, hypernyms, and similar-to relations from WordNet (Miller, 1995). For words that do not appear in WordNet, we use distributional similarity (Lee, 1999) as a proxy for word relatedness. We use a list of words with known polarity (Stone et al., 1966) to label some of the nodes in the graph. We then define a random walk model where the set of nodes correspond to the state space, and"
W12-4102,H05-2018,0,0.0365532,"(Norris, 1997) from any word with unknown polarity to the set of positive seeds and the set of negative seeds. If the absolute difference of the two mean hitting times is below a certain threshold, the word is classified as neutral. Otherwise, it is labeled with the class that has the smallest mean hitting time. 3.2 Identifying Attitude from Text The first step toward identifying attitude is to identify words with positive/negative semantic orientation. The semantic orientation or polarity of a word indicates the direction the word deviates from the norm (Lehrer, 1974). We use OpinionFinder (Wilson et al., 2005a) to identify words with positive or negative semantic orientation. The polarity of a word is also affected by the context where the word appears. For example, a positive word that appears in a negated context should have a negative polarity. Other polarized words sometimes appear as neutral words in some contexts. Hence, we use the method described in (Wilson et al., 2005b) to identify the contextual polarity of words given their isolated polarity. A large set of features is used for that purpose 9 including words, sentences, structure, and other features. Our overall objective is to find th"
W12-4102,H05-1044,0,0.119794,"(Norris, 1997) from any word with unknown polarity to the set of positive seeds and the set of negative seeds. If the absolute difference of the two mean hitting times is below a certain threshold, the word is classified as neutral. Otherwise, it is labeled with the class that has the smallest mean hitting time. 3.2 Identifying Attitude from Text The first step toward identifying attitude is to identify words with positive/negative semantic orientation. The semantic orientation or polarity of a word indicates the direction the word deviates from the norm (Lehrer, 1974). We use OpinionFinder (Wilson et al., 2005a) to identify words with positive or negative semantic orientation. The polarity of a word is also affected by the context where the word appears. For example, a positive word that appears in a negated context should have a negative polarity. Other polarized words sometimes appear as neutral words in some contexts. Hence, we use the method described in (Wilson et al., 2005b) to identify the contextual polarity of words given their isolated polarity. A large set of features is used for that purpose 9 including words, sentences, structure, and other features. Our overall objective is to find th"
W12-4102,W03-1017,0,0.0872073,"edness graph to classify words as either positive or negative. Subjectivity analysis is yet another research line that is closely related to our general goal of mining attitude. The objective of subjectivity analysis is to identify text that presents opinion as opposed to objective text that presents factual information (Wiebe, 2000). Prior work on subjectivity analysis mainly consists of two main categories: subjectivity of a phrase or word is analyzed regardless of the context (Wiebe, 2000; Hatzivassiloglou and Wiebe, 2000; Banea et al., 2008), or within its context (Riloff and Wiebe, 2003; Yu and Hatzivassiloglou, 2003; Nasukawa and Yi, 2003; Popescu and Etzioni, 2005). Hassan et al. (2010) presents a method for identifying sentences that display an attitude from the text writer toward the text recipient. Our work is different from subjectivity analysis because we are not only interested in discriminating between opinions and facts. Rather, we are interested in identifying the polarity of interactions between individuals. Our method is not restricted to phrases or words, rather it generalizes this to identifying the polarity of an interaction between two individuals based on several posts they exchange. 2.2"
W13-1710,P03-1054,0,0.0385907,"he entire training data, i.e., every n-gram occurring in the training data is used as a feature. Similarly, we create features with 1,2 and 3-grams of words. Each word n-gram is used as a separate feature. We explore both binary features for each character or word n-gram, as well as normalized count features. Part-Of-Speech N-grams Several investigations, for example those conducted by Kochmar (2011) and Wong and Dras (2011), have found that part-ofspeech tags can be useful for NLI. Therefore we include part-of-speech (POS) n-grams as features. We parse the sentences with the Stanford Parser (Klein and Manning, 2003) and extract the POS tags. We use binary features describing the presence or absence of POS bigrams in a document, as well as numerical features describing their relative frequency in a document. Function Words Koppel et al. (2005) found that function words can help identify someone’s native language. To this end, we include a categorical feature for the presence of function words that are included in list of 321 function words. Use of punctuation Based on our experience with speakers of native languages, as well as Kochmar’s (2011) observations of written English produced by Germanic and Roma"
W13-1710,W13-1706,0,0.0604496,"nguage and sound transfer, which are how people apply knowledge of their native language, and its phonology and orthography, respectively, to a second language. For example, Tsur and Rappoport (2007) found that character bigrams are quite useful for NLI, which led them to suggest that second language learners’ word choice may in part be driven by their native languages. Analysis of such language and sound translation patterns might be useful in understanding the process of language acquisition in humans. 2 Previous Work The work presented in this paper was done as part of the NLI shared task (Tetreault et al., 2013), which is the first time this problem has been the subject of a shared task. However, several researchers have investigated NLI and similar problems. Authorship attribution, a related problem, has been well studied in the literature, starting from the seminal work on disputed Federalist Papers by Mosteller and Wallace (1964). The goal of authorship attribution is to assign a text to one author from a candidate set 82 Proceedings of the Eighth Workshop on Innovative Use of NLP for Building Educational Applications, pages 82–88, c Atlanta, Georgia, June 13 2013. 2013 Association for Computation"
W13-1710,W07-0602,0,0.451935,"any possible applications for an NLI system, as noted by Kochmar (2011): finding the origins of anonymous text; error correction in various tasks including speech recognition, part-ofspeech tagging, and parsing; and in the field of second language acquisition for identifying learner difficulties. We are most interested in statistical approaches to this problem because it may point towards fruitful avenues of research in language and sound transfer, which are how people apply knowledge of their native language, and its phonology and orthography, respectively, to a second language. For example, Tsur and Rappoport (2007) found that character bigrams are quite useful for NLI, which led them to suggest that second language learners’ word choice may in part be driven by their native languages. Analysis of such language and sound translation patterns might be useful in understanding the process of language acquisition in humans. 2 Previous Work The work presented in this paper was done as part of the NLI shared task (Tetreault et al., 2013), which is the first time this problem has been the subject of a shared task. However, several researchers have investigated NLI and similar problems. Authorship attribution, a"
W13-1710,U09-1008,0,0.0188032,"roved feature normalization. In this paper, we present the features used in our system, describe our experiments and provide an analysis of our results. 1 Introduction The task of Native Language Identification (NLI) is the task of identifying the native language of a writer or a speaker by analyzing their writing in English. Previous work in this area shows that there are several linguistic cues that can be used to do such identification. Based on their native language, different speakers tend to make different kinds of errors pertaining to spelling, punctuation, and grammar (Garfield, 1964; Wong and Dras, 2009; Kochmar, 2011). We describe the complete set of features we considered in Section 4. We evaluate different combinations of these features, and different ways of normalizing them in Section 5. There are many possible applications for an NLI system, as noted by Kochmar (2011): finding the origins of anonymous text; error correction in various tasks including speech recognition, part-ofspeech tagging, and parsing; and in the field of second language acquisition for identifying learner difficulties. We are most interested in statistical approaches to this problem because it may point towards fru"
W13-1710,D11-1148,0,0.0164446,"atures using both characters and word N-grams. For characters, we consider 2,3 and 4-grams, with padding characters at the beginning and end of each sentence. The features are generated over the entire training data, i.e., every n-gram occurring in the training data is used as a feature. Similarly, we create features with 1,2 and 3-grams of words. Each word n-gram is used as a separate feature. We explore both binary features for each character or word n-gram, as well as normalized count features. Part-Of-Speech N-grams Several investigations, for example those conducted by Kochmar (2011) and Wong and Dras (2011), have found that part-ofspeech tags can be useful for NLI. Therefore we include part-of-speech (POS) n-grams as features. We parse the sentences with the Stanford Parser (Klein and Manning, 2003) and extract the POS tags. We use binary features describing the presence or absence of POS bigrams in a document, as well as numerical features describing their relative frequency in a document. Function Words Koppel et al. (2005) found that function words can help identify someone’s native language. To this end, we include a categorical feature for the presence of function words that are included in"
W13-1710,U11-1015,0,0.0248657,"Missing"
W13-1710,D12-1064,0,0.0146974,"ures and showed that adding these features improved the accuracy significantly. They also investigated classification models based on LDA (Wong et al., 2011), but did not find them 83 to be useful overall. They did, however, notice that some of the topics were capturing information that would be useful for identifying particular native languages. They also proposed the use of adaptor grammars (Johnson et al., 2007), which are a generalization of probabilistic context-free grammars, to capture collocational pairings. In a later paper, Wong et al. explored the use of adapter grammars in detail (Wong et al., 2012) and showed that an extension of adaptor grammars to discover collocations beyond lexical words can produce features useful for the NLI task. 3 Dataset The experiments for this paper were performed using the TOEFL11 dataset (Blanchard et al., 2013) provided as part of the shared task. The dataset contains essays written in English from native speakers of 11 languages (Arabic, Chinese, French, German, Hindi, Italian, Japanese, Korean, Spanish, Telugu, and Turkish). The corpus contains 12,099 essays per language sampled evenly from 8 prompts or topics. This dataset was designed specifically to s"
W13-3403,W08-0211,1,0.886205,"Missing"
W14-5317,W12-2108,0,0.0438718,"re presented. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 146 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146–154, Dublin, Ireland, August 23 2014. 2 Related Work Recent directions in language identification have included finer-grained language identification (King and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs (Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely related languages. Language identification for closely related languages has been considered by several researchers, though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguishing Croatian from Serbian and Slovenian is explored by Ljubeˇsi´c et al. (2007), who used a list of most frequent words along with a Markov model and a word blacklist, a list of words that are not allowed to appear in a certain language. A similar approach was later used by Tiedemann and Ljubeˇs"
W14-5317,Y08-1042,0,0.315839,"ssifier with large amounts of training data. They also find training on parallel data to be important, as it allows the machine learning methods to pick out features relating to the differences between the languages themselves, rather than learning differences in domain. Zampieri et al. consider classes that would be most often classified as language varieties rather than separate languages or dialects (Zampieri et al., 2012; Zampieri and Gebrekidan, 2012; Zampieri et al., 2013). A similar problem of distinguishing among Chinese text from mainland China, Singapore, and Taiwan is considered by Huang and Lee (2008) who approach the problem by computing similarity between a document and a corpus according to the size of the intersection between the sets of types in each. A similar, but somewhat different problem of automatically identifying lexical variants between closely related languages is considered in (Peirsman et al., 2010). Using distributional methods, they are able to identify Netherlandic Dutch synonyms for words from Belgian Dutch. 3 Data This paper’s training data and evaluation data both come from the DSL corpus collection (DSLCC) (Tan et al., 2014). We use the training section of this data"
W14-5317,N13-1131,1,0.814219,"hods, its format eschews the standard “Results” section, instead providing comparisons of methods as they are presented. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 146 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146–154, Dublin, Ireland, August 23 2014. 2 Related Work Recent directions in language identification have included finer-grained language identification (King and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs (Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely related languages. Language identification for closely related languages has been considered by several researchers, though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguishing Croatian from Serbian and Slovenian is explored by Ljubeˇsi´c et al. (2007), who used a list of most frequent words along with a Markov model and a word blacklist, a list of words that"
W14-5317,Q14-1003,0,0.0414156,"section, instead providing comparisons of methods as they are presented. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 146 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146–154, Dublin, Ireland, August 23 2014. 2 Related Work Recent directions in language identification have included finer-grained language identification (King and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs (Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely related languages. Language identification for closely related languages has been considered by several researchers, though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguishing Croatian from Serbian and Slovenian is explored by Ljubeˇsi´c et al. (2007), who used a list of most frequent words along with a Markov model and a word blacklist, a list of words that are not allowed to appear in a certain langua"
W14-5317,D13-1084,0,0.0421018,"ws the standard “Results” section, instead providing comparisons of methods as they are presented. This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings footer are added by the organizers. License details: http://creativecommons.org/licenses/by/4.0/ 146 Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 146–154, Dublin, Ireland, August 23 2014. 2 Related Work Recent directions in language identification have included finer-grained language identification (King and Abney, 2013; Nguyen and Dogruoz, 2013; Lui et al., 2014), language identification for microblogs (Bergsma et al., 2012; Carter et al., 2013), and the task of this paper, language identification for closely related languages. Language identification for closely related languages has been considered by several researchers, though it has lacked a systematic evaluation before the DSL shared task. The problem of distinguishing Croatian from Serbian and Slovenian is explored by Ljubeˇsi´c et al. (2007), who used a list of most frequent words along with a Markov model and a word blacklist, a list of words that are not allowed to appear"
W14-5317,C12-1160,0,0.14887,"Missing"
W96-0101,H92-1022,0,0.0432983,"Missing"
W96-0101,W95-0101,0,0.0689771,"Missing"
W96-0101,E95-1021,0,0.0511575,"Missing"
W96-0101,A92-1018,0,0.0240993,"ical results to support our claim. We demonstrate that, besides providing good estimates for disambiguation, word classes solve some of the problems caused by sparse training data. We describe a part-of-speech tagger built on these principles and we suggest a methodology for developing an adequate training corpus. 1 Introduction In the part-of-speech hterature, whether taggers are based on a rule-based approach (Klein and Simmons, 1963), (Brill, 1992), (Voutilainen, 1993), or on a statistical one (Bahl and Mercer, 1976), (Leech et al., 1983), (Merialdo, 1994), (DeRose, 1988), (Church, 1989), (Cutting et al., 1992), there is a debate as to whether more attention should be paid to lexical probabilities rather than contextual ones. (Church, 1992) claims that part-of-speech taggers depend almost exclusively on lexical probabilities, whereas other researchers, such as Voutilainen (Karlsson et al., 1995) argue that word ambiguities vary widely in function of the specific text and genre. Indeed, part of Church&apos;s argument is relevant if a system is based on a large corpus such as the Brown corpus (Francis and Ku~era, 1982) which represents one million surface forms of morpho-syntacticaJly disambiguated words f"
W96-0101,J88-1003,0,0.0892779,"Missing"
W96-0101,J94-2001,0,0.115141,"Missing"
W96-0101,H94-1050,0,0.0192724,"ation is simplified and results cannot be compared. 6 Implementation and performance of the part-of-speech tagger We have developed a part-of-speech tagger using only a finite-state machine framework. The input string is represented as a finite-state generator, and the tagging is obtained through composition with a pipeline of finite-state transducers (FST&apos;s). Besides the modules for pre-processing and tokenization, the tagger includes a morphological FST and a statistical FST, which incorporates linguistic and statistical knowledge. We have used a toolkit developed at AT&T Bell Laboratories (Pereira et al., 1994) which manipulates weighted and unweighted finite-state machines (acceptors or transducers). Using these tools, we have created a set of programs which generate finitestate transducers from descriptions of linguistic rules (in the form of negative constraints) and for encoding distribution information obtained through statistical learning. Statistical decisions on genotypes are represented by weights - the lower cost, the higher the chance of a particular tag to be picked. With this representation, we are able to prefer one n-gram decision over another based on the cost. The morphological FST"
W96-0101,W93-0306,0,0.0668275,"Missing"
W96-0101,A92-1000,0,\N,Missing
W96-0512,A94-1002,0,0.187057,"Missing"
W96-0512,M92-1001,0,0.0172397,"CIVILIAN"" DEATH: ""1 CIVILIAN"" Figure 1: Excerpts from a MUC-4 Template. ([nessage ( s y s t e m (id "" T S T 3 - M U C 4 - 0 0 1 0 "" ) ) (source (secondary "" N C C O S C "" ) ) (incident ( d a t e ""01 NOV 89"") (location ""El S a l v a d o r "" ) ( t y p e attack) (at~ge ~ccozttplished)) ( p e r p e t r a t o r ( c a t e g o r y terr-act) (org-id "" T H E F M L N "" ) (org-conI rep-fact)) ( v i c t i m (description civilian) ( n u m b e r 1) ) SUMMONS Figure 2: Parsed MUC-4 Template. Our choice of domain was dictated by the existence of two Message Understanding Conferences (MUC) organized by DARPA [Sundheim 1992] in the domain of terrorism in Latin America. The participants were asked to fill templates (as shown in Figure 1) with information extracted from news articles. We parsed the templates (Figure 2), adding information about the primary and secondary sources of news 1. S U M M O N S (SUMMarizing Online NewS articles) is based on an architecture used in P L A N D o c [McKeown et al. 1994], developed jointly by Bellcore and Columbia University. It consists of a content planner which decides what information is to be included in the summary, and a surface generator, based on the F U F / S U R G E"
W97-0903,P88-1000,0,0.0679493,"th domain-specific knowledge that is not explicitly present in the input. [n FLOWDoC and ZEDDoc, semantic enrichment is done at various stages by consulting external ontologies. • D i s c o u r s e Organizer: The discourse organizer performs all the remaining functions prior to lexicalization and surface generation2. Three sub-modules apply general discourse coherence constraints at the levels of discourse, sentence, and sentence constituent. The first module performs aggregation and text linearization operations using an ontology of rhetorical predicates derived from Hobbs (1985) and Polanyi (1988). Linear order and prominence of the subconstituents are then determined, followed by constraints on subconstituents that affect lexical choice (e.g., centering and informational constraints, as in (Passonneau, 1996)). 2|n previous work we referred to this module as the Sentence Planner (Passonneau et al., 1996). A Common Representation All three systems employ a consistent, standardized attribute-value data format that persists from each module to the next. Examples of this internal data format were shown in Figures 1-3. This fbrmat is used for representing and processing conceptualsemantic,"
W97-0903,W94-0319,0,0.0301262,"of P L A N D o c &apos; s architecture as possible, often adapting and generalizing modules that were originally written with only the P L A N D o c system in mind. All three systems employ a modular pipeline architecture. A pipeline architecture is one that separates the functions involved in text generation, such as content planning, discourse organization, lexicalization, and syntactic realization, into distinct modules that operate in sequence. Modular pipeline architectures have a long history of use in text gen16 eration systems (Kukich, 1983a; McKeown, 1985; McDonald and Pustejovsky, 1986; Reiter, 1994), although recent work argues for the need for interaction between modules (Danlos, 1987; Rubinoff, 1992; McKeown et al., 1993). The most powerful argument for using pipeline architectures is the potential benefit of re-using individual modules for subsequent applications. However, with the exception of surface realization modules such as F U F / S U R G E (Elhadad, 1992; Robin, 1994), actual code re-use has been minimal due to the lack of agreement about the order and grouping of subprocesses into modules. In P L A N D o c , FLowDoc, and ZEDDoc, we utilize the following main modules, in the o"
W97-0903,P93-1031,1,0.461759,"th only the P L A N D o c system in mind. All three systems employ a modular pipeline architecture. A pipeline architecture is one that separates the functions involved in text generation, such as content planning, discourse organization, lexicalization, and syntactic realization, into distinct modules that operate in sequence. Modular pipeline architectures have a long history of use in text gen16 eration systems (Kukich, 1983a; McKeown, 1985; McDonald and Pustejovsky, 1986; Reiter, 1994), although recent work argues for the need for interaction between modules (Danlos, 1987; Rubinoff, 1992; McKeown et al., 1993). The most powerful argument for using pipeline architectures is the potential benefit of re-using individual modules for subsequent applications. However, with the exception of surface realization modules such as F U F / S U R G E (Elhadad, 1992; Robin, 1994), actual code re-use has been minimal due to the lack of agreement about the order and grouping of subprocesses into modules. In P L A N D o c , FLowDoc, and ZEDDoc, we utilize the following main modules, in the order listed below: • M e s s a g e G e n e r a t o r : The message generator transcribes the raw data from LEIS-PLAN execution"
W97-0903,A94-1002,1,0.829776,"t generation applications. At the same time, to take full advantage of these opportunities, text generation systems must be easily adaptable to new domains, changing data formats, and distinct underlying ontologies. One crucial factor contributing to the generalization and subsequent practical and commercial viability of text generation systems is the adaptation and re-use of text generation modules and the development of re-usable tools and techniques. In this paper, we focus on the lessons learned during the successive development of three text generation systems at Bellcore: P L A N D o c (McKeown et al., 1994) summarizes execution traces of an expert system for telephone network capacity expansion analysis; FLOwDoc (Passonneau et al., 1996) provides summaries of the most important events in flow diagrams constructed during business reengineering; and Z E D D o c (Passonnean et al., 1997) produces summaries of activity for a user-specified set of advertisements within a user-specified time period from logs of WWW page hits. We built FLowDoc and Z E D D o c by adapting components of the P L A N D o c system. The transfer of the original P L A N D o c modules to new domains led to the replacement of s"
W97-0903,P95-1053,0,0.465208,"icantly adapted our P L A N DOC architecture for use in FLOwDOC, but we were able to re-use the F L o w D o c architecture and much of its code in Z E D D o c . Figure 4 contrasts the architecture of P L A N D o c with those of F L O w D o c and Z E D D o c . In fact, the functions of the Lexicalization and Surface Generation modules remained constant across all three systems. But the functions of the first three modules shifted significantly from P L A N D o c to FLOwDOC. In particular, the function of message aggregation lay exclusively in the Discourse Organization module in P L A N D o c (Shaw, 1995), whereas aggregation functions are executed in both the Message Generation and Discourse Organization modules in FLOWDOC. Because the development of domain-independent, plug-and-play ontology modules is one of the major features that affected these shifts in function, and because such modules greatly increase the portability of the system, we devote the next section to a more detailed description of the function of ontological generalization. 6 (a) Overall architecture for P L A N D o c . Ontological Generalization Ontological generalization refers to the problem of composing, with the help o"
