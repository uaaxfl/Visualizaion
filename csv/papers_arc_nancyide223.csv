2020.nlpcovid19-2.28,{A}sk{M}e: A {LAPPS} {G}rid-based {NLP} Query and Retrieval System for Covid-19 Literature,2020,-1,-1,2,0,16302,keith suderman,Proceedings of the 1st Workshop on {NLP} for {COVID}-19 (Part 2) at {EMNLP} 2020,0,"In a recent project, the Language Application Grid was augmented to support the mining of scientific publications. The results of that ef- fort have now been repurposed to focus on Covid-19 literature, including modification of the LAPPS Grid {``}AskMe{''} query and retrieval engine. We describe the AskMe system and discuss its functionality as compared to other query engines available to search covid-related publications."
2020.lrec-1.855,Infrastructure for Semantic Annotation in the Genomics Domain,2020,0,0,6,0,6326,mahmoud elhaj,Proceedings of the 12th Language Resources and Evaluation Conference,0,"We describe a novel super-infrastructure for biomedical text mining which incorporates an end-to-end pipeline for the collection, annotation, storage, retrieval and analysis of biomedical and life sciences literature, combining NLP and corpus linguistics methods. The infrastructure permits extreme-scale research on the open access PubMed Central archive. It combines an updatable Gene Ontology Semantic Tagger (GOST) for entity identification and semantic markup in the literature, with a NLP pipeline scheduler (Buster) to collect and process the corpus, and a bespoke columnar corpus database (LexiDB) for indexing. The corpus database is distributed to permit fast indexing, and provides a simple web front-end with corpus linguistics methods for sub-corpus comparison and retrieval. GOST is also connected as a service in the Language Application (LAPPS) Grid, in which context it is interoperable with other NLP tools and data in the Grid and can be combined with them in more complex workflows. In a literature based discovery setting, we have created an annotated corpus of 9,776 papers with 5,481,543 words."
2020.lrec-1.893,Interchange Formats for Visualization: {LIF} and {MMIF},2020,-1,-1,4,0,18013,kyeongmin rim,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Promoting interoperrable computational linguistics (CL) and natural language processing (NLP) application platforms and interchange-able data formats have contributed improving discoverabilty and accessbility of the openly available NLP software. In this paper, wediscuss the enhanced data visualization capabilities that are also enabled by inter-operating NLP pipelines and interchange formats.For adding openly available visualization tools and graphical annotation tools to the Language Applications Grid (LAPPS Grid) andComputational Linguistics Applications for Multimedia Services (CLAMS) toolboxes, we have developed interchange formats that cancarry annotations and metadata for text and audiovisual source data. We descibe those data formats and present case studies where wesuccessfully adopt open-source visualization tools and combine them with CL tools."
2020.iwltp-1.10,Towards Standardization of Web Service Protocols for {NLP}aa{S},2020,-1,-1,2,0,15849,jindong kim,Proceedings of the 1st International Workshop on Language Technology Platforms,0,"Several web services for various natural language processing (NLP) tasks ({`}{`}NLP-as-a-service{''} or NLPaaS) have recently been made publicly available. However, despite their similar functionality these services often differ in the protocols they use, thus complicating the development of clients accessing them. A survey of currently available NLPaaS services suggests that it may be possible to identify a minimal application layer protocol that can be shared by NLPaaS services without sacrificing functionality or convenience, while at the same time simplifying the development of clients for these services. In this paper, we hope to raise awareness of the interoperability problems caused by the variety of existing web service protocols, and describe an effort to identify a set of best practices for NLPaaS protocol design. To that end, we survey and compare protocols used by NLPaaS services and suggest how these protocols may be further aligned to reduce variation."
W19-4021,A Multi-Platform Annotation Ecosystem for Domain Adaptation,2019,0,0,2,0.67915,11279,richard castilho,Proceedings of the 13th Linguistic Annotation Workshop,0,"This paper describes an ecosystem consisting of three independent text annotation platforms. To demonstrate their ability to work in concert, we illustrate how to use them to address an interactive domain adaptation task in biomedical entity recognition. The platforms and the approach are in general domain-independent and can be readily applied to other areas of science."
L18-1025,Three Dimensions of Reproducibility in Natural Language Processing,2018,0,3,7,0,29526,bretonnel cohen,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,"Despite considerable recent attention to problems with reproducibility of scientific research, there is a striking lack of agreement about even the definition of the term. That is a problem, because the lack of a consensus definition makes it difficult to compare studies of reproducibility, and thus to have even a broad overview of the state of the issue in natural language processing. This paper proposes an ontology of reproducibility in that field. We show that three dimensions of reproducibility, corresponding to three kinds of claims in natural language processing papers, can account for a variety of types of research reports. These dimensions are reproducibility of a conclusion, of a finding, and of a value. Three biomedical natural language processing papers by the authors of this paper are analyzed with respect to these dimensions."
L18-1206,Bridging the {LAPPS} {G}rid and {CLARIN},2018,0,0,2,0,17752,erhard hinrichs,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1327,Mining Biomedical Publications With The {LAPPS} {G}rid,2018,0,2,1,1,16303,nancy ide,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-0808,"Representation and Interchange of Linguistic Annotation. An In-Depth, Side-by-Side Comparison of Three Designs",2017,10,2,2,0.67915,11279,richard castilho,Proceedings of the 11th Linguistic Annotation Workshop,0,"For decades, most self-respecting linguistic engineering initiatives have designed and implemented custom representations for various layers of, for example, morphological, syntactic, and semantic analysis. Despite occasional efforts at harmonization or even standardization, our field today is blessed with a multitude of ways of encoding and exchanging linguistic annotations of these types, both at the levels of {`}abstract syntax{'}, naming choices, and of course file formats. To a large degree, it is possible to work within and across design plurality by conversion, and often there may be good reasons for divergent design reflecting differences in use. However, it is likely that some abstract commonalities across choices of representation are obscured by more superficial differences, and conversely there is no obvious procedure to tease apart what actually constitute contentful vs. mere technical divergences. In this study, we seek to conceptually align three representations for common types of morpho-syntactic analysis, pinpoint what in our view constitute contentful differences, and reflect on the underlying principles and specific requirements that led to individual choices. We expect that a more in-depth understanding of these choices across designs may led to increased harmonization, or at least to more informed design of future representations."
W16-5202,{LAPPS}/Galaxy: Current State and Next Steps,2016,0,3,1,1,16303,nancy ide,Proceedings of the Third International Workshop on Worldwide Language Service Infrastructure and Second Workshop on Open Infrastructures and Analysis Frameworks for Human Language Technologies ({WLSI}/{OIAF}4{HLT}2016),0,"The US National Science Foundation (NSF) SI2-funded LAPPS/Galaxy project has developed an open-source platform for enabling complex analyses while hiding complexities associated with underlying infrastructure, that can be accessed through a web interface, deployed on any Unix system, or run from the cloud. It provides sophisticated tool integration and history capabilities, a workflow system for building automated multi-step analyses, state-of-the-art evaluation capabilities, and facilities for sharing and publishing analyses. This paper describes the current facilities available in LAPPS/Galaxy and outlines the project{'}s ongoing activities to enhance the framework."
L16-1073,The Language Application Grid and Galaxy,2016,0,0,1,1,16303,nancy ide,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The NSF-SI2-funded LAPPS Grid project is a collaborative effort among Brandeis University, Vassar College, Carnegie-Mellon University (CMU), and the Linguistic Data Consortium (LDC), which has developed an open, web-based infrastructure through which resources can be easily accessed and within which tailored language services can be efficiently composed, evaluated, disseminated and consumed by researchers, developers, and students across a wide variety of disciplines. The LAPPS Grid project recently adopted Galaxy (Giardine et al., 2005), a robust, well-developed, and well-supported front end for workflow configuration, management, and persistence. Galaxy allows data inputs and processing steps to be selected from graphical menus, and results are displayed in intuitive plots and summaries that encourage interactive workflows and the exploration of hypotheses. The Galaxy workflow engine provides significant advantages for deploying pipelines of LAPPS Grid web services, including not only means to create and deploy locally-run and even customized versions of the LAPPS Grid as well as running the LAPPS Grid in the cloud, but also access to a huge array of statistical and visualization tools that have been developed for use in genomics research."
W14-5204,The Language Application Grid Web Service Exchange Vocabulary,2014,15,10,1,1,16303,nancy ide,Proceedings of the Workshop on Open Infrastructures and Analysis Frameworks for {HLT},0,"In the context of the Linguistic Applications LAPPS Grid project, we have undertaken the definition of a Web Service Exchange Vocabulary WS-EV specifying a terminology for a core of linguistic objects and properties exchanged among NLP tools that consume and produce linguistically annotated data. The goal is not to define a new set of terms, but rather to provide a single web location where terms relevant for exchange among NLP tools are defined and provide a sameAs link to all known web-based definitions that correspond to them. The WS-EV is intended to be used by a federation of six grids currently being formed but is usable by any web service platform."
W14-3005,{F}rame{N}et and Linked Data,2014,28,1,1,1,16303,nancy ide,Proceedings of Frame Semantics in {NLP}: A Workshop in Honor of Chuck {F}illmore (1929-2014),0,"FrameNet is the ideal resource for representation as linked data, and several renderings of the resource in RDF/OWL have been created. FrameNet has also been and continues to be linked to other major resources, including WordNet, BabelNet, and MASC, in the Linguistic Linked Open Data cloud. Although so far the supporting technologies have not enabled easy and widespread access to the envisioned massive network of language resources, a conflation of recent efforts suggests this may be a reality in the not-too-distant future."
ide-etal-2014-language,The Language Application Grid,2014,19,32,1,1,16303,nancy ide,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The Language Application (LAPPS) Grid project is establishing a framework that enables language service discovery, composition, and reuse and promotes sustainability, manageability, usability, and interoperability of natural language Processing (NLP) components. It is based on the service-oriented architecture (SOA), a more recent, web-oriented version of the ÂpipelineÂ architecture that has long been used in NLP for sequencing loosely-coupled linguistic analyses. The LAPPS Grid provides access to basic NLP processing tools and resources and enables pipelining such tools to create custom NLP applications, as well as composite services such as question answering and machine translation together with language resources such as mono- and multi-lingual corpora and lexicons that support NLP. The transformative aspect of the LAPPS Grid is that it orchestrates access to and deployment of language resources and processing functions available from servers around the globe and enables users to add their own language resources, services, and even service grids to satisfy their particular needs."
C14-1054,Biber Redux: Reconsidering Dimensions of Variation in {A}merican {E}nglish,2014,30,6,2,0,721,rebecca passonneau,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"Genre classification has been found to improve performance in many applications of statistical NLP, including language modeling for spoken language, domain adaptation of statistical parsers, and machine translation. It has also been found to benefit retrieval of spoken or written documents. At its base, however, classification assumes separability. This paper revisits an assumption that genre variation is continuous along multiple dimensions, and an early use of principal component analysis to find these dimensions. Results on a very heterogeneous corpus of post1990s American English reveal four major dimensions, three of which echo those found in prior work and the fourth depending on features not used in the earlier study. The resulting model can provide a basis for more detailed analysis of sub-genres and the relation between genre and situations of language use, as well as a means to predict distributional properties of new genres."
W13-2312,Importing {MASC} into the {ANNIS} linguistic database: A case study of mapping {G}r{AF},2013,8,5,2,0,11002,arne neumann,Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse,0,"This paper describes the importation of Manually Annotated Sub-Corpus (MASC) data and annotations into the linguistic database ANNIS, which allows users to visualize and query linguistically-annotated corpora. We outline the process of mapping MASCxe2x80x99s GrAF representation to ANNISxe2x80x99s internal format relANNIS and demonstrate how the system provides access to multiple annotation layers in the corpus. This access provides information about inter-layer relations and dependencies that have been previously difficult to explore, and which are highly valuable for continued development of language processing applications."
W12-3608,A Model for Linguistic Resource Description,2012,9,1,1,1,16303,nancy ide,Proceedings of the Sixth Linguistic Annotation Workshop,0,"This paper describes a comprehensive standard for resource description developed within ISO TC37 SC4). The standard is instantiated in a system of XML headers that accompany data and annotation documents represented using the the Linguistic Annotation Framework's Graph Annotation Format (GrAF) (Ide and Suderman, 2007; Ide and Suderman, Submitted). It provides mechanisms for describing the organization of the resource, documenting the conventions used in the resource, associating data and annotation documents, and defining and selecting defined portions of the resource and its annotations. It has been designed to accommodate the use of XML technologies for processing, including XPath, XSLT, and, by virtue of the system's linkage strategy, RDF/OWL, and to accommodate linkage to web-based ontologies and data category registries such as the OLiA ontologies (Chiarcos, 2012) and ISOCat (Marc Kemps-Snijders and Wright, 2008)."
passonneau-etal-2012-masc,The {MASC} Word Sense Corpus,2012,9,21,4,0,721,rebecca passonneau,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The MASC project has produced a multi-genre corpus with multiple layers of linguistic annotation, together with a sentence corpus containing WordNet 3.1 sense tags for 1000 occurrences of each of 100 words produced by multiple annotators, accompanied by indepth inter-annotator agreement data. Here we give an overview of the contents of MASC and then focus on the word sense sentence corpus, describing the characteristics that differentiate it from other word sense corpora and detailing the inter-annotator agreement studies that have been performed on the annotations. Finally, we discuss the potential to grow the word sense sentence corpus through crowdsourcing and the plan to enhance the content and annotations of MASC through a community-based collaborative effort."
de-melo-etal-2012-empirical,Empirical Comparisons of {MASC} Word Sense Annotations,2012,6,9,3,0,3978,gerard melo,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We analyze how different conceptions of lexical semantics affect sense annotations and how multiple sense inventories can be compared empirically, based on annotated text. Our study focuses on the MASC project, where data has been annotated using WordNet sense identifiers on the one hand, and FrameNet lexical units on the other. This allows us to compare the sense inventories of these lexical resources empirically rather than just theoretically, based on their glosses, leading to new insights. In particular, we compute contingency matrices and develop a novel measure, the Expected Jaccard Index, that quantifies the agreement between annotations of the same data based on two different resources even when they have different sets of categories."
W10-1806,{A}nveshan: A Framework for Analysis of Multiple Annotators{'} Labeling Behavior,2010,31,20,4,0,1435,vikas bhardwaj,Proceedings of the Fourth Linguistic Annotation Workshop,0,"Manual annotation of natural language to capture linguistic information is essential for NLP tasks involving supervised machine learning of semantic knowledge. Judgements of meaning can be more or less subjective, in which case instead of a single correct label, the labels assigned might vary among annotators based on the annotators' knowledge, age, gender, intuitions, background, and so on. We introduce a framework Anveshan, where we investigate annotator behavior to find outliers, cluster annotators by behavior, and identify confusable labels. We also investigate the effectiveness of using trained annotators versus a larger number of untrained annotators on a word sense annotation task. The annotation data comes from a word sense disambiguation task for polysemous words, annotated by both trained annotators and untrained annotators from Amazon's Mechanical turk. Our results show that Anveshan is effective in uncovering patterns in annotator behavior, and we also show that trained annotators are superior to a larger number of untrained annotators for this task."
W10-1840,Anatomy of Annotation Schemes: Mapping to {G}r{AF},2010,9,15,1,1,16303,nancy ide,Proceedings of the Fourth Linguistic Annotation Workshop,0,"In this paper, we apply the annotation scheme design methodology defined in (Bunt, 2010) and demonstrate its use for generating a mapping from an existing annotation scheme to a representation in GrAF format. The most important features of this methodology are (1) the distinction of the abstract and concrete syntax of an annotation language; (2) the specification of a formal semantics for the abstract syntax; and (3) the formalization of the relation between abstract and concrete syntax, which guarantees that any concrete syntax inherits the semantics of the abstract syntax, and thus guarantees meaning-preserving mappings between representation formats. By way of illustration, we apply this mapping strategy to annotations from ISO-TimeML, PropBank, and FrameNet."
P10-2013,The Manually Annotated Sub-Corpus: A Community Resource for and by the People,2010,15,54,1,1,16303,nancy ide,Proceedings of the {ACL} 2010 Conference Short Papers,0,"The Manually Annotated Sub-Corpus (MASC) project provides data and annotations to serve as the base for a communitywide annotation effort of a subset of the American National Corpus. The MASC infrastructure enables the incorporation of contributed annotations into a single, usable format that can then be analyzed as it is or ported to any of a variety of other formats. MASC includes data from a much wider variety of genres than existing multiply-annotated corpora of English, and the project is committed to a fully open model of distribution, without restriction, for all data and annotations produced or contributed. As such, MASC is the first large-scale, open, community-based effort to create much needed language resources for NLP. This paper describes the MASC project, its corpus and annotations, and serves as a call for contributions of data and annotations from the language processing community."
ide-etal-2010-anc2go,{ANC}2{G}o: A Web Application for Customized Corpus Creation,2010,5,9,1,1,16303,nancy ide,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We describe a web application called ÂANC2GoÂ that enables the user to select data from the Open American National Corpus (OANC) and the Manually Annotated Sub-corpus (MASC) together with some or all of the annotations available. The user also may select from among a variety of options for output format, or may receive the selected portions of the corpus and annotations in their original GrAF XML standoff format.. The request is processed by merging the annotations selected and rendering them in the desired output format, then bundling the results and making it available for download. Thus, users can create a customized corpus with data and annotations of their choosing, delivered in the format that is most convenient for their use. ANC2Go will be released as a web service in the near future. Both the OANC and MASC are freely available for any use from the American National Corpus website and may be accessed through the ANC2Go application, or they may downloaded in their entirety."
passonneau-etal-2010-word,Word Sense Annotation of Polysemous Words by Multiple Annotators,2010,27,19,4,0,721,rebecca passonneau,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We describe results of a word sense annotation task using WordNet, involving half a dozen well-trained annotators on ten polysemous words for three parts of speech. One hundred sentences for each word were annotated. Annotators had the same level of training and experience, but interannotator agreement (IA) varied across words. There was some effect of part of speech, with higher agreement on nouns and adjectives, but within the words for each part of speech there was wide variation. This variation in IA does not correlate with number of senses in the inventory, or the number of senses actually selected by annotators. In fact, IA was sometimes quite high for words with many senses. We claim that the IA variation is due to the word meanings, contexts of use, and individual differences among annotators. We find some correlation of IA with sense confusability as measured by a sense confusion threshhold (CT). Data mining for association rules on a flattened data representation indicating each annotator's sense choices identifies outliers for some words, and systematic differences among pairs of annotators on others."
cieri-etal-2010-road,A Road Map for Interoperable Language Resource Metadata,2010,0,5,7,0,17560,christopher cieri,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"LRs remain expensive to create and thus rare relative to demand across languages and technology types. The accidental re-creation of an LR that already exists is a nearly unforgivable waste of scarce resources that is unfortunately not so easy to avoid. The number of catalogs the HLT researcher must search, with their different formats, make it possible to overlook an existing resource. This paper sketches the sources of this problem and outlines a proposal to rectify along with a new vision of LR cataloging that will to facilitates the documentation and exploitation of a much wider range of LRs than previously considered."
Y09-2026,{L}atin Etymologies as Features on {BNC} Text Categorization,2009,27,1,3,0,16748,alex fang,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"This paper presents an early experimental work on BNC Text Categorization (TC) with Latin etymologies as features, emphasis on spoken and written texts. Two aims achieved in this study: (1) to explore discriminative new linguistic features rather than lots of noise-bringing xe2x80x9cbag-of-wordsxe2x80x9d (BoW). (2) to build up a base step to represent texts in distinct types of linguistic features with different weighting scheme rather than a plain feature vectors of BoW. The experiments disclose a notable distinct distribution pattern of Latin etymologies in spoken and written BNC texts. The performance of a home-made classifier based on the probability distribution ranges of Latin etymologies reaches a precision of 72.31% and recall of 73.22% on BNC spoken texts and precision of 73.31% and recall of 69.98% on BNC written texts."
W09-3004,"Bridging the Gaps: Interoperability for {G}r{AF}, {GATE}, and {UIMA}",2009,4,19,1,1,16303,nancy ide,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"This paper explores interoperability for data represented using the Graph Annotation Framework (GrAF) (Ide and Suderman, 2007) and the data formats utilized by two general-purpose annotation systems: the General Architecture for Text Engineering (GATE) (Cunningham, 2002) and the Unstructured Information Management Architecture (UIMA). GrAF is intended to serve as a pivot to enable interoperability among different formats, and both GATE and UIMA are at least implicitly designed with an eye toward interoperability with other formats and tools. We describe the steps required to perform a round-trip rendering from GrAF to GATE and GrAF to UIMA CAS and back again, and outline the commonalities as well as the differences and gaps that came to light in the process."
W09-3034,The {SILT} and {F}la{R}e{N}et International Collaboration for Interoperability,2009,-1,-1,1,1,16303,nancy ide,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,None
W09-2402,Making Sense of Word Sense Variation,2009,31,15,3,0,721,rebecca passonneau,Proceedings of the Workshop on Semantic Evaluations: Recent Achievements and Future Directions ({SEW}-2009),0,"We present a pilot study of word-sense annotation using multiple annotators, relatively polysemous words, and a heterogenous corpus. Annotators selected senses for words in context, using an annotation interface that presented WordNet senses. Interannotator agreement (IA) results show that annotators agree well or not, depending primarily on the individual words and their general usage properties. Our focus is on identifying systematic differences across words and annotators that can account for IA variation. We identify three lexical use factors: semantic specificity of the context, sense concreteness, and similarity of senses. We discuss systematic differences in sense selection across annotators, and present the use of association rules to mine the data for systematic differences across annotators."
ide-etal-2008-masc,{MASC}: the Manually Annotated Sub-Corpus of {A}merican {E}nglish,2008,14,34,1,1,16303,nancy ide,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"To answer the critical need for sharable, reusable annotated resources with rich linguistic annotations, we are developing a Manually Annotated Sub-Corpus (MASC) including texts from diverse genres and manual annotations or manually-validated annotations for multiple levels, including WordNet senses and FrameNet frames and frame elements, both of which have become significant resources in the international computational linguistics community. To derive maximal benefit from the semantic information provided by these resources, the MASC will also include manually-validated shallow parses and named entities, which will enable linking WordNet senses and FrameNet frames within the same sentences into more complex semantic structures and, because named entities will often be the role fillers of FrameNet frames, enrich the semantic and pragmatic information derivable from the sub-corpus. All MASC annotations will be published with detailed inter-annotator agreement measures. The MASC and its annotations will be freely downloadable from the ANC website, thus providing maximum accessibility for researchers from around the globe."
caselli-etal-2008-bilingual,A Bilingual Corpus of Inter-linked Events,2008,6,2,2,0,6,tommaso caselli,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"This paper describes the creation of a bilingual corpus of inter-linked events for Italian and English. Linkage is accomplished through the Inter-Lingual Index (ILI) that links ItalWordNet with WordNet. The availability of this resource, on the one hand, enables contrastive analysis of the linguistic phenomena surrounding events in both languages, and on the other hand, can be used to perform multilingual temporal analysis of texts. In addition to describing the methodology for construction of the inter-linked corpus and the analysis of the data collected, we demonstrate that the ILI could potentially be used to bootstrap the creation of comparable corpora by exporting layers of annotation for words that have the same sense."
W07-1501,{G}r{AF}: A Graph-based Format for Linguistic Annotations,2007,21,149,1,1,16303,nancy ide,Proceedings of the Linguistic Annotation Workshop,0,"In this paper we describe the Graph Annotation Format (GrAF) and show how it is used represent not only independent linguistic annotations, but also sets of merged annotations as a single graph. To demonstrate this, we have automatically transduced several different annotations of the Wall Street Journal corpus into GrAF and show how the annotations can then be merged, analyzed, and visualized using standard graph algorithms and tools. We also discuss how, as a standard graph representation, it allows for the application of well-established graph traversal and analysis algorithms to produce information about interactions and commonalities among merged annotations. GrAF is an extension of the Linguistic Annotation Framework (LAF) (Ide and Romary, 2004, 2006) developed within ISO TC37 SC4 and as such, implements state-of-the-art best practice guidelines for representing linguistic annotations."
W07-1529,Shared Corpora Working Group Report,2007,8,1,2,0,3082,adam meyers,Proceedings of the Linguistic Annotation Workshop,0,"We seek to identify a limited amount of representative corpora, suitable for annotation by the computational linguistics annotation community. Our hope is that a wide variety of annotation will be undertaken on the same corpora, which would facilitate: (1) the comparison of annotation schemes; (2) the merging of information represented by various annotation schemes; (3) the emergence of NLP systems that use information in multiple annotation schemes; and (4) the adoption of various types of best practice in corpus annotation. Such best practices would include: (a) clearer demarcation of phenomena being annotated; (b) the use of particular test corpora to determine whether a particular annotation task can feasibly achieve good agreement scores; (c) The use of underlying models for representing annotation content that facilitate merging, comparison, and analysis; and (d) To the extent possible, the use of common annotation categories or a mapping among categories for the same phenomenon used by different annotation groups.n n This study will focus on the problem of identifying such corpora as well as the suitability of two candidate corpora: the Open portion of the American National Corpus (Ide and Macleod, 2001; Ide and Suderman, 2004) and the Controversial portions of the WikipediaXML corpus (Denoyer and Gallinari, 2006)."
W06-2716,Layering and Merging Linguistic Annotations,2006,3,10,2,0,16302,keith suderman,Proceedings of the 5th Workshop on {NLP} and {XML} ({NLPXML}-2006): Multi-Dimensional Markup in Natural Language Processing,0,"The American National Corpus and its annotations are represented in a stand-off XML format compliant with the specifications of ISO TC37 SC4 WG1's Linguistic Annotation Framework. Because few systems that enable search and access of the corpus currently support stand-off markup, the project has developed a SAX like parser that generates ANC data with annotations in-line, in a variety of output formats."
ide-suderman-2006-integrating,Integrating Linguistic Resources: The {A}merican National Corpus Model,2006,5,21,1,1,16303,nancy ide,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the architecture of the American National Corpus and the design decisions we have made in order to make the corpus easy to use with a variety of existing tools with varying functionality, and to allow for layering multiple annotations over the data. The overall goal of the ANC project is to provide an Âopen linguistic infrastructureÂ for American English, consisting of as many self-generated or contributed annotations of the data as possible together with derived. The availability of a wide variety of annotations for the same data and in a common format should significantly simplify the processing required to extract annotations from different sources and enable use of the ANC and its annotations with off-the-shelf software."
ide-romary-2006-representing,Representing Linguistic Corpora and Their Annotations,2006,5,49,1,1,16303,nancy ide,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"A Linguistic Annotation Framework (LAF) is being developed within the International Standards Organization Technical Committee 37 Sub-committee on Language Resource Management (ISO TC37 SC4). LAF is intended to provide a standardized means to represent linguistic data and its annotations that is defined broadly enough to accommodate all types of linguistic annotations, and at the same time provide means to represent precise and potentially complex linguistic information. The general principles informing the design of LAF have been previously reported (Ide and Romary, 2003; Ide and Romary, 2004a). This paper describes some of the more technical aspects of the LAF design that have been addressed in the process of finalizing the specifications for the standard."
tufis-etal-2004-word,Word Sense Disambiguation as a Wordnets{'} Validation Method in Balkanet,2004,6,5,3,0,32036,dan tufis,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"BalkaNet is a European project which aims at the development of monolingual wordnets for five languages in the Balkans area (Bulgarian, Greek, Romanian Serbia, and Turkish) and at improvement of the Czech wordnet developed in the EuroWordNet project. The wordnets are aligned to the Princeton Wordnet, according to the principles established by the EuroWordNet consortium. One of the main concerns of this project is the interlingual validation of the wordnets alignment. To this end, we have developed a WSD system based on parallel corpora which exploits the common intuition according to which words that are reciprocal translations in a parallel texts should have the same (or closely related) interlingual meanings. With wordnets under construction our WSD system is mainly a validation tool, pinpointing wrong interlingual alignments, incomplete or missing synsets in one or another of the wordnets."
ide-woolner-2004-exploiting,Exploiting Semantic Web Technologies for Intelligent Access to Historical Documents,2004,2,7,1,1,16303,nancy ide,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The FDR/Pearl Harbor Project involves the enhancement of materials drawn from the Franklin D. Roosevelt Library and Digital Archives, which includes a range of image, sound, video and textual data. The project is undertaking the encoding, annotation, and multi-modal linkage of a portion of the collection, and enhancement of a web-based interface that enables exploitation of state-of-theart methods for search and retrieval. We are currently developing a pilot project that includes government correspondence and documents produced in the sixth months prior to and including December 7, 1941, the date of the Japanese attack on Pearl Harbor, which has obvious historical, political, and general interest. The major activities in the project involve development of a model for historical documents and associated data and its instantiation using W3 standards, including XML, the Resource Definition Framework (RDF and RDF schemas), and the Ontology Web Language (OWL); development of automated means, or enhancement of existing software, to identify and mark relevant elements within these data; and exploration of the potential to automatically extract ontological information so as to enable sophisticated search and retrieval via inferencing."
ide-suderman-2004-american,The {A}merican National Corpus First Release,2004,7,56,1,1,16303,nancy ide,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The First Release of the American National Corpus (ANC) was made available in mid-fall, 2003. The data includes approximately 11 million words of American English, including written and spoken data and a variety of text types annotated for part of speech and lemma. The corpus is provided in XML format conformant to the XML Corpus Encoding Standard (XCES) (http://www.xml-ces.org), and is distributed in both a stand-off version (where annotation is in an XML document separate from the primary texts) and a merged version (where annotation is included in-line in the texts). The merged version includes annotation for part of speech and lemma produced by the Biber tagger; in stand-off annotation, in addition to the Biber tagging, morpho-syntactic annotations of the data are provided using the CLAWS 5 and 7 tagsets as well as several other tagsets."
ide-romary-2004-registry,A Registry of Standard Data Categories for Linguistic Annotation,2004,2,45,1,1,16303,nancy ide,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"In this paper we describe the most recent work within ISO TC37/SC 4, and in particular the development of a Data Category Registry (DCR) component of the Linguistic Annotation Framework. The DCR will contain a formally defined set of linguistic categories in common use within the language engineering community for reference and use in linguistically annotated resources. We outline the first proposals for creation and management of the DCR, as a solicitation for input from the community."
C04-1192,"Fine-Grained Word Sense Disambiguation Based on Parallel Corpora, Word Alignment, Word Clustering and Aligned Wordnets",2004,13,58,3,0,32036,dan tufis,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"The paper presents a method for word sense disambiguation based on parallel corpora. The method exploits recent advances in word alignment and word clustering based on automatic extraction of translation equivalents and being supported by available aligned wordnets for the languages in the corpus. The wordnets are aligned to the Princeton Wordnet, according to the principles established by EuroWordNet. The evaluation of the WSD system, implementing the method described herein showed very encouraging results. The same system used in a validation mode, can be used to check and spot alignment errors in multilingually aligned wordnets as BalkaNet and EuroWordNet."
W03-1901,Outline of the International Standard Linguistic Annotation Framework,2003,8,39,1,1,16303,nancy ide,Proceedings of the {ACL} 2003 Workshop on Linguistic Annotation: Getting the Model Right,0,"This paper describes the outline of a linguistic annotation framework under development by ISO TC37 SC WG1-1. This international standard provides an architecture for the creation, annotation, and manipulation of linguistic resources and processing software. The goal is to provide maximum flexibility for encoders and annotators, while at the same time enabling interchange and re-use of annotated linguistic resources. We describe here the outline of the standard for the purposes of enabling annotators to begin to explore how their schemes may map into the framework."
W03-1905,{RDF} Instantiation of {ISLE}/{MILE} Lexical Entries,2003,1,15,1,1,16303,nancy ide,Proceedings of the {ACL} 2003 Workshop on Linguistic Annotation: Getting the Model Right,0,"In this paper we describe the overall model for MILE lexical entries and provide an instantiation of the model in RDF/OWL. This work has been done with an eye toward the goal of creating a web-based registry of lexical data categories and enabling the description of lexical information by establishing relations among them, and/or using predefined objects that may reside at various locations on the web. It is also assumed that using OWL specifications to enhance specifications of the ontology of lexical objects will eventually enable the exploitation of inferencing engines to retrieve and possibly create lexical information on the fly, as suited to particular contexts. As such, the model and RDF instantiation provided here are in line with the goals of ISO TC37 SC4, and should be fully mappable to the proposed pivot."
W03-0804,International Standard for a Linguistic Annotation Framework,2003,7,89,1,1,16303,nancy ide,Proceedings of the {HLT}-{NAACL} 2003 Workshop on Software Engineering and Architecture of Language Technology Systems ({SEALTS}),0,"This paper describes the outline of a linguistic annotation framework under development by ISO TC37 SC WG1-1. This international standard will provide an architecture for the creation, annotation, and manipulation of linguistic resources and processing software. The outline described here results from a meeting of approximately 20 experts in the field, who determined the principles and fundamental structure of the framework. The goal is to provide maximum flexibility for encoders and annotators, while at the same time enabling interchange and re-use of annotated linguistic resources."
W02-0808,Sense Discrimination with Parallel Corpora,2002,16,97,1,1,16303,nancy ide,Proceedings of the {ACL}-02 Workshop on Word Sense Disambiguation: Recent Successes and Future Directions,0,"This paper describes an experiment that uses translation equivalents derived from parallel corpora to determine sense distinctions that can be used for automatic sense-tagging and other disambiguation tasks. Our results show that sense distinctions derived from cross-lingual information are at least as reliable as those made by human annotators. Because our approach is fully automated through all its steps, it could provide means to obtain large samples of sense-tagged data without the high cost of human annotation."
calzolari-etal-2002-towards,Towards Best Practice for Multiword Expressions in Computational Lexicons,2002,6,99,4,0,18003,nicoletta calzolari,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The importance a nd role of multi-word expressions (MWE) in the description and p rocessing o f natural l anguage has been long recognized. However, multi-word information has often been relegated to the marginal role of idiosyncratic lexical information. The need for MWE lexicons grows even more acute for multi-lingual applications, for which (sometimes complex) correspondences must be identified, classified, and recorded. Within the XMELLT and ISLE projects we have started to investigate the potential to develop multi-lingual, multi-word expression lexicons incorporating both syntactic and semantic information. We aim at specifying means to acquire and represent multi-word lexical entries for multiple languages, and establishing uniform (or inter-translatable) standards for describing multi-word lexical entries. We explored theoretical approaches used in large lexicon-building projects, in p articular FrameNet and SIMPLE. They constitute interesting frameworks for the explicit syntactic and semantic representation of MWEs, due mainly to their ability to capture semantic multidimensionality, through frame e lements and qualia relations respectively. We a lso developed an abstract data model for lexical information together with a representation in XML for it. Our goal is to define a set of minimal lexicon xe2x80x9cobjectsxe2x80x9d, which can serve not only as a model for MWEs but also for lexical data in general."
ide-etal-2002-american,The {A}merican National Corpus: More Than the Web Can Provide,2002,8,35,1,1,16303,nancy ide,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"The American National Corpus (ANC) project is developing a corpus comparable to the British National Corpus (BNC), covering American English. Recent interest in the web as a source of corpus materials has caused some in the language processing community to suggest that the development of a corpus of American English is unnecessary. However, we argue that far from being rendered superfluous by the availability of web materials, the ANC is likely to provide a resource for developing web acquisition techniques to support tasks such as genre and language detection and automatic annotation. This paper presents a comparison of the ANC in terms of both content and format with a test corpus compiled from web data, and a discussion of points of intersection and divergence."
ide-romary-2002-standards,Standards for Language Resources,2002,0,50,1,1,16303,nancy ide,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,"This paper presents an abstract data model for linguistic annotations and its implementation using XML, RDF and related standards; and to outline the work of a newly formed committee of the International Standards Organization (ISO), ISO/TC 37/SC 4 Language Resource Management, which will use this work as its starting point. The primary motive for presenting the latter is to solicit the participation of members of the research community to contribute to the work of the committee."
P01-1040,A Common Framework for Syntactic Annotation,2001,7,30,1,1,16303,nancy ide,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"It is widely recognized that the proliferation of annotation schemes runs counter to the need to re-use language resources, and that standards for linguistic annotation are becoming increasingly mandatory. To answer this need, we have developed a representation framework comprised of an abstract model for a variety of different annotation types (e.g., morpho-syntactic tagging, syntactic annotation, co-reference annotation, etc.), which can be instantiated in different ways depending on the annotator s approach and goals. In this paper we provide an overview of our representation framework and demonstrate its applicability to syntactic annotation. We show how the framework can contribute to comparative evaluation and merging of parser output and diverse syntactic annotation schemes."
W00-1506,The {XML} Framework and Its Implications for the Development of Natural Language Processing Tools,2000,8,5,1,1,16303,nancy ide,Proceedings of the {COLING}-2000 Workshop on Using Toolsets and Architectures To Build {NLP} Systems,0,"The eXtensible Markup Language (XML) (Bray, et al., 1998) is the emerging standard for data representation and exchange on the World Wide Web. The XML Framework includes very powerful mechanisms for accessing and manipulating XML documents that are likely to significantly impact the development of tools for processing natural language and annotated corpora."
P00-1053,A Hierarchical Account of Referential Accessibility,2000,16,20,1,1,16303,nancy ide,Proceedings of the 38th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we outline a theory of referential accessibility called Veins Theory (VT). We show how VT addresses the problem of left satellites, currently a problem for stack-based models, and show that VT can be used to significantly reduce the search space for antecedents. We also show that VT provides a better model for determining domains of referential accessibility, and discuss how VT can be used to address various issues of structural ambiguity."
ide-etal-2000-xces,{XCES}: An {XML}-based Encoding Standard for Linguistic Corpora,2000,8,130,1,1,16303,nancy ide,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The Corpus Encoding Standard (CES) is a part of the EAGLES Guidelines developed by the Expert Advisory Group on Language Engineering Standards (EAGLES) that provides a set of encoding standards for corpus-based work in natural language processing applications. We have instantiated the CES as an XML application called XCES, based on the same data architecture comprised of a primary encoded text and standoff annotation in separate documents. Conversion to XML enables use of some of the more powerful mechanisms provided in the XML framework, including the XSLT Transformation Language, XML Schemas, and support for interrescue reference together with an extensive path syntax for pointers. In this paper, we describe the differences between the CES and XCES DTDs and demonstrate how XML mechanisms can be used to select from and manipulate annotated corpora encoded according to XCES specifications. We also provide a general overview of XML and the XML mechanisms that are most relevant to language engineering research and applications."
macleod-etal-2000-american,The {A}merican National Corpus: A Standardized Resource for {A}merican {E}nglish,2000,12,15,2,0,51384,catherine macleod,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"Linguistic research has become heavily reliant on text corpora over the past ten years. Such resources are becoming increasingly available through efforts such as the Linguistic Data Consortium (LDC) in the US and the European Language Resources Association (ELRA) in Europe. However, in the main the corpora that are gathered and distributed through these and other mechanisms consist of texts which can be easily acquired and are available for re-distribution without undue problems of copyright, etc. This practice has resulted in a vast over-representation among available corpora of certain genres, in particular newspaper samples, which comprise the greatest percentage of texts currently available from, for example, the LDC, and which also dominate the training data available for speech recognition purposes. Other available corpora typically consist of technical reports, transcriptions of parliamentary and other proceedings, short telephone conversations, and the like. The upshot of this is that corpusbased natural language processing has relied heavily on language samples representative of usage in a handful of limited and linguistically specialized domains."
erjavec-etal-2000-concede,The Concede Model for Lexical Databases,2000,6,18,3,0,15753,tomavz erjavec,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The value of language resources is greatly enhanced if they share a common markup with an explicit minimal semantics. Achieving this goal for lexical databases is difficult, as large-scale resources can realistically only be obtained by up-translation from pre-existing dictionaries, each with its own proprietary structure. This paper describes the approach we have taken in the Concede project, which aims to develop compatible lexical databases for six Central and Eastern European languages. Starting with sample entries from original presentation-oriented electronic representations of dictionaries, we transformed the data into an intermediate TEI-compatible representation to provide a common baseline for evaluating and comparing the dictionaries. We then developed a more restrictive encoding, formalised as an XML DTD with a clearly-defined semantic interpretation. We present this DTD and discuss a sample conversion from TEI, together with an application which hyperlinks a HTML representation of the dictionary to on-line concordancing over a corpus."
C00-1031,An Empirical Investigation of the Relation Between Discourse Structure and Co-Reference,2000,15,16,2,1,14259,dan cristea,{COLING} 2000 Volume 1: The 18th International Conference on Computational Linguistics,0,"We compare the potential of two classes of linear and hierarchical models of discourse to determine co-reference links and resolve anaphors. The comparison uses a corpus of thirty texts, which were manually annotated for co-reference and discourse structure."
W99-0508,Parallel Translations as Sense Discriminators,1999,17,16,1,1,16303,nancy ide,{SIGLEX}99: Standardizing Lexical Resources,0,"This article reports the results of a p r e hmlna ry analysis of translation equivalents in four languages from different language famdles, extracted from an on-hne parallel corpus of George Orwell's Nmeteen Eighty-Four The goal of the study is to determine the degree to which translatmn equivalents for different meamngs of a polysemous word In Enghsh are lexlcahzed differently across a variety of languages, and to detelmme whether this information can be used to structure or create a set of sense distinctions useful in natural language processing apphcatmns A coherence Index is computed that measures the tendency for different senses o1 the same English word to be lexlcahzed differently, and flora this data a clustering algorithm is used to create sense hierat chles"
W99-0106,Discourse Structure and Co-Reference: An Empirical Study,1999,26,20,2,1,14259,dan cristea,The Relation of Discourse/Dialogue Structure and Reference,0,"In most cases,, the COLLECT module determines an LPA by enumerating all antecedents in a window of text that pLeced__es the anaphor under scrutiny (Hobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997; Ge et al., 1998). This window can be as small as two or three sentences or as large as the entire preceding text. The FILTER module usually imposes semantic constraints by requiring that the anaphor and potential antecedents have the same number and gender, that selectional restrictions are obeyed, etc. The PREFERENCE module imposes preferences on potential antecedents on the basis of their grammatical roles, parallelism, frequency, proximity, etc. In some cases, anaphora resolution systems implement these modules explicitly (I-Iobbs, 1978; Lappin and Leass, 1994; Mitkov, 1997; Kameyama, 1997). In other cases, these modules are integrated by means of statistical (Ge et al., 1998) or uncertainty reasoning techniques (Mitkov, 1997)."
W98-1102,Encoding Linguistic Corpora,1998,2,15,1,1,16303,nancy ide,Sixth Workshop on Very Large Corpora,0,None
P98-1044,Veins Theory: A Model of Global Discourse Cohesion and Coherence,1998,24,82,2,1,14259,dan cristea,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"In this paper, we propose a generalization of Centering Theory (CT) (Grosz, Joshi, Weinstein (1995)) called Veins Theory (VT), which extends the applicability of centering rules from local to global discourse. A key facet of the theory involves the identification of xc2xabveinsxc2xbb over discourse structure trees such as those defined in RST, which delimit domains of referential accessibility for each unit in a discourse. Once identified, reference chains can be extended across segment boundaries, thus enabling the application of CT over the entire discourse. We describe the processes by which veins are defined over discourse structure trees and how CT can be applied to global discourse by using these chains. We also define a discourse xc2xabsmoothnessxc2xbb index which can be used to compare different discourse structures and interpretations, and show how VT can be used to abstract a span of text in the context of the whole discourse. Finally, we validate our theory by analyzing examples from corpora of English, French, and Romanian."
P98-1050,{M}ultext-{E}ast: Parallel and Comparable Corpora and Lexicons for Six {C}entral and {E}astern {E}uropean Languages,1998,5,70,3,0,44040,ludmila dimitrova,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 1",1,"The EU Copernicus project Multext-East has created a multi-lingual corpus of text and speech data, covering the six languages of the project: Bulgarian, Czech, Estonian, Hungarian, Romanian, and Slovene. In addition, wordform lexicons for each of the languages were developed. The corpus includes a parallel component consisting of Orwell's Nineteen Eighty-Four, with versions in all six languages tagged for part-of-speech and aligned to English (also tagged for POS). We describe the encoding format and data architecture designed especially for this corpus, which is generally usable for encoding linguistic corpora. We also describe the methodology for the development of a harmonized set of morphosyntactic descriptions (MSDs), which builds upon the scheme for western European languages developed within the EAGLES project. We discuss the special concerns for handling the six project languages, which cover three distinct language families."
J98-2008,Book Reviews: Text Databases: One Database Model and Several Retrieval Languages,1998,0,2,1,1,16303,nancy ide,Computational Linguistics,0,None
J98-1001,Introduction to the Special Issue on Word Sense Disambiguation: The State of the Art,1998,224,669,1,1,16303,nancy ide,Computational Linguistics,0,"Compte tenu du progres effectue recemment dans le domaine de la desambiguisation du sens des mots, les As. font ici le point sur l'etat de la recherche dans ce domaine depuis ces 50 dernieres annees et considerent les prochaines etapes a franchir. Dans un 1 e r temps, ils passent en revue les principales approches de la desambiguisation du sens des mots : des premieres tentatives effectuees dans le cadre de la traduction automatique aux methodes actuelles basees sur les corpus, en passant par les methodes basees sur l'intelligence artificielle et les methodes utilisant des bases de connaissance. Dans un 2 n d temps, ils examinent les problemes laisses en suspens (le role du contexte, la division des sens, l'evaluation des resultats) et proposent quelques orientations pour la recherche future"
C98-1044,Veins Theory: A Model of Global Discourse Cohesion and Coherence,1998,24,82,2,1,14259,dan cristea,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"In this paper, we propose a generalization of Centering Theory (CT) (Grosz, Joshi, Weinstein (1995)) called Veins Theory (VT), which extends the applicability of centering rules from local to global discourse. A key facet of the theory involves the identification of xc2xabveinsxc2xbb over discourse structure trees such as those defined in RST, which delimit domains of referential accessibility for each unit in a discourse. Once identified, reference chains can be extended across segment boundaries, thus enabling the application of CT over the entire discourse. We describe the processes by which veins are defined over discourse structure trees and how CT can be applied to global discourse by using these chains. We also define a discourse xc2xabsmoothnessxc2xbb index which can be used to compare different discourse structures and interpretations, and show how VT can be used to abstract a span of text in the context of the whole discourse. Finally, we validate our theory by analyzing examples from corpora of English, French, and Romanian."
C98-1049,Multext-East: Parallel and Comparable Corpora and Lexicons for Six Central and {E}astern {E}uropean Languages,1998,5,70,3,0,44040,ludmila dimitrova,{COLING} 1998 Volume 1: The 17th International Conference on Computational Linguistics,0,"The EU Copernicus project Multext-East has created a multi-lingual corpus of text and speech data, covering the six languages of the project: Bulgarian, Czech, Estonian, Hungarian, Romanian, and Slovene. In addition, wordform lexicons for each of the languages were developed. The corpus includes a parallel component consisting of Orwell's Nineteen Eighty-Four, with versions in all six languages tagged for part-of-speech and aligned to English (also tagged for POS). We describe the encoding format and data architecture designed especially for this corpus, which is generally usable for encoding linguistic corpora. We also describe the methodology for the development of a harmonized set of morphosyntactic descriptions (MSDs), which builds upon the scheme for western European languages developed within the EAGLES project. We discuss the special concerns for handling the six project languages, which cover three distinct language families."
C94-1094,Encoding standards for large text resources: The {T}ext {E}ncoding {I}nitiative,1994,5,2,1,1,16303,nancy ide,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"The Text Encoding Initiative (TEI) is an international project established in 1988 to develop guidelines for the preparation and interchange of electronic texts for research, and to satisfy a broad range of uses by the language industries more generally. The need for standardized encoding practices has become inxreasingly critical as the need to use and, most importantly, reuse vast amounts of electronic text has dramatically increased for both research and industry, in particular for natural language processing. In January 1994, the TEI issued its Guidelines for the Encoding and Interchange of Machine-Readable Texts, which provide standardized encoding conventions for a large range of text types and features relevant for a broad range of applications."
C94-1097,{MULTEXT}: Multilingual Text Tools and Corpora,1994,4,79,1,1,16303,nancy ide,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"MULTEXT (Multilingual Text Tools and Corpora) is the largest project funded in the Commission of European Communities Linguistic Research and Engineering Program. The project will contribute to the development of generally usable software tools to manipulate and analyse text corpora and to create multilingual text corpora with structural and linguistic markup. It will attempt to establish conventions for the encoding of such corpora, building on and contributing to the preliminary recommendations of the relevant international and European standardization initiatives. MULTEXT will also work towards establishing a set of guidelines for text software development, which will be widely published in order to enable future development by others. All tools and data developed within the project will be made freely and publicly available."
1993.eamt-1.2,Knowledge extraction from machine-readable dictionaries: an evaluation,1993,25,26,1,1,16303,nancy ide,Third International EAMT Workshop: Machine Translation and the Lexicon,0,"Machine-readable versions of everyday dictionaries have been seen as a likely source of information for use in natural language processing because they contain an enormous amount of lexical and semantic knowledge. However, after 15 years of research, the results appear to be disappointing. No comprehensive evaluation of machine-readable dictionaries (MRDs) as a knowledge source has been made to date, although this is necessary to determine what, if anything, can be gained from MRD research. To this end, this paper will first consider the postulates upon which MRD research has been based over the past fifteen years, discuss the validity of these postulates, and evaluate the results of this work. We will then propose possible future directions and applications that may exploit these years of effort, in the light of current directions in not only NLP research, but also fields such as lexicography and electronic publishing."
C92-2089,A Feature-Based Model for Lexical Databases,1992,18,10,2,0,50383,jean veronis,{COLING} 1992 Volume 2: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,"To date, no fully suitable data model for lexical databases has been proposed. As lexical databases have proliferated in multiple formats, there has been growing concern over the reusability of lexical resources. In this paper, we propose a model based on feature structures which overcomes most of the problems inherent in classical database models, and in particular enables accessing, manipulating or merging information structured in multiple ways. Because of their widespread use in the representation of linguistic information, the applicability of feature structures to lexical databases seems natural, although to our knowledge this has not yet been implemented. The use of feature structures in lexical databases also opens up the possibility of compatibility with computational lexicons."
E91-1040,An Assessment of Semantic Information Automatically Extracted From Machine Readable Dictionaries,1991,10,23,2,0,50383,jean veronis,Fifth Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"In this paper we provide a quantitative evaluation of information automatically extracted from machine readable dictionaries. Our results show that for any one dictionary, 55--70% of the extracted information is garbled in some way. However, we show that these results can be dramatically reduced to about 6% by combining the information extracted from five dictionaries. It therefore appears that even if individual dictionaries are an unreliable source of semantic information, multiple dictionaries can play an important role in building large lexical-semantic databases."
C90-2067,Word Sense Disambiguation with Very Large Neural Networks Extracted from Machine Readable Dictionaries,1990,10,145,2,0,50383,jean veronis,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"In this paper, we describe a means for automatically building very large neural networks (VLNNs) from definition texts in machine-readable dictionaries, and demonstrate the use of these networks for word sense disambiguation. Our method brings together two earlier, independent approaches to word sense disambiguation: the use of machine-readable dictionaries and spreading and activation models. The automatic construction of VLNNs enables real-size experiments with neural networks for natural language processing, which in turn provides insight into their behaviour and design and can lead to possible improvements."
