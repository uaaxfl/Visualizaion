2009.mtsummit-wpt.3,P91-1022,0,0.544113,"ntence alignment in Sec. 4., and introduce sentence filtering, including the evaluation of its impact on SMT in Sec. 5 as well as the final parallel corpus in Sec. 6, and conclude this paper. http://www.itl.nist.gov/iad/mig/tests/mt/ Revised on 2009/08/06 -17- 2 Related Work To get parallel sentences from parallel corpora, different approaches can be used for sentence alignment. The approaches can be based on a) sentence length, b) lexical information in bilingual dictionaries, c) statistical translation model, or d) the composite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly d"
2009.mtsummit-wpt.3,P93-1002,0,0.601926,"mposite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the w"
2009.mtsummit-wpt.3,P91-1023,0,0.379605,"Sec. 4., and introduce sentence filtering, including the evaluation of its impact on SMT in Sec. 5 as well as the final parallel corpus in Sec. 6, and conclude this paper. http://www.itl.nist.gov/iad/mig/tests/mt/ Revised on 2009/08/06 -17- 2 Related Work To get parallel sentences from parallel corpora, different approaches can be used for sentence alignment. The approaches can be based on a) sentence length, b) lexical information in bilingual dictionaries, c) statistical translation model, or d) the composite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment"
2009.mtsummit-wpt.3,2001.mtsummit-papers.30,0,0.717795,"Missing"
2009.mtsummit-wpt.3,P07-2045,0,0.00531596,"icate fusing strategies than simply using average or multiplication. Filter is shown to be the best among all ensemble methods, which can be explained by the good filtering effects of Len and DictN for misaligned sentences among the highly ranked sentence pairs in the sorted list of Tran. 5.3 Impact of Sentence Filtering on SMT Although the experiment shows that sentence filtering can help identify really parallel sentences, we may wonder whether the sentence filtering actually leads to better SMT performance. Therefore, we evaluated the impact of sentence filtering on SMT. The Moses toolkit (Koehn et al., 2007) was used to conduct Chinese-&gt;English SMT experiments and BLEU and NIST scores are used as the evaluation metrics. We followed the instruction of the baseline system for the shared task in the 2008 ACL workshop on SMT. 9 8 Before the ensemble of individual scores, we first need to normalize the scores into the range between 0 and 1 according to their distributions: the length-based and dictionary-based scores are already within the range; the translation score roughly follows a linear distribution. The weights for Tran, Len, DictN are 99, 30 and 16, respectively. They are got by the exhaustive"
2009.mtsummit-wpt.3,2005.mtsummit-papers.11,0,0.131317,"Missing"
2009.mtsummit-wpt.3,ma-2006-champollion,0,0.401073,"g/tests/mt/ Revised on 2009/08/06 -17- 2 Related Work To get parallel sentences from parallel corpora, different approaches can be used for sentence alignment. The approaches can be based on a) sentence length, b) lexical information in bilingual dictionaries, c) statistical translation model, or d) the composite of more than one approach. The sentence-length-based approach (Brown et al. 1991; Gale and Church, 1991) aligns sentences based on the number of words or characters in each sentence. Dictionary-based techniques use extensive online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does"
2009.mtsummit-wpt.3,moore-2002-fast,0,0.792182,"e online bilingual lexicons to match sentences. For instance, Ma (2006) described Champollion, a lexicon-based sentence aligner designed for robust alignment of potential noisy parallel text, and increased the robustness of the alignment by assigning greater weights to less frequent translated words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and"
2009.mtsummit-wpt.3,J03-1002,0,0.0138393,"Missing"
2009.mtsummit-wpt.3,J05-4003,0,0.0820068,"ord-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a similarity measure, and selected only the highly ranked aligned sentence alignments. Although the construction of our ChineseE"
2009.mtsummit-wpt.3,J03-3002,0,0.0858242,"words. Statistical translation model is also used for sentence alignment. Chen (1993) constructed a simple statistical word-to-word translation model on the fly during sentence alignment and then found the alignment that maximizes the probability of generating the corpus. Simard and Plamondon (1998) and Moore (2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a sim"
2009.mtsummit-wpt.3,2007.mtsummit-papers.63,0,0.166161,"c, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a similarity measure, and selected only the highly ranked aligned sentence alignments. Although the construction of our ChineseEnglish patent parallel corpus is similar to that of the Japanese-English one (Utiyama and Isahara, 2007), we have made the following modifications on the basis of our data: 1) all sections of the patents, instead of only two parts in the description section, were used to find sentence alignments; 2) for sentence filtering, we integrated three individual measures, including the dictionary-based one (Utiyama and Isahara, 2007), and the experiments showed the combination of measures can improve the performance of sentence filtering. We also did SMT experiments, showing that filtering out misaligned sentences could improve SMT performance. 3 The Chinese-English Patents Parallel We use about 7000 Chi"
2009.mtsummit-wpt.3,D08-1058,0,0.0160422,"S c and S e respectively. 3) The bidirectional translation probability score Pt (Tran): it combines the translation probability value of both directions (i.e. Chinese-&gt;English and English-&gt;Chinese), instead of using only one direction (Moore, 2002; Chen, 2003). It is computed as follows: log ( P(S e |S c ))  log ( P(S c |S e )) lc  le where P( S e |S c ) denotes the probability that a translator will produce S e in English when presented with S c in Chinese, and vice versa for P(Sc |Se ) . pt ( S c , S e )  A wide variety of ensemble methods have been used in various fields (Polikar, 2006; Wan, 2008). -20- We evaluate the following8: 1) Average (Avg): the average of the individual scores; 2) Multiplication (Mul): the product of the individual scores; 3) Linear Combination (LinC): the weighted average by associating each individual score with a weight, indicating the relative confidence in the value; 4) Filter: use Pt for sorting, but if Pd or Pt of a sentence pair is lower than a predefined threshold, that pair will be moved to the end of the sorting list. The thresholds can be empirically set based on the data. 5.2 Empirical Evaluation of Sentence Filtering To assess the performance of i"
2009.mtsummit-wpt.3,I05-1023,0,0.0259188,"(2002) both used a composite method in which the first pass does alignment at the sentence length level and the second pass uses IBM Model-1. Non-parallel corpora or comparable corpora, in addition to clean, ideal parallel corpora, are also used to mine parallel sentences. For instance, Resnik and Smith (2003) introduced the STRAND system for mining parallel text on the web for low-density language pairs. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Wu and Fung (2005) exploited Inversion Transduction Grammar to retrieve truly parallel sentence translations from large collections of highly non-parallel docuements. Utiyama and Isahara (2003) aligned articles and sentences from noisy parallel news articles, then sorted the aligned sentences according to a similarity measure, and selected only the highly ranked aligned sentence alignments. Although the construction of our ChineseEnglish patent parallel corpus is similar to that of the Japanese-English one (Utiyama and Isahara, 2007), we have made the following modifications on the basis of our data: 1) all sec"
2009.mtsummit-wpt.3,J93-1004,0,\N,Missing
2009.mtsummit-wpt.3,J93-2003,0,\N,Missing
2009.mtsummit-wpt.3,P03-1010,0,\N,Missing
2011.mtsummit-papers.13,2008.amta-papers.2,0,0.0702532,"ment contexts into current SMT systems for document-level translation. In particular, we focus on translation consistency which is one of the most important issues in document-level MT. We propose a 3-step approach to incorporating document contexts into a traditional SMT system, and demonstrate that our approach can effectively reduce the errors caused by inconsistent translation. More interestingly, it is observed that using document contexts is promising for BLEU improvement. 2 Related Work To date, only a few studies have improved MT systems with the use of document contexts. For example, Brown (2008) proposed a method to improve SMT and Example-Based Machine Translation (EBMT) systems using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent featu"
2011.mtsummit-papers.13,P05-1048,0,0.0299149,"slation (EBMT) systems using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chinese word segmentation. 3 Document-level Consistency Verification Given a source document Df, the task of document-level SMT is to find an optimal tar"
2011.mtsummit-papers.13,2007.tmi-papers.6,0,0.0313509,"using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chinese word segmentation. 3 Document-level Consistency Verification Given a source document Df, the task of document-level SMT is to find an optimal target document De* by: D"
2011.mtsummit-papers.13,P07-1005,0,0.0660032,"similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chinese word segmentation. 3 Document-level Consistency Verification Given a source document Df, the task of document-level SMT is to find an optimal target document De* by: De* arg max Pr( De |D"
2011.mtsummit-papers.13,P07-2045,0,0.00568597,"manually removed the candidate checkpoints where the consistency of translation is not strongly required. The system was evaluated by the number of errors at checkpoints. We also reported the BLEU(-SBP) (Chiang et al., 2008) score to show the impact of our approach on translation accuracy. 4 4.1 Experiments Baseline System Our experiments were conducted on ChineseEnglish translation based on the open-source phrase-based MT system NiuTrans 5 . The NiuTrans uses two reordering models, including a maximum entropy-based lexicalized reordering model (Xiong et al., 2006) and a MSD reordering model (Koehn et al., 2007). In addition, it adopts all standard features used in the state-of-the-art SMT system Moses (Koehn et al., 2007), such as bi-directional phrase translation probabilities and n-gram language model. The feature weights were optimized using MERT (Och, 2003). By default, the distortion limit was set to 8, k was set to 1 (Equations 3, 5-7), and D was set to 0 (Equation 5). 4.3 5 Results in Default Settings We first investigate the effectiveness of our methods on error reduction in the default settings. Table 1 compares various methods in terms of the number of errors at checkpoints, where Post and"
2011.mtsummit-papers.13,P03-1021,0,0.0566132,"h on translation accuracy. 4 4.1 Experiments Baseline System Our experiments were conducted on ChineseEnglish translation based on the open-source phrase-based MT system NiuTrans 5 . The NiuTrans uses two reordering models, including a maximum entropy-based lexicalized reordering model (Xiong et al., 2006) and a MSD reordering model (Koehn et al., 2007). In addition, it adopts all standard features used in the state-of-the-art SMT system Moses (Koehn et al., 2007), such as bi-directional phrase translation probabilities and n-gram language model. The feature weights were optimized using MERT (Och, 2003). By default, the distortion limit was set to 8, k was set to 1 (Equations 3, 5-7), and D was set to 0 (Equation 5). 4.3 5 Results in Default Settings We first investigate the effectiveness of our methods on error reduction in the default settings. Table 1 compares various methods in terms of the number of errors at checkpoints, where Post and Rede stand for the post-editing and the re-decoding methods used in final translation generation (Step 3) respectively, M1 and M2 stand for the two counting methods shown in Equations (6-7). We see that all our proposed methods are effective to reduce th"
2011.mtsummit-papers.13,P94-1019,0,0.0549308,"ave improved MT systems with the use of document contexts. For example, Brown (2008) proposed a method to improve SMT and Example-Based Machine Translation (EBMT) systems using document-level similarity between the documents in the training corpus and the input document. Another example is (Zhao and Xing, 2007) in which a bilingual topic model was proposed to capture the document-level topical aspects of SMT. However, no previous work has addressed the issue of translation consistency in document-level MT. The problem discussed in this paper is similar to the lexical selection problem in SMT (Wu and Palmer, 1994). There have been some attempts at using context-dependent features to select appropriate target lexical items for SMT systems (Carpuat and Wu, 2005; Carpuat and Wu, 2007; Chan et al., 2007). However, these studies were all in the scenario of sentence-level MT. By contrast, we focus more on using document contexts to address the issue in document translation. Actually, the translation consistency issue has been discussed in some related tasks. For example, Wang et al. (2007)’s work showed that consistency information was very helpful in dealing with the out-ofvocabulary (OOV) problem for Chine"
2011.mtsummit-papers.13,P06-1066,0,0.0486244,"ords7 as the candidate checkpoints; 2) and then manually removed the candidate checkpoints where the consistency of translation is not strongly required. The system was evaluated by the number of errors at checkpoints. We also reported the BLEU(-SBP) (Chiang et al., 2008) score to show the impact of our approach on translation accuracy. 4 4.1 Experiments Baseline System Our experiments were conducted on ChineseEnglish translation based on the open-source phrase-based MT system NiuTrans 5 . The NiuTrans uses two reordering models, including a maximum entropy-based lexicalized reordering model (Xiong et al., 2006) and a MSD reordering model (Koehn et al., 2007). In addition, it adopts all standard features used in the state-of-the-art SMT system Moses (Koehn et al., 2007), such as bi-directional phrase translation probabilities and n-gram language model. The feature weights were optimized using MERT (Och, 2003). By default, the distortion limit was set to 8, k was set to 1 (Equations 3, 5-7), and D was set to 0 (Equation 5). 4.3 5 Results in Default Settings We first investigate the effectiveness of our methods on error reduction in the default settings. Table 1 compares various methods in terms of the"
2020.acl-main.322,N18-1118,0,0.194803,"NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT FrEn task by careful use of noise generation and dropout methods. 1 Introduction Sentence-level neural machine translation (NMT) systems ignore the discourse phenomena and encode the individual source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale docume"
2020.acl-main.322,P18-1163,0,0.0186764,"lts are reported on small training datasets. Here we examine the effects of the noise-based method on different sized datasets. We trained the Inside-Random model and the Gaussiannoise model on different datasets consisting of 500K to 5M sentence pairs. Seen from Figure 2, the baseline model achieves better translation performance when we increase the data size. More interestingly, it is observed that Inside-Random and Gaussian-noise perform slightly better than 3515 BLEU 24 Base Inside ments mainly come from the robust training instead of the leverage of contextual information. Additionally, Cheng et al. (2018) added the Gaussian noise to word embedding to simulate lexical-level perturbations for more robust training. Differently, we added the Gaussian noise to the encoder output which plays a similar role with context-encoder, which provides additional training signals. Gaussian 22 20 18 500k 1M 2M 5M Data Volume Figure 2: BLEU scores vs. different data volume on ZhEn sentence-level dataset. dropout = 0.1 and σ = 0.3. the baseline, and the gaps gradually decrease with the volume increasing. This is reasonable that models trained on large-scale data may suffer less from the overfitting problem. 5 Re"
2020.acl-main.322,D19-6503,0,0.513442,"Missing"
2020.acl-main.322,P18-1118,0,0.497129,"ll. Also, we establish a new state-of-the-art on IWSLT FrEn task by careful use of noise generation and dropout methods. 1 Introduction Sentence-level neural machine translation (NMT) systems ignore the discourse phenomena and encode the individual source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily available. Most context-"
2020.acl-main.322,N19-1313,0,0.476841,"new state-of-the-art on IWSLT FrEn task by careful use of noise generation and dropout methods. 1 Introduction Sentence-level neural machine translation (NMT) systems ignore the discourse phenomena and encode the individual source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily available. Most context-aware NMT systems are"
2020.acl-main.322,D18-1325,0,0.417812,"the training data is small. Also, we establish a new state-of-the-art on IWSLT FrEn task by careful use of noise generation and dropout methods. 1 Introduction Sentence-level neural machine translation (NMT) systems ignore the discourse phenomena and encode the individual source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily"
2020.acl-main.322,N19-4009,0,0.0376768,"larger EnglishRussian (En-Ru) dataset provided by Voita et al. (2018), consisting of 2M sentence pairs selected from publicly available OpenSubtitles2018 corpus. The data statistics of each language pair can be seen in Table 1. We chose the Transformer-base model as the sentence-level baseline. The context encoder also used the same setting as the sentence-level baseline. We used Adam (Kingma and Ba, 2014) for optimization, and trained the systems on a single TiTan V GPU4 . The learning rate strategy was the same as that used in Vaswani et al. (2017). Our implementation was based on Fairseq (Ott et al., 2019). More details can be found in our repository5 . 4 Results and Discussion To study whether the context-encoder network captures contextual information in training, we present three types of context as the input of the contextencoder: • Context: the previous sentence of the current sentence. • Random: a sentence consisting of words randomly sampled from the source vocabulary. • Fixed: a fixed sentence input for contextencoder. 4.1 Baseline Selection Weight sharing (Voita et al., 2018) and two-stage training (Zhang et al., 2018) strategies have been proven essential to build strong context-aware"
2020.acl-main.322,P16-1162,0,0.0932916,"y available datasets. For ChineseEnglish (Zh-En) and French-English (Fr-En), we used Ted talks from IWSLT15 and IWSLT16 (Cettolo et al., 2012) evaluation campaigns as the training data. We validated on dev2010, and tested on tst2010-2013 (Zh-En), tst2010 (Fr-En) respectively. For English-German (En-De), we evaluated on WMT18 task 1 . For more convincing results, we also randomly sampled 500k/1M/2M/5M sentence pairs from the Chinese-English corpus provided by WMT2 and test on newstest2017. We preprocessed the sentences with Moses tokenizer3 except Chinese sentences and used byte pair encoding (Sennrich et al., 2016) with 32K merged operations to 3513 1 We used the News-Commentary v14 as the train set http://www.statmt.org/wmt19/translation-task.html 3 http://www.statmt.org/moses 2 Lang. Zh-En Fr-En En-De En-Ru Train Valid Test doc. sent. doc. sent. doc. sent. 1708 209K 8 887 56 5473 1803 220K 8 887 11 1664 8462 329K 130 3004 122 2998 - 2M - 10k - 10k Table 1: Details of datasets on different language pairs. segment words into sub-word units. The Chinese sentences were word segmented by the tool provided within NiuTrans (Xiao et al., 2012). For Fr-En and Zh-En tasks, we lowercased all sentences to obtain"
2020.acl-main.322,P19-1021,0,0.0194415,"tention network to model the contextual information. Maruf and Haffari (2018) built a context-aware NMT system using a memory network, and Maruf et al. (2019) encoded the whole document with selective attention network. However, most of the work mentioned above utilized more complex modules to capture the contextual information, which can be approximately regarded as multi-encoder systems. For a fair evaluation of context-aware NMT methods, we argue that one should build a strong enough sentence-level baseline with carefully regularized methods, especially on small datasets (Kim et al., 2019; Sennrich and Zhang, 2019). Beyond this, Bawden et al. (2018) and Voita et al. (2019) acknowledged that BLEU score is insufficient to evaluate context-aware models, and they emphasized that multi-encoder architectures alone had a limited capacity to exploit discourse-level context. In this work, we take a further step to explore the main cause, showing that the context-encoder acts more like a noise generator, and the BLEU improve6 Conclusions We have shown that, in multi-encoder contextaware NMT, the BLEU improvement is not attributed to the leverage of contextual information. Even though we feed the incorrect context"
2020.acl-main.322,W17-4811,0,0.431167,"source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily available. Most context-aware NMT systems are evaluated on small datasets and significant BLEU improvements are reported (Wang et al., 2017; Zhang et al., 2018; Tu et al., 2018). In our experiments, we find that the improvement persists if we feed pseudo sentences into the cont"
2020.acl-main.322,Q18-1029,0,0.12793,"nd the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily available. Most context-aware NMT systems are evaluated on small datasets and significant BLEU improvements are reported (Wang et al., 2017; Zhang et al., 2018; Tu et al., 2018). In our experiments, we find that the improvement persists if we feed pseudo sentences into the context encoder, especially when we train the system on small-scale data. A natural question here is: How much does the improvement come from the leverage of contextual information in multi-encoder? In this work, we aim to investigate what kinds of information that the context-aware model captures. We re-implement several widely used context-aware architectures based on the multiencoder paradigm, and do an in-depth analysis to study whether the context encoder captures the contextual information. B"
2020.acl-main.322,P19-1116,0,0.218892,"Missing"
2020.acl-main.322,P18-1117,0,0.518113,"sy training plays an important role in multi-encoder-based NMT, especially when the training data is small. Also, we establish a new state-of-the-art on IWSLT FrEn task by careful use of noise generation and dropout methods. 1 Introduction Sentence-level neural machine translation (NMT) systems ignore the discourse phenomena and encode the individual source sentences with no use of contexts. In recent years, the context-aware models which learn contextual information from surrounding sentences have shown promising results in generating consistent and coherent translations (Zhang et al., 2018; Voita et al., 2018; Kim et al., 2019; Voita et al., 2019; Bawden et al., 2018; Miculicich et al., 2018; Maruf and Haffari, 2018; Maruf et al., 2019). There are two common approaches to incorporating contexts into NMT: the simple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter"
2020.acl-main.322,D17-1301,0,0.0505238,"ple way is to concatenate the context and the current sentence ∗ to form a context-aware input sequence (Agrawal et al., 2018; Tiedemann and Scherrer, 2017), whereas a more widely-used approach utilizes additional neural networks to encode context sentences (Jean et al., 2017; Voita et al., 2018; Zhang et al., 2018). Here we name the former as the single-encoder approach and name the latter as the multi-encoder approach. However, large-scale document corpora are not easily available. Most context-aware NMT systems are evaluated on small datasets and significant BLEU improvements are reported (Wang et al., 2017; Zhang et al., 2018; Tu et al., 2018). In our experiments, we find that the improvement persists if we feed pseudo sentences into the context encoder, especially when we train the system on small-scale data. A natural question here is: How much does the improvement come from the leverage of contextual information in multi-encoder? In this work, we aim to investigate what kinds of information that the context-aware model captures. We re-implement several widely used context-aware architectures based on the multiencoder paradigm, and do an in-depth analysis to study whether the context encoder"
2020.acl-main.322,P12-3004,1,0.784377,"kenizer3 except Chinese sentences and used byte pair encoding (Sennrich et al., 2016) with 32K merged operations to 3513 1 We used the News-Commentary v14 as the train set http://www.statmt.org/wmt19/translation-task.html 3 http://www.statmt.org/moses 2 Lang. Zh-En Fr-En En-De En-Ru Train Valid Test doc. sent. doc. sent. doc. sent. 1708 209K 8 887 56 5473 1803 220K 8 887 11 1664 8462 329K 130 3004 122 2998 - 2M - 10k - 10k Table 1: Details of datasets on different language pairs. segment words into sub-word units. The Chinese sentences were word segmented by the tool provided within NiuTrans (Xiao et al., 2012). For Fr-En and Zh-En tasks, we lowercased all sentences to obtain comparable results with previous work. We also conducted experiments on a larger EnglishRussian (En-Ru) dataset provided by Voita et al. (2018), consisting of 2M sentence pairs selected from publicly available OpenSubtitles2018 corpus. The data statistics of each language pair can be seen in Table 1. We chose the Transformer-base model as the sentence-level baseline. The context encoder also used the same setting as the sentence-level baseline. We used Adam (Kingma and Ba, 2014) for optimization, and trained the systems on a si"
2020.acl-main.322,D18-1049,0,0.278296,"Missing"
2020.acl-main.322,J96-1002,0,\N,Missing
2020.acl-main.322,2012.eamt-1.60,0,\N,Missing
2020.acl-main.592,N18-1127,0,0.0542982,"Missing"
2020.acl-main.592,P19-1285,0,0.0631671,"Missing"
2020.acl-main.592,N19-1078,0,0.0113589,"1 × 10−4 . A larger window size (= 70) for BPTT was applied for the WikiText103. All experiments were run on a single NVIDIA 1080Ti. After the search process, we trained the learned architectures on the same data. To make it comparable with previous work, we copied the setup in Merity et al. (2018b). For PTB, the size of hidden layers was set as 850 and the training epoch was 3,000. While for the WikiText-103, we enlarged the number of hidden units to 2,500 and trained the model for 30 epochs. Additionally, we transferred the learned architecture to NER and chunking tasks with the setting in Akbik et al. (2019). We only modified the batch size to 24 and hidden size to 512. 5.2 5.2.1 Results Language Modeling tasks Here we report the perplexity scores, number of parameters and search cost on the PTB and WikiText103 datasets (Table 2). First of all, the joint ESS method improves the performance on language modeling tasks significantly. Moreover, it does not introduce many parameters. Our ESS method achieves state-of-the-art result on the PTB task. It outperforms the manually designed MogrifierLSTM by 4.5 perplexity scores on the test set. On 6634 Perplexity Perplexity 63.5 NAS 700 joint intra 550 400"
2020.acl-main.592,D19-1539,0,0.04279,"Missing"
2020.acl-main.592,D19-1367,1,0.86799,"ecture on other tasks. Again, it shows promising improvements on both NER and chunking benchmarks, and yields new state-of-the-art results on NER tasks. This indicates a promising line of research on largescale pre-learned architectures. More interestingly, it is observed that the inter-cell NAS is helpful in modeling rare words. For example, it yields a bigger improvement on the rare entity recognition task (WNUT) than that on the standard NER task (CoNLL). Related work NAS is a promising method toward AutoML (Hutter et al., 2018), and has been recently applied to NLP tasks (So et al., 2019; Jiang et al., 2019; Li and Talwalkar, 2019). Several research teams have investigated search strategies for NAS. The very early approaches adopted evolutionary algorithms to model the problem (Angeline et al., 1994; Stanley and Miikkulainen, 2002), while Bayesian and reinforcement learning methods made big progresses in computer vision and NLP later (Bergstra et al., 2013; Baker et al., 2017; Zoph and Le, 2017). More recently, gradient-based methods were successfully applied to language modeling and image classification based on RNNs and CNNs (Liu et al., 2019a). In particular, differentiable architecture searc"
2020.acl-main.592,N16-1030,0,0.143047,"Missing"
2020.acl-main.592,P19-1233,0,0.232166,"n recently applied to NLP tasks (So et al., 2019; Jiang et al., 2019; Li and Talwalkar, 2019). Several research teams have investigated search strategies for NAS. The very early approaches adopted evolutionary algorithms to model the problem (Angeline et al., 1994; Stanley and Miikkulainen, 2002), while Bayesian and reinforcement learning methods made big progresses in computer vision and NLP later (Bergstra et al., 2013; Baker et al., 2017; Zoph and Le, 2017). More recently, gradient-based methods were successfully applied to language modeling and image classification based on RNNs and CNNs (Liu et al., 2019a). In particular, differentiable architecture search has been of great interest to the community because of its efficiency and compatibility to off-the-shelf tools of gradient-based optimization. Despite of great success, previous studies restricted themselves to a small search space of neural networks. For example, most NAS systems were designed to find an architecture of recurrent or convolutional cell, but the remaining parts of the network are handcrafted (Zhong et al., 2018; Brock et al., 2018; Elsken et al., 2019). For a larger search space, Zoph et al. (2018) optimized the normal cell"
2020.acl-main.592,N18-1202,0,0.0569163,"Missing"
2020.acl-main.592,N18-1117,0,0.0638926,"Missing"
2020.acl-main.592,P19-1176,1,0.891921,"Missing"
2020.acl-main.592,C18-1255,1,0.834066,"ock et al., 2018; Elsken et al., 2019). For a larger search space, Zoph et al. (2018) optimized the normal cell (i.e., the cell that preserves the dimensionality of the input) and reduction cell (i.e., the cell that reduces the spatial dimension) simultaneously and explored a larger region of the space than the singlecell search. But it is still rare to see studies on the issue of search space though it is an important factor to NAS. On the other hand, it has been proven that the additional connections between cells help in RNN or Transformer-based models (He et al., 2016; Huang et al., 2017; Wang et al., 2018, 2019). These results motivate us to take a step toward the automatic design of inter-cell connections and thus search in a larger space of neural architectures. 3 Inter-Cell and Intra-Cell NAS In this work we use RNNs for description. We choose RNNs because of their effectiveness at preserving past inputs for sequential data processing tasks. Note that although we will restrict ourselves to RNNs for our experiments, the method and discussion here can be applied to other types of models. 6630 3.1 Problem Statement F (α, β) For a sequence of input vectors {x1 , ..., xT }, an RNN makes a cell o"
2020.acl-main.592,D18-1279,0,0.0507525,"Missing"
2020.acl-main.592,P18-4013,0,0.0521793,"Missing"
2020.coling-main.352,P17-1176,0,0.0171192,"rce NMT Koehn and Knowles (2017) show that NMT systems result in worse translation performance in lowresource scenarios. Researchers have developed promising approaches to this problem which mainly focus on introducing external knowledge to improve low-resource NMT performance. Data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017) alleviates this problem by generating pseudo parallel data. A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperp"
2020.coling-main.352,P16-1185,0,0.0451091,"urce machine translation benchmarks and different sized data of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009"
2020.coling-main.352,D19-5611,0,0.0191281,"ve low-resource NMT performance. Data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017) alleviates this problem by generating pseudo parallel data. A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-resource scenarios without any external resources. This is consistent with our motivation. The difference lies in that they focus on the model settings, and we explore the training strategy which utilizes bilingual data effectively fo"
2020.coling-main.352,N19-1423,0,0.167108,"ted impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009; Kumar et al., 2010; Shrivastava et al., 2016). For example, Arpit et al. (2017) have pointed out that deep neural networks tend to prioritize learnin"
2020.coling-main.352,P17-2090,0,0.170636,"tion benchmarks and different sized data of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009; Kumar et al., 2010;"
2020.coling-main.352,N18-1032,0,0.0170843,"nowles (2017) show that NMT systems result in worse translation performance in lowresource scenarios. Researchers have developed promising approaches to this problem which mainly focus on introducing external knowledge to improve low-resource NMT performance. Data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017) alleviates this problem by generating pseudo parallel data. A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-"
2020.coling-main.352,D18-1398,0,0.0159213,"nowles (2017) show that NMT systems result in worse translation performance in lowresource scenarios. Researchers have developed promising approaches to this problem which mainly focus on introducing external knowledge to improve low-resource NMT performance. Data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017) alleviates this problem by generating pseudo parallel data. A large amount of auxiliary parallel corpus from other related language pairs can be used to pre-train model parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-"
2020.coling-main.352,kocmi-bojar-2017-curriculum,0,0.183166,", the neural network explores harder samples effectively utilizing the previous knowledge learned from easier samples. Weinshall et al. (2018) demonstrate curriculum learning speeds up the learning process, especially at the beginning of training. Curriculum learning has been applied to several tasks, including language modeling (Bengio et al., 2009), image classification (Weinshall et al., 2018), and human attribute analysis (Wang et al., 2019). Curriculum learning has recently shown to train large-scale translation tasks efficiently and effectively by controlling the way of feeding samples. Kocmi and Bojar (2017) construct mini-batch contains sentences similar in length and linguistic phenomena, then organize the order by increased complexity in one epoch. Zhang et al. (2018) group the training samples into shards based on model-based and linguistic difficulty criteria, then train with hand-crafted curriculum schedules. Platanios et al. (2019) propose competence-based curriculum learning that select training samples based on sample difficulty and model competence. Kumar et al. (2019) use reinforcement learning to learn the curriculum automatically. Liu et al. (2020) propose a norm-based curriculum lea"
2020.coling-main.352,W18-6325,0,0.0829692,"Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009; Kumar et al., 2010; Shrivastava et al., 2016). For example, Arpit et al. (2017) ha"
2020.coling-main.352,W17-3204,0,0.151456,"e to learn. We test our DCL method in a Transformerbased system. Experimental results show that DCL outperforms several strong baselines on three low-resource machine translation benchmarks and different sized data of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the"
2020.coling-main.352,P07-2045,0,0.0212126,"Missing"
2020.coling-main.352,N19-1208,0,0.037664,"shown to train large-scale translation tasks efficiently and effectively by controlling the way of feeding samples. Kocmi and Bojar (2017) construct mini-batch contains sentences similar in length and linguistic phenomena, then organize the order by increased complexity in one epoch. Zhang et al. (2018) group the training samples into shards based on model-based and linguistic difficulty criteria, then train with hand-crafted curriculum schedules. Platanios et al. (2019) propose competence-based curriculum learning that select training samples based on sample difficulty and model competence. Kumar et al. (2019) use reinforcement learning to learn the curriculum automatically. Liu et al. (2020) propose a norm-based curriculum learning method based on the norm of word embedding to improve the efficiency of training an NMT system. Zhou et al. (2020) propose uncertainty-aware cur3978 riculum learning. To the best of our knowledge, this is the first comprehensive discussion of curriculum learning in a low-resource setup. On the other hand, curriculum learning is similar to data selection and data sampling methods. More similar work is that Wang et al. (2018) propose a dynamic sampling method that calcula"
2020.coling-main.352,2020.acl-main.41,0,0.459194,"n be found in recent papers (Bengio et al., 2009; Kumar et al., 2010; Shrivastava et al., 2016). For example, Arpit et al. (2017) have pointed out that deep neural networks tend to prioritize learning “easy” samples first. This agrees with the idea of curriculum learning (Bengio et al., 2009) in that an easy-to-hard learning strategy can yield better convergence for training. In NMT, curriculum learning is not new. Several research groups have applied it to large-scale translation tasks although few of them discuss the issue in a low-resource setup (Zhang et al., 2018; Platanios et al., 2019; Liu et al., 2020). The first question here is how to define the “difficulty” of a training sample. Previous work resorts to functions that produce a difficulty score for each training sample. This score is then used to reorder samples before training. But the methods of this type enforce a static scoring strategy and somehow disagrees with the fact that the sample difficulty might be changing when the model is ∗ Corresponding author This work is licensed under a Creative Commons Attribution 4.0 International Licence. creativecommons.org/licenses/by/4.0/. Licence details: http:// 3977 Proceedings of the 28th In"
2020.coling-main.352,W18-6301,0,0.0209758,"dle the entire training set Dtrain . We measure the sample difficulty and model competence before every phase, then the |Dtrain |∗ c(t) easiest training samples are selected to train the NMT model. Benefited from dynamic measurement, the newlyupdated model has enough competence to learn samples with the appropriate difficulties. 4.3 Batching Goyal et al. (2017) address the optimization difficulty when training a neural network with large batches and exhibit good generalization. Large batches have demonstrated better performance in high-resource NMT tasks due to the lower gradient noise scale (Ott et al., 2018; Popel and Bojar, 2018). However, this method degrades the performance in low-resource tasks due to poorer generalization (Keskar et al., 2017). The dominant NMT batches the samples with similar lengths to speed training up (Khomenko et al., 2017). To reduce gradient noise, we propose a batching method which batches the samples based on similar difficulty in our curriculum learning method. Samples with similar difficulty indicate their losses fall at a similar rate. It might have a stabilized gradient direction and leads to better performance. 4.4 Training Strategy Zhang et al. (2018) define"
2020.coling-main.352,N19-4009,0,0.0240534,"other language pairs, we apply the same tokenization using Moses scripts (Koehn et al., 2007). We learn Byte-Pair Encoding (Sennrich et al., 2016b) subword segmentation with 10,000 merge operations for IWSLT datasets and 32,000 merge operations for WMT dataset. Especially, we learn BPE with a shared vocabulary for WMT dataset. Dataset # Train # Dev # Test IWSLT’15 En-Th IWSLT’15 En-Zh IWSLT’17 En-Ja WMT’16 En-De 85019 213377 226834 50K/100K/300K/4.5M 4904 6360 6861 6003 756 1080 1452 2999 Table 1: Number of sentences in each dataset. 5.2 Model Settings In all experiments, we use the fairseq (Ott et al., 2019)3 implementation of the Transformer. Inspired by (Sennrich and Zhang, 2019), we select different hyperparameters for each dataset to build a strong baseline in low-resource NMT4 . The model hyperparameters are shown in Table 2. We use the Adam optimizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.98, and  = 10−9 . We increase the learning rate from 10−7 to 0.0005 for IWSLT datasets and 0.0007 for WMT dataset with linear warmup over the first 4000 steps and decay the learning rate by inverse square root way. For all experiments, we use the dropout of 0.1 and label smoothing ls = 0.1 for regul"
2020.coling-main.352,P02-1040,0,0.10688,". Output: A NMT model with dynamic curriculum learning. 1: t = 0; Randomly initialize the model parameters θ 0 ; 2: while t &lt; T do 3: Evaluate the Dtrain and get loss l(s; θ t ) of every training sample s; 4: for all s in Dtrain do 5: Measure the difficulty of s by Equation 6; 6: end for 7: Sort Dtrain based on the difficulty of every training sample; 8: Evaluate the Ddev and get BLEUt ; 9: Estimate the model competence c(t) by Equation 8; 10: Train the NMT model with the |Dtrain |∗ c(t) easiest training samples; 11: t ← t + 1; 12: end while method, the sentence-level evaluation metric BLEU (Papineni et al., 2002) presents more superiorities (Shen et al., 2016). In this way, the model competence avoids the prior hypothesis of the training process and is related to the real performance on the unseen dataset. Specifically, we pre-train a vanilla NMT model and record the best BLEU value on the development set as curriculum length BLEUT . The model competence is estimated as: c(t) = min(1, BLEUt (1 − c0 ) + c0 ) BLEUT ∗ β (8) where BLEUt is the BLEU at phase t, β ∈ (0, 1] is a coefficient to control the curriculum speed. With a smaller β, the progress of curriculum learning is faster and the model can be t"
2020.coling-main.352,N18-1202,0,0.173326,"n (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009; Kumar et al., 2010; Shrivastava et al., 2016). For example, Arpit et al. (2017) have pointed out that deep neural networks tend"
2020.coling-main.352,N19-1119,0,0.284985,"l parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-resource scenarios without any external resources. This is consistent with our motivation. The difference lies in that they focus on the model settings, and we explore the training strategy which utilizes bilingual data effectively for low-resource NMT. 2.2 Curriculum Learning Curriculum learning (Bengio et al., 2009) is motivated by the learning strategy of biological organisms which orders the training samples in an easy-to-hard manner (Elman, 1993). Benefited from organized training, the neural"
2020.coling-main.352,P19-1021,0,0.0793673,"d to pre-train model parameters and transfer to target language pair (Zoph et al., 2016; Chen et al., 2017; Gu et al., 2018a; Gu et al., 2018b; Kocmi and Bojar, 2018). Pre-trained language models trained with a large amount of monolingual data (Peters et al., 2018; Devlin et al., 2019) improve the quality of NMT model significantly (Clinchant et al., 2019; Yang et al., 2020; Zhu et al., 2020). However, these approaches rely on a large number of external resources or conditions, e.g., the auxiliary parallel corpus related to the source or target language, or a large amount of monolingual data. Sennrich and Zhang (2019) demonstrate the competitive NMT model can be trained with the appropriate hyperparameters in low-resource scenarios without any external resources. This is consistent with our motivation. The difference lies in that they focus on the model settings, and we explore the training strategy which utilizes bilingual data effectively for low-resource NMT. 2.2 Curriculum Learning Curriculum learning (Bengio et al., 2009) is motivated by the learning strategy of biological organisms which orders the training samples in an easy-to-hard manner (Elman, 1993). Benefited from organized training, the neural"
2020.coling-main.352,P16-1009,0,0.44275,"elines on three low-resource machine translation benchmarks and different sized data of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers"
2020.coling-main.352,P16-1162,0,0.837162,"elines on three low-resource machine translation benchmarks and different sized data of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers"
2020.coling-main.352,P16-1159,0,0.0193216,"ing. 1: t = 0; Randomly initialize the model parameters θ 0 ; 2: while t &lt; T do 3: Evaluate the Dtrain and get loss l(s; θ t ) of every training sample s; 4: for all s in Dtrain do 5: Measure the difficulty of s by Equation 6; 6: end for 7: Sort Dtrain based on the difficulty of every training sample; 8: Evaluate the Ddev and get BLEUt ; 9: Estimate the model competence c(t) by Equation 8; 10: Train the NMT model with the |Dtrain |∗ c(t) easiest training samples; 11: t ← t + 1; 12: end while method, the sentence-level evaluation metric BLEU (Papineni et al., 2002) presents more superiorities (Shen et al., 2016). In this way, the model competence avoids the prior hypothesis of the training process and is related to the real performance on the unseen dataset. Specifically, we pre-train a vanilla NMT model and record the best BLEU value on the development set as curriculum length BLEUT . The model competence is estimated as: c(t) = min(1, BLEUt (1 − c0 ) + c0 ) BLEUT ∗ β (8) where BLEUt is the BLEU at phase t, β ∈ (0, 1] is a coefficient to control the curriculum speed. With a smaller β, the progress of curriculum learning is faster and the model can be trained on the entire training set earlier. We su"
2020.coling-main.352,P18-2048,0,0.0699047,"ed on sample difficulty and model competence. Kumar et al. (2019) use reinforcement learning to learn the curriculum automatically. Liu et al. (2020) propose a norm-based curriculum learning method based on the norm of word embedding to improve the efficiency of training an NMT system. Zhou et al. (2020) propose uncertainty-aware cur3978 riculum learning. To the best of our knowledge, this is the first comprehensive discussion of curriculum learning in a low-resource setup. On the other hand, curriculum learning is similar to data selection and data sampling methods. More similar work is that Wang et al. (2018) propose a dynamic sampling method that calculates the decline of loss during training to improve the NMT training efficiency. They start training from the full training set and then gradually decrease. This is contrary to the idea of curriculum learning. 3 Problem Definition Let Dtrain be the training corpus and |Dtrain |be the corpus size. s = (x, y) is a training sample in Dtrain , where x is the source sentence and y is the target sentence. NMT systems learn a conditional probability P (y|x): P (y|x; θ) = |y| Y P (yi |x, y&lt;i ; θ) (1) i=1 where |y |is the length of y, θ is a set of model pa"
2020.coling-main.352,P12-3004,1,0.796735,"t2013 as the development set. We use tst2015 as the test set for Zh-En and En-Th, tst2017 for En-Ja. To simulate different amounts of training resources, we randomly subsample 50K/100K/300K sentence pairs from WMT’16 English-German dataset and denote them as 50K/100K/300K. Furthermore, we also verify the effect of our method in the high-resource scenarios with all WMT’16 English-German training set (4.5M). We concatenate newstest2012 and newstest2013 as the development set and newstest2016 as the test set. Data statistics are shown in Table 1. We tokenize the Chinese sentences using NiuTrans (Xiao et al., 2012) word segmentation tookit and Japanese sentences using MeCab2 . For other language pairs, we apply the same tokenization using Moses scripts (Koehn et al., 2007). We learn Byte-Pair Encoding (Sennrich et al., 2016b) subword segmentation with 10,000 merge operations for IWSLT datasets and 32,000 merge operations for WMT dataset. Especially, we learn BPE with a shared vocabulary for WMT dataset. Dataset # Train # Dev # Test IWSLT’15 En-Th IWSLT’15 En-Zh IWSLT’17 En-Ja WMT’16 En-De 85019 213377 226834 50K/100K/300K/4.5M 4904 6360 6861 6003 756 1080 1452 2999 Table 1: Number of sentences in each d"
2020.coling-main.352,2020.acl-main.620,0,0.108174,"order by increased complexity in one epoch. Zhang et al. (2018) group the training samples into shards based on model-based and linguistic difficulty criteria, then train with hand-crafted curriculum schedules. Platanios et al. (2019) propose competence-based curriculum learning that select training samples based on sample difficulty and model competence. Kumar et al. (2019) use reinforcement learning to learn the curriculum automatically. Liu et al. (2020) propose a norm-based curriculum learning method based on the norm of word embedding to improve the efficiency of training an NMT system. Zhou et al. (2020) propose uncertainty-aware cur3978 riculum learning. To the best of our knowledge, this is the first comprehensive discussion of curriculum learning in a low-resource setup. On the other hand, curriculum learning is similar to data selection and data sampling methods. More similar work is that Wang et al. (2018) propose a dynamic sampling method that calculates the decline of loss during training to improve the NMT training efficiency. They start training from the full training set and then gradually decrease. This is contrary to the idea of curriculum learning. 3 Problem Definition Let Dtrain"
2020.coling-main.352,D16-1163,0,0.0682361,"of WMT’16 En-De. 1 Introduction Recently, neural machine translation (NMT) has demonstrated impressive performance improvements and became the de-facto standard (Sutskever et al., 2014; Bahdanau et al., 2015; Wu et al., 2016; Vaswani et al., 2017). However, like other neural methods, NMT is data-hungry. This makes it challenging when we train such a model in low-resource scenarios (Koehn and Knowles, 2017). Researchers have developed promising approaches to low-resource NMT. Among these are data augmentation (Sennrich et al., 2016a; Cheng et al., 2016; Fadaee et al., 2017), transfer learning (Zoph et al., 2016; Kocmi and Bojar, 2018), and pre-trained models (Peters et al., 2018; Devlin et al., 2019). But these approaches rely on external data other than bi-text. To date, it is rare to see work on the effective use of bilingual data for low-resource NMT. In general, the way of feeding samples plays an important role in training neural models. A good instance is that it is popular to shuffle the input data for robust training in state-of-the-art systems. More systematic studies on this issue can be found in recent papers (Bengio et al., 2009; Kumar et al., 2010; Shrivastava et al., 2016). For example"
2020.coling-main.377,D18-1338,0,0.288652,"ment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the encoder layers to be the same as the decoder layers (He et al., 2018). Instead, in this work, we propose layer-wise multi-view learning to address this problem from the perspective of model training, without changing the model structure. Our method’s highlight is that ∗ Work done during Ph.D. study at Northeastern University. Corresponding author. This work is licensed under a Creati"
2020.coling-main.377,P19-1425,0,0.0227968,"rm Transformer with a 12-layer encoder. The results were measured on the IWSLT’14 De→En validation set. i=0 indicates the embedding layer. to the primary view by feeding an auxiliary view. Figure 2 shows that the vector similarity between the i-th encoder layer and the topmost layer grows as the increase of i. Therefore, we can regard the middle layer’s auxiliary view as a noisy version of the primary view. Training with noises has been widely proven to effectively improve the model’s generalization ability, such as dropout (Srivastava et al., 2014), adversarial training (Miyato et al., 2017; Cheng et al., 2019) etc. We also experimentally confirm that our model is more robust than the single view model when injecting random noises into the encoding representation. Dark knowledge. Typically, the prediction target in Lnll is a one-hot distribution: Only the gold label is 1, while the others are 0. A better alternative is label smoothing (Szegedy et al., 2016), which reduces the probability of gold label by  and redistributes  to all non-gold labels on average. However, label smoothing ignores the relationship between non-gold labels. For example, if the current ground-truth is “improve”, then “promo"
2020.coling-main.377,D18-1217,0,0.12509,"e input data (Xu et al., 2013), where one of the keys is view construction. In our scenario, a view is the hidden representation of the input sentence (an array of hidden vectors for each token, e.g., H ∗ ). In this work, we further propose to take the off-the-shelf output of each encoder layer (i.e., H (l) 2 ) to construct the redundant views. In NLP, previous implementations of view construction generally require the model to recalculate on the reconstructed input, such as using different orders of n-grams in the bag-of-word model (Matsubara et al., 2005), randomly masking the input tokens (Clark et al., 2018). As opposed to them, our method is very cheap as the by-product of the standard layer-by-layer encoding process. According to the definition of a view, we can regard the vanilla Transformer as a single-view model since only the topmost encoder layer (also called primary view) is fed to the decoder. In contrast, MV-Transformer additionally contains an intermediate layer Ma (1 ≤ Ma < M ) as the auxiliary view 3 . The choice of Ma can be arbitrary, and we discuss its effect in § 4.2. Our goal is to learn a better single model with the help of the auxiliary view. Partially shared parameters. In t"
2020.coling-main.377,D18-1457,0,0.322233,"); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the"
2020.coling-main.377,D16-1139,0,0.0284317,"l ensemble MV-Transformer can be thought of as consisting of two models: A large model as the primary view, and a small model (with shallower encoder) as the auxiliary view. Here we compare with the other three methods of integrating multiple models: • Oneway-KD. Similar to Eq. 7 but detach the teacher’s prediction, i.e., gradients of the teacher’s prediction is not tracked, posing a one-way transfer from primary view to auxiliary view. • Seq-KD. Train the large model first and then translate the original training set by beam search to construct the distilled training set for the small model (Kim and Rush, 2016). • Ensemble. Independently train the two models and combine their predictions at inference time, e.g., by algorithmic average. Experiments are done on IWSLT’14 De→En, where the small model has a 3-layer encoder. As shown in Table 2, we can see that: (1) Oneway-KD suffers from severe degradation than MV when detaching the primary view, which indicates that making mutual learning between the primary view and auxiliary view is critical; (2) Seq-KD is almost useless or even badly hurts the performance (vs. Baseline (3L)), which is against the previous belief that Seq-KD helps the small model a lo"
2020.coling-main.377,W04-3250,0,0.0198279,"le 1: BLEU scores on five translation tasks. For (deep) Transformer, Aux./Pri. denotes the independently trained model with Ma /M -layer encoder respectively. For (deep) MV-Transformer, Aux./Pri. denotes the used view at inference time. ∆ denotes the improved BLEU score over the Transformer baseline when using multi-view learning at the same encoder depth. † denotes our implementation. Boldface and ∗ represent local and global best results, respectively. All the MV-Transformer results are significantly better (p<0.01) than the Transformer counterparts, measured by paired bootstrap resampling (Koehn, 2004). uses sacrebleu 10 , all other datasets are evaluated by multi-bleu.perl. Only De→En is reported by case insensitive BLEU. 3.2 Main results In addition to Transformer, we also re-implemented three previously proposed models that incorporate multiple encoder layers: multi-layer representation fusion (MLRF)(Wang et al., 2018), hierarchical aggregation (HieraAgg) (Dou et al., 2018), and transparent attention (TA) (Bapna et al., 2018). Table 1 shows the results of the five translation tasks on PostNorm and PreNorm. First, our MV-Transformer outperforms all baselines across the board. Specifically"
2020.coling-main.377,N18-1202,0,0.027903,"der finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017). However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network ("
2020.coling-main.377,W18-5431,0,0.0247007,"er representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017). However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-li"
2020.coling-main.377,P19-1021,0,0.0615377,"Missing"
2020.coling-main.377,C18-1255,1,0.872017,"odel. 1 Introduction Neural Machine Translation (NMT) adopts the encoder-decoder paradigm to model the entire translation process (Bahdanau et al., 2015). Specifically, the encoder finds a multi-layer representation of the source sentence, and the decoder queries the topmost encoding representation to produce the target sentence through a cross-attention mechanism (Wu et al., 2016; Vaswani et al., 2017). However, such overreliance on the topmost encoding layer is problematic in two aspects: (1) Prone to over-fitting, especially when the encoder is under-trained, such as in low-resource tasks (Wang et al., 2018); (2) It cannot make full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; D"
2020.coling-main.377,P19-1176,1,0.913927,"ake full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the encoder layers to"
2020.coling-main.377,P19-1624,0,0.0591465,"ake full use of representations extracted from lower encoder layers, which are syntactically and semantically complementary to higher layers (Peters et al., 2018; Raganato and Tiedemann, 2018). Researchers have proposed many methods to make the model aware of various encoder layers besides the topmost to mitigate this issue. Almost all of them resort to the adjustment of network structure, which can be further divided into two categories. The first is to merge the feature representations extracted by distinct encoder layers before being fed to the decoder (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b). The differences between them lie in the design of the merge function: through self-attention (Wang et al., 2018), recurrent neural network (Wang et al., 2019b), or tree-like hierarchical merge (Dou et al., 2018). Moreover, the second makes each decoder layer explicitly align to a parallel encoder layer (He et al., 2018) or all encoder layers (Bapna et al., 2018). However, the above methods either complicate the original model (Wang et al., 2018; Dou et al., 2018; Wang et al., 2019b; Bapna et al., 2018) or limit the model’s flexibility, such as requiring the number of the encoder layers to"
2020.coling-main.526,D16-1250,0,0.053542,"Missing"
2020.coling-main.526,P17-1042,0,0.0446501,"dings. Early work (Mikolov et al., 2013) relies on a seed dictionary to learn the source-target word embedding mapping. Xing et al. (2015) enforce the word embeddings to be of unit length and the orthogonal constraint on the linear mapping. Faruqui and Dyer (2014) on the other hand use Canonical Correlation Analysis (CCA) to project both source and target embeddings to a common low-dimensional space. Artetxe et al. (2016) show that the above methods are variants of the same objective. Smith et al. (2017) further show that this objective is closely related to the orthogonal Procrustes problem. Artetxe et al. (2017) obtain competitive results using the self-learning with a seed dictionary of only 25 word pairs. Adversarial methods. Zhang et al. (2017a) attempt the unsupervised bilingual dictionary induction task using the adversarial network. They use a generator to transform the source word embeddings to the target word embeddings and a discriminator to classify whether the given embedding is sampled from the true target word embeddings or generated by the generator. The generator is trained to fool the discriminator and the discriminator is trained to identify the generated word embeddings. In the end,"
2020.coling-main.526,P18-1073,0,0.416762,"E is the raw embeddings, n is the initial dimension 2: D←∅ . Set the dictionary to empty 3: while n ≤ 300 do . 300 is the dimension of the raw embeddings ¯ with dimension min(n, 300) using PCA and dropmax 4: Reduce E to E 5: if D = ∅ then ¯ 6: Run the initialization and the self-learning on E 7: else ¯ with D as the initial dictionary 8: Run the self-learning on E 9: end if 10: Translate 4K most frequent words and store the results in D 11: n←n×2 12: end while 13: return WX and WY 14: end procedure 5993 System En-Es Es-En En-Fr Fr-En En-It It-En En-De De-En MUSE (Lample et al., 2018) VecMap (Artetxe et al., 2018) C-MUSE (Hartmann et al., 2019) POSTPROC (Vulic et al., 2020) 83.20 82.33 82.33 82.73 83.66 84.60 84.73 85.47 82.66 82.47 82.40 82.73 82.39 83.60 83.73 84.00 78.20 79.13 79.13 79.13 77.90 79.80 79.67 80.60 75.10 75.33 75.27 76.00 72.93 74.27 74.20 75.33 Proposed method (dim 50) Proposed method (dim 100) Proposed method (dim 200) Proposed method (dim 300) 40.33 63.47 80.33 82.40 37.40 61.80 80.27 84.60 53.53 72.73 80.40 82.60 48.07 71.13 79.67 83.67 48.13 68.40 76.47 78.93 45.00 66.73 76.67 79.67 31.60 63.67 71.27 75.33 30.33 61.27 71.20 74.33 Table 3: The accuracy of different UBDI systems on"
2020.coling-main.526,P19-1494,0,0.437928,"to project the word embeddings to a lower-dimensional space. Then they apply a variant of the Iterative Closest Point method to find the source and target word embeddings mapping. Zhou et al. (2019) use normalizing flows to match the distribution of source and target word embeddings. But they rely on a numeral seed dictionary and the additional word frequency information. More recently, Hartmann et al. (2019) find that more robust results can be obtained by using the adversarial method to produce the initial dictionary for the advanced self-learning (with the stochastic dictionary induction). Artetxe et al. (2019) first generate a pseudo parallel corpus by an unsupervised machine translation system. They then extract a bilingual dictionary from the word alignment learned on that corpus. This simple process shows much better results than previous methods. Vulic et al. (2020) introduce a simple post-processing step to improve UBDI performance on distant language pairs. 5998 8 Conclusion In this work, we pinpoint in which part the representative UBDI system, VecMap, fails on distant language pairs. We identify a gap between the initialization performance and the minimum initialization performance for the"
2020.coling-main.526,Q17-1010,0,0.0639702,"hod 37.33 35.27 48.87 33.08 47.6 55.53 21.60 13.64 - IDR - Dropmax 40.20 0 41.07 0 47.64 0 34.18 0 0.13 0.33 0.20 0.27 0.07 20.73 0 14.25 Table 5: Ablation study of the proposed method on distant language pairs. reproduce the C-MUSE and POSTPROC system using Python. All these systems are run with the default hyper-parameters settings. Our method is based on the open-sourced VecMap implementation. We evaluate the baseline and our method on 4 similar language pairs, En-{Es, Fr, It, De}, and 4 distant language pairs, En-{Zh, Ja, Vi, Th}. We use the pretrained 300-dimensional fastText embeddings (Bojanowski et al., 2017)3 . The evaluation dictionaries are from Lample et al. (2018). We trim all vocabularies to the 20K most frequent words for training. Specifically, VecMap retains the top-4K words for the initialization, while others use the whole vocabulary. All experiments are done on a single Nvidia GTX 1080Ti. We run each experiment 3 times but with different random seeds, then pick the one with the highest cosine similarity of induced nearest neighbors as the final result. This unsupervised model selection criterion has shown to correlate well with UBDI performance (Hartmann et al., 2019). 5.2 Results Tabl"
2020.coling-main.526,E14-1049,0,0.110886,"Missing"
2020.coling-main.526,P19-1070,0,0.114822,"Missing"
2020.coling-main.526,D18-1043,0,0.387086,"tween the k largest Pkgraphs as the sum 2 eigenvalues λ of L1 and L2 , 4 = i=1 (λ1i − λ2i ) . The higher 4 is, the less similar the graphs are. As shown in Figure 4, the eigenvector similarity drops significantly when the dimension is reduced. This implies that the underlying nearest neighbor graphs of two languages become similar in low-dimensional space. This helps the algorithm to succeed in low-dimensional space as the assumption it makes is held. This phenomenon might be the result that many language pairs share some principle axes of variation, especially the ones with high eigenvalues (Hoshen and Wolf, 2018). 6.3 Hubness Cross-lingual word embeddings are known to suffer from the hubness problem (Lample et al., 2018), where a few points (known as hubs) are the nearest neighbors of many other points in high-dimensional spaces. As suggested in Section 4, distant language pairs might suffer more from this problem and the dropmax trick helps to alleviate this problem. Thus we would like to know to what extent the dropmax trick helps in the hubness problem. Here we adopt the hubness metric proposed by Ormazabal et al. (2019) for evaluation. This metric measures the percentage of target words H that are"
2020.coling-main.526,D19-1328,0,0.436928,"Missing"
2020.coling-main.526,N19-1386,0,0.0551632,"t al., 2017b) minimizes Earth-Mover’s distance between the transformed source and target embeddings distribution. Lample et al. (2018) improve the results by treating the dictionary produced by the adversarial network as the seed dictionary of the self-learning. To mitigate the hubness problem (Radovanovic et al., 2010), they propose an effective nearest neighbors retrieval method CSLS for dictionary induction. Xu et al. (2018) minimize Sinkhorn distance instead and introduce the circle consistency such that a source word embedding can be translated back after translating it to a target word. Mohiuddin and Joty (2019) extract latent codes from word embeddings and align words according to their latent codes. Non-adversarial methods. There is another line of research that focuses on a non-adversarial approach. Artetxe et al. (2018) propose a heuristic to induce an initial dictionary by exploiting the structural similarity of embeddings. They also propose the stochastic dictionary induction method, which significantly improves the robustness as well as the performance of self-learning. Hoshen and Wolf (2018) assume that many language pairs share some principle axes of variation. Therefore they first use PCA t"
2020.coling-main.526,2020.acl-main.318,0,0.0343256,"Missing"
2020.coling-main.526,P18-1072,0,0.0848626,"Missing"
2020.coling-main.526,D19-1449,0,0.041175,"Missing"
2020.coling-main.526,2020.repl4nlp-1.7,0,0.0309337,"word embeddings. But they rely on a numeral seed dictionary and the additional word frequency information. More recently, Hartmann et al. (2019) find that more robust results can be obtained by using the adversarial method to produce the initial dictionary for the advanced self-learning (with the stochastic dictionary induction). Artetxe et al. (2019) first generate a pseudo parallel corpus by an unsupervised machine translation system. They then extract a bilingual dictionary from the word alignment learned on that corpus. This simple process shows much better results than previous methods. Vulic et al. (2020) introduce a simple post-processing step to improve UBDI performance on distant language pairs. 5998 8 Conclusion In this work, we pinpoint in which part the representative UBDI system, VecMap, fails on distant language pairs. We identify a gap between the initialization performance and the minimum initialization performance for the self-learning to succeed, which is responsible for its failure. We propose Iterative Dimension Reduction to bridge this gap. Our method obtains substantial gains in distant language pairs without scarifying the performance of similar language pairs. It has shown to"
2020.coling-main.526,N15-1104,0,0.0667128,"Missing"
2020.coling-main.526,D18-1268,0,0.134918,"ator and the discriminator is trained to identify the generated word embeddings. In the end, the generator will be used to induce the bilingual dictionary. Their following work (Zhang et al., 2017b) minimizes Earth-Mover’s distance between the transformed source and target embeddings distribution. Lample et al. (2018) improve the results by treating the dictionary produced by the adversarial network as the seed dictionary of the self-learning. To mitigate the hubness problem (Radovanovic et al., 2010), they propose an effective nearest neighbors retrieval method CSLS for dictionary induction. Xu et al. (2018) minimize Sinkhorn distance instead and introduce the circle consistency such that a source word embedding can be translated back after translating it to a target word. Mohiuddin and Joty (2019) extract latent codes from word embeddings and align words according to their latent codes. Non-adversarial methods. There is another line of research that focuses on a non-adversarial approach. Artetxe et al. (2018) propose a heuristic to induce an initial dictionary by exploiting the structural similarity of embeddings. They also propose the stochastic dictionary induction method, which significantly"
2020.coling-main.526,P17-1179,0,0.161033,"Missing"
2020.coling-main.526,D17-1207,0,0.0475176,"Missing"
2020.coling-main.526,N19-1161,0,0.645134,"Missing"
2020.emnlp-main.72,D18-1338,0,0.367244,"ork design: both the encoder and decoder learn representations of word sequences by a stack of layers (Vaswani et al., 2017; Wu et al., 2016; Gehring et al., 2017), building on an interesting line of work in improving such models. The simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model (Vaswani et al., 2017), deep systems have shown ∗ Corresponding author. promising BLEU improvements by either easing the information flow through the network (Bapna et al., 2018) or constraining the gradient norm across layers (Zhang et al., 2019; Xu et al., 2020; Liu et al., 2020). An improved system can even learn a 35-layer encoder, which is 5× deeper than that of vanilla Transformer (Wang et al., 2019). Although these methods have enabled training deep neural MT (NMT) models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner (i.e., BLEU score). It is thus natural to study how much a deep NMT system is able to learn that is differe"
2020.emnlp-main.72,2020.emnlp-main.463,0,0.11086,"wani et al., 2017; Wu et al., 2016; Gehring et al., 2017), building on an interesting line of work in improving such models. The simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model (Vaswani et al., 2017), deep systems have shown ∗ Corresponding author. promising BLEU improvements by either easing the information flow through the network (Bapna et al., 2018) or constraining the gradient norm across layers (Zhang et al., 2019; Xu et al., 2020; Liu et al., 2020). An improved system can even learn a 35-layer encoder, which is 5× deeper than that of vanilla Transformer (Wang et al., 2019). Although these methods have enabled training deep neural MT (NMT) models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner (i.e., BLEU score). It is thus natural to study how much a deep NMT system is able to learn that is different from the shallow counterpart. Beyond this, training an extremely deep model is expensive although a n"
2020.emnlp-main.72,N19-1423,0,0.0435555,"g et al., 2020). 7.2 Efficient Training Methods When the model goes deeper, a challenge is the long training time for model convergence and the huge GPU cost. To alleviate this issue, several attempts have been made. Chang et al. (2018) proposed a multi-level training method by interpolating a residual block right after each existing block to accelerate the training of ResNets in computer version. Similarly, Gong et al. (2019) adopted a progressive stacking strategy to transfer the knowledge from a shallow model to a deep model, thus successfully trained a large-scale pre-training model BERT (Devlin et al., 2019) at a faster rate with comparable performance on downstream tasks. Unlike previous work, we only copy parameters of the g top-most layers and employ sparse connections across each stacking block in our shallow to deep training method, which has not been discussed yet in learning deep MT models. 8 Conclusions We have investigated the behaviour of the welltrained deep Transformer models and found that stacking more layers could improve the representation ability of NMT systems. Higher layers share more global information over different positions and adjacent layers behave similarly. Also, we hav"
2020.emnlp-main.72,W18-6301,0,0.0183709,"both the WMT En-De and En-Fr tasks. For a stronger system, we employed relative position representation (RPR) to strengthen the position embedding model (Shaw et al., 2018). We only used the relative key in each layer. We batched sentence pairs by approximate length, and limited input/output tokens per batch to 4, 096/GPU. Following the method of (Wang et al., 3 BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a +version.1.2.12 4 https://github.com/tensorflow/ tensor2tensor 999 WMT En-De Systems WMT En-Fr Params Time Speedup BLEU Sacrebleu Params Vaswani et al. (2017) (Big) Shaw et al. (2018) (Big) Ott et al. (2018) (Big) Wu et al. (2019b) (Big) Wang et al. (2019) (Deep) Wei et al. (2020) (Deep) Wei et al. (2020) (Big+Deep) Base (Pre-Norm) Big (Pre-Norm) 213M N/A 210M N/A 210M N/A 270M N/A 137M N/A 272M N/A 512M N/A 63M 4.79 210M 36.05 Deep-24L S DT-24L Deep-RPR-24L S DT-RPR-24L Deep-48L S DT-48L Deep-RPR-48L S DT-RPR-48L Deep-24L (Big) S DT-24L (Big) Deep-RPR-24L (Big) S DT-RPR-24L (Big) 118M 118M 118M 118M 194M 194M 194M 194M 437M 437M 437M 437M 8.66 6.16 9.80 6.71 16.38 10.65 19.58 11.75 37.41 18.31 38.80 18.51 Time Speedup BLEU Sacrebleu N/A N/A N/A N/A N/A N/A N/A N/A N/A 28.40 29.20 29.30 29.92 29."
2020.emnlp-main.72,P16-1162,0,0.0678739,"ning steps. Here step num and warmup steps are the current training step number and the warmup-step number. dmodel is the size of the layer output. See Figure 5 for a comparison of different learning schemas. 5 Experiments We report the experimental results on two widely used benchmarks - WMT’16 English-German (EnDe) and WMT’14 English-French (En-Fr). Data For the En-De task, we used the same preprocessed data with (Vaswani et al., 2017; Ott et al., 2019; Wang et al., 2019), consisting of approximate 4.5M tokenized sentence pairs. All sentences were segmented into sequences of sub-word units (Sennrich et al., 2016) with 32K merge operations using a vocabulary shared by source and target sides. We selected newstest2012+newstest2013 as validation data and newstest2014 as test data. For the En-Fr task, we replicated the setup of Vaswani et al. (2017) with 36M training sentence pairs from WMT14. We validated the En-Fr system on the union set of newstest2012 and newstest2013, and tested it on newstest2014. We filtered out sentences of more than 200 words and generated a shared vocabulary with 40K merge operations on both source and target side. We re-merged sub-word units to form complete words in the final"
2020.emnlp-main.72,N18-2074,0,0.0610634,"timizer (Kingma and Ba, 2015) with β1 = 0.9, β2 = 0.997, and  = 10−8 . We adopted the same learning rate schedule as the latest implementation of Tensor2Tensor4 . For deep models, the learning rate (lr) first increased linearly for warmup = 8, 000 steps from 1e−7 to 2e−3 . After warmup, the learning rate decayed proportionally to the inverse square root of the current step. For our S DT method presented in Section 4, we set h = g = p = 6 on both the WMT En-De and En-Fr tasks. For a stronger system, we employed relative position representation (RPR) to strengthen the position embedding model (Shaw et al., 2018). We only used the relative key in each layer. We batched sentence pairs by approximate length, and limited input/output tokens per batch to 4, 096/GPU. Following the method of (Wang et al., 3 BLEU+case.mixed+numrefs.1+smooth.exp+tok.13a +version.1.2.12 4 https://github.com/tensorflow/ tensor2tensor 999 WMT En-De Systems WMT En-Fr Params Time Speedup BLEU Sacrebleu Params Vaswani et al. (2017) (Big) Shaw et al. (2018) (Big) Ott et al. (2018) (Big) Wu et al. (2019b) (Big) Wang et al. (2019) (Deep) Wei et al. (2020) (Deep) Wei et al. (2020) (Big+Deep) Base (Pre-Norm) Big (Pre-Norm) 213M N/A 210M"
2020.emnlp-main.72,P19-1176,1,0.170458,"he simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model (Vaswani et al., 2017), deep systems have shown ∗ Corresponding author. promising BLEU improvements by either easing the information flow through the network (Bapna et al., 2018) or constraining the gradient norm across layers (Zhang et al., 2019; Xu et al., 2020; Liu et al., 2020). An improved system can even learn a 35-layer encoder, which is 5× deeper than that of vanilla Transformer (Wang et al., 2019). Although these methods have enabled training deep neural MT (NMT) models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner (i.e., BLEU score). It is thus natural to study how much a deep NMT system is able to learn that is different from the shallow counterpart. Beyond this, training an extremely deep model is expensive although a narrow-anddeep network can speed up training (Wang et al., 2019). For example, it takes us 3× longer time to train the model whe"
2020.emnlp-main.72,D19-1083,0,0.0629027,"d sequences by a stack of layers (Vaswani et al., 2017; Wu et al., 2016; Gehring et al., 2017), building on an interesting line of work in improving such models. The simplest of these increases the model capacity by widening the network, whereas more recent work shows benefits from stacking more layers on the encoder side. For example, for the popular Transformer model (Vaswani et al., 2017), deep systems have shown ∗ Corresponding author. promising BLEU improvements by either easing the information flow through the network (Bapna et al., 2018) or constraining the gradient norm across layers (Zhang et al., 2019; Xu et al., 2020; Liu et al., 2020). An improved system can even learn a 35-layer encoder, which is 5× deeper than that of vanilla Transformer (Wang et al., 2019). Although these methods have enabled training deep neural MT (NMT) models, questions remain as to the nature of the problem. The main question here is: why and how deep networks help in NMT. Note that previous work evaluates these systems in a black-box manner (i.e., BLEU score). It is thus natural to study how much a deep NMT system is able to learn that is different from the shallow counterpart. Beyond this, training an extremely"
2020.emnlp-main.72,C18-1255,1,0.664526,"ral Language Processing, pages 995–1005, c November 16–20, 2020. 2020 Association for Computational Linguistics si+1 L LayerNorm SubLayer L LayerNorm SubLayer si si (a) Pre-norm (b) Post-norm where si and si+1 are the output of sub-layers i and i+1. See Figure 1 (a) for the architecture of a pre-norm sub-layer. Pre-norm residual network has been found to be more efficient for back-propagation over a large number of layers than the post-norm architecture (Wang et al., 2019; Li et al., 2019). si+1 • Dense Connections. Direct layer connections can make easy access to distant layers in the stack (Wang et al., 2018; Bapna et al., 2018). Let {y1 , ..., yN } be the output of the stacked layers. We define a network G(y1 , ..., yj−1 ) that reads all layer output vectors prior to layer j and generates a new vector. Then, G(y1 , ..., yj−1 ) is regarded as a part of the input of layer j. In this way, we create direct connections from layers {1, ..., j − 1} to layer j. For G(·), we choose a linear model as in (Wang et al., 2019). Figure 1: Pre-norm and Post-norm sub-layer architectures. of information through the deep network but does not require large memory footprint as in dense networks. We experiment with t"
2020.emnlp-main.72,2020.acl-main.40,0,0.230095,"ight prevent us from exploiting deeper models in large-scale systems. In this paper, we explore why deep architectures work to render learning NMT models more effectively. By investigating the change of the hidden states in different layers, we find that new representations are learned by continually stacking layers on top of the base model. More stacked layers lead to a stronger model of representing the sentence. This particularly makes sense in the deep NMT scenario because it has been proven that deep models can benefit from an enriched representation (Wang et al., 2019; Wu et al., 2019b; Wei et al., 2020). In addition, the finding here inspires us to develop a simple yet efficient method to train a deep NMT encoder: we train model parameters from shallow to deep, rather than training the entire model from scratch. To stabilize training, we design a sparse linear combination method of connecting lower-level layers to the top. It makes efficient pass 995 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 995–1005, c November 16–20, 2020. 2020 Association for Computational Linguistics si+1 L LayerNorm SubLayer L LayerNorm SubLayer si si (a) Pre-norm (b)"
2020.emnlp-main.72,P19-1558,0,0.335375,"48 layers. This might prevent us from exploiting deeper models in large-scale systems. In this paper, we explore why deep architectures work to render learning NMT models more effectively. By investigating the change of the hidden states in different layers, we find that new representations are learned by continually stacking layers on top of the base model. More stacked layers lead to a stronger model of representing the sentence. This particularly makes sense in the deep NMT scenario because it has been proven that deep models can benefit from an enriched representation (Wang et al., 2019; Wu et al., 2019b; Wei et al., 2020). In addition, the finding here inspires us to develop a simple yet efficient method to train a deep NMT encoder: we train model parameters from shallow to deep, rather than training the entire model from scratch. To stabilize training, we design a sparse linear combination method of connecting lower-level layers to the top. It makes efficient pass 995 Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing, pages 995–1005, c November 16–20, 2020. 2020 Association for Computational Linguistics si+1 L LayerNorm SubLayer L LayerNorm SubLayer si"
2020.emnlp-main.72,2020.acl-main.38,0,0.0602053,"Missing"
2020.findings-emnlp.385,P84-1044,0,0.330199,"Missing"
2020.findings-emnlp.385,D16-1139,0,0.172076,"tions, especially for the industry. In this work, we propose to use multi-task learning to train a flexible depth model that can adapt to different depth configurations during inference. Experimental results show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and another flexible depth model training method——LayerDrop. 1 Introduction As neural machine translation models become heavier and heavier (Vaswani et al., 2017), we have to resort to model compress techniques (e.g., knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016)) to deploy smaller models in devices with limited resources, such as mobile phones. However, a practical challenge is that the hardware conditions of different devices vary greatly. To ensure the same calculation latency, customizing distinct model sizes (e.g., depth, width) for different devices is necessary, which leads to huge model training and maintenance costs (Yu et al., 2019). For example, we need to distill the pretrained large model into N individual small models. The situation becomes worse for the industry when considering more translation directions and more frequent model iterat"
2020.findings-emnlp.385,N18-1202,0,0.0161069,"sub-network is unique and deterministic in our method, resulting in consistent sub-network used between training and inference. 4308 where t(i) is the number ofP tasks in which the i-th d ˆ φ(D) layer participates and t¯ = d∈D . The second is average layer distance (ALD), which requires the distance between adjacent layers in the subnetwork SN(d) = {La1 , La2 , . . . , Lad } should be large. For example, for a 6-layer network, if we want to build a 2-layer sub-network, it is unreasonable to select {L1 , L2 } directly because the features extracted by adjacent layers are semantically similar (Peters et al., 2018; Raganato and Tiedemann, 2018). Therefore, we use the average distance between layers in all sub-networks as the metric: P P |ai+1 − ai | Algorithm 1: Training Flexible Depth Model by Multi-Task Learning. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 pre-train MM −N on training data D; generate distillation data D0 by MM −N ; M0M −N ← MM −N ; for t in 1, 2, . . . , T do B ← sample batch from D0 ; gradient G ← 0; ˆ ) ⊗ φ(N ˆ ) do for (mi , ni ) in φ(M SNe , SNd ← F(mi , M ), F(ni , N ); Feed B into network (SNe , SNd ); Collect gradient g by Back-Propa.; G ← G + g; end Optimize M0M −N with gradient G; e"
2020.findings-emnlp.385,W18-5431,0,0.0232718,"e and deterministic in our method, resulting in consistent sub-network used between training and inference. 4308 where t(i) is the number ofP tasks in which the i-th d ˆ φ(D) layer participates and t¯ = d∈D . The second is average layer distance (ALD), which requires the distance between adjacent layers in the subnetwork SN(d) = {La1 , La2 , . . . , Lad } should be large. For example, for a 6-layer network, if we want to build a 2-layer sub-network, it is unreasonable to select {L1 , L2 } directly because the features extracted by adjacent layers are semantically similar (Peters et al., 2018; Raganato and Tiedemann, 2018). Therefore, we use the average distance between layers in all sub-networks as the metric: P P |ai+1 − ai | Algorithm 1: Training Flexible Depth Model by Multi-Task Learning. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 pre-train MM −N on training data D; generate distillation data D0 by MM −N ; M0M −N ← MM −N ; for t in 1, 2, . . . , T do B ← sample batch from D0 ; gradient G ← 0; ˆ ) ⊗ φ(N ˆ ) do for (mi , ni ) in φ(M SNe , SNd ← F(mi , M ), F(ni , N ); Feed B into network (SNe , SNd ); Collect gradient g by Back-Propa.; G ← G + g; end Optimize M0M −N with gradient G; end Return M0M −N ALD = where Z"
2020.findings-emnlp.385,P19-1176,1,0.908774,"e 1, we first demonstrate that when there is a large gap between the predefined layer dropout during training and the actual pruning ratio during inference, LayerDrop’s performance is poor. To solve this problem, we propose to use multitask learning to train a flexible depth model by treating each supported depth configuration as a task. We reduce the supported depth space for the aggressive model compression rate and propose an effective deterministic sub-network assignment method to eliminate the mismatch between training and inference in LayerDrop. Experimental results on deep Transformer (Wang et al., 2019) show that our approach can simultaneously support decoding in 24 depth configurations and is superior to the individual training and LayerDrop. 2 2.1 Flexible depth model and LayerDrop Flexible depth model We first give the definition of flexible depth model (FDM): given a neural machine translation model MM −N whose encoder depth is M and decoder depth is N , in addition to (M,N), if MM −N can also simultaneously decode with different depth configurations (mi , ni )ki=1 where mi ≤ M and ni ≤ N and obtain the comparable performance with independently trained model Mmi −ni , we refer to MM −N"
2020.ngt-1.24,D16-1139,0,0.150864,"an opensource tensor toolkit written in C++ and CUDA 1 https://github.com/NiuTrans/NiuTensor based on dynamic computational graphs. NiuTensor is developed for facilitating NLP research and industrial deployment. The system is lightweight, high-quality, production-ready, and incorporated with the latest research ideas. We investigated with a different number of encoder/decoder layers to make trade-offs between translation performance and speed. We first trained several strong teacher models and then compressed teachers to compact student models via knowledge distillation (Hinton et al., 2015; Kim and Rush, 2016). We find that using a deep encoder (up to 35 layers) and a shallow decoder (1 layer) gives reasonable improvements in speed while maintaining high translation quality. We also optimized the Transformer model decoding in engineering, such as caching the decoder’s attention results and using low precision data type. We present teacher models and training details in Section 2, then in Section 3 we describe how to obtain lightweight student models for efficient decoding. Optimizations for the decoding across different devices are discussed in Section 4. We show the details of our submissions and"
2020.ngt-1.24,W17-3207,0,0.044102,"Missing"
2020.ngt-1.24,P07-2045,0,0.0211435,"0 , . . . , yl ) G (y0 , . . . , yl ) = l X (l+1) Wk LN (yk ) (1) (2) k=0 where yl is the output of the lt h layer and W is the weights of different layers. We employed the dynamic linear combination of layers Transformer architecture incorporated with relative position representations as our teacher network, call it Transformer-DLCL-RPR. 2.2 Training Details We followed the constrained condition of the WMT 2019 English-German news translation task and used the same data filtering method as (Li et al., 2019). We also normalized punctuation and tokenized all sentences with the Moses tokenizer (Koehn et al., 2007). The training set contains about 10M sentences pairs after processed. In our systems, the data was tokenized, and jointly byte pair encoded (Sennrich et al., 2016) with 32K merge operations using a shared vocabulary. After decoding, we removed the BPE separators and de-tokenize all tokens. We trained four teacher models using newstest2018 as the development set with fairseq (Ott et al., 2019). Table 1 shows the results of all teacher models and their ensemble, where we report SacreBLEU (Post, 2018) and the model size. The difference between teachers is the number of encoder layers and whether"
2020.ngt-1.24,N19-4009,0,0.0194714,"n of the WMT 2019 English-German news translation task and used the same data filtering method as (Li et al., 2019). We also normalized punctuation and tokenized all sentences with the Moses tokenizer (Koehn et al., 2007). The training set contains about 10M sentences pairs after processed. In our systems, the data was tokenized, and jointly byte pair encoded (Sennrich et al., 2016) with 32K merge operations using a shared vocabulary. After decoding, we removed the BPE separators and de-tokenize all tokens. We trained four teacher models using newstest2018 as the development set with fairseq (Ott et al., 2019). Table 1 shows the results of all teacher models and their ensemble, where we report SacreBLEU (Post, 2018) and the model size. The difference between teachers is the number of encoder layers and whether they contain a dynamic linear combination of layers. All teachers have 6 decoder layers, 512 hidden dimensions, and 8 attention heads. We shared the source-side and target-side embeddings with the decoder output weights. The maximum relative length was 8, and the maximum position for both source and target was 1024. We used the Adam optimizer (Kingma Model Transformer-35-6 Transformer-35-6+DL"
2020.ngt-1.24,W18-6319,0,0.0115812,"). We also normalized punctuation and tokenized all sentences with the Moses tokenizer (Koehn et al., 2007). The training set contains about 10M sentences pairs after processed. In our systems, the data was tokenized, and jointly byte pair encoded (Sennrich et al., 2016) with 32K merge operations using a shared vocabulary. After decoding, we removed the BPE separators and de-tokenize all tokens. We trained four teacher models using newstest2018 as the development set with fairseq (Ott et al., 2019). Table 1 shows the results of all teacher models and their ensemble, where we report SacreBLEU (Post, 2018) and the model size. The difference between teachers is the number of encoder layers and whether they contain a dynamic linear combination of layers. All teachers have 6 decoder layers, 512 hidden dimensions, and 8 attention heads. We shared the source-side and target-side embeddings with the decoder output weights. The maximum relative length was 8, and the maximum position for both source and target was 1024. We used the Adam optimizer (Kingma Model Transformer-35-6 Transformer-35-6+DLCL Transformer-40-6 Transformer-40-6+DLCL Ensemble Param. 152M 152M 168M 168M 640M BLEU 43.3 43.7 44.5 43.9"
2020.ngt-1.24,W18-2715,0,0.0430122,"Missing"
2020.ngt-1.24,P16-1162,0,0.0585894,"mployed the dynamic linear combination of layers Transformer architecture incorporated with relative position representations as our teacher network, call it Transformer-DLCL-RPR. 2.2 Training Details We followed the constrained condition of the WMT 2019 English-German news translation task and used the same data filtering method as (Li et al., 2019). We also normalized punctuation and tokenized all sentences with the Moses tokenizer (Koehn et al., 2007). The training set contains about 10M sentences pairs after processed. In our systems, the data was tokenized, and jointly byte pair encoded (Sennrich et al., 2016) with 32K merge operations using a shared vocabulary. After decoding, we removed the BPE separators and de-tokenize all tokens. We trained four teacher models using newstest2018 as the development set with fairseq (Ott et al., 2019). Table 1 shows the results of all teacher models and their ensemble, where we report SacreBLEU (Post, 2018) and the model size. The difference between teachers is the number of encoder layers and whether they contain a dynamic linear combination of layers. All teachers have 6 decoder layers, 512 hidden dimensions, and 8 attention heads. We shared the source-side an"
2020.ngt-1.24,N18-2074,0,0.168728,"it for NLP tasks. We explored the combination of deep encoder and shallow decoder in Transformer models via model compression and knowledge distillation. The neural machine translation decoding also benefits from FP16 inference, attention caching, dynamic batching, and batch pruning. Our systems achieve promising results in both translation quality and efficiency, e.g., our fastest system can translate more than 40,000 tokens per second with an RTX 2080 Ti while maintaining 42.9 BLEU on newstest2018. 1 Introduction In recent years, the Transformer model and its variants (Vaswani et al., 2017; Shaw et al., 2018; So et al., 2019; Wu et al., 2019; Wang et al., 2019) have established state-of-the-art results on machine translation (MT) tasks. However, achieving high performance requires an enormous amount of computations (Strubell et al., 2019), limiting the deployment of these models on devices with constrained hardware resources. The efficiency task aims at developing MT systems to achieve not only translation accuracy but also memory efficiency or translation speed across different devices. This competition constraints systems to translate 1 million English sentences within 2 hours. Our goal is to i"
2020.ngt-1.24,P17-2091,0,0.0278461,"Missing"
2020.ngt-1.24,P19-1355,0,0.0295517,"ttention caching, dynamic batching, and batch pruning. Our systems achieve promising results in both translation quality and efficiency, e.g., our fastest system can translate more than 40,000 tokens per second with an RTX 2080 Ti while maintaining 42.9 BLEU on newstest2018. 1 Introduction In recent years, the Transformer model and its variants (Vaswani et al., 2017; Shaw et al., 2018; So et al., 2019; Wu et al., 2019; Wang et al., 2019) have established state-of-the-art results on machine translation (MT) tasks. However, achieving high performance requires an enormous amount of computations (Strubell et al., 2019), limiting the deployment of these models on devices with constrained hardware resources. The efficiency task aims at developing MT systems to achieve not only translation accuracy but also memory efficiency or translation speed across different devices. This competition constraints systems to translate 1 million English sentences within 2 hours. Our goal is to improve the quality of translations while maintaining enough speed. We participated in both CPUs and GPUs tracks in the shared task. Our system was built with NiuTensor, an opensource tensor toolkit written in C++ and CUDA 1 https://git"
2020.ngt-1.24,P19-1176,1,0.920074,"encoder and shallow decoder in Transformer models via model compression and knowledge distillation. The neural machine translation decoding also benefits from FP16 inference, attention caching, dynamic batching, and batch pruning. Our systems achieve promising results in both translation quality and efficiency, e.g., our fastest system can translate more than 40,000 tokens per second with an RTX 2080 Ti while maintaining 42.9 BLEU on newstest2018. 1 Introduction In recent years, the Transformer model and its variants (Vaswani et al., 2017; Shaw et al., 2018; So et al., 2019; Wu et al., 2019; Wang et al., 2019) have established state-of-the-art results on machine translation (MT) tasks. However, achieving high performance requires an enormous amount of computations (Strubell et al., 2019), limiting the deployment of these models on devices with constrained hardware resources. The efficiency task aims at developing MT systems to achieve not only translation accuracy but also memory efficiency or translation speed across different devices. This competition constraints systems to translate 1 million English sentences within 2 hours. Our goal is to improve the quality of translations while maintaining e"
2020.ngt-1.24,C18-1255,1,0.752758,"ang et al., 2019; Li et al., 2020) focus on designing new attention mechanisms and Transformer architectures. Shaw et al. (2018) extended the self-attention to consider the relative position representations or distances between words. Wu et al. (2019) replaced the self-attention components with lightweight and dynamic convolutions. Deep Transformer mod204 Proceedings of the 4th Workshop on Neural Generation and Translation (WNGT 2020), pages 204–210 c Online, July 10, 2020. 2020 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d els also attracted a lot of attention. Wang et al. (2018) proposed a multi-layer representation fusion approach to learn a better representation from the stack. Wang et al. (2019) analyzed the high risk of gradient vanishing or exploring in the standard Transformer, which place the layer normalization (Ba et al., 2016) after the attention and feed-forward components. They showed that a deep Transformer model can surpass the big one by proper use of layer normalization and dynamic combinations of different layers. In their method, the input of layer l + 1 is defined by: xl+1 = G (y0 , . . . , yl ) G (y0 , . . . , yl ) = l X (l+1) Wk LN (yk ) (1) (2)"
2020.ngt-1.24,D19-1083,0,0.0348527,"Missing"
2020.wmt-1.117,C04-1046,0,0.164638,"models and multilingual pretraining methods significantly improve translation quality estimation performance. Our system achieved remarkable results in multiple level tasks, e.g., our submissions obtained the best results on all tracks in the sentence-level Direct Assessment task1 . 1 • We propose a simple strategy to convert document-level tasks into word- and sentencelevel tasks. • We explore effective ensemble methods for both word- and sentence-level predictions. Introduction Quality estimation (QE) evaluates the quality of machine translation output without human reference translations (Blatz et al., 2004). It has a wide range of applications in post-editing and quality control for machine translation. We participated in all tasks and language pairs at the WMT 2020 QE shared task2 , including sentence-level Direct Assessment tasks, word and sentence-level post-editing effort tasks, and document-level QE tasks. We investigated transfer learning and ensemble methods using recently proposed multilingual pre-trained models (Devlin et al., 2019; Conneau et al., 2020) as well as deep transformer models (Wang et al., 2019a). Our main contributions are as follows: • We apply multi-phase pretraining (Gu"
2020.wmt-1.117,2020.acl-main.747,0,0.128071,"Missing"
2020.wmt-1.117,N19-1423,0,0.176242,"nd sentence-level predictions. Introduction Quality estimation (QE) evaluates the quality of machine translation output without human reference translations (Blatz et al., 2004). It has a wide range of applications in post-editing and quality control for machine translation. We participated in all tasks and language pairs at the WMT 2020 QE shared task2 , including sentence-level Direct Assessment tasks, word and sentence-level post-editing effort tasks, and document-level QE tasks. We investigated transfer learning and ensemble methods using recently proposed multilingual pre-trained models (Devlin et al., 2019; Conneau et al., 2020) as well as deep transformer models (Wang et al., 2019a). Our main contributions are as follows: • We apply multi-phase pretraining (Gururangan et al., 2020) methods under both high- and low-resource settings to QE tasks. 1 Our number of submissions exceeded the daily or total limit. 2 http://www.statmt.org/wmt20/ quality-estimation-task.html Results on different level tasks show that our methods are very competitive. Our submissions achieved the best Pearson correlation on all language pairs of the sentence-level Direct Assessment task and the best results on English-Ch"
2020.wmt-1.117,2020.acl-main.740,0,0.0493901,"Missing"
2020.wmt-1.117,W19-5406,0,0.0713145,"Missing"
2020.wmt-1.117,P19-3020,0,0.0497136,"Missing"
2020.wmt-1.117,W17-4763,0,0.0651883,"ps://en.wikipedia.org/wiki/ Matthews_correlation_coefficient 1019 3.1 4 Datasets and Resources The labeled data consists of 7K sentences for training and 1K sentences for development for each language pair. We used the additional parallel data provided by the organizers to train predictors, containing about 20M En-Zh sentence pairs and 23M En-De sentence pairs after pre-processing with the NiuTrans SMT toolkit (Xiao et al., 2012). Pretrained language models include mBERT and XLM-R, were also used for Task 2. 3.2 Predictor-Estimator Models The predictor-estimator architecture and its variants (Kim et al., 2017; Kepler et al., 2019b) had established state-of-the-art on WMT QE tasks. The system consists of a word prediction module (predictor) trained from additional large-scale parallel corpora and a quality estimation module (estimator) trained from quality-annotated data. For the sentence-level tasks and target-side wordlevel tasks, we employed the official bi-RNN predictor-estimator trained with OpenKiwi (Kepler et al., 2019b) as the baseline. Similar to Wang et al. (2019b), we used NMT models trained with back-translation as predictors. The original predictor and estimator use RNNs to encode the"
2020.wmt-1.117,P16-1162,0,0.0348623,"tasks, we employed the official bi-RNN predictor-estimator trained with OpenKiwi (Kepler et al., 2019b) as the baseline. Similar to Wang et al. (2019b), we used NMT models trained with back-translation as predictors. The original predictor and estimator use RNNs to encode the source and predict tags or scores. We also implemented two transformer-based predictors which replace the RNN with transformer (Vaswani et al., 2017) or deep transformer architectures (Wang et al., 2019a; Li et al., 2019). We compared different tokenizing strategies such as word segmentation and byte pair encoding (BPE) (Sennrich et al., 2016) for all language pairs. 3.3 Multi-task learning The word- and sentence-level tasks are highly related to their annotations are commonly based on the HTER measure. We used a linear summation of sentence-level and target word-level objective losses as follows: L = Lmt.word + Lmt.gap + LHT ER (4) where the components denote the loss of targetword, target-gap, and predictions for HTER score. We also trained models using source sentence and origin/post-edited MT output to predict the source-side word level tags: LSRC = Lsrc−mt + Lsrc−pe (5) Document-Level QE Task This task aims to predict document"
2020.wmt-1.117,2020.wmt-1.79,0,0.0617894,"Missing"
2020.wmt-1.117,2020.acl-main.558,0,0.597273,"Missing"
2020.wmt-1.117,2016.amta-researchers.2,0,0.256191,"Missing"
2020.wmt-1.117,P19-1176,1,0.909104,"he quality of machine translation output without human reference translations (Blatz et al., 2004). It has a wide range of applications in post-editing and quality control for machine translation. We participated in all tasks and language pairs at the WMT 2020 QE shared task2 , including sentence-level Direct Assessment tasks, word and sentence-level post-editing effort tasks, and document-level QE tasks. We investigated transfer learning and ensemble methods using recently proposed multilingual pre-trained models (Devlin et al., 2019; Conneau et al., 2020) as well as deep transformer models (Wang et al., 2019a). Our main contributions are as follows: • We apply multi-phase pretraining (Gururangan et al., 2020) methods under both high- and low-resource settings to QE tasks. 1 Our number of submissions exceeded the daily or total limit. 2 http://www.statmt.org/wmt20/ quality-estimation-task.html Results on different level tasks show that our methods are very competitive. Our submissions achieved the best Pearson correlation on all language pairs of the sentence-level Direct Assessment task and the best results on English-Chinese post-editing effort tasks. We present methods for the sentence-level Di"
2020.wmt-1.117,P12-3004,1,0.778359,"earson’s correlation for the HTER prediction. There are two language pairs in both the word- and sentencelevel tasks, including English-German (En-De) and English-Chinese (En-Zh). 5 https://en.wikipedia.org/wiki/ Matthews_correlation_coefficient 1019 3.1 4 Datasets and Resources The labeled data consists of 7K sentences for training and 1K sentences for development for each language pair. We used the additional parallel data provided by the organizers to train predictors, containing about 20M En-Zh sentence pairs and 23M En-De sentence pairs after pre-processing with the NiuTrans SMT toolkit (Xiao et al., 2012). Pretrained language models include mBERT and XLM-R, were also used for Task 2. 3.2 Predictor-Estimator Models The predictor-estimator architecture and its variants (Kim et al., 2017; Kepler et al., 2019b) had established state-of-the-art on WMT QE tasks. The system consists of a word prediction module (predictor) trained from additional large-scale parallel corpora and a quality estimation module (estimator) trained from quality-annotated data. For the sentence-level tasks and target-side wordlevel tasks, we employed the official bi-RNN predictor-estimator trained with OpenKiwi (Kepler et al"
2020.wmt-1.117,W19-5410,0,0.0493715,"Missing"
2020.wmt-1.117,W19-5407,0,0.0375846,"Missing"
2020.wmt-1.37,W19-5304,0,0.0164178,"ments on the structures of the model in Table 1. And we kept six decoder layers unchanged because it only could gain a few improvements tough many model parameters increased. Deep Network: This model structure simply changes encoder layers, hidden size and other hyper-parameters based on vanilla Transformer. DLCL Network: For a deeper network, we employed DLCL (Wang et al., 2019) to get more diverse models. Filter size: This hyper-parameter represents the dimension size of feed-forward network (FFN) and simply increasing this could bring some improvements (Wang et al., 2018; Sun et al., 2019; Bawden et al., 2019). Notably, when using the deep Transformer architecture, the training time and model parameters will increase sharply with the augment of the FFN size. egy is a very effective approach to improve performance when the test set only composed of manual translations and we mainly reused (Li et al., 2019) iterative KD strategy to implement selflearning. Specifically, we designed a new iterative fine-tuning process which consists of three steps: 1)using ensemble models to decode valid and test source side sentences then fine-tune models with those pseudo data, 2) fine-tune with the valid set by a sm"
2020.wmt-1.37,P07-2045,0,0.0128684,"and wider Transformer (Vaswani et al., 2017) architectures to get reliable baselines, nucleus sampling (Holtzman et al., 2020) in backtranslation to generate more suitable pseudo bilingual sentences, more effectively fine-tuning strategy to adapt domain. Particularly in the lowresources tasks, {TA,IU}→EN, we built multilingual neural machine translation by using some similar language to get better performance and further 2 System Overview 2.1 Data Preprocessing and Filtering For EN→ZH and JA↔EN tasks, we first normalized the punctuation in Chinese and Japanese monolingual data by using Moses (Koehn et al., 2007) normalize-punctuation.perl script. English and Inuktitut sentences were segmented by Moses, while Chinese, Japanese and Tamil used NiuTrans (Xiao et al., 2012), MeCab1 and IndicNLP2 separately for word segmentation. After converting numbers and punctuation into English pattern, and then we normalized English words in Japanese sentences to Japanese by using Sudachi (Takaoka et al., 2018). As previous work (Wang et al., 2018) indicated that it’s important to clean data strictly, so this year 1 2 https://github.com/taku910/mecab https://github.com/anoopkunchukuttan/indic nlp library 338 Proceedi"
2020.wmt-1.37,W19-5325,1,0.881035,"Missing"
2020.wmt-1.37,2020.emnlp-main.72,1,0.848762,"Missing"
2020.wmt-1.37,W19-5333,0,0.0709345,"tle translation quality. RPR and relative length: The relative position representation (RPR) (Shaw et al., 2018) improves self-attention by adding relative position information. The relative length which we set 8 is the key parameter of this method. For choosing models to ensemble, we utilized the ensemble search method which used a script to traverse all possible combinations then recorded the best one. For JA↔EN, we chose 6 of 10 while other tasks were 4 of 10. 2.5 Iterative KD and Fine-tuning Sun et al. (2019) showed the self-learning strat340 2.6 Reranking For JA↔EN tasks, we followed the Ng et al. (2019), using a neural language model, and a reverse translation model. Different from the last year, we used several length penalties to generate more candidates. 2.7 Post Editing For tasks to the English side, we only confirmed the numbers whether to generate correctly by designing a rule-based script which generated two lists for source and target sentences separately. For EN→ZH, the strategy was the same as the last year Li et al. (2019) and particularly dealt with the name’s translation by using rules to delete the English name copy in Chinese sentences. For EN→JA task, we transferred English p"
2020.wmt-1.37,N19-4009,0,0.0212851,"Japanese pattern. ... Model n Model 2 Model 1 Start End Ensemble decode Pseudo data using {valid, test} Models Models (Original) (Original) Valid data Models Models (Domain (Domain tune) tune) Pseudo data using test Models Models (Dev (Dev tune) tune) Models Models (Test (Test self-learing) self-learing) Figure 1: Iterative fine-tuning process 3 3.1 Experiment Experiment Settings For all tasks, we implemented the TransformerBase as our baseline and all of our architectures were pre-normalize Wang et al. (2019) for stable training except Transformer-Big. We implemented models based on Fairseq (Ott et al., 2019) and trained on eight 2080Ti GPUs. We used Adam optimizer (Kingma and Ba, 2014) during training, β1 = 0.9, β2 = 0.997 for pre-normalize architectures and training batch was 2048 token while we accumulated gradient 4 times for achieving bigger batch size. We shuffled the training data before generate training batch and the training batch each epoch, so we didn’t consider the document information. The max learning rate and warmup-steps we set were 0.002 and 8000 separately for deep models but 0.0016 and 16000 for deep and wide models. During training, we used fp16 to accelerate training with few"
2020.wmt-1.37,P16-1162,0,0.0526379,"ion Task Corpus, TED Talks total six parallel data corpus about 14.35 million and News crawl, News Commentary, Common Crawl , TED Talks 4 Japanese monolingual data corpus about 1.7 billion. After the data filter, 12 million parallel data was left and 11 million selected by the neural language model was used as training data. Cleaning several billion low-quality monolingual data will cost too much time, so here we shuffled all the data then split it into dozens of parts, one of which was 20 million. Finally we used total eight of them, each piece was carefully cleaned. Before we also used BPE (Sennrich et al., 2016) models with 32,000 merge operations for both sides to reduce UNK size in vocabulary. We implemented back-translation two times, the first was beam search while the second was Nucleus Sampling to generate translations. Each time we selected 12 million mono data sampled from all the remaining data. Tough the second time didn’t increase significantly compared with the first time, the performance was further improved with the increase of the model parameters. Considering the training time, we finally chose 35 million training data on both sides. Notably, as the official stated that the test targe"
2020.wmt-1.37,N18-2074,0,0.0243475,"lid set by a small training batch and learning rate, 3) selflearning with in-domain data which chose by only test source side. Repeat these steps two or three times according to the increase of the valid score in the third step. Figure 1 shows these steps. Notably, for being consistent with the composition of the test set, we picked out the data that the source side is real while the target side is manual from the previous valid set. In this way, we found that iterative fine-tuning can promote news title translation quality. RPR and relative length: The relative position representation (RPR) (Shaw et al., 2018) improves self-attention by adding relative position information. The relative length which we set 8 is the key parameter of this method. For choosing models to ensemble, we utilized the ensemble search method which used a script to traverse all possible combinations then recorded the best one. For JA↔EN, we chose 6 of 10 while other tasks were 4 of 10. 2.5 Iterative KD and Fine-tuning Sun et al. (2019) showed the self-learning strat340 2.6 Reranking For JA↔EN tasks, we followed the Ng et al. (2019), using a neural language model, and a reverse translation model. Different from the last year,"
2020.wmt-1.37,W19-5341,0,0.0935248,"carried out experiments on the structures of the model in Table 1. And we kept six decoder layers unchanged because it only could gain a few improvements tough many model parameters increased. Deep Network: This model structure simply changes encoder layers, hidden size and other hyper-parameters based on vanilla Transformer. DLCL Network: For a deeper network, we employed DLCL (Wang et al., 2019) to get more diverse models. Filter size: This hyper-parameter represents the dimension size of feed-forward network (FFN) and simply increasing this could bring some improvements (Wang et al., 2018; Sun et al., 2019; Bawden et al., 2019). Notably, when using the deep Transformer architecture, the training time and model parameters will increase sharply with the augment of the FFN size. egy is a very effective approach to improve performance when the test set only composed of manual translations and we mainly reused (Li et al., 2019) iterative KD strategy to implement selflearning. Specifically, we designed a new iterative fine-tuning process which consists of three steps: 1)using ensemble models to decode valid and test source side sentences then fine-tune models with those pseudo data, 2) fine-tune with"
2020.wmt-1.37,D19-1167,0,0.0285461,"d gained 0.18 increase, because different models were too similar after fine-tuning. And we fixed the punctuation and the score improved 0.52 BLEU. During the post process, we fixed the number and punctuation translation. 3.5 Valid 12.8 14.2 19.2 20.9 22.8 23.4 23.6 23.8 TA→EN Results The Ta→EN task is similar to IU→EN but more complicated, because more data corpus and language can be used to build the multilingual system. Specifically, we total used {Hindi (HI), Kannada (KN), Malayalam (ML), Punjabi (PA), Telugu (TE), Urdu (UR)}→EN total six other languages, 17 million sentences according to Kudugunta et al. (2019)’s work showed similar languages with TA. From Table 5, it can be seen that using similar languages to build a multilingual system can indeed improve the performance. Also, using iterative back-translation is still an effective way but couldn’t add too much pseudo language data because this will make the real target language data account for the whole data was too small, which leaded to performance damage. During the backtranslation process, due to too many languages in one model, we followed Johnson et al. (2017)’s approach to build a reverse model to ensure translation quality. For the model"
2020.wmt-1.37,L18-1355,0,0.0265925,"o get better performance and further 2 System Overview 2.1 Data Preprocessing and Filtering For EN→ZH and JA↔EN tasks, we first normalized the punctuation in Chinese and Japanese monolingual data by using Moses (Koehn et al., 2007) normalize-punctuation.perl script. English and Inuktitut sentences were segmented by Moses, while Chinese, Japanese and Tamil used NiuTrans (Xiao et al., 2012), MeCab1 and IndicNLP2 separately for word segmentation. After converting numbers and punctuation into English pattern, and then we normalized English words in Japanese sentences to Japanese by using Sudachi (Takaoka et al., 2018). As previous work (Wang et al., 2018) indicated that it’s important to clean data strictly, so this year 1 2 https://github.com/taku910/mecab https://github.com/anoopkunchukuttan/indic nlp library 338 Proceedings of the 5th Conference on Machine Translation (WMT), pages 338–345 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics we used a stricter data filter scheme than Li et al. (2019) and the rules were following: • Filter sentences length ratio lower than 0.4 or upper than 3 and punctuation ratio more than 0.3. • Remove sentences that have the long word which co"
2020.wmt-1.37,W18-6430,1,0.892322,"he performance will significantly improve. Also, iterative fine-tuning strategy we implemented is effective during adapting domain. For Inuktitut→English and Tamil→English tasks, we built multilingual models separately and employed pretraining word embedding to obtain better performance. 1 Introduction This paper describes the NiuTrans submissions to the WMT20 news tasks, including English→Chinese (EN→ZH), Tamil→English (TA→EN), Inuktitut→English (IU→EN) and Japanese↔English (JA↔EN) five directions and all of our systems were built with constrained data sets. Some useful methods in the WMT18 (Wang et al., 2018) and WMT19 (Li et al., 2019) submissions are also reused this time, such as model ensemble, knowledge distillation (KD) et al., and we explore some novel approaches this year. For this participation, we experimented with some deeper and wider Transformer (Vaswani et al., 2017) architectures to get reliable baselines, nucleus sampling (Holtzman et al., 2020) in backtranslation to generate more suitable pseudo bilingual sentences, more effectively fine-tuning strategy to adapt domain. Particularly in the lowresources tasks, {TA,IU}→EN, we built multilingual neural machine translation by using so"
2020.wmt-1.37,P19-1176,1,0.912468,"r learning, we utilized only one model which all the language shared the same parameters including word embeddings and vocab. Bilingual data were reused to fine-tune the model for adapting parameters to the target language after model convergence. 339 Model Tag Base Big Deep25 Deep25-filter Deep30-RPR DLCL35-RPR DLCL40-RPR Deep15-filter-768-RPR Depth 6 6 25 25 30 35 40 15 Hidden Size 512 1024 512 512 512 512 512 768 Filter Size 2048 4096 2048 4096 2048 2048 2048 4096 RPR Attention 7 7 7 7 3 3 3 3 Table 1: Transformer Architectures. 2.4 Model Architectures and Ensemble Inspired by deep network Wang et al. (2019), we tried to use simple deep, or deep and wide network architectures based on the Transformer to explore the relationship of performance and model parameters. We mainly carried out experiments on the structures of the model in Table 1. And we kept six decoder layers unchanged because it only could gain a few improvements tough many model parameters increased. Deep Network: This model structure simply changes encoder layers, hidden size and other hyper-parameters based on vanilla Transformer. DLCL Network: For a deeper network, we employed DLCL (Wang et al., 2019) to get more diverse models. F"
2020.wmt-1.37,P12-3004,1,0.800483,"e suitable pseudo bilingual sentences, more effectively fine-tuning strategy to adapt domain. Particularly in the lowresources tasks, {TA,IU}→EN, we built multilingual neural machine translation by using some similar language to get better performance and further 2 System Overview 2.1 Data Preprocessing and Filtering For EN→ZH and JA↔EN tasks, we first normalized the punctuation in Chinese and Japanese monolingual data by using Moses (Koehn et al., 2007) normalize-punctuation.perl script. English and Inuktitut sentences were segmented by Moses, while Chinese, Japanese and Tamil used NiuTrans (Xiao et al., 2012), MeCab1 and IndicNLP2 separately for word segmentation. After converting numbers and punctuation into English pattern, and then we normalized English words in Japanese sentences to Japanese by using Sudachi (Takaoka et al., 2018). As previous work (Wang et al., 2018) indicated that it’s important to clean data strictly, so this year 1 2 https://github.com/taku910/mecab https://github.com/anoopkunchukuttan/indic nlp library 338 Proceedings of the 5th Conference on Machine Translation (WMT), pages 338–345 c Online, November 19–20, 2020. 2020 Association for Computational Linguistics we used a s"
2021.acl-long.162,R19-1050,0,0.0227496,"89.11 sent./s 289.80 sent./s 88.42 sent./s 225.46 sent./s 214.06 sent./s 247.90 sent./s 194.23 sent./s 199.74 sent./s 199.29 sent./s 158.29 sent./s 321.79 sent./s 412.91 sent./s 406.68 sent./s 281.97 sent./s 306.91 sent./s 309.11 sent./s Speedup 1.00× 2.34× 2.51× 2.60× 2.03× 2.09× 2.09× 1.00× 2.55× 2.42× 2.80× 2.20× 2.26× 2.25× 1.00× 2.03× 2.61× 2.57× 1.78× 1.94× 1.95× Table 1: Results of Transformer-base on different tasks (sent./s: translated sentences per second). 4.2 Model Setup Our baseline system is based on the open-source implementation of the Transformer model presented in Ott et al. (2019)’s work. For all machine translation tasks, we experiment with the Transformer-base (base) setting. We additionally run the Transformer-big (big) (Vaswani et al., 2017) and Transformer-deep (deep) (Wang et al., 2019; Zhang et al., 2020) settings on the large En-De dataset. All systems consist of a 6-layer encoder and a 6-layer decoder, except that the Transformerdeep encoder has 48 layers (depth) (Li et al., 2020). The embedding size (width) is set to 512 for Transformer-base/deep and 1,024 for Transformerbig. The FFN hidden size equals to 4× embedding size in all settings. We stop training un"
2021.acl-long.162,P19-1356,0,0.0175529,"ight matrices by the weight class they belong to, i.e., different weight classes clusters all their instantiations to form their own groups. In the previous example, the W1 weight class will form a group [T1 , T2 , · · · , TLt ], where each Ti is the W1 weight instance in the i-th FFN and Lt is the number of layers in the teacher network. These weight matrices are then used to generate the W1 weight instances in the student network. The parameter generator further divides each group into smaller subsets with weight matrices from adjacent layers, because the adjacent layers function similarly (Jawahar et al., 2019) and so as their weights. This way additionally makes the later transformation more light-weighted. Namely, given a group of Lt weight matrices, the parameter generator splits it into Ls subsets, where Ls is the number of layers in the student network. For example, the i-th subset of the group of  W1 weight class in the previous example will be T(i−1)∗Lt /Ls +1 , T(i−1)∗Lt /Ls +2 , · · · , Ti∗Lt /Ls . This subset is used to generate the weight matrix Si , which corresponds to W1 weight instance in the i-th FFN of the student network. 3.1.2 Weight Transformation Given a subset of teacher weig"
2021.acl-long.162,P07-1034,0,0.104151,"ew source of knowledge and a new way to leverage this knowledge. It transfers the knowledge in parameters of the teacher network to the student network via a parameter generator. Therefore, it is orthogonal to other knowledge distillation variants. 6.2 Transfer Learning Transfer learning aims at transferring knowledge from a source domain to a target domain. Based on what knowledge is transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007; Dai et al., 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al., 2018; Gao et al., 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019). Perhaps the most related work is Platanios et al. (2018)’s work. Their method falls into the parameter-based category. They use a universal parameter generator to share the knowledge among translation tasks. This parameter generator pro2083 duces a translation model"
2021.acl-long.162,N18-1202,0,0.0431075,"network via a parameter generator. Therefore, it is orthogonal to other knowledge distillation variants. 6.2 Transfer Learning Transfer learning aims at transferring knowledge from a source domain to a target domain. Based on what knowledge is transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007; Dai et al., 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al., 2018; Gao et al., 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019). Perhaps the most related work is Platanios et al. (2018)’s work. Their method falls into the parameter-based category. They use a universal parameter generator to share the knowledge among translation tasks. This parameter generator pro2083 duces a translation model from a given languagespecific embedding. Though we similarly employ the idea of a parameter generator, our weight distillation aims at transfe"
2021.acl-long.162,D18-1039,0,0.0266079,"transferred to the model in the target domain, transfer learning methods can be classified into three categories (Pan and Yang, 2010): instance-based methods reuse certain parts of the data in the source domain (Jiang and Zhai, 2007; Dai et al., 2007); feature-based methods use the representation from the model learned in the source domain as the input (Peters et al., 2018; Gao et al., 2008); parameter-based methods directly fine-tune the model learned in the source domain with the target domain data (Yang et al., 2019; Liu et al., 2019; Devlin et al., 2019). Perhaps the most related work is Platanios et al. (2018)’s work. Their method falls into the parameter-based category. They use a universal parameter generator to share the knowledge among translation tasks. This parameter generator pro2083 duces a translation model from a given languagespecific embedding. Though we similarly employ the idea of a parameter generator, our weight distillation aims at transferring knowledge from one model to another rather than from one translation task to another. Therefore our parameter generator takes a model instead of a language-specific embedding as its input and is only used once. 7 Conclusion In this work, we"
2021.acl-long.162,2020.acl-main.686,0,0.0350292,"0.37 +0.76 +0.80 Valid 50.83 51.87 51.62 51.34 51.22 51.26 51.28 ∆BLEU 0.00 +1.04 +0.79 +0.51 +0.39 +0.43 +0.45 W D 1 2 3 4 5 6 256 512 BLEUKD/WD Params BLEUKD/WD Params 38.46/40.34 30M 43.51/45.39 65M 45.33/47.21 32M 50.02/50.45 72M 47.30/49.09 34M 51.18/51.99 80M 47.90/50.08 36M 51.05/52.05 87M 48.87/50.70 38M 52.15/52.00 94M 49.78/50.73 40M 52.40/53.09 102M Table 4: Ablation study of using different weight matrices solely. Table 5: Compression study with various depth (D) and width (W) of both the encoder and decoder. widths, we slice the teacher weight matrices to fit the student network (Wang et al., 2020). Table 3 shows that initializing the student networks with the teacher parameters improves KD, supporting our claim that knowledge in parameters is complementary to KD but missed. We also see that WD outperforms this simple initialization, which implies that using all teacher parameters helps to obtain a better student. the warmup steps than the learning rate. This is because more warmup steps will run the network with a high learning rate in a longer period. A high learning rate has been proven to be harmful as shown in the middle part of Fig. 3. 5.2 Sensitivity Analysis The left part of Fig"
2021.acl-long.162,P19-1176,1,0.909905,"./s 306.91 sent./s 309.11 sent./s Speedup 1.00× 2.34× 2.51× 2.60× 2.03× 2.09× 2.09× 1.00× 2.55× 2.42× 2.80× 2.20× 2.26× 2.25× 1.00× 2.03× 2.61× 2.57× 1.78× 1.94× 1.95× Table 1: Results of Transformer-base on different tasks (sent./s: translated sentences per second). 4.2 Model Setup Our baseline system is based on the open-source implementation of the Transformer model presented in Ott et al. (2019)’s work. For all machine translation tasks, we experiment with the Transformer-base (base) setting. We additionally run the Transformer-big (big) (Vaswani et al., 2017) and Transformer-deep (deep) (Wang et al., 2019; Zhang et al., 2020) settings on the large En-De dataset. All systems consist of a 6-layer encoder and a 6-layer decoder, except that the Transformerdeep encoder has 48 layers (depth) (Li et al., 2020). The embedding size (width) is set to 512 for Transformer-base/deep and 1,024 for Transformerbig. The FFN hidden size equals to 4× embedding size in all settings. We stop training until the model stops improving on the validation set. All experiments are done on 8 NVIDIA TITIAN V GPUs with mixed-precision training (Micikevicius et al., 2018). At test time, the model is decoded with a beam of wi"
2021.acl-long.162,P12-3004,1,0.76102,"ovided within NIST12 OpenMT1 . We choose the evaluation data of mt06 as the validation set, and mt08 as the test set. For the En-De task, we use the WMT14 EnglishGerman dataset (4.5M pairs). We share the source and target vocabularies. We choose newstest-2013 as the validation set and newstest-2014 as the test set. For all datasets, we tokenize every sentence using the script in the Moses toolkit and segment every word into subword units using Byte-Pair Encoding (Sennrich et al., 2016). The number of the BPE merge operations is set to 32K. We remove sentences with more than 250 subword units (Xiao et al., 2012). In addition, we evaluate the results using multi-bleu.perl. 1 LDC2000T46, LDC2000T47, LDC2000T50, LDC2003E14, LDC2005T10, LDC2002E18, LDC2007T09, LDC2004T08 2079 WMT16 En-Ro NIST12 Zh-En WMT14 En-De System Teacher T INY + KD + WD S MALL + KD + WD Teacher T INY + KD + WD S MALL + KD + WD Teacher T INY + KD + WD S MALL + KD + WD Depth 6 1 1 1 2 2 2 6 1 1 1 2 2 2 6 1 1 1 2 2 2 Width 512 256 256 256 512 512 512 512 256 256 256 512 512 512 512 256 256 256 512 512 512 Test 31.64 29.65 30.03 30.89 31.22 30.97 31.65 45.14 41.90 42.78 44.60 44.30 44.89 46.20 27.47 24.62 26.51 27.12 26.68 27.47 28.18"
2021.acl-long.204,2020.emnlp-main.644,0,0.062272,"9; Dong et al., 2021). Generally, MTL requires a careful design of the loss functions and more complicated architectures. In a similar way, more recent work pre-trains different components of the ST system, and consolidates them into one. For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020). More sophisticated methods include better training and fine-tuning (Wang et al., 2020a,b), the shrink mechanism (Liu et al., 2020), the adversarial regularizer (Alinejad and Sarkar, 2020), and etc. Although pre-trained models have quickly become dominant in many NLP tasks, they are still found to underperform the cascaded model in ST. This motivates us to explore the reasons why this happens and methods to solve the problems accordingly. 3 Why is ST Encoding Difficult? Following previous work in end-to-end models (Berard et al., 2016; Weiss et al., 2017), we envision an encoding-decoding process in which an input sequence is encoded into a representation vector, and the vector is then decoded into an output sequence. 2620 ASR MT Below CTC 0.60 0.40 0.20 BLEU(ST) 0.60 Localness"
2021.acl-long.204,N18-1008,0,0.0443448,"Missing"
2021.acl-long.204,N19-1006,0,0.077468,"additional training data is allowed for ASR and MT. Introduction Corresponding author The source code is available at https://github.com/xuchen neu/SATE 1 Model the pipeline of translation (Duong et al., 2016; Berard et al., 2016; Weiss et al., 2017). Promising results on small-scale tasks are generally favorable. However, speech-to-translation paired data is scarce. Researchers typically use pre-trained Automatic Speech Recognition (ASR) and Machine Translation (MT) models to boost ST systems (Berard et al., 2018). For example, one can initialize the ST encoder using a large-scale ASR model (Bansal et al., 2019). But we note that, despite significant development effort, our end-to-end ST system with pre-trained models was not able to outperform the cascaded ST counterpart when the ASR and MT data size was orders of magnitude larger than that of ST (see Table 1). In this paper, we explore reasons why pretraining has been challenging in ST, and how pretrained ASR and MT models might be used together to improve ST. We find that the ST encoder plays both roles of acoustic encoding and textual encoding. This makes it problematic to view an ST encoder as either an individual ASR encoder or an individual MT"
2021.acl-long.204,N16-1109,0,0.272408,"reated ST as a pipeline of running an ASR system and an MT system sequentially (Ney, 1999; Mathias and Byrne, 2006; Schultz et al., 2004). This allows the use of off-the-shelf models, and was (and is) popular in practical ST systems. However, these systems were sensitive to the errors introduced by different component systems and the high latency of the long pipeline. As another stream in the ST area, end-to-end methods have been promising recently (Berard et al., 2016; Weiss et al., 2017; Berard et al., 2018). The rise of end-to-end ST can be traced back to the success of deep neural models (Duong et al., 2016). But, unlike other well-defined tasks in deep learning, annotated speech-to-translation data is scarce, which prevents well-trained ST models. A simple solution to this issue is data augmentation (Pino et al., 2019, 2020). This method is model-free but generating large-scale synthetic data is time consuming. As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021). Generally, MTL requires"
2021.acl-long.204,N19-1202,0,0.186725,"Missing"
2021.acl-long.204,2020.acl-demos.34,0,0.0147882,"the TED talks. We run the experiments on the English-German speech translation dataset of 400 hours speech with 230K utterances. We select the model on the dev set (1,408 utterances) and report results on the tstCOMMON set (2,641 utterances). Unrestricted Setting We use the additional ASR and MT data for pre-training. The 960 hours LibriSpeech ASR corpus is used for the English ASR model. We extract 10M sentences pairs from the WMT14 English-French and 18M sentence pairs from the Opensubtitle20183 English-German translation datasets. Preprocessing Followed the preprocessing recipes of ESPnet (Inaguma et al., 2020), we remove the utterances of more than 3,000 frames and augment speech data by speed perturbation with factors of 0.9, 1.0, and 1.1. The 80-channel log-mel filterbank coefficients with 3-dimensional pitch features are extracted for speech data. We use the lower-cased transcriptions without punctuations. The text is tokenized using the scripts of Moses (Koehn et al., 2007). We learn Byte-Pair Encoding (Sennrich et al., 2016) subword segmentation with 10,000 merge operations based on a shared source and target vocabulary for all datasets. All experiments are implemented based on the ESPnet tool"
2021.acl-long.204,L18-1001,0,0.0282569,", and MT models. Q(·|·) is the teacher distribution and P(·|·) is the student distribution. θASR , θCTC , θMT and θST are the model parameters. We can rewrite Eq. (8) to obtain a new loss: L = α · β · LCTC + (1 − β) · LKD CTC  +(1 − α) ·  γ · LTrans + (1 − γ) · LKD Trans (14) where both β and γ are the hyper-parameters that balance the preference between the teacher distribution and the ground truth. 2623 5 Experiments 5.1 5.2 Datasets and Preprocessing We consider restricted and unrestricted settings on speech translation tasks. We run experiments on the LibriSpeech English-French (En-Fr) (Kocabiyikoglu et al., 2018) and MuST-C EnglishGerman (En-De) (Gangi et al., 2019) corpora, which correspond to the low-resource and highresource datasets respectively. Available ASR and MT data is only from the ST data under the restricted setting. For comparison in practical scenarios, the unrestricted setting allows the additional data for ASR and MT models. LibriSpeech En-Fr Followed previous work, we use the clean speech translation training set of 100 hours, including 45K utterances and doubled translations of Google Translate. We select the model on the dev set (1,071 utterances) and report results on the test set"
2021.acl-long.204,P07-2045,0,0.0130899,"Missing"
2021.acl-long.204,N18-1202,0,0.014006,"ply use an ASR encoder as the acoustic encoder, and use an MT encoder as the textual encoder. Note that SATE is in general a cascaded model, in response to the pioneering work in ST (Ney, 1999). It can be seen as cascading the ASR and MT systems in an end-to-end fashion. 4.2 The Adaptor Now we turn to the design of the adaptor. Note that the pre-trained MT encoder assumes that the input is a word embedding sequence. Simply stacking the MT encoder and the ASR encoder obviously 2622 hssoft = P(π|hs ) · W e (9) Also, an informative representation should contain information in the original input (Peters et al., 2018). The output acoustic representation of the ASR encoder generally involves paralinguistic information, such as emotion, accent, and emphasis. They are not expressed in the form of text explicitly but might be helpful for translation. For example, the generation of the declarative or exclamatory sentences depends on the emotions of the speakers. We introduce a single-layer neural network to learn to map the acoustic representation to the latent space of the textual encoder, which preserves the acoustic information: A(hs , P (π|hs )) = λ · hsmap + (1 − λ) · hssoft (11) where λ is the weight of h"
2021.acl-long.204,W18-6319,0,0.0196814,"048 feed-forward size. For the unrestricted setting, we use the superior architecture Conformer (Gulati et al., 2020) on the ASR and ST tasks and widen the model by increasing the hidden size to 512 and attention heads to 8. The ASR5 and MT models pre-train with the additional data and fine-tune the model parameters with the task-specific data. During inference, we average the model parameters on the best 5 checkpoints based on the performance of the development set. We use beam search with a beam size of 4 for all models. Different from previous work, we report the case-sensitive SacreBLEU6 (Post, 2018) for future standardization comparison across papers. 5.3 http://opus.nlpl.eu/OpenSubtitles-v2018.php Results Results on MuST-C En-De Table 2 summaries the experimental results on the MuST-C En-De task. Under the restricted setting, the cascaded ST model translates the output of the ASR model, which degrades the performance compared with the MT model that translates from the reference transcription. The performance of the E2E ST baseline with pre-training is only slightly lower than the cascaded counterpart. SATE outperforms the baseline 4 https://github.com/espnet/espnet We use the pre-traine"
2021.acl-long.204,P16-1162,0,0.0364284,"the WMT14 English-French and 18M sentence pairs from the Opensubtitle20183 English-German translation datasets. Preprocessing Followed the preprocessing recipes of ESPnet (Inaguma et al., 2020), we remove the utterances of more than 3,000 frames and augment speech data by speed perturbation with factors of 0.9, 1.0, and 1.1. The 80-channel log-mel filterbank coefficients with 3-dimensional pitch features are extracted for speech data. We use the lower-cased transcriptions without punctuations. The text is tokenized using the scripts of Moses (Koehn et al., 2007). We learn Byte-Pair Encoding (Sennrich et al., 2016) subword segmentation with 10,000 merge operations based on a shared source and target vocabulary for all datasets. All experiments are implemented based on the ESPnet toolkit4 . We use the Adam optimizer with β1 = 0.9, β2 = 0.997 and adopt the default learning schedule in ESPnet. We apply dropout with a rate of 0.1 and label smoothing ls = 0.1 for regularization. For reducing the computational cost, the input speech features are processed by two convolutional layers, which have a stride of 2 × 2 and downsample the sequence by a factor of 4 (Weiss et al., 2017). The encoder consists of 12 lay"
2021.acl-long.204,Q19-1020,0,0.0177703,"ced back to the success of deep neural models (Duong et al., 2016). But, unlike other well-defined tasks in deep learning, annotated speech-to-translation data is scarce, which prevents well-trained ST models. A simple solution to this issue is data augmentation (Pino et al., 2019, 2020). This method is model-free but generating large-scale synthetic data is time consuming. As an alternative, researchers used multi-task learning (MTL) to robustly train the ST model so that it could benefit from additional guide signals (Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021). Generally, MTL requires a careful design of the loss functions and more complicated architectures. In a similar way, more recent work pre-trains different components of the ST system, and consolidates them into one. For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020). More sophisticated methods include better training and fine-tuning (Wang et al., 2020a,b), the shrink mechanism (Liu et al., 2020), the adversarial regularizer (A"
2021.acl-long.204,2020.acl-main.344,0,0.0182873,"Weiss et al., 2017; Anastasopoulos and Chiang, 2018; Berard et al., 2018; Sperber et al., 2019; Dong et al., 2021). Generally, MTL requires a careful design of the loss functions and more complicated architectures. In a similar way, more recent work pre-trains different components of the ST system, and consolidates them into one. For example, one can initialize the encoder with an ASR model, and initialize the decoder with the target-language side of an MT model (Berard et al., 2018; Bansal et al., 2019; Stoian et al., 2020). More sophisticated methods include better training and fine-tuning (Wang et al., 2020a,b), the shrink mechanism (Liu et al., 2020), the adversarial regularizer (Alinejad and Sarkar, 2020), and etc. Although pre-trained models have quickly become dominant in many NLP tasks, they are still found to underperform the cascaded model in ST. This motivates us to explore the reasons why this happens and methods to solve the problems accordingly. 3 Why is ST Encoding Difficult? Following previous work in end-to-end models (Berard et al., 2016; Weiss et al., 2017), we envision an encoding-decoding process in which an input sequence is encoded into a representation vector, and the vector"
2021.acl-long.204,D18-1475,0,0.118472,"e Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing, pages 2619–2630 August 1–6, 2021. ©2021 Association for Computational Linguistics • Modeling deficiency: the MT encoder tries to capture long-distance dependency structures of language, but the ASR encoder focuses more on local dependencies in the input sequence. Since the ST encoder is initialized by the pre-trained ASR encoder (Berard et al., 2018), it fails to model large contexts in the utterance. But a large scope of representation learning is necessary for translation (Yang et al., 2018). • Representation inconsistency: on the decoder side of ST, the MT decoder is in general used to initialize the model. The assumption here is that the upstream component is an MT-like encoder, whereas the ST encoder actually behaves more like an ASR encoder. We address these problems by marrying the world of ASR encoding with the world of MT encoding. We propose a Stacked Acoustic-andTextual Encoding (SATE) method to cascade the ASR encoder and the MT encoder. It first reads and processes the sequence of acoustic features as a usual ASR encoder. Then an adaptor module passes the acoustic enco"
2021.findings-emnlp.357,2020.emnlp-main.211,0,0.0322675,"d Tu, 2020). In this paper we show that Transformer can be optimized for efficiency by a bag of techniques. These techniques are easy to implement and some of them have been tested in related studies. Here we focus on using them in combination for Transformer speedup which has not been well investigated. In particular, our work is based on the following facts: ∗ † Authors contributed equally. Corresponding author. Attention FFN Attention FFN Output Time (s) Baseline MDN 5.81 16.93 5.53 18.79 223.55 8.74 38.26 0.00 48.51 8.61 • The attention model does not need to be multiheaded in some cases (Behnke and Heafield, 2020). • The feedforward network sub-layer is removable (Hsu et al., 2020). • Knowledge Distillation (Hinton et al., 2015) is crucial to squeeze out the last potential. Removing some regularization measures like label smoothing (Szegedy et al., 2016) also helps when training such models. All these methods are compatible with popular Transformer codebases. In this work, we implement them on the decoder side because it occupies the inference time in many sequence generation tasks (Hsu et al., 2020; Kim et al., 2019). The end result is a simplified and fast Transformer decoder (see Table 1) - Mini-Dec"
2021.findings-emnlp.357,2020.findings-emnlp.433,0,0.0815094,"Missing"
2021.findings-emnlp.357,W17-4123,0,0.0274478,"r model on WMT14 En-De (FFN: the feedforward network, Output: the output projection). • The default Byte-Pair Encoding (BPE) setting (Sennrich et al., 2016) has a great impact on efficiency but is generally not optimal. • A shallow decoder (with a deeper encoder) is preferred for a fast system (Kasai et al., 2020). Introduction Standard implementation of Transformer (Vaswani et al., 2017) is not efficient for inference. Researchers have explored more efficient architectures (Zhang et al., 2018; Xiao et al., 2019; Li et al., 2021) or break the auto-regressive constraint in sequence generation (Gu et al., 2017). But most of these require significant updates of the model or hardware-dependent designs. It is still natural to ask whether the Transformer system can be optimized in a simple way (Hsu et al., 2020; Kasai et al., 2020; Kim et al., 2019; Wang and Tu, 2020). In this paper we show that Transformer can be optimized for efficiency by a bag of techniques. These techniques are easy to implement and some of them have been tested in related studies. Here we focus on using them in combination for Transformer speedup which has not been well investigated. In particular, our work is based on the followi"
2021.findings-emnlp.357,2020.sustainlp-1.7,0,0.133224,"erally not optimal. • A shallow decoder (with a deeper encoder) is preferred for a fast system (Kasai et al., 2020). Introduction Standard implementation of Transformer (Vaswani et al., 2017) is not efficient for inference. Researchers have explored more efficient architectures (Zhang et al., 2018; Xiao et al., 2019; Li et al., 2021) or break the auto-regressive constraint in sequence generation (Gu et al., 2017). But most of these require significant updates of the model or hardware-dependent designs. It is still natural to ask whether the Transformer system can be optimized in a simple way (Hsu et al., 2020; Kasai et al., 2020; Kim et al., 2019; Wang and Tu, 2020). In this paper we show that Transformer can be optimized for efficiency by a bag of techniques. These techniques are easy to implement and some of them have been tested in related studies. Here we focus on using them in combination for Transformer speedup which has not been well investigated. In particular, our work is based on the following facts: ∗ † Authors contributed equally. Corresponding author. Attention FFN Attention FFN Output Time (s) Baseline MDN 5.81 16.93 5.53 18.79 223.55 8.74 38.26 0.00 48.51 8.61 • The attention model"
2021.findings-emnlp.357,D19-5632,0,0.0197573,"ns in the inference speed are expected. 3 3.1 Experiments Setup We evaluate our methods on the WMT14 En-De, WMT14 En-Fr and NIST12 Zh-En machine translation tasks. We tokenize every sentence using a script from Moses and segment every word into subword units using BPE (Sennrich et al., 2016). The number of the BPE merge operations is set to 32K in the baseline and 10K for the target language Deep Configuration. Because our model is deep, in our model. In addition, we remove sentences we follow the deep model training setup provided with more than 250 subword units (Xiao et al., in Wang et al. (2019). 2012). We choose Transformer-base (Vaswani et al., Weight Distillation. We also adopt a simplified version of weight distillation (WD) for training 2017) as our baseline. The hyper-parameters of (Lin et al., 2020). This method initializes the stu- the Mini-Decoder Network (MDN) are the same as dent model with the corresponding weights from the baseline except for those mentioned in Section the teacher model, e.g., the first layer in the teacher 2.3. To produce consistent results for distillation, encoder is reused in the first layer in the student we choose the baseline with 10K BPE merges a"
2021.findings-emnlp.357,P16-1162,0,0.677523,"ransformer can be improved by combining some simple and hardware-agnostic methods, including tuning hyper-parameters, better design choices and training strategies. On the WMT news translation tasks, we improve the inference efficiency of a strong Transformer system by 3.80× on CPU and 2.52× on GPU. The code is publicly available at https://github.com/Lollipop321/minidecoder-network. 1 Encoder Decoder Table 1: Profiling results of the Transformer baseline and our model on WMT14 En-De (FFN: the feedforward network, Output: the output projection). • The default Byte-Pair Encoding (BPE) setting (Sennrich et al., 2016) has a great impact on efficiency but is generally not optimal. • A shallow decoder (with a deeper encoder) is preferred for a fast system (Kasai et al., 2020). Introduction Standard implementation of Transformer (Vaswani et al., 2017) is not efficient for inference. Researchers have explored more efficient architectures (Zhang et al., 2018; Xiao et al., 2019; Li et al., 2021) or break the auto-regressive constraint in sequence generation (Gu et al., 2017). But most of these require significant updates of the model or hardware-dependent designs. It is still natural to ask whether the Transform"
2021.findings-emnlp.357,P19-1580,0,0.0170754,"ach method in “Model Structure Updates” influences the model performance and inference speed after applying techniques from “Byte-Pair Encoding”. Shallow Decoder. Recent work has shown that the deep encoder and shallow decoder architecture is promising in system speedup (Kasai et al., 2020; Li et al., 2021). In this work we follow the same idea by restricting the decoder to a 1-layer network and stacking more encoder layers until the total number of parameters matches the baseline. Pruning Heads. Researchers have found that most heads could be safely pruned and leaving the performance intact (Voita et al., 2019; Michel et al., 2019). So we retain only one head in decoder attentions. Dropping FFN. Hsu et al. (2020) suggests that FFN is the least important component in the decoder. So we drop all FFNs in the decoder. After dropping FFN, there are only attentions and no other non-linearity except layer normalization in the model. Inspired by the observations in Table 1, the Transformer decoder can be improved for each of its components. In this section, we describe how to Factorizing Output. 4228 The weight matrix W System BLEU Baseline1 + Shallow Decoder + Pruning Heads + Dropping FFN + Factorizing Ou"
2021.findings-emnlp.357,P19-1176,1,0.796681,"ome deviations in the inference speed are expected. 3 3.1 Experiments Setup We evaluate our methods on the WMT14 En-De, WMT14 En-Fr and NIST12 Zh-En machine translation tasks. We tokenize every sentence using a script from Moses and segment every word into subword units using BPE (Sennrich et al., 2016). The number of the BPE merge operations is set to 32K in the baseline and 10K for the target language Deep Configuration. Because our model is deep, in our model. In addition, we remove sentences we follow the deep model training setup provided with more than 250 subword units (Xiao et al., in Wang et al. (2019). 2012). We choose Transformer-base (Vaswani et al., Weight Distillation. We also adopt a simplified version of weight distillation (WD) for training 2017) as our baseline. The hyper-parameters of (Lin et al., 2020). This method initializes the stu- the Mini-Decoder Network (MDN) are the same as dent model with the corresponding weights from the baseline except for those mentioned in Section the teacher model, e.g., the first layer in the teacher 2.3. To produce consistent results for distillation, encoder is reused in the first layer in the student we choose the baseline with 10K BPE merges a"
2021.findings-emnlp.357,2020.coling-main.529,0,0.0329191,"ncoder) is preferred for a fast system (Kasai et al., 2020). Introduction Standard implementation of Transformer (Vaswani et al., 2017) is not efficient for inference. Researchers have explored more efficient architectures (Zhang et al., 2018; Xiao et al., 2019; Li et al., 2021) or break the auto-regressive constraint in sequence generation (Gu et al., 2017). But most of these require significant updates of the model or hardware-dependent designs. It is still natural to ask whether the Transformer system can be optimized in a simple way (Hsu et al., 2020; Kasai et al., 2020; Kim et al., 2019; Wang and Tu, 2020). In this paper we show that Transformer can be optimized for efficiency by a bag of techniques. These techniques are easy to implement and some of them have been tested in related studies. Here we focus on using them in combination for Transformer speedup which has not been well investigated. In particular, our work is based on the following facts: ∗ † Authors contributed equally. Corresponding author. Attention FFN Attention FFN Output Time (s) Baseline MDN 5.81 16.93 5.53 18.79 223.55 8.74 38.26 0.00 48.51 8.61 • The attention model does not need to be multiheaded in some cases (Behnke and"
2021.findings-emnlp.357,P12-3004,1,0.665214,"Missing"
2021.findings-emnlp.357,P18-1166,0,0.104635,"hub.com/Lollipop321/minidecoder-network. 1 Encoder Decoder Table 1: Profiling results of the Transformer baseline and our model on WMT14 En-De (FFN: the feedforward network, Output: the output projection). • The default Byte-Pair Encoding (BPE) setting (Sennrich et al., 2016) has a great impact on efficiency but is generally not optimal. • A shallow decoder (with a deeper encoder) is preferred for a fast system (Kasai et al., 2020). Introduction Standard implementation of Transformer (Vaswani et al., 2017) is not efficient for inference. Researchers have explored more efficient architectures (Zhang et al., 2018; Xiao et al., 2019; Li et al., 2021) or break the auto-regressive constraint in sequence generation (Gu et al., 2017). But most of these require significant updates of the model or hardware-dependent designs. It is still natural to ask whether the Transformer system can be optimized in a simple way (Hsu et al., 2020; Kasai et al., 2020; Kim et al., 2019; Wang and Tu, 2020). In this paper we show that Transformer can be optimized for efficiency by a bag of techniques. These techniques are easy to implement and some of them have been tested in related studies. Here we focus on using them in com"
2021.iwslt-1.9,N19-1202,0,0.0653287,"Missing"
2021.iwslt-1.9,P19-1285,0,0.0140972,"f CTC objective α is set to 0.3 for all ASR and ST models. The model architecture is showed in Figure 14 . 3.2 Conformer Conformer (Gulati et al., 2020) models both local and global dependencies by combining the Convolutional Neural Network and Transformers. It has shown superiority and achieved promising results in ASR tasks. We replace the Transformer blocks in the encoder by the conformer blocks, which include two macaron-like feed-forward networks, multihead self attention modules, and convolution modules. Note that we use the RPE proposed in Shaw et al. (2018) rather than Transformer-XL (Dai et al., 2019). Model Architecture In this section, we describe the baseline model and the architecture improvements. Then, the experimental results are shown to demonstrate the effectiveness. 3.1 Position Embedding Acoustic Feature Table 1: Data statistics of the ASR, MT, and ST corpora. 3 Masked Multi-Head Attention Baseline Model Our system is based on deep Transformer (Vaswani et al., 2017) implemented on the fairseq toolkit (Ott et al., 2019). Furthermore, dynamic linear combination of layers (DLCL) (Wang et al., 2019) method is employed to train the deep model effectively (Li et al., 2020a,b). To redu"
2021.iwslt-1.9,N16-1109,0,0.018984,"English transcription. We use the allowed English-German translation data from WMT 2020 (Barrault et al., 2020) and OpenSubtitles2018 (Lison and Tiedemann, 2016). We filter the training bilingual data followed Li et al. (2019), which includes length ratio, language detection, and so on. Speech translation (ST) aims to learn models that can predict, given some speech in the source language, the translation into the target language. Endto-end (E2E) approaches have become popular recently for its ability to free designers from cascading different systems and shorten the pipeline of translation (Duong et al., 2016; Berard et al., 2016; Weiss et al., 2017). This paper describes the submission of the NiuTrans E2E ST system for the IWSLT 2021 (Anastasopoulos et al., 2021) offline task, which translates from the English audio to the German text translation directly without intermediate transcription. Our baseline model is based on the DLCL Transformer (Vaswani et al., 2017; Wang et al., 2019) model with Connectionist Temporal Classification (CTC) (Graves et al., 2006) loss on the encoders (Bahar et al., 2019). We enhance it with the superior model architecture Conformer (Gulati et al., 1 We only described"
2021.iwslt-1.9,D16-1139,0,0.0604143,"Missing"
2021.iwslt-1.9,2020.emnlp-main.72,1,0.776088,"former-XL (Dai et al., 2019). Model Architecture In this section, we describe the baseline model and the architecture improvements. Then, the experimental results are shown to demonstrate the effectiveness. 3.1 Position Embedding Acoustic Feature Table 1: Data statistics of the ASR, MT, and ST corpora. 3 Masked Multi-Head Attention Baseline Model Our system is based on deep Transformer (Vaswani et al., 2017) implemented on the fairseq toolkit (Ott et al., 2019). Furthermore, dynamic linear combination of layers (DLCL) (Wang et al., 2019) method is employed to train the deep model effectively (Li et al., 2020a,b). To reduce the computational cost, the input speech features are processed by two convolutional layers, which have a stride of 2. This downsamples 3.3 Relative Position Encoding Due to the non-sequential modeling of the original self attention modules, the vanilla Transformer employs the position embedding by a deterministic sinusoidal function to indicate the absolute position of each input element (Vaswani et al., 2017). However, this scheme is far from ideal for acoustic modeling (Pham et al., 2020). 2 We use the latest MusST-C v2 dataset released by IWSLT 2021. 3 http://i13pc106.ira.u"
2021.iwslt-1.9,W18-6319,0,0.0138889,"yers. The decoder consists of 6 Transformer layers. Each layer comprises 512 hidden units, 8 attention heads, and 2048 feed-forward size. Pre-norm is applied for training a deep model. The weight of CTC objective α for multitask learning is set to 0.3 for all models. All the models are trained for 50 epochs on one machine with 8 NVIDIA 2080Ti GPUs. During inference, we average the model parameters on the final 10 checkpoints. We use beam search with a beam size of 5 for all models. The coefficient of length normalization is tuned on the development set. We report the case-sensitive SacreBLEU (Post, 2018) on the MuST-C tst-COMMON set, IWSLT tst2019 and tst2020 test set. The organizers provide the segmentation of the test sets and allow the participants to use the own segmentation. We simply use the segmentation provided by the WerRTCVAD8 toolkit. Ensemble Decoding Ensemble decoding is an effective method to improve performance by integrating the predictions from multiple models. It has been proved in the WMT competitions (Wang et al., 2018; Li et al., 2019). In our systems, we train multiple ST models with different training data for diverse ensemble decoding. The models are chosen based on th"
2021.iwslt-1.9,N18-2074,0,0.128356,"et al., 2019; Bahar et al., 2019). The weight of CTC objective α is set to 0.3 for all ASR and ST models. The model architecture is showed in Figure 14 . 3.2 Conformer Conformer (Gulati et al., 2020) models both local and global dependencies by combining the Convolutional Neural Network and Transformers. It has shown superiority and achieved promising results in ASR tasks. We replace the Transformer blocks in the encoder by the conformer blocks, which include two macaron-like feed-forward networks, multihead self attention modules, and convolution modules. Note that we use the RPE proposed in Shaw et al. (2018) rather than Transformer-XL (Dai et al., 2019). Model Architecture In this section, we describe the baseline model and the architecture improvements. Then, the experimental results are shown to demonstrate the effectiveness. 3.1 Position Embedding Acoustic Feature Table 1: Data statistics of the ASR, MT, and ST corpora. 3 Masked Multi-Head Attention Baseline Model Our system is based on deep Transformer (Vaswani et al., 2017) implemented on the fairseq toolkit (Ott et al., 2019). Furthermore, dynamic linear combination of layers (DLCL) (Wang et al., 2019) method is employed to train the deep m"
2021.iwslt-1.9,L16-1147,0,0.033184,"tion. The training data can be divided into three categories: ASR, MT, and ST corpora1 . ASR corpora. ASR corpora are used to generate synthetic speech translation data. We only use the Common Voice (Ardila et al., 2020) and LibriSpeech (Panayotov et al., 2015) corpora. Furthermore, we filter the noisy training data in the Common Voice corpus by force decoding and keep 1 million utterances. MT corpora. Machine translation (MT) corpora are used to translate the English transcription. We use the allowed English-German translation data from WMT 2020 (Barrault et al., 2020) and OpenSubtitles2018 (Lison and Tiedemann, 2016). We filter the training bilingual data followed Li et al. (2019), which includes length ratio, language detection, and so on. Speech translation (ST) aims to learn models that can predict, given some speech in the source language, the translation into the target language. Endto-end (E2E) approaches have become popular recently for its ability to free designers from cascading different systems and shorten the pipeline of translation (Duong et al., 2016; Berard et al., 2016; Weiss et al., 2017). This paper describes the submission of the NiuTrans E2E ST system for the IWSLT 2021 (Anastasopoulos"
2021.iwslt-1.9,N19-4009,0,0.020884,"d-forward networks, multihead self attention modules, and convolution modules. Note that we use the RPE proposed in Shaw et al. (2018) rather than Transformer-XL (Dai et al., 2019). Model Architecture In this section, we describe the baseline model and the architecture improvements. Then, the experimental results are shown to demonstrate the effectiveness. 3.1 Position Embedding Acoustic Feature Table 1: Data statistics of the ASR, MT, and ST corpora. 3 Masked Multi-Head Attention Baseline Model Our system is based on deep Transformer (Vaswani et al., 2017) implemented on the fairseq toolkit (Ott et al., 2019). Furthermore, dynamic linear combination of layers (DLCL) (Wang et al., 2019) method is employed to train the deep model effectively (Li et al., 2020a,b). To reduce the computational cost, the input speech features are processed by two convolutional layers, which have a stride of 2. This downsamples 3.3 Relative Position Encoding Due to the non-sequential modeling of the original self attention modules, the vanilla Transformer employs the position embedding by a deterministic sinusoidal function to indicate the absolute position of each input element (Vaswani et al., 2017). However, this sche"
2021.iwslt-1.9,1983.tc-1.13,0,0.333474,"Missing"
2021.iwslt-1.9,W18-6430,1,0.910281,"versity, Shenyang, China 2 NiuTrans Research, Shenyang, China {xuchenneu,liuxiaoqianneu,liuxiaowenneu}@outlook.com, {tigerneu,huangcananneu}@outlook.com, {xiaotong,zhujingbo}@mail.neu.edu.cn Abstract 2020), relative position encoding (RPE) (Shaw et al., 2018), and stacked acoustic and textual encoding (SATE) (Xu et al., 2021). To augment the training data, the English transcriptions of the automatic speech recognition (ASR) and speech translation corpora are translated to the German translation. Finally, we employ the ensemble decoding method to integrate the predictions from multiple models (Wang et al., 2018) trained with the different datasets. This paper is structured as follows. The training data is summarized in Section 2, then we describe the model architecture in Section 3 and data augmentation in Section 4. We present the ensemble decoding method in Section 5. The experimental settings and final results are shown in Section 6. This paper describes the submission of the NiuTrans end-to-end speech translation system for the IWSLT 2021 offline task, which translates from the English audio to German text directly without intermediate transcription. We use the Transformer-based model architectur"
2021.iwslt-1.9,P19-1176,1,0.898759,"Missing"
2021.iwslt-1.9,2021.acl-long.204,1,0.760178,"strengthen the encoding and achieve an improvement of 0.45 and 0.26 BLEU points. SATE achieves a remarkable improvement by encoding the acoustic representation and textual representation respectively. We will explore better architecture designs in the future. Stacked Acoustic and Textual Encoding The previous work (Bahar et al., 2019) employs the CTC loss on the top layer of the encoder, which forces the encoders to learn soft alignments between speech and transcription. However, the CTC loss demonstrates strong preference for locally attentive models, which is inconsistent with the ST model (Xu et al., 2021). In our systems, we use the stacked acoustic-andtextual encoding (SATE) (Xu et al., 2021) method to encode the speech features. It calculates the CTC loss based on the hidden states of the intermediate layer rather than the top layer. The layers below CTC also extract the acoustic representation like an ASR encoder, while the upper layers with no CTC encode the global representation for translation. An adaptor layer is introduced to bridge the acoustic and textual encoding. 3.5 Size Total The latest work (Pham et al., 2020; Gulati et al., 2020) points out that the relative position encoding e"
C08-1142,J96-1002,0,0.0592474,"Missing"
C08-1142,P94-1020,0,0.0386717,"Missing"
C08-1142,N06-1016,0,0.0716357,"Missing"
C08-1142,N06-2015,1,0.78772,"Missing"
C08-1142,W02-1006,0,0.0160402,"Missing"
C08-1142,P96-1006,0,0.072439,"Missing"
C08-1142,D07-1082,1,\N,Missing
C08-1142,P04-1075,0,\N,Missing
C08-1142,P00-1016,0,\N,Missing
C08-1142,P02-1016,0,\N,Missing
C08-1142,I08-1048,1,\N,Missing
C08-1143,J96-1002,0,0.044465,"Missing"
C08-1143,P94-1020,0,0.0184848,"Missing"
C08-1143,P07-1007,0,0.0408627,"Missing"
C08-1143,W02-1006,0,0.0118415,"Missing"
C08-1143,N06-1016,0,\N,Missing
C08-1143,D07-1082,1,\N,Missing
C08-1143,P04-1075,0,\N,Missing
C08-1143,P96-1006,0,\N,Missing
C08-1143,P00-1016,0,\N,Missing
C08-1143,P02-1016,0,\N,Missing
C08-1143,I08-1048,1,\N,Missing
C10-1151,P01-1005,0,0.014086,"-decoding) approach to improve parsing accuracy by leveraging bracket structure consensus between multiple parsing decoders trained on individual treebanks. Experimental results show the effectiveness of the proposed approach, which outperforms stateof-the-art baselines, especially on long sentences. 1 Introduction Recent years have seen extensive applications of machine learning methods to natural language processing problems. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is expensive in time and labor. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) in Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same"
C10-1151,D09-1161,0,0.0635626,"tures in the parse trees are different. Specifically put, although the internal structures of the parse trees are different, both CTB and TCT agree to take “中国 传统 文化” as a noun phrase. Motivated by this observation, we would like to guide parsers that are trained on CTB and TCT respectively to verify their output interactively by using consensus information implicitly contained in these treebanks. Better performance is expected when such information is considered. A feasible framework to make use of consensus information is n-best combination (Henderson and Brill, 1999; Sagae and Lavie, 2006; Zhang et al., 2009; Fossum and Knight, 2009). In contrast 1345 to previous work on n-best combination where multiple parsers, say, Collins parser (Collins, 1999) and Berkeley parser (Petrov et al., 2006) are trained on the same training data, n-best combination for heterogeneous parsing is instead allowed to use either a single parser or multiple parsers which are trained on heterogeneous treebanks. Consensus information can be incorporated during the combination of the output (n-best list of full parse trees following distinct annotation standards) of individual parsers. However, despite the success of n-best"
C10-1151,A00-2018,0,0.0940505,"years have seen extensive applications of machine learning methods to natural language processing problems. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is expensive in time and labor. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) in Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same tasks, it is significant if such corpora can be used jointly. At first sight, a direct combination of multiple corpora is a way to this end. However, corpora created for the same NLP tasks are generally built by different organizations. Thus such corpora often follow different annotation standards and/or even diffe"
C10-1151,N09-2064,0,0.0808228,"rees are different. Specifically put, although the internal structures of the parse trees are different, both CTB and TCT agree to take “中国 传统 文化” as a noun phrase. Motivated by this observation, we would like to guide parsers that are trained on CTB and TCT respectively to verify their output interactively by using consensus information implicitly contained in these treebanks. Better performance is expected when such information is considered. A feasible framework to make use of consensus information is n-best combination (Henderson and Brill, 1999; Sagae and Lavie, 2006; Zhang et al., 2009; Fossum and Knight, 2009). In contrast 1345 to previous work on n-best combination where multiple parsers, say, Collins parser (Collins, 1999) and Berkeley parser (Petrov et al., 2006) are trained on the same training data, n-best combination for heterogeneous parsing is instead allowed to use either a single parser or multiple parsers which are trained on heterogeneous treebanks. Consensus information can be incorporated during the combination of the output (n-best list of full parse trees following distinct annotation standards) of individual parsers. However, despite the success of n-best combination methods, they"
C10-1151,W99-0623,0,0.0413618,"bracket structures. That is, not all bracket structures in the parse trees are different. Specifically put, although the internal structures of the parse trees are different, both CTB and TCT agree to take “中国 传统 文化” as a noun phrase. Motivated by this observation, we would like to guide parsers that are trained on CTB and TCT respectively to verify their output interactively by using consensus information implicitly contained in these treebanks. Better performance is expected when such information is considered. A feasible framework to make use of consensus information is n-best combination (Henderson and Brill, 1999; Sagae and Lavie, 2006; Zhang et al., 2009; Fossum and Knight, 2009). In contrast 1345 to previous work on n-best combination where multiple parsers, say, Collins parser (Collins, 1999) and Berkeley parser (Petrov et al., 2006) are trained on the same training data, n-best combination for heterogeneous parsing is instead allowed to use either a single parser or multiple parsers which are trained on heterogeneous treebanks. Consensus information can be incorporated during the combination of the output (n-best list of full parse trees following distinct annotation standards) of individual parse"
C10-1151,D07-1117,0,0.0143621,"S))) is an indicator function where I(c, CS(H which returns one if c ∈ CS(T ) is compatible ˆ k (S)), zero othwith all the elements in CS(H erwise. Two spans, [a, b] and [i, j] are said to be compatible if they satisfy one of the following conditions: 1) i &gt; b; 2) a &gt; j; 3) a ≤ i ≤ b and j ≤ b; 4) i ≤ a ≤ j and b ≤ j. Fig 4 uses two example to illustrate the concept of compatibility. 3 Experiments 3.1 Data and Performance Metric The most recent version of the CTB corpus, CTB 6.0 and the CIPS ParsEval data are used as heterogeneous treebanks in the experiments. Following the split utilized in (Huang et al., 2007), we divided the dataset into blocks of 10 files. For each block, the first file was added to the CTB development data, the second file was added to the CTB testing data, and the remaining 8 files were added to the CTB training data. For the sake of parsing efficiency, we randomly sampled 1,000 sentences of no more than 40 words from the CTB test set. CTB-Partitions #Sentences #Words Ave-Length TCT-Partitions #Sentences #Words Ave-Length Train 22,724 627,833 30.1 Train 32,771 354,767 10.6 Dev 2,855 78,653 30.0 Dev N/A N/A N/A Test 1,000 25,100 20.3 Test 1,000 10,400 10.4 Table 1: Basic statist"
C10-1151,P09-1066,0,0.394278,"rained on heterogeneous treebanks. Consensus information can be incorporated during the combination of the output (n-best list of full parse trees following distinct annotation standards) of individual parsers. However, despite the success of n-best combination methods, they suffer from the limited scope of n-best list. Taking this into account, we prefer to apply the co-decoding approach such that consensus information is expected to affect the entire procedure of searching hypothesis space. 2.2 System Overview The idea of co-decoding is recently extensively studied in the literature of SMT (Li et al., 2009; Liu et al., 2009). As the name shows, co-decoding requires multiple decoders be combined and proceed collaboratively. As with n-best combination, there are at least two ways to build multiple decoders: we can either use multiple parsers trained on the same training data (use of diversity of models), or use a single parser on different training data (use of diversity of datasets) 1 . Both ways can build multiple decoders which are to be integrated into co-decoding. For the latter case, one method to get diverse training data is to use different portions of the same training set. In this study"
C10-1151,P09-1065,0,0.141873,"eneous treebanks. Consensus information can be incorporated during the combination of the output (n-best list of full parse trees following distinct annotation standards) of individual parsers. However, despite the success of n-best combination methods, they suffer from the limited scope of n-best list. Taking this into account, we prefer to apply the co-decoding approach such that consensus information is expected to affect the entire procedure of searching hypothesis space. 2.2 System Overview The idea of co-decoding is recently extensively studied in the literature of SMT (Li et al., 2009; Liu et al., 2009). As the name shows, co-decoding requires multiple decoders be combined and proceed collaboratively. As with n-best combination, there are at least two ways to build multiple decoders: we can either use multiple parsers trained on the same training data (use of diversity of models), or use a single parser on different training data (use of diversity of datasets) 1 . Both ways can build multiple decoders which are to be integrated into co-decoding. For the latter case, one method to get diverse training data is to use different portions of the same training set. In this study we extend the case"
C10-1151,P09-1006,0,0.40052,"his paper is dedicated to solving the problem of how to use jointly multiple disparate treebanks for constituent syntactic parsing. Hereafter, treebanks of different annotations are 1344 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1344–1352, Beijing, August 2010 called heterogeneous treebanks, and correspondingly, the problem of syntactic parsing with heterogeneous treebanks is referred to as heterogeneous parsing. Previous work on heterogeneous parsing is often based on treebank transformation (or treebank conversion) (Wang et al., 1994; Niu et al., 2009). The basic idea is to transform annotations of one treebank (source treebank) to fit the standard of another treebank (target treebank). Due to divergences of treebank annotations, such transformation is generally achieved in an indirect way by selecting transformation results from the output of a parser trained on the target treebank. A common property of all the work mentioned above is that transformation accuracy is heavily dependent on the performance of parsers trained on the target treebank. Sometimes transformation accuracy is not so satisfactory that techniques like instance pruning a"
C10-1151,P06-1055,0,0.317518,"extensive applications of machine learning methods to natural language processing problems. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is expensive in time and labor. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) in Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same tasks, it is significant if such corpora can be used jointly. At first sight, a direct combination of multiple corpora is a way to this end. However, corpora created for the same NLP tasks are generally built by different organizations. Thus such corpora often follow different annotation standards and/or even different linguistic theori"
C10-1151,N06-2033,0,0.0629553,"Missing"
C10-1151,C02-1145,0,0.0329048,"lly, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is expensive in time and labor. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) in Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same tasks, it is significant if such corpora can be used jointly. At first sight, a direct combination of multiple corpora is a way to this end. However, corpora created for the same NLP tasks are generally built by different organizations. Thus such corpora often follow different annotation standards and/or even different linguistic theories. We take CTB and TCT as a case study. Although both CTB and TCT are Chomskian-style treebank"
C10-1151,P94-1034,0,0.886825,"ed independently. This paper is dedicated to solving the problem of how to use jointly multiple disparate treebanks for constituent syntactic parsing. Hereafter, treebanks of different annotations are 1344 Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010), pages 1344–1352, Beijing, August 2010 called heterogeneous treebanks, and correspondingly, the problem of syntactic parsing with heterogeneous treebanks is referred to as heterogeneous parsing. Previous work on heterogeneous parsing is often based on treebank transformation (or treebank conversion) (Wang et al., 1994; Niu et al., 2009). The basic idea is to transform annotations of one treebank (source treebank) to fit the standard of another treebank (target treebank). Due to divergences of treebank annotations, such transformation is generally achieved in an indirect way by selecting transformation results from the output of a parser trained on the target treebank. A common property of all the work mentioned above is that transformation accuracy is heavily dependent on the performance of parsers trained on the target treebank. Sometimes transformation accuracy is not so satisfactory that techniques like"
C10-1151,J03-4003,0,\N,Missing
C10-2154,D07-1079,0,0.0844003,"extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects the translation accuracy of syntax-based systems (DeNeefe et al., 2007; Shen et al., 2008). However, these systems suffer from a problem that translation rules are extracted using only 1-best parse tree generated by a single parser, which generally results in relatively low rule coverage due to the limited scope in rule extraction (Mi and Huang, 2008). To alleviate this problem, a straightforward solution is to enlarge the scope of rule extraction, and obtain translation rules by using a group of diversified parse trees instead of a single parse tree. For example, Mi and Huang (2008) used k-best parses and forest to extract translation rules for improving the ru"
C10-2154,P05-1067,0,0.0120742,"nglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been prove"
C10-2154,P03-2041,0,0.0924117,"od on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule"
C10-2154,N04-1035,0,0.20786,"terminals) and variables (non-terminals) at leaves. Figure 1 shows the translation rules extracted from a word-aligned sentence pair with a targetside parse tree. Figure 1: Translation rules extracted from a string-tree pair. 1346 Figure 2: Rule extraction using two different parsers (Berkeley Parser and Collins Parser). The shaded rectangles denote the translation rules that can be extracted from the parse tree generated by one parser but cannot be extracted from the parse tree generated by the other parser. To obtain basic translation rules, the (minimal) GHKM extraction method proposed in (Galley et al, 2004) is utilized. The basic idea of GHKM extraction is to compute the set of the minimally-sized translation rules that can explain the mappings between source-language string and target-language tree while respecting the alignment and reordering between the two languages. For example, from the string-tree pair shown at the top of Figure 1, we extract the minimal GHKM translation rules r1-6. In addition to GHKM extraction, the SPMT models (Marcu et al., 2006) are employed to obtain phrasal rules that are not covered by GHKM extraction. For example, rule r8 in Figure 1 is a SPMT rule that is not ob"
C10-2154,P07-1019,0,0.0312328,"5.3 Parser Indicator Features For each rule, we define N indicator features (i.e. τ (r , i ) ) to indicate a rule is extracted by using which parsers, and add them into the translation model. By training the feature weights with Minimum Error Rate Training (MERT), the system can learn preferences for different parsers automatically. 6 Experiments The experiments are conducted on ChineseEnglish translation in a state-of-the-art string-totree SMT system. 6.1 and the composed rules are generated by composing two or three minimal GHKM and SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are"
C10-2154,2006.amta-papers.8,0,0.642775,"ts show that our method improves the baseline system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora, even achieves a +1 BLEU improvement when working with the kbest extraction method. More interestingly, we observe that the MT performance is not very sensitive to the parsing performance of the parsers used in rule extraction. Actually, the MT system does not show different preferences for different parsers. 2 Related Work In machine translation, some efforts have been made to improve rule coverage and advance the performance of syntax-based systems. For example, Galley et al. (2006) proposed the idea of rule composing which composes two or more rules with shared states to form a larger, composed rule. Their experimental results showed that the rule composing method could significantly improve the translation accuracy of their syntax-based system. Following Galley et al. (2006)’s work, Marcu et al. (2006) proposed SPMT models to improve the coverage of phrasal rules, and demonstrated that the system performance could be further improved by using their proposed models. Wang et al. (2007) described a binarization method that binarized parse trees to improve the rule coverag"
C10-2154,W04-3250,0,0.0602451,"SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are Stanford Parser4, Berkeley Parser 5 , Collins Parser (Dan Bikel’s reimplementation of Collins Model 2) 6 and Charniak Parser7. The former two are state-of-the-art nonlexicalized parsers, while the latter two are stateof-the-art lexicalized parsers. All the parsers are trained on sections 02-21 of the Wall Street Journal (WSJ) Treebank, and tuned on section 22. Table 2 summarizes the performance of the parsers. Experimental Setup Our bilingual data consists of 370K sentence pairs (9M Chinese words + 10M English words) which h"
C10-2154,P06-1077,0,0.134429,"imple and effective method to improve rule coverage by using multiple parsers in translation rule extraction, and then empirically investigate the effectiveness of our method on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performa"
C10-2154,D08-1022,0,0.719676,"shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects the translation accuracy of syntax-based systems (DeNeefe et al., 2007; Shen et al., 2008). However, these systems suffer from a problem that translation rules are extracted using only 1-best parse tree generated by a single parser, which generally results in relatively low rule coverage due to the limited scope in rule extraction (Mi and Huang, 2008). To alleviate this problem, a straightforward solution is to enlarge the scope of rule extraction, and obtain translation rules by using a group of diversified parse trees instead of a single parse tree. For example, Mi and Huang (2008) used k-best parses and forest to extract translation rules for improving the rule coverage in their forest-based SMT system, and achieved promising results. However, most previous work used the parse trees generated by only one parser, which still suffered somewhat from the relatively low diversity in the outputs of a single parser. Addressing this issue, we i"
C10-2154,P05-1034,0,0.0951562,"Missing"
C10-2154,P08-1066,0,0.0390355,"empirically investigate the effectiveness of our method on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributi"
C10-2154,2008.amta-papers.18,0,0.0357989,"Missing"
C10-2154,D07-1078,0,0.0361915,"Missing"
C10-2154,D09-1038,1,0.814184,"utomatically. 6 Experiments The experiments are conducted on ChineseEnglish translation in a state-of-the-art string-totree SMT system. 6.1 and the composed rules are generated by composing two or three minimal GHKM and SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are Stanford Parser4, Berkeley Parser 5 , Collins Parser (Dan Bikel’s reimplementation of Collins Model 2) 6 and Charniak Parser7. The former two are state-of-the-art nonlexicalized parsers, while the latter two are stateof-the-art lexicalized parsers. All the parsers are trained on sections 02-21 of the Wall Street Jo"
C10-2154,P01-1067,0,0.0737178,"rsers in translation rule extraction, and then empirically investigate the effectiveness of our method on ChineseEnglish translation tasks. Experimental results show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al."
C10-2154,N06-1033,1,0.816388,"different parsers automatically. 6 Experiments The experiments are conducted on ChineseEnglish translation in a state-of-the-art string-totree SMT system. 6.1 and the composed rules are generated by composing two or three minimal GHKM and SPMT rules3. We use a CKY-style decoder with cube pruning (Huang and Chiang, 2007) and beam search to decode new Chinese sentences. By default, the beam size is set to 30. For integrating n-gram language model into decoding efficiently, rules containing more than two variables or source word sequences are binarized using the synchronous binarization method (Zhang et al., 2006; Xiao et al., 2009). The system is evaluated in terms of the caseinsensitive NIST version BLEU (using the shortest reference length), and statistical significant test is conducted using the re-sampling method proposed by Koehn (2004). 6.2 Four syntactic parsers are chosen for the experiments. They are Stanford Parser4, Berkeley Parser 5 , Collins Parser (Dan Bikel’s reimplementation of Collins Model 2) 6 and Charniak Parser7. The former two are state-of-the-art nonlexicalized parsers, while the latter two are stateof-the-art lexicalized parsers. All the parsers are trained on sections 02-21 o"
C10-2154,P08-1064,0,0.0131556,"lts show that extracting translation rules using multiple parsers improves a string-to-tree system by over 0.9 BLEU points on both NIST 2004 and 2005 test corpora. 1 Introduction Recently various syntax-based models have been extensively investigated in Statistical Machine Translation (SMT), including models between source trees and target strings (Quirk et al., 2005; Liu et al., 2006; Huang et al., 2006), source strings and target trees (Yamada and Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects th"
C10-2154,C10-1151,1,0.824466,"k-best parses to improve multi-parser based rule extraction in practice. z The MT performance is not influenced by the parsing performance of the parsers used in rule extraction very much. Actually, the MT system does not show different preferences for different parsers. 1-best 8.5 Figure 4: Multi-parser based rule extraction & rule extraction with k-best parses (MT05). 7 8 In this paper, we present a simple and effective method to improve rule coverage by using multiple parsers in translation rule extraction. Experimental results show that 38.4 38.2 geneous decoding (or parsing) techniques (Zhu et al., 2010) to make use of heterogeneous grammars in the stage of decoding. Both topics are very interesting and worth studying in our future work. Besides k-best extraction, our method can also be applied to other rule extraction schemes, such as forest-based rule extraction. As (Mi and Huang, 2008) has shown that forest-based extraction is more effective than k-best extraction in improving translation accuracy, it is expected to achieve further improvements by using multiparser based rule extraction and forest-based rule extraction together. Discussion and Future Work In this work, all the parsers are"
C10-2154,W06-1606,0,0.380496,"Knight, 2001; Galley et al., 2006; Shen et al., 2008), or source trees and target trees (Eisner, 2003; Ding and Palmer, 2005; Cowan et al., 2006; Zhang et al., 2008; Liu et al., 2009). In these models, automatic extraction of translation rules is an important issue, in which translation rules are typically extracted using parse trees on source/target-language side or both sides of the bilingual text. Exploiting the syntactic information encoded in translation rules, syntax-based systems have shown to achieve comparable performance with phrase-based systems, even outperform them in some cases (Marcu et al., 2006). Among all the factors contributing to the success of syntax-based systems, rule coverage has been proved to be an important one that affects the translation accuracy of syntax-based systems (DeNeefe et al., 2007; Shen et al., 2008). However, these systems suffer from a problem that translation rules are extracted using only 1-best parse tree generated by a single parser, which generally results in relatively low rule coverage due to the limited scope in rule extraction (Mi and Huang, 2008). To alleviate this problem, a straightforward solution is to enlarge the scope of rule extraction, and"
C10-2154,P09-1063,0,\N,Missing
C10-2154,W06-1628,0,\N,Missing
C10-2154,P06-1121,0,\N,Missing
C10-2176,P01-1005,0,0.0124345,"arser trained on a target treebank during the parser assigning parse trees to sentences in the source treebank. Experiments on two Chinese treebanks show significant improvements in conversion accuracy over baseline systems, especially when training data used for building the parser is small in size. 1 Introduction Recent years have seen extensive applications of machine learning methods to natural language processing problems. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is time consuming and labor intensive. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) for Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for t"
C10-2176,A00-2018,0,0.594475,"ave seen extensive applications of machine learning methods to natural language processing problems. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is time consuming and labor intensive. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) for Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same tasks, it is significant if such corpora can be used jointly. Such attempt is especially significant for some languages that have limited size of labeled data. At first sight, a direct combination of multiple corpora is a way to this end. However, corpora created for the same NLP tasks are generally built by diffe"
C10-2176,P99-1065,0,0.0539159,"raining data used for building the parser is small in size. The rest of the paper is structured as follows. In Section 2 we describe previous work on treebank conversion. In Section 3, we describe in detail the informed decoding approach. Section 4 presents experimental results which demonstrate the effectiveness of our approach. Finally, Section 5 concludes our work. 2 Related Work Previous work on treebank conversion can be grouped into two categories according to whether grammar formalisms of treebanks are identical. One type focuses on converting treebanks of different grammar formalisms. Collins et al. (1999) 1 The terminology decoding is referred to the parsing phase of a parser. 2 Note that although we use Chinese treebanks, our approach is language independent. addressed constituent syntactic parsing on Czech using a treebank converted from a Prague dependency treebank, where conversion rules derived from head-dependent pairs and heuristic rules are applied. Xia and Palmer (2001) compared three algorithms for conversion from dependency structures to phrase structures. The algorithms expanded each node in input dependency structures into a projection chain, and labeled the newly inserted node wi"
C10-2176,P08-1067,0,0.0694319,"s reflect underlying sentence meaning. On the other hand, although Fig. 2(a) also has (minor) differences in tree structures from Fig. 2(c), it is preferred as the conversion result3 . From the example we can get inspired by the observation that original annotations in a source treebank are informative and necessary to converting parse trees in the source treebank. In general, conversion like that from Fig. 2(c) • Selecting-from-k-best works on the basis of k-best lists. Unfortunately, we often see very few variations in k-best lists. For example, 50-best trees present only 5 to 6 variations (Huang, 2008). The lack of diversities in k-best lists makes information from the source treebank less effective in selecting parse trees. By contrast, incorporating such information into decoding makes the information affect the whole parse forest. 3.2 Formalization of Information from Source Treebank In this paper, information from a source treebank translates into two strategies which help a target parser to prune illegal partial parse trees and to rank legal partial parse trees higher. Following are the two strategies: 3 Note that we don’t deny existence of annotation distinctions between the treebanks"
C10-2176,W07-2416,0,0.0285931,"chains. Xia et al. (2008) automatically extracted conversion rules from a target treebank and proposed strategies to handle the case when more than one conversion rule are applicable. Instead of using conversion rules, Niu et al. (2009) proposed to convert a dependency treebank to a constituency one by using a parser trained on a constituency treebank to generate kbest lists for sentences in the dependency treebank. Optimal conversion results are selected from the k-best lists. There also exists work in the reverse direction: from a constituency treebank to a dependency treebank (Nivre, 2006; Johansson and Nugues, 2007). Relatively few efforts have been put on conversion between treebanks that have the same grammar formalisms but follow different annotation standards. Wang et al. (1994) applied a similar framework as in (Niu et al., 2009) to convert from a simple constituency treebank to a more informative one. The basic idea is to apply a parser built on a target treebank to generate k-best lists for sentences in the source treebank. Then, a matching metric is defined on the number of identical bracketing spans between two trees. Such a function computes a score for each parse tree in a k-best list and its"
C10-2176,P09-1006,0,0.107723,"t pairs and heuristic rules are applied. Xia and Palmer (2001) compared three algorithms for conversion from dependency structures to phrase structures. The algorithms expanded each node in input dependency structures into a projection chain, and labeled the newly inserted node with syntactic categories. The three algorithms differ only in heuristics adopted to build projection chains. Xia et al. (2008) automatically extracted conversion rules from a target treebank and proposed strategies to handle the case when more than one conversion rule are applicable. Instead of using conversion rules, Niu et al. (2009) proposed to convert a dependency treebank to a constituency one by using a parser trained on a constituency treebank to generate kbest lists for sentences in the dependency treebank. Optimal conversion results are selected from the k-best lists. There also exists work in the reverse direction: from a constituency treebank to a dependency treebank (Nivre, 2006; Johansson and Nugues, 2007). Relatively few efforts have been put on conversion between treebanks that have the same grammar formalisms but follow different annotation standards. Wang et al. (1994) applied a similar framework as in (Niu"
C10-2176,P06-1055,0,0.0815472,"ve applications of machine learning methods to natural language processing problems. Typically, increase in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is time consuming and labor intensive. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) for Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same tasks, it is significant if such corpora can be used jointly. Such attempt is especially significant for some languages that have limited size of labeled data. At first sight, a direct combination of multiple corpora is a way to this end. However, corpora created for the same NLP tasks are generally built by different organizations. Th"
C10-2176,C02-1145,0,0.218279,"rease in the scale of training data boosts the performance of machine learning methods, which in turn enhances the quality of learning-based NLP systems (Banko and Brill, 2001). However, annotating data by human is time consuming and labor intensive. For this reason, human-annotated corpora are considered as the most valuable resource for NLP. In practice, there often exist more than one corpus for the same NLP tasks. For example, for constituent syntactic parsing (Collins, 1999; Charniak, 2000; Petrov et al., 2006) for Chinese, in addition to the most popular treebank Chinese Treebank (CTB) (Xue et al., 2002), there are also other treebanks such as Tsinghua Chinese Treebank (TCT) (Zhou, 1996). For the purpose of full use of readily available human annotations for the same tasks, it is significant if such corpora can be used jointly. Such attempt is especially significant for some languages that have limited size of labeled data. At first sight, a direct combination of multiple corpora is a way to this end. However, corpora created for the same NLP tasks are generally built by different organizations. Thus such corpora often follow different annotation standards and/or even different linguistic the"
C10-2176,P94-1034,0,0.884799,"ble. Instead of using conversion rules, Niu et al. (2009) proposed to convert a dependency treebank to a constituency one by using a parser trained on a constituency treebank to generate kbest lists for sentences in the dependency treebank. Optimal conversion results are selected from the k-best lists. There also exists work in the reverse direction: from a constituency treebank to a dependency treebank (Nivre, 2006; Johansson and Nugues, 2007). Relatively few efforts have been put on conversion between treebanks that have the same grammar formalisms but follow different annotation standards. Wang et al. (1994) applied a similar framework as in (Niu et al., 2009) to convert from a simple constituency treebank to a more informative one. The basic idea is to apply a parser built on a target treebank to generate k-best lists for sentences in the source treebank. Then, a matching metric is defined on the number of identical bracketing spans between two trees. Such a function computes a score for each parse tree in a k-best list and its corresponding parse tree in the source treebank. Finally, the parse tree with the highest score in a k-best list is selected to be the conversion result. The difference b"
C10-2176,D09-1161,0,0.0291315,"Missing"
C10-2176,J03-4003,0,\N,Missing
C12-1106,D12-1133,0,0.381527,"tructural features on both sides of the attachment point are accessible so as to make more informed decision. In this work, we further generalize the algorithm of (Goldberg and Elhadad, 2010) to a general sequential labelling framework. We apply this framework to Chinese POS tagging and dependency parsing which has never been studied under the easy-first framework before. One characteristic of Chinese dependency parsing is that parsing performance can be dramatically affected by the quality of POS tags of the input sentence (Li et al., 2010). Recent work (Li et al., 2010; Hatori et al., 2011; Bohnet and Nivre, 2012) empirically verified that solving POS tagging and dependency parsing jointly can boost the performance of both the two tasks. To further improve tagging and parsing accuracy, we also solve the two tasks jointly. While previous joint methods are either graph-based or transition-based algorithm, in this work we propose the first joint tagging and dependency parsing algorithm under the easy-first framework. In addition, we also adopt a different training strategy to learn the model parameters. Previous approaches all train their joint model with the objective of minimizing the loss between the r"
C12-1106,W02-1001,0,0.139634,"uracy and runs fast. And our joint model achieves tagging accuracy of 94.27 which is the best result reported so far. KEYWORDS: dependency parsing, POS tagging, perceptron, easy-first Proceedings of COLING 2012: Technical Papers, pages 1731–1746, COLING 2012, Mumbai, December 2012. 1731 1 Introduction To sequential labelling problems, such as POS tagging or incremental parsing, traditional approaches 1) decompose the input sequence into several individual items each of which corresponds to a token of the input sequence; 2) predict these items in a fixed left-to-right (or right-to-left) order (Collins 2002; Ratnaparkhi 1996). The drawback of such fixed order approach is that when predicting one item, only the labels on the left side can be used while the labels on the right side is still unavailable. (Goldberg and Elhadad, 2010) proposed the easy-first dependency parsing algorithm to relax the fixed left-to-right order and to incorporate more structural features from both sides of the attachment point. Comparing with a deterministic transition based parser which parses the sentence left-to-right, their deterministic easy-first parser achieves significant better accuracy. The key idea behind the"
C12-1106,D07-1098,0,0.0695046,"is selected, then both tagging and parsing error cause parameter update; if is selected, then only tagging errors can cause parameter update. This can be done by adding all valid attach actions to the compatible set regardless whether those actions are indeed compatible with the gold reference (line 6 to line 7); For , only parsing errors cause parameter update which can be achieved similar to the case of . 4 Experiments To make comparison with previous works, we use Penn Chinese Treebank 5.1 (CTB5) (Xue et al., 2005) to evaluate our method. We use the standard split of CTB5 as described in (Duan et al., 2007): section 001-815 and 1001-1136 are used as training set, section 886-931 and 1148-1151 are used as development set, section 816-885 and 1137-1147 are used as test set. Head finding rules of (Zhang and Clark 2008b) are used to convert the constituent trees into dependency trees. An Intel Core i7 870 2.93 GHz machine is used for evaluation. For POS tagging and dependency parsing, the number of training iterations are selected according to the model’s performance on the development set. The model which achieves the highest score on the development set is selected to run on the test set. 4.1 POS"
C12-1106,D09-1127,0,0.0172753,"Word – – – – 79.03 79.29 78.43 78.73 78.87 Root – – – – 74.70 74.65 67.14 68.29 68.50 Compl – – – – 27.19 27.24 28.98 29.34 29.29 – – 32.7 9 J-N Speed 93.82 93.51 5.8 93.84 2 391 385 355 TABLE 4 – Parsing performance. H&S-10 and Z&N-11 denote parsers in Huang and Sagae (2010) and Zhang and Nirve (2011), respectively. H&S-H and Z&N-H denote Hatori et al., (2011)’s re-implementation of H&S-10 and Z&N-11, respectively. Li-10-O2/O3 denotes the 2rd/3rd graph based model of Li et al., (2010) either or or is a preposition. For Chinese, PP attachment ambiguity is not as prevalent as that of English (Huang et al., 2009) and we found that use these features without any limitation yields better results. VTT includes valence features, tri-gram features and third order features which were proved useful for transition based parsers (Zhang and Nivre, 2011). For OP , some additional order preference feature templates are added. Parsing results are shown in table 4. “GoldPOS” denotes the input with gold standard POS tag. “AutoPOS” denotes that the training set are assigned with gold standard POS tag while the test set are tagged by our easy-first tagger. “J-N” denotes that we use 10-fold Jack-Nifing to train the mod"
C12-1106,P10-1110,0,0.0608227,"0 GoldPOS Word 85.20 86.00 Root 78.32 – Compl 33.72 36.90 VTT OP 85.12 85.96 86.18 86.00 84.62 85.18 85.22 78.30 80.87 78.58 77.59 74.70 75.38 75.48 32.77 35.03 34.07 34.02 36.12 36.27 36.80 Tag Accuracy – – AutoPOS Word – – 77.13 78.04 – – 77.45 77.64 77.66 Root – – 72.49 75.55 – – 68.50 68.92 68.35 Compl – – 25.13 26.07 – – 28.89 28.19 28.45 Word – – – – 79.03 79.29 78.43 78.73 78.87 Root – – – – 74.70 74.65 67.14 68.29 68.50 Compl – – – – 27.19 27.24 28.98 29.34 29.29 – – 32.7 9 J-N Speed 93.82 93.51 5.8 93.84 2 391 385 355 TABLE 4 – Parsing performance. H&S-10 and Z&N-11 denote parsers in Huang and Sagae (2010) and Zhang and Nirve (2011), respectively. H&S-H and Z&N-H denote Hatori et al., (2011)’s re-implementation of H&S-10 and Z&N-11, respectively. Li-10-O2/O3 denotes the 2rd/3rd graph based model of Li et al., (2010) either or or is a preposition. For Chinese, PP attachment ambiguity is not as prevalent as that of English (Huang et al., 2009) and we found that use these features without any limitation yields better results. VTT includes valence features, tri-gram features and third order features which were proved useful for transition based parsers (Zhang and Nivre, 2011). For OP , some additio"
C12-1106,P11-1089,0,0.0401458,"hm. By incorporating additional loss during training, our method achieves the best tagging accuracy reported so far. Shen et al. (2007) proposed a bi-directional POS tagging algorithm which achieves state-of-the-art accuracy on English POS tagging. Comparing to their method, our tagging algorithm in this paper is much simpler and we are the first to use order preference features in POS tagging. Also this is the first work that applies easy-first tagging on Chinese. For joint POS tagging and (unlabelled, projective) dependency parsing, Li et al. (2010) proposed the first graph based algorithm. Lee et al. (2011) proposed a graphical model to solve the joint problem. Hatori et al. (2011) proposed the first transition based algorithm. Bohnet and Nivre (2012) extended Hatori et al. (2011) to labelled non-projective dependency parsing. Different from the works talked above, our method is based on the easy-first framework. In addition, all previous joint methods optimize a single loss in the training phase while we are the first to train the joint model with additional loss. Hall et al. (2011) proposed the augmented-loss training for dependency parser that aims at adapting the parser to other domains or t"
C12-1106,H05-1059,0,0.0869697,"Missing"
C12-1106,P08-1101,0,0.0685946,"le set regardless whether those actions are indeed compatible with the gold reference (line 6 to line 7); For , only parsing errors cause parameter update which can be achieved similar to the case of . 4 Experiments To make comparison with previous works, we use Penn Chinese Treebank 5.1 (CTB5) (Xue et al., 2005) to evaluate our method. We use the standard split of CTB5 as described in (Duan et al., 2007): section 001-815 and 1001-1136 are used as training set, section 886-931 and 1148-1151 are used as development set, section 816-885 and 1137-1147 are used as test set. Head finding rules of (Zhang and Clark 2008b) are used to convert the constituent trees into dependency trees. An Intel Core i7 870 2.93 GHz machine is used for evaluation. For POS tagging and dependency parsing, the number of training iterations are selected according to the model’s performance on the development set. The model which achieves the highest score on the development set is selected to run on the test set. 4.1 POS tagging Following Zhang and Clark (2008a), in the experiments, we also use a dictionary to limit the number of candidate tags for each word. That is, for a word which occurs more than M times in the training data"
C12-1106,D08-1059,0,0.0610951,"le set regardless whether those actions are indeed compatible with the gold reference (line 6 to line 7); For , only parsing errors cause parameter update which can be achieved similar to the case of . 4 Experiments To make comparison with previous works, we use Penn Chinese Treebank 5.1 (CTB5) (Xue et al., 2005) to evaluate our method. We use the standard split of CTB5 as described in (Duan et al., 2007): section 001-815 and 1001-1136 are used as training set, section 886-931 and 1148-1151 are used as development set, section 816-885 and 1137-1147 are used as test set. Head finding rules of (Zhang and Clark 2008b) are used to convert the constituent trees into dependency trees. An Intel Core i7 870 2.93 GHz machine is used for evaluation. For POS tagging and dependency parsing, the number of training iterations are selected according to the model’s performance on the development set. The model which achieves the highest score on the development set is selected to run on the test set. 4.1 POS tagging Following Zhang and Clark (2008a), in the experiments, we also use a dictionary to limit the number of candidate tags for each word. That is, for a word which occurs more than M times in the training data"
C12-1106,P11-2033,0,0.0311594,"N-11 denote parsers in Huang and Sagae (2010) and Zhang and Nirve (2011), respectively. H&S-H and Z&N-H denote Hatori et al., (2011)’s re-implementation of H&S-10 and Z&N-11, respectively. Li-10-O2/O3 denotes the 2rd/3rd graph based model of Li et al., (2010) either or or is a preposition. For Chinese, PP attachment ambiguity is not as prevalent as that of English (Huang et al., 2009) and we found that use these features without any limitation yields better results. VTT includes valence features, tri-gram features and third order features which were proved useful for transition based parsers (Zhang and Nivre, 2011). For OP , some additional order preference feature templates are added. Parsing results are shown in table 4. “GoldPOS” denotes the input with gold standard POS tag. “AutoPOS” denotes that the training set are assigned with gold standard POS tag while the test set are tagged by our easy-first tagger. “J-N” denotes that we use 10-fold Jack-Nifing to train the model. “Tag Accuracy” denotes the test set tagging accuracy. From table 4, we can see that valence and tri-gram features are also effective for easy-first parser. For GoldPOS, word accuracy boosted from 84.62 to 85.18. For AutoPOS and J-N"
C12-1106,D11-1138,0,\N,Missing
C12-1106,D11-1109,0,\N,Missing
C12-1106,I11-1136,0,\N,Missing
C12-1106,N10-1115,0,\N,Missing
C12-1106,P07-1096,0,\N,Missing
C12-1194,P11-1070,0,0.0457625,"Missing"
C12-1194,W08-2102,0,0.115098,"Missing"
C12-1194,A00-2018,0,0.664737,"Missing"
C12-1194,P05-1022,0,0.248583,"Missing"
C12-1194,D09-1060,0,0.117273,"Missing"
C12-1194,P96-1025,0,0.454105,"Missing"
C12-1194,P97-1003,0,0.679597,"Missing"
C12-1194,W02-1001,0,0.0452382,"Missing"
C12-1194,P04-1015,0,0.48486,"Missing"
C12-1194,P99-1059,0,0.159813,"Missing"
C12-1194,P10-1110,0,0.0829761,"Missing"
C12-1194,D09-1087,0,0.181321,"Missing"
C12-1194,D10-1002,0,0.227748,"Missing"
C12-1194,J93-2004,0,0.0398783,"Missing"
C12-1194,P06-1043,0,0.0770732,"Missing"
C12-1194,W04-0308,0,0.0546817,"Missing"
C12-1194,W07-2201,0,0.076429,"Missing"
C12-1194,N10-1003,0,0.0254194,"Missing"
C12-1194,N07-1051,0,0.198272,"Missing"
C12-1194,W97-0301,0,0.202207,"Missing"
C12-1194,W05-1513,0,0.502947,"Missing"
C12-1194,P06-2089,0,0.0379412,"Missing"
C12-1194,P06-1054,0,0.0572841,"Missing"
C12-1194,W03-3023,0,0.344173,"Missing"
C12-1194,D09-1161,0,0.0321568,"Missing"
C12-1194,D08-1059,0,0.060491,"Missing"
C12-1194,W09-3825,0,0.340789,"Missing"
C12-1194,P11-1069,0,0.144215,"Missing"
C12-1194,J03-4003,0,\N,Missing
C14-1195,D07-1090,0,0.00939615,"The test sets (newswire: 1,779 sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12 (mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST MT12 and the Google counts corpus using the ”stupid” backoff method (Brants et al., 2007). For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All"
C14-1195,P05-1033,0,0.528215,"traction and decoding for hierarchical phrase-based translation. We obtain tree-to-string rules by the GHKM method and use them to complement Hiero-style rules. All these rules are then employed to decode new sentences with source language parse trees. We experiment with our approach in a state-of-the-art Chinese-English system and demonstrate +1.2 and +0.8 BLEU improvements on the NIST newswire and web evaluation data of MT08 and MT12. 1 Introduction Synchronous context free grammars (SCFGs) are widely used in statistical machine translation (SMT), with hierarchical phrase-based translation (Chiang, 2005) as the dominant approach. Hiero grammars are easily extracted from word-aligned parallel corpora and can capture complex nested translation relationships. Hiero grammars are formally syntactic, but rules are not constrained by source or target language syntax. This lack of constraint can lead to intractable decoding and bad performance due to the over-generation of derivations in translation. To avoid these problems, the extraction and application of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised; and rules are limited to two non-terminal"
C14-1195,P10-1146,0,0.0917775,"obustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licenced under a Creative Commons Attribution 4"
C14-1195,J10-3008,1,0.897734,"Missing"
C14-1195,P03-2041,0,0.455159,"Missing"
C14-1195,D12-1109,0,0.0143501,"ed in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with both word alignment and source (or targe"
C14-1195,P06-1121,0,0.157895,"to intractable decoding and bad performance due to the over-generation of derivations in translation. To avoid these problems, the extraction and application of SCFG rules is typically constrained by a source language span limit; (non-glue) rules are lexicalised; and rules are limited to two non-terminals which are not allowed to be adjacent in the source language. These constraints can yield good performing translation systems, although at a sacrifice in the ability to model long-distance movement and complex reordering of multiple constituents. By contrast, the GHKM approach to translation (Galley et al., 2006) relies on a syntactic parse on either the source or target language side to guide SCFG extraction and translation. The parse tree provides linguistically-motivated constraints both in grammar extraction and in translation. This allows for looser span constraints; rules need not be lexicalised; and rules can have more than two non-terminals to model complex reordering multiple constituents. There are also modelling benefits as more meaningful features can be used to encourage derivations with ”well-formed” syntactic tree structures. However, GHKM can have robustness problems in that translatio"
C14-1195,W10-1761,0,0.0406427,"Missing"
C14-1195,P07-1019,0,0.0225341,"type 1 rules directly. For tree-to-string rules associated with type 2, we attempt to match rules to the source syntactic tree. If a match is found: the source span of the matching tree fragment is noted and the CYK cell for that span is selected; the tree-to-string rule is converted to a Hiero-style rule; and that rule is added to the list of rules in the selected CYK cell. Once this process is finished, RTN construction, expansion, and language model composition proceeds as usual. Similar modifications could be made to incorporate these rules into cube pruning (Chiang, 2007), cube growing (Huang and Chiang, 2007), and PDT intersection and expansion (Iglesias et al., 2011). We now elaborate on the rule matching strategy. Type 1 Rules The source sentence is parsed as is usual in Hiero-style translation, with the exception that we impose no span limit on rule applications for source spans corresponding to constituents in the Chinese syntactic tree. Rule matching, the procedure that determines if a rule applies to a source span, is based on string matching (see Figure 4(a)). For example, the type 1 rule h9 in Figure 4(c) can be applied to spans (1,13) and (2,13) since both of them agree with tree constitu"
C14-1195,D10-1027,0,0.0168031,"an limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with both word alignment"
C14-1195,2006.amta-papers.8,0,0.366008,"Missing"
C14-1195,E09-1044,1,0.894124,"Missing"
C14-1195,D11-1127,1,0.87318,"Missing"
C14-1195,N13-1060,0,0.0232239,"Missing"
C14-1195,P06-1077,0,0.614988,"Typically these are: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingua"
C14-1195,P09-1063,0,0.365919,"Missing"
C14-1195,P09-1065,0,0.0321399,"Missing"
C14-1195,D08-1076,0,0.0276395,"sducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were optimized using lattice-based minimum error rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments a"
C14-1195,W06-1606,0,0.0321305,"ifference lies in that the tree-to-string rule indicator feature does not distinguish between different syntactic labels, whereas soft syntactic features do. • Features in syntactic MT. In general tree-to-string rules have their own features which are different from those used in Hiero-style systems. For example, the features in syntactic MT systems can be defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we choose five popular features used in syntactic MT systems, including the bi-directional phrase-based conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabilities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates. For example, the phrase-based features are the probabilities of translating between the frontier nodes of sr and tr . The syntax-based features are the probabilities of generating r conditioned on its root, 2 We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline system. 2068 source and target language sides, respectively. More formally, we use the following estimates for these probabilities:"
C14-1195,P08-1114,0,0.143232,"multiple constituents. There are also modelling benefits as more meaningful features can be used to encourage derivations with ”well-formed” syntactic tree structures. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We al"
C14-1195,D08-1022,0,0.0227804,"not distinguish between different syntactic labels, whereas soft syntactic features do. • Features in syntactic MT. In general tree-to-string rules have their own features which are different from those used in Hiero-style systems. For example, the features in syntactic MT systems can be defined as the generation probabilities conditioned on the root symbol of the tree-fragment. Here we choose five popular features used in syntactic MT systems, including the bi-directional phrase-based conditional translation probabilities (Marcu et al., 2006) and three syntax-based conditional probabilities (Mi and Huang, 2008). All these probabilities can be computed by relative-frequency estimates. For example, the phrase-based features are the probabilities of translating between the frontier nodes of sr and tr . The syntax-based features are the probabilities of generating r conditioned on its root, 2 We experimented with soft syntactic features (Marton and Resnik, 2008) but found no improvement over our baseline system. 2068 source and target language sides, respectively. More formally, we use the following estimates for these probabilities: P Pphr (tr |sr ) = r00 :ϕ(sr00 )=ϕ(sr )∧tr00 =tr P P Pphr (sr |tr ) ="
C14-1195,P08-1023,0,0.02228,"re: (a) a rule span limit to be applied in decoding and sometimes also in rule extraction, set to 10; (b) a limit on the rank of the grammar (number of non-terminals that can appear on a rule), set to 2; and (c) a prohibition of consecutive non-terminals on the source language side of a rule (except the glue rules). 2.2 Tree-to-String Translation Instead of modelling the problem based on surface strings, tree-to-string systems model the translation equivalency relations from source language syntactic trees to target language strings using derivations of tree-to-string rules (Liu et al., 2006; Mi et al., 2008; Huang and Mi, 2010; Feng et al., 2012). A tree-to-string rule is a tuple hsr , tr , ∼i, where sr is a source language tree-fragment with terminals and non-terminals at leaves; tr is a string of target-language terminals and non-terminals; and ∼ is a 1-to-1 alignment between the non-terminals of sr and tr , for example, VP(VV(提高) x1 :NN) → increases x1 is a tree-to-string rule, where the non-terminals labeled with the same index x1 indicate the alignment. To obtain tree-to-string rules, a popular way is to perform the GHKM rule extraction (Galley et al., 2006) on the bilingual sentences with"
C14-1195,P02-1040,0,0.0950826,"ror rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments are organized as follows: • Baseline and Span Limits (exp01 and exp02) First we study the effect of removing the span limit for tree constituents, that is, SCFG rules can be 2069 Entry System exp01 exp02 exp03 exp04 exp05 exp06 exp07 exp08 exp09 exp10 baseline += no span limit += t-to-s rules += t-to-s features t-to-s baseline exp04 on spans &gt; 10 exp04 with null trans. exp04 + left binariz. exp04 + right binariz. exp04 + forest binariz. tune (1755) 35.84 36.05 36.63 36.82 34.63 36.17 36.10 37.11 36.58 37.03 mt08 (691) 35.85 36.08 36.51 36.49 34.44 36.11 36.03 37.46 36.56 37.2"
C14-1195,N07-1051,0,0.0249299,"track. Word alignments are obtained using MTTK (Deng and Byrne, 2008) in both Chinese-to-English and English-to-Chinese directions, and then unioning the links. The data from newswire and web genres was used for tuning and test. The development sets contain 1,755 sentences and 2160 sentences for the two genres respectively. The test sets (newswire: 1,779 sentences, web: 1768 sentences) contain all newswire and web evaluation data of MT08 (mt08), MT12 (mt12), and MT08 progress test (mt08.p). All Chinese sentences in the training, development and test sets were parsed using the Berkeley parser (Petrov and Klein, 2007). A Kneser-Ney 4-gram language model was trained on the AFP and Xinhua portions of the English Gigaword in addition to the English side of the parallel corpus. A stronger 5-gram language model was trained on all English data of NIST MT12 and the Google counts corpus using the ”stupid” backoff method (Brants et al., 2007). For decoding we use HiFST, which is implemented with weighted finite state transducers (de Gispert et al., 2010). A two-pass decoding strategy is adopted; first, only the 4-gram language model and the translation model are activated; and then, the 5-gram language model is app"
C14-1195,W13-2225,1,0.875894,"Missing"
C14-1195,2010.iwslt-papers.18,0,0.0179447,"inese syntactic structure indicating the reordered translations of NP and VP. However, such a rule would not normally be included in a Hiero grammar, as it would require consecutive source language non-terminals (see Figure 3). 3 The Proposed Approach Both the tree-to-string model and the hierarchical phrase-based model have their own strengths and weaknesses. For example, tree-to-string systems are good at modelling long distance reordering, while hierarchical phrase-based systems are relatively more powerful in handling ill-formed sentences1 and free translations (Zhao and Al-Onaizan, 2008; Vilar et al., 2010). Here we present a method to enhance hierarchical phrase-based systems with tree-to-string rules and benefit from both models. The idea is simple: we obtain both the tree-to-string grammar and the Hiero-style SCFG from the training data, and then use tree-to-string rules as additional rules in decoding with the SCFG. Figure 2 shows an overview of our approach and the usual hierarchical phrase-based approach. Our approach requires source language parse trees to be input in both rule extraction and decoding. In rule extraction, we acquire tree-to-string rules using the GHKM method and Hiero-sty"
C14-1195,J10-2004,0,0.0237385,"es the baseline result using the Hiero model (i.e., type 1 rules only). To investigate the effect of failed parse trees on system performance, we also report the BLEU score including null translations for which the parser fails. As shown in exp07, there are significantly lower BLEU scores when null translations are included. It indicates that our approach is more robust than standard treeto-string systems which would generate an empty translation if the source language parser fails. • Results on Binarization (exp08-10) Tree binarization is a widely used method to improve syntactic MT systems (Wang et al., 2010). exp08-10 show the results of our improved system with left-heavy, right-heavy and forest-based bina2070 Reference: After North Korea demanded concessions from U.S. again before the start of a new round of six-nation talks , ... Baseline: In the new round of six-nation talks on North Korea again demanded that U.S. in the former promise concessions , ... GHKM+Hiero: After North Korea again demanded that U.S. promised concessions before the new round of six-nation talks , ... a Hiero rule X → h 在 X1 后, after X1 i is applied on span (1,15) Input: 北韩2 再度3 要求4 美国5 于6 新7 回合8 六9 国10 会谈11 前12 承诺13 让步"
C14-1195,P12-3004,1,0.88289,"guage model and the translation model are activated; and then, the 5-gram language model is applied for second-pass rescoring of the translation lattices generated by the first-pass decoding stage. We extracted SCFG rules from the parallel corpus using the standard heuristics (Chiang, 2007) and filtering strategies (Iglesias et al., 2009). The span limit was set to 10 in extracting basic phrases and decoding. All features weights were optimized using lattice-based minimum error rate training (Macherey et al., 2008). For tree-to-string extraction, we used a reimplementation of the GHKM method (Xiao et al., 2012) and extracted rules from a 600K-sentence portion of the parallel data. To prune the tree-to-string rule set, we restricted the extraction to rules with at most 5 frontier non-terminals and 5 terminals. Also, we discarded lexicalized rules with a Chinese-to-English translation probability of < 0.02 and non-lexicalized rules with a Chinese-to-English translation probability of < 0.10. 4.2 Results We report MT performance in Table 1 by case-insensitive BLEU (Papineni et al., 2002). The experiments are organized as follows: • Baseline and Span Limits (exp01 and exp02) First we study the effect of"
C14-1195,D11-1020,0,0.361476,"Missing"
C14-1195,P08-1064,0,0.375097,"Missing"
C14-1195,D08-1060,0,0.302527,"s. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licenced under a Creative Common"
C14-1195,W06-3119,0,0.264258,"rmed” syntactic tree structures. However, GHKM can have robustness problems in that translation relies on the quality of the parse tree and the diversity of rule types can lead to sparsity and limited coverage. In this paper we describe a simple but effective approach to introducing source language syntax into hierarchical phrase-based translation to get the benefits of both approaches. Unlike previous work, we do not resort to soft/hard syntactic constraints (Marton and Resnik, 2008; Li et al., 2013) or Hiero-style rule extraction algorithms for incorporating syntactic annotation into SCFGs (Zollmann and Venugopal, 2006; Zhao and Al-Onaizan, 2008; Chiang, 2010). We instead use GHKM syntactic rules to augment the baseline Hiero grammar and decoder. Our approach uses GHKM rules if possible and Hiero rules if not. We report performance on a state-of-the-art Chinese-English system. In a large-scale NIST evaluation task, we find significant improvements of over 1.2 and 0.8 BLEU relative to a strong Hiero baseline on the newswire and web evaluation data of MT08 and MT12. We also investigate variations in the GHKM formalism and find, for example, that our approach works well with binarized trees. This work is licen"
C14-1195,2010.iwslt-keynotes.2,0,\N,Missing
C14-1195,J07-2003,0,\N,Missing
C18-1255,2014.iwslt-evaluation.1,0,0.0832556,"Missing"
C18-1255,W17-3203,0,0.0145395,"ut size of the 0. All other parameters are initialized from U(− √f an in in parameter matrix. We use negative Maximum Likelihood Estimation (MLE) as loss function, and train all the models using Adam (Kingma and Ba, 2014) with β1 = 0.9, β2 = 0.98, and ϵ = 10−9 . We run training for 40 epochs with a mini-batch of 80. The learning rate is scheduled as described in (Vaswani et al., 2017): lr = d−0.5 · min(t−0.5 , t · 16k−1.5 ), where t is the step number. After that, we restart Adam and continue the training for additional 20 epochs with a fixed learning rate 5e−5 and a smaller mini-batch of 32 (Denkowski and Neubig, 2017). At test time, translations are generated by beam search with length normalization. By tuning on the validation set, we use a beam of width 8 and a length normalization weight of 1.6. For Chinese-English systems, we use a 6-layer encoder and a 6-layer decoder, with d = 512 and 2048 hidden units in the FNN sub-layer. We restart Adam after 10 epochs and train the model for 5 additional epochs. A beam of width 12 and a length normalization weight of 1.3 are employed. Note that the network equipped with our fusion layer increases a fraction of the computation cost than original one. E.g., in Chin"
C18-1255,P17-1138,0,0.0356522,"test. For Chinese-English translation, we use parts of the bitext provided within NIST12 OpenMT5 . We choose NIST 2006 (MT06) as the validation set, and 2004 (MT04), 2005 (MT05), 2008 (MT08) as the 4 5 W1 can be unique for each layer. We discuss this issue in Section 4.4. LDC2000T46, LDC2000T47, LDC2000T50, LDC2003E14, LDC2005T10, LDC2002E18, LDC2007T09, LDC2004T08 3019 System Existing Systems Baselines MLRF Systems RNN-MIXER (Ranzato et al., 2015) RNN-BSO (Wiseman and Rush, 2016) RNN-AC(Bahdanau et al., 2016) RNN-NPMT (Huang et al., 2017b) RNN-NPMT + LM (Huang et al., 2017b) ConvSeq2Seq-MLE (Edunov et al., 2017) ConvSeq2Seq-Risk (Edunov et al., 2017) Transformer-MLE +RestartAdam +RestartAdam-4Layers +RestartAdam-6Layers Enc-AVG Enc-FNN Enc-SA (nhop =4) Enc-SA (nhop =6) Dec-AVG Dec-FNN Dec-SA (nhop =4) Dec-SA (nhop =6) Both-FNN Both-SA (nhop = 4) Both-FNN-SA (nhop =4) #Param. 10.97M 10.97M 12.82M 16.50M 10.97M 11.63M 11.90M 12.16M 10.97M 11.63M 11.90M 13.47M 12.29M 12.82M 12.55M Valid. 32.96 33.91 33.58 34.14 34.20 34.35 34.34 34.55 34.48 34.61 34.02 34.38 34.83 34.73 34.30 34.59 34.55 Test 20.73 26.36 28.53 28.96 29.16 31.74 32.85 31.75 32.67 32.57 32.97 32.71 33.31 33.06 33.40 32.80 32.93 33.29 33.5"
C18-1255,P17-1012,0,0.245091,"rong Transformer baseline on IWSLT German-English and NIST Chinese-English MT tasks respectively. The result is new state-ofthe-art in German-English translation. 1 Introduction Neural models that use the encoder-decoder architecture to capture the translation equivalence relation between languages have been widely adopted over the last few years. The simplest of these relies on one recurrent neural network layer on both the encoder and decoder sides (Bahdanau et al., 2015), whereas others have successfully explored the high-level representation of language via deeper models (Wu et al., 2016; Gehring et al., 2017b; Vaswani et al., 2017). It has been noted that increasing the network depth is one of the factors contributing to the success of neural machine translation (NMT). To this end, one can stack a number of layers for an enriched sentence representation. E.g., popular NMT systems require 4 stacked layers or more for state-of-the-art results on large-scale translation tasks (Wu et al., 2016). Unfortunately, a straightforward implementation of deep neural networks has been found to underperform shallow models due to the poor convergence. One solution to this problem is to add identity mapping (or s"
C18-1255,P15-1002,0,0.0958461,"Missing"
C18-1255,P16-1162,0,0.10073,"method. Therefore, we choose the version described in Eqs. (6-7) for the empirical study. Table 1 shows a comparison of the methods used in this work. For good convergence, we use layer normalization after the fusion layer in this work. 4 Experiments We evaluate our proposed approach on German-English and Chinese-English translation tasks. 4.1 Setup For German-English translation, we use the data of the IWSLT 2014 German-English track (Cettolo et al., 2014). We follow Ranzato et al. (2015)’s work for preprocessing. We use a joint source and target byte-pair encoding with 10k merge operations (Sennrich et al., 2016). The source and target vocabulary sizes are 8,389 and 6,428 respectively. We remove the sentences with more than 175 words or 100 subword units. This results in 160K sentence pairs for training. We randomly sample 7K sentences from the training data for held-out validation, and concatenate dev2010, dev2012, tst2010, tst2011, and tst2012 for test. For Chinese-English translation, we use parts of the bitext provided within NIST12 OpenMT5 . We choose NIST 2006 (MT06) as the validation set, and 2004 (MT04), 2005 (MT05), 2008 (MT08) as the 4 5 W1 can be unique for each layer. We discuss this issue"
C18-1255,E17-3017,0,0.0607178,"Missing"
C18-1255,P17-1013,0,0.0272153,"with multiple stacked layers is challenging. It has been observed that introducing direct connections between layers can drastically improve the performance of deep neural models. Methods include highway networks (Srivastava et al., 2015), residual connections (He et al., 2016a), dense connections (Huang et al., 2017a), and fast-forward connections (Zhou et al., 2016). E.g., in machine translation, residual networks have been a popular way to address the issue due to its simplicity (Wu et al., 2016; Gehring et al., 2017a; Gehring et al., 2017b; Vaswani et al., 2017). Another related study is Wang et al. (2017). They introduce linear associative units to reduce the length of gradient propagation in recurrent neural networks (RNNs), and demonstrate promising improvements on their RNN-based NMT systems. But previous studies all focus on using the top-level sentence representation for prediction, and ignore the access to the representations encoded in lower-level layers. The next obvious step is toward models that make full use of all stacked layers for prediction (call it representation fusion). Some research groups have been aware of this and explored solutions. E.g., Gehring et al. (2017b) find that"
C18-1255,D16-1137,0,0.0537151,"Missing"
C18-1255,P12-3004,1,0.885328,"Missing"
C18-1255,Q16-1027,0,0.0219878,"e word embedding layer has a larger weight. This is reasonable because most of these words have specific meanings and do not need high-level representations for modeling large context in disambiguation. 3023 5 Related Work Training neural networks with multiple stacked layers is challenging. It has been observed that introducing direct connections between layers can drastically improve the performance of deep neural models. Methods include highway networks (Srivastava et al., 2015), residual connections (He et al., 2016a), dense connections (Huang et al., 2017a), and fast-forward connections (Zhou et al., 2016). E.g., in machine translation, residual networks have been a popular way to address the issue due to its simplicity (Wu et al., 2016; Gehring et al., 2017a; Gehring et al., 2017b; Vaswani et al., 2017). Another related study is Wang et al. (2017). They introduce linear associative units to reduce the length of gradient propagation in recurrent neural networks (RNNs), and demonstrate promising improvements on their RNN-based NMT systems. But previous studies all focus on using the top-level sentence representation for prediction, and ignore the access to the representations encoded in lower-le"
D07-1082,H92-1022,0,0.0164247,"rom the resulting classifier (Lewis and Catlett, 1994). We utilize a maximum entropy (ME) model (Berger et al., 1996) to design the basic classifier used in active learning for WSD. The advantage of the ME model is the ability to freely incorporate features from diverse sources into a single, wellgrounded statistical model. A publicly available ME toolkit (Zhang et. al., 2004) was used in our experiments. In order to extract the linguistic features necessary for the ME model, all sentences containing the target word were automatically part786 of-speech (POS) tagged using the Brill POS tagger (Brill, 1992). Three knowledge sources were used to capture contextual information: unordered single words in topical context, POS of neighboring words with position information, and local collocations. These are same as three of the four knowledge sources used in (Lee and Ng, 2002). Their fourth knowledge source (named syntactic relations) was not used in our work. 5 Stopping Conditions In active learning algorithm, defining the stopping condition for active learning is a critical problem, because it is almost impossible for the human annotator to label all unlabeled samples. This is a problem of estimati"
D07-1082,J98-1001,0,0.0132267,"pling, we propose a bootstrap-based oversampling (BootOS) method that works better than ordinary over-sampling in active learning for WSD. Finally, we investigate when to stop active learning, and adopt two strategies, max-confidence and min-error, as stopping conditions for active learning. According to experimental results, we suggest a prediction solution by considering max-confidence as the upper bound and min-error as the lower bound for stopping conditions. 1 Introduction Word sense ambiguity is a major obstacle to accurate information extraction, summarization, and machine translation (Ide and Veronis, 1998). In recent years, a variety of techniques for machine learning algorithms have demonstrated remarkable performance for automated word sense disambiguation (WSD) (Chan and Ng, 2006; Dagan et. al., 2006; Xue et. al., 2006; Kohomban and Lee. 2005; Dang and Palmer, 2005), when enough labeled training data is available. However, creating Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu a large sense-tagged corpus is very expensive and time-consuming, because these data have to be annotated by human experts. A"
D07-1082,J93-2004,0,0.0308527,"Missing"
D07-1082,P06-1057,0,0.0172745,"Missing"
D07-1082,S01-1020,0,\N,Missing
D07-1082,N06-1016,0,\N,Missing
D07-1082,W02-1006,0,\N,Missing
D07-1082,N06-2015,1,\N,Missing
D07-1082,P06-1012,0,\N,Missing
D07-1082,P05-1005,0,\N,Missing
D07-1082,P04-1036,0,\N,Missing
D07-1082,P06-2118,0,\N,Missing
D07-1082,P05-1006,0,\N,Missing
D09-1038,W98-1115,0,0.0439035,"or the SCFG learnt automatically from the training corpus. It can work with an efficient CKY-style binarizer to search for the lowest-cost binarization. We apply our method into a state-of-the-art string-to-tree SMT system. The experimental results show that our method outperforms the synchronous binarization method (Zhang et al., 2006) with over 0.8 BLEU scores on both NIST 2005 and NIST 2008 Chinese-to-English evaluation data sets. 2 Related Work The problem of binarization originates from the parsing problem in which several binarization methods are studied such as left/right binarization (Charniak et al., 1998; Tsuruoka and Tsujii, 2004) and head binarization (Charniak et al., 2006). Generally, the pruning issue in SMT decoding is unnecessary for the parsing problem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tr"
D09-1038,P05-1033,0,0.0773653,"owever, as shown by Chiang (2007), SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtai"
D09-1038,J07-2003,0,0.862926,"method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 1 Microsoft Research Asia Sigma Center Beijing, China, 100080 muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com binarize an SCFG in such a way that both the source-side and target-side virtual non-terminals have contiguous spans. This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system. However, as shown by Chiang (2007), SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004"
D09-1038,P06-1121,0,0.465965,"SCFGbased decoding with an integrated n-gram language model still has a time complexity of ?(?3 ? 4(?−1) ), where m is the source sentence length, and ? is the vocabulary size of the language model. Although it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtain a set of equivalent binary rules using th"
D09-1038,W07-0405,0,0.0851011,"oblem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2006) for syntaxbased SMT systems. In addition, it cannot be easily integrated into the decoding of some syntax-based models (Galley et al., 2004; Marcu et al., 2006), because it does not guarantee contiguous spans on the source language side. the ? ?ℎ binary rule in ℬ ?? . Figure 3 illustrates the meanings of these notations with a sample grammar. G R1 : VP → VB NP 会 JJR , VB NP"
D09-1038,W04-3250,0,0.186026,"Missing"
D09-1038,W06-1606,0,0.331083,"2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2006) for syntaxbased SMT systems. In addition, it cannot be easily integrated into the decoding of some syntax-based models (Galley et al., 2004; Marcu et al., 2006), because it does not guarantee contiguous spans on the source language side. the ? ?ℎ binary rule in ℬ ?? . Figure 3 illustrates the meanings of these notations with a sample grammar. G R1 : VP → VB NP 会 JJR , VB NP will be JJR R2 : S → NP 会 VP , NP will VP binarization G’ (R1) v11 : VP → V12 JJR , V12 JJR v12 : V12 → VB V13 , VB V13 v13 : V13 → NP 会 , NP will be (R2) v21 : S → V22 VP , v22 : V22 → NP 会 , rule bucket v11 3 Synchronous Binarization Optimization by Cost Reduction As discussed in Section 1, binarizing an SCFG in a fixed (left-heavy) way (Zhang et al., 2006) may lead to a large n"
D09-1038,H05-1101,0,0.0456339,"h it is not exponential in theory, the actual complexity can still be very high in practice. Here is an example extracted from real data. Given the following SCFG rule: VP → VB NP 会 JJR , VB NP will be JJR Introduction Recently Statistical Machine Translation (SMT) systems based on Synchronous Context Free Grammar (SCFG) have been extensively investigated (Chiang, 2005; Galley et al., 2004; Galley et al., 2006) and have achieved state-of-the-art performance. In these systems, machine translation decoding is cast as a synchronous parsing task. Because general SCFG parsing is an NPhard problem (Satta and Peserico, 2005), practical SMT decoders based on SCFG parsing requires an equivalent binary SCFG that is directly learned from training data to achieve polynomial time complexity using the CKY algorithm (Kasami, 1965; Younger, 1967) borrowed from CFG parsing techniques. Zhang et al. (2006) proposed synchronous binarization, a principled method to we can obtain a set of equivalent binary rules using the synchronous binarization method (Zhang et al., 2006) as follows: VP → V1 JJR , V1 → VB V2 , V2 → NP 会 , V1 JJR VB V2 NP will be This binarization is shown with the solid lines as binarization (a) in Figure 1."
D09-1038,D08-1018,0,0.0770688,"NIST 2008 Chinese-to-English evaluation data sets. 2 Related Work The problem of binarization originates from the parsing problem in which several binarization methods are studied such as left/right binarization (Charniak et al., 1998; Tsuruoka and Tsujii, 2004) and head binarization (Charniak et al., 2006). Generally, the pruning issue in SMT decoding is unnecessary for the parsing problem, and the accuracy of parsing does not rely on the binarization method heavily. Thus, many efforts on the binarization in parsing are made for the efficiency improvement instead of the accuracy improvement (Song et al., 2008). Binarization is also an important topic in the research of syntax-based SMT. A synchronous 363 binarization method is proposed in (Zhang et al., 2006) whose basic idea is to build a left-heavy binary synchronous tree (Shapiro and Stephens, 1991) with a left-to-right shift-reduce algorithm. Target-side binarization is another binarization method which is proposed by Huang (2007). It works in a left-to-right way on the target language side. Although this method is comparatively easy to be implemented, it just achieves the same performance as the synchronous binarization method (Zhang et al., 2"
D09-1038,D07-1078,0,0.038172,"r development data set comes from NIST2003 evaluation data in which the sentences of more than 20 words are excluded to speed up the Minimum Error Rate Training (MERT). The test data sets are the NIST evaluation sets of 2005 and 2008. Our string-to-tree SMT system is built based on the work of (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the training corpus, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Before the rule extraction, we also binarize the parse trees on the English side using Wang et al. (2007) „s method to increase the coverage of GHKM and SPMT rules. There are totally 4.26M rules after the low frequency rules are filtered out. The pruning strategy is similar to the cube pruning described in (Chiang, 2007). To achieve acceptable translation speed, the beam size is set to 50 by default. The baseline system is based on the synchronous binarization (Zhang et al., 2006). 4.2 Binarization Schemes Besides the baseline (Zhang et al., 2006) and iterative cost reduction binarization methods, we also perform right-heavy and random synchronous binarizations for comparison. In this paper, the"
D09-1038,N06-1033,0,0.250451,"ing polynomial time complexity of decoding for SCFG parsing based machine translation systems. In this paper, we first investigate the excess edge competition issue caused by a leftheavy binary SCFG derived with the method of Zhang et al. (2006). Then we propose a new binarization method to mitigate the problem by exploring other alternative equivalent binary SCFGs. We present an algorithm that iteratively improves the resulting binary SCFG, and empirically show that our method can improve a string-to-tree statistical machine translations system based on the synchronous binarization method in Zhang et al. (2006) on the NIST machine translation evaluation tasks. 1 Microsoft Research Asia Sigma Center Beijing, China, 100080 muli@microsoft.com dozhang@microsoft.com mingzhou@microsoft.com binarize an SCFG in such a way that both the source-side and target-side virtual non-terminals have contiguous spans. This property of synchronous binarization guarantees the polynomial time complexity of SCFG parsers even when an n-gram language model is integrated, which has been proved to be one of the keys to the success of a string-to-tree syntax-based SMT system. However, as shown by Chiang (2007), SCFGbased decod"
D09-1038,N04-1035,0,\N,Missing
D09-1038,N06-1022,0,\N,Missing
D14-1021,W09-0437,0,0.0218632,"2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word usage, rich morphology and paraphrasing all add to the difficulty of rule extraction. In addition, restricting target word orders by hard translation rules can also hurt output fluency. ∗ * Work done while visiting Singapore University of Technology and Design (SUTD) 177 Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 177–182, c October 25-29, 2014, Doha, Qatar. 2014 Association for Computational Linguistics below λ · Pmax are filtered out, where Pmax is the probability of t"
D14-1021,P07-1020,0,0.0211782,"algorithm, and one potential of the SMT architecture is that it can directly benefit from advances in statistical NLG technology. As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis comp"
D14-1021,P08-1024,0,0.0633776,"Missing"
D14-1021,J93-2003,0,0.0506297,"minimizing engineering efforts. Shown in Figure 1, the end-to-end system consists of two main components: lexical transfer and synthesis. The former provides candidate translations for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the"
D14-1021,P05-1033,0,0.0949535,"et equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli"
D14-1021,E14-1028,0,0.178381,"Missing"
D14-1021,W11-2107,0,0.0896313,"Missing"
D14-1021,C12-1121,0,0.0359306,"Missing"
D14-1021,W99-0604,0,0.134086,"r provides candidate translations for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the target side of a valid phrase pair contains over s words. Given an input source sentence, the lexical transfer unit finds all valid target transl"
D14-1021,P05-1034,0,0.0509849,"thesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translat"
D14-1021,N13-1025,0,0.0341822,"Missing"
D14-1021,D08-1052,0,0.0180782,"ependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word"
D14-1021,W02-1039,0,0.0634627,"s for (overlapping) source words and phrases. Although lexicons and rules can be used for this step, we take a simple statistical alignment-based approach. The latter searches for a target translation by constructing dependency trees bottom-up. The process can be viewed as a syntax-based generation process from a bag of overlapping translation options. 2.1 Lexical transfer We perform word alignment using IBM model 4 (Brown et al., 1993), and then extract phrase pairs according to the alignment and automaticallyannotated target syntax. In particular, consistent (Och et al., 1999) and cohesive (Fox, 2002) phrase pairs are extracted from intersected alignments in both directions: the target side must form a projective span, with a single root, and the source side must be contiguous. A resulting phrase pair consists of the source phrase, its target translation, as well as the head position and head part-of-speech (POS) of the target span, which are useful for target synthesis. We further restrict that neither the source nor the target side of a valid phrase pair contains over s words. Given an input source sentence, the lexical transfer unit finds all valid target translation options for overlap"
D14-1021,N04-1035,0,0.0948608,"phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage p"
D14-1021,P06-1139,0,0.0331291,"urrently rather separated research fields. The system is not strongly dependent on the specific generation algorithm, and one potential of the SMT architecture is that it can directly benefit from advances in statistical NLG technology. As discussed in the introduction, our work is closely related to previous studies on syntactic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reorderin"
D14-1021,P12-2061,0,0.0155088,"actic MT, with the salient difference that we do not rely on hard translation rules, but allow free target synthesis. The contrast can be summarized as “translation by parsing” vs “translation by generation”. There has been a line of research on generation for translation. Soricut and Marcu (2006) use a form of weighted IDL-expressions (Nederhof and Satta, 2004) for generation. Bangalore et al. (2007) treats MT as a combination of global lexical transfer and word ordering; their generation component does not perform lexical selection, relying on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming th"
D14-1021,P09-1091,0,0.221722,"ng on an n-gram language model to order target words. Goto et al. (2012) use a monotonic phrasebased system to perform target word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et a"
D14-1021,E09-1097,0,0.299711,"et word selection, and treats target ordering as a post-processing step. More recently, Chen et al. (2014) translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SM"
D14-1021,D09-1043,0,0.0746518,"U comparable to the state-of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis s"
D14-1021,J99-4005,0,0.137947,"ined consistently in a discriminative model. Experiments using the IWSLT 2010 dataset show that the system achieves BLEU comparable to the state-of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing t"
D14-1021,J10-4005,0,0.0465794,"Missing"
D14-1021,P12-3004,1,0.847822,"Chinese-English dataset, which consists of training sentence pairs from the dialog task (dialog) and Basic Travel and Expression Corpus (BTEC). The union of dialog and BTEC are taken as our training set, which contains 30,033 sentence pairs. For system tuning, we use the IWSLT 2004 test set (also released as the second development test set of IWSLT 2010), which contains 500 sentences. For final test, we use the IWSLT 2003 test set (also released as the first development test set of IWSLT 2010), which contains 506 sentences. The Chinese sentences in the datasets are segmented using NiuTrans3 (Xiao et al., 2012), while POS-tagging of both English and Chinese is performed using ZPar4 version 0.5 (Zhang and Clark, 2011). We train the English POS-tagger using the WSJ sections of the Penn Treebank (Marcus et al., 1993), turned into lower-case. For syntactic parsing of both English and Chinese, we use the default models of ZPar 0.5. We choose three baseline systems: a string-totree (S2T) system, a tree-to-string (T2S) system and a tree-to-tree (T2T) system (Koehn, 2010). The Moses release 1.0 implementations of all three systems are used, with default parameter settings. IRSTLM5 release 5.80.03 (Federico"
D14-1021,D11-1020,1,0.858858,"both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibility of function word usage, rich morphol"
D14-1021,P06-1096,0,0.0607434,"Missing"
D14-1021,P06-1077,1,0.766363,"s, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), whi"
D14-1021,W06-1606,0,0.034268,"output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), which performs dependency-based linearization. The linearization task for MT is different from the monolingual task in that not all translation options are used to build the output, and that bilingual correspondences need to be taken into account dur1 Introduction Translation rules have been central to hierarchical phrase-based and syntactic statistical machine translation (SMT) (Galley et al., 2004; Chiang, 2005; Liu et al., 2006; Quirk et al., 2005; Marcu et al., 2006; Shen and Joshi, 2008; Xie et al., 2011). They are attractive by capturing the recursiveness of languages and syntactic correspondences between them. One important advantage of translation rules is that they allow efficient decoding by treating MT as a statistical parsing task, transforming a source sentence to its translation via recursive rule application. The efficiency takes root in the fact that target word orders are encoded in translation rules. This fact, however, also leads to rule explosion, noise and coverage problems (Auli et al., 2009), which can hurt translation quality. Flexibi"
D14-1021,J93-2004,0,0.0459525,"Missing"
D14-1021,D13-1112,0,0.0442095,"Missing"
D14-1021,D11-1106,1,0.885569,"of-the-art syntactic SMT systems. Figure 1: Overall system architecture. A potential solution to the problems above is to treat translation as a generation task, representing syntactic correspondences using soft features. Both adequacy and fluency can potentially be improved by giving full flexibility to target synthesis, and leaving all options to the statistical model. The main challenge to this method is a significant increase in the search space (Knight, 1999). To this end, recent advances in tackling complex search tasks for text generation offer some solutions (White and Rajkumar, 2009; Zhang and Clark, 2011). In this short paper, we present a preliminary investigation on the possibility of building a syntactic SMT system that does not use hard translation rules, by utilizing recent advances in statistical natural language generation (NLG). The overall architecture is shown in Figure 1. Translation is performed by first parsing the source sentence, then transferring source words and phrases to their target equivalences, and finally synthesizing the target output. We choose dependency grammar for both the source and the target syntax, and adapt the syntactic text synthesis system of Zhang (2013), w"
D14-1021,E12-1075,1,0.701608,") translate source dependencies arc-by-arc to generate pseudo target dependencies, and generate the translation by reordering of arcs. In contrast with these systems, our system relies more heavily on a syntax-based synthesis component, in order to study the usefulness of statistical NLG on SMT. With respect to syntax-based word ordering, Chang and Toutanova (2007) and He et al. (2009) study a simplified word ordering problem by assuming that the un-ordered target dependency tree is given. Wan et al. (2009) and Zhang and Clark (2011) study the ordering of a bag of words, without input syntax. Zhang et al. (2012), Zhang (2013) and Song et al. (2014) further extended this line of research by adding input syntax and allowing joint inflection and ordering. de Gispert et al. (2014) use a phrase-structure grammer for word ordering. Our generation system is based on the work of Zhang (2013), but further allows lexical selection. Our work is also in line with the work of Liang et al. (2006), Blunsom et al. (2008), Flanigan et al. (2013) and Yu et al. (2013) in that we build a discriminative model for SMT. Acknowledgement The work has been supported by the Singapore Ministration of Education Tier 2 project T2"
D14-1021,P07-1002,0,\N,Missing
D14-1021,C14-1104,1,\N,Missing
D17-1150,P17-1175,0,0.0355135,"Missing"
D17-1150,W14-4012,0,0.241662,"Missing"
D17-1150,D14-1179,0,0.0632807,"Missing"
D17-1150,P16-1078,0,0.067992,"ence of tokens. The most fundamental approaches transform the source sentence sequentially into a fixed-length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b). Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Erig"
D17-1150,D13-1176,0,0.458623,"lexical and phrase vectors. Using a tree-based rare word encoding, the proposed model is extended to sub-word level to alleviate the out-of-vocabulary (OOV) problem. Empirical results reveal that the proposed model significantly outperforms sequence-to-sequence attention-based and tree-based neural translation models in English-Chinese translation tasks. 1 PRP1 VP3 VBP5 I Neural machine translation (NMT) automatically learns the abstract features of and semantic relationship between the source and target sentences, and has recently given state-of-the-art results for various translation tasks (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). The most widely used model is the encoder-decoder framework (Sutskever et al., 2014), in which the source sentence is encoded into a dense representation, followed by a decoding process which generates the target translation. By exploiting the attention mechanism (Bahdanau et al., 2015), the generation of target words is conditional on the source hidden states, rather than on the context vector alone. From a model architecture perspective, prior studies of the attentive Corresponding author take NP4 PRT6 up NP7 PP8 DT9 NN10 IN11 a position in N"
D17-1150,E17-2093,0,0.113445,"Missing"
D17-1150,D15-1166,0,0.384069,"GRU. The i-th leaf node vector is calculated as: l hli = fGRU (xi , hli−1 ), (1) where xi is the i-th source word embedding and hli−1 denotes the previous hidden state. The parent hidden state h↑i,j summarizes its left child h↑i,k and a non-linear function fsof tmax : p(yj |y1 , ..., yj−1 , x, tr; θ) = fsof tmax (cj ), where cj is the composite hidden state, which consists of a target hidden state sj and a context vector dj : cj = ftanh ([sj , dj ]). Given the previous target word yj−1 , the concatenation of the previous hidden state sj−1 and the previous context vector cj−1 (input-feeding) (Luong et al., 2015), sj , is calculated using a standard sequential GRU network: dec sj = fgru (yj−1 , [sj−1 , cj−1 ]). The context vector dj is computed using an attention model which is used to softly summarize the attended part of the source-side representations. Eriguchi et al. (2016) adopted a tree-based attention mechanism to consider both the word and phrase vectors: right child h↑k+1,j (i &lt; k &lt; j) by applying the tree-GRU (Zhou et al., 2016) as follows: ↑ zi,j = ↑ = ri,k ↑ = rk+1,j e h↑i,j = h↑i,j = R ↑ U(z) hk+1,j i=1 αj (t) = et = + N −1 X k=1 αj (k)hpk , (2) PN exp(et ) PN −1 l i=1 exp(ei ) + (Va )T t"
D17-1150,P02-1040,0,0.0997821,"ectively set as 620 and 1,000. Due to the concatenation in the bidirectional leaf-node encoding, the dimensions of the forward and backward vectors, which are half of those of the other hidden states, are set to 500. In order to prevent over-fitting, the training data is shuffled following each epoch. Moreover, the model parameters are optimized using AdaDelta (Zeiler, 2012), due to its capability for dynamically adapting the learning rate. We set the mini-batch size to 16 and the beam search size to 5. The accuracy of the translation relative to a reference is assessed using the BLEU metric (Papineni et al., 2002). In order to give an equitable comparison, all the NMT models used for comparison are implemented or re-implemented using GRU in our code, based on dl4mt4 . 4.3 Enhanced Hierarchical Representations Firstly, the effectiveness of the enhanced hierarchical representations is evaluated through a set of experiments, the results of which are summarized in Table 3. Compared with the original tree-based encoder (Eriguchi et al., 2016), the model with bidirectional leaf-node encoding (described in Section 3.1) shows better performance. This also reveals that the future context at leaf level can contr"
D17-1150,P16-2049,0,0.0377221,"annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b). Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotati"
D17-1150,P15-1150,0,0.247232,"Missing"
D17-1150,P12-3004,1,0.83203,"ed 1.4M sentence pairs, in which the maximum length of the sentence was 40, from the LDC parallel corpus3 as our training data. The models were developed using NIST mt08 data and were examined using NIST mt04, mt05, and mt06 data. The number of sentences in each dataset is shown in Table 1. On the English side, we used the constituent parser (Zeng et al., 2014, 2015) to produce a binary syntactic tree for each sentence, in constrast to the use of the HPSG parser by Eriguchi et al. (2016). On the Chinese side, the sentences are segmented using the Chinese word segmentation toolkit of NiuTrans (Xiao et al., 2012). To avoid data sparsity, words referring to time, date and number, which are low in frequency, are generalized as ‘$time’, ‘$date’ and ‘$number’. In addition, as described in Section 3.3, the vocabularies are further compressed by segmenting the rare words into sub-word units using BPE. 4.2 Experimental Settings As shown in Table 2, which gives the statistics of the token types, we limit the source and target vo3 Our training data was selected from LDC2000T46, LDC2000T50, LDC2003E14, LDC2004T08, LDC2004T08 and LDC2005T10. Training Set |V |in En |V |in Zh Original 159k 198k Generalization 120k"
D17-1150,C16-1274,0,0.0951232,". By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up fashion, whereby the annotation vector of each phrase is derived from its constituent sub-phrases. As a result, the learned representations are limited to local information, while failing to capture the global meaning of a sentence. As illustrated in Figure 1, the phrases “take up”1 and “a position”2 have different meanings in different contexts. However, in composing the representations hVP3 and hNP7 for phrases VP3 and NP7 , the current approaches do not account for the differences in meaning which arise as a result of ig"
D17-1150,W16-2209,0,0.0354317,"length context vector, and the annotation vector of each word summarizes the preceding words (Sutskever et al., 2014; Cho et al., 2014b). Although Bahdanau et al. (2015) used a bidirectional recurrent neural network (RNN) (Schuster and Paliwal, 1997) to consider preceding and following words jointly, these sequential representations are insufficient to fully capture the semantics of a sentence, due to the fact that they do not account for the syntactic interpretations of sentence structure (Eriguchi et al., 2016; Tai et al., 2015). By incorporating additional features into a sequential model, Sennrich and Haddow (2016) and Stahlberg et al. (2016) suggest that a greater amount of linguistic information can improve the translation performance. The tree-to-sequence model encodes a source sentence according to a given syntactic tree 1432 Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 1432–1441 c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics over the sentence. The existing tree-based encoders (Tai et al., 2015; Eriguchi et al., 2016; Zhou et al., 2016) recursively generate phrase (sentence) representations in a bottom-up f"
D17-1150,P16-1162,0,0.147293,"formation between the word and phrase vectors. To alleviate the out-ofvocabulary (OOV) problem, we further extend the proposed tree-based model to the sub-word level 1 Take up has the meanings of start doing something new, use space/time, accept an offer, etc. 2 Position has the meanings of location, job offer, rank/status, etc. hp1,8 hp2,8 hp2,3 hp4,8 hp6,8 hp4,5 hp7,8 hl1 hl2 hl3 hl4 hl5 I e tak up a i pos hl6 n tio in hl7 hl8 hl9 m i the roo heos Figure 2: The tree-based model of Eriguchi et al. (2016) comprising a structured and sequential encoder. by integrating byte-pair encoding (BPE) (Sennrich et al., 2016) into the tree-based model (as described in Section 3.3). Experimental results for the NIST English-to-Chinese translation task reveal that the proposed model significantly outperforms the vanilla tree-based (Eriguchi et al., 2016) and sequential NMT models (Bahdanau et al., 2015) (Section 4.1). 2 Tree-Based Neural Machine Translation A neural machine translation system (NMT) aims to use a single neural network to build a translation model, which is trained to maximize the conditional distribution of sentence pairs using a parallel training corpus (Kalchbrenner and Blunsom, 2013; Sutskever et"
D19-1367,N19-1078,0,0.39835,"dation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and output of the RNN layer. We train the network using SGD with a learning rate of 0.1 and a gradient clipping threshold of 5.0. We reduce the learning rate by a factor of 0.25 if the test error does not decrease for 2 epochs. Table 2 shows a comparison of different methods. Our baseline uses RNN cells generated from random initialized whose F1-score varies greatly and is lower than that of"
D19-1367,C18-1139,0,0.163871,"tion perplexities over 4 different runs at different search epochs. We see that I-DARTS is easier to converge than DARTS (4 hours). It is 1.4X faster than that of DARTS. Another interesting finding is that I-DARTS achieves a lower validation perplexity than DARTS during architecture search. This may indicate better architectures found by I-DARTS because the search model is optimized with respect to validation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout"
D19-1367,N16-1030,0,0.483792,"TS during architecture search. This may indicate better architectures found by I-DARTS because the search model is optimized with respect to validation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and output of the RNN layer. We train the network using SGD with a learning rate of 0.1 and a gradient clipping threshold of 5.0. We reduce the learning rate by a factor of 0.25 if the test error does not decrease for 2 epochs. Table 2 shows a compa"
D19-1367,P16-1101,0,0.0541327,"e search. This may indicate better architectures found by I-DARTS because the search model is optimized with respect to validation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and output of the RNN layer. We train the network using SGD with a learning rate of 0.1 and a gradient clipping threshold of 5.0. We reduce the learning rate by a factor of 0.25 if the test error does not decrease for 2 epochs. Table 2 shows a comparison of different m"
D19-1367,D14-1162,0,0.0787964,"because the search model is optimized with respect to validation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and output of the RNN layer. We train the network using SGD with a learning rate of 0.1 and a gradient clipping threshold of 5.0. We reduce the learning rate by a factor of 0.25 if the test error does not decrease for 2 epochs. Table 2 shows a comparison of different methods. Our baseline uses RNN cells generated from random initialized w"
D19-1367,P17-1161,0,0.0154193,"er 4 different runs at different search epochs. We see that I-DARTS is easier to converge than DARTS (4 hours). It is 1.4X faster than that of DARTS. Another interesting finding is that I-DARTS achieves a lower validation perplexity than DARTS during architecture search. This may indicate better architectures found by I-DARTS because the search model is optimized with respect to validation perplexity. Then, we test the learned architecture in a named entity recognition system on the English data from CoNLL-2003 shared task (Sang and Meulder, 2003). Following previous work (Akbik et al., 2018; Peters et al., 2017), we report the averaged F1 score over 5 runs on the test set. For modeling, we choose the single-layer RNN-CRF 3587 Valid Perplexity model because it achieved state-of-the-art results on several sequence labeling tasks (Lample et al., 2016; Ma and Hovy, 2016). We use GloVe 100dimensional word embeddings (Pennington et al., 2014) and pooled contextual embeddings (Akbik et al., 2019) as pre-trained word embeddings. We replace the standard bidirectional LSTMs with the discovered recurrent neural cells. Also, we set the hidden layer size to 512 and apply variational dropout to the input and outpu"
D19-1367,N18-1202,0,0.0612954,"Missing"
D19-1367,W03-0419,0,0.113383,"Missing"
I05-1026,P94-1002,0,0.544454,"to divide a document into topically-coherent sections, each corresponding to a relevant subject. Linear text segmentation has been applied in document summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, th"
I05-1026,J97-1003,0,0.784031,"to divide a document into topically-coherent sections, each corresponding to a relevant subject. Linear text segmentation has been applied in document summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, th"
I05-1026,J91-1002,0,0.149373,"to divide a document into topically-coherent sections, each corresponding to a relevant subject. Linear text segmentation has been applied in document summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, th"
I05-1026,P93-1041,0,0.0605184,"to divide a document into topically-coherent sections, each corresponding to a relevant subject. Linear text segmentation has been applied in document summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, th"
I05-1026,P94-1050,0,0.0359194,"ument summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. Some efforts have"
I05-1026,W97-0304,0,0.375905,"ument summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. Some efforts have"
I05-1026,P93-1020,0,0.0555297,"to divide a document into topically-coherent sections, each corresponding to a relevant subject. Linear text segmentation has been applied in document summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, th"
I05-1026,P99-1046,0,0.0207538,"ument summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. Some efforts have"
I05-1026,A00-2004,0,0.0285431,"ument summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. Some efforts have"
I05-1026,W01-0514,0,0.0336087,"ument summarization, information retrieval, and text understanding. For example, in recent years, passage-retrieval techniques based on linear text segmentation, are becoming increasingly popular in information retrieval as relevant text passages often provide better answers than complete document texts in response to user queries[1]. In recent years, many techniques have been applied to linear text segmentation. Some have used linguistic information[2,3,4,5,6,9] such as cue phrases, punctuation marks, prosodic features, reference, and new words occurrence. Others have used statistical methods[7,8,10,11,12,13,14,15] such as those based on word cooccurrence, lexical cohesion relations, semantic network, similarity between adjacent parts of texts, similarity between all parts of a text, dynamic programming algorithm, and HMM model. R. Dale et al. (Eds.): IJCNLP 2005, LNAI 3651, pp. 292 – 301, 2005. © Springer-Verlag Berlin Heidelberg 2005 Using Multiple Discriminant Analysis Approach for Linear Text Segmentation 293 In linear text segmentation study, there are two critical problems involving automatic boundary detection and automatic determination of the number of segments in a document. Some efforts have"
I05-1026,P98-2244,0,0.026931,"efforts have focused on using similarity between adjacent parts of a text to solve topic boundary detection. In fact, the similarity threshold is very hard to set, and it is very difficult to identify exactly topic boundaries only according to similarity between adjacent parts of a text. Other works have focused on the similarity between all parts of a text. Reynar[7] and Choi[13] used dotplots technique to perform linear text segmentation which can be seen as a form of approximate and local optimization. Yaari[16] has used agglomerative clustering to perform hierarchical segmentation. Others[10,17,18,19] used dynamic programming to perform exact and global optimization in which some prior parameters are needed. These parameters can be obtained via uninformative prior probabilities[18], or estimated from training data[19]. In this paper, we propose a new statistical model for linear text segmentation, which uses Multiple Discriminant Analysis (MDA) method to define a global criterion function for document segmentation. Our method focuses on within-segment word similarity and between-segment word similarity. This process can achieve global optimization in addressing the two aforementioned probl"
I05-1026,P01-1064,0,0.0335318,"efforts have focused on using similarity between adjacent parts of a text to solve topic boundary detection. In fact, the similarity threshold is very hard to set, and it is very difficult to identify exactly topic boundaries only according to similarity between adjacent parts of a text. Other works have focused on the similarity between all parts of a text. Reynar[7] and Choi[13] used dotplots technique to perform linear text segmentation which can be seen as a form of approximate and local optimization. Yaari[16] has used agglomerative clustering to perform hierarchical segmentation. Others[10,17,18,19] used dynamic programming to perform exact and global optimization in which some prior parameters are needed. These parameters can be obtained via uninformative prior probabilities[18], or estimated from training data[19]. In this paper, we propose a new statistical model for linear text segmentation, which uses Multiple Discriminant Analysis (MDA) method to define a global criterion function for document segmentation. Our method focuses on within-segment word similarity and between-segment word similarity. This process can achieve global optimization in addressing the two aforementioned probl"
I05-1026,E03-1058,0,0.0954399,"efforts have focused on using similarity between adjacent parts of a text to solve topic boundary detection. In fact, the similarity threshold is very hard to set, and it is very difficult to identify exactly topic boundaries only according to similarity between adjacent parts of a text. Other works have focused on the similarity between all parts of a text. Reynar[7] and Choi[13] used dotplots technique to perform linear text segmentation which can be seen as a form of approximate and local optimization. Yaari[16] has used agglomerative clustering to perform hierarchical segmentation. Others[10,17,18,19] used dynamic programming to perform exact and global optimization in which some prior parameters are needed. These parameters can be obtained via uninformative prior probabilities[18], or estimated from training data[19]. In this paper, we propose a new statistical model for linear text segmentation, which uses Multiple Discriminant Analysis (MDA) method to define a global criterion function for document segmentation. Our method focuses on within-segment word similarity and between-segment word similarity. This process can achieve global optimization in addressing the two aforementioned probl"
I05-1026,H92-1089,0,\N,Missing
I05-1026,C98-2239,0,\N,Missing
I05-3015,W98-0706,0,0.0953966,"Missing"
I08-1048,J96-1002,0,0.162952,"Missing"
I08-1048,H92-1022,0,0.0353329,"Missing"
I08-1048,N06-1016,0,0.378872,"., 2007), part-of-speech tagging (Engelson and Dagan, 1999), information extraction (Thompson et al., 1999), statistical parsing (Steedman et al., 2003), and word sense disambiguation (Zhu and Hovy, 2007). Previous studies reported that active learning can help in reducing human labeling effort. With selective sampling techniques such as uncertainty sampling (Lewis and Gale, 1994) and committeebased sampling (McCallum and Nigam, 1998), the size of the training data can be significantly reduced for text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998), word sense disambiguation (Chen, et al. 2006; Zhu and Hovy, 2007), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007) tasks. Interestingly, deciding when to stop active learning is an issue seldom mentioned issue in these studies. However, it is an important practical topic, since it obviously makes no sense to continue the active learning procedure until the whole corpus has been labeled. How to define an adequate stopping criterion remains an unsolved problem in active learning. In principle, this is a problem of estimation of classifier effectiveness (Lewis and Gale, 1994). However, in real-world applications, it"
I08-1048,N06-2015,1,0.805293,"Missing"
I08-1048,W02-1006,0,0.194715,"Missing"
I08-1048,J93-2004,0,0.0496142,"Missing"
I08-1048,P04-1075,0,0.641095,"mount of human labeling effort by building an system that automatically selects the most informative unlabeled example for human annotation at each annotation cycle. In recent years active learning has attracted a lot of research interest, and has been studied in many natural language processing (NLP) tasks, such as text classification (TC) Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu (Lewis and Gale, 1994; McCallum and Nigam, 1998), chunking (Ngai and Yarowsky, 2000), named entity recognition (NER) (Shen et al., 2004; Tomanek et al., 2007), part-of-speech tagging (Engelson and Dagan, 1999), information extraction (Thompson et al., 1999), statistical parsing (Steedman et al., 2003), and word sense disambiguation (Zhu and Hovy, 2007). Previous studies reported that active learning can help in reducing human labeling effort. With selective sampling techniques such as uncertainty sampling (Lewis and Gale, 1994) and committeebased sampling (McCallum and Nigam, 1998), the size of the training data can be significantly reduced for text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998), word sense d"
I08-1048,N03-1031,0,0.00982727,"le. In recent years active learning has attracted a lot of research interest, and has been studied in many natural language processing (NLP) tasks, such as text classification (TC) Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu (Lewis and Gale, 1994; McCallum and Nigam, 1998), chunking (Ngai and Yarowsky, 2000), named entity recognition (NER) (Shen et al., 2004; Tomanek et al., 2007), part-of-speech tagging (Engelson and Dagan, 1999), information extraction (Thompson et al., 1999), statistical parsing (Steedman et al., 2003), and word sense disambiguation (Zhu and Hovy, 2007). Previous studies reported that active learning can help in reducing human labeling effort. With selective sampling techniques such as uncertainty sampling (Lewis and Gale, 1994) and committeebased sampling (McCallum and Nigam, 1998), the size of the training data can be significantly reduced for text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998), word sense disambiguation (Chen, et al. 2006; Zhu and Hovy, 2007), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007) tasks. Interestingly, deciding when to st"
I08-1048,D07-1051,0,0.575681,"ling effort by building an system that automatically selects the most informative unlabeled example for human annotation at each annotation cycle. In recent years active learning has attracted a lot of research interest, and has been studied in many natural language processing (NLP) tasks, such as text classification (TC) Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu (Lewis and Gale, 1994; McCallum and Nigam, 1998), chunking (Ngai and Yarowsky, 2000), named entity recognition (NER) (Shen et al., 2004; Tomanek et al., 2007), part-of-speech tagging (Engelson and Dagan, 1999), information extraction (Thompson et al., 1999), statistical parsing (Steedman et al., 2003), and word sense disambiguation (Zhu and Hovy, 2007). Previous studies reported that active learning can help in reducing human labeling effort. With selective sampling techniques such as uncertainty sampling (Lewis and Gale, 1994) and committeebased sampling (McCallum and Nigam, 1998), the size of the training data can be significantly reduced for text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998), word sense disambiguation (Chen, et"
I08-1048,D07-1082,1,0.941925,"of research interest, and has been studied in many natural language processing (NLP) tasks, such as text classification (TC) Eduard Hovy University of Southern California Information Sciences Institute 4676 Admiralty Way Marina del Rey, CA 90292-6695 hovy@isi.edu (Lewis and Gale, 1994; McCallum and Nigam, 1998), chunking (Ngai and Yarowsky, 2000), named entity recognition (NER) (Shen et al., 2004; Tomanek et al., 2007), part-of-speech tagging (Engelson and Dagan, 1999), information extraction (Thompson et al., 1999), statistical parsing (Steedman et al., 2003), and word sense disambiguation (Zhu and Hovy, 2007). Previous studies reported that active learning can help in reducing human labeling effort. With selective sampling techniques such as uncertainty sampling (Lewis and Gale, 1994) and committeebased sampling (McCallum and Nigam, 1998), the size of the training data can be significantly reduced for text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998), word sense disambiguation (Chen, et al. 2006; Zhu and Hovy, 2007), and named entity recognition (Shen et al., 2004; Tomanek et al., 2007) tasks. Interestingly, deciding when to stop active learning is an issue seldom mentioned issu"
I08-1048,P00-1016,0,\N,Missing
I08-1048,I05-7009,1,\N,Missing
I08-2124,P06-1009,0,0.0334763,"to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004); and Word Sense Disambiguation (Chen et al., 2006). In this paper, we use CRF models to perform a more complex task on the primary TTE experimental results and adapt"
I08-2124,N06-1016,0,0.0126479,"ta extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004); and Word Sense Disambiguation (Chen et al., 2006). In this paper, we use CRF models to perform a more complex task on the primary TTE experimental results and adapt it to process new biomedical data. 3 In order to construct the minimum information required to interpret a TTE, we consider a set of specific components as shown in Table 1. Figure 1 gives an example of description of a complete TTE in a single sentence. In the research articles, this information is usually spread over many such sentences. 3.2 CRF Labeling We use a plain text sentence for input and attempt to label each token with a field label. In addition to the four pre-define"
I08-2124,D07-1088,1,0.765065,"arily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004); and Word Sense Disambiguation (Chen et al., 2006). In this paper, we use CRF models to perform a more complex task on the primary TTE experimental results and adapt it to process new biomedical data. 3 In order to"
I08-2124,P06-1027,0,0.0294613,"equired fields values (e.g. Ravichandran and Hovy, 2002; Mann and Yarowsky, 2005; Feng et al., 2006). However, this only works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004);"
I08-2124,P05-1060,0,0.0141489,"on their own interaction with the research literature (Stephan et al., 2001; Burns and Cheng, 2006). But this process of data entry and curation is manual. Current approaches on biomedical text mining (e.g., Srinivas et al., 2005; OKanohara et al., 2006) tend to address the tasks of named entity recognition or relation extraction, and our goal is more complex: to extract computational representations of the minimum information in a given experiment type. Pattern-based IE approaches employ seed data to learn useful patterns to pinpoint required fields values (e.g. Ravichandran and Hovy, 2002; Mann and Yarowsky, 2005; Feng et al., 2006). However, this only works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2"
I08-2124,P06-1059,0,0.0503132,"Missing"
I08-2124,N04-1042,0,0.0259424,"ly works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004); and Word Sense Disambiguation (Chen et al., 2006). In this paper, we use CRF models to perform a more complex task on the p"
I08-2124,P02-1006,1,0.651966,"of knowledge statements based on their own interaction with the research literature (Stephan et al., 2001; Burns and Cheng, 2006). But this process of data entry and curation is manual. Current approaches on biomedical text mining (e.g., Srinivas et al., 2005; OKanohara et al., 2006) tend to address the tasks of named entity recognition or relation extraction, and our goal is more complex: to extract computational representations of the minimum information in a given experiment type. Pattern-based IE approaches employ seed data to learn useful patterns to pinpoint required fields values (e.g. Ravichandran and Hovy, 2002; Mann and Yarowsky, 2005; Feng et al., 2006). However, this only works if the data corpus is rich enough to learn variant surface patterns and does not necessarily generalize to more complex situations, such as our domain problem. Within biomedical articles, sentences tend to be long and the prose structure tends to be more complex than newsprint. 871 The CRF model (Lafferty et al., 2001) provides a compact way to integrate different types of features for sequential labeling problems. Reported work includes improved model variants (e.g., Jiao et al., 2006) and applications such as web data ex"
I08-2124,P04-1075,0,0.0309927,"Missing"
I08-2124,P02-1016,0,0.0133651,"ved model variants (e.g., Jiao et al., 2006) and applications such as web data extraction (Pinto et al., 2003), scientific citation extraction (Peng and McCallum, 2004), word alignment (Blunsom and Cohn, 2006), and discourselevel chunking (Feng et al., 2007). Pool-based active learning was first successfully applied to language processing on text classification (Lewis and Gale, 1994; McCallum and Nigam, 1998; Tong and Koller, 2000). It was also gradually applied to NLP tasks, such as information extraction (Thompson et al., 1999); semantic parsing (Thompson et al., 1999); statistical parsing (Tang et al., 2002); NER (Shen et al., 2004); and Word Sense Disambiguation (Chen et al., 2006). In this paper, we use CRF models to perform a more complex task on the primary TTE experimental results and adapt it to process new biomedical data. 3 In order to construct the minimum information required to interpret a TTE, we consider a set of specific components as shown in Table 1. Figure 1 gives an example of description of a complete TTE in a single sentence. In the research articles, this information is usually spread over many such sentences. 3.2 CRF Labeling We use a plain text sentence for input and attemp"
I08-2124,N04-4028,0,\N,Missing
I08-4004,P06-1060,0,0.0278536,"Missing"
I08-4004,N04-1001,0,0.0323195,"Missing"
I08-4004,P02-1014,0,0.124563,"Missing"
I08-4004,J01-4004,0,0.122929,"Missing"
I08-4004,H05-1013,0,\N,Missing
I08-4004,P02-1064,0,\N,Missing
I08-4004,N03-1028,0,\N,Missing
I08-4004,P02-1016,0,\N,Missing
I08-4009,I05-3017,0,0.301743,"alance the performance on IV and out-ofvocabulary (OOV) words by combining these two methods according to this belief. In this paper, we provide a more detailed evaluation metric of IV and OOV words than Bakeoff to analyze CT method and combination method, which is a typical way to seek such balance. Our evaluation metric shows that CT outperforms dictionary-based (or so called word-based in general) segmentation on both IV and OOV words within Bakeoff 1 Introduction Chinese Word Segmentation (CWS) has been witnessed a prominent progress in the last three Bakeoffs (Sproat and Emerson, 2003), (Emerson, 2005), (Levow, 2006). One of the reasons for this progress is that Bakeoff provides standard corpora and objective metric, which makes the result of each system comparable. Through those evaluations researchers can recognize the advantage and disadvantage of their methods and improve their systems accordingly. However, in the evaluation metric of Bakeoff, only the overall F measure, precision, recall, IV (invocabulary) recall and OOV (out-of-vocabulary) recall are included and such a metric is not sufficient to give a completely measure on the performance, especially when the performance on IV and"
I08-4009,W06-0115,0,0.0526585,"mance on IV and out-ofvocabulary (OOV) words by combining these two methods according to this belief. In this paper, we provide a more detailed evaluation metric of IV and OOV words than Bakeoff to analyze CT method and combination method, which is a typical way to seek such balance. Our evaluation metric shows that CT outperforms dictionary-based (or so called word-based in general) segmentation on both IV and OOV words within Bakeoff 1 Introduction Chinese Word Segmentation (CWS) has been witnessed a prominent progress in the last three Bakeoffs (Sproat and Emerson, 2003), (Emerson, 2005), (Levow, 2006). One of the reasons for this progress is that Bakeoff provides standard corpora and objective metric, which makes the result of each system comparable. Through those evaluations researchers can recognize the advantage and disadvantage of their methods and improve their systems accordingly. However, in the evaluation metric of Bakeoff, only the overall F measure, precision, recall, IV (invocabulary) recall and OOV (out-of-vocabulary) recall are included and such a metric is not sufficient to give a completely measure on the performance, especially when the performance on IV and OOV word segmen"
I08-4009,I05-3025,0,0.136476,"d such a metric is not sufficient to give a completely measure on the performance, especially when the performance on IV and OOV word segmentation need to be evaluated. An important issue is that segmentation based on which, word or character, can yield the better performance on IV words. We give a detailed explanation about this issue as following. Since CWS was firstly treated as a characterbased tagging task (we call it “CT” for short hereafter) in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al., 2004), (Tseng et al., 2005), (Low et al., 2005), (Zhao et al., 2006). Relatively to dictionary-based * The work is done when the first author is working in MSRA as an intern. 61 Sixth SIGHAN Workshop on Chinese Language Processing segmentation (we call it “DS” for short hereafter), CT method can achieve a higher accuracy on OOV word recognition and a better performance of segmentation in whole. Thus, CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006. Although CT has shown its merits in word segmentation task, some researchers still hold the belief that on IV words DS can perform better than CT"
I08-4009,C04-1081,0,0.218383,"(out-of-vocabulary) recall are included and such a metric is not sufficient to give a completely measure on the performance, especially when the performance on IV and OOV word segmentation need to be evaluated. An important issue is that segmentation based on which, word or character, can yield the better performance on IV words. We give a detailed explanation about this issue as following. Since CWS was firstly treated as a characterbased tagging task (we call it “CT” for short hereafter) in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al., 2004), (Tseng et al., 2005), (Low et al., 2005), (Zhao et al., 2006). Relatively to dictionary-based * The work is done when the first author is working in MSRA as an intern. 61 Sixth SIGHAN Workshop on Chinese Language Processing segmentation (we call it “DS” for short hereafter), CT method can achieve a higher accuracy on OOV word recognition and a better performance of segmentation in whole. Thus, CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006. Although CT has shown its merits in word segmentation task, some researchers still hold the belief that"
I08-4009,W03-1719,0,0.0864695,"Many efforts were paid to balance the performance on IV and out-ofvocabulary (OOV) words by combining these two methods according to this belief. In this paper, we provide a more detailed evaluation metric of IV and OOV words than Bakeoff to analyze CT method and combination method, which is a typical way to seek such balance. Our evaluation metric shows that CT outperforms dictionary-based (or so called word-based in general) segmentation on both IV and OOV words within Bakeoff 1 Introduction Chinese Word Segmentation (CWS) has been witnessed a prominent progress in the last three Bakeoffs (Sproat and Emerson, 2003), (Emerson, 2005), (Levow, 2006). One of the reasons for this progress is that Bakeoff provides standard corpora and objective metric, which makes the result of each system comparable. Through those evaluations researchers can recognize the advantage and disadvantage of their methods and improve their systems accordingly. However, in the evaluation metric of Bakeoff, only the overall F measure, precision, recall, IV (invocabulary) recall and OOV (out-of-vocabulary) recall are included and such a metric is not sufficient to give a completely measure on the performance, especially when the perfo"
I08-4009,W02-1815,0,0.285464,"n metric of Bakeoff, only the overall F measure, precision, recall, IV (invocabulary) recall and OOV (out-of-vocabulary) recall are included and such a metric is not sufficient to give a completely measure on the performance, especially when the performance on IV and OOV word segmentation need to be evaluated. An important issue is that segmentation based on which, word or character, can yield the better performance on IV words. We give a detailed explanation about this issue as following. Since CWS was firstly treated as a characterbased tagging task (we call it “CT” for short hereafter) in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al., 2004), (Tseng et al., 2005), (Low et al., 2005), (Zhao et al., 2006). Relatively to dictionary-based * The work is done when the first author is working in MSRA as an intern. 61 Sixth SIGHAN Workshop on Chinese Language Processing segmentation (we call it “DS” for short hereafter), CT method can achieve a higher accuracy on OOV word recognition and a better performance of segmentation in whole. Thus, CT has drawn more and more attention and became the dominant method in the Bakeoff 2005 and 2006. Although"
I08-4009,N06-2049,0,0.143627,"Missing"
I08-4009,P06-2123,0,0.0318735,"Missing"
I08-4009,P07-1106,0,0.0534477,"orpora (AS, MSRA and PKU) this pure CT method gets the best result. Even on IV word, this pure CT approach outperforms Zhang‟s CT method and produces comparable results with combination with EIV tags, which shows that pure CT method can perform well on IV words too. Moreover, this character-based tagging approach is more clear and simple than the confidence measure method. Although character-based tagging became mainstream approach in the last two Bakeoffs, it does not mean that word information is valueless in Chinese word segmentation. A word-based perceptron algorithm is proposed recently (Zhang and Clark, 2007), which views Chinese word segmentation task from a new angle instead of character-based tagging and gets comparable results with the best results of Bakeoff. Discussion and Related Works Although the method such as confidence measure can be helpful at some circumstance, our experiment shows that pure character-based tagging (pure CT) can work well with reasonable features and tag set. In (Zhao et al., 2006), an enhanced CRF tag set is proposed to distinguish different positions in the multi-character words when the word length is less than 6. In this method, feature templates are almost the s"
I08-4009,Y06-1012,1,0.910019,"Missing"
I08-4009,I05-3027,0,\N,Missing
I08-4009,W03-1726,0,\N,Missing
I08-4015,C04-1081,0,0.038038,"m of character-based tagging. To further improve the performance of our segmenter, we employ a word-based approach to increase the in-vocabulary (IV) word recall and a post-processing to increase the out-of-vocabulary (OOV) word recall. We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora. 1 2 Introduction Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al., 2004), (Tseng et al., 2005), (Low et al., 2005), (Zhao et al., 2006). Thus, as a powerful sequence tagging model, CRF became the dominant method in the Bakeoff 2006 (Levow, 2006). In this paper, we improve basic segmenter under the CRF work frame in two aspects, namely IV and OOV identification respectively. We use the result from word-based segmentation to revise the CRF output so that we gain a higher IV word recall. For the OOV part a post-processing rule is proposed to find those OOV words which are wrongly segmented into several fractions. Our 98 Our Word Segmentation System In this section, w"
I08-4015,I05-3027,0,0.117845,"Missing"
I08-4015,I08-4009,1,0.569446,"kept as the final result. Sixth SIGHAN Workshop on Chinese Language Processing The restriction that WT should not be “S” is reasonable because word-based segmentation is incapable to recognize the OOV word and always segments OOV word into single characters. Besides CRF model is better at dealing with OOV word than our word-based segmentation. When WT is “S” it is possible that current word is an OOV word and segmented into single character wrongly by the word-based segmenter, so the CT of the character should be kept under such situation. For more detail about this analysis please refer to (Wang et al., 2008). 2.3 Post-processing rule The rules we described in last subsection is helpful to improve the IV word recall and now we introduce our post-processing rule to improve the OOV recall. Our post-processing rule is designed to deal with one typical type of OOV errors, namely an OOV word wrongly segmented into several parts. In practice many OOV errors belong to such type. The rule is quite simple. When we read a sentence from the result we get by the last step, we also kept the last N sentences in memory, in our system we set N equals to 20. We do this because adjacent sentences are always relevan"
I08-4015,W02-1815,0,0.0312684,"ing Bakeoff. Base on Conditional Random Field (CRF) model, a basic segmenter is designed as a problem of character-based tagging. To further improve the performance of our segmenter, we employ a word-based approach to increase the in-vocabulary (IV) word recall and a post-processing to increase the out-of-vocabulary (OOV) word recall. We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora. 1 2 Introduction Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al., 2004), (Tseng et al., 2005), (Low et al., 2005), (Zhao et al., 2006). Thus, as a powerful sequence tagging model, CRF became the dominant method in the Bakeoff 2006 (Levow, 2006). In this paper, we improve basic segmenter under the CRF work frame in two aspects, namely IV and OOV identification respectively. We use the result from word-based segmentation to revise the CRF output so that we gain a higher IV word recall. For the OOV part a post-processing rule is proposed to find those OOV words which are w"
I08-4015,W06-0115,0,0.0658766,"sing to increase the out-of-vocabulary (OOV) word recall. We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora. 1 2 Introduction Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al., 2004), (Tseng et al., 2005), (Low et al., 2005), (Zhao et al., 2006). Thus, as a powerful sequence tagging model, CRF became the dominant method in the Bakeoff 2006 (Levow, 2006). In this paper, we improve basic segmenter under the CRF work frame in two aspects, namely IV and OOV identification respectively. We use the result from word-based segmentation to revise the CRF output so that we gain a higher IV word recall. For the OOV part a post-processing rule is proposed to find those OOV words which are wrongly segmented into several fractions. Our 98 Our Word Segmentation System In this section, we describe our system in more detail. Our system includes three modules: a basic CRF tagger, a word-base segmenter to improve the IV recall and a post-processing rule to imp"
I08-4015,I05-3025,0,0.123213,"prove the performance of our segmenter, we employ a word-based approach to increase the in-vocabulary (IV) word recall and a post-processing to increase the out-of-vocabulary (OOV) word recall. We participate in the word segmentation closed test on all five corpora and our system achieved four second best and one the fifth in all the five corpora. 1 2 Introduction Since Chinese Word Segmentation was firstly treated as a character-based tagging task in (Xue and Converse, 2002), this method has been widely accepted and further developed by researchers (Peng et al., 2004), (Tseng et al., 2005), (Low et al., 2005), (Zhao et al., 2006). Thus, as a powerful sequence tagging model, CRF became the dominant method in the Bakeoff 2006 (Levow, 2006). In this paper, we improve basic segmenter under the CRF work frame in two aspects, namely IV and OOV identification respectively. We use the result from word-based segmentation to revise the CRF output so that we gain a higher IV word recall. For the OOV part a post-processing rule is proposed to find those OOV words which are wrongly segmented into several fractions. Our 98 Our Word Segmentation System In this section, we describe our system in more detail. Our"
I08-4015,Y06-1012,1,0.911063,"Missing"
I08-4015,W04-1122,0,0.0286115,"quite simple. When we read a sentence from the result we get by the last step, we also kept the last N sentences in memory, in our system we set N equals to 20. We do this because adjacent sentences are always relevant and some named entity likely occurs repeatedly in these sentences. Then, we scan these sentences to find all n-grams (n from 2 to 7) and count their occurrence. If certain n-gram appears more than a threshold and this n-gram never appears in training corpus, the n-gram will be selected as a word candidate. Then, we filter these word candidates according to the context entropy (Luo and Song, 2004). Assume w is a word candidate appears n times in the current sentence and last N sentences and   {a0 , a1 ,..., al } is the set of left side characters of w . Left Context Entropy (LCE) can be defined as: LCE ( w)  1 n C (ai , w) log  n ai C (ai , w) Here, C (ai , w) is the count of concurrence of ai and w . For the Right Context Entropy, the definition is the same except change left into right. Now, we define Context Entropy (CE) of a word candidate w as min( LCE (w), RCE (w)) . The word candidates with CE larger than a predefined 100 threshold will be bind as a whole word in test corp"
I08-4015,N06-2049,0,\N,Missing
P10-1076,P05-1033,0,0.691849,"is work. We evaluate our method on Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combin"
P10-1076,J07-2003,0,0.484468,"Parser is used to generate the English parse trees for the rule extraction of the syntax-based system. The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set. To speed up MERT, all the sentences with more than 20 Chinese words are removed. The test sets are the NIST evaluation sets of MT04, MT05 and MT06. The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric. Statistical significant test is conducted using the bootstrap resampling method proposed by Koehn (2004). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. By default, both of the beam size and the size of n-best list are set to 20. In the settings of boosting-based system combination, the maximum number of iterations is set to 30, and k (in Equation 7) is set to 5. The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram. 5.3 Evaluation of Translations First we investigate the effectiveness of the boosting-based system combination on the three systems. Figures 2-5 show the BLEU curves on the development and test sets, where"
P10-1076,D08-1024,0,0.0598251,"Missing"
P10-1076,W99-0613,0,0.098175,".47 41.51* Hiero 46.51 47.44* 44.52 45.47* 42.47 43.44* 39.39 40.10* Syntaxbased 46.92 48.70* 46.88 49.40* 45.21 47.02* 40.52 41.88* Table 2: Oracle performance of various systems. * = significantly better than baseline (p < 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s method calculated errors over the phrases that were chosen by phrase-based systems, and could not be applied to many other SMT systems, such as hierarchical phrase-based systems and syntax-based systems. Differing from Lagarda and Casacuberta’s work, we are concerned more with"
P10-1076,P05-1067,0,0.0542414,"nglish Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and"
P10-1076,D09-1114,1,0.814642,"iffering from Lagarda and Casacuberta’s work, we are concerned more with proposing a general framework which can work with most of the current SMT models and empirically demonstrating its effectiveness on various SMT systems. There are also some other studies on building diverse translation systems from a single translation engine for system combination. The first attempt is (Macherey and Och, 2007). They empirically showed that diverse translation systems could be generated by changing parameters at early-stages of the training procedure. Following Macherey and Och (2007)’s work, Duan et al. (2009) proposed a feature subspace method to build a group of translation systems from various different sub-models of an existing SMT system. However, Duan et al. (2009)’s method relied on the heuristics used in feature sub-space selection. For example, they used the remove-one-feature strategy and varied the order of n-gram language model to obtain a satisfactory group of diverse systems. Compared to Duan et al. (2009)’s method, a main advantage of our method is that it can be applied to most of the SMT systems without designing any heuristics to adapt it to the specified systems. 7 Discussion and"
P10-1076,P03-2041,0,0.0433043,"n Chinese-to-English Machine Translation (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combi"
P10-1076,D08-1089,0,0.0533933,"Missing"
P10-1076,2008.amta-srw.3,0,0.132024,"nd Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, sin"
P10-1076,P07-1019,0,0.0341874,"Berkeley Parser is used to generate the English parse trees for the rule extraction of the syntax-based system. The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set. To speed up MERT, all the sentences with more than 20 Chinese words are removed. The test sets are the NIST evaluation sets of MT04, MT05 and MT06. The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric. Statistical significant test is conducted using the bootstrap resampling method proposed by Koehn (2004). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. By default, both of the beam size and the size of n-best list are set to 20. In the settings of boosting-based system combination, the maximum number of iterations is set to 30, and k (in Equation 7) is set to 5. The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram. 5.3 Evaluation of Translations First we investigate the effectiveness of the boosting-based system combination on the three systems. Figures 2-5 show the BLEU curves on the development and test sets, where"
P10-1076,N03-1017,0,0.0224707,"Missing"
P10-1076,W04-3250,0,0.091731,"e Xinhua portion of English Gigaword corpus. Berkeley Parser is used to generate the English parse trees for the rule extraction of the syntax-based system. The data set used for weight training in boostingbased system combination comes from NIST MT03 evaluation set. To speed up MERT, all the sentences with more than 20 Chinese words are removed. The test sets are the NIST evaluation sets of MT04, MT05 and MT06. The translation quality is evaluated in terms of case-insensitive NIST version BLEU metric. Statistical significant test is conducted using the bootstrap resampling method proposed by Koehn (2004). Beam search and cube pruning (Huang and Chiang, 2007) are used to prune the search space in all the three baseline systems. By default, both of the beam size and the size of n-best list are set to 20. In the settings of boosting-based system combination, the maximum number of iterations is set to 30, and k (in Equation 7) is set to 5. The ngram consensuses-based features (in Equation 9) used in system combination ranges from unigram to 4-gram. 5.3 Evaluation of Translations First we investigate the effectiveness of the boosting-based system combination on the three systems. Figures 2-5 show"
P10-1076,2006.amta-papers.25,0,0.0162744,"ore of the group of baseline candidates that are generated in advance (Section 5.1). We see that the diversities of all the systems increase during iterations in most cases, though a few drops occur at a few points. It indicates that our method is very effective to generate diversified member systems. In addition, the diversities of baseline systems (iteration 1) are much lower 5.4 Diversity among Member Systems We also study the change of diversity among the outputs of member systems during iterations. The diversity is measured in terms of the Translation Error Rate (TER) metric proposed in (Snover et al., 2006). A higher TER score means that more edit operations are performed if we transform one translation output into another 745 than those of the systems generated by boosting (iterations 2-30). Together with the results shown in Figures 2-5, it confirms our motivation that the diversified translation outputs can lead to performance improvements over the baseline systems. Also as shown in Figures 6-9, the diversity of the Hiero system is much lower than that of the phrase-based and syntax-based systems at each individual setting of iteration number. This interesting finding supports the observation"
P10-1076,2008.eamt-1.14,0,0.432255,"n baseline (p < 0.05). 6 Related Work Boosting is a machine learning (ML) method that has been well studied in the ML community (Freund, 1995; Freund and Schapire, 1997; Collins et al., 2002; Rudin et al., 2007), and has been successfully adopted in natural language processing (NLP) applications, such as document classification (Schapire and Singer, 2000) and named entity classification (Collins and Singer, 1999). However, most of the previous work did not study the issue of how to improve a single SMT engine using boosting algorithms. To our knowledge, the only work addressing this issue is (Lagarda and Casacuberta, 2008) in which the boosting algorithm was adopted in phrase-based SMT. However, Lagarda and Casacuberta (2008)’s method calculated errors over the phrases that were chosen by phrase-based systems, and could not be applied to many other SMT systems, such as hierarchical phrase-based systems and syntax-based systems. Differing from Lagarda and Casacuberta’s work, we are concerned more with proposing a general framework which can work with most of the current SMT models and empirically demonstrating its effectiveness on various SMT systems. There are also some other studies on building diverse transla"
P10-1076,D09-1038,1,0.848696,"irs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfactory baseline performance, we train each SMT system for 5 times using MERT with different initial values of feature weights to generate a group of baseline candidates, and then select the best-performing one from this group as the final baseline system (i.e. the starting point in the boosting process) for the following experiments. 5.2 Experimental Setup Our b"
P10-1076,P09-1066,0,0.0115942,"ng function: 4 e* = arg max ∑ t =1 β t ⋅ φt (e) + ψ (e, H (v)) (8) T e∈H ( v ) where φt (e) is the log-scaled model score of e in the t-th member system, and β t is the corresponding feature weight. It should be noted that e ∈ Hi may not exist in any Hi &apos; ≠ i . In this case, we can still calculate the model score of e in any other member systems, since all the member systems are based on the same model and share the same feature space. ψ (e, H (v)) is a consensusbased scoring function which has been successfully adopted in SMT system combination (Duan et al., 2009; Hildebrand and Vogel, 2008; Li et al., 2009). The computation of ψ (e, H (v)) is based on a linear combination of a set of n-gram consensuses-based features. ψ (e, H (v)) = ∑θ n+ ⋅ hn+ (e, H (v)) + n ∑θ − n ⋅ hn− (e, H (v)) (9) n For each order of n-gram, hn+ (e, H (v)) and hn− (e, H (v)) are defined to measure the n-gram agreement and disagreement between e and other translation candidates in H(v), respectively. θ n+ and θ n− are the feature weights corresponding to hn+ (e, H (v)) and hn− (e, H (v)) . As hn+ (e, H (v)) and hn− (e, H (v)) used in our work are exactly the same as the features used in (Duan et al., 2009) and similar to th"
P10-1076,P06-1066,0,0.0539712,"Missing"
P10-1076,P06-1096,0,0.025681,"in Equation 4 with two parameters α t and li in it. α t can be regarded as a measure of the importance that the t-th weak system gains in boosting. The definition of α t guarantees that α t always has a positive value3. A main effect of α t is to scale the weight updating (e.g. a larger α t means a greater update). li is the loss on the i-th sample. For each i, let {ei1, ..., ein} be the n-best translation candidates produced by the system. The loss function is defined to be: li = BLEU(ei * , ri ) − 1 k ∑ j =1 BLEU(eij, ri) (7) k where BLEU(eij, ri) is the smoothed sentence-level BLEU score (Liang et al., 2006) of the translation e with respect to the reference translations ri, and ei* is the oracle translation which is selected from {ei1, ..., ein} in terms of BLEU(eij, ri). li can be viewed as a measure of the average cost that we guess the top-k translation candidates instead of the oracle translation. The value of li counts for the magnitude of weight update, that is, a larger li means a larger weight update on Dt(i). The definition of the loss function here is similar to the one used in (Chiang et al., 2008) where only the top-1 translation candidate (i.e. k = 1) is taken into account. 3.3 Syst"
P10-1076,N06-1033,0,0.0353874,"system all phrase pairs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfactory baseline performance, we train each SMT system for 5 times using MERT with different initial values of feature weights to generate a group of baseline candidates, and then select the best-performing one from this group as the final baseline system (i.e. the starting point in the boosting process) for the following experiments. 5.2 Expe"
P10-1076,P06-1077,0,0.0363268,"ion (MT) tasks in three baseline systems, including a phrase-based system, a hierarchical phrasebased system and a syntax-based system. The experimental results on three NIST evaluation test sets show that our method leads to significant improvements in translation accuracy over the baseline systems. 1 Introduction Recent research on Statistical Machine Translation (SMT) has achieved substantial progress. Many SMT frameworks have been developed, including phrase-based SMT (Koehn et al., 2003), hierarchical phrase-based SMT (Chiang, 2005), syntax-based SMT (Eisner, 2003; Ding and Palmer, 2005; Liu et al., 2006; Galley et al., 2006; Cowan et al., 2006), etc. With the emergence of various structurally different SMT systems, more and more studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simpl"
P10-1076,D07-1105,0,0.0909649,"g from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, since we do not always have the access to multiple SMT engines due to the high cost of developing and tuning SMT systems. To reduce the burden of system development, it might be a nice way to combine a set of translation systems built from a single translation engine. A key issue here is how to generate an ensemble of diversified translation systems from a single translation engine in a"
P10-1076,W06-1606,0,0.0329812,"stem with two reordering models including the maximum entropy-based lexicalized reordering model proposed by Xiong et al. (2006) and the hierarchical phrase reordering model proposed by Galley and Manning (2008). In this system all phrase pairs are limited to have source length of at most 3, and the reordering limit is set to 8 by default4. The second SMT system is an in-house reimplementation of the Hiero system which is based on the hierarchical phrase-based model proposed by Chiang (2005). The third SMT system is a syntax-based system based on the string-to-tree model (Galley et al., 2006; Marcu et al., 2006), where both the minimal GHKM and SPMT rules are extracted from the bilingual text, and the composed rules are generated by combining two or three minimal GHKM and SPMT rules. Synchronous binarization (Zhang et al., 2006; Xiao et al., 2009) is performed on each translation rule for the CKYstyle decoding. In this work, baseline system refers to the system produced by the boosting-based system combination when the number of iterations (i.e. T ) is set to 1. To obtain satisfactory baseline performance, we train each SMT system for 5 times using MERT with different initial values of feature weight"
P10-1076,E06-1005,0,0.0445527,"studies are focused on combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, since we do not always have the access to multiple SMT engines due to the high cost of developing and tuning SMT systems. To reduce the burden of system devel"
P10-1076,P02-1038,0,0.15398,"a single engine u(λ*) by adjusting the weight vector λ* in a principled way. In this work, we assume that u1 = u2 =...= uT = u. Our goal is to find a series of λ*i and build a combined system from {u(λ*i)}. To achieve this goal, we propose a Background Given a source string f, the goal of SMT is to find a target string e* by the following equation. e* = arg max(Pr(e |f )) (1) e where Pr(e |f ) is the probability that e is the translation of the given source string f. To model the posterior probability Pr(e |f ) , most of the state-of-the-art SMT systems utilize the loglinear model proposed by Och and Ney (2002), as follows, exp(∑ m =1 λ m ⋅ hm ( f , e)) M Pr(e |f ) = ∑ e &apos; exp(∑ m=1 λ m ⋅ hm( f , e &apos;)) M (2) where {hm( f, e ) |m = 1, ..., M} is a set of features, and λm is the feature weight corresponding to the m-th feature. hm( f, e ) can be regarded as a function that maps every pair of source string f and target string e into a non-negative value, and λm can be viewed as the contribution of hm( f, e ) to the overall score Pr(e |f ) . In this paper, u denotes a log-linear model that has M fixed features {h1( f ,e ), ..., hM( f ,e )}, λ = {λ1, ..., λM} denotes the M parameters of u, and u(λ) denot"
P10-1076,P03-1021,0,0.0703408,"(Freund and Schapire, 1997; Schapire, 2001), the basic idea of this method is to use weak systems (member systems) to form a strong system (combined system) by repeatedly calling weak system trainer on different distributions over the training samples. However, since most of the boosting algorithms are designed for the classification problem that is very different from the translation problem in natural language processing, several key components have to be redesigned when boosting is adapted to SMT system combination. 3.1 Training In this work, Minimum Error Rate Training (MERT) proposed by Och (2003) is used to estimate feature weights λ over a series of training samples. As in other state-of-the-art SMT systems, BLEU is selected as the accuracy measure to define the error function used in MERT. Since the weights of training samples are not taken into account in BLEU2, we modify the original definition of BLEU to make it sensitive to the distribution Dt(i) over the training samples. The modified version of BLEU is called weighted BLEU (WBLEU) in this paper. Let E = e1 ... em be the translations produced by the system, R = r1 ... rm be the reference translations where ri = {ri1, ..., riN},"
P10-1076,P07-1040,0,0.0300808,"combining multiple SMT systems for achieving higher translation accuracy rather than using a single translation system. The basic idea of system combination is to extract or generate a translation by voting from an ensemble of translation outputs. Depending on how the translation is combined and what voting strategy is adopted, several methods can be used for system combination, e.g. sentence-level combination (Hildebrand and Vogel, 2008) simply selects one from original translations, while some more sophisticated methods, such as wordlevel and phrase-level combination (Matusov et al., 2006; Rosti et al., 2007), can generate new translations differing from any of the original translations. One of the key factors in SMT system combination is the diversity in the ensemble of translation outputs (Macherey and Och, 2007). To obtain diversified translation outputs, most of the current system combination methods require multiple translation engines based on different models. However, this requirement cannot be met in many cases, since we do not always have the access to multiple SMT engines due to the high cost of developing and tuning SMT systems. To reduce the burden of system development, it might be a"
P10-1076,W06-1628,0,\N,Missing
P10-1076,P06-1121,0,\N,Missing
P11-2073,P05-1033,0,0.180947,"Missing"
P11-2073,P10-1146,0,0.0250602,"Missing"
P11-2073,P03-2041,0,0.0681744,"Missing"
P11-2073,D08-1022,0,0.0304453,"Missing"
P11-2073,N04-1035,0,0.110983,"Missing"
P11-2073,W07-0405,0,0.0361647,"Missing"
P11-2073,P07-1019,0,0.0571337,"Missing"
P11-2073,2006.amta-papers.8,0,0.0606234,"Missing"
P11-2073,D10-1014,0,0.0221351,"Missing"
P11-2073,C10-1080,0,0.0231974,"Missing"
P11-2073,P06-1055,0,0.0390111,"Missing"
P11-2073,C10-2154,1,0.850668,"Missing"
P11-2073,P09-1020,0,\N,Missing
P11-2073,P08-1023,0,\N,Missing
P11-2073,P06-1077,0,\N,Missing
P11-2073,N09-1027,0,\N,Missing
P11-2073,N09-1025,0,\N,Missing
P11-2073,W06-1606,0,\N,Missing
P11-2073,P03-1021,0,\N,Missing
P11-2073,P08-1114,0,\N,Missing
P11-2073,W04-3250,0,\N,Missing
P11-2126,P08-1109,0,0.0667856,"Missing"
P11-2126,J02-3001,0,0.0689423,"Fc (s1 , ts ) = − Fc (s2 , ts ) = + Fc (s1 ◦s2 , ts ) = + Fr (Ns (s1 ), Ns (s2 )) = − Ff (RF (s1 ), q1 ) = − Fp (RF (s1 ), q1 ) = “v ↑ dj ↑ zj ↓,” Table 1: An example of new features. Suppose we are considering the sentence depicted in Fig. 1. Frontier-words feature Ff (RF (s1 ), q1 ) A feature function which decides whether the right frontier word of s1 and q1 are in the same base phrase in ts . Here, a base phrase is defined to be any phrase which dominates no other phrases. Path feature Fp (RF (s1 ), q1 ) Syntactic path features are widely used in the literature of semantic role labeling (Gildea and Jurafsky, 2002) to encode information of both structures and grammar labels. We define a string-valued feature function Fp (RF (s1 ), q1 ) which connects the right frontier word of s1 to q1 in ts . To better understand the above feature functions, we re-examine the example depicted in Fig. 1. Suppose that we use a shift-reduce-based heterogeneous parser to convert the TCT-style parse to the CTB-style parse and that stack S currently contains two partial parses: s2 :[NP (NN 情报) (NN 专家)] and s1 : (VV 认为). In such a state, we can see that spans of both s2 and s1 ◦ s2 correspond to constituents in ts but that of"
P11-2126,P08-1067,0,0.0849836,"Missing"
P11-2126,P09-1059,0,0.369464,"Missing"
P11-2126,W09-3803,0,0.0396331,"Missing"
P11-2126,P09-1006,0,0.267979,"Missing"
P11-2126,P08-1108,0,0.0469604,"iations in k-best lists (Huang, 2008). Zhu and Zhu (2010) propose to incorporate bracketing structures as parsing constraints in the decoding phase of a CKY-style parser. Their approach shows significant improvements over Wang et al. (1994). However, it suffers from binary distinctions (consistent or inconsistent), as discussed in Section 1. The approach in this paper is reminiscent of co-training (Blum and Mitchell, 1998; Sagae and Lavie, 2006b) and up-training (Petrov et al., 2010). Moreover, it coincides with the stacking method used for dependency parser combination (Martins et al., 2008; Nivre and McDonald, 2008), the Pred method for domain adaptation (Daum´e III and Marcu, 2006), and the method for annotation adaptation of word segmentation and POS tagging (Jiang et al., 2009). As one of the most related works, Jiang and Liu (2009) present a similar approach to conversion between dependency treebanks. In contrast to Jiang and Liu (2009), the task studied in this paper, phrase-structure treebank conversion, is relatively complicated and more efforts should be put into feature engineering. 5 Conclusion To avoid binary distinctions used in previous approaches to automatic treebank conversion, we propose"
P11-2126,P06-1055,0,0.203782,"Missing"
P11-2126,D10-1069,0,0.0163752,"r formalisms, such as from a dependency treebank to a constituency treebank (Niu et al., 2009). However, it suffers from limited variations in k-best lists (Huang, 2008). Zhu and Zhu (2010) propose to incorporate bracketing structures as parsing constraints in the decoding phase of a CKY-style parser. Their approach shows significant improvements over Wang et al. (1994). However, it suffers from binary distinctions (consistent or inconsistent), as discussed in Section 1. The approach in this paper is reminiscent of co-training (Blum and Mitchell, 1998; Sagae and Lavie, 2006b) and up-training (Petrov et al., 2010). Moreover, it coincides with the stacking method used for dependency parser combination (Martins et al., 2008; Nivre and McDonald, 2008), the Pred method for domain adaptation (Daum´e III and Marcu, 2006), and the method for annotation adaptation of word segmentation and POS tagging (Jiang et al., 2009). As one of the most related works, Jiang and Liu (2009) present a similar approach to conversion between dependency treebanks. In contrast to Jiang and Liu (2009), the task studied in this paper, phrase-structure treebank conversion, is relatively complicated and more efforts should be put int"
P11-2126,P06-2089,0,0.433564,"to build a source parser, so we focus only on the latter two factors. To build a heterogeneous parser, we use feature-based parsing algorithms in order to easily incorporate features that encode source-side bracketing structures. Theoretically, any featurebased approaches are applicable, such as Finkel et al. (2008) and Tsuruoka et al. (2009). In this paper, we use the shift-reduce parsing algorithm for its simplicity and competitive performance. 2.2 Shift-Reduce-Based Heterogeneous Parser The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al. (2006). Shift-reduce parsing is a state transition process, where a state is defined to be a tuple hS, Qi. Here, S is a stack containing partial parses, and Q is a queue containing word-POS pairs to be processed. At each state transition, a shift-reduce parser either shifts the top item of Q onto S, or reduces the top one (or two) items on S. A shift-reduce-based heterogeneous parser proceeds similarly as the standard shift-reduce parsing algorithm. In the training phase, each target-style parse tree in the training data is transformed into a binary tree (Charniak et al., 19"
P11-2126,N06-2033,0,0.470868,"to build a source parser, so we focus only on the latter two factors. To build a heterogeneous parser, we use feature-based parsing algorithms in order to easily incorporate features that encode source-side bracketing structures. Theoretically, any featurebased approaches are applicable, such as Finkel et al. (2008) and Tsuruoka et al. (2009). In this paper, we use the shift-reduce parsing algorithm for its simplicity and competitive performance. 2.2 Shift-Reduce-Based Heterogeneous Parser The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al. (2006). Shift-reduce parsing is a state transition process, where a state is defined to be a tuple hS, Qi. Here, S is a stack containing partial parses, and Q is a queue containing word-POS pairs to be processed. At each state transition, a shift-reduce parser either shifts the top item of Q onto S, or reduces the top one (or two) items on S. A shift-reduce-based heterogeneous parser proceeds similarly as the standard shift-reduce parsing algorithm. In the training phase, each target-style parse tree in the training data is transformed into a binary tree (Charniak et al., 19"
P11-2126,E09-1090,0,0.012565,"cribed above, we need to decide the following three factors: 716 (1) a parsing model for building a source parser, (2) a parsing model for building a heterogeneous parser, and (3) features for building a heterogeneous parser. In principle, any off-the-shelf parsers can be used to build a source parser, so we focus only on the latter two factors. To build a heterogeneous parser, we use feature-based parsing algorithms in order to easily incorporate features that encode source-side bracketing structures. Theoretically, any featurebased approaches are applicable, such as Finkel et al. (2008) and Tsuruoka et al. (2009). In this paper, we use the shift-reduce parsing algorithm for its simplicity and competitive performance. 2.2 Shift-Reduce-Based Heterogeneous Parser The heterogeneous parser used in this paper is based on the shift-reduce parsing algorithm described in Sagae and Lavie (2006a) and Wang et al. (2006). Shift-reduce parsing is a state transition process, where a state is defined to be a tuple hS, Qi. Here, S is a stack containing partial parses, and Q is a queue containing word-POS pairs to be processed. At each state transition, a shift-reduce parser either shifts the top item of Q onto S, or r"
P11-2126,P94-1034,0,0.582033,"een put onto the task of automatic conversion of a treebank (source treebank) to fit a different standard which is exhibited by another treebank (target treebank). Treebank conversion is desirable primarily because source-style and target-style annotations exist for non-overlapping text samples so that a larger target-style treebank can be obtained through such conversion. Hereafter, source and target treebanks are named as heterogenous treebanks due to their different annotation standards. In this paper, we focus on the scenario of conversion between phrase-structure heterogeneous treebanks (Wang et al., 1994; Zhu and Zhu, 2010). Due to the availability of annotation in a source treebank, it is natural to use such annotation to guide treebank conversion. The motivating idea is illustrated in Fig. 1 which depicts a sentence annotated with standards of Tsinghua Chinese Treebank (TCT) (Zhou, 1996) and Penn Chinese Treebank (CTB) (Xue et al., 2002), respectively. Suppose that the conversion is in the direction from the TCTstyle parse (left side) to the CTB-style parse (right 1 side). The constituents vp:[将/will 投降/surrender], To show how severe this problem might be, Section 3.1 dj:[敌人/enemy 将/will 投降"
P11-2126,C02-1145,0,0.1009,"Missing"
P11-2126,C10-2176,1,0.853453,"sk of automatic conversion of a treebank (source treebank) to fit a different standard which is exhibited by another treebank (target treebank). Treebank conversion is desirable primarily because source-style and target-style annotations exist for non-overlapping text samples so that a larger target-style treebank can be obtained through such conversion. Hereafter, source and target treebanks are named as heterogenous treebanks due to their different annotation standards. In this paper, we focus on the scenario of conversion between phrase-structure heterogeneous treebanks (Wang et al., 1994; Zhu and Zhu, 2010). Due to the availability of annotation in a source treebank, it is natural to use such annotation to guide treebank conversion. The motivating idea is illustrated in Fig. 1 which depicts a sentence annotated with standards of Tsinghua Chinese Treebank (TCT) (Zhou, 1996) and Penn Chinese Treebank (CTB) (Xue et al., 2002), respectively. Suppose that the conversion is in the direction from the TCTstyle parse (left side) to the CTB-style parse (right 1 side). The constituents vp:[将/will 投降/surrender], To show how severe this problem might be, Section 3.1 dj:[敌人/enemy 将/will 投降/surrender], and np:"
P11-2126,P06-1054,0,\N,Missing
P11-2126,D08-1017,0,\N,Missing
P12-2055,P06-2014,0,0.0795763,"es on the source/target side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Some blocked Tree-to-string Rules: r1: AS(了) → have r2: NN(进口) → the imports r3: S (NN:x1 VP:x2)"
P12-2055,P07-1003,0,0.0953072,"et side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Some blocked Tree-to-string Rules: r1: AS(了) → have r2: NN(进口) → the imports r3: S (NN:x1 VP:x2) → x1 x2 Some blocked Tr"
P12-2055,W08-0306,0,0.081699,"AD VP NN S Figure 1. A real example of Chinese-English sentence pair with word alignment and both-side parse trees. Introduction Most syntax-based statistical machine translation (SMT) systems typically utilize word alignments and parse trees on the source/target side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spuriou"
P12-2055,N04-1035,0,0.148628,"Missing"
P12-2055,C04-1154,0,0.0867544,"Missing"
P12-2055,D09-1024,0,0.082941,"ta. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Some blocked Tree-to-string Rules: r1: AS(了) → have r2: NN(进口) → the imports r3: S (NN:x1 VP:x2) → x1 x2 Some blocked Tree-to-tree Rules: r4: AS(了) → VBZ(have) r5: NN(进口) → NP(D"
P12-2055,H05-1012,0,0.0275651,"of our TSA method on translation quality in tree-to-string and tree-totree translation tasks. Table 5 shows that our TSA method can improve both syntax-based translation systems. As mentioned before, the resulting TSAs are essentially optimized by the translation model. Based on such TSAs, experiments show that spurious link deletion and new valuable link insertion can improve translation quality for tree-to-string and tree-to-tree systems. 5 Related Work Previous studies have made great efforts to incorporate statistics and linguistic heuristics or syntactic information into word alignments (Ittycheriah and Roukos 2005; Taskar et al. 2005; Moore et al. 2006; Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). For example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some prev"
P12-2055,J10-3002,0,0.064516,"suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extraction by deleting spurious links and adding new valuable links based on bilingual translation span correspondences. The proposed approach has two promising properties. Some blocked Tree-to-string Rules: r1: AS(了) → have r2: NN(进口) → the imports r3: S (NN:x1 VP:x2) → x1 x2 Some blocked Tree-to-tree Rules: r4: AS(了) → VBZ(have) r5: NN(进口) → NP(DT(the) NNS(imports"
P12-2055,D07-1038,0,0.261463,"anshao 了 le VV AS VP AD VP NN S Figure 1. A real example of Chinese-English sentence pair with word alignment and both-side parse trees. Introduction Most syntax-based statistical machine translation (SMT) systems typically utilize word alignments and parse trees on the source/target side to learn syntactic transformation rules from parallel data. The approach suffers from a practical problem that even one spurious (word alignment) link can prevent some desirable syntactic translation rules from extraction, which can in turn affect the quality of translation rules and translation performance (May and Knight 2007; Fossum et al. 2008). To address this challenge, a considerable amount of previous research has been done to improve alignment quality by incorporating some statistics and linguistic heuristics or syntactic information into word alignments (Cherry and Lin 2006; DeNero and Klein 2007; May and Knight 2007; Fossum et al. 2008; Hermjakob 2009; Liu et al. 2010). Unlike their efforts, this paper presents a simple approach that automatically builds the translation span alignment (TSA) of a sentence pair by utilizing a phrase-based forced decoding technique, and then improves syntactic rule extractio"
P12-2055,P03-1021,0,0.00511311,"erated by composing two or three minimal rules. A 5-gram language model was trained on the Xinhua portion of English Gigaword corpus. Beam search and cube pruning techniques (Huang and Chiang 2007) were used to prune the search space for all the systems. The base feature set used for all systems is similar to that used in (Marcu et al. 2006), including 14 base features in total such as 5gram language model, bidirectional lexical and phrase-based translation probabilities. All features were log-linearly combined and their weights were optimized by performing minimum error rate training (MERT) (Och 2003). The development data set used for weight training comes from NIST MT03 evaluation set, consisting of 326 sentence pairs of less than 20 words in each Chinese sentence. Two test sets are NIST MT04 (1788 sentence pairs) and MT05 (1082 sentence pairs) evaluation sets. The translation quality is evaluated in terms of the caseinsensitive IBM-BLEU4 metric. 4.2 Effect on Word Alignment To investigate the effect of the TSA method on word alignment, we designed an experiment to evaluate alignment quality against gold standard annotations. There are 200 random chosen and manually aligned Chinese-Engli"
P12-2055,N10-1014,0,0.0172675,"(2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the 283 target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignments between the source and tar"
P12-2055,P10-1032,0,0.0197267,"r example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the 283 target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignment"
P12-2055,C10-1118,0,0.01806,"r example, Fossum et al. (2008) used a discriminatively trained model to identify and delete incorrect links from original word alignments to improve stringto-tree transformation rule extraction, which incorporates four types of features such as lexical and syntactic features. This paper presents an approach to incorporating translation span alignments into word alignments to delete spurious links and add new valuable links. Some previous work directly models the syntactic correspondence in the training data for syntactic rule extraction (Imamura 2001; Groves et al. 2004; Tinsley et al. 2007; Sun et al. 2010a, 2010b; Pauls et al. 2010). Some previous methods infer syntactic correspondences between the source and the 283 target languages through word alignments and constituent boundary based syntactic constraints. Such a syntactic alignment method is sensitive to word alignment behavior. To combat this, Pauls et al. (2010) presented an unsupervised ITG alignment model that directly aligns syntactic structures for string-to-tree transformation rule extraction. One major problem with syntactic structure alignment is that syntactic divergence between languages can prevent accurate syntactic alignment"
P12-2055,P12-3004,1,0.889912,"Missing"
P12-2055,P06-1065,0,\N,Missing
P12-2055,P06-1055,0,\N,Missing
P12-2055,W06-1606,0,\N,Missing
P12-2055,P07-1019,0,\N,Missing
P12-2055,2007.mtsummit-papers.62,0,\N,Missing
P12-2055,H05-1010,0,\N,Missing
P12-3004,J07-2003,0,0.61242,"it, including a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1 Introduction We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al., 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al., 2004; Liu et al., 2006). In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build transla"
P12-3004,P03-2041,0,0.010524,"o-tree rules on all pairs of source and target tree-fragments. 21 Decoding the hierarchical phrase-based and syntaxbased models. For efficient integration of ngram language model into decoding, rules containing more than two variables are binarized into binary rules. In addition to the rules learned from bilingual data, glue rules are employed to glue the translations of a sequence of chunks. z z 4 Decoding as tree-parsing (or tree-based decoding). If the parse tree of source sentence is provided, decoding (for tree-tostring and tree-to-tree models) can also be cast as a tree-parsing problem (Eisner, 2003). In tree-parsing, translation rules are first mapped onto the nodes of input parse tree. This results in a translation tree/forest (or a hypergraph) where each edge represents a rule application. Then decoding can proceed on the hypergraph as usual. That is, we visit in bottom-up order each node in the parse tree, and calculate the model score for each edge rooting at the node. The final output is the 1-best/k-best translations maintained by the root node of the parse tree. Since tree-parsing restricts its search space to the derivations that exactly match with the input parse tree, it in gen"
P12-3004,N04-1035,0,0.0610923,"a simple and fast language model, and an implementation of minimum error rate training for weight tuning. 1 Introduction We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al., 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al., 2004; Liu et al., 2006). In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build translation systems that have placed well at recent MT evalua"
P12-3004,D08-1089,0,0.0327335,"ls. To build new translation systems, all you need is a collection of wordaligned sentences 3 , and a set of additional sentences with one or more reference translations for weight tuning and test. Once the data is prepared, the MT system can be created using a 2 http://www.nlp.org.cn/project/project.php?proj_id=14 To obtain word-to-word alignments, several easy-to-use toolkits are available, such as GIZA++ and Berkeley Aligner. 3 20 4 Term MSD refers to the three orientations (reordering types), including Monotone (M), Swap (S), and Discontinuous (D). probabilities of the three orientations (Galley and Manning, 2008). 3.2 Translation Rule Extraction For the hierarchical phrase-based model, we follow the general framework of SCFG where a grammar rule has three parts – a source-side, a target-side and alignments between source and target nonterminals. To learn SCFG rules from word-aligned sentences, we choose the algorithm proposed in (Chiang, 2007) and estimate the associated feature values as in the phrase-based system. For the syntax-based models, all non-terminals in translation rules are annotated with syntactic labels. We use the GHKM algorithm to extract (minimal) translation rules from bilingual sen"
P12-3004,N03-1017,0,0.0449568,"Missing"
P12-3004,P07-2045,0,0.0149649,"current approaches to statistical machine translation, NiuTrans is based on a log-linear 1 http://www.gnu.org/licenses/gpl-2.0.html 19 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) s"
P12-3004,W09-0424,0,0.0618017,"tical machine translation, NiuTrans is based on a log-linear 1 http://www.gnu.org/licenses/gpl-2.0.html 19 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) source and (or) target lang"
P12-3004,P06-1077,0,0.106485,"nguage model, and an implementation of minimum error rate training for weight tuning. 1 Introduction We present NiuTrans, a new open source machine translation toolkit, which was developed for constructing high quality machine translation systems. The NiuTrans toolkit supports most statistical machine translation (SMT) paradigms developed over the past decade, and allows for training and decoding with several state-of-the-art models, including: the phrase-based model (Koehn et al., 2003), the hierarchical phrase-based model (Chiang, 2007), and various syntax-based models (Galley et al., 2004; Liu et al., 2006). In particular, a unified framework was adopted to decode with different models and ease the implementation of decoding algorithms. Moreover, some useful utilities were distributed with the toolkit, such as: a discriminative reordering model, a simple and fast language model, and an implementation of minimum error rate training that allows for various evaluation metrics for tuning the system. In addition, the toolkit provides easy-to-use APIs for the development of new features. The toolkit has been used to build translation systems that have placed well at recent MT evaluations, such as the"
P12-3004,P03-1021,0,0.231643,"threading techniques to speed-up the system. sequence of commands. Given a number of sentence-pairs and the word alignments between them, the toolkit first extracts a phrase table and two reordering models for the phrase-based system, or a Synchronous Context-free/Tree-substitution Grammar (SCFG/STSG) for the hierarchical phrase-based and syntax-based systems. Then, an n-gram language model is built on the targetlanguage corpus. Finally, the resulting models are incorporated into the decoder which can automatically tune feature weights on the development set using minimum error rate training (Och, 2003) and translate new sentences with the optimized weights. In the following, we will give a brief review of the above components and the main features provided by the toolkit. 3.1 Phrase Extraction and Reordering Model We use a standard way to implement the phrase extraction module for the phrase-based model. That is, we extract all phrase-pairs that are consistent with word alignments. Five features are associated with each phrase-pair. They are two phrase translation probabilities, two lexical weights, and a feature of phrase penalty. We follow the method proposed in (Koehn et al., 2003) to es"
P12-3004,P11-1027,0,0.0234971,"otated with syntactic labels. We use the GHKM algorithm to extract (minimal) translation rules from bilingual sentences with parse trees on source-language side and/or target-language side 5 . Also, two or more minimal rules can be composed together to obtain larger rules and involve more contextual information. For unaligned words, we attach them to all nearby rules, instead of using the most likely attachment as in (Galley et al., 2006). 3.3 N-gram Language Modeling The toolkit includes a simple but effective n-gram language model (LM). The LM builder is basically a “sorted” trie structure (Pauls and Klein, 2011), where a map is developed to implement an array of key/value pairs, guaranteeing that the keys can be accessed in sorted order. To reduce the size of resulting language model, low-frequency n-grams are filtered out by some thresholds. Moreover, an n-gram cache is implemented to speed up n-gram probability requests for decoding. 3.4 is repeated for several times until no better weights (i.e., weights with a higher BLEU score) are found. In this way, our program can introduce some randomness into weight training. Hence users do not need to repeat MERT for obtaining stable and optimized weights"
P12-3004,W10-1738,0,0.0161795,"ngs of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) source and (or) target language side(s). z It offers a wide choice of decoding algorithms. For example, the toolkit has several useful decoding o"
P12-3004,P96-1021,0,0.0239638,"s to obtain new items. Once a new item is created, the associated scores are computed (with an integrated n-gram language model). Then, the item is added into the list of the corresponding cell. This procedure stops when we reach the final state (i.e., the cell associates with the entire source span). The decoder can work with all (hierarchical) phrase-based and syntax-based models. In particular, our toolkit provides the following decoding modes. z Phrase-based decoding. To fit the phrasebased model into the CKY paring framework, we restrict the phrase-based decoding with the ITG constraint (Wu, 1996). In this way, each pair of items in adjunct cells can be composed in either monotone order or inverted order. Hence the decoding can be trivially implemented by a three-loop structure as in standard CKY parsing. This algorithm is actually the same as that used in parsing with bracketing transduction grammars. z Decoding as parsing (or string-based decoding). This mode is designed for decoding with SCFGs/STSGs which are used in the hierarchical phrase-based and syntax-based systems. In the general framework of synchronous grammars and tree transducers, decoding can be regarded as a parsing pro"
P12-3004,P06-1066,0,0.0492136,"Missing"
P12-3004,W06-3119,0,0.0205075,"n, NiuTrans is based on a log-linear 1 http://www.gnu.org/licenses/gpl-2.0.html 19 Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 19–24, c Jeju, Republic of Korea, 8-14 July 2012. 2012 Association for Computational Linguistics model where a number of features are defined to model the translation process. Actually NiuTrans is not the first system of this kind. To date, several open-source SMT systems (based on either phrasebased models or syntax-based models) have been developed, such as Moses (Koehn et al., 2007), Joshua (Li et al., 2009), SAMT (Zollmann and Venugopal, 2006), Phrasal (Cer et al., 2010), cdec (Dyer et al., 2010), Jane (Vilar et al., 2010) and SilkRoad 2 , and offer good references for the development of the NiuTrans toolkit. While our toolkit includes all necessary components as provided within the above systems, we have additional goals for this project, as follows: z 3 It fully supports most state-of-the-art SMT models. Among these are: the phrase-based model, the hierarchical phrase-based model, and the syntax-based models that explicitly use syntactic information on either (both) source and (or) target language side(s). z It offers a wide choi"
P12-3004,P08-1023,0,\N,Missing
P12-3004,P10-4002,0,\N,Missing
P12-3004,P06-1121,0,\N,Missing
P13-1043,P05-1022,0,0.0285913,"Missing"
P13-1043,A00-2018,0,0.0580873,"oduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies. Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between different candidate outp"
P13-1043,D09-1060,1,0.318907,"ed, and output the best state item in the agenda. With this new transition process, we experimented with several extended features,and found that the templates in Table 2 are useful to improve the accuracies further. Here si ll denotes the left child of si ’s left child. Other notations can be explained in a similar way. 4.2 Dependency Relations: Lexical Dependencies Lexical dependencies represent linguistic relations between words: whether a word modifies another word. The idea of exploiting lexical dependency information from auto-parsed data has been explored before for dependency parsing (Chen et al., 2009) and constituent parsing (Zhu et al., 2012). To extract lexical dependencies, we first run the baseline parser on unlabeled data. To simplify the extraction process, we can convert auto-parsed constituency trees into dependency trees by using Penn2Malt. 2 From the dependency trees, we extract bigram lexical dependencies hw1 , w2 , L/Ri where the symbol L (R) means that w1 (w2 ) is the head of w2 (w1 ). We also extract trigram lexical 4 Semi-supervised Parsing with Large Data This section discusses how to extract information from unlabeled data or auto-parsed data to further improve shift-reduc"
P13-1043,P12-1023,1,0.715706,"Missing"
P13-1043,P04-1015,0,0.364497,"cq0 w, s0 cq0 t, q0 wq1 w, q0 wq1 t, q0 tq1 w, q0 tq1 t, s1 wq0 w, s1 wq0 t, s1 cq0 w, s1 cq0 t s0 cs1 cs2 c, s0 ws1 cs2 c, s0 cs1 wq0 t s0 cs1 cs2 w, s0 cs1 cq0 t, s0 ws1 cq0 t s0 cs1 wq0 t, s0 cs1 cq0 w NN address i=1 Φ(ai ) · θ~ Here Φ(ai ) represents the feature vector for the ith action ai in state item α. It is computed by applying the feature templates in Table 1 to the context of α. N is the total number of actions in α. The model parameter ~ θ is trained with the averaged perceptron algorithm, applied to state items (sequence of actions) globally. We apply the early update strategy (Collins and Roark, 2004), stopping parsing for parameter updates when the goldstandard state item falls off the agenda. 2.3 NP address NNS issues issues items for the same sentence can have different numbers of unary actions. Take the phrase “address issues” for example, two possible parses are shown in Figure 2 (a) and (b), respectively. The first parse corresponds to the action sequence [SHIFT, SHIFT, REDUCE-R-NP, FINISH], while the second parse corresponds to the action sequence [SHIFT, SHIFT, UNARY-NP, REDUCE-LVP, FINISH], which consists of one more action than the first case. In practice, variances between state"
P13-1043,P97-1003,0,0.0366356,"d as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies. Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between differen"
P13-1043,D12-1133,0,0.0157449,"2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the framework of global trai"
P13-1043,W08-2102,0,0.0934831,"Missing"
P13-1043,D09-1087,0,0.0306782,"Missing"
P13-1043,P10-1110,0,0.023533,"ang Chen∗ , Min Zhang∗ and Jingbo Zhu† † Natural Language Processing Lab., Northeastern University, China ‡ Singapore University of Technology and Design, Singapore ∗ Soochow University, China and Institute for Infocomm Research, Singapore zhumuhua@gmail.com yue zhang@sutd.edu.sg chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg zhujingbo@mail.neu.edu.cn Abstract Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In"
P13-1043,D10-1002,0,0.0607489,"Missing"
P13-1043,P08-1067,0,0.0401316,"Missing"
P13-1043,N06-2033,0,0.0133712,"eam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (Zhang and Clark, 2009), maintaining all the aforementioned benefits. However, the effects were not as significant as for transition-based dependency parsing. The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie, 2006; Zhang and Clark, 2009). One difference between phrasestructure parsing and dependency parsing is that for the former, parse trees with different numbers of unary rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentence, the largest output can take twice as many as actions to build as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local d"
P13-1043,P08-1066,0,0.0170814,"Missing"
P13-1043,P12-1026,0,0.0106504,"tions are captured by word clustering, lexical dependencies, and a dependency language model, respectively. Based on the information, we propose a set of novel features specifically designed for shift-reduce constituent parsing. [S|s0 , i, false, k, c] [S|X, i, false, k + 1, c + cu ] [S, n, false, k, c] [S, n, true, k + 1, c + cf ] [S, n, true, k, c] [S, n, true, k + 1, c + ci ] 4.1 Paradigmatic Relations: Word Clustering Figure 3: Deductive system of the extended transition system. Word clusters are regarded as lexical intermediaries for dependency parsing (Koo et al., 2008) and POS tagging (Sun and Uszkoreit, 2012). We employ the Brown clustering algorithm (Liang, 2005) on unannotated data (word segmentation is performed if necessary). In the initial state of clustering, each word in the input corpus is regarded as a cluster, then the algorithm repeatedly merges pairs of clusters that cause the least decrease in the likelihood of the input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the cluster ID (a bit string) of an input wor"
P13-1043,P08-1068,0,0.00967274,"nd structural relations. These relations are captured by word clustering, lexical dependencies, and a dependency language model, respectively. Based on the information, we propose a set of novel features specifically designed for shift-reduce constituent parsing. [S|s0 , i, false, k, c] [S|X, i, false, k + 1, c + cu ] [S, n, false, k, c] [S, n, true, k + 1, c + cf ] [S, n, true, k, c] [S, n, true, k + 1, c + ci ] 4.1 Paradigmatic Relations: Word Clustering Figure 3: Deductive system of the extended transition system. Word clusters are regarded as lexical intermediaries for dependency parsing (Koo et al., 2008) and POS tagging (Sun and Uszkoreit, 2012). We employ the Brown clustering algorithm (Liang, 2005) on unannotated data (word segmentation is performed if necessary). In the initial state of clustering, each word in the input corpus is regarded as a cluster, then the algorithm repeatedly merges pairs of clusters that cause the least decrease in the likelihood of the input corpus. The clustering results are a binary tree with words appearing as leaves. Each cluster is represented as a bit-string from the root to the tree node that represents the cluster. We define a function CLU(w) to return the"
P13-1043,W03-3023,0,0.0272721,"y rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentence, the largest output can take twice as many as actions to build as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce pro"
P13-1043,P08-1101,1,0.41006,"u† , Yue Zhang‡ , Wenliang Chen∗ , Min Zhang∗ and Jingbo Zhu† † Natural Language Processing Lab., Northeastern University, China ‡ Singapore University of Technology and Design, Singapore ∗ Soochow University, China and Institute for Infocomm Research, Singapore zhumuhua@gmail.com yue zhang@sutd.edu.sg chenwenliang@gmail.com mzhang@i2r.a-star.edu.sg zhujingbo@mail.neu.edu.cn Abstract Various methods have been proposed to address the disadvantages of greedy local parsing, among which a framework of beam-search and global discriminative training have been shown effective for dependency parsing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohn"
P13-1043,W09-3825,1,0.736189,"outputs for the same input. This turns out to have a large empirical impact on the framework of global training and beam search. We propose a simple yet effective extension to the shift-reduce process, which eliminates size differences between action sequences in beam-search. Our parser gives comparable accuracies to the state-of-the-art chart parsers. With linear run-time complexity, our parser is over an order of magnitude faster than the fastest chart parser. The above global-learning and beam-search framework can be applied to transition-based phrase-structure (constituent) parsing also (Zhang and Clark, 2009), maintaining all the aforementioned benefits. However, the effects were not as significant as for transition-based dependency parsing. The best reported accuracies of transition-based constituent parsers still lag behind the state-of-the-art (Sagae and Lavie, 2006; Zhang and Clark, 2009). One difference between phrasestructure parsing and dependency parsing is that for the former, parse trees with different numbers of unary rules require different numbers of actions to build. Hence the scoring model needs to disambiguate between transitions sequences with different sizes. For the same sentenc"
P13-1043,J93-2004,0,0.0616461,"Missing"
P13-1043,N06-1020,0,0.0167397,"Missing"
P13-1043,P05-1012,0,0.0147001,"ion-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the parsing accuracies. Our method is conceptually simple, requiring only one additional transition action to eliminate size differences between different candidate outputs. On standard evaluat"
P13-1043,nivre-etal-2006-maltparser,0,0.0175741,"Missing"
P13-1043,N07-1051,0,0.0287651,"Missing"
P13-1043,W97-0301,0,0.268373,"Missing"
P13-1043,W05-1513,0,0.352838,"needs to disambiguate between transitions sequences with different sizes. For the same sentence, the largest output can take twice as many as actions to build as the 1 Introduction Transition-based parsers employ a set of shiftreduce actions and perform parsing using a sequence of state transitions. The pioneering models rely on a classifier to make local decisions, and search greedily for a transition sequence to build a parse tree. Greedy, classifier-based parsers have been developed for both dependency grammars (Yamada and Matsumoto, 2003; Nivre et al., 2006) and phrase-structure grammars (Sagae and Lavie, 2005). With linear run-time complexity, they were commonly regarded as a faster but less accurate alternative to graph-based chart parsers (Collins, 1997; Charniak, 2000; McDonald et al., 2005). 434 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 434–443, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics smallest one. This turns out to have a significant empirical impact on parsing with beam-search. We propose an extension to the shift-reduce process to address this problem, which gives significant improvements to the p"
P13-1043,P11-2033,1,0.692073,"sing (Zhang and Clark, 2008; Huang and Sagae, 2010). While beam-search reduces error propagation compared with greedy search, a discriminative model that is globally optimized for whole sequences of transition actions can avoid local score biases (Lafferty et al., 2001). This framework preserves the most important advantage of greedy local parsers, including linear run-time complexity and the freedom to define arbitrary features. With the use of rich non-local features, transition-based dependency parsers achieve state-of-the-art accuracies that are comparable to the best-graph-based parsers (Zhang and Nivre, 2011; Bohnet and Nivre, 2012). In addition, processing tens of sentences per second (Zhang and Nivre, 2011), these transition-based parsers can be a favorable choice for dependency parsing. Shift-reduce dependency parsers give comparable accuracies to their chartbased counterparts, yet the best shiftreduce constituent parsers still lag behind the state-of-the-art. One important reason is the existence of unary nodes in phrase structure trees, which leads to different numbers of shift-reduce actions between different outputs for the same input. This turns out to have a large empirical impact on the"
P13-1043,C12-1194,1,0.77715,"enda. With this new transition process, we experimented with several extended features,and found that the templates in Table 2 are useful to improve the accuracies further. Here si ll denotes the left child of si ’s left child. Other notations can be explained in a similar way. 4.2 Dependency Relations: Lexical Dependencies Lexical dependencies represent linguistic relations between words: whether a word modifies another word. The idea of exploiting lexical dependency information from auto-parsed data has been explored before for dependency parsing (Chen et al., 2009) and constituent parsing (Zhu et al., 2012). To extract lexical dependencies, we first run the baseline parser on unlabeled data. To simplify the extraction process, we can convert auto-parsed constituency trees into dependency trees by using Penn2Malt. 2 From the dependency trees, we extract bigram lexical dependencies hw1 , w2 , L/Ri where the symbol L (R) means that w1 (w2 ) is the head of w2 (w1 ). We also extract trigram lexical 4 Semi-supervised Parsing with Large Data This section discusses how to extract information from unlabeled data or auto-parsed data to further improve shift-reduce parsing accuracies. We consider three typ"
P13-1043,J03-4003,0,\N,Missing
P13-2020,P07-1096,0,0.0845905,"htly improved. This may because that the accuracy of the greedy baseline tagger is already very high and it is hard to get further improvement. Table 2 and table 3 also show that the speed of both tagging and dependency parsing drops linearly with the growth of beam width. 5.2 s 1 3 5 PTB uas compl 91.77 45.29 92.29 46.28 92.50 46.82 92.74 48.12 CTB speed uas compl 84.54 33.75 221 85.11 34.62 124 85.62 37.11 71 86.00 35.87 39 Table 3: Parsing accuracy vs beam width. ‘uas’ and ‘compl’ denote unlabeled score and complete match rate respectively (all excluding punctuations). PTB (Collins, 2002) (Shen et al., 2007) (Huang et al., 2012) this work 1 this work CTB 97.11 (Hatori et al., 2012) 97.33 (Li et al., 2012) 97.35 (Ma et al., 2012) 97.22 this work 1 97.28 this work 93.82 93.88 93.84 93.87 94.01† Table 4: Tagging results on the test set. ‘†’ denotes statistically significant over the greedy baseline by McNemar’s test ( ) Systems (Huang and Sagae, 2010) (Zhang and Nivre, 2011) (Li et al., 2012) this work this work s 8 64 － 1 8 uas compl 85.20 33.72 86.00 36.90 86.55 － 84.79 32.98 86.33† 36.13 Table 5: Parsing results on CTB test set. Systems (Huang and Sagae, 2010) (Zhang and Nivre, 2011) (Koo and Col"
P13-2020,W02-1001,0,0.934328,"2012; Søggard and Wulff, 2012). By processing the input tokens in an easyto-hard order, the algorithm could make use of structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). To enlarge the search space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning (Collins, 2002) enable the use of non-local features that are helpful to improve parsing performance without overfitting (Zhang and Nivre, 2012). Due to these advantages, beam search and global learning has been applied to many NLP tasks (Collins and Roark 2004; Zhang and Clark, 2007). However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing. While applying beam-search is relatively straightforward, the main difficulty comes from combining easy-first dependency parsing with perceptron-based global learning. In particular, one needs"
P13-2020,D08-1052,0,0.0537551,"Missing"
P13-2020,C12-2105,0,0.0224662,"Missing"
P13-2020,P08-1101,0,0.238503,"uld fail to ensure validity of update (see the example in figure 1). For validity of update, we propose a simple solution which is based on “early update” and which can accommodate spurious ambiguity. The basic idea is to use the correct action sequence which was 3 Following (Zhang and Nivre, 2012), we say the training algorithm is global if it optimizes the score of an entire action sequence. A local learner trains a classifier which distinguishes between single actions. 4 As shown in (Goldberg and Nivre 2012), most transitionbased dependency parsers (Nivre et al., 2003; Huang and Sagae 2010;Zhang and Clark 2008) ignores spurious ambiguity by using a static oracle which maps a dependency tree to a single action sequence. pruned right at the step when all correct sequence falls off the beam (as C1 in figure 1). Algorithm 2 shows the pseudo-code of the training procedure over one training sample ( ), a sentence-tree pair. Here we assume to be the set of all correct action sequences/subsequences. At step k, the algorithm constructs a correct action sequence  of length k by extending those in (line 3). It also checks whether no longer contains any correct sequence. If so,  together with are used for par"
P13-2020,P11-2033,0,0.43518,"use CTB 5.1 and the split suggested by (Duan et al., 2007) for both tagging and dependency parsing. We also use Penn2Malt and the head-finding rules of (Zhang and Clark 2008) to convert constituency trees into dependencies. For dependency parsing, we assume gold segmentation and POS tags for the input. 5 112 http://w3.msi.vxu.se/~nivre/research/Penn2Malt.html Features used in English dependency parsing are listed in table 1. Besides the features in (Goldberg and Elhadad, 2010), we also include some trigram features and valency features which are useful for transition-based dependency parsing (Zhang and Nivre, 2011). For English POS tagging, we use the same features as in (Shen et al., 2007). For Chinese POS tagging and dependency parsing, we use the same features as (Ma et al., 2012). All of our experiments are conducted on a Core i7 (2.93GHz) machine, both the tagger and parser are implemented using C++. 5.1 Final results Tagging results on the test set together with some previous results are listed in table 4. Dependency parsing results on CTB and PTB are listed in table 5 and table 6, respectively. On CTB, tagging accuracy of our greedy baseline is already comparable to the state-of-the-art. As the b"
P13-2020,C12-2136,0,0.156696,"structured information on both sides of the hard token thus making more indicative predictions. However, rich structured information also causes exhaustive inference intractable. As an alternative, greedy search which only explores a tiny fraction of the search space is adopted (Goldberg and Elhadad, 2010). To enlarge the search space, a natural extension to greedy search is beam search. Recent work also shows that beam search together with perceptron-based global learning (Collins, 2002) enable the use of non-local features that are helpful to improve parsing performance without overfitting (Zhang and Nivre, 2012). Due to these advantages, beam search and global learning has been applied to many NLP tasks (Collins and Roark 2004; Zhang and Clark, 2007). However, to the best of our knowledge, no work in the literature has ever applied the two techniques to easy-first dependency parsing. While applying beam-search is relatively straightforward, the main difficulty comes from combining easy-first dependency parsing with perceptron-based global learning. In particular, one needs to guarantee that each parameter update is valid, i.e., the correct action sequence has lower model score than the predicted one1"
P13-2020,N10-1115,0,0.631914,"tension of is: ( ) ( ) Here, ‘ ’ means insert to the end of . Following (Huang et al., 2012), in order to formalize beam search, we also use the ( ) operation which returns the top s action sequences in according to ( ). Here, denotes a set of action sequences, ( ) denotes the sum of feature vectors of each action in Pseudo-code of easy-first with beam search is shown in algorithm 1. Beam search grows s (beam width) action sequences in parallel using a 111 Algorithm 2: Perceptron-based training over one training sample ( ) Input: ( ), s, parameter Output: new parameter ( ) ( ( )) Features of (Goldberg and Elhadad, 2010) for p in pi-1, pi, pi+1 wp-vlp, wp-vrp, tp-vlp, tp-vrp, tlcp, trcp, wlcp, wlcp for p in pi-2, pi-1, pi, pi+1, pi+2 tp-tlcp, tp-trcp, tp-tlcp-trcp for p, q, r in (pi-2, pi-1, pi), (pitp-tq-tr, tp-tq-wr ( ) 1, pi+1, pi), (pi+1, pi+2 ,pi) for p, q in (pi-1, pi) tp-tlcp-tq, tp-trcp-tq, ,tp-tlcp-wq,, // top correct extension from the beam tp-trcp-wq, tp-wq-tlcq, tp-wq-trcq 1 2 for 1 1 do ( ) 3  ( ) 4 5 if // all correct seq. falls off the beam ( 6 ) ( ) 7 break 8 if ( ) // full update ( ) 9 ( ) 10 return Table 1: Feature templates for English dependency parsing. wp denotes the head word of p, t"
P13-2020,N12-1015,0,0.372593,"ing with perceptron-based global learning. In particular, one needs to guarantee that each parameter update is valid, i.e., the correct action sequence has lower model score than the predicted one1. The difficulty in ensuring validity of parameter update for the easy-first algorithm is caused by its spurious ambiguity, i.e., the same result might be derived by more than one action sequences. For algorithms which do not exhibit spurious ambiguity, “early update” (Collins and Roark 2004) is always valid: at the k-th step when the single correct action sequence falls off the beam, 1 As shown by (Huang et al., 2012), only valid update guarantees the convergence of any perceptron-based training. Invalid update may lead to bad learning or even make the learning not converge at all. 110 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 110–114, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics Algorithm 1: Easy-first with beam search Input: sentence of n words, beam width s Output: one best dependency tree ( ) ( ) ( ) // top s extensions from the beam // initially, empty beam Figure 2: An example of parsing “I am valid”. Spurious"
P13-2020,P10-1001,0,0.0840505,"Missing"
P13-2020,C12-1103,0,0.101632,"Missing"
P13-2020,P10-1110,0,\N,Missing
P13-2020,P07-1106,0,\N,Missing
P13-2020,C12-2115,0,\N,Missing
P13-2020,C12-1106,1,\N,Missing
P14-1014,P07-1034,0,0.046321,"eeping the learned representations unchanged yields better performance compared with further optimizing them on the source domain data. We release our tools at https://github.com/majineu/TWeb. For future work, we would like to investigate the two-phase approach to more challenging tasks, such as web domain syntactic parsing. We believe that high-accuracy web domain taggers and parsers would benefit a wide range of downstream tasks such as machine translation2 . Besides learning representations, another line of research addresses domain-adaptation by instance re-weighting (Bickel et al., 2007; Jiang and Zhai, 2007) or feature re-weighting (Satpal and Sarawagi, 2007). Those methods assume that each example x that has a non-zero probability on the source domain must have a non-zero probability on the target domain, and vice-versa. As pointed out by Titov (2011), such an assumption is likely to be too restrictive since most NLP tasks adopt word-based or lexicon-based features that vary significantly across different domains. 8 Acknowledgements We would like to thank Hugo Larochelle for his advices on re-implementing WRRBM. We also thank Nan Yang, Shujie Liu and Tong Xiao for the fruitful discussions, and t"
P14-1014,W06-1615,0,0.663678,"classification, which might be of thousands of dimensions. Such high dimensional input gives rise to high computational cost and it is not clear whether those approaches can be applied to large scale unlabelled data, with hundreds of millions of training examples. Our method learns representations from only word ngrams with n ranging from 3 to 5, which can be easily applied to large scale-data. In addition, while Titov (2011) and Glorot et al. (2011) use the learned representation to improve cross-domain classification tasks, we are the first to apply it to cross-domain structured prediction. Blitzer et al. (2006) propose to induce shared representations for domain adaptation, which is based on the alternating structure optimization representations mainly comes from better accuracy of out-of-vocabulary (oov) words. By contrast, using n-gram representations improves the performance on both oov and non-oov. 5.4 Effect of Unlabelled Domain Data In some circumstances, we may know beforehand that the target domain data belongs to a certain sub-domain, such as the email domain. In such cases, it might be desirable to train WRRBM using data only on that domain. We conduct experiments to test whether using the"
P14-1014,W02-1001,0,0.0341955,"text, in our case). Our approach consists of two phrases. In the pre-training phase, we learn an encoder that con144 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 144–154, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics set, higher than those given by ensembled syntactic parsers. Our code will be publicly available at https://github.com/majineu/TWeb. 2 2.2 Most of the indicative features for POS disambiguation can be found from the words and word combinations within a local context (Ratnaparkhi, 1996; Collins, 2002). Inspired by this observation, we apply the RBM to learn feature representations from word n-grams. More specifically, given the ith word wi of a sentence, we apply RBMs to model the joint distribution of the n-gram (wi−l , · · · , wi+r ), where l and r denote the left and right window, respectively. Note that the visible units of RBMs are binary. While in our case, each visible variable corresponds to a word, which may take on tens-of-thousands of different values. Therefore, the RBM need to be re-factorized to make inference tractable. We utilize the Word Representation RBM (WRRBM) factoriz"
P14-1014,P13-2020,1,0.845517,"ction layer of web-feature module with the projection matrix of the learned WRRBM, or ngram-level representation, which corresponds to initializing both the projection and sigmoid layers of the webfeature module by the learned WRRBM. In each case, there can be two different training strategies depending on whether the learned representations are further adjusted or kept unchanged during the fine-turning phrase. Experimental results under the 4 combined settings on the development sets are illustrated in Figure 2, 3 and 4, where the Baseline We reimplemented the greedy easy-first POS tagger of Ma et al. (2013), which is used for all the experiments. While the tagger of Ma et al. (2013) utilizes a linear scorer, our tagger adopts the neural network as its scorer. The neural network of our baseline tagger only contains the sparse-feature module. We use this baseline to examine the performance of a tagger trained purely on the source domain. Feature templates are shown in Table 3, 149 method baseline word-adjust word-fix ngram-adjust ngram-fix WSJ word-fixed word-adjust ngram-fixed ngram-adjust Accuracy 96.9 96.8 all 89.81 +0.09 +0.11 +0.53 +0.69 non-oov 92.42 −0.05 +0.13 +0.52 +0.60 oov 65.64 +1.38 +"
P14-1014,N10-1115,0,0.0143344,"back-propagation updates can be used to update the model’s parameters (line 8 ∼ line 11). For the special case where w ˆ and w do refer to the same word w, it can be easily verified that the two separate back-propagation updates equal to the standard back-propagation with a loss 1 + nn(w, t) − nn(w, tˆ) on the input hwi. The algorithm proposed here belongs to a general framework named guided learning, where search and learning interact with each other. The algorithm learns not only a local classifier, but also the inference order. While previous work (Shen et al., 2007; Zhang and Clark, 2011; Goldberg and Elhadad, 2010) apply guided learning to train a linear classifier by using variants of the perceptron algorithm, we are the first to combine guided learning with a neural network, by using a margin loss and a modified back-propagation algorithm. 5 5.1 about labelled and unlabelled data are summarized in Table 1 and Table 2, respectively. The raw web domain data contains much noise, including spelling error, emotions and inconsistent capitalization. Following some participants (Le Roux et al., 2012), we conduct simple preprocessing steps to the input of the development and the test sets1 • Neutral quotes are"
P14-1014,P06-1055,0,0.0670542,"Missing"
P14-1014,E09-3005,0,0.0237759,"t shows accuracies of the official baseline and that of the top 2 participants. training set likelihood, while our approach is to minimize the margin loss using guided learning. (ASO) method of Ando and Zhang (2005). The idea is to project the original feature representations into low dimensional representations, which yields a high-accuracy classifier on the target domain. The new representations are induced based on the auxiliary tasks defined on unlabelled data together with a dimensionality reduction technique. Such auxiliary tasks can be specific to the supervised task. As pointed out by Plank (2009), for many NLP tasks, defining the auxiliary tasks is a non-trivial engineering problem. Compared with Blitzer et al. (2006), the advantage of using RBMs is that it learns representations in a pure unsupervised manner, which is much simpler. 7 Conclusion We built a web-domain POS tagger using a two-phase approach. We used a WRRBM to learn the representation of the web text and incorporate the representation in a neural network, which is trained using guided learning for easy-first POS tagging. Experiment showed that our approach achieved significant improvement in tagging the web domain text."
P14-1014,D11-1106,1,0.756249,"this way, two separate back-propagation updates can be used to update the model’s parameters (line 8 ∼ line 11). For the special case where w ˆ and w do refer to the same word w, it can be easily verified that the two separate back-propagation updates equal to the standard back-propagation with a loss 1 + nn(w, t) − nn(w, tˆ) on the input hwi. The algorithm proposed here belongs to a general framework named guided learning, where search and learning interact with each other. The algorithm learns not only a local classifier, but also the inference order. While previous work (Shen et al., 2007; Zhang and Clark, 2011; Goldberg and Elhadad, 2010) apply guided learning to train a linear classifier by using variants of the perceptron algorithm, we are the first to combine guided learning with a neural network, by using a margin loss and a modified back-propagation algorithm. 5 5.1 about labelled and unlabelled data are summarized in Table 1 and Table 2, respectively. The raw web domain data contains much noise, including spelling error, emotions and inconsistent capitalization. Following some participants (Le Roux et al., 2012), we conduct simple preprocessing steps to the input of the development and the te"
P14-1014,W96-0213,0,0.875293,"aw input data (web text, in our case). Our approach consists of two phrases. In the pre-training phase, we learn an encoder that con144 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 144–154, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics set, higher than those given by ensembled syntactic parsers. Our code will be publicly available at https://github.com/majineu/TWeb. 2 2.2 Most of the indicative features for POS disambiguation can be found from the words and word combinations within a local context (Ratnaparkhi, 1996; Collins, 2002). Inspired by this observation, we apply the RBM to learn feature representations from word n-grams. More specifically, given the ith word wi of a sentence, we apply RBMs to model the joint distribution of the n-gram (wi−l , · · · , wi+r ), where l and r denote the left and right window, respectively. Note that the visible units of RBMs are binary. While in our case, each visible variable corresponds to a word, which may take on tens-of-thousands of different values. Therefore, the RBM need to be re-factorized to make inference tractable. We utilize the Word Representation RBM"
P14-1014,D13-1061,0,0.0358404,"resources in data cleaning. Table 5: Effect of unlabelled data. “+acc” denotes improvement in tagging accuracy and “cov” denotes the lexicon coverages. 6 Learning representations has been intensively studied in computer vision tasks (Bengio et al., 2007; Lee et al., 2009a). In NLP, there is also much work along this line. In particular, Collobert et al. (2011) and Turian et al. (2010) learn word embeddings to improve the performance of in-domain POS tagging, named entity recognition, chunking and semantic role labelling. Yang et al. (2013) induce bi-lingual word embeddings for word alignment. Zheng et al. (2013) investigate Chinese character embeddings for joint word segmentation and POS tagging. While those approaches mainly explore token-level representations (word or character embeddings), using WRRBM is able to utilize both word and n-gram representations. Titov (2011) and Glorot et al. (2011) propose to learn representations from the mixture of both source and target domain unlabelled data to improve cross-domain sentiment classification. Titov (2011) also propose a regularizer to constrain the inter-domain variability. In particular, their regularizer aims to minimize the Kullback-Leibler (KL)"
P14-1014,P07-1096,0,0.0826462,"niversity, China ‡ Singapore University of Technology and Design majineu@gmail.com yue zhang@sutd.edu.sg zhujingbo@mail.neu.edu.cn Abstract verts the web text into an intermediate representation, which acts as useful features for prediction tasks. We integrate the learned encoder with a set of well-established features for POS tagging (Ratnaparkhi, 1996; Collins, 2002) in a single neural network, which is applied as a scorer to an easyfirst POS tagger. We choose the easy-first tagging approach since it has been demonstrated to give higher accuracies than the standard left-to-right POS tagger (Shen et al., 2007; Ma et al., 2013). In the fine-tuning phase, the parameters of the network are optimized on a set of labelled training data using guided learning. The learned model preserves the property of preferring to tag easy words first. To our knowledge, we are the first to investigate guided learning for neural networks. The idea of learning representations from unlabelled data and then fine-tuning a model with such representations according to some supervised criterion has been studied before (Turian et al., 2010; Collobert et al., 2011; Glorot et al., 2011). While most previous work focus on in-doma"
P14-1014,P11-1007,0,0.0804275,". In NLP, there is also much work along this line. In particular, Collobert et al. (2011) and Turian et al. (2010) learn word embeddings to improve the performance of in-domain POS tagging, named entity recognition, chunking and semantic role labelling. Yang et al. (2013) induce bi-lingual word embeddings for word alignment. Zheng et al. (2013) investigate Chinese character embeddings for joint word segmentation and POS tagging. While those approaches mainly explore token-level representations (word or character embeddings), using WRRBM is able to utilize both word and n-gram representations. Titov (2011) and Glorot et al. (2011) propose to learn representations from the mixture of both source and target domain unlabelled data to improve cross-domain sentiment classification. Titov (2011) also propose a regularizer to constrain the inter-domain variability. In particular, their regularizer aims to minimize the Kullback-Leibler (KL) distance between the marginal distributions of the learned representations on the source and target domains. Their work differs from ours in that their approaches learn representations from the feature vectors for sentiment classification, which might be of thousand"
P14-1014,P10-1040,0,0.653482,"as been demonstrated to give higher accuracies than the standard left-to-right POS tagger (Shen et al., 2007; Ma et al., 2013). In the fine-tuning phase, the parameters of the network are optimized on a set of labelled training data using guided learning. The learned model preserves the property of preferring to tag easy words first. To our knowledge, we are the first to investigate guided learning for neural networks. The idea of learning representations from unlabelled data and then fine-tuning a model with such representations according to some supervised criterion has been studied before (Turian et al., 2010; Collobert et al., 2011; Glorot et al., 2011). While most previous work focus on in-domain sequential labelling or cross-domain classification tasks, we are the first to learn representations for web-domain structured prediction. Previous work treats the learned representations either as model parameters that are further optimized in supervised fine-tuning (Collobert et al., 2011) or as fixed features that are kept unchanged (Turian et al., 2010; Glorot et al., 2011). In this work, we investigate both strategies and give empirical comparisons in the cross-domain setting. Our results suggest t"
P14-1014,I13-1183,0,0.109622,"ing (Hinton et al., 2006; Bengio et al., 2007), DBNs are capable of modelling higher order non-linear relations between the input, and has been demonstrated to improve performance for many computer vision tasks (Hinton et al., 2006; Bengio et al., 2007; Lee et al., 2009a). However, in this work we do not observe further improvement by employing DBNs. This may partly be due to the fact that unlike computer vision tasks, the input structure of POS tagging or other sequential labelling tasks is relatively simple, and a single non-linear layer is enough to model the interactions within the input (Wang and Manning, 2013). 3 Figure 1: The proposed neural network. The webfeature module (lower left) and sparse-feature module (lower right) are combined by a shared output layer (upper). is identical to the training data of the pre-trained WRRBM. The first layer is a linear projection layer, where each word in the input is projected into a Ddimensional real value vector using the projection operation described in Section 2.2. The output of this layer o1w is the concatenation of the projections of wi−l , . . . , wi+r :  1  Mw wi−l   .. o1w =  (8)  . 1 Mw wi+r Neural Network for POS Disambiguation We integrate"
P14-1014,N07-1051,0,\N,Missing
P14-1014,P05-1001,0,\N,Missing
P14-1014,P13-1017,0,\N,Missing
P14-2092,P03-1021,0,0.0193089,"Missing"
P14-2092,P10-1146,0,0.0475086,"Missing"
P14-2092,2006.amta-papers.17,0,0.0641054,"Missing"
P14-2092,P06-1048,0,0.0113997,"standard phrase-based model for gf ull (d) in which not all segments of the sentence need to respect syntactic constraints. Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 (2) 2.3 Model Score Computation In this work both the skeleton translation model gskel (d) and full translation model gf ull (d) resemble the usual forms used in phrase-based MT, i.e., the model score is computed by a linear combination of a g"
P14-2092,P07-1040,0,0.081951,"Missing"
P14-2092,C08-1018,0,0.0204848,"h not all segments of the sentence need to respect syntactic constraints. Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 (2) 2.3 Model Score Computation In this work both the skeleton translation model gskel (d) and full translation model gf ull (d) resemble the usual forms used in phrase-based MT, i.e., the model score is computed by a linear combination of a group of phrase-based features and language models"
P14-2092,W08-0329,0,0.0606487,"Missing"
P14-2092,P03-2041,0,0.0613666,"Missing"
P14-2092,N07-1023,0,0.0336756,"del for gf ull (d) in which not all segments of the sentence need to respect syntactic constraints. Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 (2) 2.3 Model Score Computation In this work both the skeleton translation model gskel (d) and full translation model gf ull (d) resemble the usual forms used in phrase-based MT, i.e., the model score is computed by a linear combination of a group of phrase-based featu"
P14-2092,J82-2005,0,0.777665,"Missing"
P14-2092,2006.amta-papers.8,0,0.0791381,"Missing"
P14-2092,P10-1096,0,0.0144214,"he sentence need to respect syntactic constraints. Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 (2) 2.3 Model Score Computation In this work both the skeleton translation model gskel (d) and full translation model gf ull (d) resemble the usual forms used in phrase-based MT, i.e., the model score is computed by a linear combination of a group of phrase-based features and language models. In phrase-based MT, the tr"
P14-2092,P12-2068,0,0.0175741,"syntactic constraints. Obviously the skeleton used in this work can be viewed as a simplified sentence. Thus the problem is in principle the same as sentence simplification/compression. The motivations of defining the problem in this way are two-fold. First, as the skeleton is a well-formed (but simple) sentence, all current MT approaches are applicable to the skeleton translation problem. Second, obtaining simplified sentences by word deletion is a well-studied issue (Knight and Marcu, 2000; Clarke and Lapata, 2006; Galley and McKeown, 2007; Cohn and Lapata, 2008; Yamangil and Shieber, 2010; Yoshikawa et al., 2012). Many good sentence simpliciation/compression methods are available to our work. Due to the lack of space, we do not go deep into this problem. In Section 3.1 we describe the corpus and system employed for automatic generation of sentence skeletons. 2.2 (2) 2.3 Model Score Computation In this work both the skeleton translation model gskel (d) and full translation model gf ull (d) resemble the usual forms used in phrase-based MT, i.e., the model score is computed by a linear combination of a group of phrase-based features and language models. In phrase-based MT, the translation problem is mode"
P14-2092,N03-1017,0,0.0309021,"Missing"
P14-2092,P06-1077,0,0.106715,"Missing"
P14-2092,2011.mtsummit-papers.6,0,0.0466045,"Missing"
P14-2092,P08-1114,0,0.0575302,"Missing"
P14-2092,2006.amta-papers.13,0,0.0778089,"Missing"
P14-2092,2006.eamt-1.24,0,0.25591,"Missing"
P14-2092,W99-0604,0,0.190929,"tions on the NIST Chinese-English MT evaluation data. 1 • We develop a skeleton-based model which divides translation into two sub-models: a skeleton translation model (i.e., translating the key elements) and a full translation model (i.e., translating the remaining source words and generating the complete translation). Introduction Current Statistical Machine Translation (SMT) approaches model the translation problem as a process of generating a derivation of atomic translation units, assuming that every unit is drawn out of the same model. The simplest of these is the phrase-based approach (Och et al., 1999; Koehn et al., 2003) which employs a global model to process any sub-strings of the input sentence. In this way, all we need is to increasingly translate a sequence of source words each time until the entire sentence is covered. Despite good results in many tasks, such a method ignores the roles of each source word and is somewhat different from the way used by translators. For example, an important-first strategy is generally adopted in human translation - we translate the key elements/structures (or skeleton) of the sentence first, and then translate the remaining parts. This especially mak"
P14-2092,P12-3004,1,\N,Missing
P14-2092,P08-1064,0,\N,Missing
P14-2128,D12-1133,0,0.0896542,"Missing"
P14-2128,J08-4003,0,0.533264,"5 88.15 0 ∼ 15 91.84 90.81 92.96 92.45 21 − 40 15 ∼ 30 91.82 90.15 92.63 93.11 &gt; 30 83.87 75.00 76.61 77.42 0 ∼ 15 89.83 88.06 90.78 90.89 41 − 60 15 ∼ 30 88.01 88.89 88.76 89.77 &gt; 30 − − − − Table 2: Parsing accuracies vs punctuation ratios, on the development set System Dev UAS Test UAS Dev UAS-p Test UAS-p Dev− UAS Test− UAS E-F 91.83 91.75 83.20 84.67 90.64 90.40 A-S 90.71 90.34 79.69 79.64 89.55 89.33 A-S-64 93.02 92.84 84.80 87.80 91.87 91.75 MST 92.56 92.10 84.42 85.67 90.11 89.82 Parser1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2 , and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). Table 1: Parsing accuracies. “E-F” and “MST” denote easy-first parser and MSTparser, respectively. “A-S” and “A-S 64” denote our arc-standard parser with beam width 1 and 64, respectively. “UAS” and “UAS-p” denote word and punctuation unlabelled attachment score, respectively. “− ” denotes the data set with punctuations removed. 2.2"
P14-2128,P13-1104,0,0.117611,"Missing"
P14-2128,W11-0303,0,0.14348,"Missing"
P14-2128,W02-1001,0,0.320993,"ithub.com/majineu/Parser/Punc/A-STD. 2 Influence of Punctuations on Parsing In this section, we conduct a set of experiments to show the influence of punctuations on dependency parsing accuracies. 2.1 Punctuations and Parsing Accuracy Setup We use the Wall Street Journal portion of the Penn Treebank with the standard splits: sections 02-21 are used as the training set; section 22 and section 23 are used as the development and test set, respectively. Penn2Malt is used to convert bracketed structures into dependencies. We use our own implementation of the Part-Of-Speech (POS) tagger proposed by Collins (2002) to tag the development and test sets. Training set POS tags are generated using 10-fold jack-knifing. Parsing accuracy is evaluated using unlabelled attachment score (UAS), which is the percentage of words that are assigned the correct lexical heads. To show that the influence of punctuations on parsing is independent of specific parsing algorithms, we conduct experiments using three parsers, each representing a different parsing methodology: the open source MST1 We trained a second order labelled parser with all the configurations set to the default value. The code is publicly available at h"
P14-2128,D08-1059,1,0.88905,"63 93.11 &gt; 30 83.87 75.00 76.61 77.42 0 ∼ 15 89.83 88.06 90.78 90.89 41 − 60 15 ∼ 30 88.01 88.89 88.76 89.77 &gt; 30 − − − − Table 2: Parsing accuracies vs punctuation ratios, on the development set System Dev UAS Test UAS Dev UAS-p Test UAS-p Dev− UAS Test− UAS E-F 91.83 91.75 83.20 84.67 90.64 90.40 A-S 90.71 90.34 79.69 79.64 89.55 89.33 A-S-64 93.02 92.84 84.80 87.80 91.87 91.75 MST 92.56 92.10 84.42 85.67 90.11 89.82 Parser1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2 , and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). Table 1: Parsing accuracies. “E-F” and “MST” denote easy-first parser and MSTparser, respectively. “A-S” and “A-S 64” denote our arc-standard parser with beam width 1 and 64, respectively. “UAS” and “UAS-p” denote word and punctuation unlabelled attachment score, respectively. “− ” denotes the data set with punctuations removed. 2.2 Our first experiment is to show that, compared with words, punctuations are more"
P14-2128,N10-1115,0,0.035385,"Parsing accuracies vs punctuation ratios, on the development set System Dev UAS Test UAS Dev UAS-p Test UAS-p Dev− UAS Test− UAS E-F 91.83 91.75 83.20 84.67 90.64 90.40 A-S 90.71 90.34 79.69 79.64 89.55 89.33 A-S-64 93.02 92.84 84.80 87.80 91.87 91.75 MST 92.56 92.10 84.42 85.67 90.11 89.82 Parser1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2 , and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). Table 1: Parsing accuracies. “E-F” and “MST” denote easy-first parser and MSTparser, respectively. “A-S” and “A-S 64” denote our arc-standard parser with beam width 1 and 64, respectively. “UAS” and “UAS-p” denote word and punctuation unlabelled attachment score, respectively. “− ” denotes the data set with punctuations removed. 2.2 Our first experiment is to show that, compared with words, punctuations are more difficult to parse and to learn. To see this, we evaluate the parsing accuracies of the selected parsers on words and punctuations, sep"
P14-2128,P11-2033,1,0.959265,"o Company, 358 Wener Rd., Hangzhou, China, 310012 majineu@gmail.com yue zhang@sutd.edu.sg zhujingbo@mail.neu.edu.cn Abstract Moreover, experimental results showed that parsing accuracy of content words drops on sentences which contain higher ratios of punctuations. One reason for this result is that projective dependency parsers satisfy the “no crossing links” constraint, and errors in punctuations may prevent correct word-word dependencies from being created (see section 2). In addition, punctuations cause certain type of features inaccurate. Take valency features for example, previous work (Zhang and Nivre, 2011) has shown that such features are important to parsing accuracy, e.g., it may inform the parser that a verb already has two objects attached to it. However, such information might be inaccurate when the verb’s modifiers contain punctuations. Ultimately, it is the dependencies between words that provide useful information for real world applications. Take machine translation or information extraction for example, most systems take advantage of the head-modifier relationships between word pairs rather than word-punctuation pairs to make better predictions. The fact that most previous work evalua"
P14-2128,P10-1110,0,0.0976296,"Missing"
P14-2128,C12-2136,1,0.917822,"Missing"
P14-2128,P13-2020,1,0.805497,"nt set System Dev UAS Test UAS Dev UAS-p Test UAS-p Dev− UAS Test− UAS E-F 91.83 91.75 83.20 84.67 90.64 90.40 A-S 90.71 90.34 79.69 79.64 89.55 89.33 A-S-64 93.02 92.84 84.80 87.80 91.87 91.75 MST 92.56 92.10 84.42 85.67 90.11 89.82 Parser1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2 , and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). Table 1: Parsing accuracies. “E-F” and “MST” denote easy-first parser and MSTparser, respectively. “A-S” and “A-S 64” denote our arc-standard parser with beam width 1 and 64, respectively. “UAS” and “UAS-p” denote word and punctuation unlabelled attachment score, respectively. “− ” denotes the data set with punctuations removed. 2.2 Our first experiment is to show that, compared with words, punctuations are more difficult to parse and to learn. To see this, we evaluate the parsing accuracies of the selected parsers on words and punctuations, separately. Results are listed in Table 1, where r"
P14-2128,E06-1011,0,0.0431084,"64 MST 0 ∼ 15 94.56 93.87 95.28 94.90 1 ∼ 20 15 ∼ 30 92.88 92.00 94.43 93.55 &gt; 30 87.67 90.05 88.15 88.15 0 ∼ 15 91.84 90.81 92.96 92.45 21 − 40 15 ∼ 30 91.82 90.15 92.63 93.11 &gt; 30 83.87 75.00 76.61 77.42 0 ∼ 15 89.83 88.06 90.78 90.89 41 − 60 15 ∼ 30 88.01 88.89 88.76 89.77 &gt; 30 − − − − Table 2: Parsing accuracies vs punctuation ratios, on the development set System Dev UAS Test UAS Dev UAS-p Test UAS-p Dev− UAS Test− UAS E-F 91.83 91.75 83.20 84.67 90.64 90.40 A-S 90.71 90.34 79.69 79.64 89.55 89.33 A-S-64 93.02 92.84 84.80 87.80 91.87 91.75 MST 92.56 92.10 84.42 85.67 90.11 89.82 Parser1 (McDonald and Pereira, 2006), our own re-implementation of an arc-standard transitionbased parser (Nivre, 2008), which is trained using global learning and beam-search (Zhang and Clark, 2008) with a rich feature set (Zhang and Nivre, 2011) 2 , and our own re-implementation of the easy-first parser (Goldberg and Elhadad, 2010) with an extended feature set (Ma et al., 2013). Table 1: Parsing accuracies. “E-F” and “MST” denote easy-first parser and MSTparser, respectively. “A-S” and “A-S 64” denote our arc-standard parser with beam width 1 and 64, respectively. “UAS” and “UAS-p” denote word and punctuation unlabelled attach"
P14-2128,P05-1012,0,0.216525,"Missing"
P15-4025,J08-2004,0,0.0346422,"is extremely heavy. We will optimize the implementation of dependency parsing in our future work. Experiments We ran our system on several benchmarks. Specifically, we trained and tested word segmentation, POS tagging, chunking, and constituent parsing on CTB5.1: articles 001-270 and 440-1151 were used for training and articles 271-300 were used for testing. The performance of named entity recognition was reported on OntoNotes, where 49,011 sentences were used for training and 1,340 sentences were used for testing. For semantic role labeling, we adopted the same data set and splitting as in (Xue, 2008). Finally, the data set and splitting in (Zhang and Clark, 2011) were used to evaluate the performance of dependency parsing. All results were reported on a machine with a 4 Conclusions and Future Work We have presented the NiuParser Chinese syntactic and semantic analysis toolkit. It can handle several parsing tasks for Chinese, including word segmentation, part-of-speech tagging, named entity recognition, chunking, constituent parsing, dependency parsing, and constituent parser-based semantic role labeling. The NiuParser system is fast and shows state-of-the-art performance on several benchm"
P15-4025,J11-1005,0,0.0323647,"tion of dependency parsing in our future work. Experiments We ran our system on several benchmarks. Specifically, we trained and tested word segmentation, POS tagging, chunking, and constituent parsing on CTB5.1: articles 001-270 and 440-1151 were used for training and articles 271-300 were used for testing. The performance of named entity recognition was reported on OntoNotes, where 49,011 sentences were used for training and 1,340 sentences were used for testing. For semantic role labeling, we adopted the same data set and splitting as in (Xue, 2008). Finally, the data set and splitting in (Zhang and Clark, 2011) were used to evaluate the performance of dependency parsing. All results were reported on a machine with a 4 Conclusions and Future Work We have presented the NiuParser Chinese syntactic and semantic analysis toolkit. It can handle several parsing tasks for Chinese, including word segmentation, part-of-speech tagging, named entity recognition, chunking, constituent parsing, dependency parsing, and constituent parser-based semantic role labeling. The NiuParser system is fast and shows state-of-the-art performance on several benchmarks. Moreover, it supports several advanced features, such as t"
P15-4025,P11-2033,0,0.0370241,"ilities, the identification stage applies the algorithm of enforcing non-overlapping arguments (Jiang and Ng, 2006) to maximize the logprobability of the entire labeled parse tree. In the classification stage, the classifier assigns labels to arguments independently. Transition-based Parsing Syntactic parsers can be grouped into two categories according to decoding algorithms: dynamic programming-based and transition-based. For the purpose of efficiency, we implement the constituent and two versions of dependency parsers in the NiuParser system with transition-based methods (Zhu et al., 2013; Zhang and Nivre, 2011; Chen and Manning, 2014). Specifically, parsers are variants of shift-reduce parsers, which start from an initial state and reach a final state by performing an action in each stage transition. Figure 2 and Figure 3 present an example parse of the two parsers, respectively. One version of the dependency parsers follows the work in (Chen and Manning, 2014), regarding the state transition process as a sequence of classification decisions. In each transition, a best action is chosen by a Neural Network classifier. The 2.3 Improvements and Advanced Features 2.3.1 Word Segmentation In Chinese sent"
P15-4025,P13-1043,1,0.842721,". With such probabilities, the identification stage applies the algorithm of enforcing non-overlapping arguments (Jiang and Ng, 2006) to maximize the logprobability of the entire labeled parse tree. In the classification stage, the classifier assigns labels to arguments independently. Transition-based Parsing Syntactic parsers can be grouped into two categories according to decoding algorithms: dynamic programming-based and transition-based. For the purpose of efficiency, we implement the constituent and two versions of dependency parsers in the NiuParser system with transition-based methods (Zhu et al., 2013; Zhang and Nivre, 2011; Chen and Manning, 2014). Specifically, parsers are variants of shift-reduce parsers, which start from an initial state and reach a final state by performing an action in each stage transition. Figure 2 and Figure 3 present an example parse of the two parsers, respectively. One version of the dependency parsers follows the work in (Chen and Manning, 2014), regarding the state transition process as a sequence of classification decisions. In each transition, a best action is chosen by a Neural Network classifier. The 2.3 Improvements and Advanced Features 2.3.1 Word Segme"
P15-4025,J96-1002,0,0.110432,"ents in a parse tree as arguments with respect to a specified predicate (See Figure 4). Here, semantic role labeling is formalized as a two-stage classification problem. The first stage (called identification) conducts a binary classification to decide whether a constituent in a parse tree is an argument. After the first stage, a set of constituents is fed to the second stage (called classification) classifier which is a multi-class classifier, used for assigning each argument an appropriate semantic label. The statistical model used in the semantic role labeling subsystem is Maximum Entropy (Berger et al., 1996), which provides classification decisions with corresponding probabilities. With such probabilities, the identification stage applies the algorithm of enforcing non-overlapping arguments (Jiang and Ng, 2006) to maximize the logprobability of the entire labeled parse tree. In the classification stage, the classifier assigns labels to arguments independently. Transition-based Parsing Syntactic parsers can be grouped into two categories according to decoding algorithms: dynamic programming-based and transition-based. For the purpose of efficiency, we implement the constituent and two versions of"
P15-4025,D14-1082,0,0.0489682,"tion stage applies the algorithm of enforcing non-overlapping arguments (Jiang and Ng, 2006) to maximize the logprobability of the entire labeled parse tree. In the classification stage, the classifier assigns labels to arguments independently. Transition-based Parsing Syntactic parsers can be grouped into two categories according to decoding algorithms: dynamic programming-based and transition-based. For the purpose of efficiency, we implement the constituent and two versions of dependency parsers in the NiuParser system with transition-based methods (Zhu et al., 2013; Zhang and Nivre, 2011; Chen and Manning, 2014). Specifically, parsers are variants of shift-reduce parsers, which start from an initial state and reach a final state by performing an action in each stage transition. Figure 2 and Figure 3 present an example parse of the two parsers, respectively. One version of the dependency parsers follows the work in (Chen and Manning, 2014), regarding the state transition process as a sequence of classification decisions. In each transition, a best action is chosen by a Neural Network classifier. The 2.3 Improvements and Advanced Features 2.3.1 Word Segmentation In Chinese sentences, words like dates,"
P15-4025,W06-1617,0,0.0276101,") conducts a binary classification to decide whether a constituent in a parse tree is an argument. After the first stage, a set of constituents is fed to the second stage (called classification) classifier which is a multi-class classifier, used for assigning each argument an appropriate semantic label. The statistical model used in the semantic role labeling subsystem is Maximum Entropy (Berger et al., 1996), which provides classification decisions with corresponding probabilities. With such probabilities, the identification stage applies the algorithm of enforcing non-overlapping arguments (Jiang and Ng, 2006) to maximize the logprobability of the entire labeled parse tree. In the classification stage, the classifier assigns labels to arguments independently. Transition-based Parsing Syntactic parsers can be grouped into two categories according to decoding algorithms: dynamic programming-based and transition-based. For the purpose of efficiency, we implement the constituent and two versions of dependency parsers in the NiuParser system with transition-based methods (Zhu et al., 2013; Zhang and Nivre, 2011; Chen and Manning, 2014). Specifically, parsers are variants of shift-reduce parsers, which s"
P15-4025,W06-0127,0,\N,Missing
P18-2047,D17-1151,0,0.0294931,"Missing"
P18-2047,D14-1179,0,0.0120422,"Missing"
P18-2047,D13-1176,0,0.0550282,"imited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our approach yields +0.4 ∼ 1.5 BLEU improvements over the state-of-the-art baselines. 1 Introduction In the past few years, Neural Machine Translation (NMT) has achieved state-of-the-art performance in many translation tasks. It models the translation problem using neural networks with no assumption of the hidden structures between two languages, and learns the model parameters from bilingual texts in an end-to-end fashion (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Cho et al., 2014). In such systems, target words are generated over a sequence of time steps. The model score is simply defined as the sum of the log-scale word probabilities: |y | X log P(y|x) = log P(yj |y&lt;j , x) (1) j=1 where x and y are the source and target sentences, and P(yj |y&lt;j , x) is the probability of generating the j-th word yj given the previously-generated words y&lt;j and the source sentence x. However, the straightforward implementation of this model suffers from many problems, the most obvious one being the bias that the system tends to choose shorter t"
P18-2047,P09-5002,0,0.0505731,"2 aij , β) = log 0.8 + log 1.2 ... = 1.5 Figure 1: The coverage score for a running example (Chinese pinyin-English and β = 0.8). We test our approach on the NIST ChineseEnglish and WMT English-German translation tasks, and it outperforms several state-of-the-art baselines by 0.4∼1.5 BLEU points. illustration): 2 where β is a parameter that can be tuned on a development set. This model has two properties: c(x, y) = |x | X i The Coverage Score Given a word sequence, a coverage vector indicates whether the word of each position is translated. This is trivial for statistical machine translation (Koehn, 2009) because there is no overlap between the translation units of a hypothesis, i.e., we have a 0-1 coverage vector. However, it is not the case for NMT where the coverage is modeled in a soft way. In NMT, no explicit translation units or rules are used. The attention mechanism is used instead to model the correspondence between a source position and a target position (Bahdanau et al., 2015). For a given target position j, the attention-based NMT computes attention score aij for each source position i. aij can be regarded as the measure of the correspondent strength between i and j, and is normali"
P18-2047,W17-3204,0,0.0209735,"a coverage-based feature into NMT. Unlike previous studies, we do not resort to developing extra models nor reranking the limited n-best translations. Instead, we develop a coverage score and apply it to each decoding step. Our approach has several benefits, • Our approach does not require to train a huge neural network and is easy to implement. • Our approach works on beam search for each target position and thus can access more translation hypotheses. • Our approach works consistently well under different sized beam search and sentence lengths contrary to what is observed in other systems (Koehn and Knowles, 2017). We offer a simple and effective method to seek a better balance between model confidence and length preference for Neural Machine Translation (NMT). Unlike the popular length normalization and coverage models, our model does not require training nor reranking the limited n-best outputs. Moreover, it is robust to large beam sizes, which is not well studied in previous work. On the Chinese-English and English-German translation tasks, our approach yields +0.4 ∼ 1.5 BLEU improvements over the state-of-the-art baselines. 1 Introduction In the past few years, Neural Machine Translation (NMT) has"
P18-2047,D15-1166,0,0.0766835,"re β is similar to model warm-up, which makes the model easy to run in the first few decoding steps. Note that our way of truncation is different from Wu et al. (2016)’s, where they clip the coverage into [0, 1] and ignore the fact that a source word may be translated into multiple target words and its coverage should be of a value larger than 1. For decoding, we incorporate the coverage score into beam search via linear combination with the NMT model score as below, 1 As the discussion of the attention mechanism is out of the scope of this work, we refer the reader to Bahdanau et al. (2015); Luong et al. (2015) for more details. 293 Entry b=10 s(x, y) = (1 − α) · log P(y|x) + α · c(x, y) (3) 3 b=500 b=100 where y is a partial translation generated during decoding, log P(y|x) is the model score, and α is the coefficient for linear interpolation. In standard implementation of NMT systems, once a hypothesis is finished, it is removed from the beam and the beam shrinks accordingly. Here we choose a different decoding strategy. We keep the finished hypotheses in the beam until the decoding completes, which means that we compare the finished hypotheses with partial translations at each step. This method h"
P18-2047,P16-1162,0,0.0732288,"baseline systems were based on the opensource implementation of the NMT model presented in Luong et al. (2017). The model was consisted of a 4-layer bi-directional LSTM encoder and a 4-layer LSTM decoder. The size of the embedding and hidden layers was set to 1024. We applied the additive attention model on top of the multi-layer LSTMs (Bahdanau et al., 2015). For training, we used the Adam optimizer (Kingma and Ba, 2015) where the learning rate and batch size were set to 0.001 and 128. We selected the top 30k entries for both source and target vocabularies. For the English-German task, BPE (Sennrich et al., 2016) was used for better performance. For comparison, we re-implemented the length normalization (LN) and coverage penalty (CP) methods (Wu et al., 2016). We used grid search to tune all hyperparameters on the development set as Wu et al. (2016). Specifically, weights for both CP and our CS are evaluated in interval [0, 1] with step 0.1, while the weight for LN is in interval [0.5, 1.5]. We found that the settings determined with beam size 10 can be reliably applied to larger beam sizes in the preliminary experiments and thus we tuned all systems with beam size 10. For Chinese-English translation,"
P18-2047,P16-1008,0,0.222956,"s. It is in general to normalize the model score by translation length (say length normalization) to eliminate this system bias (Wu et al., 2016). Though widely used, length normalization is not a perfect solution. NMT systems still have under-translation and over-translation problem even with a normalized model. It is due to the lack of the coverage model that indicates the degree a source word is translated. As an extreme case, a source word might be translated for several times, which results in many duplicated target words. Several research groups have proposed solutions to this bad case (Tu et al., 2016; Mi et al., 2016). E.g., Tu et al. (2016) developed a coveragebased model to measure the fractional count that a source word is translated during decoding. It can be jointly learned with the NMT model. Alternatively, one can rerank the n-best outputs by coverage-sensitive models, but this method just affects the final output list which has a very limited scope (Wu et al., 2016). In this paper we present a simple and effective approach by introducing a coverage-based feature into NMT. Unlike previous studies, we do not resort to developing extra models nor reranking the limited n-best translat"
P18-2047,P12-3004,1,0.800817,"70 24.17 23.57 24.17 23.69 Table 1: BLEU results of NMT systems. base = base system, LN = length normalization, CP = coverage penalty, and CS = our coverage score. Setup We evaluated our approach on Chinese-English and German-English translation tasks. We used 1.8M sentence Chinese-English bitext provided within NIST12 OpenMT2 and 4.5M sentence German-English bitext provided within WMT16. For Chinese-English translation, we chose the evaluation data of NIST MT06 as the development set, and MT08 as the test set. All Chinese sentences were word segmented using the tool provided within NiuTrans (Xiao et al., 2012). For German-English translation, we chose newstest2013 as the development set and newstest2014 as the test set. Our baseline systems were based on the opensource implementation of the NMT model presented in Luong et al. (2017). The model was consisted of a 4-layer bi-directional LSTM encoder and a 4-layer LSTM decoder. The size of the embedding and hidden layers was set to 1024. We applied the additive attention model on top of the multi-layer LSTMs (Bahdanau et al., 2015). For training, we used the Adam optimizer (Kingma and Ba, 2015) where the learning rate and batch size were set to 0.001"
P19-1176,P17-4012,0,0.233608,"ntion model. In this work, we continue the line of research and go towards a much deeper encoder for Transformer. We choose encoders to study because they have a greater impact on performance than decoders and require less computational cost (Domhan, 2018). Our contributions are threefold: • We show that the proper use of layer normalization is the key to learning deep encoders. The deep network of the encoder can be optimized smoothly by relocating the layer normalization unit. While the location of layer normalization has been discussed in recent systems (Vaswani et al., 2018; Domhan, 2018; Klein et al., 2017), as far as we know, its impact has not been studied in deep Trans2 For example, a standard Transformer encoder has 6 layers. Each of them consists of two sub-layers. More sub-layers are involved on the decoder side. 1810 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810–1822 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics xl F L yl LN xl+1 (a) post-norm residual unit xl LN F yl L xl+1 (b) pre-norm residual unit Figure 1: Examples of pre-norm residual unit and postnorm residual unit. F = sub-layer, an"
P19-1176,D15-1166,0,0.0342435,"Transformer-Big/Base baseline (6-layer encoder) by 0.4∼2.4 BLEU points. As another bonus, the deep model is 1.6X smaller in size and 3X faster in training than Transformer-Big1 . 1 Introduction Neural machine translation (NMT) models have advanced the previous state-of-the-art by learning mappings between sequences via neural networks and attention mechanisms (Sutskever et al., 2014; Bahdanau et al., 2015). The earliest of these read and generate word sequences using a series of recurrent neural network (RNN) units, and the improvement continues when 4-8 layers are stacked for a deeper model (Luong et al., 2015; Wu et al., 2016). More recently, the system based on multi-layer self-attention (call it Transformer) has shown strong results on several large∗ Corresponding author. The source code is available at https://github. com/wangqiangneu/dlcl 1 scale tasks (Vaswani et al., 2017). In particular, approaches of this kind benefit greatly from a wide network with more hidden states (a.k.a. Transformer-Big), whereas simply deepening the network has not been found to outperform the “shallow” counterpart (Bapna et al., 2018). Do deep models help Transformer? It is still an open question for the discipline"
P19-1176,W18-6301,0,0.170192,"Big is updated for 100k/300k steps on the En-De task as Vaswani et al. (2017), 50k/100k steps on the Zh-En-Small task, and 200k/500k steps on the Zh-En-Large task. In our model, we use the dynamic linear combination of layers for both encoder and decoder. For efficient computation, we only combine the output of a complete layer rather than a sub-layer. It should be noted that for deep models (e.g. L ≥ 20), it is hard to handle a full batch in a single GPU due to memory size limitation. We solve this issue by accumulating gradients from two small batches (e.g. batch = 2048) before each update (Ott et al., 2018). In our primitive experiments, we observed that training with larger batches and learning rates worked well for deep models. Therefore all the results of deep models are reported with batch = 8192, lr = 2×10−3 and warmup = 16,000 unless otherwise stated. For fairness, we only use half of the updates of baseline (e.g. update = 50k) to ensure the same amount of data that we actually 12 https://github.com/tensorflow/ tensor2tensor Results Results on the En-De Task In Table 1, we first report results on WMT En-De where we compare to the existing systems based on self-attention. Obviously, while a"
P19-1176,P18-1167,0,0.157394,"canu et al., 2013; Bapna et al., 2018). We note that, despite the significant development effort, simply stacking more layers cannot benefit the system and leads to a disaster of training in some of our experiments. A promising attempt to address this issue is Bapna et al. (2018)’s work. They trained a 16layer Transformer encoder by using an enhanced attention model. In this work, we continue the line of research and go towards a much deeper encoder for Transformer. We choose encoders to study because they have a greater impact on performance than decoders and require less computational cost (Domhan, 2018). Our contributions are threefold: • We show that the proper use of layer normalization is the key to learning deep encoders. The deep network of the encoder can be optimized smoothly by relocating the layer normalization unit. While the location of layer normalization has been discussed in recent systems (Vaswani et al., 2018; Domhan, 2018; Klein et al., 2017), as far as we know, its impact has not been studied in deep Trans2 For example, a standard Transformer encoder has 6 layers. Each of them consists of two sub-layers. More sub-layers are involved on the decoder side. 1810 Proceedings of"
P19-1176,D18-1457,0,0.24854,"value. For pre-norm Transformer, we define G(·) 5 Some of the other single-step methods, e.g. the RungeKutta method, can obtain a higher order by taking several intermediate steps (Butcher, 2003). Higher order generally means more accurate. 1812 y0 y1 y2 y3 x1 x2 x3 x4 y0 y1 y2 y3 x1 x2 x3 x4 1 0 1 0 0 1 0 0 0 1 y0 y1 y2 y3 x1 x2 x3 x4 1 (a) 1 1 1 1 1 1 1 1 1 (b) 1 0 1 0 0 1 .1 .3 .2 .4 y0 y1 y2 y3 x1 x2 x3 x4 1.8 (c) .4 1.2 .3 .2 .8 .1 .3 .5 .7 (d) Figure 2: Connection weights for 3-layer encoder: (a) residual connection (He et al., 2016a), (b) dense residual connection (Britz et al., 2017; Dou et al., 2018), (c) multi-layer representation fusion (Wang et al., 2018b)/transparent attention (Bapna et al., 2018) and (d) our approach. y0 denotes the input embedding. Red denotes the weights are learned by model. et al., 2017; Dou et al., 2018). Multi-layer representation fusion (Wang et al., 2018b) and transparent attention (call it TA) (Bapna et al., 2018) methods can learn a weighted model to fuse layers but they are applied to the topmost layer only. The DLCL model can cover all these methods. It provides ways of weighting and connecting layers in the entire stack. We emphasize that although the id"
P19-1176,N18-1202,0,0.0478456,"encoder by resorting to auxiliary losses in intermediate layers. This method is orthogonal to our DLCL method, though it is used for language modeling, which is not a very heavy task. Densely Residual Connections. Densely residual connections are not new in NMT. They have been studied for different architectures, e.g., RNN (Britz et al., 2017) and Transformer (Dou et al., 2018). Some of the previous studies fix the weight of each layer to a constant, while others learn a weight distribution by using either the self-attention model (Wang et al., 2018b) or a softmax-normalized learnable vector (Peters et al., 2018). They focus more on learning connections from lower-level layers to the topmost layer. Instead, we introduce additional connectivity into the network and learn more densely connections for each layer in an end-to-end fashion. 8 Conclusion We have studied deep encoders in Transformer. We have shown that the deep Transformer models can be easily optimized by proper use of layer normalization, and have explained the reason behind it. Moreover, we proposed an approach based on a dynamic linear combination of layers and successfully trained a 30-layer Transformer system. It is the deepest encoder"
P19-1176,P16-1009,0,0.0361058,"example, the standard residual network is a special case of DLCL, where Wll+1 = 1, and Wkl+1 = 0 for k < l. Figure (2) compares different methods of connecting a 3-layer network. We see that the densely residual network is a fully-connected network with a uniform weighting schema (Britz 4 Experimental Setup We first evaluated our approach on WMT’16 English-German (En-De) and NIST’12 ChineseEnglish (Zh-En-Small) benchmarks respectively. To make the results more convincing, we also experimented on a larger WMT’18 Chinese-English dataset (Zh-En-Large) with data augmentation by back-translation (Sennrich et al., 2016a). 4.1 Datasets and Evaluation For the En-De task, to compare with Vaswani et al. (2017)’s work, we use the same 4.5M preprocessed data 7 , which has been tokenized and 6 Let the encoder depth be M and the decoder depth be N (M &gt; N for a deep encoder model). Then TA newly adds O(M × N ) connections, which are fewer than ours of O(M 2 ) 7 https://drive.google.com/uc?export= download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8 1813 Model Param. Vaswani et al. (2017) (Base) 65M 137M 213M 379M † 210M † 210M 356M 210M 62M 211M 106M 62M 121M 62M 211M 106M 62M 137M Bapna et al. (2018)-deep (Base, 16L) Vaswani e"
P19-1176,P16-1162,0,0.138954,"example, the standard residual network is a special case of DLCL, where Wll+1 = 1, and Wkl+1 = 0 for k < l. Figure (2) compares different methods of connecting a 3-layer network. We see that the densely residual network is a fully-connected network with a uniform weighting schema (Britz 4 Experimental Setup We first evaluated our approach on WMT’16 English-German (En-De) and NIST’12 ChineseEnglish (Zh-En-Small) benchmarks respectively. To make the results more convincing, we also experimented on a larger WMT’18 Chinese-English dataset (Zh-En-Large) with data augmentation by back-translation (Sennrich et al., 2016a). 4.1 Datasets and Evaluation For the En-De task, to compare with Vaswani et al. (2017)’s work, we use the same 4.5M preprocessed data 7 , which has been tokenized and 6 Let the encoder depth be M and the decoder depth be N (M &gt; N for a deep encoder model). Then TA newly adds O(M × N ) connections, which are fewer than ours of O(M 2 ) 7 https://drive.google.com/uc?export= download&id=0B_bZck-ksdkpM25jRUN2X2UxMm8 1813 Model Param. Vaswani et al. (2017) (Base) 65M 137M 213M 379M † 210M † 210M 356M 210M 62M 211M 106M 62M 121M 62M 211M 106M 62M 137M Bapna et al. (2018)-deep (Base, 16L) Vaswani e"
P19-1176,N18-2074,0,0.109651,"Missing"
P19-1176,1983.tc-1.13,0,0.401031,"Missing"
P19-1176,P12-3004,1,0.877973,"Updates, which can be used to approximately measure the required training time. † denotes an estimate value. Note that “-deep” represents the best-achieved result as depth changes. jointly byte pair encoded (BPE) (Sennrich et al., 2016b) with 32k merge operations using a shared vocabulary 8 . We use newstest2013 for validation and newstest2014 for test. For the Zh-En-Small task, we use parts of the bitext provided within NIST’12 OpenMT9 . We choose NIST MT06 as the validation set, and MT04, MT05, MT08 as the test sets. All the sentences are word segmented by the tool provided within NiuTrans (Xiao et al., 2012). We remove the sentences longer than 100 and end up with about 1.9M sentence pairs. Then BPE with 32k operations is used for both sides independently, resulting in a 44k Chinese vocabulary and a 33k English vocabulary respectively. For the Zh-En-Large task, we use exactly the same 16.5M dataset as Wang et al. (2018a), composing of 7.2M-sentence CWMT corpus, 4.2M-sentence UN and News-Commentary combined corpus, and back-translation of 5M-sentence monolingual data from NewsCraw2017. We refer the reader to Wang et al. (2018a) for the details. 8 The tokens with frequencies less than 5 are filtere"
P19-1176,Q16-1027,0,0.0280018,"Missing"
P19-1176,W18-1819,0,0.243782,"er encoder by using an enhanced attention model. In this work, we continue the line of research and go towards a much deeper encoder for Transformer. We choose encoders to study because they have a greater impact on performance than decoders and require less computational cost (Domhan, 2018). Our contributions are threefold: • We show that the proper use of layer normalization is the key to learning deep encoders. The deep network of the encoder can be optimized smoothly by relocating the layer normalization unit. While the location of layer normalization has been discussed in recent systems (Vaswani et al., 2018; Domhan, 2018; Klein et al., 2017), as far as we know, its impact has not been studied in deep Trans2 For example, a standard Transformer encoder has 6 layers. Each of them consists of two sub-layers. More sub-layers are involved on the decoder side. 1810 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 1810–1822 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics xl F L yl LN xl+1 (a) post-norm residual unit xl LN F yl L xl+1 (b) pre-norm residual unit Figure 1: Examples of pre-norm residual unit and postno"
P19-1176,P17-1013,0,0.0334172,"ntext of neural machine translation since the emergence of RNN-based models. To ease optimization, researchers tried to reduce the number of non-linear transitions (Zhou et al., x1 4 8 x1 x6 6 x2 x3 4 x4 2 x11 2 x5 x6 0 x7 y0 x16 0 y1 y2 4.1 3.3 3.2 1.7 2.3 0.2 0.5 y5 y10 y15 y20 y25 y6 −2 1.1 0.0 0.0 0.1 0.8 0.2 0.0 0.5 x22 ∼ x31 −4 y0 y5 x11 ∼ x21 −2 x31 y4 (b) 6-layer decoder of DLCL x21 x26 y3 0.0 0.5 0.2 0.0 0.0 0.1 y30 (a) 30-layer encoder of DLCL (c) Weight distribution of y10 in the encoder Figure 5: A visualization example of learned weights in our 30-layer pre-norm DLCL model. 2016; Wang et al., 2017). But these attempts are limited to the RNN architecture and may not be straightforwardly applicable to the current Transformer model. Perhaps, the most relevant work to what is doing here is Bapna et al. (2018)’s work. They pointed out that vanilla Transformer was hard to train if the depth of the encoder was beyond 12. They successfully trained a 16-layer Transformer encoder by attending the combination of all encoder layers to the decoder. In their approach, the encoder layers are combined just after the encoding is completed, but not during the encoding process. In contrast, our approach a"
P19-1176,W18-6430,1,0.928098,"proach to learning deep networks, and plays an important role in Transformer. In principle, residual networks can be seen as instances of the ordinary differential equation (ODE), behaving like the forward Euler discretization with an initial value (Chang et al., 2018; Chen et al., 2018b). Euler’s method is probably the most popular firstorder solution to ODE. But it is not yet accurate enough. A possible reason is that only one previous step is used to predict the current value 5 (Butcher, 2003). In MT, the single-step property of the residual network makes the model “forget” distant layers (Wang et al., 2018b). As a result, there is no easy access to features extracted from lower-level layers if the model is very deep. Here, we describe a model which makes direct links with all previous layers and offers efficient access to lower-level representations in a deep stack. We call it dynamic linear combination of layers (DLCL). The design is inspired by the linear multi-step method (LMM) in numerical ODE (Ascher and Petzold, 1998). Unlike Euler’s method, LMM can effectively reuse the information in the previous steps by linear combination to achieve a higher order. Let {y0 , ..., yl } be the output of"
P19-1176,C18-1255,1,0.936275,"proach to learning deep networks, and plays an important role in Transformer. In principle, residual networks can be seen as instances of the ordinary differential equation (ODE), behaving like the forward Euler discretization with an initial value (Chang et al., 2018; Chen et al., 2018b). Euler’s method is probably the most popular firstorder solution to ODE. But it is not yet accurate enough. A possible reason is that only one previous step is used to predict the current value 5 (Butcher, 2003). In MT, the single-step property of the residual network makes the model “forget” distant layers (Wang et al., 2018b). As a result, there is no easy access to features extracted from lower-level layers if the model is very deep. Here, we describe a model which makes direct links with all previous layers and offers efficient access to lower-level representations in a deep stack. We call it dynamic linear combination of layers (DLCL). The design is inspired by the linear multi-step method (LMM) in numerical ODE (Ascher and Petzold, 1998). Unlike Euler’s method, LMM can effectively reuse the information in the previous steps by linear combination to achieve a higher order. Let {y0 , ..., yl } be the output of"
P19-1176,D18-1338,0,\N,Missing
P19-1352,P18-2049,0,0.0200019,"This is similar to the Chinese word “sangsheng” (paired with “killed”) and the English words “died” and “killed”. Figure 6(c) shows that the representations of the Chinese and English words which relate to “president” are very close. 4 Related Work Many previous works focus on improving the word representations of NMT by capturing the fine-grained (character) or coarse-grained (sub-word) monolingual characteristics, such as character-based NMT (Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Cho et al., 2014; Chen et al., 2016), sub-word NMT (Sennrich et al., 2016b; Johnson et al., 2017; Ataman and Federico, 2018), and hybrid NMT (Luong and Manning, 2016). They effectively consider and utilize the morphological information to enhance the word representations. Our work aims to enhance word representations through the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the sou"
P19-1352,P18-1008,0,0.0298591,"n vectors. While in (b), the two word embeddings are made up of two parts, indicating the shared (lined nodes) and the private (unlined nodes) features. This enables the two words to make use of common representation units, leading to a closer relationship between them. With the introduction of ever more powerful architectures, neural machine translation (NMT) has become the most promising machine translation method (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). For word representation, different architectures— including, but not limited to, recurrence-based (Chen et al., 2018), convolution-based (Gehring et al., 2017) and transformation-based (Vaswani et al., 2017) NMT models—have been taking advantage of the distributed word embeddings to capture the syntactic and semantic properties of words (Turian et al., 2010). Corresponding author Rd (a) Standard Introduction ∗ Long NMT usually utilizes three matrices to represent source embeddings, target input embeddings, and target output embeddings (also known as pre-softmax weight), respectively. These embeddings occupy most of the model parameters, which constrains the improvements of NMT because the recent methods beco"
P19-1352,P16-1186,0,0.0318903,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,P16-2058,0,0.0190662,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,P17-1106,1,0.801916,"ical meaning. Based on these observations, we find that the alignment quality is not a key factor affecting the model performance. In contrast, pairing as many as similar words possible helps the model to better learn the bilingual vector space, which improves the translation performance. The following qualitative analyses support these observations either. 3.5 Analysis of the Translation Results Table 6 shows two translation examples of the NIST Chinese-English translation task. To better understand the translations produced by these two models, we use layer-wise relevance propagation (LRP) (Ding et al., 2017) to produce the attention maps of the selected translations, as shown in Figure 4 and 5. In the first example, the Chinese word “sangsheng” is a low-frequency word and its ground truth is “killed”. It is observed the inadequate representation of “sangsheng” leads to a decline in the translation quality of the vanilla, direct bridging, and decoder WT methods. In our proposed 3619 president chief 0 zongtong also zhuxi sangsheng killed −0.1 died zhuxi president zongtong chief also 0.3 yebing 0.35 juzhang president chairman zongtong weiyuanzhang chief premier 0.3 0.2 −0.2 bing −0.3 ye −0.4 −0.2 0"
P19-1352,N13-1073,0,0.201607,"as parallel words that are the translation of each other. According to the word frequency, each source word x is paired with a target aligned word yˆ that has the highest alignment probability among the candidates, and is computed as follows: yˆ = arg max logA(y|x) (1) y∈a(x) where a(·) denotes the set of aligned candidates. It is worth noting the target words that have been paired with the source words cannot be used as candidates. A(·|·) denotes the alignment probability. These can be obtained by either the intrinsic attention mechanism (Bahdanau et al., 2015) or unsupervised word aligner (Dyer et al., 2013). 2.1.2 Words with Same Word Form As shown in Figure 2(b), the sub-word “Ju@@” simultaneously exists in English and German sentences. This kind of word tends to share a medium number of features of the word embeddings. Most of the time, the source and target words with the same word form also share similar lexical meaning. This category of words generally includes Arabic numbers, punctuations, named entities, cognates and loanwords. However, there are some bilingual homographs where the words in the source and target languages look the same but have completely different meanings. For example,"
P19-1352,D17-1146,0,0.228186,"Missing"
P19-1352,N18-1032,0,0.0214543,"pturing the fine-grained (character) or coarse-grained (sub-word) monolingual characteristics, such as character-based NMT (Costa-Juss`a and Fonollosa, 2016; Ling et al., 2015; Cho et al., 2014; Chen et al., 2016), sub-word NMT (Sennrich et al., 2016b; Johnson et al., 2017; Ataman and Federico, 2018), and hybrid NMT (Luong and Manning, 2016). They effectively consider and utilize the morphological information to enhance the word representations. Our work aims to enhance word representations through the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate"
P19-1352,P02-1040,0,0.104132,"Missing"
P19-1352,D13-1176,0,0.102581,"and (b) shared-private word embeddings. In (a), the English word “Long” and the German word “Lange”, which have similar lexical meanings, are represented by two private d-dimension vectors. While in (b), the two word embeddings are made up of two parts, indicating the shared (lined nodes) and the private (unlined nodes) features. This enables the two words to make use of common representation units, leading to a closer relationship between them. With the introduction of ever more powerful architectures, neural machine translation (NMT) has become the most promising machine translation method (Kalchbrenner and Blunsom, 2013; Sutskever et al., 2014; Bahdanau et al., 2015). For word representation, different architectures— including, but not limited to, recurrence-based (Chen et al., 2018), convolution-based (Gehring et al., 2017) and transformation-based (Vaswani et al., 2017) NMT models—have been taking advantage of the distributed word embeddings to capture the syntactic and semantic properties of words (Turian et al., 2010). Corresponding author Rd (a) Standard Introduction ∗ Long NMT usually utilizes three matrices to represent source embeddings, target input embeddings, and target output embeddings (also kno"
P19-1352,W04-3250,0,0.118362,"Missing"
P19-1352,P18-1164,0,0.205747,"is method can also be adapted to sub-word NMT with a shared source-target sub-word vocabulary and it performs well in language pairs with many of the same characters, such as English-German and English-French (Vaswani et al., 2017). Unfortunately, this method is not applicable to languages that are written in different alphabets, such as Chinese-English (Hassan et al., 2018). Another challenge facing the source and target word embeddings of NMT is the lack of interactions. This degrades the attention performance, leading to some unaligned translations that hurt the translation quality. Hence, Kuang et al. (2018) propose to bridge the source and target embeddings, which brings better attention to the related source and target words. Their method is applicable to any language pairs, providing a tight interaction between the source and target word pairs. However, their method requires additional components and model parameters. In this work, we aim to enhance the word representations and the interactions between the source and target words, while using even fewer parameters. To this end, we present a languageindependent method, which is called sharedprivate bilingual word embeddings, to share a part of"
P19-1352,E17-2025,0,0.382093,"ce and target words with the same word form also share similar lexical meaning. This category of words generally includes Arabic numbers, punctuations, named entities, cognates and loanwords. However, there are some bilingual homographs where the words in the source and target languages look the same but have completely different meanings. For example, the German word “Gift” means “Poison” in English. That is the reason we propose to first pair the words with similar lexical meaning instead of those words with same word forms. This might be the potential limitation of the three-way WT method (Press and Wolf, 2017), where words with the same word form indiscriminately share the same word embedding. 2.1.3 Unrelated Words We regard source and target words that cannot be paired with each other as unrelated words. Figure 2(c) shows an example of a pair of unrelated words. This category is mainly composed of lowfrequency words, such as misspelled words, special characters, and foreign words. In standard NMT, the embeddings of low-frequency words are usually inadequately trained, resulting in a poor word representation. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Sl"
P19-1352,W17-4739,0,0.0393269,"Missing"
P19-1352,P16-1162,0,0.847068,"nt the word embeddings Experiments We carry out our experiments on the small-scale IWSLT’17 {Arabic (Ar), Japanese (Ja), Korean (Ko), Chinese (Zh)}-to-English (En) translation tasks, medium-scale NIST Chinese-English (ZhEn) translation task, and large-scale WMT’14 English-German (En-De) translation task. For the IWSLT {Ar, Ja, Ko, Zh}-to-En translation tasks, there are respectively 236K, 234K, 227K, and 235K sentence pairs in each training set.4 The validation set is IWSLT17.TED.tst2014 and the test set is IWSLT17.TED.tst2015. For each language, we learn a BPE model with 16K merge operations (Sennrich et al., 2016b). For the NIST Zh-En translation task, the training corpus consists of 1.25M sentence pairs with 27.9M Chinese words and 34.5M English words. We use the NIST MT06 dataset as the validation set and the test sets are the NIST MT02, MT03, MT04, MT05, MT08 datasets. To compare with the recent works, the vocabulary size is limited to 4 https://wit3.fbk.eu/mt.php?release= 2017-01-trnted 3616 Architecture SMT* RNNsearch* Transformer Zh⇒En Vanilla Source bridging Target bridging Direct bridging Vanilla Direct bridging Decoder WT Shared-private Params 74.8M 78.5M 76.6M 78.9M 90.2M 90.5M 74.9M 62.8M E"
P19-1352,P10-1040,0,0.341652,"Missing"
P19-1352,D18-1100,0,0.0187321,"the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embedding matrices. As shown in Figure 3, the source embedding Ex ∈ R|V |×d is computed as follows:: Ex = Exlm ⊕ Exwf ⊕ Exur (2) where ⊕ is the row concatenation operator. Ex(·) ∈ R|V(·) |×d represents the word embeddings of the source words belong to different categories, e.g. lm represents the words with similar lexical meaning. |V(·) |denotes the vocabulary size of the corresponding category. The process of feature sharing is also implemented by matrix co"
P19-1352,D17-1154,0,0.0174627,"w that our model with fewer parameters yields consistent improvements over the strong Transformer baselines. 2 Approach In monolingual vector space, similar words tend to have commonalities in the same dimensions of their word vectors (Mikolov et al., 2013). These commonalities include: (1) a similar degree (value) of the same dimension and (2) a similar positive or negative correlation of the same dimension. Many previous works have noticed this phenomenon and have proposed to use shared vectors to represent similar words in monolingual vector space toward model compression (Li et al., 2016; Zhang et al., 2017b; Li et al., 2018). Motivated by these works, in NMT, we assume that the source and target words that have similar characteristics should also have similar vectors. Hence, we propose to perform this sharing technique in bilingual vector space. More precisely, we share the features (dimensions) between the paired source and target embeddings (vectors). However, in contrast to the previous studies, we also model the private features of the word embedding to preserve the private characteristics of words for source and target languages. The private 3614 features allow the words to better learn th"
P19-1352,C16-1291,0,0.0216196,"tures that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate translation results with the assistance of prior guidance. We provide an alternative that integrates the prior alignments through the sharing of features, which can also leads to a reduction of model parameters. Kuang et al. (2018) propose to shorten the path length between the related source and target embeddings to enhance the embedding layer. We believe that the shared features can be seem as the zero distance between the paired word embeddings. Our proposed method also uses several ideas from the three-way WT m"
P19-1352,P16-1100,0,0.041235,"ion. These words are often treated as noises and they are generally ignored 3615 Pxlm ∈ R2×2 Slm ∈ R2×3 Long (Lange) Ju@@( (Ju@@) Sur ∈ R2×1 Sundial (Fiehlt) Exwf ∈ R2×5 ⊕ → ˜ ⊕ De@@( (De@@) Laden (Bericht) Long Pxwf ∈ R2×3 Swf ∈ R2×2 Ex ∈ R6×5 → ˜ ⊕ Italy (Italien) Exlm ∈ R2×5 Italy → Ju@@ De@@ Pxur ∈ R2×4 ˜ ⊕ Exur ∈ R2×5 ⊕ Laden Sundial → Figure 3: The example of assembling the source word embedding matrix. The words in parentheses denote the paired words sharing features with them. by the NMT systems (Feng et al., 2017). Motivated by the frequency clustering methods proposed by Chen et al. (2016) where they cluster the words with similar frequency for training a hierarchical language model, in this work, we propose to use a small vector to model the possible features that might be shared between the source and target words which are unrelated but having similar word frequencies. In addition, it can be regarded as a way to improve the robustness of learning the embeddings of low-frequency words because of the noisy dimensions (Wang et al., 2018). 2.2 3 Implementation Before looking up embedding at each training step, the source and target embedding matrix are assembled by the sub-embed"
P19-1352,D16-1249,0,0.0233985,"the bilingual features that are cooperatively learned by the source and target words. Recently, Gu et al. (2018) propose to use the pre-trained target (English) embeddings as a universal representation to improve the representation learning of the source (low-resource) languages. 3620 In our work, both the source and target embeddings can make use of the common representation unit, i.e. the source and target embedding help each other to learn a better representation. The previously proposed methods have shown the effectiveness of integrating prior word alignments into the attention mechanism (Mi et al., 2016; Liu et al., 2016; Cheng et al., 2016; Feng et al., 2017), leading to more accurate and adequate translation results with the assistance of prior guidance. We provide an alternative that integrates the prior alignments through the sharing of features, which can also leads to a reduction of model parameters. Kuang et al. (2018) propose to shorten the path length between the related source and target embeddings to enhance the embedding layer. We believe that the shared features can be seem as the zero distance between the paired word embeddings. Our proposed method also uses several ideas from"
P19-1352,D14-1179,0,\N,Missing
P19-1352,Q17-1024,0,\N,Missing
W06-0141,W03-1728,0,0.0201229,"sed on Support Vector Machines (SVMs), a basic segmenter is designed regarding Chinese word segmentation as a problem of character-based tagging. Moreover, we proposed postprocessing rules specially taking into account the properties of results brought out by the basic segmenter. Our system achieved good ranks in all four corpora. 1 1.2 Feature Templates We utilized four of the five basic feature templates suggested in (Low et al. , 2005), described as follows: • Cn (n = −2, −1, 0, 1, 2) SVM-based Chinese Word Segmenter • Cn Cn + 1(n = −2, −1, 0, 1) We built out segmentation system following (Xue and Shen, 2003), regarding Chinese word segmentation as a problem of character-based tagging. Instead of Maximum Entropy, we utilized Support Vector Machines as an alternate. SVMs are a state-of-the-art learning algorithm, owing their success mainly to the ability in control of generalization error upper-bound, and the smooth integration with kernel methods. See details in (Vapnik, 1995). We adopted svm-light1 as the specific implementation of the model. • Pu (C0 ) • T (C−2 )T (C−1 )T (C0 )T (C1 )T (C2 ) where C refers to a Chinese character. The first two templates specify a context window with the size of"
W06-0141,I05-3025,0,0.026989,"Missing"
W06-0141,J95-4004,0,0.0609134,"). In our system, we first gathered all the words from the training data whose length are greater than six Chinese characters, filtering out dates and numbers, which was covered by Finite State Automation as a pre-processing stage. For each words collected, regard the first two and three characters as NE prefix, which indicates the beginning of a Name Entity. The collection of prefixes is termed Sp(ref ix) . Analogously, the collection Ss(uf f ix) of suffixes is brought up in the same way. Obviously not all the prefixes (suffixes) are good indicators for Name Entities. Partly inheriting from (Brill, 1995), we applied error-driven learning to filter prefixes in Sp and suffixes in Ss . Specifically, if a prefix and a suffix are both matched in a sequence, all the characters between them, together with the prefix and the suffix, are merged into a single segmentation unit. The resulted unit is compared with corresponding sequence in training data. If they were not exactly matched, the prefix and suffix were removed from collections respectively. Finally resulted Sp and Ss are utilized to recognize Name Entities in the initial segmentation results. 2.2 OOV Rules The following rules are termed OOV r"
W09-3532,W03-1501,0,0.519236,"Missing"
W09-3532,P08-1113,0,0.0518888,"Missing"
W09-3532,P08-1062,0,0.156295,"Missing"
W09-3532,E03-1035,0,0.220912,"Missing"
W09-3532,W03-1502,0,0.409059,"Missing"
W09-3532,N04-1036,0,0.326397,"Missing"
W09-3532,P02-1051,0,0.300322,"Missing"
W09-3532,H05-1061,0,0.204608,"is is very helpful for the mining phase. 3.2.2 Bilingual Expansion Given an input organization name ni , suppose si is one of its candidate translations, we expand si with ni to form a query “ si + ni ”. This kind of expansion is called as bilingual expansion. Bilingual expansion is very useful to verify whether a candidate translation is the correct translation. To give readers more information or they are not sure about the translation of original named entity, the Chinese authors usually include both the original form of a named entity and its translation in the mix-language web pages [Fei Huang et al, 2005]. So the correct translation pair is likely to obtain more supports from the returned web pages than those incorrect translation pairs. Thus bilingual expansion is very useful for the re-ranking phase. Besides, for an input organization name, if one of its incorrect candidate translations si is very person name and can be translated by Pinyin mapping. Mining When the expanded queries are submitted to search engine, the correct translation of the input organization name may be contained in the returned web pages. Because the translation of an organization name must be also an organization name"
W09-3532,W04-3248,0,0.404107,"Missing"
W09-3532,W05-0712,0,0.229192,"prepositions and articles) for an input organization name. For example, the organization name “中国建设银行/China Construction Bank” and the organization name “中国农业银 行/Agricultural Bank of China”, they are very similar both in surface forms and in syntax structures, but their translation orders are different, and their treatments of particles are also different. Generally, there are two strategies usually used for named entity translation in previous research. One is alignment based approach, and the other is generation based approach. Alignment based approach (Chen et al. 2003; Huang et al. 2003; Hassan and Sorensen, 2005; and so on) extracts named entities translation pairs from parallel or comparable corpus by some alignment technologies, and this approach is not suitable to solve the above two problems. Because new organization names are constantly being created, and alignment based method usually fails to cover these new organization names that don’t occur in the bilingual corpus. Traditional generation based approach (AlOnaizan and Knight, 2002; Jiang et al .2007; Yang et al. 2008; and so on) usually consists of two parts. Firstly, it will generate some candidate translations for the input; then it will r"
W09-3532,lee-etal-2004-alignment,0,0.767348,"Missing"
W09-3532,J93-2003,0,\N,Missing
W09-3532,P06-1142,0,\N,Missing
W09-3532,W03-0317,0,\N,Missing
W09-3532,P03-1021,0,\N,Missing
W10-4110,P91-1022,0,0.515199,"Missing"
W10-4110,J93-2003,0,0.0453684,"Missing"
W10-4110,2007.mtsummit-papers.9,0,0.0661439,"Missing"
W10-4110,P93-1002,0,0.315112,"Missing"
W10-4110,J07-2003,0,0.119073,"Missing"
W10-4110,P91-1023,0,0.712647,"Chinese bilingual patents. To our knowledge, this is the largest single parallel corpus in terms of sentence pairs. Moreover, we estimate the potential for mining multilingual parallel corpora involving English, Chinese, Japanese, Korean, German, etc., which would to some extent reduce the parallel data acquisition bottleneck in multilingual information processing. 1 Introduction Multilingual data are critical resources for building many applications, such as machine translation (MT) and cross-lingual information retrieval. Many parallel corpora have been built, such as the Canadian Hansards (Gale and Church, 1991), the Europarl corpus (Koehn, 2005), the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation. However, few parallel corpora exist for many language pairs, such as Chinese-Japanese, Japanese-Korean, ChineseFrench or Japanese-German. Even for language pairs with several parallel corpora, such as Chinese-English and Arabic-English, the size of parallel corpora is still a major limitation for SMT systems to achieve higher performance. In this paper, we present a way which could, to some extent, reduce the parallel data acquisition bottleneck in multilingual lang"
W10-4110,2001.mtsummit-papers.30,0,0.151725,"ice and then file its international application also in Chinese under the PCT. Later on, it may have the patent translated into English and file it in USA patent office, which means the patent becomes bilingual. If the applicant continues to file it in Japan with Japanese, it would be trilingual. Even more, it would be quadrilingual or involve more languages when it is filed in other countries with more languages. Such multilingual patents are considered comparable (or noisy parallel) because they are not parallel in the strict sense but still closely related in terms of information conveyed (Higuchi et al., 2001; Lu et al., 2009). 4 A Large English-Chinese Parallel Corpus Mined from Bilingual Patents In this section, we introduce the English-Chinese bilingual patents harvested from the Web and the method to mine parallel sentences from them. SMT experiments on the final parallel corpus are also described. 4.1 Harvesting English-Chinese Bilingual Patents The official patent office in China is the State Intellectual Property Office (SIPO). In early 2009, by searching on its website, we found about 200K Chinese patents previously filed as PCT applications in English and crawled their bibliographical dat"
W10-4110,2005.mtsummit-papers.11,0,0.0292542,"this is the largest single parallel corpus in terms of sentence pairs. Moreover, we estimate the potential for mining multilingual parallel corpora involving English, Chinese, Japanese, Korean, German, etc., which would to some extent reduce the parallel data acquisition bottleneck in multilingual information processing. 1 Introduction Multilingual data are critical resources for building many applications, such as machine translation (MT) and cross-lingual information retrieval. Many parallel corpora have been built, such as the Canadian Hansards (Gale and Church, 1991), the Europarl corpus (Koehn, 2005), the Arabic-English and English-Chinese parallel corpora used in the NIST Open MT Evaluation. However, few parallel corpora exist for many language pairs, such as Chinese-Japanese, Japanese-Korean, ChineseFrench or Japanese-German. Even for language pairs with several parallel corpora, such as Chinese-English and Arabic-English, the size of parallel corpora is still a major limitation for SMT systems to achieve higher performance. In this paper, we present a way which could, to some extent, reduce the parallel data acquisition bottleneck in multilingual language processing. Based on multiling"
W10-4110,P07-2045,0,0.00362542,"Missing"
W10-4110,P08-1113,0,0.0425735,"Missing"
W10-4110,P09-1098,0,0.0330593,"Missing"
W10-4110,2009.mtsummit-wpt.3,1,0.729366,"international application also in Chinese under the PCT. Later on, it may have the patent translated into English and file it in USA patent office, which means the patent becomes bilingual. If the applicant continues to file it in Japan with Japanese, it would be trilingual. Even more, it would be quadrilingual or involve more languages when it is filed in other countries with more languages. Such multilingual patents are considered comparable (or noisy parallel) because they are not parallel in the strict sense but still closely related in terms of information conveyed (Higuchi et al., 2001; Lu et al., 2009). 4 A Large English-Chinese Parallel Corpus Mined from Bilingual Patents In this section, we introduce the English-Chinese bilingual patents harvested from the Web and the method to mine parallel sentences from them. SMT experiments on the final parallel corpus are also described. 4.1 Harvesting English-Chinese Bilingual Patents The official patent office in China is the State Intellectual Property Office (SIPO). In early 2009, by searching on its website, we found about 200K Chinese patents previously filed as PCT applications in English and crawled their bibliographical data, titles, abstrac"
W10-4110,ma-2006-champollion,0,0.291728,"duced in Section 4, followed by the quantity estimation of multilingual patents involving other language pairs in Section 5. We discuss the results in Section 6, and conclude in Section 7. 2 Related Work Parallel sentences could be extracted from parallel documents or comparable corpora. Different approaches have been proposed to align sentences in parallel documents consisting of the same content in different languages based on the following information: a) the sentence length in bilingual sentences (Brown et al. 1991; Gale and Church, 1991); b) lexical information in bilingual dictionaries (Ma, 2006); c) statistical translation model (Chen, 1993), or the composite of more than one approach (Simard and Plamondon, 1998; Moore, 2002). To overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, which raises further challenges since the bilingual contents are not strictly parallel. For instance, Zhao and Vogel (2002) investigated the mining of parallel sentences for Web bilingual news. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maxi"
W10-4110,moore-2002-fast,0,0.149931,"scuss the results in Section 6, and conclude in Section 7. 2 Related Work Parallel sentences could be extracted from parallel documents or comparable corpora. Different approaches have been proposed to align sentences in parallel documents consisting of the same content in different languages based on the following information: a) the sentence length in bilingual sentences (Brown et al. 1991; Gale and Church, 1991); b) lexical information in bilingual dictionaries (Ma, 2006); c) statistical translation model (Chen, 1993), or the composite of more than one approach (Simard and Plamondon, 1998; Moore, 2002). To overcome the lack of parallel documents, comparable corpora are also used to mine parallel sentences, which raises further challenges since the bilingual contents are not strictly parallel. For instance, Zhao and Vogel (2002) investigated the mining of parallel sentences for Web bilingual news. Munteanu and Marcu (2005) presented a method for discovering parallel sentences in large Chinese, Arabic, and English comparable, non-parallel corpora based on a maximum entropy classifier. Cao et al., (2007) and Lin et al., (2008) proposed two different methods utilizing the parenthesis pattern to"
W10-4110,J05-4003,0,0.0723704,"Missing"
W10-4110,J04-4002,0,0.170472,"Missing"
W10-4110,2007.mtsummit-papers.63,0,0.11045,"sion The websites from which the Chinese and English patents were downloaded were quite slow to access, and were occasionally down during access. To avoid too much workload for the websites, the downloading speed had been limited. Some large patents would cost much time for the websites to respond and had be specifically handled. It took considerable efforts to obtain these comparable patents. In addition our English-Chinese corpus mined in this study is at least one order of magnitude larger, we give some other differences between ours and those introduced in Section 2 (Higuchi et al., 2001; Utiyama and Isahara, 2007; Lu et al, 2009) 1) Their bilingual patents were identified by the priority information in the US patents, and could not be easily extended to language pairs without English; while our method using PCT applications as the pivot could be easily extended to other language pairs as illustrated in Section 5. 2) The translation process is different: their patents were filed in USA Patent Office in English by translating from Japanese or Chinese, while our patents were first filed in English as a PCT application, and later translated into Chinese. The different translation processes may have differ"
W10-4110,J93-1004,0,\N,Missing
W10-4135,N06-2049,0,0.0601164,"allengeable setting, where training and test data are obtained from different domains. This setting is widely known as domain adaptation. For domain adaptation, either a large-scale unlabeled target domain data or a small size of labeled target domain data is required to adapt a system built on source domain data to the target domain. In this word segmentation competition, unfortunately, only a small size of unlabeled target domain data is available. Thus we focus on handling out-of-vocabulary (OOV) words. For this purpose, our system is based on a combination of subword-based tagging method (Zhang et al., 2006) and accessor variety-based new word recognition method (Feng et al., 2004). In more detail, we adopted and extended subword-based method. Subword list is augmented with newword list recognized by accessor variety method. Description unigram of characters bigram of characters trigram of characters whether punctuation type of characters Table 1: Basic Features for CRF-based Segmenter We participated in the close track of the word segmentation competition, on all the four test datasets, in two of which our system is ranked at the 1st position with respect to the metric of OOV recall. 2 2.1 Syste"
W10-4135,W06-0127,0,0.0633504,"Missing"
W10-4135,I08-4017,0,0.0212503,"ed with newword list recognized by accessor variety method. Description unigram of characters bigram of characters trigram of characters whether punctuation type of characters Table 1: Basic Features for CRF-based Segmenter We participated in the close track of the word segmentation competition, on all the four test datasets, in two of which our system is ranked at the 1st position with respect to the metric of OOV recall. 2 2.1 System Description Subword-based Tagging with CRFs The backbone of our system is a character-based segmenter with the application of Conditional Random Fields (CRFs) (Zhao and Kit, 2008). In detail, we apply a six-tag tagging scheme, as in (Zhao et al., 2006). That is , each Chinese character can be assigned to one of the tags in {B, B2 , B3 , M , E, S }. Refer to (Zhao et al., 2006) for detailed meaning of the tags. Table 1 shows basic feature templates used in our system, where feature templates a, b, d, e are also used in (Zhu et al., 2006) for SVM-based word segmentation. In order to extend basic CRF-based segmenter, we first collect 2k most frequent words from training data. Hereafter, the list of such words is referred to as subword list. Moreover, singlecharacter words"
W10-4135,W06-0141,1,0.845613,"at the 1st position with respect to the metric of OOV recall. 2 2.1 System Description Subword-based Tagging with CRFs The backbone of our system is a character-based segmenter with the application of Conditional Random Fields (CRFs) (Zhao and Kit, 2008). In detail, we apply a six-tag tagging scheme, as in (Zhao et al., 2006). That is , each Chinese character can be assigned to one of the tags in {B, B2 , B3 , M , E, S }. Refer to (Zhao et al., 2006) for detailed meaning of the tags. Table 1 shows basic feature templates used in our system, where feature templates a, b, d, e are also used in (Zhu et al., 2006) for SVM-based word segmentation. In order to extend basic CRF-based segmenter, we first collect 2k most frequent words from training data. Hereafter, the list of such words is referred to as subword list. Moreover, singlecharacter words 1 , if they are not contained in the subword list, are also added. Such proce1 By single-character word, we refer to words that consist solely of a Chinese character. Feature Template f) in(str, subword-list) g) in(str, confident-word-list) Description is str in subword list is str in confident-word list Table 2: Subword Features for CRF-based Segmenter way st"
W10-4143,P05-1022,0,0.0787522,"natural language processing. The evaluation-driven methodology is a good way to spur the its development. Two main parts of the method are a benchmark database and several well-designed evaluation metrics. Its feasibility has been proven in the English language. After the release of the Penn Treebank (PTB) (Marcus et al., 1993) and the PARSEVAL metrics (Black et al., 1991), some new corpusbased syntactic parsing techniques were explored in the English language. Based on them, many state-of-art English parser were built, including the well-known Collins parser (Collins, 2003), Charniak parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov and Klein, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. These include Malt Parser (Nivre et al., 2007), MSTParser (McDonald et al., 2005), Stanford Parser (Klein and Manning, 2003) and C&C Parser (Clark and Curran, 2007). Based on the Penn Chinese Treebank (CTB) (Xue et al., 2002) developed on the similar annoJingbo Zhu Natura"
W10-4143,J07-4004,0,0.03613,"rt English parser were built, including the well-known Collins parser (Collins, 2003), Charniak parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov and Klein, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. These include Malt Parser (Nivre et al., 2007), MSTParser (McDonald et al., 2005), Stanford Parser (Klein and Manning, 2003) and C&C Parser (Clark and Curran, 2007). Based on the Penn Chinese Treebank (CTB) (Xue et al., 2002) developed on the similar annoJingbo Zhu Natural Language Processing Lab. Northeastern University zhujingbo@mail.neu.edu.cn tation scheme of PTB, these parsing techniques were also transferred to the Chinese language. (Levy and Manning, 2003) explored the feasibility of applying lexicalized PCFG in Chinese. (Li et al., 2010) proposed a joint syntactic and semantic model for parsing Chinese. But till now, there is not a good Chinese parser whose performance can approach the state-of-art English parser. It is still an open challenge fo"
W10-4143,P03-1054,0,0.0219672,"language. Based on them, many state-of-art English parser were built, including the well-known Collins parser (Collins, 2003), Charniak parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov and Klein, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. These include Malt Parser (Nivre et al., 2007), MSTParser (McDonald et al., 2005), Stanford Parser (Klein and Manning, 2003) and C&C Parser (Clark and Curran, 2007). Based on the Penn Chinese Treebank (CTB) (Xue et al., 2002) developed on the similar annoJingbo Zhu Natural Language Processing Lab. Northeastern University zhujingbo@mail.neu.edu.cn tation scheme of PTB, these parsing techniques were also transferred to the Chinese language. (Levy and Manning, 2003) explored the feasibility of applying lexicalized PCFG in Chinese. (Li et al., 2010) proposed a joint syntactic and semantic model for parsing Chinese. But till now, there is not a good Chinese parser whose performance can approach the state-of-art English"
W10-4143,J03-4003,0,0.037279,"echnique in the research area of natural language processing. The evaluation-driven methodology is a good way to spur the its development. Two main parts of the method are a benchmark database and several well-designed evaluation metrics. Its feasibility has been proven in the English language. After the release of the Penn Treebank (PTB) (Marcus et al., 1993) and the PARSEVAL metrics (Black et al., 1991), some new corpusbased syntactic parsing techniques were explored in the English language. Based on them, many state-of-art English parser were built, including the well-known Collins parser (Collins, 2003), Charniak parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov and Klein, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. These include Malt Parser (Nivre et al., 2007), MSTParser (McDonald et al., 2005), Stanford Parser (Klein and Manning, 2003) and C&C Parser (Clark and Curran, 2007). Based on the Penn Chinese Treebank (CTB) (Xue et al., 2002)"
W10-4143,J07-3004,0,0.0474427,"sh language. After the release of the Penn Treebank (PTB) (Marcus et al., 1993) and the PARSEVAL metrics (Black et al., 1991), some new corpusbased syntactic parsing techniques were explored in the English language. Based on them, many state-of-art English parser were built, including the well-known Collins parser (Collins, 2003), Charniak parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov and Klein, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. These include Malt Parser (Nivre et al., 2007), MSTParser (McDonald et al., 2005), Stanford Parser (Klein and Manning, 2003) and C&C Parser (Clark and Curran, 2007). Based on the Penn Chinese Treebank (CTB) (Xue et al., 2002) developed on the similar annoJingbo Zhu Natural Language Processing Lab. Northeastern University zhujingbo@mail.neu.edu.cn tation scheme of PTB, these parsing techniques were also transferred to the Chinese language. (Levy and Manning, 2003) explored the feasibility of applying lexicalized PCFG in C"
W10-4143,P03-1056,0,0.157706,"Missing"
W10-4143,P10-1113,0,0.048748,"Missing"
W10-4143,H05-1066,0,0.0251742,"g techniques were explored in the English language. Based on them, many state-of-art English parser were built, including the well-known Collins parser (Collins, 2003), Charniak parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov and Klein, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. These include Malt Parser (Nivre et al., 2007), MSTParser (McDonald et al., 2005), Stanford Parser (Klein and Manning, 2003) and C&C Parser (Clark and Curran, 2007). Based on the Penn Chinese Treebank (CTB) (Xue et al., 2002) developed on the similar annoJingbo Zhu Natural Language Processing Lab. Northeastern University zhujingbo@mail.neu.edu.cn tation scheme of PTB, these parsing techniques were also transferred to the Chinese language. (Levy and Manning, 2003) explored the feasibility of applying lexicalized PCFG in Chinese. (Li et al., 2010) proposed a joint syntactic and semantic model for parsing Chinese. But till now, there is not a good Chinese parser whose perform"
W10-4143,J93-2004,0,0.033614,"ntroduced the task designing ideas, data preparation methods, evaluation metrics and results of the second Chinese syntactic parsing evaluation (CIPS-Bakeoff-ParsEval-2010) jointed with SIGHAN Bakeoff tasks. 1 Introduction Syntactic parsing is an important technique in the research area of natural language processing. The evaluation-driven methodology is a good way to spur the its development. Two main parts of the method are a benchmark database and several well-designed evaluation metrics. Its feasibility has been proven in the English language. After the release of the Penn Treebank (PTB) (Marcus et al., 1993) and the PARSEVAL metrics (Black et al., 1991), some new corpusbased syntactic parsing techniques were explored in the English language. Based on them, many state-of-art English parser were built, including the well-known Collins parser (Collins, 2003), Charniak parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov and Klein, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG forma"
W10-4143,N07-1051,0,0.0284856,"n methodology is a good way to spur the its development. Two main parts of the method are a benchmark database and several well-designed evaluation metrics. Its feasibility has been proven in the English language. After the release of the Penn Treebank (PTB) (Marcus et al., 1993) and the PARSEVAL metrics (Black et al., 1991), some new corpusbased syntactic parsing techniques were explored in the English language. Based on them, many state-of-art English parser were built, including the well-known Collins parser (Collins, 2003), Charniak parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov and Klein, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. These include Malt Parser (Nivre et al., 2007), MSTParser (McDonald et al., 2005), Stanford Parser (Klein and Manning, 2003) and C&C Parser (Clark and Curran, 2007). Based on the Penn Chinese Treebank (CTB) (Xue et al., 2002) developed on the similar annoJingbo Zhu Natural Language Processing Lab. Northeastern Unive"
W10-4143,C02-1145,0,0.0655702,"er (Collins, 2003), Charniak parser (Charniak and Johnson, 2005) and Berkeley parser (Petrov and Klein, 2007). By automatically transforming the constituent structure trees annotated in PTB to other linguistic formalisms, such as dependency grammar, and combinatory categorical grammar (Hockenmaier and Steedman, 2007), many syntactic parser other than the CFG formalism were also developed. These include Malt Parser (Nivre et al., 2007), MSTParser (McDonald et al., 2005), Stanford Parser (Klein and Manning, 2003) and C&C Parser (Clark and Curran, 2007). Based on the Penn Chinese Treebank (CTB) (Xue et al., 2002) developed on the similar annoJingbo Zhu Natural Language Processing Lab. Northeastern University zhujingbo@mail.neu.edu.cn tation scheme of PTB, these parsing techniques were also transferred to the Chinese language. (Levy and Manning, 2003) explored the feasibility of applying lexicalized PCFG in Chinese. (Li et al., 2010) proposed a joint syntactic and semantic model for parsing Chinese. But till now, there is not a good Chinese parser whose performance can approach the state-of-art English parser. It is still an open challenge for parsing Chinese sentences due to some special characteristi"
W10-4153,I05-3015,1,0.75902,"n NEUKD are shown in Table 1. Domain associated term 足球队(football team) 自行车队 (cycling team) 中国象棋 (Chinese chess) 执白(white side) 芝加哥公牛 (Chicago bulls) Domain feature concept Football, Sports Traffic, Sports, cycling Sports, Chinese chess Sports, the game of go Sports, basketball Table 1: Six examples defined in the NEUKD In the domain knowledge based topic identification algorithm, all domain associated terms occurring in the given document are first mapped into domain features such as football, basketball or cycling. The most frequent domain feature is considered as the most likely topic. See Zhu and Chen (2005) for details. Two documents with the same topic can be grouped into the same cluster. Person name 杨波 杨波 杨波 杨波 Document no. 081 sports 篮球 094 098 100 射箭 射箭 射箭 weight of a word with the POS of “ns”or ”nh ” is set to be 2, the ones of “ni” POS is set to 1.5, otherwise 1.0. Algorithm 1: Multi-stage Clustering Framework Input: a person name pn, and its related document set D={d1, d2, …, dm} in which each document di contains the person name pn; Output: clustering results C={C1,C2, …,Cn}, where ∪ Ci = C and C i ∩ C j = Φ i Table 2: Examples of PND on Sportsman Class 2.4.3 Multi-Stage Clustering Fram"
W10-4168,H05-1097,0,0.0601431,"Missing"
W10-4168,P95-1026,0,0.398386,"Missing"
W10-4168,P96-1006,0,0.281134,"Missing"
W10-4168,P04-3026,0,0.0410563,"Missing"
W10-4168,J98-1004,0,\N,Missing
W12-6336,P05-1022,0,0.105137,"Missing"
W12-6336,W09-1008,0,0.0410594,"Missing"
W12-6336,P05-1010,0,0.0227823,"CFG model. However, the head driven methods may not suitable for the current task for two reasons. (1) to acquire the bi-lexical dependencies, a set of manually collected head finding rules are needed. To our knowledge, there is not such set of rules for the Sinica Treebank. (2) some of the information utilized by the head-driven model are specially designed for the Penn Treebank annotation scheme and when shifted to other annotation schemes, parsing performance dramatically decreases (Guldea, 2001). Rather than using the bi-lexical dependencies, unlexicalized methods (Klein and Manning 2003; Matsuzaki et al., 2005; Petrov et al., 2006) augment the non-terminals of the PCFG model with latent annotations, PCFGLA hereafter. Those latent annotations are aimed to capture different behavioral preferences of the same non-terminal or production rule in different local context. For example, verb phrases are further split into several subcategories that capture the behavioral preference of infinitive VPs, passive VPs and intransitive VPs. With those latent annotations, parsing performance is greatly improved. Among the unlexicalized methods listed above, the one proposed by Petrov et al., (2006) can learn the la"
W12-6336,N07-1051,0,0.0488447,"Missing"
W12-6336,W01-0521,0,\N,Missing
W12-6336,J03-4003,0,\N,Missing
W12-6336,P03-1054,0,\N,Missing
W12-6336,P06-1055,0,\N,Missing
W18-6430,W17-4739,0,0.0438288,"there is the Pas@@ port , which was released last September . so there is the Passport , which was released last September . Furious residents have savaged Sol@@ i@@ hull Council saying it was “ useless at dealing with the problem ”. 愤怒的居民猛烈抨击了 S@@ ol@@ i@@ h@@ ou@@ s@@ 委员会, 称它 “ 在处理这个问题上是无用的” 。 愤怒的居民猛烈抨击了 Solihull 委员会, 称它 “ 在处理这个问题上是无用的” 。 Table 1: Samples of the inconsistent translation of the constant literal between source and target sentence. The subword is split by “@@”. The two samples are picked up from newstest2018. proved effective in the WMT competitions (Sennrich and Haddow, 2016; Sennrich et al., 2017; Wang et al., 2017). Existing experimental results about ensemble decoding mainly concentrate upon a small number of models (e.g. 4 models (Wang et al., 2017; Sennrich et al., 2016c, 2017)). Besides, the ensembled models generally lack of sufficient diversity, for example, Sennrich et al. (2016c) use the last N checkpoints of a single training run, while Wang et al. (2017) use the same network architecture with different random initializations. In this paper, we study the effects of more diverse ensemble decoding from two perspectives: the number of models and the diversity of integrated mode"
W18-6430,P16-1009,0,0.589679,"nsformer architecture. We further improve the translation performance 2.4-2.6 BLEU points from four aspects, including architectural improvements, diverse ensemble decoding, reranking, and post-processing. Among constrained submissions, we rank 2nd out of 16 submitted systems on Chinese → English task and 3rd out of 16 on English → Chinese task, respectively. 1 Introduction Neural machine translation (NMT) exploits an encoder-decoder framework to model the whole translation process in an end-to-end fashion, and has achieved state-of-the-art performance in many language pairs (Wu et al., 2016; Sennrich et al., 2016c). This paper describes the submission of the NiuTrans neural machine translation system for the WMT 2018 Chinese ↔ English news translation tasks. Our baseline systems are based on the Transformer model due to the excellent translation performance and fast training thanks to the self-attention mechanism. Then we enhance it with checkpoint ensemble (Sennrich et al., 2016c) that averages the last N checkpoints of a single training run. To enable openvocabulary translation, all the words are segmented via byte pair encoding (BPE) (Sennrich et al., 2016b) for both Chinese and English. Also, we u"
W18-6430,D08-1024,0,0.0548014,"reranking and post-processing. For architectural improvements, we add relu dropout and attention dropout to improve the generalization ability and increase the inner dimension of feed-forward neural network to enlarge the model capacity (Hassan et al., 2018). We also use the novel Swish activation function (Ramachandran et al., 2018) and self-attention with relative positional representations (Shaw et al., 2018). Next, we explore more diverse ensemble decoding via increasing the number of models and using the models generated by different ways. Furthermore, at most 17 features tuned by MIRA (Chiang et al., 2008) are used to rerank the N-best hypotheses. At last, a post-processing algorithmic is proposed to correct the inconsistent English literals between the source and target sentence. Through these techniques, we can achieve 2.4-2.6 BLEU points improvement over the baselines. As a result, our systems rank the second out of 16 submitted systems on Chinese → English task and the third out of 16 on English → Chinese task among constrained submissions, respectively. This paper describes the submission of the NiuTrans neural machine translation system for the WMT 2018 Chinese ↔ English news translation"
W18-6430,P16-1162,0,0.758646,"nsformer architecture. We further improve the translation performance 2.4-2.6 BLEU points from four aspects, including architectural improvements, diverse ensemble decoding, reranking, and post-processing. Among constrained submissions, we rank 2nd out of 16 submitted systems on Chinese → English task and 3rd out of 16 on English → Chinese task, respectively. 1 Introduction Neural machine translation (NMT) exploits an encoder-decoder framework to model the whole translation process in an end-to-end fashion, and has achieved state-of-the-art performance in many language pairs (Wu et al., 2016; Sennrich et al., 2016c). This paper describes the submission of the NiuTrans neural machine translation system for the WMT 2018 Chinese ↔ English news translation tasks. Our baseline systems are based on the Transformer model due to the excellent translation performance and fast training thanks to the self-attention mechanism. Then we enhance it with checkpoint ensemble (Sennrich et al., 2016c) that averages the last N checkpoints of a single training run. To enable openvocabulary translation, all the words are segmented via byte pair encoding (BPE) (Sennrich et al., 2016b) for both Chinese and English. Also, we u"
W18-6430,N18-2074,0,0.117801,"Missing"
W18-6430,W17-4742,0,0.0210284,", which was released last September . so there is the Passport , which was released last September . Furious residents have savaged Sol@@ i@@ hull Council saying it was “ useless at dealing with the problem ”. 愤怒的居民猛烈抨击了 S@@ ol@@ i@@ h@@ ou@@ s@@ 委员会, 称它 “ 在处理这个问题上是无用的” 。 愤怒的居民猛烈抨击了 Solihull 委员会, 称它 “ 在处理这个问题上是无用的” 。 Table 1: Samples of the inconsistent translation of the constant literal between source and target sentence. The subword is split by “@@”. The two samples are picked up from newstest2018. proved effective in the WMT competitions (Sennrich and Haddow, 2016; Sennrich et al., 2017; Wang et al., 2017). Existing experimental results about ensemble decoding mainly concentrate upon a small number of models (e.g. 4 models (Wang et al., 2017; Sennrich et al., 2016c, 2017)). Besides, the ensembled models generally lack of sufficient diversity, for example, Sennrich et al. (2016c) use the last N checkpoints of a single training run, while Wang et al. (2017) use the same network architecture with different random initializations. In this paper, we study the effects of more diverse ensemble decoding from two perspectives: the number of models and the diversity of integrated models. We explore at mo"
W19-5325,D18-1045,0,0.0432723,"ta by mixing the pseudo corpus with the parallel part, in that the target side lexicon coverage is insufficient, such as EN ↔ {KK, GU} only consist of 0.11M and 0.5M bilingual data, respectively. How to select the appropriate sentences from the abundant monolingual data is a crucial issue due to the limitation of equipment and huge overhead time. We trained a 5-gram language model based on the mixture of development set and bilingual-target side data to score the monolingual sentences. In addition, considering the impact of sequence length, we set a threshold range from 10 to 50. Recent work (Edunov et al., 2018) has shown that different methods of generating pseudo corpus made discrepant influence on translation performance. Edunov et al. (2018) indicated that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. This year we attempted several data augmentation methods as follows: Greedy Based Ensemble Ensemble decoding is an effective system combination method to boost machine translation quality via integrating the predictions of several single models at each decode step. It has been proved effective in the past few years’ WMT tasks (Wa"
W19-5325,D16-1139,0,0.0400724,"knowledge distillation method to enhance the single model performance. Experiment results showed that the accuracy of the reverse model was extremely necessary, or you may even get worse results. Student 1 Iteration 2 Distillation Ensemble Figure 2: A simple example of Iterative Knowledge Distillation with 5 students, 2 teachers and 2 iterations 2015; Freitag et al., 2017) and ensemble iteratively. The naive approach started with a list of single model candidates as the students and the best 4 models combination retrieved from Algorithm 1 as the teacher. Sequence-level knowledge distillation (Kim and Rush, 2016) was then applied to finetune each student model with additional source data. With these enhanced student models, a stronger 4 models combination can be produced through Algorithm 1. We iterated this process until less than 0.1 BLEU improvement on the validation set. However, in the preliminary experiments we found that such iteration didn’t yield good results as we expected. We attributed this phenomenon to the deficiency of model diversity, due to the fact that all students were collapsed to a similar optimum induced by the same teacher they learnt from, which limited the potential gain from"
W19-5325,P19-1019,0,0.0718891,"ngual data. We also applied iterative knowledge distillation (Freitag et al., 2017) to leverage the source-side monolingual data. Our system also employed the conventional combination methods including ensemble and feature-based re-ranking to further improve the translation quality. We proposed a simple greedy search algorithm to find the best ensemble combination effectively and efficiently. Hypothesis combination (Hassan et al., 2018) was also adopted to generate more diverse hypotheses for better reranking. For unsupervised tasks, we mainly investigated the methodology of unsupervised SMT (Artetxe et al., 2019) and NMT (Lample and Conneau, 2019) to build our baselines, then presented a joint training strategy on top of these baselines to boost their performances. This paper was structured as follows: we described the details of our novel Deep-Transformer in Section 2, then in Section 3 we presented an overview of our universal training flow for all supervised language pairs and the unsupervised methods. The experiment settings and main results were shown in Section 4. This paper described NiuTrans neural machine translation systems for the WMT 2019 news translation tasks. We participated in 13 trans"
W19-5325,D18-1338,0,0.0219504,"dard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation methods (Sennrich et al., 2016b), including beam search, unrestricted sampling and sampling-topK proposed 2 Deep Transformer Neural machine translation models based on multi-layer self-attention (Vaswani et al., 2017) has shown strong results on several large-scale tasks. Enlarging the model capacity is an effective way to obtain stronger networks, including widening the hidden representation or deepening the model layers. Bapna et al. (2018) has shown that learning deeper networks is not easy for vanilla Transformer due to the gradient vanishing/exploding problem. 257 Proceedings of the Fourth Conference on Machine Translation (WMT), Volume 2: Shared Task Papers (Day 1) pages 257–266 c Florence, Italy, August 1-2, 2019. 2019 Association for Computational Linguistics xl F L yl LN unique edge representations per layer and head. Pre-Norm Transformer-DLCL: The Transformer-DLCL employed direct links with all the previous layers and offered efficient access to lower-level representations in a deep stack. An additional weight matrix Wl+"
W19-5325,J82-2005,0,0.707119,"Missing"
W19-5325,N12-1047,0,0.0587276,"he n-best hypothesis and the source sentence-level vectors (Hassan et al., 2018) . Sentence-Align Score: We used fast-align tool to evaluate the alignment probability between the source and the target. Translation Coverage: A SMT phrase-table to obtain the top-50 translation for each source-totarget word pair. In this way, the translation coverage score can be easily gained with respect to the dual direction hits in the dictionary with length normalization. We rescored 96-best outputs generated by several ensemble systems using a rescoring model consisting of features above by K-batched MIRA (Cherry and Foster, 2012) algorithm which is widely used in Moses4 . Feature Reranking This year we adopted an hypothesis combination strategy to pick up a potentially better translation from the N-best consisting of several different ensemble outputs. For example we generated 96 hypothesises by 8 different ensemble systems, and set the beam size as 12 during the decoding procedure instead of obtaining all 96 outputs from a sin4 https://github.com/moses-smt/ mosesdecoder 260 3.6 Unsupervised NMT our deep models can even outperform the standard Transformer-Big by 0.7-1.3 BLEU scores on different language pairs. All of"
W19-5325,W16-2323,0,0.221721,"e distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KK↔EN, GU→EN} directions, ranking 2nd in {RU→EN, DE↔CS} and 3rd in {ZH→EN, LT→EN, EN→RU, EN↔DE} among all constrained submissions. 1 Introduction Our NiuTrans team participated in 13 WMT19 shared news translation tasks, including 11 supervised and 2 unsupervised sub-tracks. We reused some effective approaches of our WMT18 submissions (Wang et al., 2018), including backtranslation by beam search (Sennrich et al., 2016b), BPE (Sennrich et al., 2016c) and further strengthened our systems by exploiting some new techniques this year. For our supervised task submissions, all the language pairs shared similar model architectures and training flow. We proposed four novel Deep-Transformer architectures based on (Wang et al., 2019) as our baseline, which outperformed the standard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation methods (Sennrich et al., 2016b), including beam search, unrestricted s"
W19-5325,W18-6408,0,0.0279134,"ifferent methods of generating pseudo corpus made discrepant influence on translation performance. Edunov et al. (2018) indicated that sampling or noisy synthetic data gives a much stronger training signal than data generated by beam or greedy search. This year we attempted several data augmentation methods as follows: Greedy Based Ensemble Ensemble decoding is an effective system combination method to boost machine translation quality via integrating the predictions of several single models at each decode step. It has been proved effective in the past few years’ WMT tasks (Wang et al., 2018; Deng et al., 2018; Junczys-Dowmunt, 2018; Sennrich et al., 2016a). We enhanced the single model by employing deep self-attentional models. Note that the improvement is poor if the single models performed strong enough and no significant benefits from increasing the participant quantity. So it’s necessary to utilize the models sufficiently to search for a better combination on the development set. We adopted an easily operable greedy-base strategy as the following: Algorithm 1 An Simple ensemble algorithm based on greedy search Input: a model list Ωcand sorted by the development scores. Output: a final model li"
W19-5325,P16-1009,0,0.552243,"e distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KK↔EN, GU→EN} directions, ranking 2nd in {RU→EN, DE↔CS} and 3rd in {ZH→EN, LT→EN, EN→RU, EN↔DE} among all constrained submissions. 1 Introduction Our NiuTrans team participated in 13 WMT19 shared news translation tasks, including 11 supervised and 2 unsupervised sub-tracks. We reused some effective approaches of our WMT18 submissions (Wang et al., 2018), including backtranslation by beam search (Sennrich et al., 2016b), BPE (Sennrich et al., 2016c) and further strengthened our systems by exploiting some new techniques this year. For our supervised task submissions, all the language pairs shared similar model architectures and training flow. We proposed four novel Deep-Transformer architectures based on (Wang et al., 2019) as our baseline, which outperformed the standard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation methods (Sennrich et al., 2016b), including beam search, unrestricted s"
W19-5325,P16-1162,0,0.845681,"e distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KK↔EN, GU→EN} directions, ranking 2nd in {RU→EN, DE↔CS} and 3rd in {ZH→EN, LT→EN, EN→RU, EN↔DE} among all constrained submissions. 1 Introduction Our NiuTrans team participated in 13 WMT19 shared news translation tasks, including 11 supervised and 2 unsupervised sub-tracks. We reused some effective approaches of our WMT18 submissions (Wang et al., 2018), including backtranslation by beam search (Sennrich et al., 2016b), BPE (Sennrich et al., 2016c) and further strengthened our systems by exploiting some new techniques this year. For our supervised task submissions, all the language pairs shared similar model architectures and training flow. We proposed four novel Deep-Transformer architectures based on (Wang et al., 2019) as our baseline, which outperformed the standard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation methods (Sennrich et al., 2016b), including beam search, unrestricted s"
W19-5325,N18-2074,0,0.0318833,"attentional counterparts in pre-norm way as default. In this section we described the details about our deep architectures as below: Pre-Norm Transformer: In recent Tensor2Tensor implementations2 , layer normalization (Lei Ba et al., 2016) was applied to the input of every sub-layer which the computation sequence could be expressed as: normalize→Transform→dropout→residual-add. In this way we could successfully train a deeper pre-norm Transformer within comparable performance with Transformer-Big or even better, only one fourth training cost. Pre-Norm Transformer-RPR: We found Transformer-RPR (Shaw et al., 2018) which simultaneously incorporating relative position information with sinusoidal position encodings for sequences in pre-norm style could outperform the pre-norm Transformer with the same encoder depth. We used clipping distance k = 20 with the 3 System Overview 3.1 Data Filter Previous work (Junczys-Dowmunt, 2018; Wang et al., 2018; Stahlberg et al., 2018) indicated that rigorous data filtering scheme is crucial, or it will lead to catastrophic loss in quality, especially in EN↔DE and EN↔RU. For most language pairs, we filter the training bilingual corpus with the following rules: • Normaliz"
W19-5325,W18-6427,0,0.0367684,"Missing"
W19-5325,W18-6430,1,0.915803,"ormer and several back-translation methods. Iterative knowledge distillation and ensemble+reranking were also employed to obtain stronger models. Our unsupervised submissions were based on NMT enhanced by SMT. As a result, we achieved the highest BLEU scores in {KK↔EN, GU→EN} directions, ranking 2nd in {RU→EN, DE↔CS} and 3rd in {ZH→EN, LT→EN, EN→RU, EN↔DE} among all constrained submissions. 1 Introduction Our NiuTrans team participated in 13 WMT19 shared news translation tasks, including 11 supervised and 2 unsupervised sub-tracks. We reused some effective approaches of our WMT18 submissions (Wang et al., 2018), including backtranslation by beam search (Sennrich et al., 2016b), BPE (Sennrich et al., 2016c) and further strengthened our systems by exploiting some new techniques this year. For our supervised task submissions, all the language pairs shared similar model architectures and training flow. We proposed four novel Deep-Transformer architectures based on (Wang et al., 2019) as our baseline, which outperformed the standard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation method"
W19-5325,P19-1176,1,0.921404,"ined submissions. 1 Introduction Our NiuTrans team participated in 13 WMT19 shared news translation tasks, including 11 supervised and 2 unsupervised sub-tracks. We reused some effective approaches of our WMT18 submissions (Wang et al., 2018), including backtranslation by beam search (Sennrich et al., 2016b), BPE (Sennrich et al., 2016c) and further strengthened our systems by exploiting some new techniques this year. For our supervised task submissions, all the language pairs shared similar model architectures and training flow. We proposed four novel Deep-Transformer architectures based on (Wang et al., 2019) as our baseline, which outperformed the standard Transformer-Big significantly in terms of both translation quality and convergence speed. As for the data augmentation aspect, we experimented several back-translation methods (Sennrich et al., 2016b), including beam search, unrestricted sampling and sampling-topK proposed 2 Deep Transformer Neural machine translation models based on multi-layer self-attention (Vaswani et al., 2017) has shown strong results on several large-scale tasks. Enlarging the model capacity is an effective way to obtain stronger networks, including widening the hidden r"
W19-5325,P12-3004,1,0.913581,"Missing"
Y18-1010,P05-1074,0,0.171026,"al and lexico-syntactic features of paraphrases, which produced more than 9,000 pairs of lexical paraphrases including relations other than synonymy. Human judgment, with or without context, was solicited. Lexical paraphrases, which often involve a replacement with synonyms or hypernyms, do not give a complete account of paraphrase itself or serve ap90 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 plications adequately. Phrasal and sentential paraphrases are indispensable (e.g. Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; CallisonBurch, 2008; Ganitkevitch et al., 2011). For phrasal paraphrases, syntactic categories often become a basic point of reference. Bannard and Callison-Burch (2005), for instance, used a bilingual parallel corpus and obtained English paraphrases by pivoting through foreign language phrases. With reference to phrase-based statistical machine translation, they aligned phrases in the corpus. Those mapping to the same phrase in another language are considered candidates and ranked by a paraphrase probability defined in terms of two translation model probabilities. The extracted paraphrases"
Y18-1010,N03-1003,0,0.0677347,"orithm based on contextual and lexico-syntactic features of paraphrases, which produced more than 9,000 pairs of lexical paraphrases including relations other than synonymy. Human judgment, with or without context, was solicited. Lexical paraphrases, which often involve a replacement with synonyms or hypernyms, do not give a complete account of paraphrase itself or serve ap90 32nd Pacific Asia Conference on Language, Information and Computation Hong Kong, 1-3 December 2018 Copyright 2018 by the authors PACLIC 32 plications adequately. Phrasal and sentential paraphrases are indispensable (e.g. Barzilay and Lee, 2003; Bannard and Callison-Burch, 2005; CallisonBurch, 2008; Ganitkevitch et al., 2011). For phrasal paraphrases, syntactic categories often become a basic point of reference. Bannard and Callison-Burch (2005), for instance, used a bilingual parallel corpus and obtained English paraphrases by pivoting through foreign language phrases. With reference to phrase-based statistical machine translation, they aligned phrases in the corpus. Those mapping to the same phrase in another language are considered candidates and ranked by a paraphrase probability defined in terms of two translation model probabi"
Y18-1010,P01-1008,0,0.182542,"ree translation from bilingual parallel corpora. Section 4 describes the experimental setup. Section 5 discusses preliminary results and future plans, followed by a conclusion in Section 6. 清晰的记忆 2 记得清清楚楚 Related work For two decades by now, methods on paraphrase extraction and generation have mostly been datadriven (Madnani and Dorr, 2010). Monolingual or bilingual corpora may be used, sometimes also with the help of existing lexical resources (e.g. Wu and Zhou, 2003). Earlier methods primarily rely on distributional similarity for finding paraphrases from identical surrounding context (e.g. Barzilay and McKeown, 2001; Ibrahim et al., 2003). For example, Barzilay and McKeown (2001) utilized a monolingual corpus consisting of multiple English translations of the same novels by foreign authors. The approach takes advantage of the many words shared by the parallel translations, assuming that phrases in aligned sentences appearing in similar contexts are paraphrases. They used a co-training algorithm based on contextual and lexico-syntactic features of paraphrases, which produced more than 9,000 pairs of lexical paraphrases including relations other than synonymy. Human judgment, with or without context, was s"
Y18-1010,D08-1021,0,0.0945071,"sed to literal translation) will be of more interest to us. Three types of paraphrases are often included in paraphrase databases (e.g. Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014): lexical, phrasal, and syntactic paraphrases. Although the inclusion of these paraphrase types has recognized that paraphrases could go beyond the replacement by synonyms or synonymous phrases, there is nevertheless a restriction of syntactic category, as it at most allows the substitution of expressions that match a given syntactic type, even when non-constituent phrases are accommodated (e.g. Callison-Burch, 2008), as long as labels in syntactic trees are used as the point of reference. Paraphrases in different phrasal categories and syntactic constructions are particularly important in translation, as individual possible renditions would have their strengths and weaknesses in a given context with a certain literary style for a specific communicative purpose. In addition to fidelity, this is even more salient when fluency is concerned across language pairs with very distinct linguistic properties, where literal translation is not always the 89 32nd Pacific Asia Conference on Language, Information and C"
Y18-1010,ganitkevitch-callison-burch-2014-multilingual,0,0.0557823,"hand, and for inspiring machine translation’s further improvement on fluency on the other. Paraphrase, in this paper, is therefore used in a slightly restricted sense to refer to alternative expressions in a target language which are not only semantically equivalent, but also fulfill other contextual criteria to be qualified as translation of the text in a source language. Along the continuum of equivalence, free translation (as opposed to literal translation) will be of more interest to us. Three types of paraphrases are often included in paraphrase databases (e.g. Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014): lexical, phrasal, and syntactic paraphrases. Although the inclusion of these paraphrase types has recognized that paraphrases could go beyond the replacement by synonyms or synonymous phrases, there is nevertheless a restriction of syntactic category, as it at most allows the substitution of expressions that match a given syntactic type, even when non-constituent phrases are accommodated (e.g. Callison-Burch, 2008), as long as labels in syntactic trees are used as the point of reference. Paraphrases in different phrasal categories and syntactic constructions are particularly important in tra"
Y18-1010,D11-1108,0,0.0229146,"Missing"
Y18-1010,N13-1092,0,0.0949831,"Missing"
Y18-1010,W03-1608,0,0.0417523,"al parallel corpora. Section 4 describes the experimental setup. Section 5 discusses preliminary results and future plans, followed by a conclusion in Section 6. 清晰的记忆 2 记得清清楚楚 Related work For two decades by now, methods on paraphrase extraction and generation have mostly been datadriven (Madnani and Dorr, 2010). Monolingual or bilingual corpora may be used, sometimes also with the help of existing lexical resources (e.g. Wu and Zhou, 2003). Earlier methods primarily rely on distributional similarity for finding paraphrases from identical surrounding context (e.g. Barzilay and McKeown, 2001; Ibrahim et al., 2003). For example, Barzilay and McKeown (2001) utilized a monolingual corpus consisting of multiple English translations of the same novels by foreign authors. The approach takes advantage of the many words shared by the parallel translations, assuming that phrases in aligned sentences appearing in similar contexts are paraphrases. They used a co-training algorithm based on contextual and lexico-syntactic features of paraphrases, which produced more than 9,000 pairs of lexical paraphrases including relations other than synonymy. Human judgment, with or without context, was solicited. Lexical parap"
Y18-1010,D15-1166,0,0.145894,"Missing"
Y18-1010,J10-3003,0,0.0641549,"iminary results show that the approach is promising and paraphrases at both sentential and sub-sentential levels covering diverse surface forms could be identified. The extracted data, upon further filtering, have great potential to supplement the example sentences available in existing bilingual dictionaries in an effective and systematic way. 1 Introduction In any language, the same meaning can often be expressed in alternative ways, or paraphrased. The recognition and generation of such meaningequivalent forms are deemed important for various natural language processing (NLP) applications (Madnani and Dorr, 2010). In the inter-lingual context, typically in translation, there is inevitably a trace of paraphrasing, whether or not it is employed consciously as a strategy in the process. Notwithstanding the different interpretation of terminology and research objectives across disciplines (e.g. translation vs paraphrase, literal translation vs free translation, etc. in translation studies, or paraphrase generation vs query expansion in the NLP community), access to such context-sensitive equivalents is essential especially when fluency, in addition to fidelity, is concerned, for machine and human translat"
Y18-1010,E17-1083,0,0.0383599,"aningpreserving transformations (Ganitkevitch et al., 2011). However, such constraints may not be appropriate for our current purpose, as the kind of paraphrase, or free translation, that we find useful often appears as different syntactic constructions on the one hand, and may not be accompanied by regular and predictable transformation patterns on the other. The pivoting approach has stayed in the mainstream of paraphrase extraction, with large paraphrase databases like the PPDB and multilingual PPDB created in the meantime (Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014). Mallinson et al. (2017) revisited the approach from the perspective of neural machine translation (NMT), without reference to any underlying grammar or creating any phrase table. In particular, pivoting is done with the NMT model, in the form of oneto-one back-translation or multi-pivoting through the K-best translations. Their system, PARANET, makes use of the attention mechanism for identifying semantically equivalent parts between the paraphrase sentence and the source sentence, with each word of the paraphrase sentence attending to words in the pivot sentence and each word in the pivot sentence attending to word"
Y18-1010,E17-3017,0,0.0370093,"Missing"
Y18-1010,P03-1016,0,0.059389,"ion 2, we will review related NLP work on paraphrase extraction and generation. In Section 3, we will introduce our proposed method for finding free translation from bilingual parallel corpora. Section 4 describes the experimental setup. Section 5 discusses preliminary results and future plans, followed by a conclusion in Section 6. 清晰的记忆 2 记得清清楚楚 Related work For two decades by now, methods on paraphrase extraction and generation have mostly been datadriven (Madnani and Dorr, 2010). Monolingual or bilingual corpora may be used, sometimes also with the help of existing lexical resources (e.g. Wu and Zhou, 2003). Earlier methods primarily rely on distributional similarity for finding paraphrases from identical surrounding context (e.g. Barzilay and McKeown, 2001; Ibrahim et al., 2003). For example, Barzilay and McKeown (2001) utilized a monolingual corpus consisting of multiple English translations of the same novels by foreign authors. The approach takes advantage of the many words shared by the parallel translations, assuming that phrases in aligned sentences appearing in similar contexts are paraphrases. They used a co-training algorithm based on contextual and lexico-syntactic features of paraphr"
