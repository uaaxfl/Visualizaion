2001.jeptalnrecital-tutoriel.3,H92-1086,0,0.0296487,"Missing"
2001.jeptalnrecital-tutoriel.3,W91-0306,0,0.0305588,"Missing"
2001.jeptalnrecital-tutoriel.3,W98-0508,0,0.087996,"Missing"
2001.jeptalnrecital-tutoriel.3,C69-0101,0,0.774033,"Missing"
2001.jeptalnrecital-tutoriel.3,P98-1019,0,0.02584,"Missing"
2001.jeptalnrecital-tutoriel.3,P96-1045,0,0.066492,"Missing"
2001.jeptalnrecital-tutoriel.3,W98-1435,0,0.0585399,"Missing"
2001.jeptalnrecital-tutoriel.3,P01-1024,0,0.0367717,"Missing"
2001.jeptalnrecital-tutoriel.3,C96-1044,0,0.263277,"Missing"
2001.jeptalnrecital-tutoriel.3,P81-1022,0,0.151273,"est pas possible de combiner le garçon avec la tête de la relative (que ou ai suivant les analyses) tant que la relative dans son entier n&apos;a pas été analysée. La seule façon d&apos;analyser le sujet de cette phrase est de combiner la et dernière à semaine, puis la dernière semaine à rencontré, puis le tout à ai, et ainsi de suite jusqu&apos;à la combinaison de la relative complète avec garçon. Autrement dit, l&apos;algorithme CKY, s&apos;il a l&apos;avantage d&apos;être simple, ne peut en aucune façon être rendu incrémental. Il existe un autre algorithme classique pour les grammaires hors-contextes, l&apos;algorithme d&apos;Earley (Earley 1970, Floyd & Biegel 1995), qui peut être aussi adapté aux grammaires de dépendance (Lombardo 1996). A l&apos;inverse de l&apos;algorithme CKY, l&apos;algorithme d&apos;Earley est un algorithme descendant. L&apos;algorithme d&apos;Earley fonctionne également en temps 0(n3) (où n est le nombre de mots de la phrase) et même en temps 0(n2) pour les grammaires non ambiguë. Néanmoins l&apos;algorithme d&apos;Earley n&apos;est pas précisément adapté à la langue naturelle qui est hautement ambiguë. En particulier, cet algorithme, s&apos;il apparaît comme plutôt incrémental, oblige en fait à construire l&apos;arbre à partir de la racine et à anticiper, dès la"
2001.jeptalnrecital-tutoriel.3,C96-1058,0,0.0378274,"Missing"
2001.jeptalnrecital-tutoriel.3,1995.mtsummit-1.1,0,0.0626426,"Missing"
2001.jeptalnrecital-tutoriel.3,P01-1029,1,0.876057,"Missing"
2001.jeptalnrecital-tutoriel.3,C92-3158,0,0.0217143,"Missing"
2001.jeptalnrecital-tutoriel.3,P98-1106,1,0.898246,"Missing"
2001.jeptalnrecital-tutoriel.3,P92-1041,0,0.0602191,"Missing"
2001.jeptalnrecital-tutoriel.3,P98-2130,0,0.0238524,"Missing"
2001.jeptalnrecital-tutoriel.3,P90-1005,0,0.216722,"Missing"
2001.jeptalnrecital-tutoriel.3,W98-0509,0,0.0386521,"Missing"
2001.jeptalnrecital-tutoriel.3,1995.iwpt-1.23,0,0.67504,"Missing"
2001.jeptalnrecital-tutoriel.3,P97-1043,0,0.0304225,"Missing"
2001.jeptalnrecital-tutoriel.3,1993.iwpt-1.22,0,0.0406518,"Missing"
2001.jeptalnrecital-tutoriel.3,P88-1031,0,0.14278,"Missing"
2003.jeptalnrecital-long.16,P01-1029,1,0.848504,"Missing"
2003.jeptalnrecital-long.16,2001.jeptalnrecital-tutoriel.3,1,0.79707,"Missing"
2003.jeptalnrecital-long.16,J97-4003,0,0.0740968,"Missing"
2004.jeptalnrecital-long.23,1995.iwpt-1.23,0,0.130256,"Missing"
2004.jeptalnrecital-long.23,P92-1010,0,0.0710454,"Missing"
2004.jeptalnrecital-long.23,J92-4004,0,0.669726,"Missing"
2005.jeptalnrecital-long.16,C96-1044,0,0.0637116,"Missing"
2005.jeptalnrecital-long.16,2001.jeptalnrecital-tutoriel.3,1,0.733693,"Missing"
2005.jeptalnrecital-long.16,2005.jeptalnrecital-long.3,1,0.784091,"Missing"
2005.jeptalnrecital-long.16,C90-3045,0,0.127933,"Missing"
2005.jeptalnrecital-long.3,P01-1029,1,0.884114,"Missing"
2005.jeptalnrecital-long.3,2001.jeptalnrecital-tutoriel.3,1,0.808041,"Missing"
2005.jeptalnrecital-long.3,1995.iwpt-1.23,0,0.0785788,"Missing"
2005.jeptalnrecital-long.3,C90-3045,0,0.0237578,"Missing"
2007.jeptalnrecital-poster.12,C90-3001,0,0.23118,"Missing"
2007.jeptalnrecital-poster.12,P05-1067,0,0.0560086,"Missing"
2007.jeptalnrecital-poster.12,J94-4004,0,0.154193,"Missing"
2007.jeptalnrecital-poster.12,2005.jeptalnrecital-long.3,1,0.672524,"Missing"
2007.jeptalnrecital-poster.12,2006.amta-papers.15,0,0.0522498,"Missing"
2007.jeptalnrecital-poster.12,1997.tmi-1.26,0,0.10115,"Missing"
2010.jeptalnrecital-court.9,W09-3744,0,0.156468,"Missing"
2010.jeptalnrecital-court.9,W01-0807,0,0.21804,"Missing"
2010.jeptalnrecital-court.9,W98-0106,1,0.753595,"Missing"
2020.udw-1.5,W19-8003,0,0.185955,"nch has both pro-drop constructions and non pro-drop constructions. Currently, the two positions s and S are annotated nsubj in the French treebanks. In interrogative constructions, one can thus have two nsubj relations. On the other hand, the first actants in position D are annotated dislocated and are therefore not distinguished from the other NPs in this position. New proposals will be made in Section 4. We will see that the situation is more complex in the case of Wolof. 3. The case of Wolof Our study of Wolof is essentially based on the analysis of the treebank UD_Wolof-WTB, annotated by Dione (2019). In Wolof, the s position of pronominal subjects must also be distinguished from the S position of lexical subjects. For example, in relative clauses, a very frequent construction in Wolof due to the absence of an adjective class (1739 relatives for 2107 sentences, i.e. 82 relatives for 100 6 In spoken French, s is optional, but not in standard written French. The prosody, as well as the position of the interrogative pronoun, makes possible the distinction between the S and D positions (i a,b). (i) a. S : A qui Pierre parle-t-il ? ‘Who does Pierre speak to?’ b. D : Pierre, à qui parle-t-il ?"
2020.udw-1.5,P01-1024,0,0.0963042,"Missing"
2020.udw-1.5,P01-1029,1,0.454073,"Missing"
2020.udw-1.5,W16-1715,1,0.894584,"Missing"
2020.udw-1.5,W19-7814,1,0.870271,"aced by a complex form with the auxiliary aller ‘go’, which is another way to move the agreement to a preverbal position. This position is analyzed as the subject in UD. This issue falls outside of the scope of this paper, but we think that this annotation is quite problematic because this position does not display the same properties as the preverbal position, and should not be named in the same way according to traditional surface-syntactic criteria (see for instance criterion C in Mel&apos;čuk 1988). In the Surface-Syntactic UD (SUD) annotation scheme, we have analyzed it as an object position (Gerdes et al. 2019). 4 All our examples are extracted from UD treebanks with their sent_id identifier. 5 By topological scheme, we refer to a linear template corresponding to a syntactic configuration. The topological model was first developed for the modeling of word order in Germanic languages during the 19th century, and was later implemented in dependency grammar (Duchier and Debusmann, 2001; Gerdes and Kahane, 2001). 35 Table 1. Pronominal indices in French 1SG 2SG 3SG 1PL 2PL 3PL D moi toi lui/elle/ça nous nous vous eux/elles s je tu il/elle/ce nous on vous ils/elles s’ Ø Ø Ø -ons Ø -ez Ø It can be noted t"
2020.udw-1.5,W17-6530,0,0.0125294,"ot be an affix, it is best to treat it as a pronoun, that is a separate word. One can use the nsubj function (and thus sometimes have two subjects), but it may be desirable to distinguish the function of elements in position s (e.g. by an nsubj:weak relation), as some pronouns may occupy the position S.12 11 Such an annotation may also relate to the object. For example, in Mandarin and Cantonese, the second actant may be detached on the left, without it being clear whether it is a topicalized or dislocated object. This was annotated dislocated in UD_Cantonese-HK, making it difficult to study (Wong et al., 2017). A dislocated:obj relation would have allowed for a better exploitation of the corpus and comparison with the Mandarin corpus. 12 As we said Dione has opted for a heterogeneous annotation of s. Despite this, it was possible to identify all occurrences, sometimes at the cost of rather complex queries. The main problem for our study of subjecthood has been the use of dislocated regardless of the role of the detached NP. 42 Abbreviations for glosses ANAPH: anaphoric DEF : definite NEG : negative PL : plural SG : singular AUX: auxiliary IMP : imperfective O : object REL : relative CL : nominal cl"
bawden-etal-2014-correcting,W12-3611,1,\N,Missing
bawden-etal-2014-correcting,W08-1301,0,\N,Missing
bawden-etal-2014-correcting,W13-3711,1,\N,Missing
bawden-etal-2014-correcting,W10-1843,1,\N,Missing
bawden-etal-2014-correcting,D07-1096,0,\N,Missing
bawden-etal-2014-correcting,lacheret-etal-2014-rhapsodie,1,\N,Missing
C98-1102,C96-1058,0,0.0831199,"ime complexity of the algorithm is O(nalalQm~×), where G is the number of ruleFSMs derived from the dependency and LP rules in the g r a m m a r and Qmax is the m a x i m u m number of states in any of the rule-FSMs. 4 A F o r m a l i z a t i o n of PP-Dependency Grammars Recall that in a pseudo-projective tree, we make a distinction between a syntactic governor and a linear governor. A node can bc &quot;lifted&quot; along a lifting path from being a dependent of its syntactic governor to being a dependent of its linear ~This type of parser has been proposed previously. See for example (Lombardi, 1996; Eisner, 1996), who also discuss Early-style parsers for projective dependency grammars. 6We can use pre-computed top-down prediction to limit the number of pairs added. 649 governor, which nmst be an ancestor of the governor. In defining a formal rewriting system for pseudo-projective trees, we will not attempt to model the &quot;lifting&quot; as a transformational step in the derivation. Rather, we will directly derive the &quot;lifted&quot; version of the tree, where a node is dependent of its linear governor. Thus, the derived structure resembles more a unistratal dependency representation like those used by (Hudson, 1990)"
C98-1102,1995.iwpt-1.23,1,0.317064,"Missing"
C98-1102,P97-1043,0,0.102938,"Missing"
C98-1102,P97-1003,0,0.0349333,"sable Non-Projective Dependency Grammar Sylvain Kahane* and Alexis Nasr t and Owen Rambow ~ • T A L A N A Universitfi Paris 7 (sk(~ccr. j u s s i e u . f r ) LIA Universit~ d &apos; A v i g n o n ( a l e x i s . n a s r O l i a , u n i v - a v i g n o n , f r ) ~cCoGenTex, Inc. ( o w e n @ c o g e n t e x . c o m ) 1 Introduction Dependency g r a m m a r has a long tradition in syntactic theory, dating back to at least TesniSre&apos;s work from the thirties. 1 Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not. One problem that has posed an impediment to more wide-spread acceptance of dependency grammars is the fact that there is no computationally tractable version of dependency g r a m m a r which is not restricted to projective analyses. However, it is well known that there are some syntactic phenomena (such as wh-movement in English or clitic climbing in Romance) that require nonprojective analyses. In this paper, we present a form of projectivity which we call pseudoprojectivity, and we pr"
C98-1102,C96-2122,0,\N,Missing
lacheret-etal-2014-rhapsodie,W10-1842,1,\N,Missing
lacheret-etal-2014-rhapsodie,W10-1843,1,\N,Missing
lacheret-etal-2014-rhapsodie,buhmann-etal-2002-annotation,0,\N,Missing
lacheret-etal-2014-rhapsodie,broeder-etal-2012-standardizing,0,\N,Missing
P01-1029,P98-1026,0,0.169016,"Missing"
P01-1029,P01-1024,0,0.578355,"Missing"
P01-1029,P98-1106,1,0.891917,"Missing"
P01-1029,C98-1102,1,\N,Missing
P01-1029,C98-1026,0,\N,Missing
P06-1018,1995.iwpt-1.23,0,0.796526,"ng polarities allows integrating the treatment of saturation in the formalism of the rules. Thus the processing of saturation will pilot the combination of structures during the generation processing. Some polarities are neutral, others are not, but a final structure must be completely neutral. Two nonneutral objects can unify (that is, identify) and form a neutral object (that is, neutralizing each other). Proper unification holds no equivalent. Polarization takes its source in categorial grammar and subsequent works on resourcesensitive logic (see Lambek’s, Girard’s or van Benthem’s works). Nasr (1995) is among the first to introduce a rule-based formalism using an explicit polarization of structures. Duchier & Thater (1999) propose a formalism for tree description where they put forward the notion of polarity (and they uses the terms of polarity and neutralization). Perrier (2000) is probably the 137 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 137–144, c Sydney, July 2006. 2006 Association for Computational Linguistics first to develop a linguistic formalism entirely based on these ideas, the Interaction Grammar. P"
P06-1018,C00-2087,0,0.782545,"tely neutral. Two nonneutral objects can unify (that is, identify) and form a neutral object (that is, neutralizing each other). Proper unification holds no equivalent. Polarization takes its source in categorial grammar and subsequent works on resourcesensitive logic (see Lambek’s, Girard’s or van Benthem’s works). Nasr (1995) is among the first to introduce a rule-based formalism using an explicit polarization of structures. Duchier & Thater (1999) propose a formalism for tree description where they put forward the notion of polarity (and they uses the terms of polarity and neutralization). Perrier (2000) is probably the 137 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL, pages 137–144, c Sydney, July 2006. 2006 Association for Computational Linguistics first to develop a linguistic formalism entirely based on these ideas, the Interaction Grammar. PUG is both an elementary formalism (structures simply combine by identifying objects) and a powerful formalism, equivalent to Turing machines and capable of handling strings, trees, dags, n-graphs and products of such structures (such as ordered trees).3 But, above all, PUG is a well-"
P06-1018,C90-3045,0,0.0450105,"ture-structure based formalism where only features are polarized. Although more or less equivalent we prefer to polarize the FS themselves, i.e. the nodes. 141 of head phrase with a subcategorized sister phrase, namely the head-daughter-phrase:9 the NHDTR of the intermediate projection of eat). HD HEAD: 1 SUBCAT: 3 HDTR: NHDTR: HD HEAD: 1 SUBCAT: 〈 2 〉 ⊕ 3 HDTR HDTR HEAD: 2 SUBCAT : elist NHDTR SC HD Q SC H NHDTR HD SC eat SC cat H V cat H Q Q V SC H NHDT R elist N H Q Q HD SC cat elist elist We propose a translation of LFG into PUG that makes LFG appear as a synchronous grammar approach (see Shieber & Schabes 1990). LFG synchronizes two structures (a phrase structure or c-structure and a dependency/functional structure or f-structure) and it can be viewed as the synchronization of a phrase structure grammar and a dependency grammar. Let us consider a first LFG rule and its translation in PUG: N cat cat LFG (Lexical Functional Grammar) and synchronous grammars [1] S → NP ↓ = ↑ SUBJ N S S elist The combination of two head-daughterphrases with the lexical entry of eat gives us the previous lexicalized rule, equivalent to the rule for eat of the dependency grammar G4 (/subj/ is the NHDTR of the maximal proj"
P06-1018,J92-4004,0,0.423,"depth 1 and the precedence relation with edges from a node to its successor. The difficulty is then to propagate the order relation to the descendants of two sister nodes when we apply a rewriting rule by substituting a tree of depth 1. The simplest solution is undeniably the one presented here, consisting to introduce objects representing the beginning and the end of phrases (our grey nodes) and to indicate the relation between a phrase, its beginning and its end by representing the phrase with an edge from the beginning to the end. 140 monotonic and cannot be simulated with PUG. As shown by Vijay-Shanker 1992, to obtain a monotonic formalism, TAG must be viewed as a grammar combining quasi-trees. Intuitively, a quasi-tree is a tree whose nodes has been split in two parts and have each one an upper part and a lower part, between which another quasi-tree can be inserted (this is the famous adjoining operation of TAG). Formally, a quasi-tree is a tree whose branches have two types: dependency relations and dominance relations (respectively noted by plain lines and dotted lines). Two nodes linked by a negative dominance relation are potentially the two parts of a same node; only the lower part can hav"
P06-1018,C04-1044,0,0.345975,"ystem of polarities P = {■,□,–,+,■} (which are called like this: ■ = black = saturated, + = positive, – = negative, □ = white = obligatory context and ■ = grey = absolutely neutral), with N = {■,■}, and a product defined by the following array (where ⊥ represents the impossibility to combine). Note that ■ is the neutral element of the product. The symbol – can be interpreted as a need and + as the corresponding resource. . ■ □ – + ■ ■ □ – + ■ ■ □ – + ■ □ □ – + ■ – – ⊥ + + ■ ⊥ ⊥ ■ ■ ⊥ ⊥ ⊥ ■ ⊥ The system {□,■} is used by Nasr (1995), while the system {■,■,–,+}, noted {=,↔,←,→}, is considered by Bonfante et al. (2004), who show advantages of negative and positive polarities for prefiltration in parsing (a set of structures bearing negative and positive polarities can only 138 be reduced into a neutral structure if the sum of negative polarities of each object type is equal the sum of positive polarities). The system (P, . ) we have presented is commutative and associative. Commutativity implies that the combination of two structures is not procedurally oriented (and we can begin a derivation by any elementary structure, provided we use only once the initial structure). Associativity implies that the combin"
P06-1018,W90-0102,0,\N,Missing
P06-1138,E91-1005,0,\N,Missing
P06-1138,P02-1003,0,\N,Missing
P06-1138,P99-1061,0,\N,Missing
P06-1138,P98-1106,1,\N,Missing
P06-1138,C98-1102,1,\N,Missing
P06-1138,P06-1018,1,\N,Missing
P06-1138,P05-1013,0,\N,Missing
P06-1138,P01-1029,1,\N,Missing
P06-1138,P98-1026,0,\N,Missing
P06-1138,C98-1026,0,\N,Missing
P06-1138,W01-0807,0,\N,Missing
P06-1138,P01-1024,0,\N,Missing
P98-1106,1995.iwpt-1.23,1,0.308224,"ependents. (4) • The order of the newly introduced dependents is consistent with the LP rule associated with the governor. • The introduced terminal string (head) is m a p p e d to the rewritten category. (5) 3We follow (Gaifman, 1965) throughout this paper by modeling a dependency grammar with a string-rewriting system. However, we will identify a derivation with its representation as a tree, and we will sometimes refer to symbols introduced in a rewrite step as ""dependent nodes"". For a model of a DG based on tree-rewriting (in the spirit of Tree Adjoining Grammar (Joshi et al., 1975)), see (Nasr, 1995). 4In this paper, we will allow finite feature structures on categories, which we will notate using subscripts; e.g., Vtrans. Since the feature structures are finite, this is simply a notational variant of a system defined only with simple category labels. N o b j Adv f J Figure 3: A sample GDG derivation LP rules are represented as regular expressions (actually, only a limited form of regular expressions) associated with each category. We use the hash sign ( # ) to denote the position of the governor (head). For example: pl:Yt. . . . Nnom I Here is a sample m-rule. d3 : V Adv Carlos ) gnom, N"
P98-1106,P97-1043,0,0.0606835,"overed by ~2"" ((Nasr, 1996)). The resulting pseudo-projectivity is a fairly weak extension to projectivity, which nevertheless covers major nonprojective linguistic structures. However, we do not pursue a purely structural definition of pseudo-projectivity in this paper. In order to define pseudo-projectivity, we in647 linguistic example of lifting rule is given in Section 4. The idea of building a projective tree by means of lifting appears in (Kunze, 1968) and is used by (Hudson, 1990) and (Hudson, unpublished). This idea can also be compared to the notion of word order domain (Reape, 1990; BrSker and Neuhaus, 1997), to the Slash feature of G P S G and HPSG, to the functional uncertainty of LFG, and to the Move-a of GB theory. 3 Projective Revisited Dependency Vclause (2) ~ gnom, Y (3) ~ Adv = (Adv)Nnom(Aux)Adv*#YobjAdv*Yt . . . . yesterday Fernando thought Vtrans Nnom eats beans slowly We will call this system g e n e r a t i v e depend e n c y g r a m m a r or GDG for short. Derivations in GDG are defined as follows. In a rewrite step, we choose a multiset of dependency rules (i.e., a set of instances of dependency rules) which contains exactly one srule and zero or more m-rules. The left-hand side non"
P98-1106,P97-1003,0,0.0396601,"e e is a tree enriched with a linear order over the set of its nodes. Finally, if l is an arc of an ordered tree T, then Supp(1) represents the s u p p o r t of l, i.e. the set of the nodes of T situated between the extremities of l, extremities included. We will say that the elements of Supp(1) are c o v e r e d by I. Introduction Dependency grammar has a long tradition in syntactic theory, dating back to at least Tesni~re's work from the thirties3 Recently, it has gained renewed attention as empirical methods in parsing are discovering the importance of relations between words (see, e.g., (Collins, 1997)), which is what dependency grammars model explicitly do, but context-free phrasestructure grammars do not. One problem that has posed an impediment to more wide-spread acceptance of dependency grammars is the fact that there is no computationally tractable version of dependency grammar which is not restricted to projective analyses. However, it is well known that there are some syntactic phenomena (such as wh-movement in English or clitic climbing in Romance) that require nonprojective analyses. In this paper, we present a form of projectivity which we call pseudoprojectivity, and we present"
P98-1106,C96-2122,0,\N,Missing
P98-1106,C96-1058,0,\N,Missing
W00-2016,P95-1021,0,\N,Missing
W00-2017,W98-0107,1,0.829061,"Missing"
W00-2017,1995.iwpt-1.23,0,0.091108,"Missing"
W00-2017,W98-0106,1,\N,Missing
W12-3611,brants-2000-inter,0,0.089727,"Missing"
W12-3611,W10-1842,0,0.176275,"Missing"
W12-3611,schuurman-etal-2004-linguistic,0,0.0319495,"e CHRISTINE Treebank (Rahman & Sampson 2000) is annotated with phrase structure, like the British component of the International Corpus of English (Nelson et al. 2002); the treebanks of English, German, and Japanese, created within the VERBMOBIL project (Hinrichs et al. 2000) have the Negra-style (Brants 2000) mixed annotation of functionally augmented constituent structures, the Venice Italian Treebank (Delmonte 2009) annotated with dependency and phrase structures, the Ester treebank for French (Cerisara et al. 2010, dependency annotation on radio transcripts), the CNG (Spoken Dutch Corpus; Schuurman et al. 2004, dependency annotation on spontaneous speech, skipping over disfluencies), the Hong Kong corpus (Cheng et al. 2008, prosodic annotation of prominence, tone, key and termination). We may add to this list the C-Oral Rom Corpus (Cresti and Moneglia 2005). The C-Oral Rom does not include an annotation of syntactic constituency or syntactic relations (what we call microsyntactic annotation), but is endowed with a rich macrosyntactic annotation (see below for the micro/macrosyntactic distinction). 1.1 Data-structures The commonly used structures of spoken data annotation do not allow any complex sy"
W12-3611,W10-1843,1,\N,Missing
W13-3716,W98-0106,1,0.66904,"ndency grammar inspired from TAG, MTT and RG. The addition of predicative adjunction allows us to obtain a modular surface syntactic grammar, the derivation structures of which can be interpreted as semantic representations. 1 2 Introduction The dependency grammar we propose is inspired by TAG (Joshi 1987), MTT (Mel’čuk 1988) and Relational Grammar (Perlmutter 1980). Like TAG, we combine elementary tree structures in order to generate the surface syntactic structures of sentences. Also like TAG, we want our derivation structures to be interpretable as semantic structures (Rambow & Joshi 1994, Candito & Kahane 1998). Like MTT, our syntactic structures are dependency trees and our semantic structures are graphs of predicate-argument relations between the lexical and grammatical meanings of the sentence. Like RG, the syntactic structures are constructed by strata and a syntactic function can be revaluated. Such a formalism has actually already been fully established (see Nasr 1995, Kahane 2001, Kahane & Lareau 2005, and Lareau 2008). Kahane 2006 exlores the underlying formalism, which we call Polarized Unification Grammar (PUG), and shows that rewriting systems (including CFG), TAG, HPSG or LFG can be stro"
W13-3716,C96-1034,0,0.545469,"e: (V) mood obj pp subj’ by-obj subj Figure 10. Passive voice Intermediate conclusion: Thanks to the grey polarities, which allow us to make an object invisible (for other modules) and to replace it with another configuration, we are able to separate the rules describing the various constructions associated with a lexeme from the rule describing the lexeme itself. In a lexicalized grammar like TAG, an elementary structure describes a lexeme with one of its constructions and the set of elementary structures associated to a given lexeme must be generated by another module (like a metagrammar in Candito 1996). An advantage of the approach presented here is that the rules associated with the lexemes and their constructions are in the same formalism. They can be precompiled to obtain a lexicalized grammar or they can be triggered on-line during text analysis or synthesis. We will now see how grey polarities can be used for modeling another way of combining lexemes: predicative adjunction. 4 hind. The formalism forces us to replace the $r dependency by a new one, but in fact we just want to “move” it to a new dependent. Sinsertion: Complex determiners $r Numerous determiners are idiomatic constructio"
W13-3716,P01-1029,1,0.778707,"e third way is grammatical completion: Rdefinite completes RBOY by saturating a grammeme in RBOY’s structure. Rindicative RSLEEP Rdefinite RBOY be interpreted as a graph of predicate-argument relations (as shown in Candito & Kahane 1998 for TAG) and is the basis for a semantic representation (see Mel’čuk 2012). Indeed, each arrow in the derivation structure can be interpreted as a semantic dependency that points from a predicate to one of its arguments. To conclude this section, it must be remarked that we focus on the syntax-semantics interface in this paper and we do not discuss word order. Gerdes & Kahane 2001 propose a formalism—topological grammar—for linearizing dependency trees, even for languages with non-projective constructions. It is possible to write a topological grammar in PUG and to combine it with the grammar presented here (see Kahane & Lareau 2005 for the combination of different modules in PUG). 3 Dependency rules The first improvement to the base grammar that we propose is to separate the rules associated with the lexemes proper from the dependency rules ensuring the realization of a given dependency (see the nodal vs. sagittal rules of Kahane & Mel’čuk 1999). We modify our previou"
W13-3716,P06-1018,1,0.928858,"ace syntactic structures of sentences. Also like TAG, we want our derivation structures to be interpretable as semantic structures (Rambow & Joshi 1994, Candito & Kahane 1998). Like MTT, our syntactic structures are dependency trees and our semantic structures are graphs of predicate-argument relations between the lexical and grammatical meanings of the sentence. Like RG, the syntactic structures are constructed by strata and a syntactic function can be revaluated. Such a formalism has actually already been fully established (see Nasr 1995, Kahane 2001, Kahane & Lareau 2005, and Lareau 2008). Kahane 2006 exlores the underlying formalism, which we call Polarized Unification Grammar (PUG), and shows that rewriting systems (including CFG), TAG, HPSG or LFG can be strongly simulated by PUG. In this paper, we propose to write a PUG with rules (i.e. elementary structures) that are simple (from a mathematical point of view), but that cover rather complex linguistic phenomena, like extraction and complex determiners. The grammar makes extensive use of predicative adjunction, an operation borrowed from TAG for combining structures. Predicative adjunction adjoins the syntactic governor to a node, which"
W13-3716,1995.iwpt-1.23,0,0.473233,"combine elementary tree structures in order to generate the surface syntactic structures of sentences. Also like TAG, we want our derivation structures to be interpretable as semantic structures (Rambow & Joshi 1994, Candito & Kahane 1998). Like MTT, our syntactic structures are dependency trees and our semantic structures are graphs of predicate-argument relations between the lexical and grammatical meanings of the sentence. Like RG, the syntactic structures are constructed by strata and a syntactic function can be revaluated. Such a formalism has actually already been fully established (see Nasr 1995, Kahane 2001, Kahane & Lareau 2005, and Lareau 2008). Kahane 2006 exlores the underlying formalism, which we call Polarized Unification Grammar (PUG), and shows that rewriting systems (including CFG), TAG, HPSG or LFG can be strongly simulated by PUG. In this paper, we propose to write a PUG with rules (i.e. elementary structures) that are simple (from a mathematical point of view), but that cover rather complex linguistic phenomena, like extraction and complex determiners. The grammar makes extensive use of predicative adjunction, an operation borrowed from TAG for combining structures. Pred"
W13-3716,P95-1021,0,0.147793,"e derivation structure associated with the derivation suggested in Fig.1 giving the derived structure in Fig. 2. We adopt the following conventions: a substitution is represented by a downward arrow, an adjunction by an upward arrow, and an inflectional completion by a horizontal arrow. With these conventions, the derivation structure can 4 We have slightly simplified Fig. 3. A map host from each grammeme to its lexeme is needed in order to ensure that the identification of two grammemes entails the identification of their host lexemes. 5 This kind of adjunction is called sister adjunction by Rambow et al. 1995. Contrary to predicative adjunction, sister adjunction does not change the weak generative capacity of the formalism. One can note that the distinction between substitution and adjunction is small in PUG: in both cases, a black and a white node are identified. The only difference is the direction of the dependency, which is not really relevant in PUG (source and target are interchangeable from a graph-theoretical perspective). 139 (V) mood subj (N) Dsubject: (V) mood ind/subj subj (N) Figure 5. Lexical vs. dependency rules7 The dependency rule Dsubject constrains the subject dependency to be"
W13-3716,J94-1004,0,0.546834,"ic and the semantic structures: loads is the syntactic head of the NP loads of books and thus the syntactic dependent of read, but ‘book’ is its semantic head and the argument of ‘read’. As a consequence, we want the derived tree and the derivation structure of (1) to be as in Fig. 11 (besides Sinsertion, which is a technical rule introduced below). (V) LOAD $r (1) Aya read loads of books. READ LLOADS-OF: pl LLOADS-OF LBOOK Figure 11. Derived tree and derivation structure for (1)11 This problem can be solved by predicative adjunction, as in the standard analysis of complex determiners in TAG (Shieber & Schabes 1994). A predicative adjunction is an adjunction where the adjunct is inserted in the syntactic governor position. This is making possible in our formalism by way of a special rule, Sinsertion, allowing the insertion of a node and a dependency between two other nodes (Fig. 12). This rule can be compared to a rule like Dto-obj, which introduces a marker. But Sinsertion is a generic rule that can apply on any dependency ($r is a variable) and does not replace it but only inserts a dependency [insert:+] be11 We have simplified the derived tree by suppressing the attribute before the grammemes. 142 (N)"
W15-2113,P01-1029,1,0.643394,"as an element X that precedes. Even if the two conjuncts X and Y are in a paradigmatic relation (they can commute and each conjunct alone can occupy the position), they are in a syntagmatric relation: they combine into a new unit, which must be encoded by a dependency. Secondly, the syntactic representation is intermediate between meaning and sound. The syntactic representation thus has to allow us to compute on one hand, the semantic representation including the predicate-argument relations between lexical meanings, and on the other hand, the topological constituents observed on the surface (Gerdes & Kahane 2001). Thirdly, the representation constrains the possible combinations of the words: A certain number of combinations are eliminated by the impossibility to associate them with a phonological or semantic representation , but equally the impossibility to associate a syntactic structure to an utterance constitutes a strong filter on the allowed combinations (from a generative point of view, this is even the primary filter). Consequently, a good syntactic representation has to be sufficiently constrained so that most badly formed utterances cannot obtain a syntactic representation (while, of course,"
W15-2113,W12-3602,0,0.0485931,"theless, dependency grammars (just as other syntactic theories, including categorial and phrase structure) are “head-driven” in the sense that syntax is mainly considered as the analysis of government.3 However, paradigmatic phenomena are by definition orthogonal to government structures and their integration into dependency structures is up for debate because commonly, dependencies express head-daughter relations. Existing dependency annotation schemes differ widely on the analysis of paradigmatic phenomena, thus reflecting important underlying syntactic choices, which often remain implicit. Ivanova et al. (2012), while comparing different dependency schemes, note that “the analysis of coordination represents a well-known area of differences” and even on a simple example like cotton, soybeans and rice, “none of the formats agree.” The high frequency of paradigmatic phenomena also implies that the choice of their syntactic analysis has important ramifications on the structure as a whole: Dependency distance and government-dependent relations both vary significantly with the type of representation given to paradigmatic phenomena, see Popel et al. (2013) for measures on the impact of the choices for coor"
W15-2113,P13-2017,0,0.0749432,"Missing"
W15-2113,P13-1051,0,0.157657,"is almost impossible to apply to cotton, soybeans and rice. (2) A similar technique is almost impossible to apply to cotton, uh high quality cotton. (3) A similar technique is almost impossible to apply to cotton, (or) maybe linen. Sentence (1) is an example of a coordination, (2) of a reformulation, (3) is an intermediate case on the continuum between the two as shown in Blanche-Benveniste et al. (1984). We consider 1 The term paradigmatic is commonly used to denote a set of elements that are of the same paradigm because they can replace one another. We prefer this term to paratactic used by Popel et al. (2013) following Tesnière 1959 chap. 133 who opposes hypotaxis (= subordination in modern terms) and parataxis (= coordination) because today paratactic commonly refers to cases of coordination without conjunction (= juxtaposition). that a formalization of coordination must be extensible to other paradigmatic phenomena in particular to cases where two elements occupy the same syntactic position without being connected by subordinating conjunctions (Gerdes & Kahane 2009). The conjuncts of such paradigmatic structures form the layers of a paradigmatic pile whose dependency structure will be laid out i"
W15-2113,schuurman-etal-2004-linguistic,0,0.105396,"Missing"
W15-2121,W15-2313,1,0.649761,"nfiguration of his stemmas is motivated by this theoretical choice. The conjuncts are placed equi-level and the coordinate conjunction is placed between them (see section 4). Conjuncts are treated as co-heads and are both connected to the governor of the coordinated phrase. (a) (b) chien de Pierre de chien Pierre Figure 2. Interpretations of Tesnière’s transfer The formal object underlying the suggested representation of transfer can be defined from a mathematical perspective. Such an object allows some edges to have other edges as vertices in addition to nodes and will be called a polygraph (Kahane & Mazziotta 2015, following Burroni 1993; Bonfante & Guiraud 2008). As was already the case with the tree-object, the polygraph-object is meaningless per se. It is the theoretical grounding on the transfer concept that gives it a semiosis. Transfer could also be encoded in a tree (Osborne in Kahane & Osborne 2015); see fig. 2b. As long as they convey the same amount of information, the depicted polygraph and its corresponding tree can be automatically converted into one another — i.e. they are formally equivalent. They have the same meaning, and the choice between one or the other can be motivated neither by"
W15-2121,W13-3723,1,0.931164,"be optional. Such is the case in the socalled “absolute oblique” (Fr. cas régime absolu, Buridant 2000: §§59 sqq.) in Old French (7). Acknowledging the structure le message la roïne and de la roïne, but refusing *le message de achieves the description.7 Examples of such a structure are not seldom. Lat. decedere (de) provinciā ‘leave (from) one’s province’ is similar, except that the optional expression of the preposition has a more obvious semantic value8. Fr. Marie habite (à) Paris ‘Mary lives 7 Note that the article is not compulsory in Old Fr. This issue will not be investigated here (see Mazziotta 2013). 8 The clause usually appears with the preposition, but “verbs compounded with ā, ab, dē, etc., (1) take the simple ablative when used figuratively; but (2) when used literally to denote actual separation or motion, they 187 in Paris’ displays the same feature: the locative preposition à is also optional. The possibility for two words to be used independently or conjointly in the same construction is illustrated by (8). It is generally considered that that in I know that and I know that you lie are two different words, namely a pronoun and a conjunction. The hypothesis favored here is, on the"
W15-2121,W08-1301,0,0.163543,"Missing"
W15-2121,de-marneffe-etal-2014-universal,0,0.0478708,"Missing"
W15-2313,J86-2003,0,0.165183,"ies that some words are more important than others with respect to their 3 The term stratification has the same meaning as in our previous work (Kahane, 1997), where stratification is applied in a dependency-based representation. 4 There were no tree in Bloomfield (1933), nor in the famous paper by Wells (1947). According to Coseriu (1980, 48), the first tree-like representation of constituency appears in Nida (1949, 87) (the “tree” may rather be a polygraph, cf. sec. 3.1) and current trees with labels on nodes became popular after Gleason (1955) and Chomsky (1957); embedded boxes are used in Hockett (1958). S lovesV VP subject MaryN NP lovesV redAdj carsN object N Mary carsN modifier redAdj (a) Constituency tree (b) Dependency tree Figure 1: Constituency and dependency trees syntactic position. Therefore, the proposed hierarchy is not based on constructed part-whole relations, but on constructed dependency relations that associates pairs of governor and dependent words – see Mel’ˇcuk (1988) for this terminology. Unlike partwhole relations, dependencies are associated with specific labels indicating the kind of grouping operation. The most common way to represent a set of dependencies is a tree-"
W15-2313,W15-2121,1,0.713627,"Missing"
W15-2313,W13-3716,1,0.858974,"Missing"
W15-2313,J94-1004,0,0.546009,"Missing"
W15-2313,J92-4004,0,0.538118,"13 The top node of an edge is nothing else than the lexical head of the corresponding constituent. This step, the replacement of a constituent by its lexical head, is equivalent to the step in Lecerf’s procedure collapsing all the nodes with their common lexical head. 158 field (1933, §12, 2) calls this grouping actor-action construction – Bloomfield’s action does not correspond exactly to a VP constituent, since the direct object relation (like watch me) is called the actiongoal construction in §12.8. Even Chomsky (1957, ch. 4) introduces the constituency tree as a derivation tree. Following Vijay-Shanker (1992), one can extend the notion of derivation tree to TAG, where each node must be interpreted as a rule. In other words, the S node of the derivation tree can be interpreted as αS = [S → NP + V P] and the relation between S and V P as the substitution of a rule αV P into a rule αS . Constituency and dependency are two ways to encode grouping operations on the same words. Constituency focuses on the stratification, that is the respective order of the grouping operations (a verb groups with its object before grouping with its subject), while dependency focuses on headedness, that is the governor-de"
W16-1715,C10-1012,0,0.0700952,"Missing"
W16-1715,cmejrek-etal-2004-prague,0,0.118759,"Missing"
W16-1715,W13-3711,1,0.855462,"ribe how “the different tagsets impose different restrictions on which phenomena can be looked up in corpora”, but the same is not done for structural annotation choices and a fortiori no guideline for choosing the most appropriate annotation scheme is put forward. 1.2 the training of statistical parsers is also an important usage consideration (Schwartz et al. 2012). 3. Annotator-oriented considerations: Dependency structures are a light-weight annotation in terms of graph complexity (compared for example to phrase structure trees) and various ergonomic annotations tools have been developed (Gerdes 2013). Moreover, the annotators’ evaluation is straightforward on dependency structures (labeled and unlabeled attachment scores, see Nilsson et al. 2007). Delimitations of our study We are here interested in syntactic and semantic dependency annotations. By dependency annotation we mean an annotation based on a tokenization of the text in basic units (morphemes, words, multi-word expressions, . . . ) and a labeled directed graph of relations between the tokens. Deciding to use a dependency annotation is a choice in itself and, as every annotation choice, must be supported by different consideratio"
W16-1715,W15-2113,1,0.910111,"foreign for various cases of semantic units beyond the token. Figure 4: complex function names of choice C2 As most of the MWEs are either lexically or structurally non-ambiguous, obtaining C2 is not more complicated than B for the annotator (naturalness). Moreover, both choices C1 and C2 are 133 UD also makes the distinction between nsubj and nsubjpass (as well as csubj and csubjpass), which is a combination of syntactic and semantic information: An nsubjpass is a syntactic subject that does not correspond to the first actant of the verb (cf. Mel’ˇcuk 1988, partially following Tesni`ere 1959[2015]: ch. 51). Maybe it would have been better to clearly separate syntax and semantics since nsubjpass can designate a second or third actant (A book ←nsubjpass– was given to Craig vs. Craig ←nsubjpass– was given a book). This could be done by indicating the semantic actance number, which subsumes UD’s analysis and the distinction between nsubj and nsubjpass: it ←subj:0– is raining (non actancial subject), Ann ←subj:1– gives Craig a book, A book ←subj:2– was given to Craig, Craig ←subj:3– was given a book (separability, transformability, level coverage). Redistribution between second and third ac"
W16-1715,W12-3602,0,0.0407986,"ds vary less than function words between languages.” But this is of course insufficient to justify numerous other choices that have been done (some of which we will discuss here). If annotation guidelines of treebanks do not answer our question, studies dedicated to the analysis and comparison of treebanks do not help much more. Kakkonen (2005) is a good example of the kinds of questions investigated in such papers, which he resumes by “What types of annotation schemes and formats are applied?” or “What kinds of annotation methods and tools are used for creating the treebanks?”. For instance, Ivanova et al. (2012) compare 7 dependency treebanks and identify “a large variation across formats”. They note that “divergent representations are in part owed to relatively superficial design decisions, as well as in part to more contentful differences in underlying linguistic assumptions”, but do not investigate further what kinds of considerations have led This article attempts to place dependency annotation options on a solid theoretical and applied footing. By verifying the validity of some basic choices of the current dependency reference framework, Universal Dependencies (UD), in a perspective of general a"
W16-1715,C12-1147,0,0.201046,"ey are more interested in “contrastive studies” and present an “automatic conversion procedure”. Corpus linguistics and annotation handbooks that we are aware of are also mainly presenting different annotation schemes. K¨ubler & Zinsmeister (2015) describe how “the different tagsets impose different restrictions on which phenomena can be looked up in corpora”, but the same is not done for structural annotation choices and a fortiori no guideline for choosing the most appropriate annotation scheme is put forward. 1.2 the training of statistical parsers is also an important usage consideration (Schwartz et al. 2012). 3. Annotator-oriented considerations: Dependency structures are a light-weight annotation in terms of graph complexity (compared for example to phrase structure trees) and various ergonomic annotations tools have been developed (Gerdes 2013). Moreover, the annotators’ evaluation is straightforward on dependency structures (labeled and unlabeled attachment scores, see Nilsson et al. 2007). Delimitations of our study We are here interested in syntactic and semantic dependency annotations. By dependency annotation we mean an annotation based on a tokenization of the text in basic units (morphem"
W16-1715,W03-2414,0,0.141571,"Missing"
W16-1715,de-marneffe-etal-2014-universal,0,0.0999511,"Missing"
W16-1715,W13-2322,0,\N,Missing
W16-1715,D07-1096,0,\N,Missing
W16-3812,C96-1034,0,0.137126,"d trees. We have only one abstract mechanism, polarization, to control the combination of rules, so we do not need ad hoc features to do that. All the features in our structures correspond to lexical information that could not be suppressed in any framework. Thus, model artifacts are minimal. Elementary PUG structures can combine to obtain less granular descriptions equivalent to TAG elementary trees. But unlike TAG, PUG uses the same mechanism for the combination of elementary pieces of information than for whole lexical units. In other words, it expresses both TAG’s grammar and metagrammar (Candito, 1996; de La Clergerie, 2010) in the same formalism, which allows us to consider at the same time very fine-grained rules and rules with a larger span (routines, for instance). In this paper, we focus on the semantics-syntax interface, including the mismatches between these two levels of representation, i.e., what generative grammar models in terms of movement. In our dependencybased approach, it is not words or constituents that are moved around, but rather the dependencies themselves, i.e., the relations between words (or constituents). This work is licenced under a Creative Commons Attribution 4"
W16-3812,W10-4414,0,0.0376262,"Missing"
W16-3812,P06-1018,1,0.84094,"e fact that each specification in the dictionary calls a separate rule. All rules are expressed in the same basic unification-based formalism in the form of elementary structures a` la Tree Adjoining Grammar (TAG). The second contribution is that our syntactic dependency structure is richer than the usual representations in most DGs. It appears as a directed acyclic graph (DAG) from the point of view of the semantics-syntax interface, but as a proper tree for the syntax-text interface. The formal framework in question, Polarized Unification Grammar (PUG), has been presented in various papers (Kahane, 2006; Kahane and Lareau, 2005; Lareau, 2008; Kahane, 2013), but the description of the lexicon in PUG has never been formally discussed. To see whether PUG could handle a wide range of lexico-syntactic phenomena, we built a formal lexicon-grammar interface on top of Lexique des formes fl´echies du franc¸ais (Lefff ), a large-scale syntactic dictionary of French (Sagot, 2010). For the sake of clarity, we translated the entries discussed here into English and adapted some notations (without modifying the dictionary’s architecture). Unlike other unification-based grammars (Shieber, 1986; Francez and"
W16-3812,W13-3716,1,0.844652,"s a separate rule. All rules are expressed in the same basic unification-based formalism in the form of elementary structures a` la Tree Adjoining Grammar (TAG). The second contribution is that our syntactic dependency structure is richer than the usual representations in most DGs. It appears as a directed acyclic graph (DAG) from the point of view of the semantics-syntax interface, but as a proper tree for the syntax-text interface. The formal framework in question, Polarized Unification Grammar (PUG), has been presented in various papers (Kahane, 2006; Kahane and Lareau, 2005; Lareau, 2008; Kahane, 2013), but the description of the lexicon in PUG has never been formally discussed. To see whether PUG could handle a wide range of lexico-syntactic phenomena, we built a formal lexicon-grammar interface on top of Lexique des formes fl´echies du franc¸ais (Lefff ), a large-scale syntactic dictionary of French (Sagot, 2010). For the sake of clarity, we translated the entries discussed here into English and adapted some notations (without modifying the dictionary’s architecture). Unlike other unification-based grammars (Shieber, 1986; Francez and Wintner, 2011), PUG makes linguistic structure more ap"
W16-3812,W15-2313,1,0.879638,"Missing"
W16-3812,sagot-2010-lefff,0,0.0338454,"h (DAG) from the point of view of the semantics-syntax interface, but as a proper tree for the syntax-text interface. The formal framework in question, Polarized Unification Grammar (PUG), has been presented in various papers (Kahane, 2006; Kahane and Lareau, 2005; Lareau, 2008; Kahane, 2013), but the description of the lexicon in PUG has never been formally discussed. To see whether PUG could handle a wide range of lexico-syntactic phenomena, we built a formal lexicon-grammar interface on top of Lexique des formes fl´echies du franc¸ais (Lefff ), a large-scale syntactic dictionary of French (Sagot, 2010). For the sake of clarity, we translated the entries discussed here into English and adapted some notations (without modifying the dictionary’s architecture). Unlike other unification-based grammars (Shieber, 1986; Francez and Wintner, 2011), PUG makes linguistic structure more apparent: we do not combine abstract feature structures, but geometrical structures such as graphs and trees. We have only one abstract mechanism, polarization, to control the combination of rules, so we do not need ad hoc features to do that. All the features in our structures correspond to lexical information that cou"
W17-6510,D12-1133,0,0.0171759,"isjoint subset of dependencies in the flux. Spans and bouquets Other characteristics of the flux can be considered. The left span (resp. right span) of the flux is the number of words on the left (resp. right) which are vertices of a dependency in the flux. For instance, the left span is 1 in position 1 (US), 2 in position 2 (clashed, with) and 3 in position 3 (clashed, in, a). The left span in a given position corresponds to the number of words awaiting a governor or a dependent on the right of this position and the right span to the number of elements expected. In a transition-based parser (Bohnet & Nivre 2012, Dyer et al. 2015), it is the minimal number of words that must be stored in the stack.2 Again, 2 Disjoint dependencies and weight Figure 3. Disjoint dependencies The weight of the flux is equal to 1 in position 3: it is not possible to find two disjoint dependencies. The weight is equal to 2 in position 2 because the subset { with &lt;case guerrillas, clashed obl&gt; fight } is disjoint but there is no disjoint subset with 3 elements. In practice all the nodes that are likely to have a dependent on the right are stored in the stack in arc-standard and arc-eager parsing strategies. 74 As we will se"
W17-6510,P15-1033,0,0.0245021,"endencies in the flux. Spans and bouquets Other characteristics of the flux can be considered. The left span (resp. right span) of the flux is the number of words on the left (resp. right) which are vertices of a dependency in the flux. For instance, the left span is 1 in position 1 (US), 2 in position 2 (clashed, with) and 3 in position 3 (clashed, in, a). The left span in a given position corresponds to the number of words awaiting a governor or a dependent on the right of this position and the right span to the number of elements expected. In a transition-based parser (Bohnet & Nivre 2012, Dyer et al. 2015), it is the minimal number of words that must be stored in the stack.2 Again, 2 Disjoint dependencies and weight Figure 3. Disjoint dependencies The weight of the flux is equal to 1 in position 3: it is not possible to find two disjoint dependencies. The weight is equal to 2 in position 2 because the subset { with &lt;case guerrillas, clashed obl&gt; fight } is disjoint but there is no disjoint subset with 3 elements. In practice all the nodes that are likely to have a dependent on the right are stored in the stack in arc-standard and arc-eager parsing strategies. 74 As we will see in the next secti"
W17-6510,2001.jeptalnrecital-tutoriel.3,1,0.59614,"Missing"
W17-6515,J86-2003,0,0.410699,"ed”. Section 2 introduces the mathematical and graphical notion of tree as well as the notion of reification, that helps understanding how diagrams are conceptualized. Section 3 attempts to define the meaning of the term dependency, in connection with the usage of trees in dependency and phrase structure syntax. Chomsky 1957’s diagram is analyzed in section 4 in order to evaluate to what extent it is “dependency-based”. The same section surveys the foundational works in ICA in the light shed by those preliminary notions (Barnard, 1836; Bloomfield, 1933; Wells, 1947; Nida, 1943; Gleason, 1955; Hockett, 1958). In the conclusion, we point out what distinguishes dependency syntax from ICA. 2 Trees and reification This section introduces the notion of tree, from an algebraic as well as a graphical perspectives (section 2.1). The notion of reification, i.e. the fact that conceptual elements are represented by discrete graphical entities in diagrams, is discussed under 2.2. 2.1 Algebraic and graphical notion of tree To understand Chomsky’s first diagram and other ICA diagrams, we need to bear in mind what a tree is. In graph theory, a tree T is algebraically de116 Proceedings of the Fourth Internationa"
W17-6515,W15-2313,1,0.932465,")) is similar to this typing operation, but direction can be expressed by other means. When directed edges correspond to bare strokes without arrows, direction can be expressed by the verticality of the diagram: the source of the edge is placed at a higher level than the target (fig. 2(a,b). 2.2 Reification In graphical trees, nodes and edges are turned into discrete graphical objects. This encoding operation is called reification (from Lat. r¯es ‘thing’; hence to reifiy ‘to turn into a thing’). Theoretical objects can be expressed by graphical objects, in which case, they are indeed reified (Kahane and Mazziotta, 2015; Mazziotta, 2016b). However, as illustrated by the alternative between the use of arrows or the use of vertically ordered strokes, the fact that diagrams are drawn on a bidimensional plane allows for the configurational expression of theoretical objects. Configurational expression competes with reification – e.g. in phrase structure trees (henceforth PST), words are often linearly ordered, which is a configurational means of expression of their precedence relations; this precedence could be reified by arrows instead. As an example of linguistic entities that are conceived as distinct notions"
W17-6515,1996.amta-1.36,0,0.708718,"Missing"
W17-7622,W03-1812,0,0.0464674,"Missing"
W17-7622,W16-1715,1,0.89552,"ystems and will not be discussed here. 183 1) The relation fixed is commonly used for MWEs with a very clear internal syntactic structure (see Figure 1).6 obl case nmod case fixed fixed fixed in the light of reports in ADP DET NOUN ADP NOUN nsubj det c&apos; the media fixed cop est n&apos; fixed importe quoi PRON AUX PART VERB ADP DET NOUN PRON Figure 1. Analyses with fixed in En-PartTUT and Fr-Original When analyzing them as fixed MWEs, we flatten the structure, losing precious information in the process, which will give us fewer instances of these syntactic relations on which to train our parser (cf. Gerdes & Kahane 2016’s principles as well as the principles given on the UD introduction page). Moreover, the analysis is somewhat contradictory: If we recognize the POSs of the components (such as the verbal nature of importe in Fr. n’importe quoi ‘anything’, lit. no matter what), then we could also recognize the dependency relations that the tokens entertain. 2) Currently, the criteria to decide which constructions enter the realm of MWEs are insufficient and we observe a lot of discrepancies between different treebanks and even inside a single treebank. For instance along with appears with three analyses. In E"
W17-7622,W17-1704,0,0.0176752,"gy and syntax, the upper boundaries between syntax and discourse organization. This paper discusses the lower boundaries in syntactic treebank development. We place our analysis in the Universal Dependency framework (UD), which constitutes a large community of more than 100 teams around the globe (Nivre et al. 2016). In this paper, we want to discuss the problem caused by idioms in syntactic annotation. The litera ture on idioms and MWEs is immense (Fillmore et al. 1988, Mel’čuk 1998, Sag et al. 2002, etc.). Our goal is not to mark the extension of MWEs on top of the syntactic annotation (see Savary et al. 2017 for a recent proposition). Our purpose is to tackle the impact of idiomaticity on the syntactic annotation itself. Most idioms (such as kick the bucket or green card) do not cause any trouble for the syntactic annotation because their internal syntactic structure is absolutely transparent (and it is precisely because they have an internal syntax that they are idioms and not words). Some expressions, however, such as not to mention, heaven knows who, by and large, Rio de la Plata (in English), are problematic for a syntactic annotation, because they do not perfectly respect the syntactic rules"
W18-6008,W16-1715,1,0.87219,"rg/u/overview/syntax.html). The goal of “maximizing parallelism between languages” might be of use for parser development of neighboring languages, but reducing language differences makes the resulting treebank by definition less interesting for typological research on syntax. In particular, UD does not account for the hierarchy between functional words and tends to flatten syntactic structures. The content-word-centric annotation is also problematic for the internal cohesion of the treebank (cf. the difficulty of coherently annotating complex prepositions that usually contain a content word, Gerdes & Kahane 2016) and it marks a break with syntactic traditions, where headedness is defined by distributional properties of individual words (Bloomfield 1933), see Section 2.1 One of the central advantages of dependency grammar is the clear distinction of category (the POS, i.e. an intrinsic distributional class) and function (i.e. the specific role a word plays towards another word). Sentences such as She became an architect and proud of it which have given rise to a considerable amount of scholarly discussions (Sag 2003) because an X-bar based phrase structure analysis requires deciding on the category of"
W18-6008,J93-2004,0,0.0604717,"three different UD relation labels (obj/xcomp/ccomp). We propose a new surface-syntactic annotation scheme, similar to UD, that we name SUD for Surface-syntactic Universal Dependencies. We want dependency links as well as the dependency labels to be defined based on purely syntactic criteria (Mel’čuk 1988), giving dependency structures closer to traditional dependency syntax (Meaning-Text Theory, Mel’čuk 1988; Word Grammar, Hudson 1984, 2007; Prague Dependency Treebank, Hajič et al. 2017) and headed constituency trees in phrase structure grammar (X-bar Syntax, Jackendoff 1977; Penn Treebank, Marcus et al. 1993). We also propose a hierarchy of SUD dependency relations that allows for under-specifications of dependency labeling. We conceived the SUD scheme as an alternative to UD and not as a competing annotation scheme, which means that the annotation scheme should have the same information content, the information being only expressed another way. Put differently, we looked for an annotation scheme based on distributionial criteria with an elementary conversion going both ways without loss, i.e. an “isomorphic” annotation. Since the principles underlying SUD are different, the isomorphism with UD ca"
W18-6008,de-marneffe-etal-2006-generating,0,0.0927647,"Missing"
W18-6008,L16-1376,0,0.0266789,"ation scheme that is applicable to all languages and proposing treebanks based on that scheme for more than 70 languages from different language families (Nivre et al. 2016). From the start, considerable efforts have been made to avoid an anglocentric scheme, going as far as analyzing English prepositions as case markers. The project is based on an ongoing and constantly evolving collaborative construction of the annotation scheme itself by means of an open online discussion group. The project welcomes and collaborates with enrichment efforts such as the enhanced UD annotation of deep syntax (Schuster & Manning 2016) or the annotation of multi-word expressions (Savary et al. 2015). Just as any annotation project, UD had to make choices among the different annotation options that commonly reflect opposing goals and downstream applications of the resulting treebanks. UD decided to stick to simple tree structures (compared to graphs with multiple governors) and to favor content words as heads, UD defines headedness indirectly via the category of the word: Content words are heads in UD and content words are usually understood as words belonging to open distributional classes, such as nouns, verbs, adjectives,"
W18-6008,C12-1147,0,0.0494644,"Missing"
W18-6008,W15-2134,0,0.049264,"Such a format is useful for every computation that concerns the form of the sentence such as word order (Chen et al. submitted) and the relation to prosody, etc. Conversely, UD might be a better entry point to the semantic content of the sentence. The lower dependency length gives psycholinguistic support to SUD treebanks. Possibly related is the fact that various experiments on parser performance also 72 7 minimization in 37 languages. Proceedings of the National Academy of Sciences, 112(33), 1033610341. consistently give an advantage to function-wordheaded structures (Schwartz et al. 2012, Silveira and Manning 2015, Kirilin and Versley 2015, Rehbein et al. 2017)15 which provides another raison d’être for parallel SUD treebanks. The whole UD 2.2 database, with its 122 treebanks, has been converted into SUD and is already accessible at https://gitlab.inria.fr/grew/ SUD. We would like to see this alternative to be distributed on the UD website as soon as possible and hope that the new scheme will benefit from discussions with the whole community and evolve in parallel to the UD scheme. Then SUD would become an alternative annotation option for UD treebank developers. As a last point, it appears that the co"
W19-7711,W15-2121,1,0.929127,"n its entirety. [...] We might start by marking those pairs of words which are felt to have the closest relationship. […] We will also lay down the rule that each word can be marked as a member of only one such pair.” 8 looks 1 looks 2 at Mary at Mary 1 photo 1 a photo 2 of a of 1 room room 1 her her Figure 6. The stratified dependency tree D0+ giving C0 a. With order on dependencies. b. With the conventions of Kahane (1997) Note that D0 and C0 are not equivalent: C0 induces stratification on D0 which is absent from D0 and conversely D0 induces headedness which is absent from C0. As stated by Kahane & Mazziotta (2015), a dependency tree is a connection structure plus headedness, while a constituency structure is a connection structure plus stratification. But the connection structure induced by a constituency tree is less fine-grained than the connection structure induced by a dependency structure, because it contains only large representatives and finer representatives cannot be deduced without adding additional information (such as headedness for instance). From the theoretical point of view, the question that arises is whether or not the stratification is useful and need to be encoded in the syntactic s"
W19-7720,W15-2113,1,0.362505,"he governor. Another solution would be to use a sub-type of conj. As shown by BlancheBenveniste (1990), there are many cases where a reformulation is not a reparation and cannot easily be differentiated from a coordination (she is a good linguist, a computational linguist). paradigmatic-EC and coordination. The absence of a first conjunct also occurs with coordination. The coordination with and and but also may have no antecedent (Gerdes & Kahane, 2009): (14) a. b. He speaks French and well. He speaks English, but badly. In these examples, there is a coordination with two illocutionary units (Gerdes & Kahane, 2015, p. 109). In (14a), the speaker makes two assertions: ‘he speaks French’ and ‘he speaks French well’. Note that the analysis of the exceptive markers as coordinating conjunctions in this paper is supported by argumentations similar to the ones made in English by Harris (1982), Reinhart (1991), and García Álvarez (2008), in Spanish by Perez-Jimenéz & Mareno-Quibén (2012), and in Egyptian Arabic by Soltan (2016). Nevertheless, none of them introduce the concept of paradigmatic construction and properly explain the link between exception and coordination. For the hypotactic-ECs, the EP has a muc"
W19-7803,W10-1843,1,0.713185,"Table 1. Inter-annotator agreement scores The agreement scores are then improved by the final adjudication, and our semi-automatic query of the corpus to look for inconsistencies using the grew tool. 2.3 Macrosyntactic segmentation Our segmentation is based on a long tradition of the study of syntax of spoken production in Romance languages (Blanche-Benveniste et al., 1990; Cresti, 2000; Degand and Simon, 2009). Our maximal syntactic units are illocutionary units, that is, assertions, questions, and demands. We use the markup developed in the Rhapsodie project of annotation of spoken French (Deulofeu et al., 2010; Pietrandrea and Kahane, 2019), which is a kind of formalized punctuation. The delimiter for illocutionary units is //. Consider this extract from a sample illustrating the markup: (1) den you go dey wrap dat food { small |r small } // cut cocoyam //= cut dat uh & // take { cocoyam |c and yam } wey you don grind //= ‘then you will wrap that food in small pieces, cut the cocoyam, cut that er… take the cocoyam and yam which you have ground.’ [DEU_A05] We also mark lists: the notation { X |Y } indicates that the phrase Y occupies the same syntactic position as X and piles up on X (Gerdes and Kah"
W19-7803,W16-1715,1,0.761466,"presentation, both cases are represented by means of a cleft structure, see the above analysis. This is congruent with many analyses of wh-words which consider that they occupy two syntactic positions, one as a complementizer and another as a pronoun inside the clause they complementize (see, in particular, (Tesnière, 1959[2015]: ch. 246)). During the conversion into UD we can only keep one of the relations, we have to keep the second rela tion as this follows the UD analysis of relative clauses. This leads to a “catastrophe” between the two syn tactically related interrogative constructions (Gerdes and Kahane, 2016). 3.3 Serial Verb Constructions The influence of adstrate vernacular languages, belonging mainly to the Niger-Congo family, is observed in the use of Serial Verb Constructions, that is “monoclausal construction[s] consisting of multiple independent verbs with no element linking them and with no predicate-argument relation between the verbs.” (Haspelmath, 2016). We used the subtyped relation compound:svc for these constructions. Sentence (8) contains an example of a serial verb construction (carry → put). (9) di man [...] just carry everyting put underground // ‘The man just carried everything"
W19-7803,W18-6008,1,0.934855,"can be studied as such to quantify phenomena that are more typical for spoken language such as left and right dislocations and disfluencies. It is also geared for the direct study of the prosody-syntax interface (Liu et al., 2019). The macrosyntactic annotation improves parsing results (see Section 2.5) and it can easily be simplified into a standard punctuation. 2.4 SUD Two different strands of thought, one rather practical, the other more theoretical, have led us to annotating the corpus not in the standard UD dependency annotation scheme but rather in the Surface-Syntactic UD scheme (SUD) (Gerdes et al., 2018). Firstly, the Nigerian annotators have been trained in a standard syntactic X-bar sentence structure, where, for example, a PP is headed by a preposition (Osborne and Gerdes, 2019). In this context, SUD is much easier to acquire than UD dependencies (Gerdes et al., 2019). 6 We look at agreement between pairs of annotators, A/B means we are looking at the agreement between the annotator A and annotator B Secondly, the NaijaSynCor project has a central typological component, and language comparisons should be possible, based on syntactic differences, which is easier in a scheme based purely on"
W19-7803,W19-7814,1,0.777659,"arsing results (see Section 2.5) and it can easily be simplified into a standard punctuation. 2.4 SUD Two different strands of thought, one rather practical, the other more theoretical, have led us to annotating the corpus not in the standard UD dependency annotation scheme but rather in the Surface-Syntactic UD scheme (SUD) (Gerdes et al., 2018). Firstly, the Nigerian annotators have been trained in a standard syntactic X-bar sentence structure, where, for example, a PP is headed by a preposition (Osborne and Gerdes, 2019). In this context, SUD is much easier to acquire than UD dependencies (Gerdes et al., 2019). 6 We look at agreement between pairs of annotators, A/B means we are looking at the agreement between the annotator A and annotator B Secondly, the NaijaSynCor project has a central typological component, and language comparisons should be possible, based on syntactic differences, which is easier in a scheme based purely on distributional criteria, such as SUD, than on the rather semantic function word vs content word distinction that constitutes the basis of UD. We can add that UD is particularly problematic for multi-words expression (MWEs) working as functional items (complex adpositions"
W19-7803,C18-1324,0,0.01797,"ponds to UD’s xcomp9 but is reversed when the governor is a copula (AUX): AUX comp:pred&gt; VERB becomes VERB cop&gt; AUX in UD. • comp:aux, for relations between a TAM (Tense–Aspect–Mood ) auxiliary and the full verb, which is also reversed in UD. • compound:svc is used for serial verb constructions, which are typical for Naija (see Section 3.3). The difference between UD and SUD annotations is exemplified in Figure 2. (5) dem go seize am // 7 “base on” is not a passive construction in Naija as there is not morphological passive. 8 Love Wantintin is the name of a radio programme. 9 As remarked by (Przepiórkowski and Patejuk, 2018) and (Gerdes et al., 2018), raising is orthogonal to the syntactic function and it would be better to add …:pred to the syntactic function in case of raising, which would give us comp:obj:pred for objects with raising, comp:obl:pred for obliques with raising and mod:pred for modifiers with raising (such as without talking in She explained it without talking). root root ‘They will seize it.’ [DEU_C01_D_6] punct nsubj aux punct obj comp:pred subj dem go seize am // PRON AUX VERB PRON PUNCT dem go they they will seize he // comp:obj seize am // PRON AUX VERB PRON PUNCT will seize he Figure 2. UD"
W19-7803,sloetjes-wittenburg-2008-annotation,0,0.0160244,"nd its meaning is transparent, the glossing was kept to a minimum. Function words do not have glosses beyond their morphological features, and only Naija lexical innovations were glossed (e.g. pikin ‘child’, patapata ‘full’). The POS annotation was manually corrected and a first dictionary of the function words and most common lexical items of Naija was created, containing the form, some orthographic variants, the POS tag, and an English gloss if necessary. This dictionary was then used on a dozen text samples inside the Elan-Corpa tool (Chanard, 2014), an extended version of the Elan tool 3 (Sloetjes and Wittenburg, 2008), which proposes the dictionary’s POS for each token for validation by the annotator. Through this semi-automatic process, the dictionary was enriched and later on used by the automatic tagger that was developed for the project 4. The POS tags follow the UD conventions (Nivre et al., 2018) with the caveat that some changes were made to accommodate the specificities of the Naija system. For example, Naija has three copulas, among which two are tagged as VERB (be and dey ‘be’) and one is tagged as PART (na ‘it is’)5. Regularly, the POS tagger is trained again on the corrected tags and thus impro"
W19-7814,W10-1843,1,0.776051,"deep-syntactic level (@fixed) that there is a fixed expression: each &gt;unk@fixed other, ad &gt;unk@fixed hoc. It is interesting to observe that the fact that some phrase does not behave according to the POS of its head exists also in other contexts not related to MWEs. We also recommend the use of the ExtPOS feature in these cases, together with a Type feature to explicit the construction: • In titles (of books, movies, songs. . . ), the head can have various POS but it is most of the times used as a proper noun: the movie Gone with the wind, ExtPOS=PROPN, Type=Title. • In grafts (Deulofeu, 1999; Deulofeu et al., 2010), which is a phenomenon mainly observed in spoken production, where a clause is used instead of a noun phrase: he bought I think it is called dowels, ExtPOS=NOUN, Type=Graft. We also suggest that UD should adopt the ExpPOS feature or an equivalent mecanism. It will allow for easier generalizations and for more precise validation of the UD treebanks. For instance, in the current validation script of UD, the dependent advmod must be ADV unless it is a MWE, which means in UD, that the dependent has a fixed relation with one of its dependent. If UD adopted a feature-based encoding of MWEs, this co"
W19-7814,W18-6008,1,0.78614,"near isomorphic to UD (Universal Dependencies). Contrary to UD, it is based on syntactic criteria (favoring functional heads) and the relations are defined on distributional and functional bases. In this paper, we will recall and specify the general principles underlying SUD, present the updated set of SUD relations, discuss the central question of MWEs, and introduce an orthogonal layer of deep-syntactic features converted from the deep-syntactic part of the UD scheme. 1 Introduction SUD (Surface-syntactic Universal Dependencies) is an annotation scheme that we proposed in a previous paper (Gerdes et al., 2018) as an alternative of the UD (Universal Dependencies) annotation scheme (Nivre and al., 2019). SUD follows surface syntax criteria (especially distributional criteria) and can be automatically converted into the UD scheme. SUD has now been used in the development of a treebank for Naija (Courtin et al., 2018; Caron et al., 2019) and treebanks for French and Chinese are in development. Some principles underlying SUD have been further clarified and will be exposed here. Section 2 recalls and specifies the general principles of SUD. For a more detailed explanation of these principles, we refer th"
W19-7814,C18-1324,0,0.0679468,"unction) must be linked to their governor by the same relation. The characterization of a relation is based on the whole paradigm of elements that can commute in the dependent position, while UD relations strongly rely on the POS of the dependent. For instance, a unique comp:obj relation for direct object complements is considered in SUD, where UD considers three relations: obj for a nominal object (I imagine a dance), ccomp for a clausal object (I imagine (that) he dances) and xcomp for a clausal object without its own subject (I imagine to dance). This last relation raises another problem. (Przepiórkowski and Patejuk, 2018) extensively argue that UD’s xcomp is particularly unsatisfactory because it is based on a property (not having its own subject), which is orthogonal to the syntactic function and can even be realized with modifiers (He came without running).2 We make a clear distinction between surface-syntactic properties, which determine relation classes, and deep-syntactic properties, such as those expressed by xcomp. In Section 3.2, we will propose to represent deep-syntactic properties with specific relation extensions. Hence, a subset of 17 UD relations (nsubj, csubj, obj, iobj, obl, xcomp, ccomp, amod,"
W19-7814,L16-1376,0,0.0324874,"ed &gt;comp:obl@agent by his attitude; (Fr) il fait pleurer &gt;comp:obj@agent (les) enfants [‘he makes the kids cry’]. UD marks expletive elements with a dedicated relation expl. We consider that this is not a surfacesyntactic relation, but it is possible to keep this information in the dedicated deep-syntactic feature @expl. See an example of an expletive subject in Figure 3. Note that our annotation scheme remains centered around a surface syntactic analysis, but we isolate semantically-oriented features more explicitly. This allows for an easier interface with the Enhanced UD annotation effort (Schuster and Manning, 2016). comp:obj subj@expl It PRON comp:pred is AUX comp:obj@agent unlikely ADJ that SCONJ subj she PRON mod comes VERB now ADV Figure 3: SUD analysis for It is unlikely that she comes now Another example of deep-syntactic features is given by the annotation of light verb constructions: We use the @lvc deep-syntactic feature. It is a feature indicating that the dependent is a predicative noun and that the governor is a light verb without semantic contribution. Nouns in light verb constructions can have a comp:obl@x dependent. • (Fr) Avoir envie de manger [‘having the urge to eat’]: avoir &gt;comp:obj@l"
W19-7912,W18-6002,1,0.881997,"Missing"
W19-7912,P98-1106,1,0.674493,"Missing"
W19-7912,W17-6510,1,0.888206,"nguages. In this paper we show that DLM can be interpreted as the flux size minimization and study the advantages of such a view. First it allows us to understand why DLM is cognitively motivated and how it is related to the constraints on the processing of sentences. Second, it opens the door to the definition of a big range of variations of DLM, taking into account other characteristics of the flux such as nested constructions and projectivity. 1 Introduction The dependency flux between two words in a sentence is the set of dependencies that link a word on the left with a word on the right (Kahane et al., 2017). The size of the flux in an inter-word position is the number of dependencies that cross this position.1 The flux size of a sentence is the sum of the sizes of the inter-word fluxes. Figure 1. An ordered dependency tree with the flux size on top of each inter-word position and, under each word, the length of the dependency from its governor On the top line, Figure 1 shows the size of the flux at each inter-word position. In the first position, between A and global, there is only one dependency crossing (A <det tax); in the second position, between global and carbon, there are two dependencies"
W19-7912,W03-3017,0,0.203081,"same flux are located. We plan in our further studies to look more precisely on the distribution of the different possible configurations of the flux. 3.3 Constraints on the potential flux It must be remarked that we do not really know the flux when processing a sentence incrementally since we do not generally know which words already processed will be linked with a word not yet processed. We call potential flux in a given inter-word position the set of words before the position which are likely to be linked to words after it. See in particular the principles of the transition-based parsing (Nivre, 2003) which consists in keeping all the words already processed and still accessible in the working memory. The largest hypothesis on the potential flux is to consider that all words before the position are accessible. But clearly some words are more likely to have dependents (for instance, only content words can have dependents in UD). It is also possible to make structural hypothesis on the potential flux. We call projective potential flux the set of words accessible while maintaining the projectivity of the analysis. We will limit our study to the projective potential flux even if we are aware t"
W19-7912,E06-1010,0,0.0986926,"Missing"
W19-8015,W17-6508,1,0.916913,"Missing"
W19-8015,de-marneffe-etal-2014-universal,0,0.0719875,"Missing"
W19-8015,W15-2112,0,0.0176681,"nd adjective-noun data extracted from the treebanks, with coherent results, also showing that these 20 languages can be arranged on a continuum with absolute head-initial and head-final patterns at the two ends. Liu further states that treebank based methods will be able to provide more complete and fine-grained typological analyses, while previous methods usually had to settle for a focus on basic word order phenomena (Hawkins 1983, Mithun 1987). These new resources allow reviewing and verifying well-known typological claims based on annotations of authentic texts (Liu et al. 2009, Liu 2010, Futrell et al. 2015). 1 The Universal Dependencies project (UD, Nivre et al. 2016), the basis of the present study, has seen a rapid growth into its present ample size with more than 140 treebanks of about 85 different lan1 The development of treebanks is a cumbersome work. Even 75 languages only cover a modest segment of the world’s languages. Another direction investigated in Östling (2015) is the use of parallel texts as the available translations of the New Testament in 986 languages. Such methods are not the subject of our paper but it is worth considering them for future works, knowing that translations con"
W19-8015,W18-6008,1,0.703058,"otation choices are based on theoretical considerations, for instance the analysis of you as an object of help rather than as a subject of understand. See Hudson (1998) for a comprehensive overview of the stakes of this particular question in a dependency perspective. 3 root mod compound Syntactic _ Syntactic comp subj obj obj dependency treebanks help you understand _ _ _ _ _ Figure 1: Example of an ordered dependency tree dependency treebanks help you understand typology. _ typology. Our study is based on Surface-Syntactic Universal Dependencies (SUD), a variant of the UD annotation scheme (Gerdes et al. 2018). SUD is better suited for word order studies as it is based on distri butional criteria whereas UD favors relations between content words. In SUD, contrary to UD, prepositional phrases are headed by prepositions, and auxiliaries and copula are analyzed just like other matrix verbs, taking the embedded verb as a dependent. The choice of the SUD version is particularly important when we consider a comprehensive view of all constructions of one language, for example Ja panese is nearly completely head-final in SUD whereas Japanese UD has a number of head-initial re lations such as adposition-nou"
W19-8015,L16-1262,0,0.102629,"Missing"
W19-8015,zeman-2008-reusable,0,0.0475618,"e of the target language (especially when the source text be longs to a marked genre such as religious texts). guages. UD has been developed with the goal of facilitating multilingual parser development, crosslingual learning, and parsing research from a perspective of language typology (Croft et al. 2017). The annotation scheme is an attempt to unify previous dependency treebank developments based on an evolution of (universal) Stanford dependencies (de Marneffe et al., 2014), Google universal part-ofspeech tags (Petrov et al., 2011), and the Interset interlingua for morphosyntactic tagsets (Zeman, 2008). The general philosophy is to provide a universal inventory of categories and guidelines to facil itate consistent annotation of similar constructions across languages, while allowing language-specific extensions when necessary. UD expects the schema, as well as the treebank data, to be “satisfactory on linguistic analysis grounds for individual languages”, and at the same time, to be appropriate for linguistic typology, i.e., to provide “a suitable basis for bringing out cross-linguistic parallelism across languages and language families”.2 One outstanding advantage of using this data set fo"
W98-0106,P95-1021,0,0.843055,"Missing"
W98-0106,W98-0107,1,0.853865,"ead a DT as a Sems. we need not only the PACP, but also a control ovcr the combination of the elementary trees : it must be checked that the argumental positions in a tree are actually filled by the right arguments.13 lt can be noted that for sentence (Sb) and (Sc), ru!ing out adjunctions of complement trees (as in DTG (RVW95)) solves the problem. Yet it might be problematic for sentence ( 1), for which we have seen that the TAG DT shows the right semantic dependencies. And it also rues out the adjunction of an athematic complement tree (such as the one for glass-of). This is investigated in (CK98). Conclusion We have shown that in the general case the DT can be viewed as a semantic representation, in the sense of MTI, provided coreference is not taken into 11 (SS94) a!ready noted that multiple adjunctions of bridge verbs at one node should be ruled out, here we find that this ho!ds for a whole trce. 12 (K89) already noted t!iat « derivations under which thematic roles, once established, are altered by further adjunctions » should be ruled out. u Another case where positions « are not filled by the right arguments » is for instance pied-piping. The XTAG derivation for the woma11 wlwse d"
W98-0106,J94-1004,0,0.317213,".fr Introduction From the parsing point of view, the derivation tree in TAG [hereafter DT] is seen as the ""history"" of the derivation but also as a linguistic representation, closer to semantics, that can be the basis of a further analysis. Because in TAG the elementary trees are lexicalized and localize · the predicate-arguments relations, several works have compared the DT to a structure involving dependencies between lexical items (RJ92; RVW95). 1 We agree with these authors that there are divergences between the DT and syntactic dependencies, but we show here that the DT - in the sense of (SS94) - can be viewed as a semantic dependency graph, namely a Sems for Meaning-Text Theory (MTI] (ZM67; M88). This requires the predicate-argument cooccurrence principle and also constraints on the adjunction of prcdicative auxiliary Lrees. We briefly introduce the representation levels in MTI before studying the dependencies shown by the DT. 2 1. Representation levels in MTT MTI distinguishes between linguistic representations and correspondance rules to go from a representation to another, at an adjacent level. For a wrüten sentence, there are 5 representations, each with a central structure : s"
W98-0107,W98-0106,1,0.917096,"ative clause interacting with n bridge verb, which has proved to be correct!y handlcll by TAG, as far as semantic dependencies arc concemed (CK98). This leads us to propose nn extension of DTO, called GAG for Graph-driven Adjunction Grammar, whose derivation controllers are graphs (Section 2). Finally, in Section 3. we develop an original analysis of wh-words in GAG.~ Introduction Tue aim of this paper is to find a fonnalism of the TAG family, where the derivation controller can be interpreted as a semantic dependency graph, in the sense of Meaning-Text Theory (ZM67; M88). In a previous paper (CK98), we study tliis interpretation of the derivation tree (DT) in the case of standard TAG. We prove that, in the general case, if the predieate-argument cooccurence principle 1 [= 1. Generalized substitution and PACP] holds and if elementary trees correspond to a semantic unit (A91), substitution arcs can be read as · generalized adjunction semantic dependencies where the dependent is the anchor of the substituted tree, and adjunction arcs DTO (RVW95) handles both clausal and nominal of any type- can be read as semantic dependencies complementation with the same operation. a in the opposite dire"
W98-0107,P95-1021,0,0.58618,"e the derivation controller can be interpreted as a semantic dependency graph, in the sense of Meaning-Text Theory (ZM67; M88). In a previous paper (CK98), we study tliis interpretation of the derivation tree (DT) in the case of standard TAG. We prove that, in the general case, if the predieate-argument cooccurence principle 1 [= 1. Generalized substitution and PACP] holds and if elementary trees correspond to a semantic unit (A91), substitution arcs can be read as · generalized adjunction semantic dependencies where the dependent is the anchor of the substituted tree, and adjunction arcs DTO (RVW95) handles both clausal and nominal of any type- can be read as semantic dependencies complementation with the same operation. a in the opposite direction. generalized substitution, called subsertion. and thus Yet we also characterized cases where the DT shows avoids the use of predicative adjunction. In order lo wrong (semantic) dependencies (cf also (RVW95)). cover the long-distance dependency data (including A problem may occur when, in the same sentence, cases not handled in TAG), this operation allows clausal complementation is handled both with pieces of the substituted element to eome in"
Y16-3011,augustinus-etal-2012-example,0,0.0401561,"Missing"
Y16-3011,2010.jeptalnrecital-demonstration.8,0,0.0253124,"Missing"
