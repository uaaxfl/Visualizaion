2020.eamt-1.66,An Overview of the {SEBAMAT} Project,2020,-1,-1,1,1,20907,reinhard rapp,Proceedings of the 22nd Annual Conference of the European Association for Machine Translation,0,SEBAMAT (semantics-based MT) is a Marie Curie project intended to con-tribute to the state of the art in machine translation (MT). Current MT systems typically take the semantics of a text only in so far into account as they are implicit in the underlying text corpora or dictionaries. Occasionally it has been argued that it may be difficult to advance MT quality to the next level as long as the systems do not make more explicit use of semantic knowledge. SEBAMAT aims to evaluate three approaches incorporating such knowledge into MT.
2020.bucc-1.2,Overview of the Fourth {BUCC} Shared Task: Bilingual Dictionary Induction from Comparable Corpora,2020,-1,-1,1,1,20907,reinhard rapp,Proceedings of the 13th Workshop on Building and Using Comparable Corpora,0,"The shared task of the 13th Workshop on Building and Using Comparable Corpora was devoted to the induction of bilingual dictionaries from comparable rather than parallel corpora. In this task, for a number of language pairs involving Chinese, English, French, German, Russian and Spanish, the participants were supposed to determine automatically the target language translations of several thousand source language test words of three frequency ranges. We describe here some background, the task definition, the training and test data sets and the evaluation used for ranking the participating systems. We also summarize the approaches used and present the results of the evaluation. In conclusion, the outcome of the competition are the results of a number of systems which provide surprisingly good solutions to the ambitious problem."
L18-1605,A Multilingual Dataset for Evaluating Parallel Sentence Extraction from Comparable Corpora,2018,0,0,3,0,8591,pierre zweigenbaum,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W17-2512,Overview of the Second {BUCC} Shared Task: Spotting Parallel Sentences in Comparable Corpora,2017,9,7,3,0,8591,pierre zweigenbaum,Proceedings of the 10th Workshop on Building and Using Comparable Corpora,0,"This paper presents the BUCC 2017 shared task on parallel sentence extraction from comparable corpora. It recalls the design of the datasets, presents their final construction and statistics and the methods used to evaluate system results. 13 runs were submitted to the shared task by 4 teams, covering three of the four proposed language pairs: French-English (7 runs), German-English (3 runs), and Chinese-English (3 runs). The best F-scores as measured against the gold standard were 0.84 (German-English), 0.80 (French-English), and 0.43 (Chinese-English). Because of the design of the dataset, in which not all gold parallel sentence pairs are known, these are only minimum values. We examined manually a small sample of the false negative sentence pairs for the most precise French-English runs and estimated the number of parallel sentence pairs not yet in the provided gold standard. Adding them to the gold standard leads to revised estimates for the French-English F-scores of at most +1.5pt. This suggests that the BUCC 2017 datasets provide a reasonable approximate evaluation of the parallel sentence spotting task."
W15-4108,A Methodology for Bilingual Lexicon Extraction from Comparable Corpora,2015,31,0,1,1,20907,reinhard rapp,Proceedings of the Fourth Workshop on Hybrid Approaches to Translation ({H}y{T}ra),0,"Dictionary extraction using parallel corpora is well established. However, for many language pairs parallel corpora are a scarce resource which is why in the current work we discuss methods for dictionary extraction from comparable corpora. Hereby the aim is to push the boundaries of current approaches, which typically utilize correlations between co-occurrence patterns across languages, in several ways: 1) Eliminating the need for initial lexicons by using a bootstrapping approach which only requires a few seed translations. 2) Implementing a new approach which first establishes alignments between comparable documents across languages, and then computes cross-lingual alignments between words and multiword-units. 3) Improving the quality of computed word translations by applying an interlingua approach, which, by relying on several pivot languages, allows an effective multi-dimensional cross-check. 4) We investigate that, by looking at foreign citations, language translations can even be derived from a single monolingual text corpus."
W15-3411,{BUCC} Shared Task: Cross-Language Document Similarity,2015,3,5,3,0,519,serge sharoff,Proceedings of the Eighth Workshop on Building and Using Comparable Corpora,0,"We summarise the organisation and results of the first shared task aimed at detecting the most similar texts in a large multilingual collection. The dataset of the shared was based on Wikipedia dumps with interlanguage links with further filtering to ensure comparability of the paired articles. The eleven system runs we received have been evaluated using the TREC evaluation metrics. 1 Task description Parallel corpora of original texts with their translations provide the basis for multilingual NLP applications since the beginning of the 1990s. Relative scarcity of such resources led to greater attention to comparable (=less parallel) resources to mine information about possible translations. Many studies have been produced within the paradigm of comparable corpora, including publications in"
W14-4701,The {C}og{AL}ex-{IV} Shared Task on the Lexical Access Problem,2014,40,8,1,1,20907,reinhard rapp,Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex),0,"The shared task of the 4th Workshop on Cognitive Aspects of the Lexicon (CogALexIV) was devoted to a subtask of the lexical access problem, namely multi-stimulus association. In this task, participants were supposed to determine automatically an expected response based on a number of received stimulus words. We describe here the task definition, the theoretical background, the training and test data sets, and the evaluation procedure used for ranking the participating systems. We also summarize the approaches used and present the results of the evaluation. In conclusion, the outcome of the competition are a number of systems which provide very good solutions to the problem."
W14-1016,Extracting Multiword Translations from Aligned Comparable Documents,2014,21,4,1,1,20907,reinhard rapp,Proceedings of the 3rd Workshop on Hybrid Approaches to Machine Translation ({H}y{T}ra),0,"Most previous attempts to identify translations of multiword expressions using comparable corpora relied on dictionaries of single words. The translation of a multiword was then constructed from the translations of its components. In contrast, in this work we try to determine the translation of a multiword unit by analyzing its contextual behaviour in aligned comparable documents, thereby not presupposing any given dictionary. Whereas with this method translation results for single words are rather good, the results for multiword units are considerably worse. This is an indication that the type of multiword expressions considered here is too infrequent to provide a sufficient amount of contextual information. Thus indirectly it is confirmed that it should make sense to look at the contextual behaviour of the components of a multiword expression individually, and to combine the results."
W14-0509,How well can a corpus-derived co-occurrence network simulate human associative behavior?,2014,16,4,2,0,21395,gemma enguix,Proceedings of the 5th Workshop on Cognitive Aspects of Computational Language Learning ({C}og{ACLL}),0,"Free word associations are the words people spontaneously come up with in response to a stimulus word. Such information has been collected from test persons and stored in databases. A well known example is the Edinburgh Associative Thesaurus (EAT). We will show in this paper that this kind of knowledge can be acquired automatically from corpora, enabling the computer to produce similar associative responses as people do. While in the past test sets typically consisted of approximately 100 words, we will use here a large part of the EAT which, in total, comprises 8400 words. Apart from extending the test set, we consider different properties of words: saliency, frequency and part-of-speech. For each feature categorize our test set, and we compare the simulation results to those based on the EAT. It turns out that there are surprising similarities which supports our claim that a corpus-derived co-occurrence network can simulate human associative behavior, i.e. an important part of language acquisition and verbal behavior."
enguix-etal-2014-graph,A Graph-Based Approach for Computing Free Word Associations,2014,45,4,2,0,21395,gemma enguix,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"A graph-based algorithm is used to analyze the co-occurrences of words in the British National Corpus. It is shown that the statistical regularities detected can be exploited to predict human word associations. The corpus-derived associations are evaluated using a large test set comprising several thousand stimulus/response pairs as collected from humans. The finding is that there is a high agreement between the two types of data. The considerable size of the test set allows us to split the stimulus words into a number of classes relating to particular word properties. For example, we construct six saliency classes, and for the words in each of these classes we compare the simulation results with the human data. It turns out that for each class there is a close relationship between the performance of our system and human performance. This is also the case for classes based on two other properties of words, namely syntactic and semantic word ambiguity. We interpret these findings as evidence for the claim that human association acquisition must be based on the statistical analysis of perceived language and that when producing associations the detected statistical regularities are replicated."
rapp-2014-corpus,Corpus-Based Computation of Reverse Associations,2014,21,5,1,1,20907,reinhard rapp,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"According to psychological learning theory an important principle governing language acquisition is co-occurrence. For example, when we perceive language, our brain seems to unconsciously analyze and store the co-occurrence patterns of the words. And during language production, these co-occurrence patterns are reproduced. The applicability of this principle is particularly obvious in the case of word associations. There is evidence that the associative responses people typically come up with upon presentation of a stimulus word are often words which frequently co-occur with it. It is thus possible to predict a response by looking at co-occurrence data. The work presented here is along these lines. However, it differs from most previous work in that it investigates the direction from the response to the stimulus rather than vice-versa, and that it also deals with the case when several responses are known. Our results indicate that it is possible to predict a stimulus word from its responses, and that it helps if several responses are given."
rapp-2014-using-word,Using Word Familiarities and Word Associations to Measure Corpus Representativeness,2014,17,2,1,1,20907,reinhard rapp,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"The definition of corpus representativeness used here assumes that a representative corpus should reflect as well as possible the average language use a native speaker encounters in everyday life over a longer period of time. As it is not practical to observe people{'}s language input over years, we suggest to utilize two types of experimental data capturing two forms of human intuitions: Word familiarity norms and word association norms. If it is true that human language acquisition is corpus-based, such data should reflect people{'}s perceived language input. Assuming so, we compute a representativeness score for a corpus by extracting word frequency and word association statistics from it and by comparing these statistics to the human data. The higher the similarity, the more representative the corpus should be for the language environments of the test persons. We present results for five different corpora and for truncated versions thereof. The results confirm the expectation that corpus size and corpus balance are crucial aspects for corpus representativeness."
C14-1200,Using Collections of Human Language Intuitions to Measure Corpus Representativeness,2014,21,0,1,1,20907,reinhard rapp,"Proceedings of {COLING} 2014, the 25th International Conference on Computational Linguistics: Technical Papers",0,"In corpus linguistics there have been numerous attempts to compile balanced corpora, resulting in text collections such as the Brown Corpus or the British National Corpus. These corpora are meant to reflect the average language use a native speaker typically encounters. But is it possible to measure in how far these efforts were successful? Assuming that humansxe2x80x99 language intuitions are based on our brainxe2x80x99s capability to statistically analyze perceived language and to memorize these statistics, we suggest a method for measuring corpus representativeness which compares corpus statistics to three types of human language intuitions as collected from test persons: Word familiarity, word association, and word relatedness. We compute a representativeness score for a corpus by extracting word frequency, word co-occurrence, and contextual statistics from it and by comparing these statistics to the human data. The higher the similarity, the more representative the corpus should be for the language environments of the test persons. Our findings confirm the expectation that corpus size and corpus balancing matter."
W13-2801,Workshop on Hybrid Approaches to Translation: Overview and Developments,2013,17,6,3,0,5326,marta costajussa,Proceedings of the Second Workshop on Hybrid Approaches to Translation,0,"A current increasing trend in machine translation is to combine data-driven and rule-based techniques. Such combinations typically involve the hybridization of different paradigms such as, for instance, the introduction of linguistic knowledge into statistical paradigms, the incorporation of data-driven components into rulebased paradigms, or the pre- and postprocessing of either sort of translation system outputs. Aiming at bringing together researchers and practitioners from the different multidisciplinary areas working in these directions, as well as at creating a brainstorming and discussion venue for Hybrid Translation approaches, the HyTra initiative was born. This paper gives an overview of the Second Workshop on Hybrid Approaches to Translation (HyTra 2013) concerning its motivation, contents and outcomes."
W12-0114,Design of a hybrid high quality machine translation system,2012,37,5,6,0,12054,bogdan babych,Proceedings of the Joint Workshop on Exploiting Synergies between Information Retrieval and Machine Translation ({ESIRMT}) and Hybrid Approaches to Machine Translation ({H}y{T}ra),0,"This paper gives an overview of the ongoing FP7 project HyghTra (2010--2014). The HyghTra project is conducted in a partnership between academia and industry involving the University of Leeds and Lingenio GmbH (company). It adopts a hybrid and bootstrapping approach to the enhancement of MT quality by applying rule-based analysis and statistical evaluation techniques to both parallel and comparable corpora in order to extract linguistic information and enrich the lexical and syntactic resources of the underlying (rule-based) MT system that is used for analysing the corpora. The project places special emphasis on the extension of systems to new language pairs and corresponding rapid, automated creation of high quality resources. The techniques are fielded and evaluated within an existing commercial MT environment."
rapp-etal-2012-identifying,Identifying Word Translations from Comparable Documents Without a Seed Lexicon,2012,8,11,1,1,20907,reinhard rapp,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"The extraction of dictionaries from parallel text corpora is an established technique. However, as parallel corpora are a scarce resource, in recent years the extraction of dictionaries using comparable corpora has obtained increasing attention. In order to find a mapping between languages, almost all approaches suggested in the literature rely on a seed lexicon. The work described here achieves competitive results without requiring such a seed lexicon. Instead it presupposes mappings between comparable documents in different languages. For some common types of textual resources (e.g. encyclopedias or newspaper texts) such mappings are either readily available or can be established relatively easily. The current work is based on Wikipedias where the mappings between languages are determined by the authors of the articles. We describe a neural-network inspired algorithm which first characterizes each Wikipedia article by a number of keywords, and then considers the identification of word translations as a variant of word alignment in a noisy environment. We present results and evaluations for eight language pairs involving Germanic, Romanic, and Slavic languages as well as Chinese."
W10-4005,The Noisier the Better: Identifying Multilingual Word Translations Using a Single Monolingual Corpus,2010,19,3,1,1,20907,reinhard rapp,Proceedings of the 4th Workshop on Cross Lingual Information Access,0,"The automatic generation of dictionaries from raw text has previously been based on parallel or comparable corpora. Here we describe an approach requiring only a single monolingual corpus to generate bilingual dictionaries for several language pairs. A constraint is that all language pairs have their target language in common, which needs to be the language of the underlying corpus. Our approach is based on the observation that monolingual corpora usually contain a considerable number of foreign words. As these are often explained via translations typically occurring close by, we can identify these translations by looking at the contexts of a foreign word and by computing its strongest associations from these. In this work we focus on the question what results can be expected for 20 language pairs involving five major European languages. We also compare the results for two different types of corpora, namely newsticker texts and web corpora . Our findings show that results are best if English is the source language, and that noisy web corpora are better suited for this task than well edited newsticker texts."
W10-3908,Utilizing Citations of Foreign Words in Corpus-Based Dictionary Generation,2010,13,3,1,1,20907,reinhard rapp,Proceedings of the Second Workshop on {NLP} Challenges in the Information Explosion Era ({NLPIX} 2010),0,"Previous work concerned with the identification of word translations from text collections has been either based on parallel or on comparable corpora of the respective languages. In the case of comparable corpora basic dictionaries have been necessary to form a bridge between the languages under consideration. We present here a novel approach to identify word translations from a single monolingual corpus without necessarily requiring dictionaries, although, as will be shown, a dictionary can still be useful for improving the results. Our approach is based on the observation that for various reasons monolingual corpora typically contain many foreign words (for example citations). Relying on standard newsticker texts, we will show that their cooccurrence-based associations can be successfully used to identify word translations."
P09-2034,The Backtranslation Score: Automatic {MT} Evalution at the Sentence Level without Reference Translations,2009,8,14,1,1,20907,reinhard rapp,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"Automatic tools for machine translation (MT) evaluation such as BLEU are well established, but have the drawbacks that they do not perform well at the sentence level and that they presuppose manually translated reference texts. Assuming that the MT system to be evaluated can deal with both directions of a language pair, in this research we suggest to conduct automatic MT evaluation by determining the orthographic similarity between a back-translation and the original source text. This way we eliminate the need for human translated reference texts. By correlating BLEU and back-translation scores with human judgments, it could be shown that the backtranslation score gives an improved performance at the sentence level."
W08-1914,The Computation of Associative Responses to Multiword Stimuli,2008,10,10,1,1,20907,reinhard rapp,Coling 2008: Proceedings of the Workshop on Cognitive Aspects of the Lexicon ({COGALEX} 2008),0,"It is shown that the behaviour of test persons as observed in association experiments can be simulated statistically on the basis of the common occurrences of words in large text corpora, thereby applying the law of association by contiguity which is well known from psychological learning theory. In particular, the focus of this work is on the prediction of the word associations as obtained from subjects on presentation of multiword stimuli. Results are presented for applications as diverse as crossword puzzle solving and the identification of word translations based on non-parallel texts."
P07-2014,Deriving an Ambiguous Word{'}s Part-of-Speech Distribution from Unannotated Text,2007,8,1,1,1,20907,reinhard rapp,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"A distributional method for part-of-speech induction is presented which, in contrast to most previous work, determines the part-of-speech distribution of syntactically ambiguous words without explicitly tagging the underlying text corpus. This is achieved by assuming that the word pair consisting of the left and right neighbor of a particular token is characteristic of the part of speech at this position, and by clustering the neighbor pairs on the basis of their middle words as observed in a large corpus. The results obtained in this way are evaluated by comparing them to the part-of-speech distributions as found in the manually tagged Brown corpus."
rapp-vide-2006-example,Example-Based Machine Translation Using a Dictionary of Word Pairs,2006,16,5,1,1,20907,reinhard rapp,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Machine translation systems, whether rule-based, example-based, or statistical, all rely on dictionaries that are in essence mappings between individual words of the source and the target language. Criteria for the disambiguation of ambiguous words and for differences in word order between the two languages are not accounted for in the lexicon. Instead, these important issues are dealt with in the translation engines. Because the engines tend to be compact and (even with data-oriented approaches) do not fully reflect the complexity of the problem, this approach generally does not account for the more fine grained facets of word behavior. This leads to wrong generalizations and, as a consequence, translation quality tends to be poor. In this paper we suggest to approach this problem by using a new type of lexicon that is not based on individual words but on pairs of words. For each pair of consecutive words in the source language the lexicon lists the possible translations in the target language together with information on order and distance of the target words. The process of machine translation is then seen as a combinatorial problem: For all word pairs in a source sentence all possible translations are retrieved from the lexicon and then those translations are discarded that lead to contradictions when constructing the target sentence. This process implicitly leads to word sense disambiguation and to language specific reordering of words."
E06-2018,Exploring the Sense Distributions of Homographs,2006,6,0,1,1,20907,reinhard rapp,Demonstrations,0,"This paper quantitatively investigates in how far local context is useful to disam-biguate the senses of an ambiguous word. This is done by comparing the co-occurrence frequencies of particular context words. First, one context word representing a certain sense is chosen, and then the co-occurrence frequencies with two other context words, one of the same and one of another sense, are compared. As expected, it turns out that context words belonging to the same sense have considerably higher co-occurrence frequencies than words belonging to different senses. In our study, the sense inventory is taken from the University of South Florida homograph norms, and the co-occurrence counts are based on the British National Corpus."
P05-3020,A Practical Solution to the Problem of Automatic Part-of-Speech Induction from Text,2005,5,9,1,1,20907,reinhard rapp,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"The problem of part-of-speech induction from text involves two aspects: Firstly, a set of word classes is to be derived automatically. Secondly, each word of a vocabulary is to be assigned to one or several of these word classes. In this paper we present a method that solves both problems with good accuracy. Our approach adopts a mixture of statistical methods that have been successfully applied in word sense induction. Its main advantage over previous attempts is that it reduces the syntactic space to only the most important dimensions, thereby almost eliminating the otherwise omnipresent problem of data sparseness."
P04-3026,A Practical Solution to the Problem of Automatic Word Sense Induction,2004,4,15,1,1,20907,reinhard rapp,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"Recent studies in word sense induction are based on clustering global co-occurrence vectors, i.e. vectors that reflect the overall behavior of a word in a corpus. If a word is semantically ambiguous, this means that these vectors are mixtures of all its senses. Inducing a word's senses therefore involves the difficult problem of recovering the sense vectors from the mixtures. In this paper we argue that the demixing problem can be avoided since the contextual behavior of the senses is directly observable in the form of the local contexts of a word. From human disambiguation performance we know that the context of a word is usually sufficient to determine its sense. Based on this observation we describe an algorithm that discovers the different senses of an ambiguous word by clustering its contexts. The main difficulty with this approach, namely the problem of data sparseness, could be minimized by looking at only the three main dimensions of the context matrices."
rapp-2004-utilizing,Utilizing the One-Sense-per-Discourse Constraint for Fully Unsupervised Word Sense Induction and Disambiguation,2004,9,6,1,1,20907,reinhard rapp,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"Recent advances in word sense induction rely on clustering related words. In this paper, instead of using a clustering algorithm, we suggest to perform a Singular Value Decomposition (SVD) which can be guaranteed to always find a global optimum. However, in order to apply this method to the problem of word sense induction, a semantic interpretation of the dimensions computed by the SVD is required. Our finding is that in our specific setting the first dimension relates to semantic similarities between words, and the second dimension distinguishes between the two main senses of an ambiguous word. Based on this result we present an algorithm for fully unsupervised word sense induction and disambiguation."
rapp-2004-freely,A Freely Available Automatically Generated Thesaurus of Related Words,2004,10,39,1,1,20907,reinhard rapp,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"A freely available English thesaurus of related words is presented that has been automatically compiled by analyzing the distributional similarities of words in the British National Corpus. The quality of the results has been evaluated by comparison with human judgments as obtained from non-native and native speakers of English who were asked to provide rankings of word similarities. According to this measure, the results generated by our system are better than the judgments of the non-native speakers and come close to the native speakersxe2x80x99 performance. An advantage of our approach is that it does not require syntactic parsing and therefore can be more easily adapted to other languages. As an example, a similar thesaurus for German has already been completed."
2003.mtsummit-papers.42,Word sense discovery based on sense descriptor dissimilarity,2003,-1,-1,1,1,20907,reinhard rapp,Proceedings of Machine Translation Summit IX: Papers,0,"In machine translation, information on word ambiguities is usually provided by the lexicographers who construct the lexicon. In this paper we propose an automatic method for word sense induction, i.e. for the discovery of a set of sense descriptors to a given ambiguous word. The approach is based on the statistics of the distributional similarity between the words in a corpus. Our algorithm works as follows: The 20 strongest first-order associations to the ambiguous word are considered as sense descriptor candidates. All pairs of these candidates are ranked according to the following two criteria: First, the two words in a pair should be as dissimilar as possible. Second, although being dissimilar their co-occurrence vectors should add up to the co-occurrence vector of the ambiguous word scaled by two. Both conditions together have the effect that preference is given to pairs whose co-occurring words are complementary. For best results, our implementation uses singular value decomposition, entropy-based weights, and second-order similarity metrics."
rapp-2002-part,A Part-of-Speech-Based Search Algorithm for Translation Memories,2002,11,3,1,1,20907,reinhard rapp,Proceedings of the Third International Conference on Language Resources and Evaluation ({LREC}{'}02),0,None
C02-1007,The Computation of Word Associations: Comparing Syntagmatic and Paradigmatic Approaches,2002,14,101,1,1,20907,reinhard rapp,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"It is shown that basic language processes such as the production of free word associations and the generation of synonyms can be simulated using statistical models that analyze the distribution of words in large text corpora. According to the law of association by contiguity, the acquisition of word associations can be explained by Hebbian learning. The free word associations as produced by subjects on presentation of single stimulus words can thus be predicted by applying first-order statistics to the frequencies of word co-occurrences as observed in texts. The generation of synonyms can also be conducted on co-occurrence data but requires second-order statistics. The reason is that synonyms rarely occur together but appear in similar lexical neighborhoods. Both approaches are systematically compared and are validated on empirical data. It turns out that for both tasks the performance of the statistical system is comparable to the performance of human subjects."
P99-1067,Automatic Identification of Word Translations from Unrelated {E}nglish and {G}erman Corpora,1999,22,443,1,1,20907,reinhard rapp,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"Algorithms for the alignment of words in translated texts are well established. However, only recently new approaches have been proposed to identify word translations from non-parallel or even unrelated texts. This task is more difficult, because most statistical clues useful in the processing of parallel texts cannot be applied to non-parallel texts. Whereas for parallel texts in some studies up to 99% of the word alignments have been shown to be correct, the accuracy for non-parallel texts has been around 30% up to now. The current study, which is based on the assumption that there is a correlation between the patterns of word co-occurrences in corpora of different languages, makes a significant improvement to about 72% of word translations identified correctly."
P98-2123,"A Freely Available Morphological Analyzer, Disambiguator and Context Sensitive Lemmatizer for {G}erman",1998,7,35,2,0,53484,wolfgang lezius,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"In this paper we present Morphy, an integrated tool for German morphology, part-of-speech tagging and context-sensitive lemmatization. Its large lexicon of more than 320, 000 word forms plus its ability to process German compound nouns guarantee a wide morphological coverage. Syntactic ambiguities can be resolved with a standard statistical part-of-speech tagger. By using the output of the tagger, the lemmatizer can determine the correct root even for ambiguous word forms. The complete package is freely available and can be downloaded from the World Wide Web."
C98-2118,"A Freely Available Morphological Analyzer, Disambiguator and Context Sensitive Lemmatizer for {G}erman",1998,7,35,2,0,53484,wolfgang lezius,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"In this paper we present Morphy, an integrated tool for German morphology, part-of-speech tagging and context-sensitive lemmatization. Its large lexicon of more than 320, 000 word forms plus its ability to process German compound nouns guarantee a wide morphological coverage. Syntactic ambiguities can be resolved with a standard statistical part-of-speech tagger. By using the output of the tagger, the lemmatizer can determine the correct root even for ambiguous word forms. The complete package is freely available and can be downloaded from the World Wide Web."
P95-1050,Identifying Word Translations in Non-Parallel Texts,1995,6,0,1,1,20907,reinhard rapp,33rd Annual Meeting of the Association for Computational Linguistics,1,None
W93-0310,Computation of Word Associations Based on Co-occurrences of Words in Large Corpora,1993,0,3,2,0,55256,manfred wettler,{V}ery {L}arge {C}orpora: Academic and Industrial Perspectives,0,None
