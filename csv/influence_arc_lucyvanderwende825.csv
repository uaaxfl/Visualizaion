1993.tmi-1.6,1992.tmi-1.12,0,0.184536,"and eventually, free text. Although these relations were only generated dynamically and never stored in a form that could be called an example base, they nevertheless constitute the level of rich semantic information we seek for our lexical knowledge base. The use of the heuristic rules described for disambiguating phrasal attachments may be considered functionally as a limited version of what is desired for the fourth component of the framework.. To date, the kind of information present in the example bases of documented EB systems has ranged from unprocessed strings (many of the examples in Furuse and Iida 1992, CTM examples in Sato 1991), to simple syntactic patterns (Sumita and Iida 1991, Tsutsumi 1992, MBT1 examples in Sato 1991), to deeper, semantic structures (Sadler 1989, Takeda, et al. 1992). Invariably, the deeper the processing involved to produce the examples, the more difficult it is to obtain them. Sadler (1989) was able to populate the LKB in the DLT system with 76,000 examples by relying on the completely regular semantic case markings of Esperanto, something which truly natural languages do not exhibit. Takeda, et al. 1992) used a parser together with interactive human verification to"
1993.tmi-1.6,1992.tmi-1.23,0,0.0171849,"e that ""...it is not yet clear whether EBMT can/should deal with the whole process of translation."" They go on to suggest that a gradual integration of EB methods with existing RB methods in MT systems is preferred and that experimentation will determine the correct balance. Sumita and Iida (1991) suggest pragmatic, but subjective criteria for implementing EB methods, including the level of translation difficulty and whether or not translations to be produced are compositional in nature. While these criteria may comprise reasonable guidelines, they lack any son of principled motivation. - 70- Grishman and Kosaka (1992) also argue for a combination of RB and ""empiricist"" approaches, including EB, statistics-based (SB), and corpus-based (CB) processing. They suggest that these latter methods ""...should be used to acquire the information which is more lexically or domain specific, and for which there is (as yet) no broad theoretical base."" However, they seem to reduce the issue to a simple distinction between the analysis/generation components of MT systems (for which, they claim, theories are welldeveloped) and transfer components of those systems (for which theories are not so developed). Jones (1992), after"
1993.tmi-1.6,J87-3005,0,0.350931,"racteristics may be combined in such a way as to provide substantial benefit to NL analysis systems. At the heart of both methods is the assumption that natural language is an ideal knowledge representation language, both in terms of expressive power and — 69— overall computational efficiency. This view has been asserted in other work that provided some of the basis for our current project (Jensen, et al. 1992) and is shared by other researchers as well (e.g., Wilks, et al. 1992). In the past few years, DB research has focused mainly on aspects of NL analysis such as phrasal attachment (e.g., Jensen and Binot 1987, Vanderwende 1990) and word sense disambiguation (e.g., Braden-Harder 1992, Wilks, et al. 1992), while EB efforts in MT have dealt with both analysis and transfer processing (e.g., Okumura, et al. 1992, Jones 1992, Sumita and Iida 1991, Watanabe 1992). There has been some debate in the MT field whether EB methods may be used effectively during analysis, and in the next section, we provide a rationale for their use in this context. Together with their similarity to DB methods, this provides justification for their use in the proposed framework, which focuses on enhancing NL analysis systems. A"
1993.tmi-1.6,1992.tmi-1.14,0,0.189038,"terms of expressive power and — 69— overall computational efficiency. This view has been asserted in other work that provided some of the basis for our current project (Jensen, et al. 1992) and is shared by other researchers as well (e.g., Wilks, et al. 1992). In the past few years, DB research has focused mainly on aspects of NL analysis such as phrasal attachment (e.g., Jensen and Binot 1987, Vanderwende 1990) and word sense disambiguation (e.g., Braden-Harder 1992, Wilks, et al. 1992), while EB efforts in MT have dealt with both analysis and transfer processing (e.g., Okumura, et al. 1992, Jones 1992, Sumita and Iida 1991, Watanabe 1992). There has been some debate in the MT field whether EB methods may be used effectively during analysis, and in the next section, we provide a rationale for their use in this context. Together with their similarity to DB methods, this provides justification for their use in the proposed framework, which focuses on enhancing NL analysis systems. Also, we characterize the complementary nature of EB and rule-based (RB) processing in creating coherent, hybrid NL systems. In the following section, we review and compare recent DB and EB work, identifying the asp"
1993.tmi-1.6,1992.tmi-1.15,0,0.0526544,"Missing"
1993.tmi-1.6,1992.tmi-1.4,0,0.0262254,"ting words from the textual context of a word to be disambiguated are ""activated"" in the network, and the activation spreads forward through the links until the network reaches a stable state in which nodes representing the correct senses (definitions) have the highest activation level. In this work, the dictionary as an example base has an explicit structure, like the lexical knowledge base we propose for our first component, although the relationships represented by the links in this structure are not labeled. The connectionist matching strategy is similar to that which has been proposed by McLean (1992) for EB machine translation, however, connectionist methods have not been included in the framework. Braden-Harder (1992) takes a somewhat different approach, making use of much of the explicitly coded information in LDOCE (e.g., grammatical codes and subject codes) as well as using a NL parser to extract genus terms from definitions and verbal arguments from example sentences. This information is then combined in a vector and matched (using techniques similar to those of Wilks and Sato mentioned above) against information gleaned from parsing the text surrounding the word to be disambiguated."
1993.tmi-1.6,C92-2083,1,0.909513,"der to analyze definitions from Webster&apos;s Seventh New Collegiate Dictionary and extract information used to determine semantic relations such as part_of and instrument. They then used this information together with a set of heuristic rules to rank the likelihood of alternate prepositional phrase attachments. These rules may be thought of as defining a matching procedure, but the relationship to current EB matching schemes is somewhat weaker than with other DB work described above. Vanderwende (1990) extended this work to the determination of participial phrase attachments, following w h i c h Montemagni and Vanderwende (1992) significantly increased the number of semantic relations th at we re being extracted from the definitions. The list now included such relations as subject_of, object_of, is_for, made_of, location_of, and means. These relations were a natural extension to the set used by Jensen and Binot, and some of them had also been proposed by Wilks, but the realization of their extraction from 4,000 noun definitions resulted from using a broad-coverage NL parser and applying sophisticated structurally-based patterns to the parsed definitions. The use of this or a similar NL parser is essential to being ab"
1993.tmi-1.6,1992.tmi-1.5,0,0.0398977,"ion language, both in terms of expressive power and — 69— overall computational efficiency. This view has been asserted in other work that provided some of the basis for our current project (Jensen, et al. 1992) and is shared by other researchers as well (e.g., Wilks, et al. 1992). In the past few years, DB research has focused mainly on aspects of NL analysis such as phrasal attachment (e.g., Jensen and Binot 1987, Vanderwende 1990) and word sense disambiguation (e.g., Braden-Harder 1992, Wilks, et al. 1992), while EB efforts in MT have dealt with both analysis and transfer processing (e.g., Okumura, et al. 1992, Jones 1992, Sumita and Iida 1991, Watanabe 1992). There has been some debate in the MT field whether EB methods may be used effectively during analysis, and in the next section, we provide a rationale for their use in this context. Together with their similarity to DB methods, this provides justification for their use in the proposed framework, which focuses on enhancing NL analysis systems. Also, we characterize the complementary nature of EB and rule-based (RB) processing in creating coherent, hybrid NL systems. In the following section, we review and compare recent DB and EB work, identif"
1993.tmi-1.6,P91-1024,0,0.311667,"ressive power and — 69— overall computational efficiency. This view has been asserted in other work that provided some of the basis for our current project (Jensen, et al. 1992) and is shared by other researchers as well (e.g., Wilks, et al. 1992). In the past few years, DB research has focused mainly on aspects of NL analysis such as phrasal attachment (e.g., Jensen and Binot 1987, Vanderwende 1990) and word sense disambiguation (e.g., Braden-Harder 1992, Wilks, et al. 1992), while EB efforts in MT have dealt with both analysis and transfer processing (e.g., Okumura, et al. 1992, Jones 1992, Sumita and Iida 1991, Watanabe 1992). There has been some debate in the MT field whether EB methods may be used effectively during analysis, and in the next section, we provide a rationale for their use in this context. Together with their similarity to DB methods, this provides justification for their use in the proposed framework, which focuses on enhancing NL analysis systems. Also, we characterize the complementary nature of EB and rule-based (RB) processing in creating coherent, hybrid NL systems. In the following section, we review and compare recent DB and EB work, identifying the aspects of this work that"
1993.tmi-1.6,C92-3161,0,0.0136271,"rich semantic information we seek for our lexical knowledge base. The use of the heuristic rules described for disambiguating phrasal attachments may be considered functionally as a limited version of what is desired for the fourth component of the framework.. To date, the kind of information present in the example bases of documented EB systems has ranged from unprocessed strings (many of the examples in Furuse and Iida 1992, CTM examples in Sato 1991), to simple syntactic patterns (Sumita and Iida 1991, Tsutsumi 1992, MBT1 examples in Sato 1991), to deeper, semantic structures (Sadler 1989, Takeda, et al. 1992). Invariably, the deeper the processing involved to produce the examples, the more difficult it is to obtain them. Sadler (1989) was able to populate the LKB in the DLT system with 76,000 examples by relying on the completely regular semantic case markings of Esperanto, something which truly natural languages do not exhibit. Takeda, et al. 1992) used a parser together with interactive human verification to obtain 30,000 examples from a limited domain dictionary. Most of the example bases described in the literature are much smaller in size, from a few hundred to a few thousand examples, and wh"
1993.tmi-1.6,C90-2067,0,0.0187114,"in a dictionary definition (and possibly related words) may be thought of in EB terms as forming example contexts which are then matched against contexts in new text to perform sense disambiguation. While this matching does not make any use of semantic information other than that implicitly represented in co-occurrence data, the vector similarity measurements used by Wilks have been shown to be quite useful in information retrieval systems. The methods used in these measurements and the context matching based on them are applicable to the second and third components of the proposed framework. Veronis and Ide (1990) augment this approach in another fashion, creating explicit links between content words in dictionary definitions and the entries for those words themselves, thereby creating a neuralnetwork-like structure throughout the dictionary. The links provide a similar function to the relatedness factor in the Wilks system. Nodes representing words from the textual context of a word to be disambiguated are ""activated"" in the network, and the activation spreads forward through the links until the network reaches a stable state in which nodes representing the correct senses (definitions) have the highes"
1993.tmi-1.6,C92-2115,0,0.106667,"— overall computational efficiency. This view has been asserted in other work that provided some of the basis for our current project (Jensen, et al. 1992) and is shared by other researchers as well (e.g., Wilks, et al. 1992). In the past few years, DB research has focused mainly on aspects of NL analysis such as phrasal attachment (e.g., Jensen and Binot 1987, Vanderwende 1990) and word sense disambiguation (e.g., Braden-Harder 1992, Wilks, et al. 1992), while EB efforts in MT have dealt with both analysis and transfer processing (e.g., Okumura, et al. 1992, Jones 1992, Sumita and Iida 1991, Watanabe 1992). There has been some debate in the MT field whether EB methods may be used effectively during analysis, and in the next section, we provide a rationale for their use in this context. Together with their similarity to DB methods, this provides justification for their use in the proposed framework, which focuses on enhancing NL analysis systems. Also, we characterize the complementary nature of EB and rule-based (RB) processing in creating coherent, hybrid NL systems. In the following section, we review and compare recent DB and EB work, identifying the aspects of this work that are included in"
C92-2083,P84-1036,0,0.0637005,"ies is concerned, &quot;sucb demands are beyond the abilities of lhe best current extraction techuiqaes&quot; (Wilks et al. 1989, p.227). However, the current stile of the art in computational linguistics demands that semantic information beyond genus terms be available now, on a large scale, to push forward the current theories, whetber that is knowledge-based parsing or parsing first with a syntactic component, followed by a semantic component. In this paper, we will focus on analyzing the definitions not for the genus terms, but for the semantic relations that can be extracted from the differentiae (Calzolari 1984). Although many have accepted the use of syntactic analyses for this purpose for some time now (for example Jeosen and Binot 1987, Klavans 1990, Ravin 1990, and Vanderwende 1990, all of which use the PLNLP F~lglish Parser to provide the structural information), many others still do not. We will demonstrate with examples why only patterns based on syntactic information (henceforth, structural patterns) provide reliable semantic relations for the differentiae. Patterns that match definition text at the string level (henceforth, striug patterns) are conceivable, but cannot capture the variations"
C92-2083,J87-3005,0,0.58492,"Missing"
C92-2083,P86-1018,0,0.0869753,"Missing"
C92-2083,J83-3002,0,\N,Missing
C92-2083,P90-1033,0,\N,Missing
C94-2125,P84-1109,0,0.189615,"quences made use of hand-coded semantic information, and they applied the analysis rules sequentially. In contrast, the task of analyzing noun sequences in unrestricted text strongly favors an algorithm according to which the rules are applied in parallel and the best interpretation is determined by weights associated with rule applications. 1. I N T R O D U C T I O N The inte~opretation of noun sequences (henceforth NSs, and also known as noun compounds or complex nominals) has long been a topic of research in natural language processing (NLP) (Finin, 1980; Sparck Jones, 1983; Leonard, 1984; Isabelle, 1984; Lehnert, 1988; and Riloff, 1989). The challenge in analyzing NSs derives from the semantic nature of the problem: their interpretation is, at best, only partially recoverable from a syntactic or a morphological analysis of NSs. To arrive at an interpretation of plum sauce which specifies that plum is the Ingredient of sauce, or of knowledge representation, specifying that knowledge is the Object of representation, requires semantic information for both the first noun (the modifier) and the second noun (the head). In this paper, we are concerned with interpreting NSs which are composed of two"
C94-2125,C92-2083,1,0.810629,"ense of each noun is a given, a situation which is not true for processing unrestricted text. This paper introdt.ces an algorithm which is specifically designed for analyzing NSs in unrestricted text. The task of processing unrestricted text has two consequences: firstly, hand-coded semantic information, and therefore a concept dependent algorithm, is no longer feasible; and secondly, the intended sense of each noun cau no longer be taken as a given. ]'he algorithm described here, therefore, relies on semantic information which has been extracted automatically fi'om an on-line dictionary (see Montemagni and Vanderwende, 1992; l)ohm et al., 1993). This algorithm manipulates a set of general rules, each of which has an associated weight, and a general procedure for matching words. The result of this algorithm is an ordered set of interpretations and partial scnsedisambiguation of the nouns by taking note of which noun senses were most relevant in each of the possible interpretations. We will begin by reviewing the chtssification schema for NSs described in Vanderwende (1993) and the type of general rules which this algorithm is designed to handle. The matching procedure will be described; by introducing a separate"
C94-2125,1993.tmi-1.6,1,0.869402,"Missing"
C98-2175,C94-2119,0,0.0100562,"h as H y p e r n y m or Synonym. For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et el. (1995) and Agirre and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et el. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet. The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words. For example, some of the top semrel paths in MindNet between pen and pencil, are shown below: pen +-M e a n s - - ~ r a w - - M eans-->pencil pene-Means--write--Means--)pencil pen--H yp-4instrument"
C98-2175,C96-1013,0,0.0609928,"Missing"
C98-2175,J92-4003,0,0.031194,"Missing"
C98-2175,P85-1037,0,0.081516,"ained in MindNet exploits the very same broadcoverage parser used in the Microsoft Word 97 grammar checker. This parser produces syntactic parse trees and deeper logical forms, to which rules are applied that generate corresponding structures of semantic relations. The parser has not been specially tuned to process dictionary definitions. All enhancements to the parser are geared to handle the immense variety of general text, of which dictionary definitions are simply a modest subset. There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al. 1985), specially constructed definition parsers (e.g., WilLs et al. 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). However, none of these has succeeded in producing the breadth of semantic relations across entire dictionaries that has been produced for MindNet. Vanderwende (1996) describes in detail the methodology used in the extraction of the semantic relations comprising MindNet. A truly broad-coverage parser is an essential component of this process, and it is the basis for extending it to other sources of information such as encyclopedias and"
C98-2175,P94-1038,0,0.00901907,"Missing"
C98-2175,E93-1028,0,0.14102,"me User relation types m relational triples (see Wilks et al. 1996). The larger contexts from which these relations have been taken have generally not been retained. For labeled relations, only a few researchers (recently, Barri~re and Popowich 1996), have appeared to be interested in entire semantic structures extracted from dictionary definitions, though they have not reported extracting a significant number of them. These relation types may be contrasted with simple co-occun'ence statistics used to create network structures from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al. (1996). Labeled relations, while more difficult to obtain, provide greater power for resolving both structural attachment and word sense ambiguities. While many researchers have acknowledged the utility of labeled relations, they have been at times either unable (e.g., for lack of a sufficiently powerful parser) or unwilling (e.g., focused on purely statistical methods) to make the effort to obtain them. This deficiency limits the characterization of word pairs such as river~bank (Wilks et al. 1996) and write~pen (Veronis and Ide 1990) to simple relatedness, whereas the labe"
C98-2175,W95-0105,0,0.00881832,"and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et el. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet. The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words. For example, some of the top semrel paths in MindNet between pen and pencil, are shown below: pen +-M e a n s - - ~ r a w - - M eans-->pencil pene-Means--write--Means--)pencil pen--H yp-4instrument e-H yp--pencil pen--Hyp--+write--Means--~pencil pen e--M eans--write ~--H yp--pencil Table 3. Highly weighted semrel paths between pen and pencil In t"
C98-2175,C90-2067,0,0.033705,"urce Subclass Synonym Time User relation types m relational triples (see Wilks et al. 1996). The larger contexts from which these relations have been taken have generally not been retained. For labeled relations, only a few researchers (recently, Barri~re and Popowich 1996), have appeared to be interested in entire semantic structures extracted from dictionary definitions, though they have not reported extracting a significant number of them. These relation types may be contrasted with simple co-occun'ence statistics used to create network structures from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al. (1996). Labeled relations, while more difficult to obtain, provide greater power for resolving both structural attachment and word sense ambiguities. While many researchers have acknowledged the utility of labeled relations, they have been at times either unable (e.g., for lack of a sufficiently powerful parser) or unwilling (e.g., focused on purely statistical methods) to make the effort to obtain them. This deficiency limits the characterization of word pairs such as river~bank (Wilks et al. 1996) and write~pen (Veronis and Ide 1990) to simple r"
C98-2175,J96-3009,0,0.0354055,"Missing"
C98-2175,C92-2070,0,0.0623561,"m MRDs, including Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks. Words were not related backward to any of the headwords whose definitions mentioned them, and words co-occurring ill the same definition were not related directly. In the fully inverted structures stored ill MindNet, however, all words are cross-linked, no matter where they appear. The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993) that LKBs created from MRDs provide spotty coverage of a language at best. Experiments described elsewhere (Richardson 1997) demonstrate the comprehensive coverage of the information contained in MindNet. Some statistics indicating the size (rounded to the nearest thousand) of the current version of MindNet and the processing time required to create it are provided in the table below. The definitions and example sentences are from the Longman Dictionary of Contemporal y English (LDOCE) and the American Heritage Dictionat T, 3 rd Edition (AHD3). Dictionaries used LDO"
C98-2175,C96-1005,0,\N,Missing
C98-2175,J93-1002,0,\N,Missing
D07-1047,P98-1009,0,0.0312126,"ey words, phrases, and concepts in a sentence or across single or multiple documents. Each sentence is then assigned a score indicating the strength of presence of key words, phrases, and so on. Sentence scoring methods utilize both purely statistical and purely semantic features, for example as in (Vanderwende et al., 2006; Nenkova et al., 2006; Yih et al., 2007). Recently, machine learning techniques have been successfully applied to summarization. The methods include binary classifiers (Kupiec et al., 1995), Markov models (Conroy et al., 2004), Bayesian methods (Daum´e III and Marcu, 2005; Aone et al., 1998), and heuristic methods to determine feature weights (Schiffman, 2002; Lin and Hovy, 2002). Graph-based methods have also been employed (Erkan and Radev, 2004a; Erkan and Radev, 2004b; Mihalcea, 2005; Mihalcea and Tarau, 2005; Mihalcea and Radev, 2006). In 2001–02, the Document Understanding Conference (DUC, 2001), issued the task of creating a 100-word summary of a single news article. The best performing systems (Hirao et al., 2002; Lal and Ruger, 2002) used various learning and semantic-based methods, although no system could outperform the baseline with statistical significance (Nenkova, 2"
D07-1047,W04-3247,0,0.0268862,"sence of key words, phrases, and so on. Sentence scoring methods utilize both purely statistical and purely semantic features, for example as in (Vanderwende et al., 2006; Nenkova et al., 2006; Yih et al., 2007). Recently, machine learning techniques have been successfully applied to summarization. The methods include binary classifiers (Kupiec et al., 1995), Markov models (Conroy et al., 2004), Bayesian methods (Daum´e III and Marcu, 2005; Aone et al., 1998), and heuristic methods to determine feature weights (Schiffman, 2002; Lin and Hovy, 2002). Graph-based methods have also been employed (Erkan and Radev, 2004a; Erkan and Radev, 2004b; Mihalcea, 2005; Mihalcea and Tarau, 2005; Mihalcea and Radev, 2006). In 2001–02, the Document Understanding Conference (DUC, 2001), issued the task of creating a 100-word summary of a single news article. The best performing systems (Hirao et al., 2002; Lal and Ruger, 2002) used various learning and semantic-based methods, although no system could outperform the baseline with statistical significance (Nenkova, 2005). After 2002, the single-document summarization task was dropped. In recent years, there has been a decline in studies on automatic single-document summar"
D07-1047,J02-4006,0,0.0117441,"Missing"
D07-1047,D07-1074,0,0.0254748,"documents. P N T (Si ) = 4 P p(q) , |q ∈ Si | q∈Si We define an entity as a title of a Wikipedia page. (5) where p(q) is the probability of a news term q and |q ∈ Si |is the number of news terms in Si . p(q) = Count(q) |q∈D |, where Count(q) is the number of times term q appears in D and |q ∈ D |is the number of news query terms in D. We also include the sum of news query terms in P Si , given by N T+ (Si ) = q∈Si p(q), and the relative probability P of news query terms in Si , given by p(q) i . N Tr (Si ) = q∈S |Si | We perform term disambiguation on each document using an entity extractor (Cucerzan, 2007). Terms are disambiguated to a Wikipedia entity only if they match a surface form in Wikipedia. Wikipedia surface forms are terms that disambiguate to a Wikipedia entity and link to a Wikipedia page with the entity as its title. For example, “WHO” and “World Health Org.” both refer to the World Health Organization, and should disambiguate to the entity “World Health Organization”. Sentences in CNN document D that contain Wikipedia entities that frequently appear in CNN document D are considered important. We calculate the average Wikipedia entity score for Si as W E(Si ) = P p(e) , |e ∈ Si | e"
D07-1047,W04-1013,0,0.107165,"Missing"
D07-1047,W01-0100,0,0.398106,"Missing"
D07-1047,I05-2004,0,0.0429514,"utilize both purely statistical and purely semantic features, for example as in (Vanderwende et al., 2006; Nenkova et al., 2006; Yih et al., 2007). Recently, machine learning techniques have been successfully applied to summarization. The methods include binary classifiers (Kupiec et al., 1995), Markov models (Conroy et al., 2004), Bayesian methods (Daum´e III and Marcu, 2005; Aone et al., 1998), and heuristic methods to determine feature weights (Schiffman, 2002; Lin and Hovy, 2002). Graph-based methods have also been employed (Erkan and Radev, 2004a; Erkan and Radev, 2004b; Mihalcea, 2005; Mihalcea and Tarau, 2005; Mihalcea and Radev, 2006). In 2001–02, the Document Understanding Conference (DUC, 2001), issued the task of creating a 100-word summary of a single news article. The best performing systems (Hirao et al., 2002; Lal and Ruger, 2002) used various learning and semantic-based methods, although no system could outperform the baseline with statistical significance (Nenkova, 2005). After 2002, the single-document summarization task was dropped. In recent years, there has been a decline in studies on automatic single-document summarization, in part because the DUC task was dropped, and in part beca"
D07-1047,P05-3013,0,0.160696,"scoring methods utilize both purely statistical and purely semantic features, for example as in (Vanderwende et al., 2006; Nenkova et al., 2006; Yih et al., 2007). Recently, machine learning techniques have been successfully applied to summarization. The methods include binary classifiers (Kupiec et al., 1995), Markov models (Conroy et al., 2004), Bayesian methods (Daum´e III and Marcu, 2005; Aone et al., 1998), and heuristic methods to determine feature weights (Schiffman, 2002; Lin and Hovy, 2002). Graph-based methods have also been employed (Erkan and Radev, 2004a; Erkan and Radev, 2004b; Mihalcea, 2005; Mihalcea and Tarau, 2005; Mihalcea and Radev, 2006). In 2001–02, the Document Understanding Conference (DUC, 2001), issued the task of creating a 100-word summary of a single news article. The best performing systems (Hirao et al., 2002; Lal and Ruger, 2002) used various learning and semantic-based methods, although no system could outperform the baseline with statistical significance (Nenkova, 2005). After 2002, the single-document summarization task was dropped. In recent years, there has been a decline in studies on automatic single-document summarization, in part because the DUC task was"
D07-1047,schiffman-2002-building,0,0.0201251,"le documents. Each sentence is then assigned a score indicating the strength of presence of key words, phrases, and so on. Sentence scoring methods utilize both purely statistical and purely semantic features, for example as in (Vanderwende et al., 2006; Nenkova et al., 2006; Yih et al., 2007). Recently, machine learning techniques have been successfully applied to summarization. The methods include binary classifiers (Kupiec et al., 1995), Markov models (Conroy et al., 2004), Bayesian methods (Daum´e III and Marcu, 2005; Aone et al., 1998), and heuristic methods to determine feature weights (Schiffman, 2002; Lin and Hovy, 2002). Graph-based methods have also been employed (Erkan and Radev, 2004a; Erkan and Radev, 2004b; Mihalcea, 2005; Mihalcea and Tarau, 2005; Mihalcea and Radev, 2006). In 2001–02, the Document Understanding Conference (DUC, 2001), issued the task of creating a 100-word summary of a single news article. The best performing systems (Hirao et al., 2002; Lal and Ruger, 2002) used various learning and semantic-based methods, although no system could outperform the baseline with statistical significance (Nenkova, 2005). After 2002, the single-document summarization task was dropped."
D07-1047,C98-1009,0,\N,Missing
D15-1021,P84-1044,0,0.207614,"Missing"
D15-1021,D14-1086,0,0.0746329,"Missing"
D15-1021,P11-1020,0,0.00577849,"s also a visual question answering dataset, where the questions are automatically generated from image captions of MS COCO dataset. This dataset has a total of 123,287 images with 117,684 questions with one-word answer about objects, numbers, colors, or locations. outdoor environments), showing multiple simultaneous events between a subset of four objects: a person, a backpack, a chair, and a trash-can. Each video was manually annotated (with very restricted grammar and lexicon) with several sentences describing what occurs in the video. • Microsoft Research Video Description Corpus (MS VDC) (Chen and Dolan, 2011) contains parallel descriptions (85,550 English ones) of 2,089 short video snippets (10-25 seconds long). The descriptions are one sentence summaries about the actions or events in the video as described by Amazon Turkers. In this dataset, both paraphrase and bilingual alternatives are captured, hence, the dataset can be useful translation, paraphrasing, and video description purposes. 3.3 Beyond Visual Description Recent work has demonstrated that n-gram language modeling paired with scene-level understanding of an image trained on large enough datasets can result in reasonable automatically"
D15-1021,N15-1053,0,0.0853302,"iled analysis of reporting bias is beyond the scope of this paper, but we found that many of the biases (e.g., people selection) found with abstract scenes (Zitnick et al., 2013) are also present with photos. User-generated Captions • SBU Captioned Photo Dataset (Ordonez et al., 2011) contains 1 million images with original user generated captions, collected in the wild by systematic querying of Flickr. This dataset is collected by querying Flickr for specific terms such as objects and actions and then filtered images with descriptions longer than certain mean length. • D´ej`a Images Dataset (Chen et al., 2015) consists of 180K unique user-generated captions associated with 4M Flickr images, where one caption is aligned with multiple images. This dataset was collected by querying Flickr for 693 high frequency nouns, then further filtered to have at least one verb and be judged as “good” captions by workers on Amazon’s Mechanical Turk (Turkers). 3.1.2 Captions of Densely Labeled Images Crowd-sourced Captions • UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image. • Flickr 30K Ima"
D15-1021,N15-1015,0,0.0118245,"is continuing to be a benchmark for comparing various aspects of vision and language research. • Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene semantics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3 This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attributes). 4 We did not use an external synonym"
D15-1021,P14-5010,0,0.0019222,"Missing"
D15-1021,N15-3006,1,0.0451788,"d Frazier measurements (Frazier, 1985); each provides a different counting on the number of nodes in the phrase markers of syntactic trees. • Part of Speech Distribution measures the distribution of nouns, verbs, adjectives, and other parts of speech. • Abstract:Concrete Ratio (#Conc, #Abs, %Abs) indicates the range of visual and non-visual concepts the dataset covers. Abstract terms are ideas or concepts, such as ‘love’ or ‘think’ and concrete terms are all the objects or events that are mainly available to the senses. For this purpose, we use a list of most common abstract terms in English (Vanderwende et al., 2015), and define concrete terms as all other words except for a small set of function words. • Average Sentence Length (Sent Len.) shows how rich and descriptive the sentences are. • Perplexity provides a measure of data skew by measuring how expected sentences are from one corpus according to a model trained on another corpus. We analyze perplexity (Ppl) for each dataset against a 5-gram language model learned on a generic 30B words English dataset. We further analyze pair-wise perplexity of datasets against each other in Section 4. Quality Criteria for Language & Vision Datasets The quality of a"
D15-1021,N15-1173,0,0.0673315,"Missing"
D15-1021,N15-1017,0,0.00967414,"ning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research. • Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene semantics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3 This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attributes). 4 We did not"
D15-1021,H89-1033,0,0.673771,"ailable corpora have played a crucial role in advancing this area of research. In this paper, we propose a set of quality metrics for evaluating and analyzing the vision & language datasets and categorize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. 1 Introduction Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing with more recent attempts on conversational robots grounded in the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In the past few years, an influx of new, large vision & language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language. Vision & language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Proc"
D15-1021,S14-1015,1,0.879416,"n size and contain more contextual descriptions. 3.1.1 3.1.3 Existing caption datasets provide images paired with captions, but such brief image descriptions capture only a subset of the content in each image. Measuring the magnitude of the reporting bias inherent in such descriptions helps us to understand the discrepancy between what we can learn for the specific task of image captioning versus what we can learn more generally from the photographs people take. One dataset useful to this end provides image annotation for content selection: • Microsoft Research Dense Visual Annotation Corpus (Yatskar et al., 2014) provides a set of 500 images from the Flickr 8K dataset (Rashtchian et al., 2010) that are densely labeled with 100,000 textual labels, with bounding boxes and facets annotated for each object. This approximates “gold standard” visual recognition. To get a rough estimate of the reporting bias in image captioning, we determined the percentage of top-level objects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A mor"
D15-1021,W10-0721,0,0.81083,"atasets provide images paired with captions, but such brief image descriptions capture only a subset of the content in each image. Measuring the magnitude of the reporting bias inherent in such descriptions helps us to understand the discrepancy between what we can learn for the specific task of image captioning versus what we can learn more generally from the photographs people take. One dataset useful to this end provides image annotation for content selection: • Microsoft Research Dense Visual Annotation Corpus (Yatskar et al., 2014) provides a set of 500 images from the Flickr 8K dataset (Rashtchian et al., 2010) that are densely labeled with 100,000 textual labels, with bounding boxes and facets annotated for each object. This approximates “gold standard” visual recognition. To get a rough estimate of the reporting bias in image captioning, we determined the percentage of top-level objects3 that are mentioned in the captions for this dataset out of all the objects that are annotated. Of the average 8.04 available top-level objects in the image, each of the captions only reports an average of 2.7 of these objects.4 A more detailed analysis of reporting bias is beyond the scope of this paper, but we fo"
D15-1021,Q13-1003,0,0.034408,"CVPR 2015 image captioning challenge and is continuing to be a benchmark for comparing various aspects of vision and language research. • Abstract Scenes Dataset (Clipart) (Zitnick et al., 2013) was created with the goal of representing real-world scenes with clipart to study scene semantics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3 This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attribu"
D15-1021,Q14-1006,0,0.177701,"ists of 180K unique user-generated captions associated with 4M Flickr images, where one caption is aligned with multiple images. This dataset was collected by querying Flickr for 693 high frequency nouns, then further filtered to have at least one verb and be judged as “good” captions by workers on Amazon’s Mechanical Turk (Turkers). 3.1.2 Captions of Densely Labeled Images Crowd-sourced Captions • UIUC Pascal Dataset (Farhadi et al., 2010) is probably one of the first datasets aligning images with captions. Pascal dataset contains 1,000 images with 5 sentences per image. • Flickr 30K Images (Young et al., 2014) extends previous Flickr datasets (Rashtchian et al., 2010), and includes 158,915 crowd-sourced captions that describe 31,783 images of people involved in everyday activities and events. • Microsoft COCO Dataset (MS COCO) (Lin et al., 2014) includes complex everyday scenes with common objects in naturally occurring contexts. Objects in the scene are labeled using per-instance segmentations. In total, this dataset contains photos of 91 basic object types with 2.5 million labeled instances in 328k images, each paired with 5 captions. This dataset gave rise to the CVPR 2015 image captioning chall"
D15-1021,P13-1006,0,0.0201542,"ntics isolated from object recognition and segmentation issues in image processing. This removes the burden of low-level vision tasks. This dataset contains 10,020 images of children playing 3.2 Video Description and Instruction Video datasets aligned with descriptions (Chen et al., 2010; Rohrbach et al., 2012; Regneri et al., 2013; Naim et al., 2015; Malmaud et al., 2015) generally represent limited domains and small lexicons, which is due to the fact that video processing and understanding is a very compute-intensive task. Available datasets include: • Short Videos Described with Sentences (Yu and Siskind, 2013) includes 61 video clips (each 35 seconds in length, filmed in three different 3 This visual annotation consists of a two-level hierarchy, where multiple Turkers enumerated and located objects and stuff in each image, and these objects were then further labeled with finer-grained object information (Has attributes). 4 We did not use an external synonym or paraphrasing resource to perform the matching between labels and captions, as the dataset itself provides paraphrases for each object: each object is labeled by multiple Turkers, who labeled Isa relations (e.g., “eagle” is a “bird”). 209 Size"
D15-1021,W03-0610,0,0.0240278,"ize them accordingly. Our analyses show that the most recent datasets have been using more complex language and more abstract concepts, however, there are different strengths and weaknesses in each. 1 Introduction Bringing together language and vision in one intelligent system has long been an ambition in AI research, beginning with SHRDLU as one of the first vision-language integration systems (Winograd, 1972) and continuing with more recent attempts on conversational robots grounded in the visual world (Kollar et al., 2013; Cantrell et al., 2010; Matuszek et al., 2012; Kruijff et al., 2007; Roy et al., 2003). In the past few years, an influx of new, large vision & language corpora, alongside dramatic advances in vision research, has sparked renewed interest in connecting vision and language. Vision & language corpora now provide alignments between visual content that can be recognized with Computer Vision (CV) algorithms and language that can be understood and generated using Natural Language Processing techniques. Fueled in part by the newly emerging data, research that blends techniques in vision and in language has increased at an incredible rate. In just ∗ 1 F.F. and N.M. contributed equally"
D15-1021,H05-2005,1,\N,Missing
D15-1021,W10-0707,0,\N,Missing
D17-1067,D14-1162,0,0.0804139,"nt probability in the list. 5. Minimum, maximum and average letter trigram probabilities using letter bigram and letter trigram counts from the Google Web Trillion Word Corpus (Brants and Franz, 2006). The purpose of these features is to capture the phonetic funniness of words (e.g., “whacking” instead of “fighting”). 6. The candidate’s similarity to the three contexts — title, overall story, and the containing sentence. This is the cosine similarity of the candidate’s word embedding vector with the average word embedding vector for each context. The vectors were computed using GloVe vectors (Pennington et al., 2014) trained with 840 billion tokens from Web data. 5.3 Train 0.712 0.856 0.778 0.727 Baseline Chance Chance Hint Linear SVM (3 feat.) Classification The full training dataset includes 40 labeled Fun Libs, each filled in by 5 different players, each of which was graded by 9 different judges. We split Train 0.571 0.595 0.606 Validation 0.543 0.591 0.600 Table 3: Baseline classification accuracies. 643 ten stories was completed by three players. Figure 3 shows the mean grade for these 30 stories. In the figure, the titles are sorted left to right based on the maximum mean story grade among the title"
D17-1067,N16-1016,0,0.111072,"Missing"
D17-1067,P13-2041,0,0.416576,"Missing"
D17-1067,W10-2914,0,0.266424,"Missing"
D17-1067,N12-2012,0,0.389983,"Missing"
D17-1067,P11-2016,0,0.410109,"Missing"
D17-1067,N10-2012,0,0.0139765,"ss of the overall filled-in story, the funniness contribution of each filled-in word, and other aspects of the humor. For each of the 40 stories in the training set, we used 5 players to fill the blanks, giving us a total of 200 filled-in stories. The players also self-graded the humor in their completed stories. Each of these stories was graded by 9 judges to represent an audience rather than an individual and to reduce effects of different humor tastes. Judges answered the following: 5.1 Language Model To supply reasonable words for each blank, we used the Microsoft Web Language Models API (Wang et al., 2010), trained using Web Page titles and available as a Web service. To generate candidate words for a blank, we use the API’s word completion tool to get a list of all possible candidate words using context windows up to four previous words. Next, for each word we computed the joint probability score, using the API’s joint probability tool to get a probability estimate of how well each candidate fits into the containing sentence. Then we ranked candidates using this score. We expect the words with high scores to be the more fitting (less humorous) words for the context, while those with low scores"
D17-1067,P12-2030,0,0.0318795,"Missing"
D17-1067,W17-0906,0,0.0661862,"Missing"
H05-2005,P98-1013,0,0.142033,"Missing"
H05-2005,P98-2180,1,0.732534,"sent two forms of MindNet: as a static lexical resource, and, as a toolkit which allows MindNets to be built from arbitrary text. We will also introduce a web-based interface to MindNet lexicons (MNEX) that is intended to make the data contained within MindNets more accessible for exploration. Both English and Japanese MindNets will be shown and will be made available, through MNEX, for research purposes. 1 MindNet A MindNet is a collection of semantic relations that is automatically extracted from text data using a broad coverage parser. Previous publications on MindNet (Suzuki et al., 2005, Richardson et al., 1998, Vanderwende 1995) have focused on the effort required to build a MindNet from the data contained in Japanese and English lexicons. Semantic Relations The semantic relations that are stored in MindNet are directed, labeled relationships between two words; see Table 1: Attributive Manner Source Cause Means Synonym Goal Part Time Hypernym Possessor TypicalObject Location Result TypicalSubject Table 1: A sampling of the semantic relations stored in MindNet relations. Anecdotally, however, the quality varies according to the relation type, with Hypernym and grammatical relations TypicalSubject an"
H05-2005,C98-2175,1,\N,Missing
H05-2005,C98-1013,0,\N,Missing
I08-1059,P01-1017,0,0.0447053,"Missing"
I08-1059,W07-1604,0,0.537721,"Felice and Pulman (2007) utilize a set of sophisticated syntactic and semantic analysis features to predict 5 common English prepositions. Obviously, this is impractical in a setting where noisy non-native text is subjected to proofing. Meanwhile, work on automated error detection on non-native text focuses primarily on detection of errors, rather than on the more difficult task of supplying viable corrections (e.g., Chodorow and Leacock, 2000). More recently, Han et al. (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al. (2003) and Chodorow et al. (2007) present techniques of automatic preposition choice modeling. These more recent efforts, nevertheless, do not attempt to integrate their methods into a more general proofing application designed to assist non-native speakers when writing English. Finally, Yi et al. (2008) designed a system that uses web counts to determine correct article usage for a given sentence, targeting ESL users. 4 System Description Our system consists of three major components: 1. Suggestion Provider (SP) 2. Language Model (LM) 3. Example Provider (EP) The Suggestion Provider contains modules for each error type discu"
I08-1059,W07-1607,0,0.562737,"Missing"
I08-1059,J93-1003,0,0.101425,"on sites are determined heuristically from the sequence of POS tags. Based on these features, we train a classifier for preposition choice and determiner choice. Currently we train decision tree classifiers with the WinMine toolkit (Chickering 2002). We also experimented with linear SVMs, but decision trees performed better overall and training and parameter optimization were considerably more efficient. Before training the classifiers, we perform feature ablation by imposing a count cutoff of 10, and by limiting the number of features to the top 75K features in terms of log likelihood ratio (Dunning 1993). We train two separate classifiers for both determiners and preposition:  decision whether or not a determiner/preposition should be present (presence/absence or pa classifier)  decision which determiner/preposition is the most likely choice, given that a determiner/preposition is present (choice or ch classifier) In the case of determiners, class values for the ch classifier are a/an and the. Preposition choice (equivalent to the ―confusion set‖ of a contextual speller) is limited to a set of 13 prepositions that figure prominently in the errors observed in the JLE corpus: about, as, at, b"
I08-1059,W02-2105,1,0.815053,"Missing"
I08-1059,han-etal-2004-detecting,0,0.212248,"Turner and Charniak (2007), for example, utilize a language model based on a statistical parser for Penn Tree Bank data. Similarly, De Felice and Pulman (2007) utilize a set of sophisticated syntactic and semantic analysis features to predict 5 common English prepositions. Obviously, this is impractical in a setting where noisy non-native text is subjected to proofing. Meanwhile, work on automated error detection on non-native text focuses primarily on detection of errors, rather than on the more difficult task of supplying viable corrections (e.g., Chodorow and Leacock, 2000). More recently, Han et al. (2004, 2006) use a maximum entropy classifier to propose article corrections in TESOL essays, while Izumi et al. (2003) and Chodorow et al. (2007) present techniques of automatic preposition choice modeling. These more recent efforts, nevertheless, do not attempt to integrate their methods into a more general proofing application designed to assist non-native speakers when writing English. Finally, Yi et al. (2008) designed a system that uses web counts to determine correct article usage for a given sentence, targeting ESL users. 4 System Description Our system consists of three major components: 1"
I08-1059,I08-2082,1,0.357362,"Missing"
I08-1059,N04-2006,0,0.131483,"Missing"
I08-1059,P00-1067,1,0.853874,"ot a significant problem for native speakers and hence remains unaddressed in proofing tools such as the grammar checker in Microsoft Word (Heidorn 2000). Plainly there is an Targeted Error Types Our system currently targets eight different error types: 1. Preposition presence and choice: In the other hand, ... (On the other hand ...) 2. Definite and indefinite determiner presence and choice: I am teacher... (am a teacher) 3. Gerund/infinitive confusion: I am interesting in this book. (interested in) 4. Auxiliary verb presence and choice: My teacher does is a good teacher (my teacher is...) 1 Liu et al. 2000 take a similar approach, retrieving example sentences from a large corpus. 449 5. Over-regularized verb inflection: I writed a letter (wrote) 6. Adjective/noun confusion: This is a China book (Chinese book) 7. Word order (adjective sequences and nominal compounds): I am a student of university (university student) 8. Noun pluralization: They have many knowledges (much knowledge) In this paper we will focus on the two most prominent and difficult errors: choice of determiner and prepositions. Empirical justification for targeting these errors comes from inspection of several corpora of non-nat"
I08-1059,W00-0708,0,0.590938,"Missing"
I08-1059,P06-1132,0,0.116761,"Missing"
I08-1059,N07-2045,0,0.363375,"Missing"
I08-1059,N07-1007,0,\N,Missing
I17-1047,N15-1053,0,0.036937,"Missing"
I17-1047,P04-1077,0,0.0230767,"ext C. The function V counts the number of verbs in the hypothesis and |h |denotes the number of tokens in the hypothesis. The function idf is the inverse document frequency, computing how common a hypothesis is across all the generated N-best lists. Here D is the set of all N-best lists and d is a specific N|D| best list. We define idf(h, D) = log |{d∈D:h∈d}| , where we set N =10 to cut short each N-best list. These parameters were selected following reranking experiments on the validation set. We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011). 5.2 Retrieval Models In addition to generation, we implemented two retrieval models customized for the tasks of question and response generation. Work in vision and language has demonstrated the effectiveness of retrieval models, where one uses the annotation (e.g., caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016b; Devlin et al., 2015; 3 An example generic question is where is this? and a generic response is I don’t know. 467 Do you think this happened"
I17-1047,P15-2017,0,0.200084,"des answers. Figure 2 contrasts an example ICG conversation with the VisDial dataset. As this example shows, IGC involves natural conversations with the image as the grounding, where the literal objects (e.g., the pumpkins) may not even be mentioned in the conversation at all, whereas VisDial targets explicit image understanding. More recently, Das et al. (2017b) have explored the VisDial dataset with richer models that incorporate deep-reinforcement learning. Related Work Vision and Language Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al."
I17-1047,D16-1230,0,0.0151424,"Example question and response generations on IGCCrowd test set. All the generation models use beam search with reranking. In the textual context, <UTT> separates different utterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual Context Sensitive Model (V-Ret). This model uses only the provided image for retrieval. First, we find a set of K nearest training images for the given test image based on cosine similarity of the f c7 vision feature vectors. Then we"
I17-1047,W15-4640,0,0.0312834,"ce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logue corpus (Lowe et al., 2015) is the largest corpus of dialogues (almost 1 million mainly 3-turn dialogues) for the specific topic of troubleshooting Ubuntu problems. On the other hand, for openended conversation modeling (chitchat), now a high demand application in AI, shared datasets with which to track progress are severely lacking. The ICG task presented here lies nicely in the continuum between the two, where the visual grounding of event-centric images constrains the topic of conversation to contentful utterances. To enable benchmarking of progress in the IGC task, we constructed the IGCCrowd dataset for validation"
I17-1047,P15-2073,1,0.665818,"tterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual Context Sensitive Model (V-Ret). This model uses only the provided image for retrieval. First, we find a set of K nearest training images for the given test image based on cosine similarity of the f c7 vision feature vectors. Then we retrieve those K annotations as our pool of K candidates. Finally, we compute the textual similarity among the questions in the pool according to a Smoothed-BLEU (Lin and Och,"
I17-1047,W16-1007,1,0.460669,"Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logu"
I17-1047,D11-1125,0,0.00844008,"|h |denotes the number of tokens in the hypothesis. The function idf is the inverse document frequency, computing how common a hypothesis is across all the generated N-best lists. Here D is the set of all N-best lists and d is a specific N|D| best list. We define idf(h, D) = log |{d∈D:h∈d}| , where we set N =10 to cut short each N-best list. These parameters were selected following reranking experiments on the validation set. We optimize all the parameters of the scoring function towards maximizing the smoothed-BLEU score (Lin and Och, 2004) using the Pairwise Ranking Optimization algorithm (Hopkins and May, 2011). 5.2 Retrieval Models In addition to generation, we implemented two retrieval models customized for the tasks of question and response generation. Work in vision and language has demonstrated the effectiveness of retrieval models, where one uses the annotation (e.g., caption) of a nearest neighbor in the training image set to annotate a given test image (Mostafazadeh et al., 2016b; Devlin et al., 2015; 3 An example generic question is where is this? and a generic response is I don’t know. 467 Do you think this happened on highway <EOS> <GO> day this not . . decoder This was not the way I imag"
I17-1047,P16-1170,1,0.732217,"Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Questions are required to be ‘natural and engaging’, i.e. a person would find them interesting to answer, but need not be answerable from the image alone. In this work, we introduce multimodal context, recognizing that images commonly come associated with a verbal commentary that can affect the interpretation. This is thus a broader, more complex task that involves implicit commonsense reasoning around both image and text. 463 2.2 Data-Driven Conversational Modeling logu"
I17-1047,N16-1147,1,0.891951,"Missing"
I17-1047,P02-1040,0,0.122669,"at this baseball game. <UTT> Nice, which team won? My team won this game. 10 for me and 28 for my dad. ding ding ding! No it wasn’t too bad of a bang up. Yes. Nah, I’m at home now. lords cricket ground . beautiful. He’s not mine! Table 4: Example question and response generations on IGCCrowd test set. All the generation models use beam search with reranking. In the textual context, <UTT> separates different utterances. The generations in bold are acceptable utterances given the underlying context. 468 evaluation. For ease of replicability, we use the standard Machine Translation metric, BLEU (Papineni et al., 2002), which captures n-gram overlap between hypotheses and multiple references. Results reported in Table 6 employ BLEU with equal weights up to 4-grams at corpus-level on the multi-reference IGCCrowd test set. Although Liu et al. (2016) suggest that BLEU fails to correlate with human judgment at the sentence level, correlation increases when BLEU is applied at the document or corpus level (Galley et al., 2015; Przybocki et al., 2008). Figure 7: The visual & textual context sensitive model with RNN encoding (V&T.RNN-Gen). Hodosh et al., 2013; Ordonez et al., 2011; Farhadi et al., 2010). 7 Visual C"
I17-1047,N16-1014,1,0.87514,"tistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following two consecutive conversational steps: • Question Generation: Gi"
I17-1047,P16-1094,1,0.153491,"tistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following two consecutive conversational steps: • Question Generation: Gi"
I17-1047,D16-1090,0,0.027098,"sations with the image as the grounding, where the literal objects (e.g., the pumpkins) may not even be mentioned in the conversation at all, whereas VisDial targets explicit image understanding. More recently, Das et al. (2017b) have explored the VisDial dataset with richer models that incorporate deep-reinforcement learning. Related Work Vision and Language Visual features combined with language modeling have shown good performance both in image captioning (Devlin et al., 2015; Xu et al., 2015; Fang et al., 2014; Donahue et al., 2015) and in question answering on images (Antol et al., 2015; Ray et al., 2016; Malinowski and Fritz, 2014), when trained on large datasets, such as the COCO dataset (Lin et al., 2014). In Visual Question Answering (VQA) (Antol et al., 2015), a system is tasked with answering a question about a given image, where the questions are constrained to be answerable directly from the image. In other words, the VQA task primarily serves to evaluate the extent to which the system has recognized the explicit content of the image. Mostafazadeh et al. (2016b) introduce the task of visual question generation (VQG), in which the system itself outputs questions about a given image. Qu"
I17-1047,D11-1054,0,0.0139427,"ional questions and responses for the IGCCrowd contexts and initial questions. Table 1 shows three full conversations found in the IGCCrowd dataset. These examples show show that eventful images lead to conversations that are semantically rich and appear to involve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models t"
I17-1047,P15-1152,0,0.0151711,"olve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC as the following tw"
I17-1047,N15-1020,1,0.516832,"rich and appear to involve commonsense reasoning. Table 2 summarizes basic dataset statistics. The IGCCrowd dataset has been released as the Microsoft Research Image-Grounded Conversation dataset (https://www.microsoft.com/en-us/ download/details.aspx?id=55324& 751be11f-ede8). This work is also closely linked to research on data-driven conversation modeling. Ritter et al. (2011) posed response generation as a machine translation task, learning conversations from parallel message-response pairs found on social media. Their work has been successfully extended with the use of deep neural models (Sordoni et al., 2015; Shang et al., 2015; Serban et al., 2015a; Vinyals and Le, 2015; Li et al., 2016a,b). Sordoni et al. (2015) introduce a context-sensitive neural language model that selects the most probable response conditioned on the conversation history (i.e., a text-only context). In this paper, we extend the contextual approach with multimodal features to build models that are capable of asking questions on topics of interest to a human that might allow a conversational agent to proactively drive a conversation forward. 3 3.1 Image-Grounded Conversations Task Definition We define the current scope of IGC"
I17-1047,N15-1173,0,0.0116461,"Missing"
I17-1047,H89-1033,0,0.533497,"Missing"
I17-1047,N10-1020,1,\N,Missing
I17-1047,P98-1013,0,\N,Missing
I17-1047,C98-1013,0,\N,Missing
J14-2010,D13-1195,0,0.0155957,"same or different relation type (Lin and Pantel 2001, inter alia). These paradigms are well described in this chapter and, once again, the authors provide plentiful information that invites the reader to go into greater depth wherever their interests lie, with the odd exception that the authors steer the reader away from any approach that uses parsing, claiming that “deeper processing (e.g., syntactic parsing) is altogether infeasible on a Web scale” (page 79, passim). We know that Google is parsing the Web already (Petrov and McDonald 2012) and parsing is becoming orders-ofmagnitude faster (Canny et al., 2013), so the reader would be well advised to stay open to the possibilities. Parsing was demonstrated to be crucial for extracting higher-order semantic relations such as Location and Purpose (Montemagni and Vanderwende 1992).3 2 Readers may also wish to know about the project NomBank (Meyers et al., 2004), which is not included in this chapter. 3 Coincidentally, this paper was presented in the same session as Marti Hearst’s paper at COLING 1992; note the page numbers in the References. At the time, however, very few institutions had access to a broad-coverage parser, but I suspect that parsing an"
J14-2010,C92-2082,0,0.214099,"of the material provides good entry points where the reader can find plentiful references if they are curious about the approach. The summary sections are very good reviews and would suffice to get a general idea of how the task of extracting semantic relations can be approached. In particular, in Section 3.5, the authors provide useful points to consider for navigating the myriad possibilities of models and resources. When no labeled data is available, a wider set of paradigms for extracting semantic relations needs to be explored (in Chapter 4). These range from manually authored patterns (Hearst 1992, inter alia) for extracting predetermined relation types, to identifying novel relations with open relation extraction. OpenIE learns relations as expressed by verbs or prepositions using sophisticated algorithms to determine when the verbs or prepositions express the same or different relation type (Lin and Pantel 2001, inter alia). These paradigms are well described in this chapter and, once again, the authors provide plentiful information that invites the reader to go into greater depth wherever their interests lie, with the odd exception that the authors steer the reader away from any app"
J14-2010,W04-2705,0,0.0346118,"the reader away from any approach that uses parsing, claiming that “deeper processing (e.g., syntactic parsing) is altogether infeasible on a Web scale” (page 79, passim). We know that Google is parsing the Web already (Petrov and McDonald 2012) and parsing is becoming orders-ofmagnitude faster (Canny et al., 2013), so the reader would be well advised to stay open to the possibilities. Parsing was demonstrated to be crucial for extracting higher-order semantic relations such as Location and Purpose (Montemagni and Vanderwende 1992).3 2 Readers may also wish to know about the project NomBank (Meyers et al., 2004), which is not included in this chapter. 3 Coincidentally, this paper was presented in the same session as Marti Hearst’s paper at COLING 1992; note the page numbers in the References. At the time, however, very few institutions had access to a broad-coverage parser, but I suspect that parsing and features derived from parsing will soon become commonplace. 516 Book Reviews In the end, the authors note that “relation extraction is not an end task, but its purpose is to build resources to be used by other NLP and AI applications” (page 80). And the application is what will determine which set of"
J14-2010,C92-2083,1,0.310681,"into greater depth wherever their interests lie, with the odd exception that the authors steer the reader away from any approach that uses parsing, claiming that “deeper processing (e.g., syntactic parsing) is altogether infeasible on a Web scale” (page 79, passim). We know that Google is parsing the Web already (Petrov and McDonald 2012) and parsing is becoming orders-ofmagnitude faster (Canny et al., 2013), so the reader would be well advised to stay open to the possibilities. Parsing was demonstrated to be crucial for extracting higher-order semantic relations such as Location and Purpose (Montemagni and Vanderwende 1992).3 2 Readers may also wish to know about the project NomBank (Meyers et al., 2004), which is not included in this chapter. 3 Coincidentally, this paper was presented in the same session as Marti Hearst’s paper at COLING 1992; note the page numbers in the References. At the time, however, very few institutions had access to a broad-coverage parser, but I suspect that parsing and features derived from parsing will soon become commonplace. 516 Book Reviews In the end, the authors note that “relation extraction is not an end task, but its purpose is to build resources to be used by other NLP and A"
klassen-etal-2014-annotating,E12-2021,0,\N,Missing
N04-4001,W00-0405,0,\N,Missing
N04-4001,W03-0510,0,\N,Missing
N04-4001,J02-4006,0,\N,Missing
N04-4001,N03-1020,0,\N,Missing
N06-1005,H05-1079,0,0.0236332,"Missing"
N06-1005,W01-0808,0,0.0203379,", Pt ) = |X|X t| We then incorporate paraphrase similarity within the lexical similarity model by allowing, for some unaligned node h ∈ Ph , where t ∈ Pt : sim(h, t) = max(MN(h, t), score(Ph , Pt )) http://research.microsoft.com/mnex 38 Our approach to paraphrase detection is most similar to the TE/ASE algorithm (Szpektor et al., 2004), and bears similarity to both DIRT (Lin and Pantel, 2001) and KnowItAll (Etzioni et al., 2004). The chief difference in our algorithm is that we generate the surface text search strings from the parsed logical forms using the generation capabilities of NLPW IN (Aikawa et al., 2001), and we verify that the syntactic relations in each discovered web snippet are isomorphic to those in the original candidate paraphrase template. 6 Task CD RC IR MT IE QA PP All Dev Set acc cws 0.8061 0.8357 0.5534 0.5885 0.6857 0.6954 0.7037 0.7145 0.5857 0.6008 0.7111 0.7121 0.7683 0.7470 0.6878 0.6888 Test Set acc cws 0.7867 0.8261 0.6429 0.6476 0.6000 0.6571 0.6000 0.6350 0.5917 0.6275 0.5308 0.5463 0.5200 0.5333 0.6250 0.6534 Table 2: Summary of accuracies and confidenceweighted scores, by task Results and Discussion In this section we present the final results of our system on the PASCA"
N06-1005,W04-3206,0,0.0372052,"hat satisfy the same syntactic roles as t1 in the original sentence. 2. Similarly, extract the slot fillers Xh for each discovered phrase template Ph in H. 3. Calculate paraphrase similarity as a function of the overlap between the slot-filler sets Xt and h ∩Xt | . Xh , i.e: score(Ph , Pt ) = |X|X t| We then incorporate paraphrase similarity within the lexical similarity model by allowing, for some unaligned node h ∈ Ph , where t ∈ Pt : sim(h, t) = max(MN(h, t), score(Ph , Pt )) http://research.microsoft.com/mnex 38 Our approach to paraphrase detection is most similar to the TE/ASE algorithm (Szpektor et al., 2004), and bears similarity to both DIRT (Lin and Pantel, 2001) and KnowItAll (Etzioni et al., 2004). The chief difference in our algorithm is that we generate the surface text search strings from the parsed logical forms using the generation capabilities of NLPW IN (Aikawa et al., 2001), and we verify that the syntactic relations in each discovered web snippet are isomorphic to those in the original candidate paraphrase template. 6 Task CD RC IR MT IE QA PP All Dev Set acc cws 0.8061 0.8357 0.5534 0.5885 0.6857 0.6954 0.7037 0.7145 0.5857 0.6008 0.7111 0.7121 0.7683 0.7470 0.6878 0.6888 Test Set a"
N06-1005,H05-1047,0,0.0364108,"Missing"
N09-1041,N04-1015,0,0.0361536,"rk, we ignore the summary focus. Here, the word topic will refer to elements of our statistical model rather than summary focus. 2 Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent 362 There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see sec"
N09-1041,P07-1069,0,0.0151406,"ences in documents have a higher proportion of general content words and so this tendency is to be expected. 369 6.1 Content Navigation A common concern in multi-document summarization is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible. While many variants of the general summarization task have been proposed which utilize such information (Vanderwende et al., 2007; Nastase, 2008), this presupposes that a user knows enough of the content of a document collection in order to propose a query. As Leuski et al. (2003) and Branavan et al. (2007) suggest, a document summarization system should facilitate content discovery and yield summaries relevant to a user’s interests. We may use H IER S UM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by φC1 , . . . , φCK (for an example see figure 3). While these topics are not adaptive to user interest, they typically reflect lexically coherent vocabularies. Conclusion In this paper we have presented an exploration of content models for multi-document summarization and demonstrated that the use of struc"
N09-1041,P06-1039,0,0.635279,"Missing"
N09-1041,D08-1035,0,0.153275,"c will refer to elements of our statistical model rather than summary focus. 2 Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent 362 There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilistic topic models (Blei et al., 2003) can offer state-of-the-art summarization quality as measured by automatic metrics (see section 5.1) and manual user evaluation (see section 5.2). We also contend that they provide convenient buil"
N09-1041,P03-1069,0,0.00693879,"enient building blocks for adding more structure to a summarization model. In particular, we utilize a variation of the hierarchical LDA topic model (Blei et al., 2004) to discover multiple specific ‘subtopics’ within a document set. The resulting model, H IER S UM (see section 3.4), can produce general summaries as well as summaries for any of the learned sub-topics. 2 Experimental Setup The task we will consider is extractive multidocument summarization. In this task we assume a document collection D consisting of documents D1 , . . . , Dn describing the same (or closely related) narrative (Lapata, 2003). Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 362–370, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics set of events. Our task will be to propose a summary S consisting of sentences in D totaling at most L words.3 Here as in much extractive summarization, we will view each sentence as a bag-of-words or more generally a bag-of-ngrams (see section 5.1). The most prevalent example of this data setting is document clusters found on news aggregator sites. 2.1 Automated Evaluation For model development we will"
N09-1041,P03-2021,0,0.0159405,"ed to consider early sentences in documents have a higher proportion of general content words and so this tendency is to be expected. 369 6.1 Content Navigation A common concern in multi-document summarization is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible. While many variants of the general summarization task have been proposed which utilize such information (Vanderwende et al., 2007; Nastase, 2008), this presupposes that a user knows enough of the content of a document collection in order to propose a query. As Leuski et al. (2003) and Branavan et al. (2007) suggest, a document summarization system should facilitate content discovery and yield summaries relevant to a user’s interests. We may use H IER S UM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by φC1 , . . . , φCK (for an example see figure 3). While these topics are not adaptive to user interest, they typically reflect lexically coherent vocabularies. Conclusion In this paper we have presented an exploration of content models for multi-document summarization and demonst"
N09-1041,N06-1059,0,0.0211644,"of T OPIC S UM model (see section 3.3). Note that many hyperparameter dependencies are omitted for compactness. where PS is the empirical unigram distribution of the candidate summary S and KL(P kQ) represents the Kullback-Lieber (KL) divergence given by P P (w) 10 This quantity represents the w P (w) log Q(w) . divergence between the true distribution P (here the document set unigram distribution) and the approximating distribution Q (the summary distribution). This criterion casts summarization as finding a set of summary sentences which closely match the document set unigram distribution. Lin et al. (2006) propose a related criterion for robust summarization evaluation, but to our knowledge this criteria has been unexplored in summarization systems. We address optimizing equation (2) as well as summary sentence ordering in section 4. KLS UM yields 6.0 R-2 without stop words, beating S UM BASIC but not with statistical significance. It is worth noting however that KLS UM’s performance matches S UM F OCUS (Vanderwende et al., 2007), the highest R-2 performing system at DUC 2006. 3.3 ROUGE -stop R-1 R-2 R-SU4 29.6 5.3 8.6 30.6 6.0 8.9 31.7 6.3 9.1 30.5 6.4 9.2 TopicSum As mentioned in section 3.2,"
N09-1041,W04-1013,0,0.55098,"ummary S consisting of sentences in D totaling at most L words.3 Here as in much extractive summarization, we will view each sentence as a bag-of-words or more generally a bag-of-ngrams (see section 5.1). The most prevalent example of this data setting is document clusters found on news aggregator sites. 2.1 Automated Evaluation For model development we will utilize the DUC 2006 evaluation set4 consisting of 50 document sets each with 25 documents; final evaluation will utilize the DUC 2007 evaluation set (section 5). Automated evaluation will utilize the standard DUC evaluation metric ROUGE (Lin, 2004) which represents recall over various n-grams statistics from a system-generated summary against a set of humangenerated peer summaries.5 We compute ROUGE scores with and without stop words removed from peer and proposed summaries. In particular, we utilize R-1 (recall against unigrams), R-2 (recall against bigrams), and R-SU4 (recall against skip-4 bigrams)6 . We present R-2 without stop words in the running text, but full development results are presented in table 1. Official DUC scoring utilizes the jackknife procedure and assesses significance using bootstrapping resampling (Lin, 2004). In"
N09-1041,D08-1080,0,0.0108533,"to select longer sentences typically chosen from an early sentence in a document. As discussed in section 3.4, H IER S UM is biased to consider early sentences in documents have a higher proportion of general content words and so this tendency is to be expected. 369 6.1 Content Navigation A common concern in multi-document summarization is that without any indication of user interest or intent providing a single satisfactory summary to a user may not be feasible. While many variants of the general summarization task have been proposed which utilize such information (Vanderwende et al., 2007; Nastase, 2008), this presupposes that a user knows enough of the content of a document collection in order to propose a query. As Leuski et al. (2003) and Branavan et al. (2007) suggest, a document summarization system should facilitate content discovery and yield summaries relevant to a user’s interests. We may use H IER S UM in order to facilitate content discovery via presenting a user with salient words or phrases from the specific content topics parametrized by φC1 , . . . , φCK (for an example see figure 3). While these topics are not adaptive to user interest, they typically reflect lexically coheren"
N09-1041,N06-2046,0,0.0327644,"f generating cogent sentences, many systems opt for an extractive approach, selecting sentences from the document set which best reflect its core content.2 1 In this work, we ignore the summary focus. Here, the word topic will refer to elements of our statistical model rather than summary focus. 2 Note that sentence extraction does not solve the problem of selecting and ordering summary sentences to form a coherent 362 There are several approaches to modeling document content: simple word frequency-based methods (Luhn, 1958; Nenkova and Vanderwende, 2005), graph-based approaches (Radev, 2004; Wan and Yang, 2006), as well as more linguistically motivated techniques (Mckeown et al., 1999; Leskovec et al., 2005; Harabagiu et al., 2007). Another strand of work (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Eisenstein and Barzilay, 2008), has explored the use of structured probabilistic topic models to represent document content. However, little has been done to directly compare the benefit of complex content models to simpler surface ones for generic multi-document summarization. In this work we examine a series of content models for multi-document summarization and argue that LDA-style probabilist"
N10-1123,W09-1402,0,0.315341,"extraction. The interdependencies among events and arguments naturally argue for joint predictions. For example, given the snippet “the level of VCAM1 mRNA”, knowing that “level” might signify an event helps to recognize the prepositional phrase (PP) as its theme. Conversely, the presence of the PP suggests that “level” is likely an event. Moreover, the word “mRNA” in the PP indicates that the event type is probably Transcription. Most existing systems adopt a pipeline architecture and reduce the task to independent classifications of events and arguments. For example, the best system UTurku (Bjorne et al., 2009) first extracts a list of candidate triggers with types, and then determines for each pair of candidate triggers or proteins whether one is a theme or cause of the other. The triggers missed in the first stage can never be recovered in the second one. Moreover, since the second stage is trained with gold triggers as input, any trigger identified in the first stage tends to get at least 2 The Shared Task also defines two other tasks (Tasks 2 and 3), which aim either to extract additional arguments (e.g., sites), or to determine if an event is a negation or speculation. In this paper, we focus o"
N10-1123,de-marneffe-etal-2006-generating,0,0.0118818,"Missing"
N10-1123,W09-1201,0,0.0214031,"ntial modeling by adapting a state-of-the-art PPI system based on MEMM. But they considered adjacent words in the sentence, which offered little help in this task, and their system trailed UTurku by 15 points in F1. All top systems for event extraction relied heavily on syntactic features. We went one step further by formulating joint predictions directly on dependency edges. While this leverages sequential correlation along argument paths, it also makes our system more prone to the adverse effect of syntactic errors. Joint syntactic and semantic processing has received much attention lately (Hajic et al., 2009). In this paper, we explore using a heuristic method to correct syntactic errors based on semantic information, and show that it leads to significant performance gain for event extraction. 3 Markov Logic In many NLP applications, there exist rich relation structures among objects, and recent work in statistical relational learning (Getoor and Taskar, 2007) and structured prediction (Bakir et al., 2007) has shown that leveraging these can greatly improve accuracy. One of the leading frameworks for joint inference is Markov logic, a probabilistic extension of firstorder logic (Domingos and Lowd,"
N10-1123,W09-1401,0,0.716147,"ith more than two thousand new abstracts added each day; the abstracts are written in grammatical English, which enables the use of advanced NLP tools such as syntactic and semantic parsers. Traditionally, research on knowledge extraction from text is primarily pursued in the field of information extraction with a rather confined goal of extracting instances for flat relational schemas with no nested structures (e.g, recognizing protein names and protein-protein interaction (PPI)). This restriction mainly stems from limitations in available resources and algorithms. The BioNLP’09 Shared Task (Kim et al., 2009) is one of the first that faced squarely information needs that are complex and highly structured. It aims to extract nested bio-molecular events from research abstracts, where an event may have variable number of arguments and may contain other events as arguments. Such nested events are ubiquitous in biomedical literature and can effectively represent complex biomedical knowledge and subsequently support reasoning and automated discovery. The task has generated much interest, with twenty-four teams having submitted their results. The top system by UTurku (Bjorne et al., 2009) attained the st"
N10-1123,N09-1024,1,0.059987,"a, we count the numbers of true and false groundings in the train data, and add the smaller of the two plus one to the variance, before dividing the global rate by it. 6 Available at http://research.microsoft.com/en-us/people/lucyv/naacl10. We found that this is effective for making learning stable in our experiments. To compute the most probable state, we used MCSAT to estimate the marginal probability of each query atom, and returned the ones with probability above a threshold. This allows us to easily trade off precision and recall by varying the threshold. To speed up burn-in, we followed Poon et al. (2009) and first ran MC-SAT with deterministic annealing for initialization. 6 Correcting Syntactic Errors With Semantic Information Two typical types of syntactic errors are PPattachment and coordination. For semantic tasks such as bio-event extraction, these errors also have the most adverse impact to performance. For example, for the snippet “involvement of p70 activation in IL-10 up-regulation by gp41”, the Stanford parser makes two errors by attaching “upregulation” to “activation” instead of “involvement”, and attaching “gp41” to “involvement” instead of “up-regulation”. This makes it very dif"
N10-1123,W09-1406,0,0.725408,"uments, joint inference can facilitate mutual disambiguation and potentially lead to substantial gain 1 http://www.ncbi.nlm.nih.gov/pubmed 813 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 813–821, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics in predictive accuracy. However, joint inference is underexplored for this task. Most participants either reduced the task to classification (e.g., by using SVM), or used heuristics to combine manual rules and statistics. The previous best joint approach was Riedel et al. (2009). While competitive, it still lags UTurku by more than 7 points in F1. In this paper, we present the first joint approach that achieves state-of-the-art results for bio-event extraction. Like Riedel et al. (2009), our system is based on Markov logic, but we adopted a novel formulation that models dependency edges in argument paths and jointly predicts them along with events and arguments. By expanding the scope of joint inference to include individual argument edges, our system can leverage fine-grained correlations to make learning more effective. On the development set, by merely adding a fe"
N10-1123,W09-1414,0,0.0234314,"a multi-class SVM. While joint inference can potentially improve accuracy, in practice, it is often very challenging to make it work (Poon and Domingos, 2007). The previous best joint approach for this task was proposed by Riedel et al. (2009) (labeled UT+DBLS in Kim et al. (2009)). Their system is also based on Markov logic (Domingos and Lowd, 2009). While competitive (ranked fourth in the evaluation), their system still lags UTurku by more than 7 points in F1. Most systems, Riedel et al.’s included, classify each candidate argument path as a whole. A notable exception is the UTokyo system (Saetre et al., 2009), which incorporated sequential modeling by adapting a state-of-the-art PPI system based on MEMM. But they considered adjacent words in the sentence, which offered little help in this task, and their system trailed UTurku by 15 points in F1. All top systems for event extraction relied heavily on syntactic features. We went one step further by formulating joint predictions directly on dependency edges. While this leverages sequential correlation along argument paths, it also makes our system more prone to the adverse effect of syntactic errors. Joint syntactic and semantic processing has receiv"
N12-1092,W11-1407,0,0.0677604,"pics to give to different students, and students want different tests with which to study and practice the material. One possible solution to providing quizzes for new source material is the automatic generation of questions. This is a task the NLP community has already embraced, and significant progress has been made in recent years with the introduction of a shared task (Rus et al., 2010). However, thus far the research community has focused on the problem of generating grammatical questions (as in Heilman and Smith (2010a)) or generating effective distractors for multiple-choice questions (Agarwal and Mannem, 2011). While both of these research threads are of critical importance, there is another key issue that must be addressed – which questions should we be asking in the first place? We have highlighted this aspect of the problem in the past (see Vanderwende (2008)) and begin to address it in this work, postulating that we can both collect human judgments on what makes a good question and train a machine learning model that can replicate these judgments. The resulting learned model can then be applied to new material for automated question generation. We see this effort as complementary to the earlier"
N12-1092,H05-1103,0,0.346835,"Missing"
N12-1092,W10-0701,0,0.0214114,"Missing"
N12-1092,W11-2705,0,0.0958047,"n these components can be used as input for more complex surface generation such as Whforms or distractor selection. Background and Related Work There already exists a large body of work in automatic question generation (QG) for educational purposes dating back to the Autoquest system (Wolfe, 1976), which used an entirely syntactic approach to generate Wh-Questions from individual sentences. In addition to Autoquest, several others have created systems for Wh-question generation using approaches including transformation rules (Mitkov and Ha, 2003), templatebased generation (Chen et al., 2009; Curto et al., 2011), and overgenerate-and-rank (Heilman and Smith, 2010a). The work in this area has largely focused on the surface form of the questions, with an emphasis on grammaticality. Alternatively, generation of gap-fill style questions (a.k.a. cloze questions) avoids these issues of grammaticality by blanking out words or spans in a known good sentence. There is a large body of existing work that has focused on generation of this 743 3 Question Generation In 1874 Röntgen a lecturer at the University of Strassburg. In Röntgen became a lecturer at the University of Strassburg. In 1874 became a lecturer at"
N12-1092,N10-1086,0,0.173855,"ettings, there is a need for automated test generation, as teachers want multiple tests for topics to give to different students, and students want different tests with which to study and practice the material. One possible solution to providing quizzes for new source material is the automatic generation of questions. This is a task the NLP community has already embraced, and significant progress has been made in recent years with the introduction of a shared task (Rus et al., 2010). However, thus far the research community has focused on the problem of generating grammatical questions (as in Heilman and Smith (2010a)) or generating effective distractors for multiple-choice questions (Agarwal and Mannem, 2011). While both of these research threads are of critical importance, there is another key issue that must be addressed – which questions should we be asking in the first place? We have highlighted this aspect of the problem in the past (see Vanderwende (2008)) and begin to address it in this work, postulating that we can both collect human judgments on what makes a good question and train a machine learning model that can replicate these judgments. The resulting learned model can then be applied to ne"
N12-1092,W10-0705,0,0.208437,"ettings, there is a need for automated test generation, as teachers want multiple tests for topics to give to different students, and students want different tests with which to study and practice the material. One possible solution to providing quizzes for new source material is the automatic generation of questions. This is a task the NLP community has already embraced, and significant progress has been made in recent years with the introduction of a shared task (Rus et al., 2010). However, thus far the research community has focused on the problem of generating grammatical questions (as in Heilman and Smith (2010a)) or generating effective distractors for multiple-choice questions (Agarwal and Mannem, 2011). While both of these research threads are of critical importance, there is another key issue that must be addressed – which questions should we be asking in the first place? We have highlighted this aspect of the problem in the past (see Vanderwende (2008)) and begin to address it in this work, postulating that we can both collect human judgments on what makes a good question and train a machine learning model that can replicate these judgments. The resulting learned model can then be applied to ne"
N12-1092,W05-0203,0,0.0462787,"Missing"
N12-1092,W03-0203,0,0.18231,"ile this pipeline currently produces gap-fill questions, we envision these components can be used as input for more complex surface generation such as Whforms or distractor selection. Background and Related Work There already exists a large body of work in automatic question generation (QG) for educational purposes dating back to the Autoquest system (Wolfe, 1976), which used an entirely syntactic approach to generate Wh-Questions from individual sentences. In addition to Autoquest, several others have created systems for Wh-question generation using approaches including transformation rules (Mitkov and Ha, 2003), templatebased generation (Chen et al., 2009; Curto et al., 2011), and overgenerate-and-rank (Heilman and Smith, 2010a). The work in this area has largely focused on the surface form of the questions, with an emphasis on grammaticality. Alternatively, generation of gap-fill style questions (a.k.a. cloze questions) avoids these issues of grammaticality by blanking out words or spans in a known good sentence. There is a large body of existing work that has focused on generation of this 743 3 Question Generation In 1874 Röntgen a lecturer at the University of Strassburg. In Röntgen became a lect"
N12-1092,J05-1004,0,0.0123049,"Missing"
N12-1092,N12-3006,1,0.708423,"ring above a threshold, and so on. Generation Although it would be possible to select every word or phrase as a candidate gap, this tactic would produce a skewed dataset composed mostly of unusable questions, which would subsequently require much more annotation to discriminate good questions from bad ones. Instead we rely on syntactic 744 and semantic constraints to reduce the number of questions that need annotation. To generate questions we first run the source sentence through a constituency parser and a semantic role labeler (components of a state-of-theart natural language toolkit from (Quirk et al., 2012)), with the rationale that important parts of the sentence will occur within a semantic role. Each verb predicate found within the roles then automatically becomes a candidate gap. From every argument to the predicate, we extract all child noun phrases (NP) and adjectival phrases (ADJP) as candidate gaps as well. Figure 1 illustrates this generation process. Classification To train the classifier for question quality, we aggregated per-question ratings into a single label (see Section 4 for details). Questions with an average rating of 0.67 or greater were considered as positive examples. This"
N12-1092,W10-4234,0,0.0383627,"self-learning settings, they would still benefit from having the means for assessment available. Even in traditional educational settings, there is a need for automated test generation, as teachers want multiple tests for topics to give to different students, and students want different tests with which to study and practice the material. One possible solution to providing quizzes for new source material is the automatic generation of questions. This is a task the NLP community has already embraced, and significant progress has been made in recent years with the introduction of a shared task (Rus et al., 2010). However, thus far the research community has focused on the problem of generating grammatical questions (as in Heilman and Smith (2010a)) or generating effective distractors for multiple-choice questions (Agarwal and Mannem, 2011). While both of these research threads are of critical importance, there is another key issue that must be addressed – which questions should we be asking in the first place? We have highlighted this aspect of the problem in the past (see Vanderwende (2008)) and begin to address it in this work, postulating that we can both collect human judgments on what makes a go"
N12-1092,D08-1027,0,0.0450133,"Missing"
N12-1092,P99-1032,0,0.0800847,"Missing"
N12-3006,D09-1111,1,0.814882,"Missing"
N12-3006,J93-2004,0,0.04636,"the NLP group at Microsoft Research. The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty,"
N12-3006,J05-1004,0,0.0277178,". The tools include both traditional linguistic analysis tools such as part-of-speech taggers, constituency and dependency parsers, and more recent developments such as sentiment detection and linguistically valid morphology. As we expand the tools we develop for our own research, the set of tools available in MSR SPLAT will be extended. The toolkit is accessible as a web service, which can be used from a broad set of programming languages. 1 Introduction The availability of annotated data sets that have become community standards, such as the Penn TreeBank (Marcus et al., 1993) and PropBank (Palmer et al., 2005), has enabled many research institutions to build core natural language processing components, including part-of-speech taggers, chunkers, and parsers. There remain many differences in how these components are built, resulting in slight but noticeable variation in the component output. In experimental settings, it has proved sometimes difficult to distinguish between improvements contributed by a specific component feature from improvements due to using a differently-trained linguistic component, such as tokenization. The community recognizes this difficulty, and shared task organizers are now"
N12-3006,P06-1055,0,0.0153341,"Missing"
N12-3006,C08-1094,0,0.0225179,"Missing"
N12-3006,J08-2002,1,0.0600596,"abels, like ARG0, ARG1, …, ARG5 for core arguments, and labels like ARGMTMP,ARGM-LOC, etc. for adjunct-like arguments. The meaning of the numbered arguments is verb-specific, with ARG0 typically representing an agent-like role, and ARG1 a patient-like role. This implementation of an SRL system follows the approach described in (Xue and Palmer, 04), and includes two log-linear models for argument identification and classification. A single syntax tree generated by the MSR SPLAT split-merge parser is used as input. Non-overlapping arguments are derived using the dynamic programming algorithm by Toutanova et al. (2008). 3 3.1 Other Language Analysis Functionality Sentence Boundary / Tokenization This analyzer identifies sentence boundaries and breaks the input into tokens. Both are represented as offsets of character ranges. Each token has both a raw form from the string and a normalized form in the PTB specification, e.g., open and close parentheses are replaced by -LRB- and -RRB-, respectively, to remove ambiguity with parentheses indicating syntactic structure. A finite state machine using simple rules and abbreviations detects sentence boundaries with high accuracy, and a set of regular expressions toke"
N12-3006,W04-3212,0,0.0291436,"Missing"
N12-3006,J03-4003,0,\N,Missing
N13-1104,P98-1013,0,0.192085,"vated features. Likewise, P RO F INDER can easily be used as a semi-supervised system if some slot designations and labeled examples are available. The idea of representing and capturing stereotypical knowledge has a long history in artificial intelligence and psychology, and has assumed various names such as frames (Minsky, 1974), schemata (Rumelhart, 1975), and scripts (Schank and Abelson, 1977). In the linguistics and computational linguistics communities, frame semantics (Fillmore, 1982) uses frames as the central representation of word meaning, culminating in the development of FrameNet (Baker et al., 1998), which contains over 1000 manually annotated frames. A similarly rich lexical resource is the MindNet project (Richardson et al., 1998). Our notion of frame is related to these representations, but there are also subtle differences. For example, Minsky’s frame emphasizes inheritance, which we do not model in this paper1 . As in semantic role labeling, FrameNet focuses on semantic roles and does not model event or frame transitions, so the scope of its frames is often no more than an event in our model. Perhaps the most similar to our frame is Roger Schank’s scripts, which capture prototypical"
N13-1104,P08-1004,0,0.00410704,"reducing such manual effort. For example, a popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012). However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task 838 (Curran et al., 2007). Filatova et al. (2006) construct simple domain templates by mining verbs and the named entity type of verbal arguments that are topical, whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While extremely scalable, this approach can only extract atomic factoids within a sentence, and the resulting triples are noisy, non-canonicalized text fragments. More relevant to our approach is the recent work in unsupervised semantic induction, such as unsupervised semantic parsing (Poon and Domingos, 2009), unsupervised semantical role labeling (Swier and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from we"
N13-1104,N04-1015,0,0.060758,"2009), unsupervised semantical role labeling (Swier and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from web search logs (Cheung and Li, 2012). As in P RO F INDER, they model distributional contexts for slots and roles. However, these approaches focus on the semantics of independent sentences or queries, and do not capture discourse-level dependencies. The modeling of frame and event transitions in P RO F INDER is similar to a sequential topic model (Gruber et al., 2007), and is inspired by the successful applications of such topic models in summarization (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009, inter alia). There are, however, two main differences. First, P RO F INDER contains not a single sequential topic model, but two (for frames and events, respectively). In addition, it also models the interdependencies among events, slots, and surface text, which is analogous to the USP model (Poon and Domingos, 2009). P RO F INDER can thus be viewed as a novel combination of state-of-the-art models in unsupervised semantics and discourse modeling. In terms of aim and capability, P RO F INDER is most similar to Chambers and Jurafsky"
N13-1104,N04-1038,0,0.0180699,"m PE−HEAD (ei |Ei ). 4. For each event argument: (a) Generate the slot Si,j from PSLOT (S|E, A, B). (b) Generate the dependency/caseframe emission depi,j ∼ PA−DEP (dep|S) and the lemma of the head word of the event argument ai,j ∼ PA−HEAD (a|S). 3.4 Figure 1: Graphical representation of our model. Hyperparameters, the stickiness factor, and the frame and event initial and transition distributions are not shown for clarity. the dependency from the event head to an event argument depi,j , our model instead emits the pair of event head and dependency relation, which we call a caseframe following Bean and Riloff (2004). 3.3 Full generative story To summarize, the distributions that are learned by our model are the default distributions PBKG (B), PF−INIT (F ), PE−INIT (E); the transition distributions PF−TRAN (Fi+1 |Fi ), PE−TRAN (Ei+1 |Ei ); and the emission distributions PSLOT (S|E, A, B), PE−HEAD (e|E, B), PA−HEAD (a|S), PA−DEP (dep|S). We used additive smoothing with uniform Dirichlet priors for all the multinomials. The overall generative story of our model is as follows: 1. Draw a Bernoulli distribution for PBKG (B) 2. Draw the frame, event, and slot distributions 3. Draw an event head emission distrib"
N13-1104,P08-1090,0,0.702432,"ences. First, P RO F INDER contains not a single sequential topic model, but two (for frames and events, respectively). In addition, it also models the interdependencies among events, slots, and surface text, which is analogous to the USP model (Poon and Domingos, 2009). P RO F INDER can thus be viewed as a novel combination of state-of-the-art models in unsupervised semantics and discourse modeling. In terms of aim and capability, P RO F INDER is most similar to Chambers and Jurafsky (2011), which culminated from a series of work for identifying correlated events and arguments in narratives (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). By adopting a probabilistic approach, P RO F INDER has a sound theoretical underpinning, and is easy to modify or extend. For example, in Section 3, we show how P RO F INDER can easily be augmented with additional linguistically-motivated features. Likewise, P RO F INDER can easily be used as a semi-supervised system if some slot designations and labeled examples are available. The idea of representing and capturing stereotypical knowledge has a long history in artificial intelligence and psychology, and has assumed various names such as frames (Minsky, 1974), s"
N13-1104,P09-1068,0,0.579316,"ntains not a single sequential topic model, but two (for frames and events, respectively). In addition, it also models the interdependencies among events, slots, and surface text, which is analogous to the USP model (Poon and Domingos, 2009). P RO F INDER can thus be viewed as a novel combination of state-of-the-art models in unsupervised semantics and discourse modeling. In terms of aim and capability, P RO F INDER is most similar to Chambers and Jurafsky (2011), which culminated from a series of work for identifying correlated events and arguments in narratives (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009). By adopting a probabilistic approach, P RO F INDER has a sound theoretical underpinning, and is easy to modify or extend. For example, in Section 3, we show how P RO F INDER can easily be augmented with additional linguistically-motivated features. Likewise, P RO F INDER can easily be used as a semi-supervised system if some slot designations and labeled examples are available. The idea of representing and capturing stereotypical knowledge has a long history in artificial intelligence and psychology, and has assumed various names such as frames (Minsky, 1974), schemata (Rumelhart, 1975), and"
N13-1104,P11-1098,0,0.719278,"r of domains and a few slots within a domain. Furthermore, additional manual effort is needed after the frames are defined in order to extract frame components from text (e.g., in annotating examples and designing features to train a supervised learning model). This paradigm makes generalizing across tasks difficult, and might suffer from annotator bias. Recently, there has been increasing interest in au837 Proceedings of NAACL-HLT 2013, pages 837–846, c Atlanta, Georgia, 9–14 June 2013. 2013 Association for Computational Linguistics tomatically inducing frames from text. A notable example is Chambers and Jurafsky (2011), which first clusters related verbs to form frames, and then clusters the verbs’ syntactic arguments to identify slots. While Chambers and Jurafsky (2011) represents a major step forward in frame induction, it is also limited in several aspects. The clustering used ad hoc steps and customized similarity metrics, as well as an additional retrieval step from a large external text corpus for slot generation. This makes it hard to replicate their approach or adapt it to new domains. Lacking a coherent model, it is also difficult to incorporate additional linguistic insights and prior knowledge. I"
N13-1104,P06-1039,0,0.00911159,"Missing"
N13-1104,P06-2027,0,0.523601,"res two stages of manual effort. First, the target representation is defined manually by domain experts. Then, manual effort is required to construct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of work in reducing such manual effort. For example, a popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012). However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task 838 (Curran et al., 2007). Filatova et al. (2006) construct simple domain templates by mining verbs and the named entity type of verbal arguments that are topical, whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While extremely scalable, this approach can only extract atomic factoids within a sentence, and the resulting triples are noisy, non-canonicalized text fragments. More rele"
N13-1104,N09-1041,1,0.409716,"and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from web search logs (Cheung and Li, 2012). As in P RO F INDER, they model distributional contexts for slots and roles. However, these approaches focus on the semantics of independent sentences or queries, and do not capture discourse-level dependencies. The modeling of frame and event transitions in P RO F INDER is similar to a sequential topic model (Gruber et al., 2007), and is inspired by the successful applications of such topic models in summarization (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009, inter alia). There are, however, two main differences. First, P RO F INDER contains not a single sequential topic model, but two (for frames and events, respectively). In addition, it also models the interdependencies among events, slots, and surface text, which is analogous to the USP model (Poon and Domingos, 2009). P RO F INDER can thus be viewed as a novel combination of state-of-the-art models in unsupervised semantics and discourse modeling. In terms of aim and capability, P RO F INDER is most similar to Chambers and Jurafsky (2011), which culminated from a series of work for identifyi"
N13-1104,E12-1029,0,0.00536895,"ly reducing engineering effort and requiring no external data. 2 Related Work In information extraction and other semantic processing tasks, the dominant paradigm requires two stages of manual effort. First, the target representation is defined manually by domain experts. Then, manual effort is required to construct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of work in reducing such manual effort. For example, a popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012). However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task 838 (Curran et al., 2007). Filatova et al. (2006) construct simple domain templates by mining verbs and the named entity type of verbal arguments that are topical, whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While"
N13-1104,P11-1112,0,0.0123109,"ir syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While extremely scalable, this approach can only extract atomic factoids within a sentence, and the resulting triples are noisy, non-canonicalized text fragments. More relevant to our approach is the recent work in unsupervised semantic induction, such as unsupervised semantic parsing (Poon and Domingos, 2009), unsupervised semantical role labeling (Swier and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from web search logs (Cheung and Li, 2012). As in P RO F INDER, they model distributional contexts for slots and roles. However, these approaches focus on the semantics of independent sentences or queries, and do not capture discourse-level dependencies. The modeling of frame and event transitions in P RO F INDER is similar to a sequential topic model (Gruber et al., 2007), and is inspired by the successful applications of such topic models in summarization (Barzilay and Lee, 2004; Daum´e III and Marcu, 2006; Haghighi and Vanderwende, 2009, inter alia). There are, h"
N13-1104,N09-1069,0,0.00744217,"dmits efficient inference by dynamic programming. In particular, after collapsing the latent assignment of frame, event, and background into a single hidden variable for each clause, the expectation and most probable assignment can be computed using standard forward-backward and Viterbi algorithms on fixed tree structures. Parameter learning can be done using EM by alternating the computation of expected counts and the maximization of multinomial parameters. In particular, P RO F INDER uses incremental EM, which has been shown to have better and faster convergence properties than standard EM (Liang and Klein, 2009). Determining the optimal number of events and slots is challenging. One solution is to adopt a nonparametric Bayesian method by incorporating a hierarchical prior over the parameters (e.g., a Dirichlet process). However, this approach can impose unrealistic restrictions on the model choice and result in intractability which requires sampling or approximate inference to overcome. Additionally, EM learning can suffer from local optima due to its nonconvex learning objective, especially when dealing with a large number hidden states without a good initialization. To address these issues, we adop"
N13-1104,N04-1019,0,0.0702176,"ation from source text. Essentially, we adapted the TAC summarization annotation to create gold-standard slots, and used them to evaluate entity extraction as in MUC-4. Dataset We used the TAC 2010 guidedsummarization dataset in our experiments (Owczarzak and Dang, 2010). This data set consists of text from five domains (termed categories in TAC), each with a template defined by TAC organizers. In total, there are 46 document clusters (termed topics in TAC), each of which contains 20 documents and has eight human-written summaries. Each summary was manually segmented using the Pyramid method (Nenkova and Passonneau, 2004) and each segment was annotated with a slot (termed aspect in TAC) from the corresponding template. Figure 3 shows an example and the full set of templates is available at http://www. nist.gov/tac/2010/Summarization/ Guided-Summ.2010.guidelines.html. In (a) (b) (c) Accidents and Natural Disasters: WHAT: what happened WHEN: date, time, other temporal markers WHERE: physical location WHY: reasons for accident/disaster WHO AFFECTED: casualties... DAMAGES: ... caused by the disaster COUNTERMEASURES: rescue efforts... (W HEN During the night of July 17,) (W HAT a 23-foot &lt;W HAT tsunami) hit the nor"
N13-1104,D07-1075,0,0.0976629,"art results while significantly reducing engineering effort and requiring no external data. 2 Related Work In information extraction and other semantic processing tasks, the dominant paradigm requires two stages of manual effort. First, the target representation is defined manually by domain experts. Then, manual effort is required to construct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of work in reducing such manual effort. For example, a popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012). However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task 838 (Curran et al., 2007). Filatova et al. (2006) construct simple domain templates by mining verbs and the named entity type of verbal arguments that are topical, whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational"
N13-1104,P06-1055,0,0.0117236,"INDucER), the first probabilistic approach to frame induction. P RO F INDER defines a joint distribution over the words in a document and their frame assignments by modeling frame and event transitions, correlations among events and slots, and their surface realizations. Given a set of documents, P RO F INDER outputs a set of induced frames with learned parameters, as well as the most probable frame assignments that can be used for event and entity extraction. The numbers of events and slots are dynamically determined by a novel application of the split-merge approach from syntactic parsing (Petrov et al., 2006). In end-to-end evaluations from text to entity extraction using standard MUC and TAC datasets, P RO F INDER achieved state-of-the-art results while significantly reducing engineering effort and requiring no external data. 2 Related Work In information extraction and other semantic processing tasks, the dominant paradigm requires two stages of manual effort. First, the target representation is defined manually by domain experts. Then, manual effort is required to construct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of wor"
N13-1104,D09-1001,1,0.0633533,"whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While extremely scalable, this approach can only extract atomic factoids within a sentence, and the resulting triples are noisy, non-canonicalized text fragments. More relevant to our approach is the recent work in unsupervised semantic induction, such as unsupervised semantic parsing (Poon and Domingos, 2009), unsupervised semantical role labeling (Swier and Stevenson, 2004) and induction (Lang and Lapata, 2011, e.g.), and slot induction from web search logs (Cheung and Li, 2012). As in P RO F INDER, they model distributional contexts for slots and roles. However, these approaches focus on the semantics of independent sentences or queries, and do not capture discourse-level dependencies. The modeling of frame and event transitions in P RO F INDER is similar to a sequential topic model (Gruber et al., 2007), and is inspired by the successful applications of such topic models in summarization (Barzi"
N13-1104,P98-2180,1,0.108186,"s are available. The idea of representing and capturing stereotypical knowledge has a long history in artificial intelligence and psychology, and has assumed various names such as frames (Minsky, 1974), schemata (Rumelhart, 1975), and scripts (Schank and Abelson, 1977). In the linguistics and computational linguistics communities, frame semantics (Fillmore, 1982) uses frames as the central representation of word meaning, culminating in the development of FrameNet (Baker et al., 1998), which contains over 1000 manually annotated frames. A similarly rich lexical resource is the MindNet project (Richardson et al., 1998). Our notion of frame is related to these representations, but there are also subtle differences. For example, Minsky’s frame emphasizes inheritance, which we do not model in this paper1 . As in semantic role labeling, FrameNet focuses on semantic roles and does not model event or frame transitions, so the scope of its frames is often no more than an event in our model. Perhaps the most similar to our frame is Roger Schank’s scripts, which capture prototypical events and participants in a scenario such as restaurant dining. In their approach, however, scripts are manually defined, making it ha"
N13-1104,N06-1039,0,0.061787,"ruct an extractor or to annotate examples to train a machine-learning system. Recently, there has been a burgeoning body of work in reducing such manual effort. For example, a popular approach to reduce annotation effort is bootstrapping from seed examples (Patwardhan and Riloff, 2007; Huang and Riloff, 2012). However, this still requires prespecified frames or templates, and selecting seed words is often a challenging task 838 (Curran et al., 2007). Filatova et al. (2006) construct simple domain templates by mining verbs and the named entity type of verbal arguments that are topical, whereas Shinyama and Sekine (2006) identify query-focused slots by clustering common named entities and their syntactic contexts. Open IE (Banko and Etzioni, 2008) limits the manual effort to designing a few domain-independent relation patterns, which can then be applied to extract relational triples from text. While extremely scalable, this approach can only extract atomic factoids within a sentence, and the resulting triples are noisy, non-canonicalized text fragments. More relevant to our approach is the recent work in unsupervised semantic induction, such as unsupervised semantic parsing (Poon and Domingos, 2009), unsuperv"
N13-1104,H01-1054,0,0.0271149,"e Slot: Victim P ERSON /O RG Words: people, priest, leader, member, judge Caseframes: kill&gt;dobj, murder&gt;dobj, release&gt;dobj, report&gt;dobj, kidnap&gt;dobj Figure 2: A partial frame learned by P RO F INDER from the MUC-4 data set, with the most probable emissions for each event and slot. Labels are assigned by the authors for readability. ate P RO F INDER’s capabilities in generalizing to a greater variety of text, we designed and conducted a novel evaluation based on the TAC guidedsummarization dataset. This evaluation was inspired by the connection between summarization and information extraction (White et al., 2001), and reflects a conceptualization of summarization as inducing and extracting structured information from source text. Essentially, we adapted the TAC summarization annotation to create gold-standard slots, and used them to evaluate entity extraction as in MUC-4. Dataset We used the TAC 2010 guidedsummarization dataset in our experiments (Owczarzak and Dang, 2010). This data set consists of text from five domains (termed categories in TAC), each with a template defined by TAC organizers. In total, there are 46 document clusters (termed topics in TAC), each of which contains 20 documents and h"
N13-1104,C98-2175,1,\N,Missing
N13-1104,C98-1013,0,\N,Missing
N15-3006,P14-1134,0,0.588109,"to the LDC data sets, and 2) we will demonstrate AMR parsers for German, French, Spanish and Japanese that make use of the same small set of LF-to-AMR conversion rules. 1 Introduction Abstract Meaning Representation (AMR) (Banarescu et al., 2014) is a semantic representation for which a large amount of manually-annotated data is being created, with the intent of constructing and evaluating parsers that generate this level of semantic representation for previously unseen text. 1 Available at: http://research.microsoft.com/msrsplat Already one method for training an AMR parser has appeared in (Flanigan et al., 2014), and we anticipate that more attempts to train parsers will follow. In this demonstration, we will present our AMR parser, which converts our existing semantic representation formalism, Logical Form (LF), into the AMR format. We do this with two goals: first, as our existing LF is close in design to AMR, we can now use the manually-annotated AMR datasets to measure the accuracy of our LF system, which may serve to provide a benchmark for parsers trained on the AMR corpus. We gratefully acknowledge the contributions made by Banarescu et al. (2014) towards defining a clear and interpretable sem"
N15-3006,flickinger-etal-2014-towards,0,0.0211558,"nnotated test suite (Suzuki, 2002). We start by curating a sentence corpus that exemplifies the syntactic and semantic phenomena that the LF is designed to cover; one might view this sentence corpus as the LF specification. When, during development, the system outputs the desired representation, that LF is saved as “gold annotation”. In this way, the gold annotations are produced by the LF system itself, automatically, and thus with good system internal consistency. We note that this method of system development is quite different from SemBanking AMR, but is similar to the method described in Flickinger et al. (2014). As part of this demonstration, we share with participants the gold annotations for the curated sentence corpora used during LF development, currently 550 sentences that are vetted to produce correct LF analyses. Note that the example in Figure 2 requires a parser to handle both the passive/active alternation as well as control verbs. We believe that there is value in curated targeted datasets to supplement annotating natural data; e.g., AMR clearly includes control phenomena in its spec (the first example is “the boy wants to go”) but in the data, there are only 3 instances of “persuade” in"
N15-3006,W02-1510,0,0.0502554,"e data. Adding training data from other sources leads to improvements on the discussion forum data, but at the cost of accuracy on newswire. The lack of sophisticated sense disambiguation in LF causes a substantial degradation in performance on newswire. 6 Data Sets for LF development The LF component was developed by authoring rules that access information from a rich lexicon consisting of several online dictionaries as well as information output by a rich grammar formalism. Authoring these LF rules is supported by a suite of tools that allow iterative development of an annotated test suite (Suzuki, 2002). We start by curating a sentence corpus that exemplifies the syntactic and semantic phenomena that the LF is designed to cover; one might view this sentence corpus as the LF specification. When, during development, the system outputs the desired representation, that LF is saved as “gold annotation”. In this way, the gold annotations are produced by the LF system itself, automatically, and thus with good system internal consistency. We note that this method of system development is quite different from SemBanking AMR, but is similar to the method described in Flickinger et al. (2014). As part"
N15-3006,W04-2807,0,\N,Missing
N15-3006,P13-2131,0,\N,Missing
N16-1098,W08-2227,1,0.249101,"Missing"
N16-1098,D13-1178,0,0.046906,"ured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008), but teasing out useful info"
N16-1098,P13-1035,0,0.0363528,"esentations. Chambers and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques per"
N16-1098,D15-1075,0,0.0394456,"ressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a r"
N16-1098,P08-1090,1,0.58165,"used on learning scripts (Schank and Abelson, 1977). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computation"
N16-1098,P09-1068,1,0.635097,"77). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008"
N16-1098,D13-1185,1,0.266302,"and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Picho"
N16-1098,N13-1104,1,0.900888,"ical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced scripts. Most relevant to this issue is work on unsupervised learning of ‘narrative chains’ (Chambers and Jurafsky, 2008) and event schemas (Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). The first requirement of any learner is to decide on a corpus to drive the learning process. We are foremost interested in a resource that is full of temporal and causal relations between events because causality is a central component of coherency. Personal stories from daily weblogs are good sources of commonsense causal information (Gordon and Swan839 Proceedings of NAACL-HLT 2016, pages 839–849, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics son, 2009; Manshadi et al., 2008), but teasing out useful information from noisy bl"
N16-1098,W07-1401,0,0.0338919,"Test’. 4.1 Story Cloze Test The cloze task (Taylor, 1953) is used to evaluate a human (or a system) for language understanding by deleting a random word from a sentence and having a human fill in the blank. We introduce ‘Story Cloze Test’, in which a system is given a four-sentence ‘context’ and two alternative endings to the story, called ‘right ending’ and ‘wrong ending’. Hence, in this test the fifth sentence is blank. Then the system’s task is to choose the right ending. The ‘right ending’ can be viewed as ‘entailing’ hypothesis in a classic Recognizing Textual Entailment (RTE) framework (Giampiccolo et al., 2007), and ‘wrong’ ending can be seen as the ’contradicting’ hypothesis. Table 4 shows three example Story Cloze Test cases. Story Cloze Test will serve as a generic story understanding evaluation framework, also applicable to evaluation of story generation models (for instance by computing the log-likelihoods assigned to the two ending alternatives by the story generation model), which does not necessarily imply requirement for explicit narrative knowledge learning. However, it is safe to say that any model that performs well on Story Cloze Test is demonstrating some level of deeper story understa"
N16-1098,E12-1034,0,0.0756634,"Missing"
N16-1098,P04-1077,0,0.0329659,"orkers participated Average # cases written by one worker Max # cases written by one worker Average payment per test case (cents) Size of the final set (verified by human) 13,500 282 47.8 1461 10 3,744 Table 5: Statistics for crowd-sourcing Story Cloze Test instances. search engine8 hits of the main event (verb) together with its semantic roles (e.g., ‘I*poison*flowers’ vs ‘I*nourish*flowers’). We extract the main verb and its corresponding roles using TRIPS semantic parser. 2. N-gram Overlap: Simply chooses the alternative which shares more n-grams with the context. We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context. 3. GenSim: Average Word2Vec: Choose the hypothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec embedding of the context. This is basically an enhanced word overlap baseline, which accounts for semantic similarity. 4. Sentiment-Full: Choose the hypothesis that matches the average sentiment of the context. We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence. 5. Sentiment-Last: Choose the hypothe"
N16-1098,P14-5010,0,0.00272135,"imply chooses the alternative which shares more n-grams with the context. We compute Smoothed-BLEU (Lin and Och, 2004) score for measuring up to four-gram overlap of an alternative and the context. 3. GenSim: Average Word2Vec: Choose the hypothesis with closer average word2vec (Mikolov et al., 2013) embedding to the average word2vec embedding of the context. This is basically an enhanced word overlap baseline, which accounts for semantic similarity. 4. Sentiment-Full: Choose the hypothesis that matches the average sentiment of the context. We use the state-of-the-art sentiment analysis model (Manning et al., 2014) which assigns a numerical value from 1 to 5 to a sentence. 5. Sentiment-Last: Choose the hypothesis that matches the sentiment of the last context sentence. 8 https://developers.google.com/ custom-search/ 846 6. Skip-thoughts Model: This model uses Skipthoughts’ Sentence2Vec embedding (Kiros et al., 2015) which models the semantic space of novels. This model is trained on the ‘BookCorpus’ (Zhu et al., 2015) (containing 16 different genres) of over 11,000 books. We use the skip-thoughts embedding of the alternatives and contexts for making decision the same way as with GenSim model. 7. Narrati"
N16-1098,P09-1025,0,0.00927628,"ing the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009). Finally, there is a rich body of work on story plot generation and creative or artistic story telling (M´endez et al., 2014; Riedl and Le´on, 2008). This paper is unique to these in its corpus of short, simple stories with a wide variety of commonsense events. We show these to be useful for learning, but also for enabling a rich evaluation framework for narrative understanding. 3 A Corpus of Short Commonsense Stories We aimed to build a corpus with two goals in mind: 1. The corpus contains a variety of commonsense causal and temporal relations between everyday events. This enables learning n"
N16-1098,W16-1007,1,0.12213,"Missing"
N16-1098,P15-1019,0,0.0707931,"Missing"
N16-1098,E14-1024,0,0.538376,"2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a text ordered sequence of events, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward child"
N16-1098,P10-1100,0,0.0173048,"g can help direct the field to a new direction of deeper language understanding. 2 Related Work Several lines of research have recently focused on learning narrative/event representations. Chambers and Jurafsky first proposed narrative chains (Chambers and Jurafsky, 2008) as a partially ordered set of narrative events that share a common actor called the ‘protagonist’. A narrative event is a tuple of an event (a verb) and its participants represented as typed dependencies. Several expansions have since been proposed, including narrative schemas (Chambers and Jurafsky, 2009), script sequences (Regneri et al., 2010), and relgrams (Balasubramanian et al., 2013). Formal probabilistic models have also been proposed to learn event schemas and frames (Cheung et al., 2013; Bamman et al., 2013; Chambers, 2013; Nguyen et al., 2015). These are trained on smaller corpora and focus less on large-scale learning. A major shortcoming so far is that these models are mainly trained on news articles. Little knowledge about everyday life events are learned. Several groups have directly addressed script learning by focusing exclusively on the narrative cloze test. Jans et al. (Jans et al., 2012) redefined the test to be a"
N16-1098,D13-1020,0,0.0780145,"vents, whereas the original did not rely on text order (Chambers and Jurafsky, 2008). Since then, others have shown language-modeling techniques perform well (Pichotta and Mooney, 2014a; Rudinger et al., 2015). This paper shows that these approaches struggle on the richer Story Cloze evaluation. There has also been renewed attention toward natural language comprehension and commonsense reasoning (Levesque, 2011; Roemmele et al., 2011; Bowman et al., 2015). There are a few recent frameworks for evaluating language comprehension (Hermann et al., 2015; Weston et al., 2015), including the MCTest (Richardson et al., 2013) as a notable one. Their framework also involves story comprehension, however, their stories are mostly fictional, on average 212 words, and geared toward children in grades 1-4. Some progress has been made in story understanding by limiting the task to the specific domains and question types. This includes research on understanding newswire involving terrorism scripts (Mueller, 2002), stories about people in a restaurant where a reasonable number of questions about time and space can be answered (Mueller, 2007), and generating stories from fairy tales (McIntyre and Lapata, 2009). Finally, the"
N16-1098,D15-1195,0,0.170589,"Missing"
N16-1098,H89-1033,0,0.710176,"onsense relations between daily events, and (2) it is a high quality collection of everyday life stories that can also be used for story generation. Experimental evaluation shows that a host of baselines and state-of-the-art models based on shallow language understanding struggle to achieve a high score on the Story Cloze Test. We discuss these implications for script and story learning, and offer suggestions for deeper language understanding. 1 Introduction Story understanding is an extremely challenging task in natural language understanding with a longrunning history in AI (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000). A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Scripts represent structured knowledge about stereotypical event sequences together with their participants. It is evident that various NLP applications (text summarization, co-reference resolution, question answering, etc.) can hugely benefit from the rich inferential capabilities that structured knowledge about events can provide. Given that developing hand-built scripts is extremely timeconsuming, there is a serious need for automatically induced"
N16-1147,N15-1053,0,0.0541033,"31) visit (321) market (311) outdoor activity (267) Table 1: The number of albums in our tiered dataset for 1 the 15 most frequent Story kinds of stories. Re-telling Storytelling Motivation and Related Work 3 Dataset Construction Extracting Photos We begin by generating a list of “storyable” event types. We leverage the idea that “storyable” events tend to involve some form of posStory 4 Preferred Photo Sequence Flickr Album Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015). Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions. There is a significant difference, yet unexplored, between"
N16-1147,P15-2017,1,0.822682,"Baseline Experiments We report baseline experiments on the storytelling task in Table 7, training on the SIS tier and testing on half the SIS validation set (valtest). Example output from each system is presented in Table 5. To highlight some differences between story and caption generation, we also train on the DII tier in isolation, and produce captions per-image, rather than in sequence. These results are shown in Table 7. To train the story generation model, we use a sequence-to-sequence recurrent neural net (RNN) approach, which naturally extends the single-image captioning technique of Devlin et al. (2015) and Vinyals et al. (2014) to multiple images. Here, we encode an image sequence by running an RNN over the fc7 vectors of each image, in reverse order. This is used as the initial hidden state to the story decoder model, which learns to produce the story one word at a time using softmax loss over the training data vocabulary. We use Gated Recurrent Units (GRUs) (Cho et al., 2014) for both the image encoder and story decoder. In the baseline system, we generate the story using a simple beam search (size=10), which has been successful in image captioning previously (Devlin et al., 2015). Howeve"
N16-1147,D13-1128,0,0.0674328,"tivity (267) Table 1: The number of albums in our tiered dataset for 1 the 15 most frequent Story kinds of stories. Re-telling Storytelling Motivation and Related Work 3 Dataset Construction Extracting Photos We begin by generating a list of “storyable” event types. We leverage the idea that “storyable” events tend to involve some form of posStory 4 Preferred Photo Sequence Flickr Album Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015). Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions. There is a significant difference, yet unexplored, between remarking that a visual scene shows “sitting i"
N16-1147,D15-1021,1,0.889893,"Missing"
N16-1147,N16-1014,1,0.758927,"The dog was happy to be in the water. The family gathered together for a meal. The food was delicious. The dog was excited to be there. The kids were playing in the water. The boat was a little too much to drink. The family got together for a cookout. They had a lot of delicious food. The dog was happy to be there. They had a great time on the beach. They even had a swim in the water. Table 5: Example stories generated by baselines. This is a predictable result given the label bias problem inherent in maximum likelihood training; recent work has looked at ways to address this issue directly (Li et al., 2016). To establish a stronger baseline, we explore several decode-time heuristics to improve the quality of the generated story. The first heuristic is to lower the decoder beam size substantially. We find that using a beam size of 1 (greedy search) significantly increases the story quality, resulting in a 4.6 gain in METEOR score. However, the same effect is not seen for caption generation, with the greedy caption model obtaining worse quality than the beam search model. This highlights a key difference in generating stories versus generating captions. Although the stories produced using a greedy"
N16-1147,P04-1077,0,0.227891,"tter understand which metric could serve as a proxy for human evaluation, we compute pairwise correlation coefficients between automatic metrics and human judgments on 3,000 stories sampled from the SIS training set. For the human judgements, we again use crowdsourcing on MTurk, asking five judges per story to rate how strongly they agreed with the statement “If these were my photos, I would like using a story like this to share my experience with my friends”.7 We take the average of the five judgments as the final score for the story. For the automatic metrics, we use METEOR,8 smoothed-BLEU (Lin and Och, 2004), and Skip-Thoughts (Kiros et al., 2015) to compute similarity between each story for a given sequence. Skip-thoughts provide a Sentence2Vec embedding which models the semantic space of novels. As Table 4 shows, METEOR correlates best with human judgment according to all the correlation coefficients. This signals that a metric such as METEOR which incorporates paraphrasing correlates best with human judgement on this task. A more 7 Scale presented ranged from “Strongly disagree” to “Strongly agree”, which we convert to a scale of 1 to 5. 8 We use METEOR version 1.5 with hter weights. r ρ τ MET"
N16-1147,P14-5010,0,0.00748671,"instill morals, and share advice; focusing AI research towards this task therefore has the potential to bring about more humanlike intelligence and understanding. Re-telling Story 3 Story 1 2 easter (259) church (243) graduation ceremony (236) office (226) father’s day (221) Story 2 Story 5 Description for Images in Isolation & in Sequences Figure 2: Dataset crowdsourcing workflow. Caption in Sequence session, e.g., “John’s birthday party,” or “Shabnam’s visit.” Using the Flickr data release (Thomee et al., 2015), we aggregate 5-grams of photo titles and descriptions, using Stanford CoreNLP (Manning et al., 2014) to extract possessive dependency patterns. We keep the heads of possessive phrases if they can be classified as an EVENT in WordNet3.0, relying on manual winnowing to target our collection efforts.2 These terms are then used to collect albums using the Flickr API.3 We only include albums with 10 to 50 photos where all album photos are taken within a 48-hour span and CC-licensed. See Table 1 for the query terms with the most albums returned. The photos returned from this stage are then presented to crowd workers using Amazon’s Mechanical Turk to collect the corresponding stories and descriptio"
N16-1147,Q14-1006,0,0.0517771,"ket (311) outdoor activity (267) Table 1: The number of albums in our tiered dataset for 1 the 15 most frequent Story kinds of stories. Re-telling Storytelling Motivation and Related Work 3 Dataset Construction Extracting Photos We begin by generating a list of “storyable” event types. We leverage the idea that “storyable” events tend to involve some form of posStory 4 Preferred Photo Sequence Flickr Album Work in vision to language has exploded, with researchers examining image captioning (Lin et al., 2014; Karpathy and Fei-Fei, 2015; Vinyals et al., 2015; Xu et al., 2015; Chen et al., 2015; Young et al., 2014; Elliott and Keller, 2013), question answering (Antol et al., 2015; Ren et al., 2015; Gao et al., 2015; Malinowski and Fritz, 2014), visual phrases (Sadeghi and Farhadi, 2011), video understanding (Ramanathan et al., 2013), and visual concepts (Krishna et al., 2016; Fang et al., 2015). Such work focuses on direct, literal description of image content. While this is an encouraging first step in connecting vision and language, it is far from the capabilities needed by intelligent agents for naturalistic interactions. There is a significant difference, yet unexplored, between remarking that a vi"
P15-1086,W11-1401,0,0.0445849,"representation of text in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic role labeling within a sentence in a supervised setting. We hope to continue in this direction of semantic focus, but extend the capabilities of question generation to include openended questions that go far beyond th"
P15-1086,N12-1092,1,0.579945,"xt in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic role labeling within a sentence in a supervised setting. We hope to continue in this direction of semantic focus, but extend the capabilities of question generation to include openended questions that go far beyond the scope of a single s"
P15-1086,W11-2705,0,0.0761953,"language question generation, work that focuses on the semantic quality of generated questions, i.e. the “what to ask about” rather than “how to ask it,” and finally work that builds a semantic representation of text in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic role labeling within a"
P15-1086,N10-1086,0,0.661269,"semantic quality of generated questions, i.e. the “what to ask about” rather than “how to ask it,” and finally work that builds a semantic representation of text in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence. Becker et al. (2012) also leverage semantic role labeling within a sentence in a supervised setting. We hope to continue in th"
P15-1086,W13-2114,0,0.386403,"Missing"
P15-1086,W03-0203,0,0.538208,"stion generation: work that focuses on the grammaticality of natural language question generation, work that focuses on the semantic quality of generated questions, i.e. the “what to ask about” rather than “how to ask it,” and finally work that builds a semantic representation of text in order to generate higher-level questions. Approaches focusing on the grammaticality of question generation date back to the AUTOQUEST system (Wolfe, 1976), which examined the generation of Wh-questions from single sentences. Later systems addressing the same goal include methods that use transformation rules (Mitkov and Ha, 2003), template-based generation (Chen et al., 2009; Curto et al., 2011) and overgenerate-and-rank methods (Heilman and Smith, 2010a). Another approach has been to create fill-in-the-blank questions from single sentences to ensure grammaticality (Agarwal et al. 2011, Becker et al. 2012). More relevant to our direction is work on the semantic aspect of question generation, which has become a more active research area in the past several years. Several authors (Mazidi and Nielsen 2014; Linberg et al. 2013) generate questions according to the semantic role patterns extracted from the source sentence."
P15-1086,D09-1026,0,0.0332093,"Missing"
P15-1086,W10-4234,0,0.299957,"pt maps over keywords (Olney et al. 2012) and minimal recursion semantics (Yao 2010) to reason over concepts in the text. While the work of (Olney et al. 2012) is impressive in its possibilities, the range of the types of questions that can be generated is restricted by a relatively specific set of relations (e.g. Is-A, PartOf) captured in the ontology of the domain (biology textbook). Mannem et al. (2010) observe as we have that &quot;capturing the exact true meaning of a paragraph is beyond the reach of current NLP systems;&quot; thus, in their system for Shared Task A (for paragraph-level questions (Rus et al. 2010)) they make use of predicate argument structures along with semantic role labeling. However, the generation of these questions is restricted to the first sentence of the paragraph. Though motivated by the same noble impulses of these authors to achieve higher-level questions, our hope is that we can bypass the challenges and constraints of semantic parsing and generate deep questions via a more holistic approach. 890 Figure 2: Coverage properties of our category-section representation: (a) fraction of Wikipedia articles covered by the top j most common Freebase types, grouped by our eight high"
P15-1086,P14-2053,0,\N,Missing
P16-1170,N12-1092,1,0.637005,"captures, however, the majority of the questions in Visual7w are designed to be answerable by only the image, making them unnatural for asking a human. Thus, learning to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to generate questions. Data Collection Methodology Task Definition: Given an image, the task is to generate a natural question which can potentially engage a human in starting a con"
P16-1170,N15-1053,0,0.0320703,"a wide gap with human performance which motivates further work on connecting images with commonsense knowledge and pragmatics. Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language. 1 Generated Caption: - A man standing next to a motorcycle. Figure 1: Example image along with its natural questions and automatically generated caption. Introduction We are witnessing a renewed interest in interdisciplinary AI research in vision & language, from descriptions of the visual input such as image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) and video transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to testing computer understanding of an image through question answering (Antol et al., 2015; Malinowski and Fritz, 2014). The most established work in the vision & language community is ‘image captioning’, where the task is to produce a literal description of the image. It has been shown (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) that a reasonable language modeling paired with deep visual features trained on large enough datasets promise"
P16-1170,W14-3348,0,0.0246802,"S COCO dataset. Human evaluation results of a recent work (Tran et al., 2016) further confirms the significant image captioning quality degradation on out-of-domain data. To further explore this difference, we crowdsourced 5 captions for each image in the V QGBing−5000 dataset using the same prompt as used to source the MS COCO captions. We call this new dataset CaptionsBing−5000 . Table 3 shows the results of testing the state-of-the-art MSR captioning system on the CaptionsBing−5000 dataset as compared to the MS COCO dataset, measured by the standard BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014) metrics. The wide gap in the results further confirms that indeed the V QGBing−5000 dataset covers a new class of images; we hope the availability of this new dataset will encourage including more diverse domains for image captioning. Bing 0.101 BLEU M S COCO 0.291 Bing 0.151 METEOR M S COCO 0.247 Table 3: Image captioning results Together with this paper we are releasing an extended set of VQG dataset to the community. We hope that the availability of this dataset will encourage the research community to tackle more end-goal oriented vision & language tasks. 4 Models In this Section we prese"
P16-1170,P15-2017,1,0.165044,"e are witnessing a renewed interest in interdisciplinary AI research in vision & language, from descriptions of the visual input such as image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) and video transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to testing computer understanding of an image through question answering (Antol et al., 2015; Malinowski and Fritz, 2014). The most established work in the vision & language community is ‘image captioning’, where the task is to produce a literal description of the image. It has been shown (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) that a reasonable language modeling paired with deep visual features trained on large enough datasets promise a good performance on image captioning, making it a less challenging task from language learning perspective. Furthermore, although this task has a great value for communities of people who are low-sighted or cannot see in all or some environments, for others, the description does not add anything to what a person has already perceived. The popularity of the image sharing applications in social media and user engagement around images is eviden"
P16-1170,D15-1021,1,0.579546,"s naturally contain objects less frequently. Hence, we see that VQG questions include the mention of more of those literal objects. Figure 3(b) shows that COCO captions have a larger vocabulary size, which reflects their longer and more descriptive sentences. VQG shows a relatively large vocabulary size as well, indicating greater diversity in question formulation than VQA and CQA. Moreover, Figure 3(c) shows that the verb part of speech is represented with high frequency in our dataset. Figure 3(d) depicts the percentage of abstract terms such as ‘think’ or ‘win’ in the vocabulary. Following Ferraro et al. (2015), we use a list of most common abstract terms in English (Vanderwende et al., 2015), and count all the other words except a set of function words as concrete. This figure supports our expectation that VQG covers more abstract concepts. Furthermore, Figure 3(e) shows inter-annotation textual similarity according to the BLEU metric (Papineni et al., 2002). Interestingly, VQG shows the highest interannotator textual similarity, which reflects on the existence of consensus among human for asking Figure 3: Comparison of various annotations on the MS COCO dataset. (a) Percentage of gold objects used"
P16-1170,P15-2073,1,0.456361,"n on AMT, asking three crowd workers to each rate the quality of candidate questions on a three-point semantic scale. 5.2 Automatic Evaluation The goal of automatic evaluation is to measure the similarity of system-generated question hypotheses and the crowdsourced question references. To capture n-gram overlap and textual similarity between hypotheses and references, we use standard Machine Translation metrics, BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014). We use BLEU with equal weights up to 4-grams and default setting of METEOR version 1.5. Additionally we use ∆BLEU (Galley et al., 2015) which is specifically tailored towards generation tasks with diverse references, such as conversations. ∆BLEU requires rating per reference, distinguishing between the quality of the references. For this purpose, we crowd-sourced three human ratings (on a scale of 1-3) per reference and used the majority rating. The pairwise correlational analysis of human and automatic metrics is presented in Table 6, where we report on Pearson’s r, Spearman’s ρ and Kendall’s τ correlation coefficients. As this table reveals, ∆BLEU strongly correlates with human judgment and we suggest it as the main evaluat"
P16-1170,N10-1086,0,0.624,"ommonsense reasoning, going beyond spatial reasoning required for ‘which’ or ‘who’ questions. This is more in line with the type of questions that VQG captures, however, the majority of the questions in Visual7w are designed to be answerable by only the image, making them unnatural for asking a human. Thus, learning to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to generate questions. Data Collec"
P16-1170,P15-1086,1,0.454133,"ng to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to generate questions. Data Collection Methodology Task Definition: Given an image, the task is to generate a natural question which can potentially engage a human in starting a conversation. Questions that are visually verifiable, i.e., that can be answered by looking at only the image, are outside the scope of this task. For instance, in Figu"
P16-1170,P16-1094,0,0.00667955,"question can naturally start a conversation. To continue progress on this task, it is possible to increase the size of the training data, but we also expect to develop models that will learn to generalize to unseen concepts. For instance, consider the examples of system errors in Table 7, where visual features can be enough for detecting the specific set of objects in each image, but the system cannot make sense of the combination of previously unseen concepts. Another natural future extension of this work is to include question generation within a conversational system (Sordoni et al., 2015; Li et al., 2016), where the context and conversation history affect the types of questions being asked. Human BLEU 0.915 (4.6e-27) 0.67 (7.0e-10) 0.51 (7.9e-10) - How long did it take to make that ice sculpture? GRNN METEOR 0.916 (4.8e-27) 0.628 (1.5e-08) 0.476 (1.6e-08) - How long has he been hiking? - Is this in a hotel room? KNN r ρ τ - How deep was the snow? - Do you enjoy the light in this bathroom? - Is the dog looking to take a shower? Table 7: Examples of errors in generation. The rows are Humanconsensus , GRNNall , and KNN+minbleu−all . Acknowledgment We would like to thank the anonymous reviewers fo"
P16-1170,P04-1077,0,0.0126356,"Missing"
P16-1170,W13-2114,0,0.0187415,"e by only the image, making them unnatural for asking a human. Thus, learning to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to generate questions. Data Collection Methodology Task Definition: Given an image, the task is to generate a natural question which can potentially engage a human in starting a conversation. Questions that are visually verifiable, i.e., that can be answered by looking at"
P16-1170,W03-0203,0,0.279042,"require high-level commonsense reasoning, going beyond spatial reasoning required for ‘which’ or ‘who’ questions. This is more in line with the type of questions that VQG captures, however, the majority of the questions in Visual7w are designed to be answerable by only the image, making them unnatural for asking a human. Thus, learning to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to gener"
P16-1170,P02-1040,0,0.11831,"formulation than VQA and CQA. Moreover, Figure 3(c) shows that the verb part of speech is represented with high frequency in our dataset. Figure 3(d) depicts the percentage of abstract terms such as ‘think’ or ‘win’ in the vocabulary. Following Ferraro et al. (2015), we use a list of most common abstract terms in English (Vanderwende et al., 2015), and count all the other words except a set of function words as concrete. This figure supports our expectation that VQG covers more abstract concepts. Furthermore, Figure 3(e) shows inter-annotation textual similarity according to the BLEU metric (Papineni et al., 2002). Interestingly, VQG shows the highest interannotator textual similarity, which reflects on the existence of consensus among human for asking Figure 3: Comparison of various annotations on the MS COCO dataset. (a) Percentage of gold objects used in annotations. (b) Vocabulary size (c) Percentage of verb POS (d) Percentage of abstract terms (e) Inter-annotation textual similarity score. The MS COCO dataset is limited in terms of the concepts it covers, due to its pre-specified set of object categories. Word frequency in V QGcoco−5000 dataset, as demonstrated in Figure 4, bears this out, with th"
P16-1170,P14-2053,0,0.0135128,"7w are designed to be answerable by only the image, making them unnatural for asking a human. Thus, learning to generate the questions in VQA task is not a useful sub-task, as the intersection between VQG and any VQA questions is by definition minimal. Previous work on question generation from textual input has focused on two aspects: the grammaticality (Wolfe, 1976; Mitkov and Ha, 2003; Heilman and Smith, 2010) and the content focus of question generation, i.e., “what to ask about”. For the latter, several methods have been explored: (Becker et al., 2012) create fill-in-the-blank questions, (Mazidi and Nielsen, 2014) and (Lindberg et al., 2013) use manually constructed question templates, while (Labutov et al., 2015) use crowdsourcing to collect a set of templates and then rank the potentially relevant templates for the selected content. To our knowledge, neither a retrieval model nor a deep representation of textual input, presented in our work, have yet been used to generate questions. Data Collection Methodology Task Definition: Given an image, the task is to generate a natural question which can potentially engage a human in starting a conversation. Questions that are visually verifiable, i.e., that c"
P16-1170,N15-1020,1,0.0955277,"Missing"
P16-1170,N15-3006,1,0.341314,"nclude the mention of more of those literal objects. Figure 3(b) shows that COCO captions have a larger vocabulary size, which reflects their longer and more descriptive sentences. VQG shows a relatively large vocabulary size as well, indicating greater diversity in question formulation than VQA and CQA. Moreover, Figure 3(c) shows that the verb part of speech is represented with high frequency in our dataset. Figure 3(d) depicts the percentage of abstract terms such as ‘think’ or ‘win’ in the vocabulary. Following Ferraro et al. (2015), we use a list of most common abstract terms in English (Vanderwende et al., 2015), and count all the other words except a set of function words as concrete. This figure supports our expectation that VQG covers more abstract concepts. Furthermore, Figure 3(e) shows inter-annotation textual similarity according to the BLEU metric (Papineni et al., 2002). Interestingly, VQG shows the highest interannotator textual similarity, which reflects on the existence of consensus among human for asking Figure 3: Comparison of various annotations on the MS COCO dataset. (a) Percentage of gold objects used in annotations. (b) Vocabulary size (c) Percentage of verb POS (d) Percentage of a"
P16-1170,N15-1173,0,0.0354635,"Our proposed task offers a new challenge to the community which we hope furthers interest in exploring deeper connections between vision & language. 1 Generated Caption: - A man standing next to a motorcycle. Figure 1: Example image along with its natural questions and automatically generated caption. Introduction We are witnessing a renewed interest in interdisciplinary AI research in vision & language, from descriptions of the visual input such as image captioning (Chen et al., 2015; Fang et al., 2014; Donahue et al., 2014; Chen et al., 2015) and video transcription (Rohrbach et al., 2012; Venugopalan et al., 2015), to testing computer understanding of an image through question answering (Antol et al., 2015; Malinowski and Fritz, 2014). The most established work in the vision & language community is ‘image captioning’, where the task is to produce a literal description of the image. It has been shown (Devlin et al., 2015; Fang et al., 2014; Donahue et al., 2014) that a reasonable language modeling paired with deep visual features trained on large enough datasets promise a good performance on image captioning, making it a less challenging task from language learning perspective. Furthermore, although thi"
P18-1086,P17-1025,0,0.0316528,"a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys1 This dataset is available at http://lair.cse.msu. edu/lair/projects/actioneffect.html 935 3 ical causality of action verbs (Gao et al., 2016) and other physical properties of verbs (Forbes and Choi, 2017; Zellers and Choi, 2017; Chao et al., 2015). But they do not address action-effect prediction. Action-Effect Data Collection We collected a dataset to support the investigation on physical action-effect prediction. This dataset consists of actions expressed in the form of verbnoun pairs, effects of actions described in language, and effects of actions depicted in images. Note that, as we would like to have a wide range of possible effects, language data and image data are collected separately. The idea of modeling object physical state change has also been studied in the computer vision commu"
P18-1086,D16-1044,0,0.0650771,"ter vision community (Fire and Zhu, 2016). Computational models have been developed to infer object states from observations and to further predict future state changes (Zhou and Berg, 2016; Wu et al., 2016, 2017). The action recognition task can be treated as detecting the transformation on object states (Fathi and Rehg, 2013; Yang et al., 2013; Wang et al., 2016). However these previous works only focus on the visual presentation of motion effects. Recent years have seen an increasing amount of work integrating language and vision, for example, visual question answering (Antol et al., 2015; Fukui et al., 2016; Lu et al., 2016), image description generation (Xu et al., 2015; Vinyals et al., 2015), and grounding language to perception (Yang et al., 2016; Roy, 2005; Tellex et al., 2011; Misra et al., 2017). While many approaches require a large amount of training data, recent works have developed zero/few shot learning for language and vision (Mukherjee and Hospedales, 2016; Xu et al., 2016, 2017a,b; Tsai and Salakhutdinov, 2017). Different from these previous works, this paper introduces a new task that connects language with vision for physical action-effect prediction. Actions (verb-noun pairs). W"
P18-1086,P16-1171,1,0.881929,"ly information extraction techniques to extract facts from a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys1 This dataset is available at http://lair.cse.msu. edu/lair/projects/actioneffect.html 935 3 ical causality of action verbs (Gao et al., 2016) and other physical properties of verbs (Forbes and Choi, 2017; Zellers and Choi, 2017; Chao et al., 2015). But they do not address action-effect prediction. Action-Effect Data Collection We collected a dataset to support the investigation on physical action-effect prediction. This dataset consists of actions expressed in the form of verbnoun pairs, effects of actions described in language, and effects of actions depicted in images. Note that, as we would like to have a wide range of possible effects, language data and image data are collected separately. The idea of modeling object physical s"
P18-1086,S13-1035,0,0.0134355,"shot learning for language and vision (Mukherjee and Hospedales, 2016; Xu et al., 2016, 2017a,b; Tsai and Salakhutdinov, 2017). Different from these previous works, this paper introduces a new task that connects language with vision for physical action-effect prediction. Actions (verb-noun pairs). We selected 40 nouns that represent everyday life objects, most of them are from the COCO dataset (Lin et al., 2014), with a combination of food, kitchen ware, furniture, indoor objects, and outdoor objects. We also identified top 3000 most frequently used verbs from Google Syntactic N-gram dataset (Goldberg and Orwant, 2013) (Verbargs set). And we extracted top frequent verb-noun pairs containing a verb from the top 3000 verbs and a noun in the 40 nouns which hold a dobj (i.e., direct object) dependency relation. This resulted in 6573 candidate verbnoun pairs. As changes to an object can occur at various dimensions (e.g., size, color, location, attachment, etc.), we manually selected a subset of verb-noun pairs based on the following criteria: (1) changes to the objects are visible (as opposed to other types such as temperature change, etc.); and (2) changes reflect one particular dimension as opposed to multiple"
P18-1086,D11-1027,0,0.154297,"own that, using a simple bootstrapping strategy, our approach can combine the noisy web data with a small number of seed examples to improve action-effect prediction. In addition, for a new verb-noun pair, our approach can infer its effect descriptions and predict action-effect relations only based on 3 image examples. The contributions of this paper are three folds. First, it introduces a new task on physical actioneffect prediction, a first step towards an under2 Related Work In the NLP community, there has been extensive work that models cause-effect relations from text (Cole et al., 2005; Do et al., 2011; Yang and Mao, 2014). Most of these previous studies address high-level causal relations between events, for example, “the collapse of the housing bubble” causes the effect of “stock prices to fall” (Sharp et al., 2016). They do not concern the kind of naive physical action-effect relations in this paper. There is also an increasing amount of effort on capturing commonsense knowledge, for example, through knowledge base population. Except for few (Yatskar et al., 2016) that acquires knowledge from images, most of the previous effort apply information extraction techniques to extract facts fro"
P18-1086,C10-1032,0,0.0280567,"these previous studies address high-level causal relations between events, for example, “the collapse of the housing bubble” causes the effect of “stock prices to fall” (Sharp et al., 2016). They do not concern the kind of naive physical action-effect relations in this paper. There is also an increasing amount of effort on capturing commonsense knowledge, for example, through knowledge base population. Except for few (Yatskar et al., 2016) that acquires knowledge from images, most of the previous effort apply information extraction techniques to extract facts from a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys1 This dataset is available at http://lair.cse.msu. edu/lair/projects/actioneffect.html 935 3 ical causality of action verbs (Gao et al., 2016) and other physical properties of verbs (Forbes and Choi, 2017; Zellers and Choi, 2017; C"
P18-1086,D16-1014,0,0.135356,"ch can infer its effect descriptions and predict action-effect relations only based on 3 image examples. The contributions of this paper are three folds. First, it introduces a new task on physical actioneffect prediction, a first step towards an under2 Related Work In the NLP community, there has been extensive work that models cause-effect relations from text (Cole et al., 2005; Do et al., 2011; Yang and Mao, 2014). Most of these previous studies address high-level causal relations between events, for example, “the collapse of the housing bubble” causes the effect of “stock prices to fall” (Sharp et al., 2016). They do not concern the kind of naive physical action-effect relations in this paper. There is also an increasing amount of effort on capturing commonsense knowledge, for example, through knowledge base population. Except for few (Yatskar et al., 2016) that acquires knowledge from images, most of the previous effort apply information extraction techniques to extract facts from a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts a"
P18-1086,D17-1106,0,0.0602605,"Missing"
P18-1086,P16-1011,1,0.851729,"e (as opposed to other types such as temperature change, etc.); and (2) changes reflect one particular dimension as opposed to multiple dimensions (as entailed by high-level actions such as “cook a meal”, which correspond to multiple dimensions of change and can be further decomposed into basic actions). As a result, we created a subset of 140 verb-noun pairs (containing 62 unique verbs and 39 unique nouns) for our investigation. In the robotics community, an important task is to enable robots to follow human natural language instructions. Previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016, 2017) explicitly model verb semantics as desired goal states and thus linking natural language commands with underlying planning systems for action planning and execution. However, these studies were carried out either in a simulated world or in a carefully curated simple environment within the limitation of the robot’s manipulation system. And they only focus on a very limited set of domain specific actions which often only involve the change of locations. In this work, we study a set of open-domain physical actions and a variety of effects perceived from the environment (i.e., from images)"
P18-1086,P15-1096,0,0.03005,"e objects are visible (as opposed to other types such as temperature change, etc.); and (2) changes reflect one particular dimension as opposed to multiple dimensions (as entailed by high-level actions such as “cook a meal”, which correspond to multiple dimensions of change and can be further decomposed into basic actions). As a result, we created a subset of 140 verb-noun pairs (containing 62 unique verbs and 39 unique nouns) for our investigation. In the robotics community, an important task is to enable robots to follow human natural language instructions. Previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016, 2017) explicitly model verb semantics as desired goal states and thus linking natural language commands with underlying planning systems for action planning and execution. However, these studies were carried out either in a simulated world or in a carefully curated simple environment within the limitation of the robot’s manipulation system. And they only focus on a very limited set of domain specific actions which often only involve the change of locations. In this work, we study a set of open-domain physical actions and a variety of effects perceived from the environment"
P18-1086,P17-1150,1,0.876715,"Missing"
P18-1086,W14-4313,1,0.777622,"(1) changes to the objects are visible (as opposed to other types such as temperature change, etc.); and (2) changes reflect one particular dimension as opposed to multiple dimensions (as entailed by high-level actions such as “cook a meal”, which correspond to multiple dimensions of change and can be further decomposed into basic actions). As a result, we created a subset of 140 verb-noun pairs (containing 62 unique verbs and 39 unique nouns) for our investigation. In the robotics community, an important task is to enable robots to follow human natural language instructions. Previous works (She et al., 2014; Misra et al., 2015; She and Chai, 2016, 2017) explicitly model verb semantics as desired goal states and thus linking natural language commands with underlying planning systems for action planning and execution. However, these studies were carried out either in a simulated world or in a carefully curated simple environment within the limitation of the robot’s manipulation system. And they only focus on a very limited set of domain specific actions which often only involve the change of locations. In this work, we study a set of open-domain physical actions and a variety of effects perceived"
P18-1086,D16-1089,0,0.0376384,"Missing"
P18-1086,speer-havasi-2012-representing,0,0.0186414,"tions which often only involve the change of locations. In this work, we study a set of open-domain physical actions and a variety of effects perceived from the environment (i.e., from images). Effects Described in Language. The basic knowledge about physical action-effect is so fundamental and shared among humans. It is often presupposed in our communication and not explicitly stated. Thus, it is difficult to extract naive action-effect relations from the existing textual data (e.g., web). This kind of knowledge is also not readily available in commonsense knowledge bases such as ConceptNet (Speer and Havasi, 2012). To overcome this problem, we applied crowd-sourcing (Amazon Mechanical Turk) and collected a dataset of language descriptions describing effects for each of the 140 verb-noun pairs. The workers were shown a verb-noun pair, and were asked to use their own words and imag936 Action ignite paper soak shirt fry potato stain shirt Effect Text The paper is on fire. The shirt is thoroughly wet. The potatoes become crisp and golden. There is a visible mark on the shirt. Table 1: Example action and effect text from our collected data. inations to describe what changes might occur to the corresponding"
P18-1086,D14-1162,0,0.0800801,"Missing"
P18-1086,D16-1201,0,0.0420641,"Missing"
P18-1086,P12-1065,0,0.0768891,"Missing"
P18-1086,N16-1019,1,0.891774,"Missing"
P18-1086,N16-1023,0,0.0872902,"2 Related Work In the NLP community, there has been extensive work that models cause-effect relations from text (Cole et al., 2005; Do et al., 2011; Yang and Mao, 2014). Most of these previous studies address high-level causal relations between events, for example, “the collapse of the housing bubble” causes the effect of “stock prices to fall” (Sharp et al., 2016). They do not concern the kind of naive physical action-effect relations in this paper. There is also an increasing amount of effort on capturing commonsense knowledge, for example, through knowledge base population. Except for few (Yatskar et al., 2016) that acquires knowledge from images, most of the previous effort apply information extraction techniques to extract facts from a large amount of web data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys1 This dataset is available at http://lair.cse.msu. edu/l"
P18-1086,D17-1099,0,0.0677201,"data (Dredze et al., 2010; Rajani and Mooney, 2016). DBPedia (Lehmann et al., 2015), Freebase (Bollacker et al., 2008), and YAGO (Suchanek et al., 2007) knowledge bases contain millions of facts about the world such as people and places. However, they do not contain basic cause-effect knowledge related to concrete actions and their effects to the world. Recent work started looking into phys1 This dataset is available at http://lair.cse.msu. edu/lair/projects/actioneffect.html 935 3 ical causality of action verbs (Gao et al., 2016) and other physical properties of verbs (Forbes and Choi, 2017; Zellers and Choi, 2017; Chao et al., 2015). But they do not address action-effect prediction. Action-Effect Data Collection We collected a dataset to support the investigation on physical action-effect prediction. This dataset consists of actions expressed in the form of verbnoun pairs, effects of actions described in language, and effects of actions depicted in images. Note that, as we would like to have a wide range of possible effects, language data and image data are collected separately. The idea of modeling object physical state change has also been studied in the computer vision community (Fire and Zhu, 2016"
P98-2180,C94-2119,0,0.00980591,"h as H y p e r n y m or Synonym. For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et al. (1995) and Agirre and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet. The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words. For example, some of the top semrel paths in MindNet between pen and pencil, are shown below: pen6-Means---draw--Means-->pencil pen<--Means--write--Means--~pencil pen--Hyp-->instrument~--Hyp---penci"
P98-2180,C96-1013,0,0.0593699,"Missing"
P98-2180,J92-4003,0,0.0185849,"utional similarity. One is based on identifying direct, paradigmatic relations between the words, such as H y p e r n y m or Synonym. For example, paradigmatic relations in WordNet have been used by many to determine similarity, including Li et al. (1995) and Agirre and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet. The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words. For example, some of the top semrel paths in MindNet between pen and pencil, are shown"
P98-2180,P85-1037,0,0.0777931,"ned in MindNet exploits the very same broadcoverage parser used in the Microsoft Word 97 grammar checker. This parser produces syntactic parse trees and deeper logical forms, to which rules are applied that generate corresponding structures of semantic relations. The parser has n o t been specially tuned to process dictionary definitions. All enhancements to the parser are geared to handle the immense variety of general text, of which dictionary definitions are simply a modest subset. There have been many other attempts to process dictionary definitions using heuristic pattern matching (e.g., Chodorow et al. 1985), specially constructed definition parsers (e.g., Wilks et al. 1996, Vossen 1995), and even general coverage syntactic parsers (e.g., Briscoe and Carroll 1993). However, none of these has succeeded in producing the breadth of semantic relations across entire dictionaries that has been produced for MindNet. Vanderwende (1996) describes in detail the methodology used in the extraction of the semantic relations comprising MindNet. A truly broad-coverage parser is an essential component of this process, and it is the basis for extending it to other sources of information such as encyclopedias and"
P98-2180,P94-1038,0,0.00795406,"Missing"
P98-2180,E93-1028,0,0.12236,"me User relation types m relational triples (see Wilks et al. 1996). The larger contexts from which these relations have been taken have generally not been retained. For labeled relations, only a few researchers (recently, Barri~re and Popowich 1996), have appeared to be interested in entire semantic structures extracted from dictionary definitions, though they have not reported extracting a significant number of them. These relation types may be contrasted with simple co-occurrence statistics used to create network structures from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al. (1996). Labeled relations, while more difficult to obtain, provide greater power for resolving both structural attachment and word sense ambiguities. While many researchers have acknowledged the utility of labeled relations, they have been at times either unable (e.g., for lack of a sufficiently powerful parser) or unwilling (e.g., focused on purely statistical methods) to make the effort to obtain them. This deficiency limits the characterization of word pairs such as river~bank (Wilks et al. 1996) and write~pen (Veronis and Ide 1990) to simple relatedness, whereas the labe"
P98-2180,W95-0105,0,0.00919111,"and Rigau (1996). The other strategy is based on identifying syntagmatic relations with other words that similar words have in common. Syntagmatic strategies for determining similarity have often been based on statistical analyses of large corpora that yield clusters of words occurring in similar bigram and trigram contexts (e.g., Brown et al. 1992, Yarowsky 1992), as well as in similar predicateargument structure contexts (e.g., Grishman and Sterling 1994). There have been a number of attempts to combine paradigmatic and syntagmatic similarity strategies (e.g., Hearst and Grefenstette 1992, Resnik 1995). However, none of these has completely integrated both syntagmatic and paradigmatic information into a single repository, as is the case with MindNet. The MindNet similarity procedure is based on the top-ranked (by weight) semrel paths between words. For example, some of the top semrel paths in MindNet between pen and pencil, are shown below: pen6-Means---draw--Means-->pencil pen<--Means--write--Means--~pencil pen--Hyp-->instrument~--Hyp---pencil pen--Hyp-->write--Means---~pencil pen6-Means--write6--Hyp--pencil Table 3. Highly weighted semrel paths between pen and pencil In the above example,"
P98-2180,C90-2067,0,0.0434765,"urce Subclass Synonym Time User relation types m relational triples (see Wilks et al. 1996). The larger contexts from which these relations have been taken have generally not been retained. For labeled relations, only a few researchers (recently, Barri~re and Popowich 1996), have appeared to be interested in entire semantic structures extracted from dictionary definitions, though they have not reported extracting a significant number of them. These relation types may be contrasted with simple co-occurrence statistics used to create network structures from dictionaries by researchers including Veronis and Ide (1990), Kozima and Furugori (1993), and Wilks et al. (1996). Labeled relations, while more difficult to obtain, provide greater power for resolving both structural attachment and word sense ambiguities. While many researchers have acknowledged the utility of labeled relations, they have been at times either unable (e.g., for lack of a sufficiently powerful parser) or unwilling (e.g., focused on purely statistical methods) to make the effort to obtain them. This deficiency limits the characterization of word pairs such as river~bank (Wilks et al. 1996) and write~pen (Veronis and Ide 1990) to simple r"
P98-2180,J96-3009,0,0.0303968,"Missing"
P98-2180,C92-2070,0,0.0720286,"rom MRDs, including Veronis and Ide (1990) and Kozima and Furugori (1993), typically only implemented forward links (from headwords to their definition words) in those networks. Words were not related backward to any of the headwords whose definitions mentioned them, and words co-occurring in the same definition were not related directly. In the fully inverted structures stored in MindNet, however, all words are cross-linked, no matter where they appear. The massive network of inverted semrel structures contained in MindNet invalidates the criticism leveled against dictionary-based methods by Yarowsky (1992) and Ide and Veronis (1993) that LKBs created from MRDs provide spotty coverage of a language at best. Experiments described elsewhere (Richardson 1997) demonstrate the comprehensive coverage of the information contained in MindNet. Some statistics indicating the size (rounded to the nearest thousand) of the current version of MindNet and the processing time required to create it are provided in the table below. The definitions and example sentences are from the Longman Dictionary of Contemporary English (LDOCE) and the American Heritage Dictionary, 3 ra Edition (AHD3). Dictionaries used Time"
P98-2180,C96-1005,0,\N,Missing
P98-2180,J93-1002,0,\N,Missing
Q13-1032,W12-2039,0,0.0553653,"Missing"
Q13-1032,W11-2401,0,0.080152,"Missing"
Q13-1032,E09-1065,0,0.131415,"can then use these features and the labels to train the classifier we desire. These are “between-item” features: they concern the relationships between  and  , as it is such features that we hope will be predictive of whether the items are similar. Note that all features below are computed after stopwords have been removed from both items. We also treat words that appear in the question as stopwords in a process termed as “question demoting” by Mohler et al. (2011), who found this resulted in noticeable improvements in measuring similarities between student answers and answer key entries. Mohler and Mihalcea (2009) showed that a feature based on an LSA decomposition (Dumais, 2004) of Wikipedia was particularly powerful for grading. In a similar vein, we computed the LSA for all English Wikipedia articles (from 2012) using the most frequent 100k words as a vocabulary; we then computed the similarity between answers using the top 100 singular vectors. In the descriptions below, we use “tf-idf vector similarity” to refer to the cosine similarity between standard tf-idf term-frequency vectors. These tf and idf scores are computed using the entire corpus of relevant answers, as this process does not make use"
Q13-1032,P11-1076,0,0.577842,"ring text, such as using LDA to group answers by inferred topics, do their best to explain these variations in terms of distributions over words, but are limited by their word-based representations of text. Ideally, we would like to learn how to group items together based on data, with an array of features that expand over time as our technologies grow more mature. We propose to model this distance function by training a classifier that predicts whether two answers should be grouped together, in the vein of past work which modeled the similarity between student answers and answer key entries (Mohler et al. 2011). The notion of this distance function is subtle. We want answers that are paraphrases of each other, such as &quot;the Congress&quot; and &quot;the houses of Cngress [sic]&quot; to be close, but &quot;the Senate and the House of Representatives&quot; to be different from these so we can mark out this more precise mode. Because we are modeling the distance between answers as opposed to the answers themselves, we can use “between-item” features that measure semantic or spelling differences. We thus supply our classifier with the best available features that can account for 392 misspellings, tenses, and other variations, wit"
Q13-1032,N12-3006,1,\N,Missing
Q13-1032,W05-0202,0,\N,Missing
ringger-etal-2004-using,A00-2018,1,\N,Missing
ringger-etal-2004-using,J93-2004,0,\N,Missing
ringger-etal-2004-using,J03-4003,0,\N,Missing
ringger-etal-2004-using,P02-1034,0,\N,Missing
ringger-etal-2004-using,P02-1035,0,\N,Missing
ringger-etal-2004-using,P95-1037,0,\N,Missing
S14-1015,W13-1302,0,0.00932687,"n. 2 et al., 2010; Ordonez et al., 2011), using text surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triplets of recognized objects in the scene, but does not include a content selection model to select the best sentence. We extend Midge with content and sentence selection rules to use it as a baseline. The visual facts we annotate are motivated by research in machine vision. Attributes are a good intermedia"
S14-1015,D10-1049,0,0.040539,"been widely used in object recognition (Felzenszwalb et al., 2010). Yet, no work tests the contribution of these labels for sentence generation. There is also a significant amount of work on other grounded language problems, where related models have been developed. Visual referring expression generation systems (Krahmer and Van Deemter, 2012; Mitchell et al., 2013; FitzGerald et al., 2013) aim to identify specific objects, a sub-problem we deal with when describing images more generally. Other research generates descriptions in simulated worlds and, like this work, uses feature rich models (Angeli et al., 2010), or syntactic structures like PCFGs (Chen et al., 2010; Konstas and Lapata, 2012) but does not combine the two. Finally, Zitnick and Parikh (2013) study sentences describing clipart scenes. They present a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have been proposed for constructing sentences from images, including copying captions from other images (Farhadi 2 Available at : http://homes.cs.washington.edu/˜my89/ 111 3 in a labeled image.4 We refer to all of these annotations,"
S14-1015,D09-1030,0,0.020787,"bias from annotators’ perception about which objects are important, since one of the problems we would like to solve is content selection. This dataset will be available for future experiments. We built on the dataset from (Rashtchian et al., 2010) which contained 8,000 Flickr images and associated descriptions gathered using Amazon Mechanical Turk (MTurk). Restricting ourselves to Creative Commons images, we sampled 500 images for annotation. We collected annotations of images in three stages using MTurk, and assigned each annotation task to 3-5 workers to improve quality through redundancy (Callison-Burch, 2009). Below we describe the process for annotating a single image. Stage 1: We prompted five turkers to list all objects in an image, ignoring objects that are parts of larger objects (e.g., the arms of a person), which we collected later in Stage 3. This list also included groups, such as crowds of people. Stage 2: For each unique object label from Stage 1, we asked two turkers to draw a polygon around the object identified.3 In cases where the object is a group, we also asked for the number of objects present (1-6 or many). Finally, we created a list of all references to the object from the firs"
S14-1015,P12-1038,0,0.0632436,"halving the difference from human performance to two baselines on 4-gram BLEU. In ablations, we measure the importance of different annotations and visual cues, showing that annotation of activities and relative bounding box information between objects are crucial to generating human-like description. 2 et al., 2010; Ordonez et al., 2011), using text surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triple"
S14-1015,de-marneffe-etal-2006-generating,0,0.104217,"Missing"
S14-1015,D13-1128,0,0.219833,"Missing"
S14-1015,E12-1076,0,0.27079,"t surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triplets of recognized objects in the scene, but does not include a content selection model to select the best sentence. We extend Midge with content and sentence selection rules to use it as a baseline. The visual facts we annotate are motivated by research in machine vision. Attributes are a good intermediate representation for categorization (Farhadi"
S14-1015,N13-1137,0,0.0823812,"Missing"
S14-1015,W99-0604,0,0.0902418,"ring something true in the image; and salience, which sentence is capturing important things in the image while still being concise. Two annotators annotated all test pairs for all criteria for a given pair of systems. Six annotators were used (none authors) and agreement was high (Cohen’s kappa = 0.963, 0.823 and 0.703 for grammar, truth and salience). Machine Translation Baseline The first baseline is designed to see if it is possible to generate good sentences from the facet string labels alone, with no visual information. We use an extension of phrase-based machine translation techniques (Och et al., 1999). We created a virtual bitext by pairing each image description (the target sentence) with a sequence10 of visual identifiers (the source “sentence”) listing strings from the facet labels. Since phrases produced by turkers lack many of the functions words needed to create fluent sentences, we added one of 47 function words either at the start or the end of each output phrase. The translation model included standard features such as language model score (using our caption language model described previously), word count, phrase count, linear distortion, and the count of deleted source words. We"
S14-1015,P10-1126,0,0.0977587,"Missing"
S14-1015,P03-1021,0,0.0393045,"s computation is intractable because we need to consider all possible sentences, so we use beam search for strings up to a fixed length. Reranking Generating directly from the process in Figure 3 results in sentences that may be short and repetitive because the model score is a product of locally normalized distributions. The reranker takes as input a candidate list c, for an image I, as decoded from the generative model. The candidate list includes the top-k scoring hypotheses for each sentence length up to a fixed maximum. A linear scoring function is used for reranking optimized with MERT (Och, 2003) to maximize BLEU-2. 5 Features We construct indicator features to capture variation in usage in different parts of the sentence, types of objects that are mentioned, visual salience, and semantic and visual coordination between objects. The features are included in the maximum entropy models used to parameterize the distributions described in Figure 3. Whenever possible, we use WordNet Synsets (Miller, 1995) instead of lexical features to limit over-fitting. Features in the generative model use tests for local properties, such as the identity of a synset of a word in WordNet, conjoined with a"
S14-1015,D13-1197,1,0.58205,"adi et al., 2009). Activity recognition is an emerging area in images (Li and Fei-Fei, 2007; Yao et al., 2011; Sharma et al., 2013) and video (Weinland et al., 2011), although less studied than object recognition. Also, parts have been widely used in object recognition (Felzenszwalb et al., 2010). Yet, no work tests the contribution of these labels for sentence generation. There is also a significant amount of work on other grounded language problems, where related models have been developed. Visual referring expression generation systems (Krahmer and Van Deemter, 2012; Mitchell et al., 2013; FitzGerald et al., 2013) aim to identify specific objects, a sub-problem we deal with when describing images more generally. Other research generates descriptions in simulated worlds and, like this work, uses feature rich models (Angeli et al., 2010), or syntactic structures like PCFGs (Chen et al., 2010; Konstas and Lapata, 2012) but does not combine the two. Finally, Zitnick and Parikh (2013) study sentences describing clipart scenes. They present a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have b"
S14-1015,2001.mtsummit-papers.68,0,0.0257801,"Missing"
S14-1015,P12-1039,0,0.0297731,"work tests the contribution of these labels for sentence generation. There is also a significant amount of work on other grounded language problems, where related models have been developed. Visual referring expression generation systems (Krahmer and Van Deemter, 2012; Mitchell et al., 2013; FitzGerald et al., 2013) aim to identify specific objects, a sub-problem we deal with when describing images more generally. Other research generates descriptions in simulated worlds and, like this work, uses feature rich models (Angeli et al., 2010), or syntactic structures like PCFGs (Chen et al., 2010; Konstas and Lapata, 2012) but does not combine the two. Finally, Zitnick and Parikh (2013) study sentences describing clipart scenes. They present a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have been proposed for constructing sentences from images, including copying captions from other images (Farhadi 2 Available at : http://homes.cs.washington.edu/˜my89/ 111 3 in a labeled image.4 We refer to all of these annotations, including the merged Object labels, as facets. These labels provide feature norms"
S14-1015,W10-0721,0,0.428906,"richly annotated images to approximate gold standard visual recognition. In collecting the data, we sought a visual annotation with sufficient coverage to support the generation of as many of the words in the original image descriptions as possible. We also aimed to make it as visually exhaustive as possible—giving equal treatment to all visible objects. This ensures less bias from annotators’ perception about which objects are important, since one of the problems we would like to solve is content selection. This dataset will be available for future experiments. We built on the dataset from (Rashtchian et al., 2010) which contained 8,000 Flickr images and associated descriptions gathered using Amazon Mechanical Turk (MTurk). Restricting ourselves to Creative Commons images, we sampled 500 images for annotation. We collected annotations of images in three stages using MTurk, and assigned each annotation task to 3-5 workers to improve quality through redundancy (Callison-Burch, 2009). Below we describe the process for annotating a single image. Stage 1: We prompted five turkers to list all objects in an image, ignoring objects that are parts of larger objects (e.g., the arms of a person), which we collecte"
S14-1015,J12-1006,0,0.0506618,"Missing"
S14-1015,D12-1130,0,0.153821,"ndaries and descriptive text, here including the facts that the children are “riding” and “walking” and that they are “young.” Our goal is to be as exhaustive as possible, giving equal treatment to all objects. For example, the annotations in Figure 1 contain enough information to generate the first three sentences and most of the content in the remaining two. Labels gathered in this way are a type of feature norms (McRae et al., 2005), which have been used in the cognitive science literature to approximate human perception and were recently used as a visual proxy in distributional semantics (Silberer and Lapata, 2012). We present the first effort, that we are aware of, for using feature norms to study image description generation. Such rich data allows us to develop significantly more comprehensive generation models. We divide generation into choices about which visual content to select and how to realize a sentence that describes that content. Our approach is grammarbased, feature-rich, and jointly models both decisions. The content selection model includes latent variables that align phrases to visual objects and features that, for example, measure how visual salience and spatial relationships influence"
S14-1015,P13-1056,0,0.0159995,"a number of factors influencing overall descriptive quality, several of which we use in sentence generation for the first time. Related Work A number of approaches have been proposed for constructing sentences from images, including copying captions from other images (Farhadi 2 Available at : http://homes.cs.washington.edu/˜my89/ 111 3 in a labeled image.4 We refer to all of these annotations, including the merged Object labels, as facets. These labels provide feature norms (McRae et al., 2005), which have recently used as a visual proxy in distributional semantics (Silberer and Lapata, 2012; Silberer et al., 2013) but have not been previous studied for generation. This annotation of 500 images (2500 sentences) yielded over 4000 object instances and 100,000 textual labels. Dataset We collected a dataset of richly annotated images to approximate gold standard visual recognition. In collecting the data, we sought a visual annotation with sufficient coverage to support the generation of as many of the words in the original image descriptions as possible. We also aimed to make it as visually exhaustive as possible—giving equal treatment to all visible objects. This ensures less bias from annotators’ percept"
S14-1015,P13-1006,0,0.0292359,"human-like description. 2 et al., 2010; Ordonez et al., 2011), using text surrounding an image in a news article (Feng and Lapata, 2010), filling visual sentence templates (Kulkarni et al., 2011; Yang et al., 2011; Elliott and Keller, 2013), and stitching together existing sentence descriptions (Gupta and Mannem, 2012; Kuznetsova et al., 2012). However, due to the lack of reliable detectors, especially for activities, many previous systems have a small vocabulary and must generate many words, including verbs, with no direct visual support. These problems also extend to video caption systems (Yu and Siskind, 2013; Krishnamoorthy et al., 2013). The Midge algorithm (Mitchell et al., 2012) is most closely related to our approach, and will provide a baseline in our experiments. Midge is syntax-driven but again uses a small vocabulary without direct visual support for every word. It outputs a large set of sentences to describe all triplets of recognized objects in the scene, but does not include a content selection model to select the best sentence. We extend Midge with content and sentence selection rules to use it as a baseline. The visual facts we annotate are motivated by research in machine vision. At"
S14-1015,W04-0216,0,0.0134982,"Missing"
S14-1015,D11-1041,0,\N,Missing
S14-1015,P02-1040,0,\N,Missing
S14-1015,W10-0707,0,\N,Missing
tepper-etal-2012-statistical,W06-3309,0,\N,Missing
tepper-etal-2012-statistical,I08-1050,0,\N,Missing
tepper-etal-2012-statistical,J02-4002,0,\N,Missing
tepper-etal-2012-statistical,W09-3603,0,\N,Missing
tepper-etal-2012-statistical,D10-1082,0,\N,Missing
W11-1825,J93-1003,0,0.0914792,"alues were rounded to the closest integer. We found, however, that adding these features did not improve results. 2.2.2 Feature combination and reduction We experimented with feature reduction and feature combination within the set of features described here. For feature reduction we tried a number of simple approaches that typically work well in text classification. The latter is similar to the task at hand, in that there is a very large but sparse feature set. We tried two feature reduction methods: a simple count cutoff, and selection of the top n features in terms of log likelihood ratio (Dunning, 1993) with the target values. For a count cutoff, we used cutoffs from 3 to 10, but we failed to observe any consistent gains. Only low cutoffs (3 and occasionally 5) would ever produce any small improvements on the development set. Using 159 log likelihood ratio (as determined on the training set), we reduced the total number of features to between 10,000 and 75,000. None of these experiments improved results, however. One potential reason for this negative result may be that there were a lot of features in our set that capture the same phenomenon in different ways, i.e. which correlate highly. By"
W11-1825,de-marneffe-etal-2006-generating,0,0.0209894,"Missing"
W11-1825,P08-2026,0,0.0458061,"ndicate edges with posterior > 0.95; edges with posterior < 0.05 were omitted. Most of the ambiguity is in the attachment of “elicited”. words in the sentence. Since proteins may consist of multiple words, for paths we picked a single representative word for each protein to act as its starting point and ending point. Generally this was the token inside the protein that is closest to the root of the dependency parse. In the case of ties, we picked the rightmost such node. 2.1.1 McClosky-Charniak-Stanford parses The organizers provide parses from a version of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. In addition, we used an improved set of parsing models that leverage unsupervised data, MCCC-I (McClosky, 2010). In both cases, the Stanford Parser was used to convert constituency trees in the Penn Treebank format into labeled dependency parses: we used the collapsed dependency format. 2.1.2 Dependency posteriors Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al. 2008). McClosky-Charniak parses in two passes: the first pass"
W11-1825,N10-1004,0,0.011112,"ltiple words, for paths we picked a single representative word for each protein to act as its starting point and ending point. Generally this was the token inside the protein that is closest to the root of the dependency parse. In the case of ties, we picked the rightmost such node. 2.1.1 McClosky-Charniak-Stanford parses The organizers provide parses from a version of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. In addition, we used an improved set of parsing models that leverage unsupervised data, MCCC-I (McClosky, 2010). In both cases, the Stanford Parser was used to convert constituency trees in the Penn Treebank format into labeled dependency parses: we used the collapsed dependency format. 2.1.2 Dependency posteriors Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al. 2008). McClosky-Charniak parses in two passes: the first pass is a generative model that produces a set of n-best candidates, and the 156 second pass is a discriminative reranker that uses a rich set of features including non-local informat"
W11-1825,P08-1023,0,0.00685334,"ion of the McClosky-Charniak parser, MCCC (McClosky and Charniak, 2008), which is a two-stage parser/reranker trained on the GENIA corpus. In addition, we used an improved set of parsing models that leverage unsupervised data, MCCC-I (McClosky, 2010). In both cases, the Stanford Parser was used to convert constituency trees in the Penn Treebank format into labeled dependency parses: we used the collapsed dependency format. 2.1.2 Dependency posteriors Effectively maintaining and leveraging the ambiguity present in the underlying parser has improved task accuracy in some downstream tasks (e.g., Mi et al. 2008). McClosky-Charniak parses in two passes: the first pass is a generative model that produces a set of n-best candidates, and the 156 second pass is a discriminative reranker that uses a rich set of features including non-local information. We renormalized the outputs from this log-linear discriminative model to get a posterior distribution over the 50-best parses. This set of parses preserved some of the syntactic ambiguity present in the sentence. The Stanford parser deterministically converts phrase-structure trees into labeled dependency graphs (de Marneffe et al., 2006). We converted each"
W11-1825,N10-1123,1,0.766932,"ticipating systems, so it would be interesting to consider whether there are some annotations in the development set that cannot be predicted by any of the participating systems1. If this is the case, then those triggers and edges would present an interesting topic for discussion. This might result either in a modification of the annotation protocols, or an opportunity for all systems to learn more. After a certain amount of feature engineering, we found it difficult to achieve further improvements in F1. Perhaps we need a significant shift in architecture, such as a shift to joint inference (Poon and Vanderwende, 2010). Our system may be limited by the pipeline architecture. 1 Our system output for the 2011development set can be downloaded from http://research.microsoft.com/bionlp/ 162 MWEs (multi-word entities) are a challenge. Better multi-word triggers accuracy may improve system performance. Multi-word proteins often led to incorrect part-of-speech tags and parse trees. Cursory inspection of the Epigenetics task shows that some domain-specific knowledge would have been beneficial. Our system had significant difficulties with the rare inverse event types, e.g. “demethylation” (e.g., there are 319 example"
W11-1825,J08-1002,0,\N,Missing
W11-1825,D08-1022,0,\N,Missing
W11-1825,W09-1402,0,\N,Missing
W11-1825,D09-1098,0,\N,Missing
W16-1007,W08-2227,1,0.373098,"Missing"
W16-1007,J08-4004,0,0.0549757,"ory for coders who have not captured this relation. The agreement according to Fleiss’s Kappa κ = 0.49 without applying basic closure and κ = 0.51 with closure10 , which shows moderate agreement. For reference, the agreement on semantic link annotation in the most recent clinical TempEval was 0.44 without closure and 0.47 with closure. 6 Related Work Given that there are no prefixed set of event entity spans for straight-forward computation of interannotator agreement, we do the following: among all the annotators, we aggregate the spans of the annotated event entity as the annotation object (Artstein and Poesio, 2008). Then, if there exists a span which is not annotated by one of the coders (annotators) it will be labeled as ‘NONE’ for its category. The agreement according to Fleiss’s Kappa κ = 0.91, which shows substantial agreement on event entity annotation. Although direct comparison of κ values is not possible, as a point of reference, the event One of the most recent temporal annotation schemas is Temporal Histories of Your Medical Event (THYME) (Styler et al., 2014). This annotation guideline was devised for the purpose of establishing timelines in clinical narratives, i.e. the free text portions co"
W16-1007,D13-1178,0,0.0209354,"s. Scripts present structured knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . This corpus contains high quality"
W16-1007,W13-2322,0,0.0370696,"and auxiliary verbs. Whenever the semantic contribution of the verb is minimal and the nonverb element of the construction is an event in the 4 These verbs are the same as aspectual events characterized by TimeML, which include ‘INITIATES’, ‘CULMINATES’, ‘TERMINATES’, ‘CONTINUES’ and ‘REINITIATES’. TRIPS ontology, we annotate the non-verb element as the event. Thus, we annotate the noun predicate ’offer’ in the sentence ‘Yesterday, John made an offer to buy the house for 350,000’, similarly to the way Abstract Meaning Representation (AMR) drops the light verb and promotes the noun predicate (Banarescu et al., 2013). This annotation is also close to the PropBank annotation of copulas and light verbs (Bonial et al., 2014), where they annotate the noun predicate and predicate adjective as the event; however, PropBank includes an explicit marking of the verb as either a light verb or a copula verb. 3 The Semantic Relations Between Event Entities A more challenging problem than event entity detection is the identification of the semantic relation that holds between events. Events take place in time, hence temporal relations between events are crucial to study. Furthermore, causality plays a crucial role in e"
W16-1007,S15-2136,0,0.059425,"Missing"
W16-1007,S13-2002,0,0.0561543,"medical disorders and conditions, e.g., cancer, heart attack, stroke, etc. – Occurring: e.g., happen, occur. Our semantic framework captures the set of event entities and their pairwise semantic relations, which together form an inter-connected network of events. In this Section we define event entities and discuss their annotation process. 2.1 of interest in various NLP applications. However, there is still no consensus regarding the span of events and how they should be annotated. There has been some good progress in domain-specific annotation of events, e.g., recent Clinical TempEval task (Bethard, 2013) and THYME annotation scheme (Styler et al., 2014), however, the detection of events in broad-coverage natural language has been an ongoing endeavor in the field. One of the existing definitions for event is provided in the TimeML annotation schema (Pustejovsky et al., 2003): Definition Event is mainly used as a term referring to any situation that can happen, occur, or hold. The definition and detection of events has been a topic to the core story. 52 – Natural-phenomenon: e.g., earthquake, tsunami. This ontology has one of the richest event hierarchies, which perfectly serves our purpose of"
W16-1007,bittar-etal-2012-temporal,0,0.0283798,"t pair. Figure 2: Frequency of semantic links in our dataset. Figure 1: Semantic annotation of a sample story. – 9 causal relations: Including ‘cause (before/overlaps)’, ‘enable (before/overlaps)’, ‘prevent (before/overlaps)’, ‘cause-to-end (before/overlaps/during)’ – 4 temporal relations: Including ‘Before’, ‘Overlaps’, ‘Contains’, ‘Identity’. The semantic relation annotation between two events should start with deciding about any causal relations and then, if there was not any causal relation, proceed to choosing any existing temporal relation. 4 Annotating at Story level It has been shown (Bittar et al., 2012) that temporal annotation can be most properly carried out by taking into account the full context for sentences, as opposed to TimeML, which is a surface-based annotation. The scope and goal of this paper very well aligns with this observation. We carry out the annotation at the story level, meaning that we annotate inter-event relations across the five sentences of a story. It suffices to do the event-event relation specification minimally given the transitivity of temporal relations. For example for three consecutive events e1 e2 e3 one should only annotate the ‘before’ relation between e1"
W16-1007,bonial-etal-2014-propbank,0,0.0623713,"construction is an event in the 4 These verbs are the same as aspectual events characterized by TimeML, which include ‘INITIATES’, ‘CULMINATES’, ‘TERMINATES’, ‘CONTINUES’ and ‘REINITIATES’. TRIPS ontology, we annotate the non-verb element as the event. Thus, we annotate the noun predicate ’offer’ in the sentence ‘Yesterday, John made an offer to buy the house for 350,000’, similarly to the way Abstract Meaning Representation (AMR) drops the light verb and promotes the noun predicate (Banarescu et al., 2013). This annotation is also close to the PropBank annotation of copulas and light verbs (Bonial et al., 2014), where they annotate the noun predicate and predicate adjective as the event; however, PropBank includes an explicit marking of the verb as either a light verb or a copula verb. 3 The Semantic Relations Between Event Entities A more challenging problem than event entity detection is the identification of the semantic relation that holds between events. Events take place in time, hence temporal relations between events are crucial to study. Furthermore, causality plays a crucial role in establishing semantic relation between events, specifically in stories. In this Section, we provide details"
W16-1007,P08-1090,1,0.737058,"is commonsense knowledge can be best represented as scripts. Scripts present structured knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commons"
W16-1007,P09-1068,1,0.706816,"be best represented as scripts. Scripts present structured knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . Thi"
W16-1007,N13-1104,1,0.852947,"knowledge about stereotypical event sequences together with their participants. A well known script is the Restaurant Script, which includes the events {Entering, Sitting down, Asking for menus, Choosing meals, etc.}, and the participants {Customer, Waiter, Chef, Tables, etc.}. A large body of work in story understanding has focused on learning scripts (Schank and Abelson, 1977). Given that developing hand-built scripts is extremely time-consuming, there is a serious need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . This corpus contains high quality2 five-sentence stori"
W16-1007,W14-2903,0,0.0947084,"Missing"
W16-1007,C14-1198,0,0.162142,"annotates temporal and causal relations in parallel (Steven Bethard and Martin, 2008). Bethard et al. annotated a dataset of 1,000 conjoined-event temporal-causal relations, collected from Wall Street Journal corpus. Each event pair was annotated manually with both temporal (BEFORE, AFTER, NO-REL) and causal relations (CAUSE, NO-REL). For example, sentence 12 is an entry in their dataset. This dataset makes no distinction between various types of causal relation. (12) Fuel tanks had leaked and contaminated the soil. - (leaked BEFORE contaminated) - (leaked CAUSED contaminated). A recent work (Mirza and Tonelli, 2014) has proposed a TimeML-style annotation standard for capturing causal relations between events. They mainly introduce ‘CLINK’, analogous to ‘TLINK’ in TimeML, to be added to the existing TimeML link tags. Under this framework, Mirza et al (Mirza and Tonelli, 2014) annotates 318 CLINKs in TempEval3 TimeBank. They only annotate explicit causal relations signaled by linguistic markers, such as {because of, as a result of, due to, so, therefore, 59 thus}. Another relevant work is Richer Event Descriptions (RED) (Ikuta et al., 2014), which combines event coreference and THYME annotations, and also"
W16-1007,N16-1098,1,0.0733092,"s need for automatically induced scripts (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Balasubramanian et al., 2013; Cheung et al., 2013; Nguyen et al., 2015). It is evident that various NLU applications (text summarization, co-reference resolution and question answering, among others) can benefit from the rich inferential capabilities that structured knowledge about events can provide. The first step for any script learner is to decide on a corpus to drive the learning process. The most recent resource for this purpose is a corpus of short commonsense stories, called ROCStories (Mostafazadeh et al., 2016), which is a corpus of 40,000 short commonsense everyday stories 1 . This corpus contains high quality2 five-sentence stories 1 These stories can be found here: http://cs. rochester.edu/nlp/rocstories 2 Each of these stories have the following major characteristics: is realistic, has a clear beginning and ending where something happens in between, does not include anything irrelevant 51 Proceedings of the 4th Workshop on Events: Definition, Detection, Coreference, and Representation, pages 51–61, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics that are fu"
W16-1007,P15-1019,0,0.0541783,"Missing"
W16-1007,E14-1024,0,0.0242643,"g happens in between, does not include anything irrelevant 51 Proceedings of the 4th Workshop on Events: Definition, Detection, Coreference, and Representation, pages 51–61, c San Diego, California, June 17, 2016. 2016 Association for Computational Linguistics that are full of stereotypical causal and temporal relations between events, making them a perfect resource for learning narrative schemas. One of the prerequisites for learning scripts from these stories is to extract events and find inter-event semantic relations. Earlier work (Chambers and Jurafsky, 2008; Chambers and Jurafsky, 2009; Pichotta and Mooney, 2014; Rudinger et al., 2015) defines verbs as events and uses TimeML-based (Pustejovsky et al., 2003) learning for temporal ordering of events. This clearly has many shortcomings, including, but not limited to (1) not capturing a wide range of non-verbal events such as ‘earthquake’, (2) not capturing a more comprehensive set of semantic relations between events such as causality, which is a core relation in stories. In this paper we formally define a new comprehensive semantic framework for capturing stereotypical event-event temporal and causal relations in commonsense stories, the details of whi"
W16-1007,W11-0419,0,0.0270517,"κ values is not possible, as a point of reference, the event One of the most recent temporal annotation schemas is Temporal Histories of Your Medical Event (THYME) (Styler et al., 2014). This annotation guideline was devised for the purpose of establishing timelines in clinical narratives, i.e. the free text portions contained in electronic health records. In their work, they combine the TimeML annotation schema with Allen Interval Algebra, identifying the five temporal relations BEFORE, OVERLAP, BEGINS-ON, ENDS-ON, and CONTAINS. Of note is that they adopt the notion of narrative containers (Pustejovsky and Stubbs, 2011), which are time slices in which events can take place, such as DOCTIME (time of the report) and before DOCTIME. As such, the THYME guideline focuses on ordering events with respect to specific time intervals, while in our work, we are only focused on the relation between two events, without concern for ordering. Their simplification of temporal links is similar to ours, however, our reasoning for simplification takes 9 This is based on the fact that any relation such as ‘A enablebefore B’ or ‘A overlaps B’ can be naively approximated to ‘A before B’. 10 Temporal closure (Gerevini et al., 1995"
W16-1007,D15-1195,0,0.0555407,"Missing"
W16-1007,bethard-etal-2008-building,0,0.134989,"Missing"
W16-1007,Q14-1012,0,0.0456985,"Missing"
W16-1007,H89-1033,0,0.739052,"ons between events. By annotating a total of 1,600 sentences in the context of 320 five-sentence short stories sampled from ROCStories corpus, we demonstrate that these stories are indeed full of causal and temporal relations. Furthermore, we show that the CaTeRS annotation scheme enables high inter-annotator agreement for broad-coverage event entity annotation and moderate agreement on semantic link annotation. 1 Introduction Understanding events and their relations in natural language has become increasingly important for various NLP tasks. Most notably, story understanding (Charniak, 1972; Winograd, 1972; Turner, 1994; Schubert and Hwang, 2000) which is an extremely challenging task in natural language understanding, is highly dependent on understanding events and their relations. Recently, we have witnessed a renewed interest in story and narrative understanding based on the progress made in core NLP tasks. Perhaps the biggest challenge of story understanding (and story generation) is having commonsense knowledge for the interpretation of narrative events. This commonsense knowledge can be best represented as scripts. Scripts present structured knowledge about stereotypical event sequences t"
W16-1007,miltsakaki-etal-2004-penn,0,\N,Missing
W16-1007,prasad-etal-2008-penn,0,\N,Missing
W16-2505,marelli-etal-2014-sick,0,0.0191622,"Missing"
W16-2505,N16-1098,1,0.795855,"Missing"
W16-2505,D15-1036,0,0.0222761,"search in the past few years. While one could evaluate a given vector representation (embedding) on various down-stream applications, it is time-consuming at both implementation and runtime, which gives rise to focusing on an intrinsic evaluation. The intrinsic evaluation has been mostly focused on textual similarity where the task is to predict how semantically similar two words/sentences are, which is evaluated against the gold human similarity scores. It has been shown that semantic similarity tasks do not accurately measure the effectiveness of an embedding in the other down-stream tasks (Schnabel et al., 2015; Tsvetkov et al., 2015). Furthermore, human annotation of similarity at sentencelevel without any underlying context can be subjective, resulting in lower inter-annotator agreement and hence a less reliable evaluation method. 1 Examples of this include the semantic relatedness (SICK) dataset (Marelli et al., 2014), where given two sentences, the task is to produce a score of how semantically related these sentences are 24 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 24–29, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Lingu"
W16-2505,D15-1243,0,0.0454412,"years. While one could evaluate a given vector representation (embedding) on various down-stream applications, it is time-consuming at both implementation and runtime, which gives rise to focusing on an intrinsic evaluation. The intrinsic evaluation has been mostly focused on textual similarity where the task is to predict how semantically similar two words/sentences are, which is evaluated against the gold human similarity scores. It has been shown that semantic similarity tasks do not accurately measure the effectiveness of an embedding in the other down-stream tasks (Schnabel et al., 2015; Tsvetkov et al., 2015). Furthermore, human annotation of similarity at sentencelevel without any underlying context can be subjective, resulting in lower inter-annotator agreement and hence a less reliable evaluation method. 1 Examples of this include the semantic relatedness (SICK) dataset (Marelli et al., 2014), where given two sentences, the task is to produce a score of how semantically related these sentences are 24 Proceedings of the 1st Workshop on Evaluating Vector Space Representations for NLP, pages 24–29, c Berlin, Germany, August 12, 2016. 2016 Association for Computational Linguistics cs.rochester.edu/"
W16-2505,D13-1020,0,\N,Missing
