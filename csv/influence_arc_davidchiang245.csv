2020.conll-1.41,P99-1070,0,0.373994,"ak generative capacity of PDAs (Autebert et al., 1997), it could affect their ability to learn; we leave exploration of nonscanning transitions for future work. a a q, x → − r, y a q, x → − r,  x ∈ Γ, a ∈ Σ a q1 , a → − q2 ,  a∈Σ a a ∈ Σ. q2 , a → − q2 ,  This PDA has a possible configuration with an empty stack (⊥) iff the input string read so far is of the form wwR . To make a weighted PDA probabilistic, we require that all transition weights be nonnegative and, for all a, q, x: XX a δ(q, x → − r, y) = 1. r∈Q y∈Γ∗ replace x with y Whereas many definitions make the model generate symbols (Abney et al., 1999), our definition makes the PDA operations conditional on the input symbol a. The difference is not very important, because the RNN controller will eventually assume responsibility for reading and writing symbols, but our definition makes the shift to an RNN controller below slightly simpler. pop x. 3.3 For simplicity, we will assume that all transitions have one of the three forms: q, x → − r, xy a q1 , x → − q1 , xa A weighted pushdown automaton (PDA) is a tuple M = (Q, Σ, Γ, δ, q0 , ⊥), where: push y on top of x This also does not reduce the weak generative capacity of PDAs. Given an input s"
2020.conll-1.41,P81-1022,0,0.611084,"Missing"
2021.iwslt-1.33,N19-1311,1,0.891721,"Missing"
2021.iwslt-1.33,P17-2090,0,0.0665327,"Missing"
2021.iwslt-1.33,P19-1555,0,0.0407118,"Missing"
2021.iwslt-1.33,W19-5321,0,0.0528972,"ntext is unlikely the cause for concatenation improving BLEU by about +1 across four language pairs. Instead, we demonstrate that the improvement comes from three other factors unrelated to discourse: context diversity, length diversity, and (to a lesser extent) position shifting. 1 David Chiang University of Notre Dame dchiang@nd.edu Introduction • Random concatenation creates greater diversity of positions, because it lets the model see sentences shifted by effectively random distances. Many attempts have been made to augment neural machine translation (MT) systems to use discourse context (Junczys-Dowmunt, 2019; Stojanovski and Fraser, 2019; Saunders et al., 2020; Zhang et al., 2018; Sun et al., 2020; L¨aubli et al., 2018; Kim et al., 2019; Tan et al., 2019; Zheng et al., 2020; Jean et al., 2017). One particularly simple method is to concatenate consecutive pairs of sentencepairs during training, but not during translation (Agrawal et al., 2018; Tiedemann and Scherrer, 2017; Ngo and Trinh, 2021; Kondo et al., 2021).1 In this paper, we confirm that this simple method helps, by roughly +1 BLEU across four lowresource language pairs. But we demonstrate that the reason it helps is not discourse context,"
2021.iwslt-1.33,N19-4009,0,0.0134516,"selfattentions so that, in each concatenated example, each sentence can only attend to itself and not the other sentence. Similarly, in cross-attention, each target sentence can only attend to its corresponding source sentence, not the other one. Table 5, row 2 shows that this masking removes a large part of the improvement due to concatenation, showing that the availability of negative contexts during training does help during translation. 3.4 Length diversity The last possible effect of concatenation that we consider is also the most subtle. Following previous work (Morishita et al., 2017; Ott et al., 2019), we first sort sentences by length, then splitting into minibatches of a fixed number of tokens. This puts sentences of similar lengths into the same minibatch, which improves computation efficiency as there is less padding. However, as observed by Morishita et al. (2017), short and long sentences are qualitatively different, so creating a minibatch of only short sentences or only long sentences approximates the full gradient less well than a minibatch of random sentences would. With random concatenation, we again put examples of similar lengths into the same minibatch, but each example may c"
2021.iwslt-1.33,D19-6503,0,0.0361152,"Missing"
2021.iwslt-1.33,W04-3250,0,0.340921,"okenized, we only apply BPE. Data statistics and hyper-parameters are summarized in Table 1. For baseline, the training data is Dorig . For concatenation, we first create Dnew , then combine it with Dorig to create the training data. Following Morishita et al. (2017), we randomly shuffle the training data and read it in chunks of 10k examples. Each chunk is sorted by source length before being packed into minibatches of roughly 4096 source/target tokens each. We calculate tokenized BLEU using multi-bleu.perl (Koehn et al., 2007) and measure statistical significance using bootstrap resampling (Koehn, 2004). As seen in Table 2, concatenation consistently outperforms the baseline across all datasets with significant improvement (p < 0.01) on almost every case. We observe that there is generally more 3.2 Position shifting Since the Transformer uses absolute positional encodings, if a word is observed only a few times, the model may have difficulty generalizing to occurrences in other positions. Moreover, if there are too few long sentences, the model may have difficulty translating words very far from the start of the sentence. In concatenation, the second sentence is shifted by a random distance"
2021.iwslt-1.33,N18-2084,0,0.0270247,"aining set), and measure BLEU on the concatenated dev set. The new BLEU scores are shown in Table 3, showing that even having discourse context available at translation time does not enable CONSEC to do better than RAND. While we acknowledge that there could be improvement due to discourse context that is not captured by BLEU, we can also say that the gain in BLEU that we do observe with concatenation is independent of the availability of discourse context. Initial experiments We experiment on four low-resource language pairs: {Galician, Slovak} to English and English to {Hebrew, Vietnamese} (Qi et al., 2018; Luong and Manning, 2015) using Transformer (Vaswani et al., 2017). We use the same setup as Nguyen and Salazar (2019), with PreNorm, FixNorm and ScaleNorm, as it has been shown to perform well on low-resource tasks. Since the data comes pretokenized, we only apply BPE. Data statistics and hyper-parameters are summarized in Table 1. For baseline, the training data is Dorig . For concatenation, we first create Dnew , then combine it with Dorig to create the training data. Following Morishita et al. (2017), we randomly shuffle the training data and read it in chunks of 10k examples. Each chunk"
2021.iwslt-1.33,P07-2045,0,0.0122149,"as it has been shown to perform well on low-resource tasks. Since the data comes pretokenized, we only apply BPE. Data statistics and hyper-parameters are summarized in Table 1. For baseline, the training data is Dorig . For concatenation, we first create Dnew , then combine it with Dorig to create the training data. Following Morishita et al. (2017), we randomly shuffle the training data and read it in chunks of 10k examples. Each chunk is sorted by source length before being packed into minibatches of roughly 4096 source/target tokens each. We calculate tokenized BLEU using multi-bleu.perl (Koehn et al., 2007) and measure statistical significance using bootstrap resampling (Koehn, 2004). As seen in Table 2, concatenation consistently outperforms the baseline across all datasets with significant improvement (p < 0.01) on almost every case. We observe that there is generally more 3.2 Position shifting Since the Transformer uses absolute positional encodings, if a word is observed only a few times, the model may have difficulty generalizing to occurrences in other positions. Moreover, if there are too few long sentences, the model may have difficulty translating words very far from the start of the se"
2021.iwslt-1.33,2020.acl-main.693,0,0.0256511,"ng BLEU by about +1 across four language pairs. Instead, we demonstrate that the improvement comes from three other factors unrelated to discourse: context diversity, length diversity, and (to a lesser extent) position shifting. 1 David Chiang University of Notre Dame dchiang@nd.edu Introduction • Random concatenation creates greater diversity of positions, because it lets the model see sentences shifted by effectively random distances. Many attempts have been made to augment neural machine translation (MT) systems to use discourse context (Junczys-Dowmunt, 2019; Stojanovski and Fraser, 2019; Saunders et al., 2020; Zhang et al., 2018; Sun et al., 2020; L¨aubli et al., 2018; Kim et al., 2019; Tan et al., 2019; Zheng et al., 2020; Jean et al., 2017). One particularly simple method is to concatenate consecutive pairs of sentencepairs during training, but not during translation (Agrawal et al., 2018; Tiedemann and Scherrer, 2017; Ngo and Trinh, 2021; Kondo et al., 2021).1 In this paper, we confirm that this simple method helps, by roughly +1 BLEU across four lowresource language pairs. But we demonstrate that the reason it helps is not discourse context, because concatenating random pairs of sentence-pairs"
2021.iwslt-1.33,W19-5345,0,0.0170991,"ause for concatenation improving BLEU by about +1 across four language pairs. Instead, we demonstrate that the improvement comes from three other factors unrelated to discourse: context diversity, length diversity, and (to a lesser extent) position shifting. 1 David Chiang University of Notre Dame dchiang@nd.edu Introduction • Random concatenation creates greater diversity of positions, because it lets the model see sentences shifted by effectively random distances. Many attempts have been made to augment neural machine translation (MT) systems to use discourse context (Junczys-Dowmunt, 2019; Stojanovski and Fraser, 2019; Saunders et al., 2020; Zhang et al., 2018; Sun et al., 2020; L¨aubli et al., 2018; Kim et al., 2019; Tan et al., 2019; Zheng et al., 2020; Jean et al., 2017). One particularly simple method is to concatenate consecutive pairs of sentencepairs during training, but not during translation (Agrawal et al., 2018; Tiedemann and Scherrer, 2017; Ngo and Trinh, 2021; Kondo et al., 2021).1 In this paper, we confirm that this simple method helps, by roughly +1 BLEU across four lowresource language pairs. But we demonstrate that the reason it helps is not discourse context, because concatenating random"
2021.iwslt-1.33,2021.naacl-srw.18,0,0.0326483,"ecause it lets the model see sentences shifted by effectively random distances. Many attempts have been made to augment neural machine translation (MT) systems to use discourse context (Junczys-Dowmunt, 2019; Stojanovski and Fraser, 2019; Saunders et al., 2020; Zhang et al., 2018; Sun et al., 2020; L¨aubli et al., 2018; Kim et al., 2019; Tan et al., 2019; Zheng et al., 2020; Jean et al., 2017). One particularly simple method is to concatenate consecutive pairs of sentencepairs during training, but not during translation (Agrawal et al., 2018; Tiedemann and Scherrer, 2017; Ngo and Trinh, 2021; Kondo et al., 2021).1 In this paper, we confirm that this simple method helps, by roughly +1 BLEU across four lowresource language pairs. But we demonstrate that the reason it helps is not discourse context, because concatenating random pairs of sentence-pairs yields the same improvement. Instead, we view concatenation as a kind of data augmentation or noising method (one which pleasantly requires no alteration to the text, unlike data augmentation methods that disturb word order • Random concatenation creates greater diversity of contexts, helping the model learn what not to attend to. • Random concatenation cr"
2021.iwslt-1.33,D18-1512,0,0.0564805,"Missing"
2021.iwslt-1.33,D19-1168,0,0.0231449,"om three other factors unrelated to discourse: context diversity, length diversity, and (to a lesser extent) position shifting. 1 David Chiang University of Notre Dame dchiang@nd.edu Introduction • Random concatenation creates greater diversity of positions, because it lets the model see sentences shifted by effectively random distances. Many attempts have been made to augment neural machine translation (MT) systems to use discourse context (Junczys-Dowmunt, 2019; Stojanovski and Fraser, 2019; Saunders et al., 2020; Zhang et al., 2018; Sun et al., 2020; L¨aubli et al., 2018; Kim et al., 2019; Tan et al., 2019; Zheng et al., 2020; Jean et al., 2017). One particularly simple method is to concatenate consecutive pairs of sentencepairs during training, but not during translation (Agrawal et al., 2018; Tiedemann and Scherrer, 2017; Ngo and Trinh, 2021; Kondo et al., 2021).1 In this paper, we confirm that this simple method helps, by roughly +1 BLEU across four lowresource language pairs. But we demonstrate that the reason it helps is not discourse context, because concatenating random pairs of sentence-pairs yields the same improvement. Instead, we view concatenation as a kind of data augmentation or n"
2021.iwslt-1.33,2015.iwslt-evaluation.11,0,0.0426092,"measure BLEU on the concatenated dev set. The new BLEU scores are shown in Table 3, showing that even having discourse context available at translation time does not enable CONSEC to do better than RAND. While we acknowledge that there could be improvement due to discourse context that is not captured by BLEU, we can also say that the gain in BLEU that we do observe with concatenation is independent of the availability of discourse context. Initial experiments We experiment on four low-resource language pairs: {Galician, Slovak} to English and English to {Hebrew, Vietnamese} (Qi et al., 2018; Luong and Manning, 2015) using Transformer (Vaswani et al., 2017). We use the same setup as Nguyen and Salazar (2019), with PreNorm, FixNorm and ScaleNorm, as it has been shown to perform well on low-resource tasks. Since the data comes pretokenized, we only apply BPE. Data statistics and hyper-parameters are summarized in Table 1. For baseline, the training data is Dorig . For concatenation, we first create Dnew , then combine it with Dorig to create the training data. Following Morishita et al. (2017), we randomly shuffle the training data and read it in chunks of 10k examples. Each chunk is sorted by source length"
2021.iwslt-1.33,W17-3208,0,0.155171,"r low-resource language pairs: {Galician, Slovak} to English and English to {Hebrew, Vietnamese} (Qi et al., 2018; Luong and Manning, 2015) using Transformer (Vaswani et al., 2017). We use the same setup as Nguyen and Salazar (2019), with PreNorm, FixNorm and ScaleNorm, as it has been shown to perform well on low-resource tasks. Since the data comes pretokenized, we only apply BPE. Data statistics and hyper-parameters are summarized in Table 1. For baseline, the training data is Dorig . For concatenation, we first create Dnew , then combine it with Dorig to create the training data. Following Morishita et al. (2017), we randomly shuffle the training data and read it in chunks of 10k examples. Each chunk is sorted by source length before being packed into minibatches of roughly 4096 source/target tokens each. We calculate tokenized BLEU using multi-bleu.perl (Koehn et al., 2007) and measure statistical significance using bootstrap resampling (Koehn, 2004). As seen in Table 2, concatenation consistently outperforms the baseline across all datasets with significant improvement (p < 0.01) on almost every case. We observe that there is generally more 3.2 Position shifting Since the Transformer uses absolute p"
2021.iwslt-1.33,W17-4811,0,0.0194767,"atenation creates greater diversity of positions, because it lets the model see sentences shifted by effectively random distances. Many attempts have been made to augment neural machine translation (MT) systems to use discourse context (Junczys-Dowmunt, 2019; Stojanovski and Fraser, 2019; Saunders et al., 2020; Zhang et al., 2018; Sun et al., 2020; L¨aubli et al., 2018; Kim et al., 2019; Tan et al., 2019; Zheng et al., 2020; Jean et al., 2017). One particularly simple method is to concatenate consecutive pairs of sentencepairs during training, but not during translation (Agrawal et al., 2018; Tiedemann and Scherrer, 2017; Ngo and Trinh, 2021; Kondo et al., 2021).1 In this paper, we confirm that this simple method helps, by roughly +1 BLEU across four lowresource language pairs. But we demonstrate that the reason it helps is not discourse context, because concatenating random pairs of sentence-pairs yields the same improvement. Instead, we view concatenation as a kind of data augmentation or noising method (one which pleasantly requires no alteration to the text, unlike data augmentation methods that disturb word order • Random concatenation creates greater diversity of contexts, helping the model learn what n"
2021.iwslt-1.33,D18-1100,0,0.042067,"Missing"
2021.iwslt-1.33,D18-1049,0,0.048133,"Missing"
2021.naacl-srw.7,2020.acl-main.449,0,0.0185449,"chine translation (Vaswani et al., 2017) was originally defined as a mapping from sequences to sequences. More recent work has explored extensions of transformers to other structures: a tree transformer would be able to make use of syntactic information, and a graph transformer would be able to make use of semantic graphs or knowledge graphs. There have been a number of proposals for transformers on trees, including phrase-structure trees and dependency trees for natural languages, and abstract syntax trees for programming languages. One common strategy is to linearize a tree into a sequence (Ahmad et al., 2020; Currey and Heafield, 2019). Another strategy is to recognize that transformers are fundamentally defined not on sequences but on bags; all information about sequential order is contained in the positional encodings, so all that is needed to construct a tree transformer is to define new positional encodings on trees (Shiv and Quirk, 2019; Omote et al., 2019). In this paper, we present a third approach, which is to enhance the encoder’s self-attention mechanism with attention masks (Shen et al., 2018), which restrict the possible positions an attention head can attend to. We extend this idea i"
2021.naacl-srw.7,W19-5203,0,0.0161174,"aswani et al., 2017) was originally defined as a mapping from sequences to sequences. More recent work has explored extensions of transformers to other structures: a tree transformer would be able to make use of syntactic information, and a graph transformer would be able to make use of semantic graphs or knowledge graphs. There have been a number of proposals for transformers on trees, including phrase-structure trees and dependency trees for natural languages, and abstract syntax trees for programming languages. One common strategy is to linearize a tree into a sequence (Ahmad et al., 2020; Currey and Heafield, 2019). Another strategy is to recognize that transformers are fundamentally defined not on sequences but on bags; all information about sequential order is contained in the positional encodings, so all that is needed to construct a tree transformer is to define new positional encodings on trees (Shiv and Quirk, 2019; Omote et al., 2019). In this paper, we present a third approach, which is to enhance the encoder’s self-attention mechanism with attention masks (Shen et al., 2018), which restrict the possible positions an attention head can attend to. We extend this idea in two new 2 Methods Like sev"
2021.naacl-srw.7,P19-1340,0,0.0217593,"ranslation-task.html 2 48 S VP NP NP PRP VBZ He S is PRP$ NN my (a) father . NP . PRP VBZ is He . VP . NP PRP$ NN my fath@@ er (b) anc left-other right-other parent left-other right-other self left-sib child left-other left-other child desc desc desc (c) Figure 1: (a) Example tree. (b) With BPE. (c) Relationships of all nodes to the second NP (dominating my father). et al., 2016). For en-vi, en-tu, en-ha, and en-ur, we used 8k joint BPE operations, and for en-de and de-en, we used 32k operations. To parse English or German sentences, we used the Berkeley Neural Parser (Kitaev and Klein, 2018; Kitaev et al., 2019) with the included benepar_en2 model for English and benepar_de for German. The parser reads in untokenized strings and writes out tokenized trees; we used the parser’s tokenization, but applied BPE to the leaves, as shown in Figure 1b. 3.2 We score detokenized system outputs using casesensitive BLEU against raw references (except on en-vi, where we use tokenized outputs and references), using bootstrap resampling (Koehn, 2004; Zhang et al., 2004) for significance testing. 3.3 The results are shown in Table 2. Relative to the linearized baseline, our method (mask) always improves, by up to +1."
2021.naacl-srw.7,P18-1249,0,0.0196599,"/www.statmt.org/wmt16/ translation-task.html 2 48 S VP NP NP PRP VBZ He S is PRP$ NN my (a) father . NP . PRP VBZ is He . VP . NP PRP$ NN my fath@@ er (b) anc left-other right-other parent left-other right-other self left-sib child left-other left-other child desc desc desc (c) Figure 1: (a) Example tree. (b) With BPE. (c) Relationships of all nodes to the second NP (dominating my father). et al., 2016). For en-vi, en-tu, en-ha, and en-ur, we used 8k joint BPE operations, and for en-de and de-en, we used 32k operations. To parse English or German sentences, we used the Berkeley Neural Parser (Kitaev and Klein, 2018; Kitaev et al., 2019) with the included benepar_en2 model for English and benepar_de for German. The parser reads in untokenized strings and writes out tokenized trees; we used the parser’s tokenization, but applied BPE to the leaves, as shown in Figure 1b. 3.2 We score detokenized system outputs using casesensitive BLEU against raw references (except on en-vi, where we use tokenized outputs and references), using bootstrap resampling (Koehn, 2004; Zhang et al., 2004) for significance testing. 3.3 The results are shown in Table 2. Relative to the linearized baseline, our method (mask) always"
2021.naacl-srw.7,W04-3250,0,0.0536968,"E operations, and for en-de and de-en, we used 32k operations. To parse English or German sentences, we used the Berkeley Neural Parser (Kitaev and Klein, 2018; Kitaev et al., 2019) with the included benepar_en2 model for English and benepar_de for German. The parser reads in untokenized strings and writes out tokenized trees; we used the parser’s tokenization, but applied BPE to the leaves, as shown in Figure 1b. 3.2 We score detokenized system outputs using casesensitive BLEU against raw references (except on en-vi, where we use tokenized outputs and references), using bootstrap resampling (Koehn, 2004; Zhang et al., 2004) for significance testing. 3.3 The results are shown in Table 2. Relative to the linearized baseline, our method (mask) always improves, by up to +1.7 BLEU for English–Turkish. The difference is statistically significant (p < 0.05) except for English–Urdu. Relative to the sequence baseline, the story is more complex. Whenever linearized helps over sequence, our method helps more, up to a total of +2.1 BLEU for German↔English (50k). But when linearized hurts, our method sometimes helps overall (all tasks with 20k lines of training) and sometimes doesn’t (e.g., English–Urdu,"
2021.naacl-srw.7,R19-1099,0,0.0215919,"a number of proposals for transformers on trees, including phrase-structure trees and dependency trees for natural languages, and abstract syntax trees for programming languages. One common strategy is to linearize a tree into a sequence (Ahmad et al., 2020; Currey and Heafield, 2019). Another strategy is to recognize that transformers are fundamentally defined not on sequences but on bags; all information about sequential order is contained in the positional encodings, so all that is needed to construct a tree transformer is to define new positional encodings on trees (Shiv and Quirk, 2019; Omote et al., 2019). In this paper, we present a third approach, which is to enhance the encoder’s self-attention mechanism with attention masks (Shen et al., 2018), which restrict the possible positions an attention head can attend to. We extend this idea in two new 2 Methods Like several previous approaches, we use linearized syntax trees. But whereas the usual linearization traverses a node both before and after its descendants, we use a preorder traversal of the tree. In other words, our linearization does not have closing brackets. Our linearization does not have enough information to reconstruct the origin"
2021.naacl-srw.7,P16-1162,0,0.127699,"Missing"
2021.naacl-srw.7,zhang-etal-2004-interpreting,0,0.102169,"and for en-de and de-en, we used 32k operations. To parse English or German sentences, we used the Berkeley Neural Parser (Kitaev and Klein, 2018; Kitaev et al., 2019) with the included benepar_en2 model for English and benepar_de for German. The parser reads in untokenized strings and writes out tokenized trees; we used the parser’s tokenization, but applied BPE to the leaves, as shown in Figure 1b. 3.2 We score detokenized system outputs using casesensitive BLEU against raw references (except on en-vi, where we use tokenized outputs and references), using bootstrap resampling (Koehn, 2004; Zhang et al., 2004) for significance testing. 3.3 The results are shown in Table 2. Relative to the linearized baseline, our method (mask) always improves, by up to +1.7 BLEU for English–Turkish. The difference is statistically significant (p < 0.05) except for English–Urdu. Relative to the sequence baseline, the story is more complex. Whenever linearized helps over sequence, our method helps more, up to a total of +2.1 BLEU for German↔English (50k). But when linearized hurts, our method sometimes helps overall (all tasks with 20k lines of training) and sometimes doesn’t (e.g., English–Urdu, with only 11k lines"
C02-1126,W00-1201,1,0.84302,"he leftmost child is chosen; for NP, the rightmost child is chosen. The argument rules, however, were not changed. This rule set is supposed to represent the kind of rule set that someone with basic familiarity with English syntax might write down in a few minutes. The reestimated models seemed to improve on this simplified rule set when parsing section 00 (see Figure 5); however, when we compared the 30th reestimated model with the initial model on section 23 (see Figure 7), there was no improvement. The third experiment was on the Chinese Treebank, starting with the same head rules used in (Bikel and Chiang, 2000). These rules were originally written by Xia for grammar development, and although we have modified them for parsing, they have not received as much fine-tuning as the English 4.4 Discussion Our hypothesis that reestimation does not improve on the original rule set for English because that rule set is already fine-tuned was partially borne out by the second and third experiments. The model trained with a simplified rule set for English showed improvement on held-out data during reestimation, but showed no improvement in the final evaluation; however, the model trained on Chinese did show a sma"
C02-1126,A97-1029,1,0.465635,"s that are combined by deleted interpolation. Let φ1 , φ2 and φ3 be functions from full history contexts Y to less specific contexts at levels 1, 2 and 3, respectively, for some parameter class with three backoff levels (with level 1 using the most specific contexts). Smoothed estimates for parameters in this class are computed as follows: 87.5 87.45 87.4 87.35 87.3 0 5 10 15 20 Iteration Figure 4: English, starting with full rule set e = λ1 e1 + (1 − λ1 )(λ2 e2 + (1 − λ2 )e3 ) where ei is the estimate of p(X |φi (Y)) for some future context X, and the λi are computed by the formula found in (Bikel et al., 1997), modified to use the multiplicative constant 5 found in the similar formula of (Collins, 1999): ! ! di−1 1 λi = 1 − (7) di 1 + 5ui /di likelihood of the training data. Second, reestimation tends to increase the size of the model in memory, since smoothing gives nonzero expected counts to many events which were unseen in training. Therefore, since the resulting model is quite large, if an event at a particular point in the derivation forest has an expected count below 10−15 , we throw it out. where di is the number of occurrences in training of the context φi (Y) (and d0 = 0), and ui is the nu"
C02-1126,A00-2018,0,0.052889,"parsing, only to strip out this hidden information prior to evaluation. Also, in all these parsing efforts lexicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents. In fact, nearly identical head-lexicalizations were used in the disS(caught–VBD) NP(boy–NN) ADVP(also–RB) VP(caught–VBD) DET NN RB VBD The boy also caught NP(ball–NN) DET NN the ball Figure 2: A simple lexicalized parse tree. criminative models described in (Magerman, 1995; Ratnaparkhi, 1997), the lexicalized PCFG models in (Collins, 1999), the generative model in (Charniak, 2000), the lexicalized TAG extractor in (Xia, 1999) and the stochastic lexicalized TAG models in (Chiang, 2000; Sarkar, 2001; Chen and VijayShanker, 2000). Inducing a lexicalized structure based on heads has a two-pronged effect: it not only allows statistical parsers to be sensitive to lexical information by including this information in the probability model’s dependencies, but it also determines which of all possible dependencies— both syntactic and lexical—will be included in the model itself. For example, in Figure 2, the nonterminal NP(boy–NN) is dependent on VP(caught–VBD) and not the other"
C02-1126,2000.iwpt-1.9,0,0.0216869,"arguments is performed as one of many preprocessing steps on training trees, using a set of rules similar to those used for the identification of heads. Also, (Collins, 1999) makes use of several other transformations, such as the identification of subjectless sentences (augmenting S nodes to become SG) and the augmentation of nonterminals for gap threading. Xia (1999) combines head-finding with argument identification to extract elementary trees for use in the lexicalized TAG formalism. Other researchers investigated this type of extraction to construct stochastic TAG parsers (Chiang, 2000; Chen and Vijay-Shanker, 2000; Sarkar, 2001). 2.3 Problems with heuristics While head-lexicalization and other tree transformations allow the construction of parsing models with more data-sensitivity and richer representations, crafting rules for these transformations has been largely an art, with heuristics handed down from researcher to researcher. What’s more, on top of the large undertaking of designing and implementing a statistical parsing model, the use of heuristics has required a further effort, forcing the researcher to bring both linguistic intuition and, more often, engineering savvy to bear whenever moving to"
C02-1126,P00-1058,1,0.783673,"exicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents. In fact, nearly identical head-lexicalizations were used in the disS(caught–VBD) NP(boy–NN) ADVP(also–RB) VP(caught–VBD) DET NN RB VBD The boy also caught NP(ball–NN) DET NN the ball Figure 2: A simple lexicalized parse tree. criminative models described in (Magerman, 1995; Ratnaparkhi, 1997), the lexicalized PCFG models in (Collins, 1999), the generative model in (Charniak, 2000), the lexicalized TAG extractor in (Xia, 1999) and the stochastic lexicalized TAG models in (Chiang, 2000; Sarkar, 2001; Chen and VijayShanker, 2000). Inducing a lexicalized structure based on heads has a two-pronged effect: it not only allows statistical parsers to be sensitive to lexical information by including this information in the probability model’s dependencies, but it also determines which of all possible dependencies— both syntactic and lexical—will be included in the model itself. For example, in Figure 2, the nonterminal NP(boy–NN) is dependent on VP(caught–VBD) and not the other way around. 2.2 Other tree transformations Lexicalization via head-finding is but one of many possible tr"
C02-1126,P98-1091,0,0.0110723,"a wide variety of node-label augmentations, including all those that Training Decoding parsed data Model annotated data + parsed data + Figure 1: Methodology for the development of a statistical parser. A + indicates augmentation. are performed by existing statistical parsers that we have examined. Second, we explore a novel use of Expectation-Maximization (Dempster et al., 1977) that iteratively reestimates a parsing model using the augmenting heuristics as a starting point. Specifically, the EM algorithm we use is a variant of the Inside-Outside algorithm (Baker, 1979; Lari and Young, 1990; Hwa, 1998). The reestimation adjusts the model’s parameters in the augmented parse-tree space to maximize the likelihood of the observed (incomplete) data, in the hopes of finding a better distribution over augmented parse trees (the complete data). The ultimate goal of this work is to minimize the human effort needed when adapting a parsing model to a new domain. 2 Background 2.1 Head-lexicalization Many of the recent, successful statistical parsers have made use of lexical information or an implicit lexicalized grammar, both for English and, more recently, for other languages. All of these parsers rec"
C02-1126,J98-4004,0,0.741707,"y to reduce the amount of human effort needed to adapt existing models to new corpora: first, we propose a flexible notation for specifying these rules that would allow them to be shared by different models; second, we report on an experiment to see whether we can use ExpectationMaximization to automatically fine-tune a set of hand-written rules to a particular corpus. 1 Introduction Most work in statistical parsing does not operate in the realm of parse trees as they appear in many treebanks, but rather on trees transformed via augmentation of their node labels, or some other transformation (Johnson, 1998). This methodology is illustrated in Figure 1. The information included in the node labels’ augmentations may include lexical items, or a node label suffix to indicate the node is an argument and not an adjunct; such extra information may be viewed as latent information, in that it is not directly present in the treebank parse trees, but may be recovered by some means. The process of recovering this latent information has largely been limited to the hand-construction of heuristics. However, as is often the case, hand-constructed heuristics may not be optimal or very robust. Also, the effort re"
C02-1126,P95-1037,0,0.752177,"parsers recover the “hidden” lexicalizations in a treebank and find the most probable lexicalized tree when parsing, only to strip out this hidden information prior to evaluation. Also, in all these parsing efforts lexicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents. In fact, nearly identical head-lexicalizations were used in the disS(caught–VBD) NP(boy–NN) ADVP(also–RB) VP(caught–VBD) DET NN RB VBD The boy also caught NP(ball–NN) DET NN the ball Figure 2: A simple lexicalized parse tree. criminative models described in (Magerman, 1995; Ratnaparkhi, 1997), the lexicalized PCFG models in (Collins, 1999), the generative model in (Charniak, 2000), the lexicalized TAG extractor in (Xia, 1999) and the stochastic lexicalized TAG models in (Chiang, 2000; Sarkar, 2001; Chen and VijayShanker, 2000). Inducing a lexicalized structure based on heads has a two-pronged effect: it not only allows statistical parsers to be sensitive to lexical information by including this information in the probability model’s dependencies, but it also determines which of all possible dependencies— both syntactic and lexical—will be included in the model"
C02-1126,J93-2004,0,0.0401962,"er MD, and symbols to the left over symbols to the right, but in rule (5) the leftmost preference (that is, the preference of the last Kleene star to match as many times as possible) takes priority, whereas in rule (6) the preference for VB takes priority. 3.2 Label patterns Since nearly all treebanks have complex nonterminal alphabets, we need a way of concisely specifying classes of labels. Unfortunately, this will necessarily vary somewhat across treebanks: all we can define that is truly treebank-independent is the ⊥ pattern, which matches any label. For Penn Treebank II style annotation (Marcus et al., 1993), in which a nonterminal symbol is a category together with zero or more functional tags, we adopt the following scheme: the atomic pattern a matches any label with category a or functional tag a; moreover, we define Boolean operators ∧, ∨, and ¬. Thus NP ∧ ¬ADV matches NP–SBJ but not NP–ADV. 1 3.3 Summary Using the structure pattern language and the label pattern language together, one can fully encode the head/argument rules used by Xia (which resemble (5) above), and the family of rule sets used by Black, Magerman, Collins, Ratnaparkhi, and others (which resemble (6) above). In Collins’ ver"
C02-1126,P95-1021,0,0.0360202,"Missing"
C02-1126,W97-0301,0,0.0805871,"the “hidden” lexicalizations in a treebank and find the most probable lexicalized tree when parsing, only to strip out this hidden information prior to evaluation. Also, in all these parsing efforts lexicalization has meant finding heads of constituents and then propagating those lexical heads to their respective parents. In fact, nearly identical head-lexicalizations were used in the disS(caught–VBD) NP(boy–NN) ADVP(also–RB) VP(caught–VBD) DET NN RB VBD The boy also caught NP(ball–NN) DET NN the ball Figure 2: A simple lexicalized parse tree. criminative models described in (Magerman, 1995; Ratnaparkhi, 1997), the lexicalized PCFG models in (Collins, 1999), the generative model in (Charniak, 2000), the lexicalized TAG extractor in (Xia, 1999) and the stochastic lexicalized TAG models in (Chiang, 2000; Sarkar, 2001; Chen and VijayShanker, 2000). Inducing a lexicalized structure based on heads has a two-pronged effect: it not only allows statistical parsers to be sensitive to lexical information by including this information in the probability model’s dependencies, but it also determines which of all possible dependencies— both syntactic and lexical—will be included in the model itself. For example,"
C02-1126,N01-1023,0,0.0440398,"has meant finding heads of constituents and then propagating those lexical heads to their respective parents. In fact, nearly identical head-lexicalizations were used in the disS(caught–VBD) NP(boy–NN) ADVP(also–RB) VP(caught–VBD) DET NN RB VBD The boy also caught NP(ball–NN) DET NN the ball Figure 2: A simple lexicalized parse tree. criminative models described in (Magerman, 1995; Ratnaparkhi, 1997), the lexicalized PCFG models in (Collins, 1999), the generative model in (Charniak, 2000), the lexicalized TAG extractor in (Xia, 1999) and the stochastic lexicalized TAG models in (Chiang, 2000; Sarkar, 2001; Chen and VijayShanker, 2000). Inducing a lexicalized structure based on heads has a two-pronged effect: it not only allows statistical parsers to be sensitive to lexical information by including this information in the probability model’s dependencies, but it also determines which of all possible dependencies— both syntactic and lexical—will be included in the model itself. For example, in Figure 2, the nonterminal NP(boy–NN) is dependent on VP(caught–VBD) and not the other way around. 2.2 Other tree transformations Lexicalization via head-finding is but one of many possible tree transformat"
C02-1126,J95-4002,0,0.04605,"ed training data, can then be used in two ways: one might hope that as a parser, it would parse more accurately than a model which only maximizes the likelihood of training data augmented by hand-written rules; and that as a treeaugmenter, it would augment trees in a more datasensitive way than hand-written rules. 4.1 Background: tree adjoining grammar The parsing model we use is based on the stochastic tree-insertion grammar (TIG) model described 1 Note that unlike the noncommutative union operator ∪, the disjunction operator ∨ has no preference for its first argument. by Chiang (2000). TIG (Schabes and Waters, 1995) is a weakly-context free restriction of tree adjoining grammar (Joshi and Schabes, 1997), in which tree fragments called elementary trees are combined by two composition operations, substitution and adjunction (see Figure 3). In TIG there are certain restrictions on the adjunction operation. Chiang’s model adds a third composition operation called sister-adjunction (see Figure 3), borrowed from D-tree substitution grammar (Rambow et al., 1995).2 There is an important distinction between derived trees and derivation trees (see Figure 3). A derivation tree records the operations that are used t"
C02-1126,J03-4003,0,\N,Missing
C02-1126,C98-1088,0,\N,Missing
C08-1136,J04-4002,0,0.779057,"put is a permutation, but in machine translation it is common to work with word-level alignments that are many-to-many; in general any set of pairs of words, one from each language, is a valid alignment for a given bilingual sentence pair. In this paper, we consider a generalized concept of common intervals given such an alignment: a common interval is a pair of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phrase. Extraction of such phrases is a common feature of state-of-the-art phrase-based and syntax-based machine translation systems (Och and Ney, 2004a; Chiang, 2005). We generalize Uno and Yagiura’s algorithm to this setting, and demonstrate a linear time algorithm for a pair of aligned sequences. The output is a tree representation of possible phrases, which directly provides a set of minimal synchronous grammar 1081 Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008), pages 1081–1088 Manchester, August 2008 rules for an SCFG-based machine translation system. For phrase-based machine translation, one can also read all phrase pairs consistent with the original alignment off of the tree in time linear"
C08-1136,H05-1101,0,0.0302716,"he algorithm of the previous section outputs the normalized decomposition tree depicted in Figure 2. From this tree, it is straightforward to obtain 3 It can be shown further that in this region, f shifts up or is unchanged. Therefore any reductions in step (4) must be in region (a). C → e4 F (1) e6 , f1 F (1) f3 E → e3 , f4 F → e5 , f2 G → e1 , f6 Figure 5: Each node from the normalized decomposition tree of Figure 2 is converted into an SCFG rule. a set of maximally-decomposed SCFG rules. As an example, the tree of Figure 2 produces the rules shown in Figure 5. We adopt the SCFG notation of Satta and Peserico (2005). Each rule has a right-hand side sequence for both languages, separated by a comma. Superscript indices in the right-hand side of grammar rules such as: A → B (1) C (2) , C (2) B (1) indicate that the nonterminals with the same index are linked across the two languages, and will eventually be rewritten by the same rule application. The example above inverts the order of B and C when translating from the source language to the target language. The SCFG rule extraction proceeds as follows. Assign a nonterminal label to each node in the tree. Then for each node (S, T ) in the tree top-down, wher"
C08-1136,P06-1123,0,0.103616,"all the children of the nodes in the chain. Then, each of the subsequences {ηi , . . . , ηj |1 < i < j ≤ k} yields a valid phrase pair. In our example, the root of the tree of Figure 2 and its left child form such a chain, with three children; the subsequence {([3, 3], [4, 4]), ([4, 6], [1, 3])} yields the phrase ([3, 6], [1, 4]). In the case of unaligned words, we can also consider all combinations of their attachments, as discussed for SCFG rule extraction. 5 Experiments on Analyzing Word Alignments One application of our factorization algorithm is analyzing human-annotated word alignments. Wellington et al. (2006) argue for the necessity of discontinuous spans (i.e., for a formalism beyond Synchronous CFG) in order for synchronous parsing to cover human-annotated word alignment data under the constraint that rules have a rank of no more than two. In a related study, Zhang and Gildea (2007) analyze the rank of the Synchronous CFG derivation trees needed to parse the same data. The number of discontinuous spans and the rank determine the complexity of dynamic programming algorithms for synchronous parsing (alignment) or machine translation decoding. Both studies make simplifying assumptions on the alignm"
C08-1136,W07-0404,1,0.887063,"on interval is a set of numbers that are consecutive in both. The breakthrough algorithm of Uno and Yagiura (2000) computes all K common intervals of two length n permutations in O(n + K) time. This is achieved by designing data structures to index possible boundaries of common intervals as the computation proceeds, so that not all possible pairs of beginning and end points need to be considered. Landau et al. (2005) and Bui-Xuan et al. (2005) show that all common intervals can be encoded in O(n) space, and adapt Uno and Yagiura’s algorithm to produce this compact representation in O(n) time. Zhang and Gildea (2007) use similar techniques to factorize Synchronous Context Free Grammars in linear time. These previous algorithms assume that the input is a permutation, but in machine translation it is common to work with word-level alignments that are many-to-many; in general any set of pairs of words, one from each language, is a valid alignment for a given bilingual sentence pair. In this paper, we consider a generalized concept of common intervals given such an alignment: a common interval is a pair of phrases such that no word pair in the alignment links a word inside the phrase to a word outside the phr"
C08-1136,J93-2003,0,0.0412268,"Alignments and Phrase Pairs Let [x, y] denote the sequence of integers between x and y inclusive, and [x, y) the integers between x and y − 1 inclusive. An aligned sequence pair or simply an alignment is a tuple (E, F, A), where E = e1 · · · en and F = f1 · · · fm are strings, and A is a set of links (x, y), where 1 ≤ x ≤ n and 1 ≤ y ≤ m, connecting E and F . For most of this paper, since we are not concerned with the identity of the symbols in E and F , we will assume for simplicity that ei = i and fj = j, so that E = [1, n] and F = [1, m]. In the context of statistical machine translation (Brown et al., 1993), we may interpret E as an English sentence, F its translation in French, and A a representation of how the words correspond to each other in the two sentences. A pair of substrings [s, t] ⊂ E and [u, v] ⊂ F is a phrase pair (Och and Ney, 2004b) if and only if the subset of links emitted from [s, t] in E is equal to the subset of links emitted from [u, v] in F , and both are nonempty. Figure 1a shows an example of a many-tomany alignment, where E = [1, 6], F = [1, 7], and A = {(1, 6), (2, 5), (2, 7), (3, 4), (4, 1), (4, 3), (5, 2), (6, 1), (6, 3)}. The eight phrase pairs in this alignment are:"
C08-1136,P05-1033,1,0.933656,"analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 Introduction Many recent syntax-based statistical machine translation systems fall into the general formalism of Synchronous Context-Free Grammars (SCFG), where the grammar rules are found by first aligning parallel text at the word level. From wordlevel alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here. In this paper, we derive an optimal, linear-time algorithm for the problem of decomposing an arbitrary word-level alignment into SCFG rules such that each rule has at least one aligned word and is minimal in the sense that it cannot be further decomposed into smaller rules. Extracting minimal rules is of interest both because rules with fewer words are more likely to generalize to new data, c 2008. Licensed under the Creative Commons Attribution-Noncommercial-Share Alike 3.0 Unported license (http://creativecommons.org/licenses/by-nc-sa/3.0/). Some righ"
C08-1136,N04-1035,0,0.105225,"a Synchronous Context-Free Grammar (SCFG) with the simplest rules possible. We also use the algorithm to precisely analyze the maximum SCFG rule length needed to cover hand-aligned data from various language pairs. 1 Introduction Many recent syntax-based statistical machine translation systems fall into the general formalism of Synchronous Context-Free Grammars (SCFG), where the grammar rules are found by first aligning parallel text at the word level. From wordlevel alignments, such systems extract the grammar rules consistent either with the alignments and parse trees for one of languages (Galley et al., 2004), or with the the word-level alignments alone without reference to external syntactic analysis (Chiang, 2005), which is the scenario we address here. In this paper, we derive an optimal, linear-time algorithm for the problem of decomposing an arbitrary word-level alignment into SCFG rules such that each rule has at least one aligned word and is minimal in the sense that it cannot be further decomposed into smaller rules. Extracting minimal rules is of interest both because rules with fewer words are more likely to generalize to new data, c 2008. Licensed under the Creative Commons Attribution-"
C10-1106,W09-1804,1,0.887675,"Missing"
C10-1106,P08-1085,0,0.277853,"Missing"
C10-1106,P07-1094,0,0.0945569,"Missing"
C10-1106,P09-1039,0,0.0627814,"Missing"
C10-1106,J94-2001,0,0.826927,"solver to do model minimization. However, solving this problem exactly using an integer programming formulation is intractable for practical purposes. We propose a novel two-stage greedy approximation scheme to replace the IP. Our method runs fast, while yielding highly accurate tagging results. We also compare our method against standard EM training, and show that we consistently obtain better tagging accuracies on test data of varying sizes for English and Italian. 1 tˆ = arg max P (w, t) t Introduction The task of unsupervised part-of-speech (POS) tagging with a dictionary as formulated by Merialdo (1994) is: given a raw word sequence and a dictionary of legal POS tags for each word type, tag each word token in the text. A common approach to modeling such sequence labeling problems is to build a bigram Hidden Markov Model (HMM) parameterized by tag-bigram transition probabilities P (ti |ti−1 ) and word-tag emission probabilities P (wi |ti ). Given a word sequence w and a tag sequence t, of length N , the joint probability P (w, t) is given by: P (w, t) = N Y i=1 P (wi |ti ) · P (ti |ti−1 ) (1) (2) Ravi and Knight (2009) attack the Merialdo task in two stages. In the first stage, they search fo"
C10-1106,C04-1197,0,0.0356354,"Missing"
C10-1106,D08-1085,1,0.894014,"Missing"
C10-1106,P09-1057,1,0.934644,"rvised Tagging Sujith Ravi and Ashish Vaswani and Kevin Knight and David Chiang University of Southern California Information Sciences Institute {sravi,avaswani,knight,chiang}@isi.edu Abstract We can train this model using the Expectation Maximization (EM) algorithm (Dempster and Rubin, 1977) which learns P (wi |ti ) and P (ti |ti−1 ) that maximize the likelihood of the observed data. Once the parameters are learnt, we can find the best tagging using the Viterbi algorithm. Model minimization has been shown to work well for the task of unsupervised part-of-speech tagging with a dictionary. In (Ravi and Knight, 2009), the authors invoke an integer programming (IP) solver to do model minimization. However, solving this problem exactly using an integer programming formulation is intractable for practical purposes. We propose a novel two-stage greedy approximation scheme to replace the IP. Our method runs fast, while yielding highly accurate tagging results. We also compare our method against standard EM training, and show that we consistently obtain better tagging accuracies on test data of varying sizes for English and Italian. 1 tˆ = arg max P (w, t) t Introduction The task of unsupervised part-of-speech"
C10-1106,P05-1044,0,0.0539854,"Missing"
C12-2013,P10-1010,1,0.908867,"Missing"
C12-2013,W11-1216,1,0.836094,"ata that would be more useful for machine translation experiments, the following steps would be required. First, the primary textual sources should be audio recordings, and transcribed using a tool that preserves the audio alignment (for later verification) and which links wordforms to lexemes (for consistency in spelling, word breaks, and glosses). Second, the transcription and glossing software should operate in tandem with curating a shared n-language lexicon to speed up the process and encourage consistency across speakers, possibly using the structures described in (Baldwin et al., 2010; Abney and Bird, 2011). 5 Conclusion Most of the world’s languages will fall out of use before the world’s linguists and computational linguists are able to collect sufficient data. However, we have been investigating simple methodologies and supporting software that are helping speakers of endangered languages in Papua New Guinea to produce usable documentation on their own. The primary data type is bilingual text – or interlinear glossed text – which serves the dual purpose of documenting a language and developing translation models. Once the translation models reach an adequate level, they could be usable as the"
C12-2013,C10-3010,0,0.0306534,"nerate a quantity of data that would be more useful for machine translation experiments, the following steps would be required. First, the primary textual sources should be audio recordings, and transcribed using a tool that preserves the audio alignment (for later verification) and which links wordforms to lexemes (for consistency in spelling, word breaks, and glosses). Second, the transcription and glossing software should operate in tandem with curating a shared n-language lexicon to speed up the process and encourage consistency across speakers, possibly using the structures described in (Baldwin et al., 2010; Abney and Bird, 2011). 5 Conclusion Most of the world’s languages will fall out of use before the world’s linguists and computational linguists are able to collect sufficient data. However, we have been investigating simple methodologies and supporting software that are helping speakers of endangered languages in Papua New Guinea to produce usable documentation on their own. The primary data type is bilingual text – or interlinear glossed text – which serves the dual purpose of documenting a language and developing translation models. Once the translation models reach an adequate level, they"
C12-2013,I11-1059,1,0.840007,"recording device. The first step is to create a text, either by recording then transcribing, or by composing directly onto paper. Chances are that the speaker will have no experience at IPA transcription and that no standardised orthography for the language exists. Thus, transcription needs to use whatever orthography people know. This practice has some documentary value, for it shows meaningful sound contrasts and word boundaries, and serves as a rough finding aid. In cases where more than one speaker transcribes content in a language, we can try to clean up the transcriptions automatically (Foda and Bird, 2011). The second step is to translate the text, providing word by word glosses plus a phrasal translation. The correspondence between this literal and “free” translation amounts to training data for an alignment model, and does not require a separate translation model. The final step is to prepare a lexicon, in order to help fix the inconsistencies in spelling and glossing between. SIL’s Fieldworks Language Explorer (FLEx) software is ideal for this purpose, though it currently lacks support for synchronisation and conflict resolution between databases. An important refinement is to conduct the ab"
C12-2013,P12-2059,0,0.0266688,"epare a lexicon, in order to help fix the inconsistencies in spelling and glossing between. SIL’s Fieldworks Language Explorer (FLEx) software is ideal for this purpose, though it currently lacks support for synchronisation and conflict resolution between databases. An important refinement is to conduct the above workflow within a cluster of closely related languages. Speakers often produce a wealth of information about lexical correspondences with neighboring languages, as illustrated in Figure 1. Armed with these correspondences, we can pool knowledge about all the languages in the cluster (Nakov and Tiedemann, 2012). We can also try to guess word translations by leveraging regular sound correspondences. eng sun water fire earth tree mountain house food pig man woman father mother aso ho noso olo misumbo ya golo numuno nosonite ije we vene meneho’we ijeneho bef yege nagami logo mei yafa kosa nohi nosena yaga bo amo afonifu itonifu gah ho nagami lo mikasi za agoka numuni nosa’neta iza ve vena ahono izo’no ino yake tina ata mopa yosa akoya nona neya afu ve a’ne afo nimo’e ita kbq zge tina teve mo’pa zafa agona nona ne’zane afu ve’nene a’re nenfa anta’nimo snp fo no soo mika yaa obura numuna aáwa’a savu wee"
C12-2013,N07-1057,0,0.188144,"aller data files. To facilitate access, the raw data is usually transcribed and translated. It should be clear that language documentation is not the same as linguistic description, which calls for linguistic expertise and which produces systematic presentations of the phonology, morphology, syntax, and semantics of the language. Nevertheless, the descriptive work cannot proceed without the language documentation. This documentation – the bilingual text collection – is the same as what is needed for statistical MT and we can expect to apply MT algorithms to the data from linguistic fieldwork (Xia and Lewis, 2007; Palmer et al., 2010). The workflow for language documentation and description has never been standardised, but there is general agreement that it involves at least the following activities: (a) recording communicative events; (b) transcribing and translating the recordings; (c) performing basic morphosyntactic analysis leading to a lexicon and to a collection of morphologically-glossed text; (d) eliciting paradigms, i.e. systematic tabulations of linguistic forms designed to reveal underlying patterns; (e) preparing descriptive reports to show how the language is structured. These activities"
C18-1214,P04-3031,0,0.247291,"Missing"
C18-1214,C14-1096,0,0.0483234,"Missing"
C18-1214,W14-2201,0,0.13972,"Missing"
C18-1214,P11-1061,0,0.107033,"Missing"
C18-1214,P17-2093,0,0.0668892,"Missing"
C18-1214,N13-1014,0,0.117134,"Missing"
C18-1214,P13-1057,0,0.0496625,"Missing"
C18-1214,P07-2045,0,0.00559034,"Missing"
C18-1214,N16-1030,0,0.0245985,"Missing"
C18-1214,petrov-etal-2012-universal,0,0.140452,"Missing"
C18-1214,Q13-1001,0,0.0606958,"Missing"
C18-1214,N03-1033,0,0.165481,"Missing"
C18-1214,N01-1026,0,0.496919,"Missing"
C18-1214,N16-1156,0,0.040947,"Missing"
C18-1214,W02-0109,0,\N,Missing
D08-1024,2007.mtsummit-papers.3,0,0.732994,"weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task. Taking this further, we test MIRA on two classes of features that make use of syntactic information and hierarchical"
D08-1024,P08-1024,0,0.283377,"tence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a la"
D08-1024,W08-0304,0,0.435015,"on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surp"
D08-1024,P08-1009,0,0.0445565,"onal cost. Marton and Resnik (2008) showed that it is possible to improve translation in a data-driven framework by incorporating source-side syntactic analysis in the form of soft syntactic constraints. This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT. In the area of source-side syntax, recent research has continued to improve tree-to-string translation models, soften the constraints of the input tree in various ways (Mi et al., 2008; Zhang et al., 2008), and extend phrase-based translation with sourceside soft syntactic constraints (Cherry, 2008). All this work shows strong promise, but Marton and Resnik’s soft syntactic constraint approach is particularly appealing because it can be used unobtrusively with any hierarchically-structured translation model. Here, we have shown that using MIRA to weight all the constraints at once removes the crucial drawback of the approach, the problem of feature selection. Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-based approach. By capturing how reordering depends on constituent length, these features improve translation quality"
D08-1024,P05-1033,1,0.688957,"for a total of 56 feature weights, we improve performance by 2.6 B on a subset of the NIST 2006 Arabic-English evaluation data. 1 Introduction Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems. However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features. One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information. Each of their new features rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a traini"
D08-1024,J07-2003,1,0.558972,"f 56 feature weights, we improve performance by 2.6 B on a subset of the NIST 2006 Arabic-English evaluation data. 1 Introduction Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems. However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features. One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information. Each of their new features rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that"
D08-1024,W02-1001,0,0.0620584,"The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007). We describe the basic algorithm first and then progressively refine it. 2.1 Basic algorithm passes through the training data are made, we only average the weight vectors from the last pass.) The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice (Freund and Schapire, 1999; Collins, 2002). We follow McDonald et al. (2005) in applying this technique to MIRA. Note that the objective (1) is not the same as that used by Watanabe et al.; rather, it is the same as that used by Crammer and Singer (2003) and related to that of Taskar et al. (2005). We solve this optimization problem using a variant of sequential minimal optimization (Platt, 1998): for each i, initialize αi j = C for a single value of j such that ei j = e∗i , and initialize αi j = 0 for all other values of j. Then, repeatedly choose a sentence i and a pair of hypotheses j, j0 , and let Let e, by abuse of notation, stan"
D08-1024,N04-4038,0,0.0240612,"rom the translations preferred by the model; thus they would cause violent updates to w. Local updating would select the topmost point labeled µ = 1. Our scheme would select one of the µ = 0.5 points, which have B scores almost as high as the max-B translations, yet are not very far from the translations preferred by the model. 2.4 Selecting hypothesis translations What is the set {ei j } of translation hypotheses? Ideally we would let it be the set of all possible translations, and let the objective function (1) take all of them into account. This is the approach taken by Taskar et al. (2004), but their approach assumes that the loss function can be decomposed into local loss functions. Since our loss function cannot be so decomposed, we select: • the 10-best translations according to the model; we then rescore the forest to obtain • the 10-best translations according to equation (11) with µ = 0.5, the first of which is the oracle translation, and • the 10-best translations with µ = ∞, to serve as negative examples. The last case is what Crammer et al. (2006) call max-loss updating (where “loss” refers to the generalized hinge loss) and Taskar et al. (2005) call lossaugmented infe"
D08-1024,W07-0414,0,0.0188539,"axB points in the upper left are not included (and would have no effect even if they were included). The µ = ∞ points in the lower-right are the negative examples: they are poor translations that are scored too high by the model, and the learning algorithm attempts to shift them to the left. 227 To perform the forest rescoring, we need to use several approximations, since an exact search for B-optimal translations is NP-hard (Leusch et al., 2008). For every derivation e in the forest, we calculate a vector c(e) of counts as in Section 2.2 except using unclipped counts of n-gram matches (Dreyer et al., 2007), that is, the number of matches for an ngram can be greater than the number of occurrences of the n-gram in any reference translation. This can be done efficiently by calculating c for every hyperedge (rule application) in the forest: • the number of output words generated by the rule • the effective reference length scaled by the fraction of the input sentence consumed by the rule • the number of n-grams formed by the application of the rule (1 ≤ n ≤ 4) • the (unclipped) number of n-gram matches formed by the application of the rule (1 ≤ n ≤ 4) We keep track of n-grams using the same scheme"
D08-1024,P08-2010,0,0.0419974,"ures rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we o"
D08-1024,W04-3250,0,0.11639,"ting a word into itself) – two classes of number/name translation rules – glue rules The probability features are base-100 logprobabilities. The rules were extracted from all the allowable parallel text from the NIST 2008 evaluation (152+175 million words of Arabic+English), aligned by IBM Model 4 using GIZA++ (union of both directions). Hierarchical rules were extracted 230 Results Table 1 shows the results of our experiments with the training methods and features described above. All significance testing was performed against the first line (MERT baseline) using paired bootstrap resampling (Koehn, 2004). First of all, we find that MIRA is competitive with MERT when both use the baseline feature set. In1 The only notable consequence this had for our experimentation is that proclitic Arabic prepositions were fused onto the first word of their NP object, so that the PP and NP brackets were coextensive. 2 We chose this policy for MIRA to avoid overfitting. However, we could have used the tuning set for this purpose, just as with MERT: in none of our runs would this change have made more than a 0.2 B difference on the development set. Train MERT MIRA Features baseline syntax (coarse) syntax (f"
D08-1024,D08-1088,0,0.0197708,"j · w) in order to approximate the effect of using the whole forest. See Figure 1 again for an illustration of the hypotheses selected for a single sentence. The maxB points in the upper left are not included (and would have no effect even if they were included). The µ = ∞ points in the lower-right are the negative examples: they are poor translations that are scored too high by the model, and the learning algorithm attempts to shift them to the left. 227 To perform the forest rescoring, we need to use several approximations, since an exact search for B-optimal translations is NP-hard (Leusch et al., 2008). For every derivation e in the forest, we calculate a vector c(e) of counts as in Section 2.2 except using unclipped counts of n-gram matches (Dreyer et al., 2007), that is, the number of matches for an ngram can be greater than the number of occurrences of the n-gram in any reference translation. This can be done efficiently by calculating c for every hyperedge (rule application) in the forest: • the number of output words generated by the rule • the effective reference length scaled by the fraction of the input sentence consumed by the rule • the number of n-grams formed by the application"
D08-1024,P06-1096,0,0.848802,"Missing"
D08-1024,C04-1072,0,0.23212,"ged together. (If multiple 225 δ= (`i j − `i j0 ) − (∆hi j − ∆hi j0 ) · w0 k∆hi j − ∆hi j0 k2 [−αi j ,αi j0 ] clip (5) where the function clip[x,y] (z) gives the closest number to z in the interval [x, y]. 2.2 Loss function Assuming B as the evaluation criterion, the loss `i j of ei j relative to e∗i should be related somehow to the difference between their B scores. However, B was not designed to be used on individual sentences; in general, the highest-B translation of a sentence depends on what the other sentences in the test set are. Sentence-level approximations to B exist (Lin and Och, 2004; Liang et al., 2006), but we found it most effective to perform B computations in the context of a set O of previously-translated sentences, following Watanabe et al. (2007). However, we don’t try to accumulate translations for the entire dataset, but simply maintain an exponentially-weighted moving average of previous translations. 1 0.9 B score More precisely: For an input sentence f, let e be some hypothesis translation and let {rk } be the set of reference translations for f. Let c(e; {rk }), or simply c(e) for short, be the vector of the following counts: |e|, the effective referen"
D08-1024,P08-1114,1,0.74718,"nt improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 B on a subset of the NIST 2006 Arabic-English evaluation data. 1 Introduction Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems. However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features. One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information. Each of their new features rewards or punishes a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations"
D08-1024,P05-1012,0,0.162454,"standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007). We describe the basic algorithm first and then progressively refine it. 2.1 Basic algorithm passes through the training data are made, we only average the weight vectors from the last pass.) The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice (Freund and Schapire, 1999; Collins, 2002). We follow McDonald et al. (2005) in applying this technique to MIRA. Note that the objective (1) is not the same as that used by Watanabe et al.; rather, it is the same as that used by Crammer and Singer (2003) and related to that of Taskar et al. (2005). We solve this optimization problem using a variant of sequential minimal optimization (Platt, 1998): for each i, initialize αi j = C for a single value of j such that ei j = e∗i , and initialize αi j = 0 for all other values of j. Then, repeatedly choose a sentence i and a pair of hypotheses j, j0 , and let Let e, by abuse of notation, stand for both output strings and thei"
D08-1024,P08-1023,0,0.0181847,"same set of features, MIRA’s performance compares favorably to MERT in terms of both translation quality and computational cost. Marton and Resnik (2008) showed that it is possible to improve translation in a data-driven framework by incorporating source-side syntactic analysis in the form of soft syntactic constraints. This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT. In the area of source-side syntax, recent research has continued to improve tree-to-string translation models, soften the constraints of the input tree in various ways (Mi et al., 2008; Zhang et al., 2008), and extend phrase-based translation with sourceside soft syntactic constraints (Cherry, 2008). All this work shows strong promise, but Marton and Resnik’s soft syntactic constraint approach is particularly appealing because it can be used unobtrusively with any hierarchically-structured translation model. Here, we have shown that using MIRA to weight all the constraints at once removes the crucial drawback of the approach, the problem of feature selection. Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-ba"
D08-1024,P02-1038,0,0.187192,"08. 2008 Association for Computational Linguistics training all of them simultaneously; and, second, we introduce a novel structural distortion model. We obtain significant improvements in both cases, and further large improvements when the two feature sets are combined. The paper proceeds as follows. We describe our training algorithm in section 2; our generalization of Marton and Resnik’s soft syntactic constraints in section 3; our novel structural distortion features in section 4; and experimental results in section 5. 2 Learning algorithm The translation model is a standard linear model (Och and Ney, 2002), which we train using MIRA (Crammer and Singer, 2003; Crammer et al., 2006), following Watanabe et al. (2007). We describe the basic algorithm first and then progressively refine it. 2.1 Basic algorithm passes through the training data are made, we only average the weight vectors from the last pass.) The technique of averaging was introduced in the context of perceptrons as an approximation to taking a vote among all the models traversed during training, and has been shown to work well in practice (Freund and Schapire, 1999; Collins, 2002). We follow McDonald et al. (2005) in applying this te"
D08-1024,P03-1021,0,0.442838,"tion quality and computational cost. We then test the method on two classes of features that address deficiencies in the Hiero hierarchical phrasebased model: first, we simultaneously train a large number of Marton and Resnik’s soft syntactic constraints, and, second, we introduce a novel structural distortion model. In both cases we obtain significant improvements in translation performance. Optimizing them in combination, for a total of 56 feature weights, we improve performance by 2.6 B on a subset of the NIST 2006 Arabic-English evaluation data. 1 Introduction Since its introduction by Och (2003), minimum error rate training (MERT) has been widely adopted for training statistical machine translation (MT) systems. However, MERT is limited in the number of feature weights that it can optimize reliably, with folk estimates of the limit ranging from 15 to 30 features. One recent example of this limitation is a series of experiments by Marton and Resnik (2008), in which they added syntactic features to Hiero (Chiang, 2005; Chiang, 2007), which ordinarily uses no linguistically motivated syntactic information. Each of their new features rewards or punishes a derivation depending on how simi"
D08-1024,P06-2101,0,0.275923,"a derivation depending on how similar or dissimilar it is to a syntactic parse of the input sentence. They found that in order to obtain the greatest improvement, these features had to be specialized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA"
D08-1024,P08-1058,0,0.0108772,"e introduce a fine-grained version of our distortion model that can be trained directly in the translation task as follows: define • word penalty from the most in-domain corpora (4.2+5.4 million words) and phrases were extracted from the remainder. We trained the coarse-grained distortion model on 10,000 sentences of the training data. Two language models were trained, one on data similar to the English side of the parallel text and one on 2 billion words of English. Both were 5gram models with modified Kneser-Ney smoothing, lossily compressed using a perfect-hashing scheme similar to that of Talbot and Brants (2008) but using minimal perfect hashing (Botelho et al., 2005). We partitioned the documents of the NIST 2004 (newswire) and 2005 Arabic-English evaluation data into a tuning set (1178 sentences) and a development set (1298 sentences). The test data was the NIST 2006 Arabic-English evaluation data (NIST part, newswire and newsgroups, 1529 sentences). To obtain syntactic parses for this data, we tokenized it according to the Arabic Treebank standard using AMIRA (Diab et al., 2004), parsed it with the Stanford parser (Klein and Manning, 2003), and then forced the trees back into the MT system’s token"
D08-1024,W04-3201,0,0.0135648,"far removed from the translations preferred by the model; thus they would cause violent updates to w. Local updating would select the topmost point labeled µ = 1. Our scheme would select one of the µ = 0.5 points, which have B scores almost as high as the max-B translations, yet are not very far from the translations preferred by the model. 2.4 Selecting hypothesis translations What is the set {ei j } of translation hypotheses? Ideally we would let it be the set of all possible translations, and let the objective function (1) take all of them into account. This is the approach taken by Taskar et al. (2004), but their approach assumes that the loss function can be decomposed into local loss functions. Since our loss function cannot be so decomposed, we select: • the 10-best translations according to the model; we then rescore the forest to obtain • the 10-best translations according to equation (11) with µ = 0.5, the first of which is the oracle translation, and • the 10-best translations with µ = ∞, to serve as negative examples. The last case is what Crammer et al. (2006) call max-loss updating (where “loss” refers to the generalized hinge loss) and Taskar et al. (2005) call lossaugmented infe"
D08-1024,P06-1091,0,0.533842,"alized for particular syntactic categories and weighted independently. Not being able to optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task. Taking this further, we test MIRA on two classes of features that make us"
D08-1024,D07-1080,0,0.791062,"o optimize them all at once using MERT, they resorted to running MERT many times in order to test different combinations of features. But it would have been preferable to use a training method that can optimize the features all at once. There has been much work on improving MERT’s performance (Duh and Kirchoff, 2008; Smith and Eisner, 2006; Cer et al., 2008), or on replacing MERT wholesale (Turian et al., 2007; Blunsom et al., 2008). This paper continues a line of research on online discriminative training (Tillmann and Zhang, 2006; Liang et al., 2006; Arun and Koehn, 2007), extending that of Watanabe et al. (2007), who use the Margin Infused Relaxed Algorithm (MIRA) due to Crammer et al. (2003; 2006). Our guiding principle is practicality: like Watanabe et al., we train on a small tuning set comparable in size to that used by MERT, but by parallel processing and exploiting more of the parse forest, we obtain results using MIRA that match or surpass MERT in terms of both translation quality and computational cost on a large-scale translation task. Taking this further, we test MIRA on two classes of features that make use of syntactic information and hierarchical structure. First, we generalize Marton an"
D08-1024,P96-1021,0,0.0250127,"an the number of occurrences of the n-gram in any reference translation. This can be done efficiently by calculating c for every hyperedge (rule application) in the forest: • the number of output words generated by the rule • the effective reference length scaled by the fraction of the input sentence consumed by the rule • the number of n-grams formed by the application of the rule (1 ≤ n ≤ 4) • the (unclipped) number of n-gram matches formed by the application of the rule (1 ≤ n ≤ 4) We keep track of n-grams using the same scheme used to incorporate an n-gram language model into the decoder (Wu, 1996; Chiang, 2007). To find the best derivation in the forest, we traverse it bottom-up as usual, and for every set of alternative subtranslations, we select the one with the highest score. But here a rough approximation lurks, because we need to calculate B on the nodes of the forest, but B does not have the optimal substructure property, i.e., the optimal score of a parent node cannot necessarily be calculated from the optimal scores of its children. Nevertheless, we find that this rescoring method is good enough for generating high-B oracle translations and low-B negative examples. 2.5 P"
D08-1024,P08-1064,0,0.0154052,"res, MIRA’s performance compares favorably to MERT in terms of both translation quality and computational cost. Marton and Resnik (2008) showed that it is possible to improve translation in a data-driven framework by incorporating source-side syntactic analysis in the form of soft syntactic constraints. This work joins a growing body of work demonstrating the utility of syntactic information in statistical MT. In the area of source-side syntax, recent research has continued to improve tree-to-string translation models, soften the constraints of the input tree in various ways (Mi et al., 2008; Zhang et al., 2008), and extend phrase-based translation with sourceside soft syntactic constraints (Cherry, 2008). All this work shows strong promise, but Marton and Resnik’s soft syntactic constraint approach is particularly appealing because it can be used unobtrusively with any hierarchically-structured translation model. Here, we have shown that using MIRA to weight all the constraints at once removes the crucial drawback of the approach, the problem of feature selection. Finally, we have introduced novel structural distortion features to fill a notable gap in the hierarchical phrase-based approach. By capt"
D08-1024,P02-1040,0,\N,Missing
D08-1024,P07-1005,1,\N,Missing
D08-1024,N03-1017,0,\N,Missing
D08-1064,W07-0414,0,0.0217744,"tion is caused by B allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained. It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim. However, there is again a dovetailing engineering concern which is quite legitimate. If one wants to select the minimum-Bayes-risk translation from a lattice (or shared forest) instead of an n-best list (Tromble et al., 2008), or to select an oracle translation from a lattice (Tillmann and Zhang, 2006; Dreyer et al., 2007; Leusch et al., 2008), or to perform discriminative training on all the examples contained in a lattice (Taskar et al., 2004), one would need a metric that can be calculated on the edges of the lattice. Of the metrics surveyed in the WMT 2007 evaluation-evaluation, only one metric, to our knowledge, has this property: word error rate (Nießen et al., 2000). Here, we deal with the related word recognition rate (McCowan et al., 2005), WRR = 1 − WER I+D+S = 1 − min |r| M−I = max |r| (5) where I is the number of insertions, D of deletions, S of substitutions, and M = |r |− D − S the number of matc"
D08-1064,N04-1035,0,0.021,"so tended to have shorter reference sentences (relative to the source sentences) than the harder genre, weblogs. For example, in one dataset, the newswire reference sets had between 1.3 and 1.37 English words per Arabic word, but the weblog reference set had 1.52 English words per Arabic word. Thus, a system that is uniformly verbose across both genres will apportion more of its output to newswire than to weblogs, serendipitously leading to a higher score. This phenomenon has subsequently been observed by Och (2008) as well. We trained three Arabic-English syntax-based statistical MT systems (Galley et al., 2004; Galley et al., 2006) using max-B training (Och, 2003): one on a newswire development set, one on a weblog development set, and one on a combined development set containing documents from both genres. We then translated a new mixed-genre test set in two ways: (1) each document with its appropriate genrespecific system, and (2) all documents with the system trained on the combined (mixed-genre) development set. In Table 3, we report the results of both approaches on the entire test dataset as well as the portion of the test dataset in each genre, for both the genre-specific and mixed-genre"
D08-1064,D08-1088,0,0.049341,"Missing"
D08-1064,W08-0301,0,0.0193383,"Missing"
D08-1064,P06-1096,0,0.0465977,"Missing"
D08-1064,C04-1072,0,0.034405,"t al., 2006), and ParaEval-Recall (Zhou et al., 2006). Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang et al., 2006; Watanabe et al., 2007), and sentenceby-sentence comparisons of outputs during error analysis. A variation on B is often used for these purposes, in which the k-gram precisions are “smoothed” by adding one to the numerator and denominator (Lin and Och, 2004); this addresses the problem of a zero k-gram match canceling out the entire score, but it does not address the problems illustrated above. The remaining issue, word deletion, is more difficult to assess. It could be argued that part of the gain due to word deletion is caused by B allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained. It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim. However, there is again a dovetailing engi"
D08-1064,N03-2021,0,0.178812,"ere one can obtain improvements in B scores that are questionable or even absurd. These situations arise because B lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modification to B and a cross between B and word error rate that address these issues while improving correlation with human judgments. 1 Introduction B (Papineni et al., 2002) was one of the first automatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics (Melamed et al., 2003; Banerjee and Lavie, 2005; Snover et al., 2006; Chan and Ng, 2008), it remains the standard in the statistical MT literature. Callison-Burch et al. (2006) have subjected B to a searching criticism, with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments. Both cases involve comparisons between statistical MT systems and other translation methods (human post-editing and a rule-based MT system), and they recommend that the use of B be restricted to comparisons between related systems or different versions of the same systems."
D08-1064,niessen-etal-2000-evaluation,0,0.283501,"have examined—except for word deletion—are traceable to the fact that B is not a sentence-level metric. Any metric which is defined as a weighted average of sentence-level scores, where the weights are system-independent, will be immune to these problems. Note that any metric involving micro-averaged precision (in which the sentence-level counts of matches and guesses are summed separately before forming their ratio) cannot have this property. Of the metrics surveyed in the WMT 2007 evaluation-evaluation (CallisonBurch et al., 2007), at least the following metrics have this property: WER (Nießen et al., 2000), TER (Snover et al., 2006), and ParaEval-Recall (Zhou et al., 2006). Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang et al., 2006; Watanabe et al., 2007), and sentenceby-sentence comparisons of outputs during error analysis. A variation on B is often used for these purposes, in which the k-gram precisions are “smoothed” by adding one to the numerator an"
D08-1064,P03-1021,0,0.0258309,"ce sentences) than the harder genre, weblogs. For example, in one dataset, the newswire reference sets had between 1.3 and 1.37 English words per Arabic word, but the weblog reference set had 1.52 English words per Arabic word. Thus, a system that is uniformly verbose across both genres will apportion more of its output to newswire than to weblogs, serendipitously leading to a higher score. This phenomenon has subsequently been observed by Och (2008) as well. We trained three Arabic-English syntax-based statistical MT systems (Galley et al., 2004; Galley et al., 2006) using max-B training (Och, 2003): one on a newswire development set, one on a weblog development set, and one on a combined development set containing documents from both genres. We then translated a new mixed-genre test set in two ways: (1) each document with its appropriate genrespecific system, and (2) all documents with the system trained on the combined (mixed-genre) development set. In Table 3, we report the results of both approaches on the entire test dataset as well as the portion of the test dataset in each genre, for both the genre-specific and mixed-genre trainings. The genre-specific systems each outperform the"
D08-1064,P02-1040,0,0.112252,"development of statistical machine translation systems. We describe three real-world situations involving comparisons between different versions of the same systems where one can obtain improvements in B scores that are questionable or even absurd. These situations arise because B lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modification to B and a cross between B and word error rate that address these issues while improving correlation with human judgments. 1 Introduction B (Papineni et al., 2002) was one of the first automatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics (Melamed et al., 2003; Banerjee and Lavie, 2005; Snover et al., 2006; Chan and Ng, 2008), it remains the standard in the statistical MT literature. Callison-Burch et al. (2006) have subjected B to a searching criticism, with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments. Both cases involve comparisons between statistical MT systems and other translation methods (human post-editin"
D08-1064,2006.amta-papers.25,0,0.340133,"hat are questionable or even absurd. These situations arise because B lacks the property of decomposability, a property which is also computationally convenient for various applications. We propose a very conservative modification to B and a cross between B and word error rate that address these issues while improving correlation with human judgments. 1 Introduction B (Papineni et al., 2002) was one of the first automatic evaluation metrics for machine translation (MT), and despite being challenged by a number of alternative metrics (Melamed et al., 2003; Banerjee and Lavie, 2005; Snover et al., 2006; Chan and Ng, 2008), it remains the standard in the statistical MT literature. Callison-Burch et al. (2006) have subjected B to a searching criticism, with two realworld case studies of significant failures of correlation between B and human adequacy/fluency judgments. Both cases involve comparisons between statistical MT systems and other translation methods (human post-editing and a rule-based MT system), and they recommend that the use of B be restricted to comparisons between related systems or different versions of the same systems. In B’s defense, comparisons between differe"
D08-1064,P06-1091,0,0.0228924,"the gain due to word deletion is caused by B allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained. It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim. However, there is again a dovetailing engineering concern which is quite legitimate. If one wants to select the minimum-Bayes-risk translation from a lattice (or shared forest) instead of an n-best list (Tromble et al., 2008), or to select an oracle translation from a lattice (Tillmann and Zhang, 2006; Dreyer et al., 2007; Leusch et al., 2008), or to perform discriminative training on all the examples contained in a lattice (Taskar et al., 2004), one would need a metric that can be calculated on the edges of the lattice. Of the metrics surveyed in the WMT 2007 evaluation-evaluation, only one metric, to our knowledge, has this property: word error rate (Nießen et al., 2000). Here, we deal with the related word recognition rate (McCowan et al., 2005), WRR = 1 − WER I+D+S = 1 − min |r| M−I = max |r| (5) where I is the number of insertions, D of deletions, S of substitutions, and M = |r |− D −"
D08-1064,D08-1065,0,0.0305721,"ord deletion, is more difficult to assess. It could be argued that part of the gain due to word deletion is caused by B allowing a system to selectively translate those parts of a sentence on which higher precision can be obtained. It would be difficult indeed to argue that an evaluation metric, in order to be fair, must be decomposable into subsentential scores, and we make no such claim. However, there is again a dovetailing engineering concern which is quite legitimate. If one wants to select the minimum-Bayes-risk translation from a lattice (or shared forest) instead of an n-best list (Tromble et al., 2008), or to select an oracle translation from a lattice (Tillmann and Zhang, 2006; Dreyer et al., 2007; Leusch et al., 2008), or to perform discriminative training on all the examples contained in a lattice (Taskar et al., 2004), one would need a metric that can be calculated on the edges of the lattice. Of the metrics surveyed in the WMT 2007 evaluation-evaluation, only one metric, to our knowledge, has this property: word error rate (Nießen et al., 2000). Here, we deal with the related word recognition rate (McCowan et al., 2005), WRR = 1 − WER I+D+S = 1 − min |r| M−I = max |r| (5) where I is th"
D08-1064,D07-1080,0,0.0145448,"ely before forming their ratio) cannot have this property. Of the metrics surveyed in the WMT 2007 evaluation-evaluation (CallisonBurch et al., 2007), at least the following metrics have this property: WER (Nießen et al., 2000), TER (Snover et al., 2006), and ParaEval-Recall (Zhou et al., 2006). Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang et al., 2006; Watanabe et al., 2007), and sentenceby-sentence comparisons of outputs during error analysis. A variation on B is often used for these purposes, in which the k-gram precisions are “smoothed” by adding one to the numerator and denominator (Lin and Och, 2004); this addresses the problem of a zero k-gram match canceling out the entire score, but it does not address the problems illustrated above. The remaining issue, word deletion, is more difficult to assess. It could be argued that part of the gain due to word deletion is caused by B allowing a system to selectively translate those parts of a sentence on which"
D08-1064,zhang-etal-2004-interpreting,0,0.0604902,"Missing"
D08-1064,W06-1610,0,0.0203609,"B is not a sentence-level metric. Any metric which is defined as a weighted average of sentence-level scores, where the weights are system-independent, will be immune to these problems. Note that any metric involving micro-averaged precision (in which the sentence-level counts of matches and guesses are summed separately before forming their ratio) cannot have this property. Of the metrics surveyed in the WMT 2007 evaluation-evaluation (CallisonBurch et al., 2007), at least the following metrics have this property: WER (Nießen et al., 2000), TER (Snover et al., 2006), and ParaEval-Recall (Zhou et al., 2006). Moreover, this evaluation concern dovetails with a frequent engineering concern, that sentence-level scores are useful at various points in the MT pipeline: for example, minimum Bayes risk decoding (Kumar and Byrne, 2004), selecting oracle translations for discriminative reranking (Liang et al., 2006; Watanabe et al., 2007), and sentenceby-sentence comparisons of outputs during error analysis. A variation on B is often used for these purposes, in which the k-gram precisions are “smoothed” by adding one to the numerator and denominator (Lin and Och, 2004); this addresses the problem of a z"
D08-1064,E06-1032,0,\N,Missing
D08-1064,koen-2004-pharaoh,0,\N,Missing
D08-1064,W07-0738,0,\N,Missing
D08-1064,W05-0909,0,\N,Missing
D08-1064,P08-1007,1,\N,Missing
D08-1064,W07-0734,0,\N,Missing
D08-1064,N04-1022,0,\N,Missing
D08-1064,W07-0718,0,\N,Missing
D08-1064,P05-1066,0,\N,Missing
D08-1064,P06-1121,1,\N,Missing
D08-1064,W04-3250,0,\N,Missing
D08-1064,J07-2003,1,\N,Missing
D13-1140,J07-2003,1,0.836534,"e cheaper to compute than sigmoid or tanh units. There is also evidence that deep neural networks with rectified linear units can be trained successfully without pre-training (Zeiler et al., 2013). Second, we train using noise-contrastive estimation or NCE (Gutmann and Hyv¨arinen, 2010; Mnih and Teh, 2012), which does not require repeated summations over the whole vocabulary. This enables us to efficiently build NPLMs on a larger scale than would be possible otherwise. We then apply this LM to MT in two ways. First, we use it to rerank the k-best output of a hierarchical phrase-based decoder (Chiang, 2007). Second, we integrate it directly into the decoder, allowing the neural LM to more strongly influence the model. We achieve gains of up to 0.6 Bleu translating French, German, and Spanish to English, and up to 1.1 Bleu on Chinese-English translation. 1387 Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 1387–1392, c Seattle, Washington, USA, 18-21 October 2013. 2013 Association for Computational Linguistics context matrices for each word in u, and φ is applied elementwise. The output of the second layer h2 is output P(w |u) D0 h2 = φ (Mh1 ) , hidde"
D13-1140,D12-1107,0,0.0178454,"Missing"
D13-1140,2012.iwslt-papers.3,0,0.321029,"lists, as has been done in previous work. But since the NPLM scores n-grams, it can also be integrated into a phrase-based or hierarchical phrase-based decoder just as a conventional n-gram model can, unlike a RNN. The most time-consuming step in computing ngram probabilities is the computation of the normalization constants Z(u). Following Mnih and Teh (2012), we set all the normalization constants to one during training, so that the model learns to produce approximately normalized probabilities. Then, when applying the LM, we can simply ignore normalization. A similar strategy was taken by Niehues and Waibel (2012). We find that a single n-gram lookup takes about 40 µs. The technique, described above, of grouping examples into minibatches works for scoring of k-best lists, but not while decoding. But caching n-gram probabilities helps to reduce the cost of the many lookups required during decoding. A final issue when decoding with a neural LM is that, in order to estimate future costs, we need to be able to estimate probabilities of n0 -grams for n0 &lt; n. In conventional LMs, this information is readily available,1 but not in NPLMs. Therefore, we defined a special word &lt;null&gt; whose embedding is the weigh"
D13-1140,J03-1002,0,0.0247929,"26.0 34.12 26.12 De-En dev test 28.8 21.5 29.1 21.5 29.3 21.9 Es-En dev test 33.5 32.0 34.1 32.2 34.22 32.12 Table 2: Results for Europarl MT experiments, without neural LM (baseline) and with neural LM for reranking and integrated decoding. The neural LM gives improvements across three different language pairs. Superscript 2 indicates a score averaged between two runs; all other scores were averaged over three runs. We ran experiments on four language pairs – Chinese to English and French, German, and Spanish to English – using a hierarchical phrase-based MT system (Chiang, 2007) and GIZA++ (Och and Ney, 2003) for word alignments. For all experiments, we used four LMs. The baselines used conventional 5-gram LMs, estimated with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on the English side of the bitext and the 329M-word Xinhua portion of English Gigaword (LDC2011T07). Against these baselines, we tested systems that included the two conventional LMs as well as two 5-gram NPLMs trained on the same datasets. The Europarl bitext NPLMs had a vocabulary size of 50k, while the other NPLMs had a vocabulary size of 100k. We used 150 dimensions for word embeddings, 750 units in hidden layer h1 ,"
D13-1140,P03-1021,0,0.0234069,"bulary size of 50k, while the other NPLMs had a vocabulary size of 100k. We used 150 dimensions for word embeddings, 750 units in hidden layer h1 , and 150 units in hidden layer h2 . We initialized the network parameters uniformly from (−0.01, 0.01) and the output biases to − log |V|, and optimized them by 10 epochs of stochastic gradient ascent, using minibatches of size 1000 and a learning rate of 1. We drew 100 noise samples per training example from the unigram distribution, using the alias method for efficiency (Kronmal and Peterson, 1979). We trained the discriminative models with MERT (Och, 2003) and the discriminative rerankers on 1000-best lists with MERT. Except where noted, we ran MERT three times and report the average score. We evaluated using case-insensitive NIST Bleu. without nonterminals were extracted from all training data, while rules with nonterminals were extracted from the FBIS corpus (LDC2003E14). We ran MERT on the development data, which was the NIST 2003 test data, and tested on the NIST 2004– 2006 test data. Reranking using the neural LM yielded improvements of 0.2–0.4 Bleu, while integrating the neural LM yielded larger improvements, between 0.6 and 1.1 Bleu. 4.1"
D13-1140,H05-1026,0,0.0468369,"expected, the MLE training time is roughly linear in |V|, whereas the NCE training time is basically constant. 5 Related Work The problem of training with large vocabularies in NPLMs has received much attention. One strategy has been to restructure the network to be more hierarchical (Morin and Bengio, 2005; Mnih and Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. 1391 We introduced a new variant of NPLMs that combines the network architecture of Bengio et al. (2003), rectified linear units (Nair and Hinton, 2010), and noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010). This model is dramatically faster to train than previous neural LMs, and can be trained on a large corpus with a large vocabulary and directly integrated into the decoder of a MT system. Our experiments ac"
D13-1140,W12-2702,0,0.0246096,"NCE training time is basically constant. 5 Related Work The problem of training with large vocabularies in NPLMs has received much attention. One strategy has been to restructure the network to be more hierarchical (Morin and Bengio, 2005; Mnih and Hinton, 2009) or to group words into classes (Le et al., 2011). Other strategies include restricting the vocabulary of the NPLM to a shortlist and reverting to a traditional n-gram LM for other words (Schwenk, 2004), and limiting the number of training examples using resampling (Schwenk and Gauvain, 2005) or selecting a subset of the training data (Schwenk et al., 2012). Our approach can be efficiently applied to large-scale tasks without limiting either the model or the data. 1391 We introduced a new variant of NPLMs that combines the network architecture of Bengio et al. (2003), rectified linear units (Nair and Hinton, 2010), and noise-contrastive estimation (Gutmann and Hyv¨arinen, 2010). This model is dramatically faster to train than previous neural LMs, and can be trained on a large corpus with a large vocabulary and directly integrated into the decoder of a MT system. Our experiments across four language pairs demonstrated improvements of up to 1.1 Bl"
D13-1140,W10-3815,0,0.0415058,"to the decoder yielded improvements of up to 0.6 Bleu. In one case (Spanish-English), integrated decoding scored higher than reranking on the development data but lower on the test data – perhaps due to the difference in domain between the two. On the other tasks, integrated decoding outperformed reranking. Speed comparison We measured the speed of training a NPLM by NCE, compared with MLE as implemented by the CSLM toolkit (Schwenk, 2013). We used the first 200k 4,000 3,000 2,000 1,000 Training time (s) NPLMs have previously been applied to MT, most notably feed-forward NPLMs (Schwenk, 2007; Schwenk, 2010) and RNN-LMs (Mikolov, 2012). However, their use in MT has largely been limited to reranking k-best lists for MT tasks with restricted vocabularies. Niehues and Waibel (2012) integrate a RBM-based language model directly into a decoder, but they only train the RBM LM on a small amount of data. To our knowledge, our approach is the first to integrate a large-vocabulary NPLM directly into a decoder for a large-scale MT task. CSLM NCE k = 1000 NCE k = 100 NCE k = 10 0 6 Conclusion 10 20 30 40 50 60 70 Vocabulary size (×1000) Figure 2: Noise contrastive estimation (NCE) is much faster, and much le"
D14-1197,N10-1083,0,0.102214,"Missing"
D14-1197,J93-2003,0,0.0520987,"in statistical machine translation. The task is to find word-level translation correspondences in parallel text. Formally, given a source sentence e consisting of words e1 , e2 , . . . , el and a target sentence f consisting of words f1 , f2 , . . . , fm , we want to infer an alignment a, a sequence of indices a1 , a2 , . . . , am which indicates, for each target word fi , the corresponding source word eai or a null word. Machine translation systems, including state-of-the-art systems, then use the word-aligned corpus to extract translation rules. The most widely used methods, the IBM models (Brown et al., 1993) and HMM (Vogel et al., 1996), define a probability distribution p(f , a |e) that models how each target word fi is generated from a source word eai with respect to an alignment a. The models, however, tend to misalign low-frequency words as they have insufficient training samples. The problem can get worse in low-resource languages. Two branches of research have tried to alleviate the problem. The † Most of the work reported here was performed while the second author was at the University of Southern California. first branch relies solely on the parallel data; however, additional assumptions"
D14-1197,P06-2014,0,0.0682537,"reported here was performed while the second author was at the University of Southern California. first branch relies solely on the parallel data; however, additional assumptions about the data are required. This includes, but is not limited to, applying prior distributions (Mermer and Sarac¸lar, 2011; Vaswani et al., 2012) or smoothing techniques (Zhang and Chiang, 2014). The other branch uses information learned from monolingual data, which is generally easier to acquire than parallel data. Previous work in this branch mostly involves applying syntactic constraints (Yamada and Knight, 2001; Cherry and Lin, 2006; Wang and Zong, 2013) and syntactic features (Toutanova et al., 2002) into the models. The use of syntactic relationships can, however, be limited between historically unrelated language pairs. Our motivation lies in the fact that a meaningful sentence is not merely a grammatically structured sentence; its semantics can provide insightful information for the task. For example, suppose that the models are uncertain about aligning e to f . If the models are informed that e is semantically related to e0 , f is semantically related to f 0 , and f 0 is a translation of e0 , it should intuitively i"
D14-1197,J07-2003,1,0.691261,"rce-limited) Arabic-English Baseline Our model Baseline (resource-limited) Our model (resource-limited) Precision Recall F1 65.2 71.4 76.9 79.7 56.1 66.5 BLEU METEOR Test 1 Test 2 Test 1 Test 2 70.6 75.3 29.4 29.9 26.7 27.0 29.7 30.0 28.5 28.8 68.1 74.4 61.5 70.2 23.6 24.7 20.3 21.6 26.0 26.8 24.4 25.6 56.1 60.0 79.0 82.4 65.6 69.5 37.7 38.2 36.2 36.8 31.1 31.6 30.9 31.4 56.7 59.4 76.1 80.7 65.0 68.4 34.1 35.0 33.0 33.8 27.9 28.7 27.7 28.6 Table 3: Experimental results. Our model improves alignments and translations on both language pairs. build a hierarchical phrase-based translation system (Chiang, 2007) trained using MIRA (Chiang, 2012). Then, we evaluated the translation quality using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014), and performed significance testing using bootstrap resampling (Koehn, 2004) with 1,000 samples. Under the resource-limited settings, our methods consistently show 1.1–1.3 BLEU (0.8–1.2 METEOR) improvements on Chinese-English and 0.8–0.9 BLEU (0.8–0.9 METEOR) improvements on Arabic-English, as shown in Table 3. These improvements are statistically significant (p < 0.01). On the full data, our method improves ChineseEnglish translation by 0.3–0"
D14-1197,W04-3250,0,0.0297401,"0.0 28.5 28.8 68.1 74.4 61.5 70.2 23.6 24.7 20.3 21.6 26.0 26.8 24.4 25.6 56.1 60.0 79.0 82.4 65.6 69.5 37.7 38.2 36.2 36.8 31.1 31.6 30.9 31.4 56.7 59.4 76.1 80.7 65.0 68.4 34.1 35.0 33.0 33.8 27.9 28.7 27.7 28.6 Table 3: Experimental results. Our model improves alignments and translations on both language pairs. build a hierarchical phrase-based translation system (Chiang, 2007) trained using MIRA (Chiang, 2012). Then, we evaluated the translation quality using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014), and performed significance testing using bootstrap resampling (Koehn, 2004) with 1,000 samples. Under the resource-limited settings, our methods consistently show 1.1–1.3 BLEU (0.8–1.2 METEOR) improvements on Chinese-English and 0.8–0.9 BLEU (0.8–0.9 METEOR) improvements on Arabic-English, as shown in Table 3. These improvements are statistically significant (p < 0.01). On the full data, our method improves ChineseEnglish translation by 0.3–0.5 BLEU (0.3 METEOR), which is unfortunately not statistically significant, and Arabic-English translation by 0.5– 0.6 BLEU (0.5 METEOR), which is statistically significant (p < 0.01). 5 Related work Most previous work on word al"
D14-1197,2011.mtsummit-papers.41,0,0.53294,"d work Most previous work on word alignment problems uses morphosyntactic-semantic features, for example, word stems, content words, orthography (De Gispert et al., 2006; Hermjakob, 2009). A variety of log-linear models have been proposed to incorporate these features (Dyer et al., 2011; BergKirkpatrick et al., 2010). These approaches usually require numerical optimization for discriminative training as well as language-specific engineering and may limit their applications to morphologically rich languages. A more semantic approach resorts to training word alignments on semantic word classes (Ma et al., 2011). However, the resulting alignments are only used to supplement the word alignments learned on lexical words. To our knowledge, our work, which directly incorporates semantic relationships in word alignment models, is novel. 6 Conclusion We have presented methods to extract word similarity from monolingual data and apply it to word alignment models. Our method can learn similar words and word similarity probabilities, which can be used inside any probability model and in many natural language processing tasks. We have demonstrated its effectiveness in statistical machine translation. The enhan"
D14-1197,P11-2032,0,0.240395,"Missing"
D14-1197,J03-1002,0,0.0369424,"The vocabulary V was the 30,000 most frequent words from each corpus 1 Catalog numbers: LDC2003E07, LDC2003E14, LDC2005E83, LDC2005T06, LDC2006E24, LDC2006E34, LDC2006E85, LDC2006E86, LDC2006E92, and LDC2006E93. 2 Excluding: United Nations proceedings (LDC2004E13), ISI Automatically Extracted Parallel Text (LDC2007E08), and Ummah newswire text (LDC2004T18) 11-20 21-30 31-40 41-50 Source word frequency 51-60 Figure 3: F1 scores for words binned by frequency. Our model gives the largest improvements for the lowest-frequency words. and the k = 10 most similar words were used. We modified GIZA++ (Och and Ney, 2003) to incorporate word similarity. For all experiments, we used the default configuration of GIZA++: 5 iterations each of IBM Model 1, 2, HMM, 3 and 4. We aligned the parallel texts in both forward and backward directions and symmetrized them using grow-diag-final-and (Koehn et al., 2005). We evaluated alignment quality using precision, recall, and F1. The results in Table 3 suggest that our modeling approach produces better word alignments. We found that our models not only learned smoother translation models for low frequency words but also ranked the conditional probabilities more accurately"
D14-1197,W14-3348,0,0.0148938,"79.7 56.1 66.5 BLEU METEOR Test 1 Test 2 Test 1 Test 2 70.6 75.3 29.4 29.9 26.7 27.0 29.7 30.0 28.5 28.8 68.1 74.4 61.5 70.2 23.6 24.7 20.3 21.6 26.0 26.8 24.4 25.6 56.1 60.0 79.0 82.4 65.6 69.5 37.7 38.2 36.2 36.8 31.1 31.6 30.9 31.4 56.7 59.4 76.1 80.7 65.0 68.4 34.1 35.0 33.0 33.8 27.9 28.7 27.7 28.6 Table 3: Experimental results. Our model improves alignments and translations on both language pairs. build a hierarchical phrase-based translation system (Chiang, 2007) trained using MIRA (Chiang, 2012). Then, we evaluated the translation quality using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014), and performed significance testing using bootstrap resampling (Koehn, 2004) with 1,000 samples. Under the resource-limited settings, our methods consistently show 1.1–1.3 BLEU (0.8–1.2 METEOR) improvements on Chinese-English and 0.8–0.9 BLEU (0.8–0.9 METEOR) improvements on Arabic-English, as shown in Table 3. These improvements are statistically significant (p < 0.01). On the full data, our method improves ChineseEnglish translation by 0.3–0.5 BLEU (0.3 METEOR), which is unfortunately not statistically significant, and Arabic-English translation by 0.5– 0.6 BLEU (0.5 METEOR), which is stati"
D14-1197,P02-1040,0,0.0893259,"Precision Recall F1 65.2 71.4 76.9 79.7 56.1 66.5 BLEU METEOR Test 1 Test 2 Test 1 Test 2 70.6 75.3 29.4 29.9 26.7 27.0 29.7 30.0 28.5 28.8 68.1 74.4 61.5 70.2 23.6 24.7 20.3 21.6 26.0 26.8 24.4 25.6 56.1 60.0 79.0 82.4 65.6 69.5 37.7 38.2 36.2 36.8 31.1 31.6 30.9 31.4 56.7 59.4 76.1 80.7 65.0 68.4 34.1 35.0 33.0 33.8 27.9 28.7 27.7 28.6 Table 3: Experimental results. Our model improves alignments and translations on both language pairs. build a hierarchical phrase-based translation system (Chiang, 2007) trained using MIRA (Chiang, 2012). Then, we evaluated the translation quality using BLEU (Papineni et al., 2002) and METEOR (Denkowski and Lavie, 2014), and performed significance testing using bootstrap resampling (Koehn, 2004) with 1,000 samples. Under the resource-limited settings, our methods consistently show 1.1–1.3 BLEU (0.8–1.2 METEOR) improvements on Chinese-English and 0.8–0.9 BLEU (0.8–0.9 METEOR) improvements on Arabic-English, as shown in Table 3. These improvements are statistically significant (p < 0.01). On the full data, our method improves ChineseEnglish translation by 0.3–0.5 BLEU (0.3 METEOR), which is unfortunately not statistically significant, and Arabic-English translation by 0.5"
D14-1197,P11-1042,0,0.227938,"3. These improvements are statistically significant (p < 0.01). On the full data, our method improves ChineseEnglish translation by 0.3–0.5 BLEU (0.3 METEOR), which is unfortunately not statistically significant, and Arabic-English translation by 0.5– 0.6 BLEU (0.5 METEOR), which is statistically significant (p < 0.01). 5 Related work Most previous work on word alignment problems uses morphosyntactic-semantic features, for example, word stems, content words, orthography (De Gispert et al., 2006; Hermjakob, 2009). A variety of log-linear models have been proposed to incorporate these features (Dyer et al., 2011; BergKirkpatrick et al., 2010). These approaches usually require numerical optimization for discriminative training as well as language-specific engineering and may limit their applications to morphologically rich languages. A more semantic approach resorts to training word alignments on semantic word classes (Ma et al., 2011). However, the resulting alignments are only used to supplement the word alignments learned on lexical words. To our knowledge, our work, which directly incorporates semantic relationships in word alignment models, is novel. 6 Conclusion We have presented methods to extr"
D14-1197,W02-1012,0,0.115672,"Missing"
D14-1197,N13-1073,0,0.023592,"00 most frequent words. We set k = 5 for illustration purposes. 3 Training ... Word alignment model In this section, we present our word alignment models by extending the standard IBM models. 1841 p(w0 |country) country 0.8363 region 0.0558 nation 0.0522 world 0.0282 city 0.0273 p(w0 |region) region 0.8338 area 0.0760 country 0.0524 province 0.0195 city 0.0181 p(w0 area region zone city areas |area) 0.8551 0.0524 0.0338 0.0326 0.0258 Model Table 1: Examples of word similarity The method can easily be applied to other related models, for example, the log-linear reparameterization of Model 2 by Dyer et al. (2013). Basically, all the IBM models involve modeling lexical translation probabilities p(f |e) which are parameterized as categorical distributions. IBM Model 1, for instance, is defined as p(f , a |e) ∝ m Y p(fi |eai ) = i=1 m Y t(fi |eai ) i=1 where each t(f |e) denotes the model parameters directly corresponding to p(f |e). Models 2–5 and the HMM-based model introduce additional components in order to capture word ordering and word fertility. However, they have p(f |e) in common. 3.1 Model To incorporate word similarity in word alignment models, we redefine the lexical translation probabilities"
D14-1197,P12-1033,1,0.888868,"from a source word eai with respect to an alignment a. The models, however, tend to misalign low-frequency words as they have insufficient training samples. The problem can get worse in low-resource languages. Two branches of research have tried to alleviate the problem. The † Most of the work reported here was performed while the second author was at the University of Southern California. first branch relies solely on the parallel data; however, additional assumptions about the data are required. This includes, but is not limited to, applying prior distributions (Mermer and Sarac¸lar, 2011; Vaswani et al., 2012) or smoothing techniques (Zhang and Chiang, 2014). The other branch uses information learned from monolingual data, which is generally easier to acquire than parallel data. Previous work in this branch mostly involves applying syntactic constraints (Yamada and Knight, 2001; Cherry and Lin, 2006; Wang and Zong, 2013) and syntactic features (Toutanova et al., 2002) into the models. The use of syntactic relationships can, however, be limited between historically unrelated language pairs. Our motivation lies in the fact that a meaningful sentence is not merely a grammatically structured sentence;"
D14-1197,D09-1024,0,0.0677581,"hinese-English and 0.8–0.9 BLEU (0.8–0.9 METEOR) improvements on Arabic-English, as shown in Table 3. These improvements are statistically significant (p < 0.01). On the full data, our method improves ChineseEnglish translation by 0.3–0.5 BLEU (0.3 METEOR), which is unfortunately not statistically significant, and Arabic-English translation by 0.5– 0.6 BLEU (0.5 METEOR), which is statistically significant (p < 0.01). 5 Related work Most previous work on word alignment problems uses morphosyntactic-semantic features, for example, word stems, content words, orthography (De Gispert et al., 2006; Hermjakob, 2009). A variety of log-linear models have been proposed to incorporate these features (Dyer et al., 2011; BergKirkpatrick et al., 2010). These approaches usually require numerical optimization for discriminative training as well as language-specific engineering and may limit their applications to morphologically rich languages. A more semantic approach resorts to training word alignments on semantic word classes (Ma et al., 2011). However, the resulting alignments are only used to supplement the word alignments learned on lexical words. To our knowledge, our work, which directly incorporates seman"
D14-1197,2005.iwslt-1.8,0,0.0200942,"tracted Parallel Text (LDC2007E08), and Ummah newswire text (LDC2004T18) 11-20 21-30 31-40 41-50 Source word frequency 51-60 Figure 3: F1 scores for words binned by frequency. Our model gives the largest improvements for the lowest-frequency words. and the k = 10 most similar words were used. We modified GIZA++ (Och and Ney, 2003) to incorporate word similarity. For all experiments, we used the default configuration of GIZA++: 5 iterations each of IBM Model 1, 2, HMM, 3 and 4. We aligned the parallel texts in both forward and backward directions and symmetrized them using grow-diag-final-and (Koehn et al., 2005). We evaluated alignment quality using precision, recall, and F1. The results in Table 3 suggest that our modeling approach produces better word alignments. We found that our models not only learned smoother translation models for low frequency words but also ranked the conditional probabilities more accurately with respect to the correct translations. To illustrate this, we categorized the alignment links from the Chinese-English low-resource experiment into bins with respect to the English source word frequency and individually evaluated them. As shown in Figure 3, the gain for low frequency"
D14-1197,P07-2045,0,0.00671324,"ect to the English source word frequency and individually evaluated them. As shown in Figure 3, the gain for low frequency words is particularly large. 4.2 Translation experiments We also ran end-to-end translation experiments. For both languages, we used subsets of the NIST 2004 and 2006 test sets as development data. We used two different data sets as test data: different subsets of the NIST 2004 and 2006 test sets (called Test 1) and the NIST 2008 test sets (called Test 2). We trained a 5-gram language model on the Xinhua portion of English Gigaword (LDC2007T07). We used the Moses toolkit (Koehn et al., 2007) to 1843 Model Chinese-English Baseline Our model Baseline (resource-limited) Our model (resource-limited) Arabic-English Baseline Our model Baseline (resource-limited) Our model (resource-limited) Precision Recall F1 65.2 71.4 76.9 79.7 56.1 66.5 BLEU METEOR Test 1 Test 2 Test 1 Test 2 70.6 75.3 29.4 29.9 26.7 27.0 29.7 30.0 28.5 28.8 68.1 74.4 61.5 70.2 23.6 24.7 20.3 21.6 26.0 26.8 24.4 25.6 56.1 60.0 79.0 82.4 65.6 69.5 37.7 38.2 36.2 36.8 31.1 31.6 30.9 31.4 56.7 59.4 76.1 80.7 65.0 68.4 34.1 35.0 33.0 33.8 27.9 28.7 27.7 28.6 Table 3: Experimental results. Our model improves alignments a"
D14-1197,D13-1140,1,0.941451,"ext represented by a sequence w1 , w2 , . . . , w2n consisting of n word tokens on the left and n word tokens on the right of w, excluding w. The submodel p(c |w) can be a categorical distribution. However, modeling the word context model, p(w0 |c), as a categorical distribution would cause severe overfitting, because the number of all possible contexts is |V |2n , which is exponential in the length of the context. We therefore parameterize it using a feedforward neural network as shown in Figure 1, since the structure has been shown to be effective for language modeling (Bengio et al., 2006; Vaswani et al., 2013). The input to the network is a one-hot representation of each word in c, where the special symbols <s>, </s>, <unk> are reserved for sentence beginning, sentence ending, and words not in the vocabulary. There is an output node for each w0 ∈ V , whose activation is p(w0 |c). Following Bengio et al. (2006), the network uses a shared linear projection matrix to the input embedding layer, which allows information sharing among the context words and also substantially reduces the number of parameters. The input embedding layer has a dimensionality of 150 for each input word. The network uses two h"
D14-1197,C96-2141,0,0.703862,"ation. The task is to find word-level translation correspondences in parallel text. Formally, given a source sentence e consisting of words e1 , e2 , . . . , el and a target sentence f consisting of words f1 , f2 , . . . , fm , we want to infer an alignment a, a sequence of indices a1 , a2 , . . . , am which indicates, for each target word fi , the corresponding source word eai or a null word. Machine translation systems, including state-of-the-art systems, then use the word-aligned corpus to extract translation rules. The most widely used methods, the IBM models (Brown et al., 1993) and HMM (Vogel et al., 1996), define a probability distribution p(f , a |e) that models how each target word fi is generated from a source word eai with respect to an alignment a. The models, however, tend to misalign low-frequency words as they have insufficient training samples. The problem can get worse in low-resource languages. Two branches of research have tried to alleviate the problem. The † Most of the work reported here was performed while the second author was at the University of Southern California. first branch relies solely on the parallel data; however, additional assumptions about the data are required."
D14-1197,Q13-1024,0,0.0178797,"ormed while the second author was at the University of Southern California. first branch relies solely on the parallel data; however, additional assumptions about the data are required. This includes, but is not limited to, applying prior distributions (Mermer and Sarac¸lar, 2011; Vaswani et al., 2012) or smoothing techniques (Zhang and Chiang, 2014). The other branch uses information learned from monolingual data, which is generally easier to acquire than parallel data. Previous work in this branch mostly involves applying syntactic constraints (Yamada and Knight, 2001; Cherry and Lin, 2006; Wang and Zong, 2013) and syntactic features (Toutanova et al., 2002) into the models. The use of syntactic relationships can, however, be limited between historically unrelated language pairs. Our motivation lies in the fact that a meaningful sentence is not merely a grammatically structured sentence; its semantics can provide insightful information for the task. For example, suppose that the models are uncertain about aligning e to f . If the models are informed that e is semantically related to e0 , f is semantically related to f 0 , and f 0 is a translation of e0 , it should intuitively increase the probabilit"
D14-1197,P01-1067,0,0.233093,". The † Most of the work reported here was performed while the second author was at the University of Southern California. first branch relies solely on the parallel data; however, additional assumptions about the data are required. This includes, but is not limited to, applying prior distributions (Mermer and Sarac¸lar, 2011; Vaswani et al., 2012) or smoothing techniques (Zhang and Chiang, 2014). The other branch uses information learned from monolingual data, which is generally easier to acquire than parallel data. Previous work in this branch mostly involves applying syntactic constraints (Yamada and Knight, 2001; Cherry and Lin, 2006; Wang and Zong, 2013) and syntactic features (Toutanova et al., 2002) into the models. The use of syntactic relationships can, however, be limited between historically unrelated language pairs. Our motivation lies in the fact that a meaningful sentence is not merely a grammatically structured sentence; its semantics can provide insightful information for the task. For example, suppose that the models are uncertain about aligning e to f . If the models are informed that e is semantically related to e0 , f is semantically related to f 0 , and f 0 is a translation of e0 , i"
D14-1197,P14-1072,1,0.830303,"nment a. The models, however, tend to misalign low-frequency words as they have insufficient training samples. The problem can get worse in low-resource languages. Two branches of research have tried to alleviate the problem. The † Most of the work reported here was performed while the second author was at the University of Southern California. first branch relies solely on the parallel data; however, additional assumptions about the data are required. This includes, but is not limited to, applying prior distributions (Mermer and Sarac¸lar, 2011; Vaswani et al., 2012) or smoothing techniques (Zhang and Chiang, 2014). The other branch uses information learned from monolingual data, which is generally easier to acquire than parallel data. Previous work in this branch mostly involves applying syntactic constraints (Yamada and Knight, 2001; Cherry and Lin, 2006; Wang and Zong, 2013) and syntactic features (Toutanova et al., 2002) into the models. The use of syntactic relationships can, however, be limited between historically unrelated language pairs. Our motivation lies in the fact that a meaningful sentence is not merely a grammatically structured sentence; its semantics can provide insightful information"
D15-1107,P07-2045,0,0.0076148,"1 λ 0 (none) 0.1 λ ≤ 0.01 λ = 0.1 λ=1 500 0 2 4 6 8 0.01) or pruning first and then fitting (λ = 1). We can also visualize the weight matrix itself over time (Figure 7), for λ = 0.1. It is striking that although this setting fits the model and prunes it at the same time, as argued above, by the first iteration it already seems to have decided roughly how many units it will eventually prune. 10 epoch 5.3 perplexity 50 λ = 0.01 λ = 0.1 λ=1 0 2 4 6 8 Evaluating on machine translation We also looked at the impact of our method on statistical machine translation systems. We used the Moses toolkit (Koehn et al., 2007) to build a phrase based machine translation system with a traditional 5-gram LM trained on the target side of our bitext. We augmented this system with neural LMs trained on the Europarl data and the Gigaword AFP data. Based on the results from the perplexity experiments, we looked at models both built with a λ = 0.1 regularizer, and without regularization (λ = 0). We built our system using the newscommentary dataset v8. We tuned our model using newstest13 and evaluated using newstest14. After standard cleaning and tokenization, there were 155k parallel sentences in the newscommentary dataset"
D15-1107,P14-1129,0,0.0618223,"Missing"
D15-1107,D13-1140,1,0.852047,"odels are simple but very effective in improving the performance of natural language systems. However, n-gram models suffer from some limitations, such as data sparsity and memory usage. As an alternative, researchers have begun exploring the use of neural networks for language modeling. For modeling n-grams, the most common approach is the feedforward network of Bengio et Introduction Neural networks have proven to be highly effective at many tasks in natural language. For example, neural language models and joint language/translation models improve machine translation quality significantly (Vaswani et al., 2013; Devlin et al., 2014). However, neural networks can be complicated to design and train well. Many decisions need to be made, and performance can be highly dependent on making them correctly. Yet the optimal settings are non-obvious and can be laborious to find, often requiring an extensive grid search involving numerous experiments. In this paper, we focus on the choice of the sizes of hidden layers. We introduce a method for automatically pruning out hidden layer units, by adding a sparsity-inducing regularizer that encourages units to deactivate if not needed, so that 908 Proceedings of the"
D15-1126,P07-1092,0,0.282244,"k dictionary entries to gain +0.5 Bleu over triangulation. (2) On Spanish-to-French via English we use only 4k sentence pairs to gain +0.7 Bleu over triangulation interpolated with a phrase table extracted from the same 4k sentence pairs. 1 Introduction Phrase-based statistical machine translation systems require considerable amounts of sourcetarget parallel data to produce good quality translation. However, large amounts of parallel data are available for only a fraction of language pairs, and mostly when one of the languages is English. Phrase table triangulation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2007) is a method for generating sourcetarget phrase tables without having access to any source-target parallel data. The intuition behind triangulation (and pivoting techniques in general) is the transitivity of translation: if a source language phrase s translates to a pivot language phrase p which in turn translates to a target language phrase t, then s should likely translate to t. Following this intuition, a triangulated sourcetarget phrase table Tˆ can be composed from a source-pivot and pivot-target phrase table (§2). However, the resulting triangulated phrase table ˆ T c"
D15-1126,N03-1017,0,0.0407955,"Missing"
D15-1126,P07-2045,0,0.00646636,"Missing"
D15-1126,W04-3250,0,0.161808,"Missing"
D15-1126,2005.mtsummit-papers.11,0,0.0966127,"nch Spanish Malagasy Data Fixing the pivot language to English, we applied our method on two data scenarios: 1. Spanish-to-French: two related languages used to simulate a low-resource setting. The baseline is phrase table interpolation (Eq. 3). 2. Malagasy-to-French: two unrelated languages for which we have a small dictionary, but no parallel corpus (aside from tuning and testing data). The baseline is triangulation alone (there is no source-target model to interpolate with). Table 1 lists some statistics of the bilingual data we used. European-language bitexts were extracted from Europarl (Koehn, 2005). For Malagasy-English, we used the Global Voices parallel data available online.1 The Malagasy-French dictionary was extracted from online resources2 and the small Malagasy-French tune/test sets were extracted3 from Global Voices. language pair sp-fr mg-fr sp-en mg-en en-fr lines of data train tune test 4k 1.5k 1.5k 1.1k 1.2k 1.2k 50k – – 100k – – 50k – – Table 2: Size of monolingual corpus per language as measured in number of tokens. 4.2 1 http://www.ark.cs.cmu.edu/global-voices http://motmalgache.org/bins/homePage 3 https://github.com/vchahun/gv-crawl 4 https://radimrehurek.com/gensim/mode"
D15-1126,N07-1061,0,0.0295566,"via English, we use only 1k dictionary entries to gain +0.5 Bleu over triangulation. (2) On Spanish-to-French via English we use only 4k sentence pairs to gain +0.7 Bleu over triangulation interpolated with a phrase table extracted from the same 4k sentence pairs. 1 Introduction Phrase-based statistical machine translation systems require considerable amounts of sourcetarget parallel data to produce good quality translation. However, large amounts of parallel data are available for only a fraction of language pairs, and mostly when one of the languages is English. Phrase table triangulation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2007) is a method for generating sourcetarget phrase tables without having access to any source-target parallel data. The intuition behind triangulation (and pivoting techniques in general) is the transitivity of translation: if a source language phrase s translates to a pivot language phrase p which in turn translates to a target language phrase t, then s should likely translate to t. Following this intuition, a triangulated sourcetarget phrase table Tˆ can be composed from a source-pivot and pivot-target phrase table (§2). However, the resulting triangul"
D15-1126,P07-1108,0,0.189918,"gain +0.5 Bleu over triangulation. (2) On Spanish-to-French via English we use only 4k sentence pairs to gain +0.7 Bleu over triangulation interpolated with a phrase table extracted from the same 4k sentence pairs. 1 Introduction Phrase-based statistical machine translation systems require considerable amounts of sourcetarget parallel data to produce good quality translation. However, large amounts of parallel data are available for only a fraction of language pairs, and mostly when one of the languages is English. Phrase table triangulation (Utiyama and Isahara, 2007; Cohn and Lapata, 2007; Wu and Wang, 2007) is a method for generating sourcetarget phrase tables without having access to any source-target parallel data. The intuition behind triangulation (and pivoting techniques in general) is the transitivity of translation: if a source language phrase s translates to a pivot language phrase p which in turn translates to a target language phrase t, then s should likely translate to t. Following this intuition, a triangulated sourcetarget phrase table Tˆ can be composed from a source-pivot and pivot-target phrase table (§2). However, the resulting triangulated phrase table ˆ T contains many spuriou"
D16-1133,C14-1096,0,0.147395,"be annotated with translations than with transcriptions. This translated speech is a potentially valuable source of information – for example, for documenting endangered languages or for training speech translation systems. In language documentation, data is usable only if it is interpretable. To make a collection of speech data usable for future studies of the language, something resembling interlinear glossed text (transcription, morphological analysis, word glosses, free translation) would be needed at minimum. New technologies are being developed to facilitate collection of translations (Bird et al., 2014), and there already exist recent examples of parallel speech collection efforts focused on endangered languages (Blachon et al., 2016; Adda et al., 2016). As for the other annotation layers, one might hope that a first pass could be done automatically. A first step towards this goal would be to automatically align spoken words with their translations, capturing information similar to that captured by word glosses. In machine translation, statistical models have traditionally required alignments between the source and target languages as the first step of training. Therefore, producing alignmen"
D16-1133,J93-2003,0,0.0649211,"rforms both a naive but strong baseline and a neural model (Duong et al., 2016). 1 https://www3.nd.edu/∼aanastas/griko/griko-data.tar.gz 1255 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1255–1263, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics 2 Background In this section, we briefly describe the existing models that the two components of our model are based on. In the next section, we will describe how we adapt and combine them to the present task. 2.1 IBM Model 2 and fast_align The IBM translation models (Brown et al., 1993) aim to model the distribution p(e |f) for an English sentence e = e1 · · · el , given a French sentence f = f1 · · · em . They all introduce a hidden variable a = a1 · · · al that gives the position of the French word to which each English word is aligned. The general form of IBM Models 1, 2 and fast_align is p(e, a |f) = p(l) l Y i=1 t(ei |fai ) δ(ai |i, l, m) where t(e |f ) is the probability of translating French word f to English word e, and δ(ai = j |i, l, m) is the probability of aligning the i-th English word with the j-th French word. In Model 1, δ is uniform; in Model 2, it is a cate"
D16-1133,N16-1109,1,0.904362,"2 (Dyer et al., 2013), more commonly known as fast_align, and k-means clustering using Dynamic Time Warping (Berndt and Clifford, 1994) as a distance measure. The two components are trained jointly using expectation-maximization. We experiment on two language pairs. One is Spanish-English, using the CALLHOME and Fisher corpora. The other is Griko-Italian; Griko is an endangered language for which we created (and make freely available)1 gold-standard translations and word alignments (Lekakou et al., 2013). In all cases, our model outperforms both a naive but strong baseline and a neural model (Duong et al., 2016). 1 https://www3.nd.edu/∼aanastas/griko/griko-data.tar.gz 1255 Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing, pages 1255–1263, c Austin, Texas, November 1-5, 2016. 2016 Association for Computational Linguistics 2 Background In this section, we briefly describe the existing models that the two components of our model are based on. In the next section, we will describe how we adapt and combine them to the present task. 2.1 IBM Model 2 and fast_align The IBM translation models (Brown et al., 1993) aim to model the distribution p(e |f) for an English senten"
D16-1133,N13-1073,0,0.16058,". A first step towards this goal would be to automatically align spoken words with their translations, capturing information similar to that captured by word glosses. In machine translation, statistical models have traditionally required alignments between the source and target languages as the first step of training. Therefore, producing alignments between speech and text would be a natural first step towards MT systems operating directly on speech. We present a model that, in order to learn such alignments, adapts and combines two components: Dyer et al.’s reparameterization of IBM Model 2 (Dyer et al., 2013), more commonly known as fast_align, and k-means clustering using Dynamic Time Warping (Berndt and Clifford, 1994) as a distance measure. The two components are trained jointly using expectation-maximization. We experiment on two language pairs. One is Spanish-English, using the CALLHOME and Fisher corpora. The other is Griko-Italian; Griko is an endangered language for which we created (and make freely available)1 gold-standard translations and word alignments (Lekakou et al., 2013). In all cases, our model outperforms both a naive but strong baseline and a neural model (Duong et al., 2016)."
D16-1133,P12-1018,0,0.0151491,"erparameters p0 , λ, and µ are not learned. We simply set p0 to zero (disallowing unaligned target words) and set µ as described above. For λ we perform a grid search over candidate values to maximize the alignment F-score on the development set. We obtain the best scores with λ = 0.5. 5 Related Work A first step towards modelling parallel speech can be performed by modelling phone-to-word alignment, instead of directly working on continuous speech. For example, Stahlberg et al. (2012) extend IBM Model 3 to align phones to words in order to build cross-lingual pronunciation lexicons. Pialign (Neubig et al., 2012) aligns characters and can be applied equally well to phones. Duong et al. (2016) use an extension of the neural attentional model of Bahdanau et al. (2015) for aligning phones to words and speech to words; we discuss this model below in Section 6.2. There exist several supervised approaches that attempt to integrate speech recognition and machine translation. However, they rely heavily on the abundance of training data, pronunciation lexicons, or language models, and therefore cannot be applied in a low- or zero-resource setting. A task somewhat similar to ours, which operates at a monolingua"
D16-1133,2013.iwslt-papers.14,0,0.0995941,"eline methods, a naive baseline, and the model of Duong et al. (2016). 6.1 Data For each language pair, we require a sentencealigned parallel corpus of source-language speech and target-language text. A subset of these sentences should be annotated with span-to-word alignments for use as a gold standard. 6.1.1 Spanish-English For Spanish-English, we use the Spanish CALLHOME corpus (LDC96S35) and the Fisher corpus 1259 (LDC2010T04), which consist of telephone conversations between Spanish native speakers based in the US and their relatives abroad, together with English translations produced by Post et al. (2013). Spanish is obviously not a low-resource language, but we pretend that it is low-resource by not making use of any Spanish ASR or resources like transcribed speech or pronunciation lexicons. Since there do not exist gold standard alignments between the Spanish speech and English words, we use the “silver” standard alignments produced by Duong et al. (2016) for the CALLHOME corpus, and followed the same procedure for the Fisher corpus as well. In order to obtain them, they first used a forced aligner to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic to"
D16-1133,C96-2141,0,0.758629,"Missing"
D19-5625,P07-2045,0,0.0142672,"uited for different types or regularizers. Additionally, all of our experiments use the same batch size, which is also related to λ. Settings We looked at four different low-resource language pairs, running experiments in five directions: Arabic-English, English-Arabic, French-English, Hausa-English, and Tigrinya-English. The Arabic and French data comes from the IWSLT 2017 Evaluation Campaign (Mauro et al., 2012). The Hausa and Tigrinya data were provided by the LORELEI project with custom train/dev/test splits. For all languages, we tokenized and truecased the data using scripts from Moses (Koehn et al., 2007). For the Arabic systems, we translit5.1.3 Random search parameters As originally proposed, the Transformer network has 6 encoder layers, all identical, and 6 decoder layers, also identical. For our random search experiments, we sample the number of attention heads from {4, 8, 16} and the model dimension (dmodel ) from {128, 256, 512, 1024, 2048}. Diverging from most implementations of the Transformer, we do not require the same number of encoder and decoder layers, but instead sample each from {2, 4, 6, 8}. Within a layer, we also sample the size of the feed-forward network (FFN), vary2 https"
D19-5625,2012.eamt-1.60,0,0.0549276,"0.1, use label smoothed cross-entropy with value 0.1, and an early stopping criterion when the learning rate is smaller than 10−5 . All of our experiments were done using the Adam optimizer (Kingma and Ba, 2015), a learning rate of 10−4 , and dropout of 0.1. At test time, we decoded using a beam of 5 with length normalization (Boulanger-Lewandowski et al., 2013) and evaluate using case-sensitive, detokenized BLEU (Papineni et al., 2002). Size 234k 235k 45k 15k Table 1: Number of parallel sentences in training bitexts. The French-English and Arabic-English data is from the 2017 IWSLT campaign (Mauro et al., 2012). The much smaller Hausa-English and Tigrinya-English data is from the LORELEI project. and Bengio, 2012). In fact, Li and Talwalkar (2019) recently demonstrated that many architecture search methods do not beat a random baseline. In practice, randomly searching hyperparameter domains allows for an intuitive mixture of continuous and categorical hyperparameters with no constraints on differentiability (Maclaurin et al., 2015) or need to cast hyperparameter values into a single high-dimensional space to predict new values (Bergstra et al., 2011). 5 5.1.1 The originally proposed Transformer mode"
D19-5625,D15-1107,1,0.93111,"r Scheirer David Chiang Department of Computer Science and Engineering University of Notre Dame {kmurray4, jkinniso, tnguye28, walter.scheirer, dchiang}@nd.edu Abstract et al. ran experiments varying numerous hyperparameters of the Transformer, but only on highresource datasets among linguistically similar languages. Popel and Bojar (2018) explored ways to train Transformer networks, but only on a highresource dataset in one language pair. Less work has been devoted to finding best practices for smaller datasets and linguistically divergent language pairs. In this paper, we apply auto-sizing (Murray and Chiang, 2015), which is a type of architecture search conducted during training, to the Transformer. We show that it is effective on very lowresource datasets and can reduce model size significantly, while being substantially faster than other architecture search methods. We make three main contributions. Neural sequence-to-sequence models, particularly the Transformer, are the state of the art in machine translation. Yet these neural networks are very sensitive to architecture and hyperparameter settings. Optimizing these settings by grid or random search is computationally expensive because it requires m"
D19-5625,P02-1040,0,0.103915,"eme. All of our systems were run using subword units (BPE) with 16,000 merge operations on concatenated source and target training data (Sennrich and Haddow, 2016). We clip norms at 0.1, use label smoothed cross-entropy with value 0.1, and an early stopping criterion when the learning rate is smaller than 10−5 . All of our experiments were done using the Adam optimizer (Kingma and Ba, 2015), a learning rate of 10−4 , and dropout of 0.1. At test time, we decoded using a beam of 5 with length normalization (Boulanger-Lewandowski et al., 2013) and evaluate using case-sensitive, detokenized BLEU (Papineni et al., 2002). Size 234k 235k 45k 15k Table 1: Number of parallel sentences in training bitexts. The French-English and Arabic-English data is from the 2017 IWSLT campaign (Mauro et al., 2012). The much smaller Hausa-English and Tigrinya-English data is from the LORELEI project. and Bengio, 2012). In fact, Li and Talwalkar (2019) recently demonstrated that many architecture search methods do not beat a random baseline. In practice, randomly searching hyperparameter domains allows for an intuitive mixture of continuous and categorical hyperparameters with no constraints on differentiability (Maclaurin et al"
D19-5625,D17-1151,0,0.0275572,"e neurons in a network over the course of training. On very low-resource language pairs, we show that auto-sizing can improve BLEU scores by up to 3.9 points while removing one-third of the parameters from the model. 1 Introduction Encoder-decoder based neural network models are the state-of-the-art in machine translation. However, these models are very dependent on selecting optimal hyperparameters and architectures. This problem is exacerbated in very low-resource data settings where the potential to overfit is high. Unfortunately, these searches are computationally expensive. For instance, Britz et al. (2017) used over 250,000 GPU hours to compare various recurrent neural network based encoders and decoders for machine translation. Strubell et al. (2019) demonstrated the neural architecture search for a large NLP model emits over four times the carbon dioxide relative to a car over its entire lifetime. Unfortunately, optimal settings are highly dependent on both the model and the task, which means that this process must be repeated often. As a case in point, the Transformer architecture has become the best performing encoder-decoder model for machine translation (Vaswani et al., 2017), displacing"
D19-5625,E17-2025,0,0.058858,"Missing"
D19-5625,W16-2209,0,0.024627,"example, the English-German WMT ’14 Transformer-base model proposed in Vaswani et al. (2017) has more Random Search As an alternative to grid-based searches, random hyperparameter search has been demonstrated to be a strong baseline for neural network architecture searches as it can search between grid points to increase the size of the search space (Bergstra 233 Dataset Ara–Eng Fra–Eng Hau–Eng Tir–Eng erated the data using the Buckwalter transliteration scheme. All of our systems were run using subword units (BPE) with 16,000 merge operations on concatenated source and target training data (Sennrich and Haddow, 2016). We clip norms at 0.1, use label smoothed cross-entropy with value 0.1, and an early stopping criterion when the learning rate is smaller than 10−5 . All of our experiments were done using the Adam optimizer (Kingma and Ba, 2015), a learning rate of 10−4 , and dropout of 0.1. At test time, we decoded using a beam of 5 with length normalization (Boulanger-Lewandowski et al., 2013) and evaluate using case-sensitive, detokenized BLEU (Papineni et al., 2002). Size 234k 235k 45k 15k Table 1: Number of parallel sentences in training bitexts. The French-English and Arabic-English data is from the 20"
D19-5625,P16-1162,0,0.130803,"Missing"
D19-5625,P19-1355,0,0.0386297,"o 3.9 points while removing one-third of the parameters from the model. 1 Introduction Encoder-decoder based neural network models are the state-of-the-art in machine translation. However, these models are very dependent on selecting optimal hyperparameters and architectures. This problem is exacerbated in very low-resource data settings where the potential to overfit is high. Unfortunately, these searches are computationally expensive. For instance, Britz et al. (2017) used over 250,000 GPU hours to compare various recurrent neural network based encoders and decoders for machine translation. Strubell et al. (2019) demonstrated the neural architecture search for a large NLP model emits over four times the carbon dioxide relative to a car over its entire lifetime. Unfortunately, optimal settings are highly dependent on both the model and the task, which means that this process must be repeated often. As a case in point, the Transformer architecture has become the best performing encoder-decoder model for machine translation (Vaswani et al., 2017), displacing RNN-based models (Bahdanau et al., 2015) along with much conventional wisdom about how to train such models. Vaswani 1. We demonstrate the effective"
D19-5634,K16-1029,0,0.0223017,"NMT models such as Sutskever et al. (2014) have most of their parameters in the embedding layers, but the transformer has a larger percentage of the model in the actual encoder and decoder layers. Though the group regularizers of auto-sizing can be applied to any parameter matrix, here we focus on the parameter matrices within the encoder and decoder layers. We note that there has been some work recently on shrinking networks through pruning. However, these differ from auto-sizing as they frequently require an arbitrary threshold and are not included during the training process. For instance, See et al. (2016) prunes networks based off a variety of thresholds and then retrains a model. Voita et al. (2019) also look at pruning, but of attention heads specifically. They do this through a relaxation of an `0 regularizer in order to make it differentiable. This allows them to not need to use a proximal step. This method too starts with pretrained model and then continues training. Michel et al. (2019) also look at pruning attention heads in the transformer. However, they too use thresholding, but only apply it at test time. Auto-sizing does not require a thresholding value, nor does it require a pre-tr"
D19-5634,W18-1819,0,0.0296201,"x decoder layers. Our model dimension was 512 and we used 8 attention heads. The feed-forward network subcomponents were of size 2048. All of our systems were run using subword units (BPE) with 32,000 merge operations on concatenated source and target training data (Sennrich and Haddow, 2016). We clip norms at 0.1, use label smoothed cross-entropy with value 0.1, and an early stopping criterion when the learning rate is smaller than 10−5 . We used the Adam optimizer (Kingma and Ba, 2015), a learning rate of 10−4 , and dropout of 0.1. Following recommendations in the fairseq and tensor2tensor (Vaswani et al., 2018) code bases, we apply layer normalization before a subcomponent as opposed to after. At test time, we decoded using a beam of 5 with length normalization (Boulanger-Lewandowski et al., 2013) and evaluate using case-sensitive, tokenized BLEU (Papineni et al., 2002). For the auto-sizing experiments, we looked at both `2,1 and `∞,1 regularizers. We experimented over a range of regularizer coefficient strengths, λ, that control how large the proximal gradient step will be. Similar to Murray and Chiang (2015), but differing from Alvarez and Salzmann (2016), we use one value of λ for all parameter m"
D19-5634,P19-1580,0,0.0277066,"s, but the transformer has a larger percentage of the model in the actual encoder and decoder layers. Though the group regularizers of auto-sizing can be applied to any parameter matrix, here we focus on the parameter matrices within the encoder and decoder layers. We note that there has been some work recently on shrinking networks through pruning. However, these differ from auto-sizing as they frequently require an arbitrary threshold and are not included during the training process. For instance, See et al. (2016) prunes networks based off a variety of thresholds and then retrains a model. Voita et al. (2019) also look at pruning, but of attention heads specifically. They do this through a relaxation of an `0 regularizer in order to make it differentiable. This allows them to not need to use a proximal step. This method too starts with pretrained model and then continues training. Michel et al. (2019) also look at pruning attention heads in the transformer. However, they too use thresholding, but only apply it at test time. Auto-sizing does not require a thresholding value, nor does it require a pre-trained model. Of particular interest are the large, positionwise feed-forward networks in each enc"
D19-5634,D15-1107,1,0.931339,"er. Whereas the most common applications of regularization will act over parameters individually, a group regularizer works over groupings of parameters. For instance, applying a sparsity inducing regularizer to a two-dimensional parameter tensor will encourage individual values to be driven to 0.0. A sparsity-inducing group regularizer will act over defined sub-structures, such as entire rows or columns, driving the entire groups to zero. Depending on model specifications, one row or column of a tensor in a neural network can correspond to one neuron in the model. Following the discussion of Murray and Chiang (2015) and Murray et al. (2019), auto-sizing works by training a neural network while using a regularizer to prune units from the network, minimizing: X L=− log P(e |f ; W) + λR(kWk). f, e in data W are the parameters of the model and R is a reg297 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 297–301 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d encoder and the decoder consist of stacked selfattention layers. The multi-head attention uses two affine transformations, followed by a soft"
D19-5634,D19-5625,1,0.917526,"pplications of regularization will act over parameters individually, a group regularizer works over groupings of parameters. For instance, applying a sparsity inducing regularizer to a two-dimensional parameter tensor will encourage individual values to be driven to 0.0. A sparsity-inducing group regularizer will act over defined sub-structures, such as entire rows or columns, driving the entire groups to zero. Depending on model specifications, one row or column of a tensor in a neural network can correspond to one neuron in the model. Following the discussion of Murray and Chiang (2015) and Murray et al. (2019), auto-sizing works by training a neural network while using a regularizer to prune units from the network, minimizing: X L=− log P(e |f ; W) + λR(kWk). f, e in data W are the parameters of the model and R is a reg297 Proceedings of the 3rd Workshop on Neural Generation and Translation (WNGT 2019), pages 297–301 c Hong Kong, China, November 4, 2019. 2019 Association for Computational Linguistics www.aclweb.org/anthology/D19-56%2d encoder and the decoder consist of stacked selfattention layers. The multi-head attention uses two affine transformations, followed by a softmax layer. Each layer has"
D19-5634,P02-1040,0,0.108303,"ta (Sennrich and Haddow, 2016). We clip norms at 0.1, use label smoothed cross-entropy with value 0.1, and an early stopping criterion when the learning rate is smaller than 10−5 . We used the Adam optimizer (Kingma and Ba, 2015), a learning rate of 10−4 , and dropout of 0.1. Following recommendations in the fairseq and tensor2tensor (Vaswani et al., 2018) code bases, we apply layer normalization before a subcomponent as opposed to after. At test time, we decoded using a beam of 5 with length normalization (Boulanger-Lewandowski et al., 2013) and evaluate using case-sensitive, tokenized BLEU (Papineni et al., 2002). For the auto-sizing experiments, we looked at both `2,1 and `∞,1 regularizers. We experimented over a range of regularizer coefficient strengths, λ, that control how large the proximal gradient step will be. Similar to Murray and Chiang (2015), but differing from Alvarez and Salzmann (2016), we use one value of λ for all parameter matrices in the network. We note that different regularization coefficient values are suited for different types or regularizers. Additionally, all of our experiments use the same batch size, which is also related to λ. W1 Figure 2: Auto-sizing FFN network. For a r"
D19-5634,W14-3302,0,\N,Missing
E06-1047,N04-4038,1,0.632821,"biguous in MSA. For example, the LA words  mn ‘from’ and  myn ‘who’ both are translated into an orthographically ambiguous form in MSA  mn ‘from’ or ‘who’. 5.1 Implementation Each word in the LA sentence is translated into a bag of MSA words, producing a sausage lattice. The lattice is scored and decoded using the SRILM toolkit with a trigram language model trained on 54 million MSA words from Arabic Gigaword (Graff, 2003). The text used for language modeling was tokenized to match the tokenization of the Arabic used in the ATB and LATB. The tokenization was done using the ASVM Toolkit (Diab et al., 2004). The 1-best path in the lattice is passed on to the Bikel parser (Bikel, 2002), which was trained on the MSA training ATB. Finally, the terminal nodes in the resulting parse structure are replaced with the original LA words. 5.2 Experimental Results Table 1 describes the results of the sentence transduction path on the development corpus (DEV) in different settings: using no POS tags in the input versus using gold POS tags in the input, and using SLXUN versus BLXUN. The baseline results are obtained by parsing the LA sentence directly using the MSA parser (with and without gold POS tags). The"
E06-1047,A00-1002,0,0.0265605,"ency trees for the MSA and LA sentences (not shown here for space considerations) are isomorphic. They differ only in the node labels. 5 Sentence Transduction In this approach, we parse an MSA translation of the LA sentence and then link the LA sentence to the MSA parse. Machine translation (MT) is not easy, especially when there are no MT resources available such as naturally occurring parallel text or transfer lexicons. However, for this task we have three encouraging insights. First, for really close languages it is possible to obtain better translation quality by means of simpler methods (Hajic et al., 2000). Second, suboptimal MSA output can still be helpful for the parsing task without necessarily being fluent or accurate (since our goal is parsing LA, not translating it to MSA). And finally, translation from LA to MSA is easier than from MSA to LA. This is a result of the availability of abundant resources for MSA as compared to LA: for example, text corpora and tree banks for 4 Levantine also has other negation markers that precede the verb, as well as the circumfi x m- -$. language modeling and a morphological generation system (Habash, 2004). One disadvantage of this approach is the lack of"
E06-1047,maamouri-etal-2006-developing,1,0.666828,"Missing"
E06-1047,J01-1004,1,0.72461,"can be thought of as a variant of the treebank-transduction approach in which the syntactic transformations are localized to elementary trees. Moreover, because a parsed MSA translation is produced as a byproduct, we can also think of this approach as being related to the sentence-transduction approach. 7.1 Preliminaries The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. In its present form, the formalism is tree-substitution grammar (Schabes, 1990) with an additional operation called sister-adjunction (Rambow et al., 2001). Because of space constraints, we omit discussion of the sister-adjunction operation in this paper. A tree-substitution grammar is a set of elementary trees. A frontier node labeled with a nonterminal label is called a substitution site. If an elementary tree has exactly one terminal symbol, that symbol is called its lexical anchor. A derivation starts with an elementary tree and proceeds by a series of composition operations. In the substitution operation, a substitution site is rewritten with an elementary tree with a matching root label. The final product is a tree with no more substitutio"
E06-1047,P00-1008,0,0.0658914,"Missing"
E06-1047,W04-3207,0,0.0165264,"LA and MSA (Section 4). We then proceed to discuss three approaches: sentence transduction, in which the LA sentence to be parsed is turned into an MSA sentence and then parsed with an MSA parser (Section 5); treebank transduction, in which the MSA treebank is turned into an LA treebank (Section 6); and grammar transduction, in which an MSA grammar is turned into an LA grammar which is then used for parsing LA (Section 7). We summarize and discuss the results in Section 8. 2 Related Work There has been a fair amount of interest in parsing one language using another language, see for example (Smith and Smith, 2004; Hwa et al., 2004) for recent work. Much of this work uses synchronized formalisms as do we in the grammar transduction approach. However, these approaches rely on parallel corpora. For MSA and its dialects, there are no naturally occurring parallel corpora. It is this fact that has led us to investigate the use of explicit linguistic knowledge to complement machine learning. We refer to additional relevant work in the appropriate sections. aries. The resulting development data comprises 1928 sentences and 11151 tokens (DEV). The test data comprises 2051 sentences and 10,644 tokens (TEST). Fo"
E06-1047,W00-1307,0,0.0120082,"stic synchronous TSG, using a straightforward generalization of the CKY and Viterbi algorithms, we obtain the highestprobability paired derivation which includes a parse for S on one side, and a parsed translation of S on the other side. It is also straightforward to calculate inside and outside probabilities for reestimation by Expectation-Maximization (EM). 7.2 An MSA-dialect synchronous grammar We now describe how we build our MSA-dialect synchronous grammar. As mentioned above, the MSA side of the grammar is extracted from the ATB in a process described by Chiang and others (Chiang, 2000; Xia et al., 2000; Chen, 2001). This process also gives us MSA-only substitution probabilities P (α |η). We then apply various transformation rules (described below) to the MSA elementary trees to produce a dialect grammar, at the same time assigning probabilities P (α0 |α). The synchronoussubstitution probabilities can then be estimated as: P (α, α0 |η, η 0 ) ≈ P (α |η)P (α0 |α) ≈ P (α |η)P (w 0 , t0 |w, t) P (¯ α0 |α ¯ , w0 , t0 , w, t) where w and t are the lexical anchor of α and its POS tag, and α ¯ is the equivalence class of α modulo lexical anchors and their POS tags. P (w0 , t0 |w, t) is assigned as d"
E06-1047,P00-1058,1,0.647244,"andwritten rules into dialect elementary trees to yield an MSAdialect synchronous grammar. This synchronous grammar can be used to parse new dialect sentences using statistics gathered from the MSA data. Thus this approach can be thought of as a variant of the treebank-transduction approach in which the syntactic transformations are localized to elementary trees. Moreover, because a parsed MSA translation is produced as a byproduct, we can also think of this approach as being related to the sentence-transduction approach. 7.1 Preliminaries The parsing model used is essentially that of Chiang (Chiang, 2000), which is based on a highly restricted version of tree-adjoining grammar. In its present form, the formalism is tree-substitution grammar (Schabes, 1990) with an additional operation called sister-adjunction (Rambow et al., 2001). Because of space constraints, we omit discussion of the sister-adjunction operation in this paper. A tree-substitution grammar is a set of elementary trees. A frontier node labeled with a nonterminal label is called a substitution site. If an elementary tree has exactly one terminal symbol, that symbol is called its lexical anchor. A derivation starts with an elemen"
E06-1047,W97-0119,0,\N,Missing
E06-1047,H05-1107,0,\N,Missing
E06-1047,J03-3002,0,\N,Missing
E06-1047,W02-2026,0,\N,Missing
E06-1047,N01-1020,0,\N,Missing
E06-1047,J90-2002,0,\N,Missing
E06-1047,P99-1067,0,\N,Missing
E06-1047,P95-1032,0,\N,Missing
E06-1047,W01-0713,0,\N,Missing
E06-1047,J00-2004,0,\N,Missing
E06-1047,N04-1034,0,\N,Missing
E17-1098,P16-1183,0,0.0714272,"kowicz et al., 2014) uses dense representations of finite automata, which is only appropriate if the automata are not too sparse (that is, most states can transition to most other states). But the automata used for natural language tend to be extremely large and sparse. In addition, the more recent work in this line assumes deterministic automata, but automata that model natural language ambiguity are generally nondeterministic. Previous work has been done on accelerating particular NLP tasks on GPUs: in machine translation, phrase-pair retrieval (He et al., 2013) and language model querying (Bogoychev and Lopez, 2016); parsing (Hall et al., 2014; Canny et al., 2013); and speech recognition (Kim et al., 2012). Our aim here is for a more general-purpose collection of algorithms for finite automata. Our work uses concepts from the work of Merrill et al. (2012), who show that GPUs can be used to accelerate breadth-first search in sparse graphs. Our approach is simple, but well-suited to the large, sparse automata that are often found in NLP applications. We show that it achieves a speedup of a factor of 5.2 on a GPU relative to a serial algorithm, and 6093 relative to OpenFST. 1044 Proceedings of the 15th Conf"
E17-1098,D13-1195,0,0.0251843,"e automata, which is only appropriate if the automata are not too sparse (that is, most states can transition to most other states). But the automata used for natural language tend to be extremely large and sparse. In addition, the more recent work in this line assumes deterministic automata, but automata that model natural language ambiguity are generally nondeterministic. Previous work has been done on accelerating particular NLP tasks on GPUs: in machine translation, phrase-pair retrieval (He et al., 2013) and language model querying (Bogoychev and Lopez, 2016); parsing (Hall et al., 2014; Canny et al., 2013); and speech recognition (Kim et al., 2012). Our aim here is for a more general-purpose collection of algorithms for finite automata. Our work uses concepts from the work of Merrill et al. (2012), who show that GPUs can be used to accelerate breadth-first search in sparse graphs. Our approach is simple, but well-suited to the large, sparse automata that are often found in NLP applications. We show that it achieves a speedup of a factor of 5.2 on a GPU relative to a serial algorithm, and 6093 relative to OpenFST. 1044 Proceedings of the 15th Conference of the European Chapter of the Association"
E17-1098,P02-1001,0,0.0928681,"ta use a variety of algorithms, but the most common are the Viterbi, forward, and backward algorithms. Several of these automata algorithms are related to one another and used for learning and inference. Speeding up these algorithms will allow faster training and development of large scale machine learning systems. The forward and backward algorithms are used to compute weights (Eq. 1), in left-to-right (Reading an input utterance from left to right) and rightto-left order, respectively. Their intermediate values are used to compute expected counts during training by expectation-maximization (Eisner, 2002). They can be computed by Algorithm 2. Algorithm 1 is one way of computing Viterbi using Equation (1). It is a straightforward algorithm, but the data structures require a brief explanation. Throughout this paper, we use zero-based indexing for arrays. Let m = |Σ|, and number the input symbols in Σ consecutively 0, . . . , m − 1. Then we can think of δ as a three-dimensional array. In general, this array is very sparse. We store it using a combination of compressed sparse row (CSR) format and coordinate (COO) format, as shown in Figure 2 where: • z is the number of transitions with nonzero wei"
E17-1098,P14-1020,0,0.225151,"sentations of finite automata, which is only appropriate if the automata are not too sparse (that is, most states can transition to most other states). But the automata used for natural language tend to be extremely large and sparse. In addition, the more recent work in this line assumes deterministic automata, but automata that model natural language ambiguity are generally nondeterministic. Previous work has been done on accelerating particular NLP tasks on GPUs: in machine translation, phrase-pair retrieval (He et al., 2013) and language model querying (Bogoychev and Lopez, 2016); parsing (Hall et al., 2014; Canny et al., 2013); and speech recognition (Kim et al., 2012). Our aim here is for a more general-purpose collection of algorithms for finite automata. Our work uses concepts from the work of Merrill et al. (2012), who show that GPUs can be used to accelerate breadth-first search in sparse graphs. Our approach is simple, but well-suited to the large, sparse automata that are often found in NLP applications. We show that it achieves a speedup of a factor of 5.2 on a GPU relative to a serial algorithm, and 6093 relative to OpenFST. 1044 Proceedings of the 15th Conference of the European Chapt"
E17-1098,N13-1033,0,0.0279452,"nd Fischer, 1980; Hillis and Steele, 1986; Mytkowicz et al., 2014) uses dense representations of finite automata, which is only appropriate if the automata are not too sparse (that is, most states can transition to most other states). But the automata used for natural language tend to be extremely large and sparse. In addition, the more recent work in this line assumes deterministic automata, but automata that model natural language ambiguity are generally nondeterministic. Previous work has been done on accelerating particular NLP tasks on GPUs: in machine translation, phrase-pair retrieval (He et al., 2013) and language model querying (Bogoychev and Lopez, 2016); parsing (Hall et al., 2014; Canny et al., 2013); and speech recognition (Kim et al., 2012). Our aim here is for a more general-purpose collection of algorithms for finite automata. Our work uses concepts from the work of Merrill et al. (2012), who show that GPUs can be used to accelerate breadth-first search in sparse graphs. Our approach is simple, but well-suited to the large, sparse automata that are often found in NLP applications. We show that it achieves a speedup of a factor of 5.2 on a GPU relative to a serial algorithm, and 609"
E17-1098,N03-1017,0,0.0152197,"rather than an input string. e/0.6 t /1 Parallel prefix sum the:l ca 6.1 the ca Other Approaches th e/0 .8 6 (b) chat </s&gt; (c) Figure 3: Example automata/transducers for (a) language model (b) translation model (c) input sentence. These three composed together form the transducer in Figure 1. models on 1k/10k/100k/150k lines of FrenchEnglish parallel data from the Europarl corpus and converted it into a finite automaton (see Figure 3a for a toy example). GIZA++ was used to word-align the same data and generate word-translation tables P( f |e) from the word alignments, as in lexical weighting (Koehn et al., 2003). We converted this table into a single-state FST (Figure 3b). The language model automaton and the translation table transducer were intersected to create a transducer similar to the one in Figure 1. For more details about the transducers (number of nodes, edges, and percentage of non-zero elements on the transducer) see Table 4. We tested on a subset of 100 sentences from the French corpus with lengths of up to 80 words. For each experimental setting, we ran on this set 1000 times and report the total time. Our experiments were run on three different systems: (1) a system with an Intel Core"
H01-1026,W00-1201,1,0.81787,"Missing"
H01-1026,P00-1058,1,0.892378,"Missing"
H01-1026,P97-1003,0,0.119704,"Missing"
H01-1026,P89-1015,0,0.0706941,"Missing"
H01-1026,W00-1306,0,0.0320163,"Missing"
H01-1026,P95-1037,0,0.189475,"Missing"
H01-1026,J93-2004,0,0.0308379,"Missing"
H01-1026,P95-1021,0,0.0479746,"Missing"
H01-1026,C92-2065,0,0.066503,"Missing"
H01-1026,C92-2066,0,0.124868,"Missing"
H01-1026,xia-etal-2000-developing,1,0.796706,"Missing"
H05-1098,P05-1033,1,0.428241,", and Analysis David Chiang, Adam Lopez, Nitin Madnani, Christof Monz, Philip Resnik, Michael Subotin Institute for Advanced Computer Studies (UMIACS) University of Maryland, College Park, MD 20742, USA {dchiang,alopez,nmadnani,christof,resnik,msubotin}@umiacs.umd.edu Abstract Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has been largely absent from the best performing machine translation systems in recent community-wide evaluations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of non"
H05-1098,W02-1039,0,0.35507,"evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a"
H05-1098,P02-1050,1,0.909542,"s. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG,"
H05-1098,P03-1040,0,0.00569351,"ase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident when looking at MT system output. Nonetheless, attempts to incorporate richer linguistic features have generally met with little success (Och et al., 2004a). Chiang (2005) introduces Hiero, a hierarchical phrase-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG, but it does not make any"
H05-1098,N03-1017,0,0.134136,"aper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based mo"
H05-1098,koen-2004-pharaoh,0,0.0221738,"e-based model for statistical machine translation. Hiero extends the standard, non-hierarchical notion of “phrases” to include nonterminal symbols, which permits it to capture both word-level and phrase-level reorderings within the same framework. The model has the formal structure of a synchronous CFG, but it does not make any commitment to a linguistically relevant analysis, and it does not require syntactically annotated training data. Chiang (2005) reported significant performance improvements in Chinese-English translation as compared with Pharaoh, a state-of-the-art phrase-based system (Koehn, 2004). In Section 2, we review the essential elements of Hiero. In Section 3 we describe extensions to this system, including new features involving named entities and numbers and support for a fourfold scale-up in training set size. Section 4 presents new evaluation results for Chinese-English as well as Arabic-English translation, obtained in the context of the 2005 NIST MT Eval exercise. In Section 5, we introduce a novel technique for fine-grained comparative analysis of MT systems, which we employ in analyzing differences between Hiero’s and Pharaoh’s translations. 2 Hiero Hiero is a stochasti"
H05-1098,P04-1077,0,0.0151475,"on the FBIS data; Hiero, with various combinations of the new features and the larger training data.4 This table also shows Hiero’s performance on the NIST 2005 MT evaluation task.5 The metric here is case-sensitive BLEU.6 Figure 2 shows the performance of two systems on Arabic in the NIST 2005 MT Evaluation task: DC, a phrase-based decoder for a model trained by Pharaoh, and Hiero. 5 Analysis Over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4 The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed because the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model. 5 Full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm. For this test, a phrase length limit of 15 was used during decoding. 6 For this task, the translation output was uppercased using the SRI-LM toolkit: essentially, it was decoded"
H05-1098,H05-2007,1,0.821428,"Missing"
H05-1098,W02-1018,0,0.0259841,"ew hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absence in phrase-based models is quite evident w"
H05-1098,N03-2021,0,0.177666,"Hiero, with various combinations of the new features and the larger training data.4 This table also shows Hiero’s performance on the NIST 2005 MT evaluation task.5 The metric here is case-sensitive BLEU.6 Figure 2 shows the performance of two systems on Arabic in the NIST 2005 MT Evaluation task: DC, a phrase-based decoder for a model trained by Pharaoh, and Hiero. 5 Analysis Over the last few years, several automatic metrics for machine translation evaluation have been introduced, largely to reduce the human cost of iterative system evaluation during the development cycle (Lin and Och, 2004; Melamed et al., 2003; Papineni et al., 2002). All are predicated on the concept 4 The third line, corresponding to the model without new features trained on the larger data, may be slightly depressed because the feature weights from the fourth line were used instead of doing minimum-error-rate training specially for this model. 5 Full results are available at http://www.nist.gov/ speech/tests/summaries/2005/mt05.htm. For this test, a phrase length limit of 15 was used during decoding. 6 For this task, the translation output was uppercased using the SRI-LM toolkit: essentially, it was decoded again using an HMM wh"
H05-1098,P00-1056,0,0.0589312,"igure 1: Example synchronous CFG • the lexical weights Pw (γ |α) and Pw (α |γ) (Koehn et al., 2003);1 2.1 Grammar • a phrase penalty exp(1); A synchronous CFG or syntax-directed transduction grammar (Lewis and Stearns, 1968) consists of pairs of CFG rules with aligned nonterminal symbols. We denote this alignment by coindexation with boxed numbers (Figure 1). A derivation starts with a pair of aligned start symbols, and proceeds by rewriting pairs of aligned nonterminal symbols using the paired rules (Figure 2). Training begins with phrase pairs, obtained as by Och, Koehn, and others: GIZA++ (Och and Ney, 2000) is used to obtain one-to-many word alignments in both directions, which are combined into a single set of refined alignments using the “final-and” method of Koehn et al. (2003); then those pairs of substrings that are exclusively aligned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: every phrase pair h f¯, e¯ i becomes a rule X → h f¯, e¯ i, and a phrase pair h f¯, e¯ i can be subtracted from a rule X → hγ1 f¯γ2 , α1 e¯ α2 i to form a new rule X → hγ1 X i γ2 , α1 X i α2 i, where i is an index not alread"
H05-1098,P02-1038,0,0.00566047,"onous CFG, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form a log-linear model learned by minimum-errorrate training; together with a modified CKY beamsearch decoder (similar to that of Wu (1996)). We describe these components in brief below. 779 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 779–786, Vancouver, October 2005. 2005 Association for Computational Linguistics 2.2 S → hS 1 X 2 , S 1 X 2 i Model The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. The weight of a derivation is PLM (e)λLM , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation. The weight of each rule is, in turn: Y (1) w(X → hγ, αi) = φi (X → hγ, αi)λi S → hX 1 , X 1 i X → hyu X 1 you X 2 , have X 2 with X 1 i X → hX 1 de X 2 , the X 2 that X 1 i X → hX 1 zhiyi, one of X 1 i X → hAozhou, Australiai X → hshi, isi i X → hshaoshu guojia, few countriesi where the φi are features defined on rules. The basic model uses the following features, analogous to Pharaoh’s default featu"
H05-1098,J04-4002,0,0.0105751,"luations. In this paper, we discuss a new hierarchical phrase-based statistical machine translation system (Chiang, 2005), presenting recent extensions to the original proposal, new evaluation results in a community-wide evaluation, and a novel technique for fine-grained comparative analysis of MT systems. 1 Introduction Hierarchical organization is a well known property of language, and yet the notion of hierarchical structure has, for the last several years, been absent from the best performing machine translation systems in community-wide evaluations. Statistical phrase-based models (e.g. (Och and Ney, 2004; Koehn et al., 2003; Marcu and Wong, 2002)) characterize a source sentence f as a flat partition of nonoverlapping subsequences, or “phrases”, f¯1 · · · f¯J , and the process of translation involves selecting target phrases e¯i corresponding to the f¯j and modifying their sequential order. The need for some way to model aspects of syntactic behavior, such as the tendency of constituents to move together as a unit, is widely recognized—the role of syntactic units is well attested in recent systematic studies of translation (Fox, 2002; Hwa et al., 2002; Koehn and Knight, 2003), and their absenc"
H05-1098,P03-1021,0,0.00789852,"verage. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 , X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i Figure 2: Example partial derivation of a synchronous CFG. The feature weights are learned by maximizing the BLEU score (Papineni et al., 2002) on held-out data, using minimum-error-rate training (Och, 2003) as implemented by Koehn. The implementation was slightly modified to ensure that the BLEU scoring matches NIST’s definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector. 3 Extensions In this section we describe our extensions to the base Hiero system that improve its performance significantly. First, we describe the addition of two new features to the Chinese model, in a manner similar to that of Och et al. (2004b); then we describe how we scaled the system up to a much larger training set. 3.1 New features supplementary ph"
H05-1098,P02-1040,0,0.109612,"nment, Koehn et al. take the maximum lexical weight; Hiero uses a weighted average. hS 1 , S 1 i ⇒ hS 2 X 3 , S 2 X 3 i ⇒ hS 4 X 5 X 3 , S 4 X 5 X 3 i ⇒ hX 6 X 5 X 3 , X 6 X 5 X 3 i ⇒ hAozhou X 5 X 3 , Australia X 5 X 3 i ⇒ hAozhou shi X 3 , Australia is X 3 i ⇒ hAozhou shi X 7 zhiyi, Australia is one of X 7 i ⇒ hAozhou shi X 8 de X 9 zhiyi, Australia is one of the X 9 that X 8 i ⇒ hAozhou shi yu X 1 you X 2 de X 9 zhiyi, Australia is one of the X 9 that have X 2 with X 1 i Figure 2: Example partial derivation of a synchronous CFG. The feature weights are learned by maximizing the BLEU score (Papineni et al., 2002) on held-out data, using minimum-error-rate training (Och, 2003) as implemented by Koehn. The implementation was slightly modified to ensure that the BLEU scoring matches NIST’s definition and that hypotheses in the n-best lists are merged when they have the same translation and the same feature vector. 3 Extensions In this section we describe our extensions to the base Hiero system that improve its performance significantly. First, we describe the addition of two new features to the Chinese model, in a manner similar to that of Och et al. (2004b); then we describe how we scaled the system up"
H05-1098,W96-0213,0,0.00943678,"t work has shown that applying statistical parsers to ungrammatical MT output is unreliable at best, with the parser often assigning unreasonable probabilities and incongruent structure (Yamada and Knight, 2002; Och et al., 2004a). Anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus. Word ngram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. First, we tagged the reference corpus with parts of speech. We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. Second, we computed the frequency freq(ti . . . t j ) of every possible tag sequence ti . . . t j in the reference corpus. Third, we computed the correspondence between each hypothesis sentence and each of its corresponding reference sentences using an approximation to maximum matching (Melamed et al., 2003). This algorithm provides a list of runs or contiguous sequences of words ei . . . e j in the reference that are also present in the hypothesis. (Note that runs"
H05-1098,2003.mtsummit-papers.53,0,0.0144904,"ed Chinese number/date translator, and created a new model feature for it. Again, the weight given to this module was optimized during minimum-error-rate training. In some cases we wrote the rules to provide multiple uniformlyweighted English translations for a Chinese phrase (for example, kå (bari) could become “the 8th” or “on the 8th”), allowing the language model to decide between the options. 3.2 The LDC Chinese-English named entity lists (900k entries) are a potentially valuable resource, but previous experiments have suggested that simply adding them to the training data does not help (Vogel et al., 2003). Instead, we placed them in a supplementary phrase-translation table, giving greater weight to phrases that occurred less frequently in the primary training data. For each entry h f, {e1 , . . . , en }i, we counted the number of times c( f ) that f appeared in the primary training data, and assigned the entry the weight c( f1)+1 , which was then distributed evenly among the supplementary phrase pairs {h f, ei i}. We then created a new model feature for named entities. When one of these 781 Scaling up training Chiang (2005) reports on experiments in ChineseEnglish translation using a model tra"
H05-1098,P96-1021,0,0.157884,"tion results for Chinese-English as well as Arabic-English translation, obtained in the context of the 2005 NIST MT Eval exercise. In Section 5, we introduce a novel technique for fine-grained comparative analysis of MT systems, which we employ in analyzing differences between Hiero’s and Pharaoh’s translations. 2 Hiero Hiero is a stochastic synchronous CFG, whose productions are extracted automatically from unannotated parallel texts, and whose rule probabilities form a log-linear model learned by minimum-errorrate training; together with a modified CKY beamsearch decoder (similar to that of Wu (1996)). We describe these components in brief below. 779 Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language c Processing (HLT/EMNLP), pages 779–786, Vancouver, October 2005. 2005 Association for Computational Linguistics 2.2 S → hS 1 X 2 , S 1 X 2 i Model The model is a log-linear model (Och and Ney, 2002) over synchronous CFG derivations. The weight of a derivation is PLM (e)λLM , the weighted language model probability, multiplied by the product of the weights of the rules used in the derivation. The weight of each rule is, in turn: Y (1) w"
H05-1098,J97-3002,0,0.094693,"ned to each other are extracted as phrase pairs. Then, synchronous CFG rules are constructed out of the initial phrase pairs by subtraction: every phrase pair h f¯, e¯ i becomes a rule X → h f¯, e¯ i, and a phrase pair h f¯, e¯ i can be subtracted from a rule X → hγ1 f¯γ2 , α1 e¯ α2 i to form a new rule X → hγ1 X i γ2 , α1 X i α2 i, where i is an index not already used. Various filters are also applied to reduce the number of extracted rules. Since one of these filters restricts the number of nonterminal symbols to two, our extracted grammar is equivalent to an inversion transduction grammar (Wu, 1997). 780 • a word penalty exp(l), where l is the number of terminals in α. The exceptions to the above are the two “glue” rules, which are the rules with left-hand side S in Figure 1. The second has weight one, and the first has weight w(S → hS 1 X 2 , S 1 X 2 i) = exp(−λg ), the idea being that parameter λg controls the model’s preference for hierarchical phrases over serial combination of phrases. Phrase translation probabilities are estimated by relative-frequency estimation. Since the extraction process does not generate a unique derivation for each training sentence pair, a distribution over"
H05-1098,P02-1039,0,0.0166923,"pect to syntax, although there remains a high degree of flexibility (Fox, 2002; Hwa et al., 2002). This suggests that in a comparative analysis of two MT systems, it may be useful to look for syntactic patterns that one system captures well in the target language and the other does not, using a syntax based metric. We propose to summarize reordering patterns using part-of-speech sequences. Unfortunately, recent work has shown that applying statistical parsers to ungrammatical MT output is unreliable at best, with the parser often assigning unreasonable probabilities and incongruent structure (Yamada and Knight, 2002; Och et al., 2004a). Anticipating that this would be equally problematic for part-of-speech tagging, we make the conservative choice to apply annotation only to the reference corpus. Word ngram correspondences with a reference translation are used to infer the part-of-speech tags for words in the system output. First, we tagged the reference corpus with parts of speech. We used MXPOST (Ratnaparkhi, 1996), and in order to discover more general patterns, we map the tag set down after tagging, e.g. NN, NNP, NNPS and NNS all map to NN. Second, we computed the frequency freq(ti . . . t j ) of ever"
H05-1098,N04-1021,0,\N,Missing
I17-2050,W14-4012,0,0.0616787,"Missing"
I17-2050,P07-2045,0,0.00920804,"the BPE merge rules to not only find the common subwords between two source languages but also ensure consistency between source and target segmentation among each language pair, we learn the rules from the union of source and target data of both the parent and child models. The rules are then used to segment the corpora. It is important to note that this results in a single vocabulary, used for both the source and target languages in both models. 4 Experiments We used Turkish-, Uzbek-, and Uyghur-English parallel texts from the LORELEI program. We tokenized all data using the Moses toolkit (Koehn et al., 2007); for Turkish-English experiments, we also truecased the data. For Uyghur-English, the word-based models were trained in the original Uyghur data written in Arabic script; for BPEbased systems, we transliterated it to Latin script as described above. For the word-based systems, we fixed the vocabulary size and replaced out-of-vocabulary s(e |f ) = lp(e) = 1 https://cis.temple.edu/˜anwar/code/ latin2uyghur.html log p(e |f ) lp(e) (5 + |e|)α . (5 + 1)α We set α = 0.8 for all of our experiments. 298 baseline BLEU size transfer BLEU size transfer+freeze BLEU size Tur-Eng word-based BPE 8.1 12.4 30"
I17-2050,W17-3204,0,0.0312335,"it effectively makes a random assignment of the parent-language word embeddings to child-language words. But if we assume that the parent and child lexicons are related, it should be beneficial to transfer source word embeddings from parent-language words to their child-language equivalents. Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods. However, it still lags behind other statistical methods on very lowresource language pairs (Zoph et al., 2016; Koehn and Knowles, 2017). A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009). However, adapting these resources is not trivial. NMT offers some simple ways of doing this. For example, Zoph et al. (2016) train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair. In particular, they showed that a French-English Thus, the problem amounts to finding a representation of the data that ensures a sufficient overlap between the vo"
I17-2050,D15-1166,0,0.0147145,"Taiwan, November 27 – December 1, 2017 2017 AFNLP English Turkish Uzbek clinic poliklinikte, poliklinie, polikliniine poliklinikasi, poliklinikaga, poliklinikalar parliament parlamentosuna, parlamentosundan, parlamentosu parlamentning, parlamentini, parlamentiga meningococcus meningokokuna, meningokosemi, meningokoklar meningokokk, meningokokkli, meningokokkning Table 1: Some examples of similar words in Turkish and Uzbek that share the same root. 2 2.1 Background Attentional Model We use the 2-layer, 512-hidden-unit global attentional model with general scoring function and input feeding by Luong et al. (2015). For the purposes of this paper, the most important detail of the model is that (as in many other models) the word types of both the source and target languages are mapped to vector representations called word embeddings, which are learned automatically with the rest of the model. 2.2 BPE 5k toks sents BPE 60k toks sents ×106 ×103 ×106 ×103 ×106 ×103 Uzb par Tur chd 1.5 0.9 102 56 2.4 1.5 92 50 1.9 1.2 103 57 Uzb par Uyg chd 1.5 1.7 102 82 2.4 2.1 90 77 2.0 2.0 103 88 Table 2: Number of tokens and sentences in our training data. guages with a very wide geographic distribution, from Turkey to"
I17-2050,D09-1141,0,0.0115336,"rent and child lexicons are related, it should be beneficial to transfer source word embeddings from parent-language words to their child-language equivalents. Introduction Neural machine translation (NMT) (Sutskever et al., 2014; Bahdanau et al., 2015) is rapidly proving itself to be a strong competitor to other statistical machine translation methods. However, it still lags behind other statistical methods on very lowresource language pairs (Zoph et al., 2016; Koehn and Knowles, 2017). A common strategy to improve learning of lowresource languages is to use resources from related languages (Nakov and Ng, 2009). However, adapting these resources is not trivial. NMT offers some simple ways of doing this. For example, Zoph et al. (2016) train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair. In particular, they showed that a French-English Thus, the problem amounts to finding a representation of the data that ensures a sufficient overlap between the vocabularies of the languages. To do this, we map the source languages to a common alphabet and use Byte Pair Encoding (BPE) (Senn"
I17-2050,P16-1162,0,0.160677,"009). However, adapting these resources is not trivial. NMT offers some simple ways of doing this. For example, Zoph et al. (2016) train a parent model on a (possibly unrelated) high-resource language pair, then use this model to initialize a child model which is further trained on a low-resource language pair. In particular, they showed that a French-English Thus, the problem amounts to finding a representation of the data that ensures a sufficient overlap between the vocabularies of the languages. To do this, we map the source languages to a common alphabet and use Byte Pair Encoding (BPE) (Sennrich et al., 2016) on the union of the vocabularies to increase the number of common subwords. In our experiments, we show that transfer learning helps word-based translation, but not always significantly. But when used on top of a much stronger BPE baseline, it yields larger and statistically significant improvements. Using Uzbek as a parent language and Turkish and Uyghur as child languages, we obtain improvements over BPE of 0.8 and 4.3 BLEU, respectively. 296 Proceedings of the The 8th International Joint Conference on Natural Language Processing, pages 296–301, c Taipei, Taiwan, November 27 – December 1, 2"
I17-2050,D16-1163,0,0.374355,"st language pair and transfer its parameters, including its source word embeddings, to another model and continue training on the second language pair. Our experiments show that transfer learning helps word-based translation only slightly, but when used on top of a much stronger BPE baseline, it yields larger improvements of up to 4.3 BLEU. 1 In this paper, we explore the opposite scenario, where the parent language pair is also lowresource, but related to the child language pair. We show that, at least in the case of three Turkic languages (Turkish, Uzbek, and Uyghur), the original method of Zoph et al. (2016) does not always work, but it is still possible to use the parent model to considerably improve the child model. The basic idea is to exploit the relationship between the parent and child language lexicons. Zoph et al.’s original method makes no assumption about the relatedness of the parent and child languages, so it effectively makes a random assignment of the parent-language word embeddings to child-language words. But if we assume that the parent and child lexicons are related, it should be beneficial to transfer source word embeddings from parent-language words to their child-language equ"
J07-2003,N04-1035,0,\N,Missing
J07-2003,N04-4026,0,\N,Missing
J07-2003,J93-2003,0,\N,Missing
J07-2003,C04-1073,0,\N,Missing
J07-2003,W05-1506,1,\N,Missing
J07-2003,P02-1040,0,\N,Missing
J07-2003,H05-1098,1,\N,Missing
J07-2003,P01-1067,0,\N,Missing
J07-2003,P05-1067,0,\N,Missing
J07-2003,H05-1095,0,\N,Missing
J07-2003,J04-4002,0,\N,Missing
J07-2003,P96-1021,0,\N,Missing
J07-2003,P05-1033,1,\N,Missing
J07-2003,P95-1037,0,\N,Missing
J07-2003,N03-1017,0,\N,Missing
J07-2003,P05-1034,0,\N,Missing
J07-2003,P02-1038,0,\N,Missing
J07-2003,P05-1069,0,\N,Missing
J07-2003,J97-3002,0,\N,Missing
J07-2003,P05-1066,0,\N,Missing
J07-2003,C92-3126,0,\N,Missing
J07-2003,N04-1033,0,\N,Missing
J07-2003,J99-4004,0,\N,Missing
J07-2003,P00-1056,0,\N,Missing
J07-2003,P03-1021,0,\N,Missing
J07-2003,2004.iwslt-evaluation.9,0,\N,Missing
J18-1005,P13-1023,0,0.134502,"nt of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences to these semantic graphs. The unification of various kinds of semantic annotation into a single representation, semantic graphs, and the creation of large, broad-coverage collections of these representations are very positive developments for r"
J18-1005,P99-1070,0,0.343387,"Missing"
J18-1005,W13-2322,0,0.530691,"process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov"
J18-1005,P91-1034,0,0.10154,"Missing"
J18-1005,J99-1004,0,0.0256352,"is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to provide a prior over semantic structures. A natural choice would be to adopt a similar log-linear framework: δ (ρ ) pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge, as in weighted context-free grammars (CFGs; Chi 1999), meaning that the corresponding probability distribution is not defined. More importantly, estimating the normalization constant is computationally difficult, whereas in the case of weighted CFGs it can be estimated relatively easily with an iterative numerical algorithm (Abney, McAllester, and Pereira 1999; Smith and Johnson 2007). A similar problem arises in Exponential Random Graph Models (Frank and Strauss 1986); the most common solution is to use Markov chain Monte Carlo (MCMC) methods (Snijders 2002). To train a model over DAGs, we can perform gradient ascent on the log likelihood: X LL"
J18-1005,P97-1003,0,0.185191,"parkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016),"
J18-1005,W95-0103,0,0.261819,"ther stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various seman"
J18-1005,P02-1001,0,0.117651,"iently using the forward-backward algorithm; for DAG automata, the expectation can be computed analogously. Algorithm 2 provides the bottom–up procedure for computing a chart of inside weights. If we compute weights in the derivation forest semiring (Goodman 1999), in which ⊗ creates an “and” node and ⊕ creates an “or” node, the resulting and/or graph has the same structure as a CFG parse forest generated by CKY, so we can simply run the inside-outside algorithm (Lari and Young 1990) on it to obtain the desired expectations. Alternatively, we could compute weights in the expectation semiring (Eisner 2002; Chiang 2012). Because the log-likelihood LL is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to provide a prior over semantic structures. A natural choice would be to adopt a similar log-linear framework: δ (ρ ) pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge,"
J18-1005,N16-1087,0,0.0374856,"niak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) conver"
J18-1005,P14-1134,0,0.0479906,"se representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equival"
J18-1005,P00-1065,1,0.624731,"Y-NC-ND 4.0) license Computational Linguistics Volume 44, Number 1 1. Introduction Statistical models of natural language semantics are making rapid progress. At the risk of oversimplifying, work in this area can be divided into two streams. One stream, semantic parsing (Mooney 2007), aims to map from sentences to logical forms that can be executed (for example, to query a knowledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible"
J18-1005,J99-4004,0,0.32362,"i=1 = N X i=1 148 ρ on Di Φ(ρi ) − Eρ|Di [Φ(ρ )]  since ∂δ(ρ ) = δ (ρ ) Φ (ρ ) ∂w (3) Chiang et al. Weighted DAG Automata for Semantic Graphs Unfortunately, we cannot derive a closed-form solution for the zeros of Equation (3). We therefore use gradient ascent. In CRF training for finite automata, the expectation in Equation (3) is computed efficiently using the forward-backward algorithm; for DAG automata, the expectation can be computed analogously. Algorithm 2 provides the bottom–up procedure for computing a chart of inside weights. If we compute weights in the derivation forest semiring (Goodman 1999), in which ⊗ creates an “and” node and ⊕ creates an “or” node, the resulting and/or graph has the same structure as a CFG parse forest generated by CKY, so we can simply run the inside-outside algorithm (Lari and Young 1990) on it to obtain the desired expectations. Alternatively, we could compute weights in the expectation semiring (Eisner 2002; Chiang 2012). Because the log-likelihood LL is concave (Boyd and Vandenberghe 2004), gradient ascent is guaranteed to converge to the unique global maximum. We may also wish to learn a distribution over the DAGs themselves—for example, in order to pro"
J18-1005,N06-2015,0,0.0282756,"rsimplifying, work in this area can be divided into two streams. One stream, semantic parsing (Mooney 2007), aims to map from sentences to logical forms that can be executed (for example, to query a knowledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations,"
J18-1005,P99-1069,0,0.188893,"d Let w ∈ R be a vector of feature weights, which are the parameters to be estimated. Then we can parameterize δ in terms of the features and feature weights: δ(t) = exp w · Φ(t) so that δ(ρ ) = exp w · Φ(ρ ) X exp w · Φ(ρ ) [[M]] (D) = run ρ on D To obtain a probability model of runs of M on D, we simply renormalize the run weights: δ (ρ ) [[M]] (D) Assume a set of training examples {(Di , ρi ) |1 ≤ i ≤ N}, where each example consists of a DAG Di and an associated run ρi . We can train the model by analogy with conditional random fields (CRFs), which are log-linear models on finite automata (Johnson et al. 1999; Lafferty, McCallum, and Pereira 2001). The training procedure is essentially gradient ascent on the log-likelihood, which is pM (ρ |D) = LL = N X log pM (ρi |Di ) i=1 = N X log δ(ρi ) − log [[M]] (Di )  i=1 The gradient of LL is: ∂LL = ∂w N  X i=1  ∂ log δ(ρ ) − ∂ log [[M]] (D ) i i ∂w ∂w N  X  1 1 ∂ ∂ = δ(ρi ) − [[M]] (Di ) δ(ρi ) ∂w [[M]] (Di ) ∂w i=1   N X X 1 ∂ δ ( ρ )  1 ∂ δ(ρi ) − = ∂w δ(ρi ) ∂w [[M]] (Di ) ρ on Di i=1   N X X δ ( ρ ) Φ(ρi ) − = Φ ( ρ ) [[M]] (Di ) i=1 = N X i=1 148 ρ on Di Φ(ρi ) − Eρ|Di [Φ(ρ )]  since ∂δ(ρ ) = δ (ρ ) Φ (ρ ) ∂w (3) Chiang et al. Weighted"
J18-1005,W15-4502,0,0.0159569,"ic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to"
J18-1005,N15-1114,0,0.0541523,"Missing"
J18-1005,J93-2004,0,0.0616988,"Missing"
J18-1005,S16-1166,0,0.0130652,"ch can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsin"
J18-1005,S15-2153,0,0.0866557,"Missing"
J18-1005,S14-2008,0,0.118213,"neration (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences to these semantic graphs. The unification of various kinds of semantic annotation into a single representation, semantic graphs, and the creation of large, broad-coverage collections of these representations are very positive developments for research in semantic processing. What is still missing—in our view—is a formal framework for creating, combining, and using models involving graphs that parallels those for strings and trees. Finite string automata and transducers served as"
J18-1005,N15-1119,0,0.0128014,"nsolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et al. 2015), and entity detection and linking (Li et al. 2015; Pan et al. 2015). But the AMR Bank is by no means the only resource of its kind. Others include the ¨ Prague Dependency Treebank (Bohmov´ a et al. 2003), DeepBank (Oepen and Lønning 2006), and Universal Conceptual Cognitive Annotation (Abend and Rappoport 2013). By and large, these resources are based on, or equivalent to, graphs, in which vertices stand for entities and edges stand for semantic relations among them. The Semantic Dependency Parsing task at SemEval 2014 and 2015 (Oepen et al. 2014, 2015) converted several such resources into a unified graph format and invited participants to map from sentences"
J18-1005,K15-1004,1,0.920031,"Missing"
J18-1005,P06-1055,0,0.0700356,"noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more recent work in semantic processing consolidates various semantics-related tasks into one. For example, the Abstract Meaning Representation (AMR) Bank (Banarescu et al. 2013) began as an effort to unify the various annotation layers of OntoNotes. It has driven the development of many systems, chiefly string-toAMR parsers like JAMR (Flanigan et al. 2014) and CAMR (Wang, Xue, and Pradhan 2015a,b), as well as many other systems submitted to the AMR Parsing task at SemEval 2016 (May 2016). AMRs have also been used for generation (Flanigan et al. 2016), summarization (Liu et"
J18-1005,W12-4209,0,0.402951,"lack a similar framework for learning and inferring semantic representations. Two such formalisms have recently been proposed for NLP: one is hyperedge replacement graph grammars, or HRGs (Bauderon and Courcelle 1987; Habel and Kreowski 1987; Habel 1992; Drewes, Kreowski, and Habel 1997), applied to AMR 120 Chiang et al. Weighted DAG Automata for Semantic Graphs ¨ parsing by various authors (Chiang et al. 2013; Peng, Song, and Gildea 2015; Bjorklund, Drewes, and Ericson 2016). The other formalism is directed acyclic graph (DAG) automata, defined by Kamimura and Slutzki (1981) and extended by Quernheim and Knight (2012). In this article, we study DAG automata in depth, with the goal of enabling efficient algorithms for natural language processing applications. After some background on the use of graph-based representations in natural language processing in Section 2, we define our variant of DAG automata in Section 3. We then show the following properties of our formalism: r r r Path languages are regular, as is desirable for a formal model of AMRs (Section 4.1). The class of hyperedge-replacement languages is closed under intersection with languages recognized by DAG automata (Section 4.2). Emptiness is dec"
J18-1005,W95-0107,0,0.44496,"o be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997; Petrov et al. 2006). In a similar way, more re"
J18-1005,W96-0213,0,0.301856,"wledge base); work in this stream tends to be on small, narrow-domain data sets like GeoQuery. The other stream aims for broader coverage, and historically tackled shallower, piecemeal tasks, like semantic role labeling (Gildea and Jurafsky 2000), word sense disambiguation (Brown et al. 1991), coreference resolution (Soon, Ng, and Lim 2001), and so on. Correspondingly, resources like OntoNotes (Hovy et al. 2006) provided separate resources for each of these tasks. This piecemeal situation parallels that of early work on syntactic parsing, which focused on subtasks like part-of-speech tagging (Ratnaparkhi 1996), noun-phrase chunking (Ramshaw and Marcus 1995), prepositional phrase attachment (Collins and Brooks 1995), and so on. As the field matured, these tasks were increasingly synthesized into a single process. This was made possible because of a single representation (phrase structure or dependency trees) that captures all of these phenomena; because of corpora annotated with these representations, like the Penn Treebank (Marcus, Marcinkiewicz, and Santorini 1993); and because of formalisms, like context-free grammars, which can model these representations practically (Charniak 1997; Collins 1997"
J18-1005,J07-4003,0,0.0362617,"pM (D, ρ ) = X [[M]] (D0 ) D0 where δ(ρ ) is a log-linear combination of weights and per-transition features as before. Here, the normalization ranges over all possible DAGs. For some values of the weight vector, this sum may diverge, as in weighted context-free grammars (CFGs; Chi 1999), meaning that the corresponding probability distribution is not defined. More importantly, estimating the normalization constant is computationally difficult, whereas in the case of weighted CFGs it can be estimated relatively easily with an iterative numerical algorithm (Abney, McAllester, and Pereira 1999; Smith and Johnson 2007). A similar problem arises in Exponential Random Graph Models (Frank and Strauss 1986); the most common solution is to use Markov chain Monte Carlo (MCMC) methods (Snijders 2002). To train a model over DAGs, we can perform gradient ascent on the log likelihood: X LL = log pM (ρi , Di ) i ∂LL = ∂w X Φ(ρi ) − ED0 ,ρ [Φ(ρ )] i by using MCMC to estimate the second expectation. Finally, we may wish to learn a distribution over DAGs by learning the states in an unsupervised manner, either because it is not practical to annotate states by hand, or because we wish to automatically find the set of stat"
J18-1005,J01-4004,0,0.292972,"Missing"
J18-1005,P87-1015,0,0.781806,"Missing"
J18-1005,P15-2141,0,0.0234808,"Missing"
J18-1005,N15-1040,0,0.0381581,"Missing"
K17-1011,P02-1040,0,\N,Missing
K17-1011,D08-1024,1,\N,Missing
K17-1011,P13-1126,0,\N,Missing
K17-1011,P01-1067,0,\N,Missing
K17-1011,C04-1072,0,\N,Missing
K17-1011,P11-2031,0,\N,Missing
K17-1011,P05-1033,1,\N,Missing
K17-1011,J03-1002,0,\N,Missing
K17-1011,K15-1007,0,\N,Missing
K17-1011,2013.mtsummit-papers.7,0,\N,Missing
K17-1011,N12-1026,0,\N,Missing
N09-1025,P08-1024,0,0.207216,"reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approa"
N09-1025,J93-2003,0,0.0195041,"Missing"
N09-1025,P07-1005,1,0.840775,"xtracted rules. (If a rule is observed with more than one set of word alignments, we keep only the most frequent one.) We then define, for each triple ( f, e, f+1 ), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f, e, f−1 ) with f−1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For System Hiero Training MERT MIRA Syntax MERT MIRA Features baseli"
N09-1025,D08-1024,1,0.651902,"st a set of hypothesis translations ei1 , . . . , ein , which are the 4.1 10-best translations according to each of: h(e) · w B(e) + h(e) · w (1) −B(e) + h(e) · w • For each i, select an oracle translation: e∗ = arg max (B(e) + h(e) · w) Let ∆hi j = e ∗ h(ei ) (2) − h(ei j ). • For each ei j , compute the loss `i j = B(e∗i ) − B(ei j ) (3) • Update w to the value of w0 that minimizes: m X 1 0 kw − wk2 + C max (`i j − ∆hi j · w0 ) (4) 1≤ j≤n 2 i=1 where C = 0.01. This minimization is performed by a variant of sequential minimal optimization (Platt, 1998). Following Chiang et al. (2008), we calculate the sentence B scores in (1), (2), and (3) in the context of some previous 1-best translations. We run 20 of these learners in parallel, and when training is finished, the weight vectors from all iterations of all learners are averaged together. Since the interface between the trainer and the decoder is fairly simple—for each sentence, the decoder sends the trainer a forest, and the trainer returns a weight update—it is easy to use this algorithm with a variety of CKY-based decoders: here, we are using it in conjunction with both the Hiero decoder and our syntax-based decoder"
N09-1025,P05-1033,1,0.617392,"particularly as it pertains to improving the best systems we have. Further: • Do syntax-based translation systems have unique and effective levers to pull when designing new features? • Can large numbers of feature weights be learned efficiently and stably on modest amounts of data? In this paper, we address these questions by experimenting with a large number of new features. We add more than 250 features to improve a syntaxbased MT system—already the highest-scoring single system in the NIST 2008 Chinese-English common-data track—by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement. This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. ∗ 218 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never"
N09-1025,J07-2003,1,0.210507,"string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of"
N09-1025,P97-1003,0,0.373709,"x MERT MIRA Features baseline syntax, distortion syntax, distortion, discount all source-side, discount baseline baseline overlap node count all target-side, discount # 11 56 61 10990 25 25 132 136 283 Tune 35.4 35.9 36.6 38.4 38.6 38.5 38.7 38.7 39.6 Test 36.1 36.9∗ 37.3∗∗ 37.6∗∗ 39.5 39.8∗ 39.9∗ 40.0∗∗ 40.6∗∗ Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM B scores. ∗ or ∗∗ = significantly better than MERT baseline (p < 0.05 or 0.01, respectively). the syntax-based system, we ran a reimplementation of the Collins parser (Collins, 1997) on the English half of the bitext to produce parse trees, then restructured and relabeled them as described in Section 3.2. Syntax-based rule extraction was performed on a 65 million word subset of the training data. For Hiero, rules with up to two nonterminals were extracted from a 38 million word subset and phrasal rules were extracted from the remainder of the training data. We trained three 5-gram language models: one on the English half of the bitext, used by both systems, one on one billion words of English, used by the syntax-based system, and one on two billion words of English, used"
N09-1025,D07-1079,1,0.641497,"Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: P(rule) = Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language mode"
N09-1025,W08-0306,1,0.527338,"l other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For System Hiero Training MERT MIRA Syntax MERT MIRA Features baseline syntax, distortion syntax, distortion, discount all source-side, discount baseline baseline overlap node count all target-side, discount # 11 56 61 10990 25 25 132 136 283 Tune 35.4 35.9 36.6 38.4 38.6 38.5 38.7 38.7 39.6 Test 36.1 36.9∗ 37.3∗∗ 37.6∗∗ 39.5 39.8∗ 39.9∗ 40.0∗∗ 40.6∗∗ Table 1: Adding new features with MIRA significantly improves translation accuracy. Scores are case-insensitive IBM B scores. ∗ or ∗∗ = significantly better than MERT baseline (p < 0.05 or 0.01, respectivel"
N09-1025,N04-1035,1,0.501726,"3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model transforms English trees into Chinese. We represent the translation model as a tree transducer (Knight and Graehl, 2005). It is obtained from bilingual text that has been word-aligned and whose English side has been syntactically parsed. From this data, we use the the GHKM minimal-rule extraction algorithm of (Galley et al., 2004) to yield rules like: NP-C(x0 :NPB PP(IN(of x1 :NPB)) ↔ x1 de x0 Though this rule can be used in either direction, here we use it right-to-left (Chinese to English). We follow Galley et al. (2006) in allowing unaligned Chinese words to participate in multiple translation rules, and in collecting larger rules composed of 219 count(rule) count(LHS-root(rule)) When we apply these probabilities to derive an English sentence e and a corresponding Chinese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and o"
N09-1025,P06-1121,1,0.630882,"y trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 minimal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: P(rule) = Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features a"
N09-1025,P08-1067,0,0.0419096,"the highest-scoring single system in the NIST 2008 Chinese-English common-data track—by +1.1 B. We also add more than 10,000 features to Hiero (Chiang, 2005) and obtain a +1.5 B improvement. This research was supported in part by DARPA contract HR0011-06-C-0022 under subcontract to BBN Technologies. ∗ 218 2 Related Work The work of Och et al (2004) is perhaps the bestknown study of new features and their impact on translation quality. However, it had a few shortcomings. First, it used the features for reranking n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Ma"
N09-1025,W02-1006,0,0.0271944,"st frequent one.) We then define, for each triple ( f, e, f+1 ), a feature that counts the number of times that f is aligned to e and f+1 occurs to the right of f ; and similarly for triples ( f, e, f−1 ) with f−1 occurring to the left of f . In order to limit the size of the model, we restrict words to be among the 100 most frequently occurring words from the training data; all other words are replaced with a token <unk>. These features are somewhat similar to features used by Watanabe et al. (2007), but more in the spirit of features used in the word sense disambiguation model introduced by Lee and Ng (2002) and incorporated as a submodel of a translation system by Chan et al. (2007); here, we are incorporating some of its features directly into the translation model. 5 Experiments For our experiments, we used a 260 million word Chinese/English bitext. We ran GIZA++ on the entire bitext to produce IBM Model 4 word alignments, and then the link deletion algorithm (Fossum et al., 2008) to yield better-quality alignments. For System Hiero Training MERT MIRA Syntax MERT MIRA Features baseline syntax, distortion syntax, distortion, discount all source-side, discount baseline baseline overlap node coun"
N09-1025,P06-1096,0,0.752695,"Missing"
N09-1025,D08-1076,0,0.134772,"8). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as possible from a relatively small dataset. We follow this approach here. 3 minimal rules"
N09-1025,P08-1114,0,0.131782,"on rules, e.g., insert-the and insert-is. There are 35 such features. 4.2 Source-side features We now turn to features that make use of source-side context. Although these features capture dependencies that cross boundaries between rules, they are still local in the sense that no new states need to be added to the decoder. This is because the entire source sentence, being fixed, is always available to every feature. 221 Soft syntactic constraints Neither of our systems uses source-side syntactic information; hence, both could potentially benefit from soft syntactic constraints as described by Marton and Resnik (2008). In brief, these features use the output of an independent syntactic parser on the source sentence, rewarding decoder constituents that match syntactic constituents and punishing decoder constituents that cross syntactic constituents. We use separatelytunable features for each syntactic category. Structural distortion features Both of our systems have rules with variables that generalize over possible fillers, but neither system’s basic model conditions a rule application on the size of a filler, making it difficult to distinguish long-distance reorderings from short-distance reorderings. To"
N09-1025,P02-1038,0,0.52616,"6). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1 , . . . , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1 , . . . , ein , which are the 4.1 10-best translations according to each of: h(e) · w B(e) + h(e) · w (1) −B(e) + h(e) · w • For each i, s"
N09-1025,P03-1021,0,0.12973,",001 New Features for Statistical Machine Translation∗ David Chiang and Kevin Knight USC Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA Abstract Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B and +1.1 B, respectively. We analyze the impact of the new feature"
N09-1025,P06-1055,0,0.128993,"in at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a batch of input sentences f1 , . . . , fm and decode each fi to obtain a forest of translations. • For each i, select from the forest a set of hypothesis translations ei1 , . . . , ein , which are the 4.1 10-best transla"
N09-1025,P07-1065,0,0.0152988,"Missing"
N09-1025,P06-1091,0,0.161178,"ng n-best lists of translations, rather than for decoding or forest reranking (Huang, 2008). Second, it attempted to incorporate syntax by applying off-the-shelf part-ofspeech taggers and parsers to MT output, a task these tools were never designed for. By contrast, we incorporate features directly into hierarchical and syntaxbased decoders. A third difficulty with Och et al.’s study was that it used MERT, which is not an ideal vehicle for feature exploration because it is observed not to perform well with large feature sets. Others have introduced alternative discriminative training methods (Tillmann and Zhang, 2006; Liang et al., 2006; Turian et al., 2007; Blunsom et al., 2008; Macherey et al., 2008), in which a recurring challenge is scalability: to train many features, we need many trainHuman Language Technologies: The 2009 Annual Conference of the North American Chapter of the ACL, pages 218–226, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics ing examples, and to train discriminatively, we need to search through all possible translations of each training example. Another line of research (Watanabe et al., 2007; Chiang et al., 2008) tries to squeeze as many features as"
N09-1025,D07-1078,1,0.613116,"content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al., 2006), following previous work (Watanabe et al., 2007; Chiang et al., 2008). Let e stand for output strings or their derivations, and let h(e) stand for the feature vector for e. Initialize the feature weights w. Then, repeatedly: • Select a"
N09-1025,D07-1080,0,0.64867,"d Chiang and Kevin Knight USC Information Sciences Institute 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292 USA Abstract Many of the new features use syntactic information, and in particular depend on information that is available only inside a syntax-based translation model. Thus they widen the advantage that syntaxbased models have over other types of models. The models are trained using the Margin Infused Relaxed Algorithm or MIRA (Crammer et al., 2006) instead of the standard minimum-error-rate training or MERT algorithm (Och, 2003). Our results add to a growing body of evidence (Watanabe et al., 2007; Chiang et al., 2008) that MIRA is preferable to MERT across languages and systems, even for very large-scale tasks. We use the Margin Infused Relaxed Algorithm of Crammer et al. to add a large number of new features to two machine translation systems: the Hiero hierarchical phrasebased translation system and our syntax-based translation system. On a large-scale ChineseEnglish translation task, we obtain statistically significant improvements of +1.5 B and +1.1 B, respectively. We analyze the impact of the new features and the performance of the learning algorithm. 1 Wei Wang Language W"
N09-1025,J97-3002,0,0.217863,"mal rules. These larger rules have been shown to substantially improve translation accuracy (Galley et al., 2006; DeNeefe et al., 2007). We apply Good-Turing discounting to the transducer rule counts and obtain probability estimates: P(rule) = Systems Used 3.1 Hiero Hiero (Chiang, 2005) is a hierarchical, string-tostring translation system. Its rules, which are extracted from unparsed, word-aligned parallel text, are synchronous CFG productions, for example: X → X1 de X2 , X2 of X1 As the number of nonterminals is limited to two, the grammar is equivalent to an inversion transduction grammar (Wu, 1997). The baseline model includes 12 features whose weights are optimized using MERT. Two of the features are n-gram language models, which require intersecting the synchronous CFG with finite-state automata representing the language models. This grammar can be parsed efficiently using cube pruning (Chiang, 2007). 3.2 Syntax-based system Our syntax-based system transforms source Chinese strings into target English syntax trees. Following previous work in statistical MT (Brown et al., 1993), we envision a noisy-channel model in which a language model generates English, and then a translation model"
N09-1025,P02-1039,1,0.475396,"y P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Och and Ney, 2002) and train them using MIRA (Crammer et al"
N09-1025,N06-1033,1,0.759347,"nese sentence c, we wind up with the joint probability P(e, c). The baseline model includes log P(e, c), the two n-gram language models log P(e), and other features for a total of 25. For example, there is a pair of features to punish rules that drop Chinese content words or introduce spurious English content words. All features are linearly combined and their weights are optimized using MERT. For efficient decoding with integrated n-gram language models, all transducer rules must be binarized into rules that contain at most two variables and can be incrementally scored by the language model (Zhang et al., 2006). Then we use a CKY-style parser (Yamada and Knight, 2002; Galley et al., 2006) with cube pruning to decode new sentences. We include two other techniques in our baseline. To get more general translation rules, we restructure our English training trees using expectationmaximization (Wang et al., 2007), and to get more specific translation rules, we relabel the trees with up to 4 specialized versions of each nonterminal symbol, again using expectation-maximization and the split/merge technique of Petrov et al. (2006). 3.3 MIRA training We incorporate all our new features into a linear model (Oc"
N09-1025,N04-1021,0,\N,Missing
N10-1014,J93-2003,0,0.0703269,"the sequence fl . The parameter pl is a left length distribution. The probabilities pmid , pright , decompose in the same way, except substituting a separate length distribution pm and pr for pl . For the T ERMINAL rule, we emit ft with a similarly decomposed distribution pterm using length distribution pw . We define the probability of generating a foreign word fj as p(fj |A, eA ) = X i∈eA 1 pt (fj |ei ) |eA | with i ∈ eA denoting an index ranging over the indices of the English words contained in eA . The reader may recognize the above expressions as the probability assigned by IBM Model 1 (Brown et al., 1993) of generating the words fl given the words eA , with one important difference – the length m of the foreign sentence is often not modeled, so the term pl (|fl |= m |A) is set to a constant and ignored. Parameterizing this length allows our model to effectively control the number of words produced at different levels of the derivation. It is worth noting how each parameter affects the model’s behavior. The pt distribution is a standard “translation” table, familiar from the IBM Models. The pinv distribution is a “distortion” parameter, and models the likelihood of inverting non-terminals B and"
N10-1014,N10-1015,1,0.831836,"sis is “correct”, but “good enough” without resorting to more computationally complicated models. In general, our model follows an “extract as much as possible” approach. We hypothesize that this approach will capture important syntactic generalizations, but it also risks including low-quality rules. It is an empirical question whether this approach is effective, and we investigate this issue further in Section 5.3. There are possibilities for improving our model’s treatment of syntactic divergence. One option is to allow the model to select trees which are more consistent with the alignment (Burkett et al., 2010), which our model can do since it permits efficient inference over forests. The second is to modify the generative process slightly, perhaps by including the “clone” operator of Gildea (2003). 123 Parameter Estimation The parameters of our model can be efficiently estimated in an unsupervised fashion using the Expectation-Maximization (EM) algorithm. The Estep requires the computation of expected counts under our model for each multinomial parameter. We omit the details of obtaining expected counts for each distribution, since they can be obtained using simple arithmetic from a single quantity"
N10-1014,P06-2014,0,0.621519,"e and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a s"
N10-1014,W07-0403,0,0.0769298,"ngle n-ary English tree obtained from a monolingual parser. However, it is worth noting that the English side of the ITG derivation is not completely fixed. Where our English trees are more than binary branching, we permit any binarization in our dynamic program. For efficiency, we also ruled out span alignments that are extremely lopsided, for example, a 1-word English span aligned to a 20-word foreign span. Specifically, we pruned any span alignment in which one side is more than 5 times larger than the other. Finally, we employ pruning based on highprecision alignments from simpler models (Cherry and Lin, 2007; Haghighi et al., 2009). We compute word-to-word alignments by finding all word pairs which have a posterior of at least 0.7 according to both forward and reverse IBM Model 1 parameters, and prune any span pairs which invalidate more than 3 of these alignments. In total, this pruning reSpan Syntactic Alignment GIZA++ Rule Syntactic Alignment GIZA++ P 50.9 56.1 P 39.6 46.2 R 83.0 67.3 R 40.3 34.7 F1 63.1 61.2 F1 39.9 39.6 Table 3: Alignment quality results for our syntactic aligner and our GIZA++ baseline. duced computation from approximately 1.5 seconds per sentence to about 0.3 seconds per s"
N10-1014,P05-1033,1,0.713885,"e being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponent"
N10-1014,D09-1037,0,0.0675543,"s permits the extraction of the general rule (PP → IN1 NP2 ; 在 NP2 IN1 ), and the more lexicalized (PP → before NP ; 在 NP 之前). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the B I NARY M ONO rule,4 p(r = A → B C; fl B fm C fr |A, eA ) which conditions on the root of the rule A and the English yield eA , as pg (A → B C |A, eA ) · pinv (I |B, C)· plef t (fl |A, eA )·pmid (fm |A, eA )·pright (fr |A, eA ) In words, we assume that the rule probability decomposes into a monolingual PCFG grammar probability pg , an inversion probability pinv , and a"
N10-1014,P07-1003,1,0.954134,"through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model"
N10-1014,W06-3105,1,0.843418,"ctly aligns to before, while 在 functions as a generic preposition, which our model handles by attaching it to the PP. This analysis permits the extraction of the general rule (PP → IN1 NP2 ; 在 NP2 IN1 ), and the more lexicalized (PP → before NP ; 在 NP 之前). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the B I NARY M ONO rule,4 p(r = A → B C; fl B fm C fr |A, eA ) which conditions on the root of the rule A and the English yield eA , as pg (A → B C |A, eA ) · pinv (I |B, C)· plef t (fl |A, eA )·pmid (fm |A, eA )·pright (fr |A, eA ) In words,"
N10-1014,N09-1026,1,0.844954,"ic system of Galley et al. (2006). For the translation rules extracted from our data, we computed standard features based on relative frequency counts, and tuned their weights using MERT (Och, 2003). We also included a language model feature, using a 5-gram language model trained on 220 million words of English text using the SRILM Toolkit (Stolcke, 2002). For tuning and test data, we used a subset of the NIST MT04 and MT05 with sentences of length at most 40. We used the first 1000 sentences of this set for tuning and the remaining 642 sentences as test data. We used the decoder described in DeNero et al. (2009) during both tuning and testing. We provide final tune and test set results in Table 4. Our alignments produce a 1.0 BLEU improvement over the baseline. Our reported syntactic results were obtained when rules were thresholded by count; we discuss this in the next section. 5.3 Analysis As discussed in Section 3.4, our aligner is designed to extract many rules, which risks inadvertently extracting low-quality rules. To quantify this, we 8 http://code.google.com/p/berkeleyparser/ 9 first examined the number of rules extracted by our 5 iterations of model 1, 5 iterations of HMM, 3 iterations align"
N10-1014,W08-0306,1,0.944423,"nts from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees onl"
N10-1014,N04-1035,1,0.906585,"RB VBN drastically fallen !易 trade ""差 surplus 大幅度 drastically 少 fall 了 (past) Figure 1: A single incorrect alignment removes an extractable node, and hence several desirable rules. We represent correct extractable nodes in bold, spurious extractable nodes with a *, and incorrectly blocked extractable nodes in bold strikethrough. chief purpose is to align nodes in the syntactic parse in one language to spans in the other – an alignment we will refer to as a “syntactic” alignment. These alignments are employed by standard syntactic rule extraction algorithms, for example, the GHKM algorithm of Galley et al. (2004). Following that work, we will assume parses are present in the target language, though our model applies in either direction. Currently, although syntactic systems make use of syntactic alignments, these alignments must be induced indirectly from word-level alignments. Previous work has discussed at length the poor interaction of word-alignments with syntactic rule extraction (DeNero and Klein, 2007; Fossum et al., 2008). For completeness, we provide a brief example of this interaction, but for a more detailed discussion we refer the reader to these presentations. 2.1 Interaction with Word Al"
N10-1014,P06-1121,1,0.926158,"source sentence to nodes in a target parse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spur"
N10-1014,P03-1011,0,0.487993,"use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when the syntax of the two languages diverges, and alignment quality drops precipitously (Zhang and Gildea, 2004), though attempts have been made to address this issue (Gildea, 2003). Our model is designed to degrade gracefully in the case of syntactic divergence. Because it can produce foreign words at any level of the derivation, our model can effectively back off to a variant of Model 1 in the case where an ITG derivation that both respects the target parse tree and the desired word-level alignments cannot be found. For example, consider the sentence pair fragment in Figure 3. It is not possible to produce an ITG derivation of this fragment that both respects the English tree and also aligns all foreign words to their obvious English counterparts. Our model handles thi"
N10-1014,P96-1024,0,0.0818471,"er sentence to about 0.3 seconds per sentence, a speed-up of a factor of 5. 4.3 Decoding Given a trained model, we extract a tree-to-string alignment as follows: we compute, for each node in the English tree, the posterior probability of a particular foreign span assignment using the same dynamic program needed for EM. We then compute the set of span assignments which maximizes the sum of these posteriors, constrained such that the foreign span assignments nest in the obvious way. This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996). 5 5.1 Experiments Alignment Quality We first evaluated our alignments against gold standard annotations. Our training data consisted of the 2261 manually aligned and translated sentences of the Chinese Treebank (Bies et al., 2007) and approximately half a million unlabeled sentences of parallel Chinese-English newswire. The unlabeled data was subsampled (Li et al., 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseli"
N10-1014,P09-1104,1,0.872906,"e obtained from a monolingual parser. However, it is worth noting that the English side of the ITG derivation is not completely fixed. Where our English trees are more than binary branching, we permit any binarization in our dynamic program. For efficiency, we also ruled out span alignments that are extremely lopsided, for example, a 1-word English span aligned to a 20-word foreign span. Specifically, we pruned any span alignment in which one side is more than 5 times larger than the other. Finally, we employ pruning based on highprecision alignments from simpler models (Cherry and Lin, 2007; Haghighi et al., 2009). We compute word-to-word alignments by finding all word pairs which have a posterior of at least 0.7 according to both forward and reverse IBM Model 1 parameters, and prune any span pairs which invalidate more than 3 of these alignments. In total, this pruning reSpan Syntactic Alignment GIZA++ Rule Syntactic Alignment GIZA++ P 50.9 56.1 P 39.6 46.2 R 83.0 67.3 R 40.3 34.7 F1 63.1 61.2 F1 39.9 39.6 Table 3: Alignment quality results for our syntactic aligner and our GIZA++ baseline. duced computation from approximately 1.5 seconds per sentence to about 0.3 seconds per sentence, a speed-up of a"
N10-1014,W06-3601,1,0.930085,"arse tree. We show that our model produces syntactically consistent analyses where possible, while being robust in the face of syntactic divergence. Alignment quality and end-to-end translation experiments demonstrate that this consistency yields higher quality alignments than our baseline. 1 Introduction Syntactic machine translation has advanced significantly in recent years, and multiple variants currently achieve state-of-the-art translation quality. Many of these systems exploit linguistically-derived syntactic information either on the target side (Galley et al., 2006), the source side (Huang et al., 2006), or both (Liu et al., 2009). Still others induce their syntax from the data (Chiang, 2005). Despite differences in detail, the vast majority of syntactic methods share a critical dependence on word alignments. In particular, they infer syntactic correspondences between the source and target languages through word alignment patterns, sometimes in combination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a l"
N10-1014,W09-0424,0,0.0225108,"ximizes the sum of these posteriors, constrained such that the foreign span assignments nest in the obvious way. This algorithm is a natural synchronous generalization of the monolingual Maximum Constituents Parse algorithm of Goodman (1996). 5 5.1 Experiments Alignment Quality We first evaluated our alignments against gold standard annotations. Our training data consisted of the 2261 manually aligned and translated sentences of the Chinese Treebank (Bies et al., 2007) and approximately half a million unlabeled sentences of parallel Chinese-English newswire. The unlabeled data was subsampled (Li et al., 2009) from a larger corpus by selecting sentences which have good tune and test set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseline alignments, we used GIZA++, trained in the standard way.9 We used the grow-diag-final alignment heuristic, as we found it outperformed union in early experiments. We trained our unsupervised syntactic aligner on the concatenation of the labelled and unlabelled data. As is standard in unsupervised alignment models, we initialized the translation parameters pt by first tr"
N10-1014,N06-1014,1,0.807301,"t set coverage, and limited to sentences of length at most 40. We parsed the English side of the training data with the Berkeley parser.8 For our baseline alignments, we used GIZA++, trained in the standard way.9 We used the grow-diag-final alignment heuristic, as we found it outperformed union in early experiments. We trained our unsupervised syntactic aligner on the concatenation of the labelled and unlabelled data. As is standard in unsupervised alignment models, we initialized the translation parameters pt by first training 5 iterations of IBM Model 1 using the joint training algorithm of Liang et al. (2006), and then trained our model for 5 EM iterations. We extracted syntactic rules using a re-implementation of the Galley et al. (2006) algorithm from both our syntactic alignments and the GIZA++ alignments. We handle null-aligned words by extracting every consistent derivation, and extracted composed rules consisting of at most 3 minimal rules. We evaluate our alignments against the gold standard in two ways. We calculated Span F-score, which compares the set of extractable nodes paired with a foreign span, and Rule F-score (Fossum et al., 2008) over minimal rules. The results are shown in Table"
N10-1014,D09-1136,0,0.010795,"the general rule (PP → IN1 NP2 ; 在 NP2 IN1 ), and the more lexicalized (PP → before NP ; 在 NP 之前). 3.1 Parameterization In principle, our model could have one parameter for each instantiation r of a rule type. This model would have an unmanageable number of parameters, producing both computational and modeling issues – it is well known that unsupervised models with large numbers of parameters are prone to degenerate analyses of the data (DeNero et al., 2006). One solution might be to apply an informed prior with a computationally tractable inference procedure (e.g. Cohn and Blunsom (2009) or Liu and Gildea (2009)). We opt here for the simpler, statistically more robust solution of making independence assumptions to keep the number of parameters at a reasonable level. Concretely, we define the probability of the B I NARY M ONO rule,4 p(r = A → B C; fl B fm C fr |A, eA ) which conditions on the root of the rule A and the English yield eA , as pg (A → B C |A, eA ) · pinv (I |B, C)· plef t (fl |A, eA )·pmid (fm |A, eA )·pright (fr |A, eA ) In words, we assume that the rule probability decomposes into a monolingual PCFG grammar probability pg , an inversion probability pinv , and a probability of left, mid"
N10-1014,P06-1077,0,0.0477303,"r model can be efficiently trained on large parallel corpora. When compared to standard word-alignmentbacked baselines, our model produces more consistent analyses of parallel sentences, leading to high-count, high-quality transfer rules. End-toend translation experiments demonstrate that these higher quality rules improve translation quality by 1.0 BLEU over a word-alignment-backed baseline. 2 Syntactic Rule Extraction Our model is intended for use in syntactic translation models which make use of syntactic parses on either the target (Galley et al., 2006) or source side (Huang et al., 2006; Liu et al., 2006). Our model’s 118 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 118–126, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics S NP VP DT* NN NN VBZ the trade surplus has ADVP RB VBN drastically fallen !易 trade ""差 surplus 大幅度 drastically 少 fall 了 (past) Figure 1: A single incorrect alignment removes an extractable node, and hence several desirable rules. We represent correct extractable nodes in bold, spurious extractable nodes with a *, and incorrectly blocked extractable nodes in bold strikethrough. chie"
N10-1014,P09-1063,0,0.0941769,"Missing"
N10-1014,J93-2004,0,0.0347448,"rminal symbol A. We modify p(fj |eA ) from the previous section to allow production of fj directly from the non-terminal7 A: p(fj |eA ) = pnt · p(fj |A) X 1 + (1 − pnt ) · pt (fj |ei ) |eA | i∈e A where pnt is a global binomial parameter which controls how often such alignments are made. This necessitates the inclusion of parameters like pt ( 的 |NP) into our translation table. Generally, these parameters do not contain much information, but rather function like a traditional N ULL rooted at some position in the tree. However, in some cases, the particular annotation used by the Penn Treebank (Marcus et al., 1993) (and hence most parsers) allows for some interesting parameters to be learned. For example, we found that our aligner often matched the Chinese word 了, which marks the past tense (among other things), to the preterminals VBD and VBN, which denote the English simple past and perfect tense. Additionally, Chinese measure words like 个 and 名 often align to the CD (numeral) preterminal. These generalizations can be quite useful – where a particular number might predict a measure word quite poorly, the generalization that measure words co-occur with the CD tag is very robust. 7 For terminal symbols"
N10-1014,D07-1038,1,0.932126,"bination with constraints from parser outputs. However, word alignments are not perfect indicators of syntactic alignment, and syntactic systems are very sensitive to word alignment behavior. Even a single spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or"
N10-1014,P04-1066,0,0.0389842,"pw parameterizes the number of words produced for a particular English word e, functioning similarly to the “fertilities” employed by IBM Models 3 and 4 (Brown et al., 1993). This allows us to model, for example, the tendency of English determiners the and a translate to nothing in the Chinese, and of English names to align to multiple Chinese words. In general, we expect an English word to usually align to one Chinese word, and so we place a weak Dirichlet prior on on the pe distribution which puts extra mass on 1-length word sequences. This is helpful for avoiding the “garbage collection” (Moore, 2004) problem for rare words. 3.2 Exploiting Non-Terminal Labels There are often foreign words that do not correspond well to any English word, which our model must also handle. We elected for a simple augmentation to our model to account for these words. When generating foreign word sequences f at a non-terminal (i.e. via the U NARY or B INARY productions), we also allow for the production of foreign words from the non-terminal symbol A. We modify p(fj |eA ) from the previous section to allow production of fj directly from the non-terminal7 A: p(fj |eA ) = pnt · p(fj |A) X 1 + (1 − pnt ) · pt (fj"
N10-1014,J03-1002,0,0.0124084,"a span of the foreign sentence which (1) contains every source word that aligns to a target word in the yield of the node and (2) contains no source words that align outside that yield. Only nodes for which a non-empty span satisfying (1) and (2) exists may form the root or leaf of a translation rule; for that reason, we will refer to these nodes as extractable nodes. Since extractable nodes are inferred based on word alignments, spurious word alignments can rule out otherwise desirable extraction points. For exam119 ple, consider the alignment in Figure 1. This alignment, produced by GIZA++ (Och and Ney, 2003), contains 4 correct alignments (the filled circles), but incorrectly aligns the to the Chinese past tense marker 了 (the hollow circle). This mistaken alignment produces the incorrect rule (DT → the ; 了), and also blocks the extraction of (VBN → fallen ; 减少 了). More high-level syntactic transfer rules are also ruled out, for example, the “the insertion rule” (NP → the NN1 NN2 ; NN1 NN2 ) and the high-level (S → NP1 VP2 ; NP1 VP2 ). 3 A Syntactic Alignment Model The most common approach to avoiding these problems is to inject knowledge about syntactic constraints into a word alignment model (Ch"
N10-1014,P03-1021,0,0.0378323,"core, which compares the set of extractable nodes paired with a foreign span, and Rule F-score (Fossum et al., 2008) over minimal rules. The results are shown in Table 3. By both measures, our syntactic aligner effectively trades recall for precision when compared to our baseline, slightly increasing overall F-score. 5.2 Translation Quality For our translation system, we used a reimplementation of the syntactic system of Galley et al. (2006). For the translation rules extracted from our data, we computed standard features based on relative frequency counts, and tuned their weights using MERT (Och, 2003). We also included a language model feature, using a 5-gram language model trained on 220 million words of English text using the SRILM Toolkit (Stolcke, 2002). For tuning and test data, we used a subset of the NIST MT04 and MT05 with sentences of length at most 40. We used the first 1000 sentences of this set for tuning and the remaining 642 sentences as test data. We used the decoder described in DeNero et al. (2009) during both tuning and testing. We provide final tune and test set results in Table 4. Our alignments produce a 1.0 BLEU improvement over the baseline. Our reported syntactic re"
N10-1014,J97-3002,0,0.936111,"to word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees only. The design of our model is such that, for divergent structures, a structurally integrated backoff to flatter word-level (or null) analyses is available. Therefore, our model is empirically robust to the case where syntactic divergence between languages prevents syntactically accurate ITG derivations. We show that, with appropriate pruning, our model can be efficiently trained on large parallel corpora. When compared"
N10-1014,P01-1067,1,0.669279,"e spurious word alignment can invalidate a large number of otherwise extractable rules, while unaligned words can result in an exponentially large set of extractable rules to choose from. Researchers have worked to incorporate syntactic information into word alignments, resulting in improvements to both alignment quality (Cherry and Lin, 2006; DeNero and Klein, 2007), and translation quality (May and Knight, 2007; Fossum et al., 2008). In this paper, we remove the dependence on word alignments and instead directly model the syntactic correspondences in the data, in a manner broadly similar to Yamada and Knight (2001). In particular, we propose an unsupervised model that aligns nodes of a parse tree (or forest) in one language to spans of a sentence in another. Our model is an instance of the inversion transduction grammar (ITG) formalism (Wu, 1997), constrained in such a way that one side of the synchronous derivation respects a syntactic parse. Our model is best suited to systems which use source- or target-side trees only. The design of our model is such that, for divergent structures, a structurally integrated backoff to flatter word-level (or null) analyses is available. Therefore, our model is empiri"
N10-1014,C04-1060,0,0.0186119,"our parameterization permits efficient inference algorithms which we will make use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when the syntax of the two languages diverges, and alignment quality drops precipitously (Zhang and Gildea, 2004), though attempts have been made to address this issue (Gildea, 2003). Our model is designed to degrade gracefully in the case of syntactic divergence. Because it can produce foreign words at any level of the derivation, our model can effectively back off to a variant of Model 1 in the case where an ITG derivation that both respects the target parse tree and the desired word-level alignments cannot be found. For example, consider the sentence pair fragment in Figure 3. It is not possible to produce an ITG derivation of this fragment that both respects the English tree and also aligns all forei"
N10-1014,N06-1033,1,0.856635,"ns can be quite useful – where a particular number might predict a measure word quite poorly, the generalization that measure words co-occur with the CD tag is very robust. 7 For terminal symbols E, this production is not possible. 122 3.3 Membership in ITG The generative process which describes our model contains a class of grammars larger than the computationally efficient class of ITG grammars. Fortunately, the parameterization described above not only reduces the number of parameters to a manageable level, but also introduces independence assumptions which permit synchronous binarization (Zhang et al., 2006) of our grammar. Any SCFG that can be synchronously binarized is an ITG, meaning that our parameterization permits efficient inference algorithms which we will make use of in the next section. Although several binarizations are possible, we give one such binarization and its associated probabilities in Table 2. 3.4 Robustness to Syntactic Divergence Generally speaking, ITG grammars have proven more useful without the monolingual syntactic constraints imposed by a target parse tree. When derivations are restricted to respect a target-side parse tree, many desirable alignments are ruled out when"
N10-1068,P09-1088,0,0.0973336,"trained on observed strings rather than string pairs. By again treating the first FSA as an FST with empty input, we can train using the FST-cascade training algorithm described in the previous paragraph. Once we have our trained cascade, we can apply it to new data, obtaining (for example) the k-best output strings for an input string. 3 Generic Bayesian Training Bayesian learning is a wide-ranging field. We focus on training using Gibbs sampling (Geman and Geman, 1984), because it has been popularly applied in the natural language literature, e.g., (Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009). Our overall plan is to give a generic algorithm for Bayesian training that is a “drop-in replacement” for EM training. That is, we input an FST cascade and data and output the same FST cascade with trained weights. This is an approximation to a purely Bayesian setup (where one would always integrate over all possible weightings), but one which makes it easy to deploy FSTs to efficiently decode new data. Likewise, we do not yet support nonparametric approaches—to create a drop-in replacement for EM, we require that all parameters be specified in the initial FST cascade. We return to this issu"
N10-1068,P02-1065,0,0.0314681,"multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DARPA contract HR0011-06-C0022. Weighted tre"
N10-1068,D08-1033,0,0.0722013,"Missing"
N10-1068,P05-1045,0,0.0169169,"transformer of strings. Such a cascade is trained on observed strings rather than string pairs. By again treating the first FSA as an FST with empty input, we can train using the FST-cascade training algorithm described in the previous paragraph. Once we have our trained cascade, we can apply it to new data, obtaining (for example) the k-best output strings for an input string. 3 Generic Bayesian Training Bayesian learning is a wide-ranging field. We focus on training using Gibbs sampling (Geman and Geman, 1984), because it has been popularly applied in the natural language literature, e.g., (Finkel et al., 2005; DeNero et al., 2008; Blunsom et al., 2009). Our overall plan is to give a generic algorithm for Bayesian training that is a “drop-in replacement” for EM training. That is, we input an FST cascade and data and output the same FST cascade with trained weights. This is an approximation to a purely Bayesian setup (where one would always integrate over all possible weightings), but one which makes it easy to deploy FSTs to efficiently decode new data. Likewise, we do not yet support nonparametric approaches—to create a drop-in replacement for EM, we require that all parameters be specified in the"
N10-1068,D08-1036,0,0.0242846,"ation after the sidetracking transition is selected? Should the path attempt to re-join the old derivation as soon as possible, and if so, how is this efficiently done? Then, how can we compute new derivation scores for all possible sidetracks, so that we can choose a new sample by an appropriate weighted coin flip? Finally, would such a sampler be reversible? In order to satisfy theoretical conditions for Gibbs sampling, if we move from sample A to sample B, we must be able to immediately get back to sample A. We take a different tack here, moving from pointwise sampling to blocked sampling. Gao and Johnson (2008) employed blocked sampling for POS tagging, and the approach works nicely for arbitrary derivation lattices. We again start with a random derivation for each example in the corpus. We then choose a training example and exchange its entire derivation lattice to the end of the corpus. We create a weighted version of this lattice, called the proposal lattice, such that we can approximately sample whole paths by stochastic generation. The probabilities are based on the event counts from the rest of the sample (the cache), and on the base distribution, 451 αP0 (r |q) + c(q, r) α + c(q) (3) where q"
N10-1068,P07-1094,0,0.598979,"revious decisions inside the cache. We use Gibbs sampling to estimate the distribution of tags given words. The key to efficient sampling is to define a sampling operator that makes some small change to the overall corpus derivation. With such an operator, we derive an incremental formula for re-scoring the probability of an entire new derivation based on the probability of the old derivation. Exchangeability makes this efficient— we pretend like the area around the small change occurs at the end of the corpus, so that both old and new derivations share the same cache. Goldwater and Griffiths (2007) choose the re-sampling operator “change the tag of a single word,” and they derive the corresponding incremental scoring formula for unsupervised tagging. For other problems, designers develop different sampling operators and derive different incremental scoring formulas. 3.2 Generic Case In order to develop a generic algorithm, we need to abstract away from these problem-specific design choices. In general, hidden derivations correspond to paths through derivation lattices, so we first and are computed in this way: P(r |q) = Figure 3: Changing a decision in the derivation lattice. All paths"
N10-1068,J08-3004,1,0.847533,"ribed general training algorithms for FST cascades and their implementation, and examined the problem of run selection for both EM and Bayesian training. This work raises several interesting points for future study. First, is there an efficient method for performing pointwise sampling on general FSTs, and would pointwise sampling deliver better empirical results than blocked sampling across a range of tasks? Second, can generic methods similar to the ones described here be developed for cascades of tree transducers? It is straightforward to adapt our methods to train a single tree transducer (Graehl et al., 2008), but as most types of tree transducers are not closed under composition (G´ecseg and Steinby, 1984), the compose/de-compose method cannot be directly applied to train cascades. Third, what is the best way to extend the FST formalism to represent non-parametric Bayesian models? Consider the English re-spacing application. We currently take observed (un-spaced) data and build a giant unigram FSA that models every letter sequence seen in the data of up to 10 letters, both words and non-words. This FSA has 207,253 transitions. We also define P0 for each individual transition, which allows a prefe"
N10-1068,knight-al-onaizan-1998-translation,1,0.78918,"atically selecting from among multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DARPA contract HR0011-06-C0022."
N10-1068,P06-2065,1,0.854535,"data. • We propose a method for automatic run selec447 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447–455, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and"
N10-1068,N03-1018,0,0.0182333,"ining runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DARPA contract HR0011-06-C0022. Weighted tree transducers play t"
N10-1068,J94-2001,0,0.720149,"s are: • We describe a Bayesian inference algorithm that can be used to train any cascade of WFSTs on end-to-end data. • We propose a method for automatic run selec447 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447–455, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform"
N10-1068,H94-1050,0,0.0137433,"a. We also investigate the problem of automatically selecting from among multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF gra"
N10-1068,D08-1085,1,0.85282,"method for automatic run selec447 Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the ACL, pages 447–455, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and compose the result with"
N10-1068,N09-1005,1,0.84273,"es, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and compose the result with Mw . This composition follows Pereira and Riley (1996), treating epsilon input and output transitions correctly, especially with regards to their weighted in"
N10-1068,P09-1057,1,0.812237,"es, California, June 2010. 2010 Association for Computational Linguistics tion, i.e., how to automatically select among multiple training runs in order to achieve the best possible task accuracy. The natural language applications we consider in this paper are: (1) unsupervised part-of-speech (POS) tagging (Merialdo, 1994; Goldwater and Griffiths, 2007), (2) letter substitution decipherment (Peleg and Rosenfeld, 1979; Knight et al., 2006; Ravi and Knight, 2008), (3) segmentation of space-free English (Goldwater et al., 2009), and (4) Japanese/English phoneme alignment (Knight and Graehl, 1998; Ravi and Knight, 2009a). Figure 1 shows how each of these problems can be represented as a cascade of finite-state acceptors (FSAs) and finite-state transducers (FSTs). 2 Generic EM Training We first describe forward-backward EM training for a single FST M. Given a string pair (v, w) from our training data, we transform v into an FST Mv that just maps v to itself, and likewise transform w into an FST Mw . Then we compose Mv with M, and compose the result with Mw . This composition follows Pereira and Riley (1996), treating epsilon input and output transitions correctly, especially with regards to their weighted in"
N10-1068,J96-3004,0,0.091326,"the problem of automatically selecting from among multiple training runs. Our experiments on four different tasks demonstrate the genericity of this framework, and, where applicable, large improvements in performance over EM. We also show, for unsupervised part-of-speech tagging, that automatic run selection gives a large improvement over previous Bayesian approaches. 1 Adam Pauls2 Introduction In this paper, we investigate Bayesian inference for weighted finite-state transducers (WFSTs). Many natural language models can be captured by weighted finite-state transducers (Pereira et al., 1994; Sproat et al., 1996; Knight and Al-Onaizan, 1998; Clark, 2002; Kolak et al., 2003; Mathias and Byrne, 2006), which offer several benefits: • WFSTs provide a uniform knowledge representation. • Complex problems can be broken down into a cascade of simple WFSTs. • Input- and output-epsilon transitions allow compact designs. • Generic algorithms exist for doing inferences with WFSTs. These include best-path decoding, k-best path extraction, composition, ∗ The authors are listed in alphabetical order. Please direct correspondence to Sujith Ravi (sravi@isi.edu). This work was supported by NSF grant IIS-0904684 and DA"
N10-1068,J98-4003,1,\N,Missing
N15-1063,J93-2003,0,0.109376,"s the regularized sum of expected completedata log-likelihoods: max λR(t1 , t2 ) + Θ1 ,Θ2 Nk X X k∈{1,2} n=1 qk (znk , xnk ) log pk (xnk , znk ) where n ranges over the appropriate sample set. Operationally, for models Θk that can be encoded as wFSTs (such as the IBM1, IBM2 and HMM word alignment models), the E-step can be carried out efficiently and exactly using dynamic programming (Eisner, 2002). Other models resort to approximation techniques – for example, the fertilitybased word alignment models apply hill-climbing and sampling heuristics in order to efficiently estimate the posteriors (Brown et al., 1993) From the computed posteriors qk we collect expected counts for each event, used to construct the M-step optimization objective. Since the MIR regularizer couples only the t-table parameters, the update rule for any remaining parameter is left unchanged (that is, one can use the usual closed-form count-and-divide solution). e, f e, f Now, let C1 and C2 denote the expected counts e, f for the t-table parameters. That is, Ck denotes the expected number of times a source-symbol type e is seen aligned to a target-symbol type f according to the posterior qk . In the M-step, we maximize the followin"
N15-1063,D08-1024,1,0.83459,"Missing"
N15-1063,D13-1173,0,0.0377823,"Missing"
N15-1063,P02-1001,0,0.0693537,"k ; Θk ). n = p(e |f ; Θ2 ) 1 This slight notational abuse helps represent both data scenarios succinctly. 611 In the M-step, the computed posteriors are used to define a convex optimization program that maximizes the regularized sum of expected completedata log-likelihoods: max λR(t1 , t2 ) + Θ1 ,Θ2 Nk X X k∈{1,2} n=1 qk (znk , xnk ) log pk (xnk , znk ) where n ranges over the appropriate sample set. Operationally, for models Θk that can be encoded as wFSTs (such as the IBM1, IBM2 and HMM word alignment models), the E-step can be carried out efficiently and exactly using dynamic programming (Eisner, 2002). Other models resort to approximation techniques – for example, the fertilitybased word alignment models apply hill-climbing and sampling heuristics in order to efficiently estimate the posteriors (Brown et al., 1993) From the computed posteriors qk we collect expected counts for each event, used to construct the M-step optimization objective. Since the MIR regularizer couples only the t-table parameters, the update rule for any remaining parameter is left unchanged (that is, one can use the usual closed-form count-and-divide solution). e, f e, f Now, let C1 and C2 denote the expected counts"
N15-1063,P08-1112,0,0.204155,"Missing"
N15-1063,N03-1017,0,0.0320515,"e transfer of source-language symbols to target-language symbols or vice versa, but not both directions. Left unchecked, independently training two such directional models (sourceto-target and target-to-source) often yields two models that diverge from this invertibility intuition. In word alignment, this can lead to disagreements between alignments inferred by a model trained in one direction and those inferred by a model trained in the reverse direction. To remedy this disparity (and other shortcomings), it is common to turn to alignment symmetrization techniques such as growdiag-final-and (Koehn et al., 2003) which heuristically combines alignments from both directions. Liang et al. (2006) suggest a more fundamental approach they call Alignment by Agreement (ABA), which jointly trains two word alignment models by maximizing their data-likelihoods along with a regularizer that rewards agreement between their alignment posteriors (computed over each parallel sentence pair). Although their EM-like optimization procedure is heuristic, it proves effective at jointly training bidirectional models. Ganchev et al. (2008) propose another approach for agreement between the directed models by adding constrai"
N15-1063,P07-2045,0,0.00872227,"Missing"
N15-1063,W04-3250,0,0.0363212,"ents. 6 Cze-Eng WMT09 WMT10 16.7 17.1 16.9 17.4 17.1 17.6 17.1 17.7 16.8 17.2 17.2 17.5 17.4 17.9 Table 2: Bleu scores. Combining ABA and MIR HMM alignments improves Bleu score significantly over all other methods. Table 1: Word alignment F1 scores. 5.4 Chi-Eng NIST08 23.6 24.6 24.0 24.4 24.2 24.6 25.1 http://www.statmt.org/moses/ 614 For example, by combining their HMM alignments (simply concatenating aligned bitexts) the total improvement reaches +1.5 Bleu on the Chinese-toEnglish task, a statistically significant improvement (p &lt; 0.05) according to a bootstrap resampling significance test (Koehn, 2004)). Table 5.4 summarizes our MT results. 6 Experiments without Parallel Data Ravi and Knight (2009) consider the challenging task of learning a Japanese-English backtransliteration model without parallel data. The goal is to correctly decode a list of 100 US senator names written in katakana script, without having access to parallel data. In this section, we reproduce their decipherment experiment and show that applying MIR to their baseline model significantly outperforms both the baseline and the bi-EM method. 6.1 Phonetic-Based Japanese Decipherment Ravi and Knight (2009) construct a English"
N15-1063,N06-1014,0,0.657576,"t not both directions. Left unchecked, independently training two such directional models (sourceto-target and target-to-source) often yields two models that diverge from this invertibility intuition. In word alignment, this can lead to disagreements between alignments inferred by a model trained in one direction and those inferred by a model trained in the reverse direction. To remedy this disparity (and other shortcomings), it is common to turn to alignment symmetrization techniques such as growdiag-final-and (Koehn et al., 2003) which heuristically combines alignments from both directions. Liang et al. (2006) suggest a more fundamental approach they call Alignment by Agreement (ABA), which jointly trains two word alignment models by maximizing their data-likelihoods along with a regularizer that rewards agreement between their alignment posteriors (computed over each parallel sentence pair). Although their EM-like optimization procedure is heuristic, it proves effective at jointly training bidirectional models. Ganchev et al. (2008) propose another approach for agreement between the directed models by adding constraints on the alignment posteriors. Unlike ABA, their optimization is exact, but it c"
N15-1063,N09-1005,0,0.270699,"17.4 17.9 Table 2: Bleu scores. Combining ABA and MIR HMM alignments improves Bleu score significantly over all other methods. Table 1: Word alignment F1 scores. 5.4 Chi-Eng NIST08 23.6 24.6 24.0 24.4 24.2 24.6 25.1 http://www.statmt.org/moses/ 614 For example, by combining their HMM alignments (simply concatenating aligned bitexts) the total improvement reaches +1.5 Bleu on the Chinese-toEnglish task, a statistically significant improvement (p &lt; 0.05) according to a bootstrap resampling significance test (Koehn, 2004)). Table 5.4 summarizes our MT results. 6 Experiments without Parallel Data Ravi and Knight (2009) consider the challenging task of learning a Japanese-English backtransliteration model without parallel data. The goal is to correctly decode a list of 100 US senator names written in katakana script, without having access to parallel data. In this section, we reproduce their decipherment experiment and show that applying MIR to their baseline model significantly outperforms both the baseline and the bi-EM method. 6.1 Phonetic-Based Japanese Decipherment Ravi and Knight (2009) construct a English-toJapanese transliteration model as a cascade of wFSTs (depicted in Figure 1, top). According to"
N15-1063,P11-1002,0,0.0550427,"Missing"
N15-1063,W11-0320,0,0.0213892,"xpected counts e, f for the t-table parameters. That is, Ck denotes the expected number of times a source-symbol type e is seen aligned to a target-symbol type f according to the posterior qk . In the M-step, we maximize the following objective with respect to t1 and t2 : arg max t1 ,t2 X e, f X e, f e, f C1 log t1 ( f |e) + e, f C2 log t2 (e |f ) + λR(t1 , t2 ) (5) which can be efficiently solved using convex programming techniques due to the concavity of R and the complete-data log-likelihoods in both t1 and t2 . In our implementation, we applied Projected Gradient Descent (Bertsekas, 1999; Schoenemann, 2011), where at each step, the parameters are updated in the direction of the M-step objective gradient at (t1 , t2 ) and then projected back onto the probability simplex. We used simple stopping conditions based on objective function value convergence and a bounded number of iterations. 612 4 4.1 Baselines Parallel Data Baseline: ABA and PostCAT Our approach is most similar to Alignment by Agreement (Liang et al., 2006) which uses a single joint objective for two word alignment models. The difference between our objective (Eq. 4) and theirs lies in their proposed regularizer, which rewards the per"
N15-1129,J93-2003,0,0.0949787,"Missing"
N15-1129,P07-1092,0,0.0271915,"Missing"
N15-1129,2014.amta-researchers.24,0,0.0679044,"Missing"
N15-1129,2005.mtsummit-papers.11,0,0.343262,"t language, resulting in an approximate source-target model Θsft . This combination process is referred to as triangulation (see §5). In particular, they construct the triangulated source-target t-table tsft from the source-pivot and pivot-target t-tables tsp , tpt using the following approximation: X tsft (t |s) = t(t |p, s) · t(p |s) p ≈ X tpt (t |p) · tsp (p |s) (1) p Subsequently, if a source-target corpus is available, they train a standard source-target model Θst , and tune the interpolation tˆ st = λinterp tst + (1 − λinterp )tsft with respect to λinterp to reduce alignment error rate (Koehn, 2005) over a hand-aligned development set. 1221 Human Language Technologies: The 2015 Annual Conference of the North American Chapter of the ACL, pages 1221–1226, c Denver, Colorado, May 31 – June 5, 2015. 2015 Association for Computational Linguistics Wang et al. (2006) propose triangulation heuristics for other model parameters; however, in this paper, we consider only t-table triangulation. 3 Our Method We now discuss two approaches that better exploit model triangulation. In the first, we use the triangulated t-table to construct a prior on the source-target t-table. In the second, we place a p"
N15-1129,J10-4005,0,0.0169175,"ces of length greater than 40 were filtered out from all training corpora. 4.2 Experiment 1: Method Comparison We trained word alignment models in both sourceto-target and target-to-source directions. We used 5 iterations of IBM Model 1 followed by 5 iterations of HMM. We tuned hyperparameters to maximize alignment F-score of the hand-aligned development set. Both interpolation parameters λinterp and λ were tuned over the range [0, 1]. For our methods, we fixed γ = 0.5, which we found effective during preliminary experiments. Alignment F-scores using grow-diag-final-and (gdfa) symmetrization (Koehn, 2010) are reported in Table 2, column 2. We conducted MT experiments using the Moses translation system (Koehn, 2005). We used a 5-gram LM trained on the Xinhua portion of English Gigaword (LDC2007T07). To tune the decoder, we used the WMT10 tune set. MT Bleu scores are reported in Table 2, columns 3–4. Both our methods outperform the baseline and the interpolation approach. In particular, the joint training approach more than doubles the gains obtained by the interpolation approach, on both F- and Bleu. We also evaluated the Czech-French and FrenchEnglish alignments produced as a by-product of our"
N15-1129,D07-1005,0,0.0910714,"Related Work The term “triangulation” comes from the phrasetable triangulation literature (Cohn and Lapata, 2007; Razmara and Sarkar, 2013; Dholakia and Sarkar, 2014), in which source-pivot and pivot-target phrase tables are triangulated according to Eq. 1 (with words replaced by phrases). The resulting triangulated phrase table can then be combined with an existing source-target phrase table, and is especially useful in increasing the source language vocabulary coverage, reducing OOVs. In our case, since word alignment is a closed vocabulary task, OOVs are never an issue. In word alignment, Kumar et al. (2007) uses multilingual parallel data to compute better sourcetarget alignment posteriors. Filali and Bilmes (2005) tag each source token and target token with their most likely translation in a pivot language, and then proceed to align (source word, source tag) tuple sequences to (target word, target tag) tuple sequences. In contrast, our word alignment method can be applied without multilingual parallel data, and does not commit to hard decisions. 6 Conclusion and Future Work We presented a simple multi-task learning algorithm that jointly trains three word alignment models over disjoint bitexts."
N15-1129,I13-1029,0,0.041103,"Missing"
N15-1129,C96-2141,0,0.894078,"on, following Wang et al., which approximates a source-target model by combining source-pivot and pivot-target models. We develop a MAP-EM algorithm that uses triangulation as a prior, and show how to extend it to a multi-task setting. On a low-resource Czech-English corpus, using French as the pivot, our multi-task learning approach more than doubles the gains in both Fand Bleu scores compared to the interpolation approach of Wang et al. Further experiments reveal that the choice of pivot language does not significantly affect performance. 1 2 Introduction Word alignment (Brown et al., 1993; Vogel et al., 1996) is a fundamental task in the machine translation (MT) pipeline. To train good word alignment models, we require access to a large parallel corpus. However, collection of parallel corpora has mostly focused on a small number of widely-spoken languages. As such, resources for almost any other pair are either limited or non-existent. To improve word alignment and MT in a lowresource setting, we design a multitask learning approach that utilizes parallel data of a third language, called the pivot language (§3). Specifically, we derive an efficient and easy-to-implement MAP-EM-like algorithm that"
N15-1129,P06-2112,0,0.703357,"widely-spoken languages. As such, resources for almost any other pair are either limited or non-existent. To improve word alignment and MT in a lowresource setting, we design a multitask learning approach that utilizes parallel data of a third language, called the pivot language (§3). Specifically, we derive an efficient and easy-to-implement MAP-EM-like algorithm that jointly trains sourcetarget, source-pivot and pivot-target alignment models, each on its own bitext, such that each model benefits from observations made by the other two. Our method subsumes the model interpolation approach of Wang et al. (2006), who independently Triangulation and Interpolation Wang et al. (2006) focus on learning a word alignment model without a source-target corpus. To do so, they assume access to both source-pivot and pivot-target bitexts on which they independently train a source-pivot word alignment model Θsp and a pivot-target model Θpt . They then combine the two models by marginalizing over the pivot language, resulting in an approximate source-target model Θsft . This combination process is referred to as triangulation (see §5). In particular, they construct the triangulated source-target t-table tsft from"
N16-1109,P10-1010,1,0.84534,"Missing"
N16-1109,C12-2013,1,0.844149,"Missing"
N16-1109,C14-1096,1,0.809603,"Missing"
N16-1109,J93-2003,0,0.0596236,"ement in measured alignment quality. We now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments fea"
N16-1109,2012.eamt-1.60,0,0.00647996,"that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into other languages. As mentioned in the introduction, a steppingstone to model parallel speech is to assume a recognizer that can produce a phonetic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM"
N16-1109,N13-1073,0,0.0289172,"now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments feature, described above provides a basic f"
N16-1109,P07-2045,0,0.00650321,"Missing"
N16-1109,D13-1019,0,0.00895223,"ity of alignment directly on source-language speech. <s> w1 wi-1 wn Decoder HT Ci Attention 2 wi Background To our knowledge, there has been relatively little research on models that operate directly on parallel speech. Typically, speech is transcribed into a word sequence or lattice using ASR, or at least a phone sequence or lattice using a phone recognizer. This normally requires manually transcribed data and a pronunciation lexicon, which can be costly to create. Recent work has introduced models that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into"
N16-1109,D15-1166,0,0.176882,"us audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the attention mechanism to mimic the structural biases in standard word based translation models. The attentional model encodes a source as a sequence of vectors, then decodes it to generate the output. At each step, it “attends” to different parts of the encoded sequence. This model has been used for translation, image caption generation, and speech recognition (Luong et al., 2015; Xu et al., 2015; Chorowski et al., 2014; Chorowski et al., 2015). Here, we briefly describe the basic attentional model, following Bahdanau et al. (2015), review the extensions for encoding structural biases (Cohn et al., 2016), and then present our novel means for adapting the approach handle parallel speech. 3.1 Base attentional model The model is shown in Figure 1. The speech signal is represented as a sequence of vectors S 1 , S 2 , . . . , S m . For the first set of experiments, each S i is a 128dimensional vector-space embedding of a phone. For the second set of experiments, each S i i"
N16-1109,N15-1038,0,0.0279326,"irectly on source-language speech. <s> w1 wi-1 wn Decoder HT Ci Attention 2 wi Background To our knowledge, there has been relatively little research on models that operate directly on parallel speech. Typically, speech is transcribed into a word sequence or lattice using ASR, or at least a phone sequence or lattice using a phone recognizer. This normally requires manually transcribed data and a pronunciation lexicon, which can be costly to create. Recent work has introduced models that do not require pronunciation lexicons, but train only on speech with text transcriptions (Lee et al., 2013; Maas et al., 2015; Graves et al., 2006). Here, we bypass phonetic transcriptions completely, and rely only on translations. Such data can be found, for example, in subtitled or dubbed movies. Some specific examples of corpora of parallel speech are the European Parliament Plenary Sessions Corpus (Van den Heuvel et al., 2006), which includes parliamentary speeches in the 21 official EU languages, as well as their interpretation into all the other languages; and the TED Talks Corpus (Cettolo et al., 2012), which provides speech in one language (usually English) together with translations into other languages. As"
N16-1109,P11-1064,0,0.00736763,"r languages. As mentioned in the introduction, a steppingstone to model parallel speech is to assume a recognizer that can produce a phonetic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM Model 3, named Model 3P, designed specifically for phone-to-word alignment. Finally, pialign (Neubig et al., 2011), an unsupervised model for joint phrase alignment and extraction, has been shown to work well at the character level (Neubig et al., 2012) and extends naturally to work on phones. 950 Encoder HS Representation S1 S2 S3 Sm Speech Signal Figure 1: The attentional model as applied to our tasks. We consider two types of input: discrete phone input, or continuous audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the att"
N16-1109,P12-1018,0,0.0352532,"ic transcription of the source language, then to model the transformation from transcription to translation. We compare against three previous models that can operate on sequences of phones. The first is simply to run GIZA++ (IBM Model 4) on a phonetic transcription (without word boundaries) of the source side. Stahlberg et al. (2012) present a modification of IBM Model 3, named Model 3P, designed specifically for phone-to-word alignment. Finally, pialign (Neubig et al., 2011), an unsupervised model for joint phrase alignment and extraction, has been shown to work well at the character level (Neubig et al., 2012) and extends naturally to work on phones. 950 Encoder HS Representation S1 S2 S3 Sm Speech Signal Figure 1: The attentional model as applied to our tasks. We consider two types of input: discrete phone input, or continuous audio, represented as PLP vectors at 10ms intervals 3 Model We base our approach on the attentional translation model of Cohn et al. (2016), an extension of Bahdanau et al. (2015) which incorporates more fine grained components of the attention mechanism to mimic the structural biases in standard word based translation models. The attentional model encodes a source as a sequ"
N16-1109,P00-1056,0,0.0780057,"truction of the “silver” standard for evaluation, described below). We also use the English translations produced by Post et al. (2013). We treat the Spanish speech as a sequence of 39dimensional PLP vectors (order 12 with energy and first and second order delta) encoding the power spectrum of the speech signal. We do not have gold standard alignments between the Spanish speech and English words for evaluation, so we produced “silver” standard alignments. We used a forced aligner (Gorman et al., 2011) to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic (Och and Ney, 2000) to align the Spanish transcription to the English translation. We then combined the two alignments to produce “silver” standard alignments between the Spanish speech and the English words. Cleaning and splitting the data based on dialogue turns, resulted in a set of 17,532 Spanish utterances from which we selected 250 for development and 500 testing. For each utterance we have the corresponding English translation, and for each word in the translation we have the corresponding span of Spanish speech. The forced aligner produces the phonetic sequences that correspond to each utterance, which w"
N16-1109,2013.iwslt-papers.14,0,0.326579,"nment smoothing Note that when T = 1 we recover the standard softmax function; we set T = 10 in both experiments. 5 Experimental Setup We work on the Spanish CALLHOME Corpus (LDC96S35), which consists of telephone conversations between Spanish native speakers based in the US and their relatives abroad. While Spanish is not a low-resource language, we pretend that it is by not using any Spanish ASR or resources like transcribed speech or pronunciation lexicons (except in the construction of the “silver” standard for evaluation, described below). We also use the English translations produced by Post et al. (2013). We treat the Spanish speech as a sequence of 39dimensional PLP vectors (order 12 with energy and first and second order delta) encoding the power spectrum of the speech signal. We do not have gold standard alignments between the Spanish speech and English words for evaluation, so we produced “silver” standard alignments. We used a forced aligner (Gorman et al., 2011) to align the speech to its transcription, and GIZA++ with the gdfa symmetrization heuristic (Och and Ney, 2000) to align the Spanish transcription to the English translation. We then combined the two alignments to produce “silve"
N16-1109,van-den-heuvel-etal-2006-tc,0,0.0903295,"Missing"
N16-1109,C96-2141,0,0.222851,"ignment quality. We now give a brief overview of these components. Previous attention. In the basic attentional model, the alignment is calculated based on the source encoding HS and the previous hidden state HTi−1 of the target, αi = Attend(HTi−1 , HS ), where Attend is a function that outputs m attention coefficients. This attention mechanism is overly simplistic, in that it is incapable of capturing patterns in the attention over different positions i. Recognising and exploiting these kinds of patterns has proven critical in traditional word based models of translation (Brown et al., 1993; Vogel et al., 1996; Dyer et al., 2013). For this reason Cohn et al. (2016) include explicit features encoding structural biases from word based models, namely absolute and relative position, Markov conditioning and fertility: 1. previous alignment, αi−1 P 2. sum of previous alignments, i−1 j=1 α j 3. source index vector, (1, 2, 3, . . . , m); and 4. target index vector (i, i, i, . . . , i). These features are concatenated to form a feature matrix β ∈ R4×m , which are added to the alignment calculation, i.e., αi = Attend(HTi−1 , HS , β) . Coverage penalty. The sum over previous alignments feature, described abov"
N18-1008,E17-2076,0,0.233342,"ed by using the output of an ASR system as input to a MT system. For example, Ney (1999) and Matusov et al. (2005) use ASR output lattices as input to translation models, integrating speech recognition uncertainty into the translation model. Recent work has focused more on modelling speech translation without explicit access to transcriptions. Duong et al. (2016) introduced a sequence-to-sequence model for speech translation without transcriptions but only evaluated on alignment, while Anastasopoulos et al. (2016) presented an unsupervised alignment method for speech-to-translation alignment. Bansal et al. (2017) used an unsupervised term discovery system (Jansen et al., 2010) to cluster recurring audio segments into pseudowords Our work is most similar to the work of Weiss et al. (2017). They used sequence-to-sequence models to transcribe Spanish speech and translate it in English, by jointly training the two tasks in a multitask scenario where the decoders share the encoder. In contrast to our work, they use a large corpus for training the model on roughly 163 hours of data, using the Spanish Fisher and CALL89 HOME conversational speech corpora. The parameter number of their model is significantly l"
N18-1008,C14-1096,0,0.0262269,"should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input. 1 Introduction Recent efforts in endangered language documentation focus on collecting spoken language resources, accompanied by spoken translations in a high resource language to make the resource interpretable (Bird et al., 2014a). For example, the BULB project (Adda et al., 2016) used the LIGAikuma mobile app (Bird et al., 2014b; Blachon et al., 2016) to collect parallel speech corpora between three Bantu languages and French. Since it’s common for speakers of endangered languages to speak one or more additional languages, collection of such a resource is a realistic goal. Speech can be interpreted either by transcription in the original language or translation to another language. Since the size of the data is extremely small, multitask models that jointly train a model for both tasks can take advantage of both sig"
N18-1008,W14-2201,0,0.0129147,"should provide useful information. Second, we apply regularization that encourages transitivity and invertibility. We show that the application of these notions on jointly trained models improves performance on the tasks of low-resource speech transcription and translation. It also leads to better performance when using attention information for word discovery over unsegmented input. 1 Introduction Recent efforts in endangered language documentation focus on collecting spoken language resources, accompanied by spoken translations in a high resource language to make the resource interpretable (Bird et al., 2014a). For example, the BULB project (Adda et al., 2016) used the LIGAikuma mobile app (Bird et al., 2014b; Blachon et al., 2016) to collect parallel speech corpora between three Bantu languages and French. Since it’s common for speakers of endangered languages to speak one or more additional languages, collection of such a resource is a realistic goal. Speech can be interpreted either by transcription in the original language or translation to another language. Since the size of the data is extremely small, multitask models that jointly train a model for both tasks can take advantage of both sig"
N18-1008,P15-1166,0,0.359847,". Note that the context vectors resulting from the two attentions are concatenated, not added. n 3 Finally, the decoder computes a sequence of output states from which a probability distribution over output words can be computed. For compactness, we will write X for the matrix whose rows are the xn , and similarly H, C, and so on. We also write A for the matrix of attention weights: [A]i j = αi j . Let θ be the parameters of our model, which we train on sentence triples (X, Y1 , Y2 ). sm = dec(sm−1 , cm , ym−1 ) P(ym ) = softmax(sm ). In a standard encoder-decoder multitask model (Figure 1b) (Dong et al., 2015; Weiss et al., 2017), we jointly model two output sequences using a shared encoder, but separate attentions and decoders: X α1mn hn c1m = 3.1 score(Y1 , Y2 |X; θ) = λ log P(Y1 |X; θ) + = dec1 (s1m−1 , c1m , y1m−1 ) (1 − λ) log P(Y2 |X, S1 ; θ) = softmax(s1m ) where λ is a parameter that controls the importance of each sub-task. In all our experiments, we set λ to 0.5. We then train the model to maximize X L(θ) = score(Y1 , Y2 |X; θ), and c2m = s2m P(y2m ) X n α2mn hn = dec2 (s2m−1 , c2m , y2m−1 ) = softmax(s2m ). where the summation is over all sentence triples in the training data. We can al"
N18-1008,N16-1109,1,0.929318,"nslations in Japanese and English.3 Since there does not exist a standard train-dev-test split, we employ a cross validation scheme for evaluation purposes. In each fold, one of the 10 narratives becomes the test set, with the previous one (mod 10) becoming the dev set, and the remaining 8 narratives becoming the training set. The models for each of the 10 folds are trained and tested separately. On average, for each fold, we train on about 2000 utterances; the dev and test sets consist of about 270 utterances. 3 4.2 Implementation We employ a 3-layer speech encoding scheme similar to that of Duong et al. (2016). The first bidirectional layer receives the audio sequence in the form of 39-dimensional Perceptual Linear Predictive (PLP) features (Hermansky, 1990) computed over overlapping 25ms-wide windows every 10ms. The second and third layers consist of LSTMs with hidden state sizes of 128 and 512 respectively. Each layer encodes every second output of the previous layer. Thus, the sequence is downsampled by a factor of 4, decreasing the computation load for the attention mechanism and the decoders. In the speech experiments, the decoders 4 The data preprocessing scripts are released with the rest of"
N18-1008,P17-2012,0,0.0236087,"ds the reconstruction models in achiev slightly higher BLEU scores in 3 out of the 6 cases. The transitivity regularizer is even more effective: in 9 out the 12 source-target language combinations, the triangle models achieve higher performance when trained using the regularizer. Some of them are statistical significant improvements, as in the case of French to English where English is the intermediate target language and German is the final target. 7 Multitask learning (Caruana, 1998) has found extensive use across several machine learning and NLP fields. For example, Luong et al. (2016) and Eriguchi et al. (2017) jointly learn to parse and translate; Kim et al. (2017) combine CTC- and attention-based models using multitask models for speech transcription; Dong et al. (2015) use multitask learning for multiple language translation. Toshniwal et al. (2017) apply multitask learning to neural speech recognition in a less traditional fashion: the lower-level outputs of the speech encoder are used for fine-grained auxiliary tasks such as predicting HMM states or phonemes, while the final output of the encoder is passed to a characterlevel decoder. Related Work The speech translation problem has been traditi"
N18-1008,P06-2112,0,0.0174747,"r: X 1 c2m = α12 mm0 sm0 s2m P(y2m ) Maximum likelihood estimation Define the score of a sentence triple to be a loglinear interpolation of the two decoders’ probabilities: n s1m P(y1m ) Learning and Inference 3.2 Regularization We can optionally add a regularization term to the objective function, in order to encourage our attention mechanisms to conform to two intuitive principles of machine translation: transitivity and invertibility. m0 = dec2 (s2m−1 , c2m , y2m−1 ) = softmax(s2m ). Transitivity attention regularizer To a first approximation, the translation relation should be transitive (Wang et al., 2006; Levinboim and Chiang, 2015): If source word xi aligns to target word 1 For simplicity, we have assumed only a single layer for both the encoder and decoder. It is possible to use multiple stacked RNNs; typically, the output of the encoder and decoder (cm and P(ym ), respectively) would be computed from the top layer only. 83 P(y21 · · · y2M2 ) P(y21 · · · y2M2 ) softmax s21 · · · s2M2 P(y1 · · · y M ) softmax s1 · · · s M decoder c1 · · · c M attention h1 · · · hN encoder x1 · · · xN (a) single-task P(y11 · · · y1M1 ) softmax s11 · · · s1M1 P(y21 · · · y2M2 ) decoder c11 · · · c1M1 attention"
N18-1008,N15-1129,1,0.853177,"0 sm0 s2m P(y2m ) Maximum likelihood estimation Define the score of a sentence triple to be a loglinear interpolation of the two decoders’ probabilities: n s1m P(y1m ) Learning and Inference 3.2 Regularization We can optionally add a regularization term to the objective function, in order to encourage our attention mechanisms to conform to two intuitive principles of machine translation: transitivity and invertibility. m0 = dec2 (s2m−1 , c2m , y2m−1 ) = softmax(s2m ). Transitivity attention regularizer To a first approximation, the translation relation should be transitive (Wang et al., 2006; Levinboim and Chiang, 2015): If source word xi aligns to target word 1 For simplicity, we have assumed only a single layer for both the encoder and decoder. It is possible to use multiple stacked RNNs; typically, the output of the encoder and decoder (cm and P(ym ), respectively) would be computed from the top layer only. 83 P(y21 · · · y2M2 ) P(y21 · · · y2M2 ) softmax s21 · · · s2M2 P(y1 · · · y M ) softmax s1 · · · s M decoder c1 · · · c M attention h1 · · · hN encoder x1 · · · xN (a) single-task P(y11 · · · y1M1 ) softmax s11 · · · s1M1 P(y21 · · · y2M2 ) decoder c11 · · · c1M1 attention softmax s21 · · · s2M2 decod"
N18-1008,N15-1063,1,0.839641,"s sake there are dependencies not shown. y1j and y1j aligns to target word y2k , then xi should also probably align to y2k . To encourage the model to preserve this relationship, we add the following transitivity regularizer to the loss function of the triangle models with a small weight λtrans = 0.2: 2 Ltrans = score(Y1 , Y2 ) − λtrans A12 A1 − A2 2 . Corpus Ainu-English Mboshi-French Spanish-English Segments Hours 1 3 240 2,668 5,131 17,394 2.5 4.4 20 Table 1: Statistics on our speech datasets. Invertibility attention regularizer The translation relation also ought to be roughly invertible (Levinboim et al., 2015): if, in the reconstruction version of the cascade model, source word xi aligns to target word y1j , then it stands to reason that y j is likely to align to xi . So, whereas Tu et al. (2017) let the attentions of the translator and the reconstructor be unrelated, we try adding the following invertibility regularizer to encourage the attentions to each be the inverse of the other, again with a weight λinv = 0.2: 2 Linv = score(Y1 , Y2 ) − λinv A1 A12 − I 2 . 3.3 Speakers through beam search a set of candidate transˆ 2 , each with a score P(Y ˆ 2 ). lations Y 3. We then output the combination th"
N18-1008,I17-2050,1,0.804707,"a set of candidate transˆ 2 , each with a score P(Y ˆ 2 ). lations Y 3. We then output the combination that yields the highest total score(Y1 , Y2 ). 3.4 Implementation All our models are implemented in DyNet (Neubig et al., 2017).2 We use a dropout of 0.2, and train using Adam with initial learning rate of 0.0002 for a maximum of 500 epochs. For testing, we select the model with the best performance on dev. At inference time, we use a beam size of 4 for each decoder (due to GPU memory constraints), and the beam scores include length normalization (Wu et al., 2016) with a weight of 0.8, which Nguyen and Chiang (2017) found to work well for lowresource NMT. Decoding Since we have two decoders, we now need to employ a two-phase beam search, following Tu et al. (2017): 1. The first decoder produces, through standard beam search, a set of triples each consistˆ 1 , a score ing of a candidate transcription Y ˆ 1 ), and a hidden state sequence S. ˆ P(Y 4 Speech Transcription and Translation We focus on speech transcription and translation of endangered languages, using three different cor2. For each transcription candidate from the first decoder, the second decoder now produces 2 Our code is available at: https:"
N18-1008,2013.iwslt-papers.14,0,0.268253,"zaville, without standard orthography. We use a corpus (Godard et al., 2017) of 5517 parallel utterances (about 4.4 hours of audio) collected from three native speakers. The corpus provides non-standard grapheme transcriptions (close to the language phonology) produced by linguists, as well as French translations. We sampled 100 segments from the training set to be our dev set, and used the original dev set (514 sentences) as our test set. Data Spanish is, of course, not an endangered language, but the availability of the CALLHOME Spanish Speech dataset (LDC2014T23) with English translations (Post et al., 2013) makes it a convenient language to work with, as has been done in almost all previous work in this area. It consists of telephone conversations between relatives (about 20 total hours of audio) with more than 240 speakers. We use the original train-dev-test split, with the training set comprised of 80 conversations and dev and test of 20 conversations each. Hokkaido Ainu is the sole surviving member of the Ainu language family and is generally considered a language isolate. As of 2007, only ten native speakers were alive. The Glossed Audio Corpus of Ainu Folklore provides 10 narratives with au"
N18-1008,P16-1162,0,0.125632,"gle-task base reverse 7.99 11.31 7.57 11.82 7.78 11.56 7.59 9.29 16.41 14.75 10.38 11.40 8.93 7.42 9.78 10.00 9.33 8.52 8.66 10.46 15.48 16.36 11.02 12.76 reconstruction + 0.2Linv reconstruction + 0.5Linv Table 3: The reconstruction model with the invertibility regularizer produces more informed attentions that result in better word discovery for Mboshi with an Mboshi-French model. Scores reported by previous work are in italics and best scores from our experiments are in bold. (en), French (fr) and German (de). All the sequences are represented by subword units with byte-pair encoding (BPE) (Sennrich et al., 2016) trained on each language with 32000 operations. reproduce the significant gains that were reported when using the reverse model (italicized in Table 3). Also, our version of both the base and reverse singletask models performed better than our reimplementation of the baseline. Furthermore, we found that we were able to obtain even better performance at the type level by combining the attention matrices of a reconstruction model trained with the invertibility regularizer. Boito et al. (2017) reported that combining the attention matrices of a base and a reverse model significantly reduced perf"
N18-1008,D16-1133,1,\N,Missing
N18-1031,P16-1160,0,0.0438599,"r the fixnorm model, but for the task of face verification. Handling rare words is an important problem for NMT that has been approached in various ways. Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well. Recently, Liu and Kirchhoff (2018) have shown that their baseline NMT system with BPE already outperforms Moses for low-resource translation. However, in their work, they use the Transformer network (Vaswani et al., 2017), which is quite different from our baseline model. It would be interesting to see if our methods benefit the TransLexicon One byproduct of"
N18-1031,P16-1154,0,0.0506249,"s, they focus on the directions, modifying the loss function to try to learn a classifier that separates the classes’ directions with something like a margin. Wang et al. (2017a) also make the same observation that we do for the fixnorm model, but for the task of face verification. Handling rare words is an important problem for NMT that has been approached in various ways. Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well. Recently, Liu and Kirchhoff (2018) have shown that their baseline NMT system with BPE already outperforms Moses for low-resource transl"
N18-1031,P16-1014,0,0.0492102,"ocused on the magnitudes, they focus on the directions, modifying the loss function to try to learn a classifier that separates the classes’ directions with something like a margin. Wang et al. (2017a) also make the same observation that we do for the fixnorm model, but for the task of face verification. Handling rare words is an important problem for NMT that has been approached in various ways. Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well. Recently, Liu and Kirchhoff (2018) have shown that their baseline NMT system with BPE already outperforms Moses for lo"
N18-1031,P15-1001,0,0.0764454,"reformulate the output layer in terms of directions and magnitudes, as we do here. Whereas we have focused on the magnitudes, they focus on the directions, modifying the loss function to try to learn a classifier that separates the classes’ directions with something like a margin. Wang et al. (2017a) also make the same observation that we do for the fixnorm model, but for the task of face verification. Handling rare words is an important problem for NMT that has been approached in various ways. Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well. Recently, Liu"
N18-1031,D16-1162,0,0.170945,"d Jenner. All three surnames occur in the training data with reference to immunologists: Fauci is the director of the National Institute of Allergy and Infectious Diseases, Margaret (not James) Chan is the former director of the World Health Organization, and Edward Jenner invented smallpox vaccine. But Chan is more frequent in the training data than Fauci, and James is more frequent than either Anthony or Margaret. Because NMT learns word representations in continuous space, it tends to translate words that “seem natural in the context, but do not reflect the content of the source sentence” (Arthur et al., 2016). This coincides with other observations that NMT’s translations are often fluent but lack accuracy (Wang et al., 2017b; Wu et al., 2016). Why does this happen? At each time step, the model’s distribution over output words e is We explore two solutions to the problem of mistranslating rare words in neural machine translation. First, we argue that the standard output layer, which computes the inner product of a vector representing the context with all possible output word embeddings, rewards frequent words disproportionately, and we propose to fix the norms of both vectors to a constant value."
N18-1031,W04-3250,0,0.177981,"hand, fixnorm+lex gets this right. To better understand how the lexical module helps in this case, we look at the top five translations for the word Fauci in fixnorm+lex: e cos θWe ,h˜ cos θWel ,hl be + ble logit source word with the highest attention score (Luong et al., 2015b). Evaluation For translation into English, we report case-sensitive NIST BLEU against detokenized references. For English-Japanese and English-Vietnamese, we report tokenized, casesensitive BLEU following Arthur et al. (2016) and Luong and Manning (2015). We measure statistical significance using bootstrap resampling (Koehn, 2004). 6 6.1 Fauci UNK Anthony Ahmedova Chan Results and Analysis Overall 0.522 0.566 0.263 0.555 0.546 0.762 −0.009 0.644 0.173 0.150 −8.71 −1.25 −8.70 −8.66 −8.73 7.0 5.6 2.4 0.3 −0.2 Our results are shown in Table 3. First, we observe, as has often been noted in the literature, that NMT tends to perform poorer than PBMT on low resource settings (note that the rows of this table are sorted by training data size). Our fixnorm system alone shows large improvements (shown in parentheses) relative to tied. Integrating the lexical module (fixnorm+lex) adds in further gains. Our fixnorm+lex models surp"
N18-1031,P07-2045,0,0.0165964,"tegrate a simple lexical module which is jointly trained with the rest of the model. We evaluate our approaches on eight language pairs with data sizes ranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU, surpassing phrasebased translation in nearly all settings.1 1 Introduction Neural network approaches to machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Gehring et al., 2017) are appealing for their single-model, end-to-end training process, and have demonstrated competitive performance compared to earlier statistical approaches (Koehn et al., 2007; Junczys-Dowmunt et al., 2016). However, there are still many open problems in NMT (Koehn and Knowles, 2017). One particular issue is mistranslation of rare words. For example, consider the Uzbek sentence:   p(e) ∝ exp We · h˜ + be where We and be are a vector and a scalar depending only on e, and h˜ is a vector depending only on the source sentence and previous output words. We propose two modifications to this layer. First, ˜ which measures how we argue that the term We · h, ˜ well e fits into the context h, favors common words disproportionately, and show that it helps to fix the norm of"
N18-1031,W17-3204,0,0.0932846,"proaches on eight language pairs with data sizes ranging from 100k to 8M words, and achieve improvements of up to +4.3 BLEU, surpassing phrasebased translation in nearly all settings.1 1 Introduction Neural network approaches to machine translation (Sutskever et al., 2014; Bahdanau et al., 2015; Luong et al., 2015a; Gehring et al., 2017) are appealing for their single-model, end-to-end training process, and have demonstrated competitive performance compared to earlier statistical approaches (Koehn et al., 2007; Junczys-Dowmunt et al., 2016). However, there are still many open problems in NMT (Koehn and Knowles, 2017). One particular issue is mistranslation of rare words. For example, consider the Uzbek sentence:   p(e) ∝ exp We · h˜ + be where We and be are a vector and a scalar depending only on e, and h˜ is a vector depending only on the source sentence and previous output words. We propose two modifications to this layer. First, ˜ which measures how we argue that the term We · h, ˜ well e fits into the context h, favors common words disproportionately, and show that it helps to fix the norm of both vectors to a constant. Second, we add a new term representing a more direct connection from the source"
N18-1031,W18-1806,0,0.08314,"2015; Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well. Recently, Liu and Kirchhoff (2018) have shown that their baseline NMT system with BPE already outperforms Moses for low-resource translation. However, in their work, they use the Transformer network (Vaswani et al., 2017), which is quite different from our baseline model. It would be interesting to see if our methods benefit the TransLexicon One byproduct of lex is the lexicon, which we can extract and examine simply by feeding each source word embedding to the FFNN module and calculating p` (y) = softmax(W ` h` +b` ). In Table 5, we show the top translations for some entries in the lexicons extracted from fixnorm+lex for Hung"
N18-1031,2015.iwslt-evaluation.11,0,0.157768,"source sentences to the encoder in reverse order during both training and testing, following Luong et al. (2015a). Information about the number and size of hidden layers is shown in Table 2. The word embedding size is always equal to the hidden layer size. Following common practice, we only trained on sentences of 50 tokens or less. We limited the vocabulary to word types that appear no less than 5 times in the training data and map the rest to UNK. For the English-Japanese and English-Vietnamese datasets, we used the vocabulary sizes reported in their respective papers (Arthur et al., 2016; Luong and Manning, 2015). For fixnorm, we tried r ∈ {3, 5, 7} and selected the best value based on the development set performance, which was r = 5 except for EnglishJapanese (BTEC), where r = 7. For fixnorm+lex, because W s h˜ t +W ` h`t takes on values in [−2r2 , 2r2 ], we reduced √ our candidate r values by roughly a factor of 2, to r ∈ {2, 3.5, 5}. A radius r = 3.5 seemed to work the best for all language pairs. • English to Vietnamese (vi), using data from the IWSLT 2015 shared task.2 • To compare our approach with that of Arthur et al. (2016), we also ran on their English to Japanese (ja) KFTT and BTEC datasets"
N18-1031,P16-1100,0,0.0424852,"but for the task of face verification. Handling rare words is an important problem for NMT that has been approached in various ways. Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well. Recently, Liu and Kirchhoff (2018) have shown that their baseline NMT system with BPE already outperforms Moses for low-resource translation. However, in their work, they use the Transformer network (Vaswani et al., 2017), which is quite different from our baseline model. It would be interesting to see if our methods benefit the TransLexicon One byproduct of lex is the lexicon, which"
N18-1031,D15-1166,0,0.266357,"uchi to UNK UNK (which is arguably better than James Chan). On the other hand, fixnorm+lex gets this right. To better understand how the lexical module helps in this case, we look at the top five translations for the word Fauci in fixnorm+lex: e cos θWe ,h˜ cos θWel ,hl be + ble logit source word with the highest attention score (Luong et al., 2015b). Evaluation For translation into English, we report case-sensitive NIST BLEU against detokenized references. For English-Japanese and English-Vietnamese, we report tokenized, casesensitive BLEU following Arthur et al. (2016) and Luong and Manning (2015). We measure statistical significance using bootstrap resampling (Koehn, 2004). 6 6.1 Fauci UNK Anthony Ahmedova Chan Results and Analysis Overall 0.522 0.566 0.263 0.555 0.546 0.762 −0.009 0.644 0.173 0.150 −8.71 −1.25 −8.70 −8.66 −8.73 7.0 5.6 2.4 0.3 −0.2 Our results are shown in Table 3. First, we observe, as has often been noted in the literature, that NMT tends to perform poorer than PBMT on low resource settings (note that the rows of this table are sorted by training data size). Our fixnorm system alone shows large improvements (shown in parentheses) relative to tied. Integrating the l"
N18-1031,P15-1002,0,0.0501751,"Missing"
N18-1031,P16-2021,0,0.0391268,"put layer in terms of directions and magnitudes, as we do here. Whereas we have focused on the magnitudes, they focus on the directions, modifying the loss function to try to learn a classifier that separates the classes’ directions with something like a margin. Wang et al. (2017a) also make the same observation that we do for the fixnorm model, but for the task of face verification. Handling rare words is an important problem for NMT that has been approached in various ways. Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well. Recently, Liu and Kirchhoff (201"
N18-1031,P02-1038,0,0.28744,"0.5 14.2 16.1 27.1 28.8   ˜ cos θW ,h˜ + be , p(e) ∝ exp kWe k khk e where We is the embedding of e, be is the e’th component of the bias bo , and θWe ,h˜ is the angle ˜ We can intuitively interpret the between We and h. ˜ has the effect of terms as follows. The term khk sharpening or flattening the distribution, reflecting whether the model is more or less certain in a particular context. The cosine similarity cos θWe ,h˜ measures how well e fits into the context. The bias be controls how much the word e is generated; it is analogous to the language model in a log-linear translation model (Och and Ney, 2002). Finally, kWe k also controls how much e is generated. Figure 1 shows that it generally correlates with frequency. But because it is multiplied by cos θWe ,h˜ , it has a stronger effect on words whose ˜ and less embeddings have direction similar to h, effect or even a negative effect on words in other directions. We hypothesize that the result is that the model learns kWe k that are disproportionately large. For example, returning to the example from Section 1, these terms are: Table 1: Preliminary experiments show that tying target embeddings with output layer weights performs as well as or"
N18-1031,J03-1002,0,0.0578402,"ion not only about the source word(s) corresponding to the current target word, but also the contexts of those source words and the preceding context of the target word. This could make the model prone to generate a target word that fits the context but doesn’t necessarily correspond to the source word(s). Count-based statistical models, by contrast, don’t have this problem, because they simply don’t model any of this context. Arthur et al. (2016) try to alleviate this issue by integrating a count-based lexicon into an NMT system. However, this lexicon must be trained separately using GIZA++ (Och and Ney, 2003), and its parameters form a large, sparse array, which can be difficult to store in GPU memory. We propose instead to use a simple feedforward neural network (FFNN) that is trained jointly with the rest of the NMT model to generate a target word based directly on the source word(s). Let f s (s = 1, . . . , m) be the embeddings of the source words. We use the attention weights to form a p(yt |y&lt;t , x) = softmax(W o h˜ t + bo + W ` h`t + b` ). For the same reasons that were given in Section 3 for normalizing h˜ t and the rows of Wto , we normalize h`t and the rows of W ` as well. Note, however,"
N18-1031,E17-2025,0,0.123924,"ent could be made for kh˜ t k: because a large kh˜ t k sharpens the distribution, causing frequent words to more strongly dominate rare words, we might want to limit it as well. We compared both approaches on a development set and ˜ found that replacing h˜ t in equation (1) with r khh˜ t k t indeed performs better, as shown in Table 1. (1) The rows of the output layer’s weight matrix W o can be thought of as embeddings of the output vocabulary, and sometimes are in fact tied to the embeddings in the input layer, reducing model size while often achieving similar performance (Inan et al., 2017; Press and Wolf, 2017). We verified this claim on some language pairs and found out that this approach usually performs better than without tying, as seen in Table 1. For this reason, we always tie the target embeddings and W o in all of our models. 335 kWe k 8 ha-en tu-en hu-en 6 ta-en ur-en ha-en tu-en uz-en hu-en en-vi en-ja (BTEC) en-ja (KFTT) 4 2 ha-en tu-en hu-en be 0 −2 5,000 10,000 0.2/0.1 0.2/0.2 0.8/0.8 0.8/1.1 1.5/1.9 2.0/2.3 2.1/2.6 3.6/5.0 7.8/8.0 4.0/3.4 4.2/4.2 10.6/10.4 21.1/13.3 29.8/17.4 27.3/15.7 17.0/7.7 17.8/21.8 48.2/49.1 layers num/size 1/512 1/512 2/512 2/512 2/512 2/512 2/512 4/768 4/768 we"
N18-1031,P16-1162,0,0.344563,"servation that we do for the fixnorm model, but for the task of face verification. Handling rare words is an important problem for NMT that has been approached in various ways. Some have focused on reducing the number of UNKs by enabling NMT to learn from a larger vocabulary (Jean et al., 2015; Mi et al., 2016); others have focused on replacing UNKs by copying source words (Gulcehre et al., 2016; Gu et al., 2016; Luong et al., 2015b). However, these methods only help with unknown words, not rare words. An approach that addresses both unknown and rare words is to use subword-level information (Sennrich et al., 2016; Chung et al., 2016; Luong and Manning, 2016). Our approach is different in that we try to identify and address the root of the rare word problem. We expect that our models would benefit from more advanced UNKreplacement or subword-level techniques as well. Recently, Liu and Kirchhoff (2018) have shown that their baseline NMT system with BPE already outperforms Moses for low-resource translation. However, in their work, they use the Transformer network (Vaswani et al., 2017), which is quite different from our baseline model. It would be interesting to see if our methods benefit the TransLexic"
N18-1116,P16-1100,0,0.233867,"oth word-level and character-level information can be helpful for generating better representations, current research which tries to exploit both word-level and character-level information only composed the word-level representation by character embeddings with the word boundary information (Ling et al., 2015b; Costa-juss`a and 1284 Proceedings of NAACL-HLT 2018, pages 1284–1293 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Fonollosa, 2016) or replaces the word representation with its inside characters when encountering the out-of-vocabulary words (Luong and Manning, 2016; Wu et al., 2016). In this paper, we propose a novel encoder-decoder model that makes use of both character and word information. More specifically, we augment the standard encoder to attend to individual characters to generate better source word representations (§3.1). We also augment the decoder with a second attention that attends to the source-side characters to generate better translations (§3.2). To demonstrate the effectiveness of the proposed model, we carry out experiments on three translation tasks: Chinese-English, EnglishChinese and English-German. Our experiments show that: (1) t"
N18-1116,W17-4712,0,0.0236227,"improve the model by incorporating multiple levels of granularity. Specifically, we propose (1) an encoder with character attention which augments the (sub)word-level representation with character-level information; (2) a decoder with multiple attentions that enable the representations from different levels of granularity to control the translation cooperatively. Experiments on three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model and a strong characterbased model. 1 Introduction Neural machine translation (NMT) models (Britz et al., 2017) learn to map from source language sentences to target language sentences via continuous-space intermediate representations. Since word is usually thought of as the basic unit of language communication (Jackendoff, 1992), early NMT systems built these representations starting from the word level (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Weng et al., 2017). Later systems tried using smaller units such as subwords to address the problem of out-of-vocabulary (OOV) words (Sennrich et al., 2016; Wu et al., 2016). Although they obtain reasonable results, these word or sub-wor"
N18-1116,P16-2058,0,0.0380798,"Missing"
N18-1116,W17-4739,0,0.0442815,"Missing"
N18-1116,P16-1162,0,0.204501,"trong characterbased model. 1 Introduction Neural machine translation (NMT) models (Britz et al., 2017) learn to map from source language sentences to target language sentences via continuous-space intermediate representations. Since word is usually thought of as the basic unit of language communication (Jackendoff, 1992), early NMT systems built these representations starting from the word level (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Weng et al., 2017). Later systems tried using smaller units such as subwords to address the problem of out-of-vocabulary (OOV) words (Sennrich et al., 2016; Wu et al., 2016). Although they obtain reasonable results, these word or sub-word methods still have some potential weaknesses. First, the learned representations ∗ Corresponding author. of (sub)words are based purely on their contexts, but the potentially rich information inside the unit itself is seldom explored. Taking the Chinese word 被打伤 (bei-da-shang) as an example, the three characters in this word are a passive voice marker, “hit” and “wound”, respectively. The meaning of the whole word, “to be wounded”, is fairly compositional. But this compositionality is ignored if the whole word"
N18-1116,D17-1013,1,0.828696,"three translation tasks demonstrate that our proposed models outperform the standard word-based model, the subword-based model and a strong characterbased model. 1 Introduction Neural machine translation (NMT) models (Britz et al., 2017) learn to map from source language sentences to target language sentences via continuous-space intermediate representations. Since word is usually thought of as the basic unit of language communication (Jackendoff, 1992), early NMT systems built these representations starting from the word level (Sutskever et al., 2014; Bahdanau et al., 2015; Cho et al., 2014; Weng et al., 2017). Later systems tried using smaller units such as subwords to address the problem of out-of-vocabulary (OOV) words (Sennrich et al., 2016; Wu et al., 2016). Although they obtain reasonable results, these word or sub-word methods still have some potential weaknesses. First, the learned representations ∗ Corresponding author. of (sub)words are based purely on their contexts, but the potentially rich information inside the unit itself is seldom explored. Taking the Chinese word 被打伤 (bei-da-shang) as an example, the three characters in this word are a passive voice marker, “hit” and “wound”, respe"
N18-1116,Q17-1026,0,0.151135,"e word or sub-word boundaries can be non-trivial. For languages like Chinese and Japanese, a word segmentation step is needed, which must usually be trained on labeled data. For languages like English and German, word boundaries are easy to detect, but subword boundaries need to be learned by methods like BPE. In both cases, the segmentation model is trained only in monolingual data, which may result in units that are not suitable for translation. On the other hand, there have been multiple efforts to build models operating purely at the character level (Ling et al., 2015a; Yang et al., 2016; Lee et al., 2017). But splitting this finely can increase potential ambiguities. For example, the Chinese word 红茶 (hong-cha) means “black tea,” but the two characters means “red” and “tea,” respectively. It shows that modeling the character sequence alone may not be able to fully utilize the information at the word or sub-word level, which may also lead to an inaccurate representation. A further problem is that character sequences are longer, making them more costly to process with a recurrent neural network model (RNN). While both word-level and character-level information can be helpful for generating better"
N18-1116,C16-1288,0,0.0317945,"Missing"
N19-1311,Q17-1010,0,0.0238815,"Missing"
N19-1311,P18-1008,0,0.0322033,"errors or typos in English. Don’t try to translate the sentences word for word (e.g. replicate the error in Spanish). Instead, try to translate it as if it was a grammatical sentence, and produce a fluent grammatical Spanish sentence that captures its meaning. 3 Our LSTM models are implemented using DyNet (Neubig et al., 2017), and our transformer models using PyTorch (Paszke et al., 2017). The transformer model uses 6 layers, 8 attention heads, the dimension for embeddings and positional feedforward are 512 and 2048 respectively . The sublayer computation sequence follows the guidelines from Chen et al. (2018). Dropout probability is set to 0.2 (also in the source embeddings, following Sperber et al. (2017)). We use the learning rate schedule in Vaswani et al. (2017) with warm-up steps of 24000 but only decay the learning rate until it reaches 10−5 as inspired by Chen et al. (2018). For testing, we select the model with the best performance on the dev set corresponding to the test set. At inference time, we use a beam size of 4 with length normalization (Wu et al., 2016) with a weight of 0.6. Experiments 3.2 In this section, we provide implementation details and the results of our NMT experiments."
N19-1311,W13-1703,0,0.0884174,"est2013 as dev and test sets, respectively. Furthermore, to test our translation methods on real grammatical errors, we introduce a new collection of Spanish translations of the JFLEG corpus (§2.3). 2.1 Grammar Error Correction Corpora To our knowledge, there are five publicly available corpora of non-native English that are annotated with corrections, which have been widely used for research in Grammar Error Correction (GEC). The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus (Yannakoudakis et al., 2011), which is only partially public, the Lang-8 corpus (Tajiri et al., 2012), which was harvested from online corrections, and the AESW 2016 Shared Task corpus, which contains corrections on texts from scientific journals. The last corpus is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). This corpus covers a wider range of English proficiency leve"
N19-1311,W18-1807,0,0.416553,"erformance of NMT. Unlike our 3075 Correct article a a an the ∅ all – 0 −4.1 −3.1 −3.8 Substituted article an the ∅ all −2.0 −5.7 – −1.5 −1.5 −2.1 −6.3 −1.8 −1.7 −1.7 0 – −2.2 −3.7 −3.4 −2.1 −7.3 −1.7 – −1.8 Substitution Table 6: Effect of article substitutions in test data (art) relative to clean test data (clean), broken down by substitution type. Different article substitutions have very different impacts on BLEU; changing an indefinite article to definite is especially damaging. work, they primarily focus on a setting where the training set is noisy but the test set is clean. In addition, Heigold et al. (2018) evaluated the robustness of word embeddings against word scrambling noise, and showed that performance in downstream tasks like POS-tagging and MT is especially hurt. Sakaguchi et al. (2017a) studied word scrambling and the Cmabrigde Uinervtisy (Cambridge University) effect, where humans are able to understand the meaning of sentences with scrambled words, performing word recognition (word level spelling correction) with a semi-character RNN system. Focusing only on character-level NMT models, Belinkov and Bisk (2018) showed that they exhibit degraded performance when presented with noisy tes"
N19-1311,N18-1055,0,0.034189,"e-side error types on the behavior of our NMT system, when trained on clean data and tested on the artificial noisy data that we created. Art Errors Table 6 shows the difference of the BLEU scores obtained on the sentences, broken down by the type of article error that was introduced. The first observation is that in all cases the difference is negative, meaning that we get higher BLEU scores when testing on clean data. Encouragingly, there is practically no difference when we substitute ‘a’ with ‘an’ or ‘an’ with ‘a’; the model 3 This model has been recently surpassed by other systems, e.g. (Junczys-Dowmunt et al., 2018), but their outputs are not available online. seems to have learned very similar representations for the two indefinite articles, and as a result such an error has no impact on the produced output. However, we observe larger performance drops when substituting indefinite articles with the definite one and vice versa; since the target language makes the same article distinction as the source language, any article source error is propagated to the produced translation. Prep Errors Due to the large number of prepositions, we cannot present a full analysis of preposition errors, but highlights are"
N19-1311,W18-2709,0,0.0272567,"t preposition or inserting a wrong one leads to performance drops of 1.2 and 0.8 BLEU points for the clean model, but drops of 0.4 and 0.7 for the mix-all model. Nn and Sva Errors We found no significant performance difference between the different nn errors. Incorrectly pluralizing a noun has the same adverse effect as singularizing it, leading to performance reductions of over 4.0 and 3.5 BLEU points respectively. We observe a similar behavior with sva errors: each error type leads to roughly the same performance degradation. 5 Related Work The effect of noise in NMT was recently studied by Khayrallah and Koehn (2018), who explored noisy situations during training due to webcrawled data. This type of noise includes misaligned, mistranslated, or untranslated sentences which, when used during training, significantly degrades the performance of NMT. Unlike our 3075 Correct article a a an the ∅ all – 0 −4.1 −3.1 −3.8 Substituted article an the ∅ all −2.0 −5.7 – −1.5 −1.5 −2.1 −6.3 −1.8 −1.7 −1.7 0 – −2.2 −3.7 −3.4 −2.1 −7.3 −1.7 – −1.8 Substitution Table 6: Effect of article substitutions in test data (art) relative to clean test data (clean), broken down by substitution type. Different article substitutions h"
N19-1311,2005.mtsummit-papers.11,0,0.0798649,"om/ antonis/nmt-grammar-noise 3070 Proceedings of NAACL-HLT 2019, pages 3070–3080 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics source of natural noise. Moreover, since English is probably the most commonly spoken non-native language (Lewis et al., 2009), our work could be directly applicable to several translation applications. Our choice of Spanish as a target language enables us to have access to existing parallel data and easily create new parallel corpora (see below, §2.3). For all experiments, we use the Europarl English-Spanish dataset (Koehn, 2005) as our training set. In the synthetic experiments of Section §2.2, we use the newstest2012 and newstest2013 as dev and test sets, respectively. Furthermore, to test our translation methods on real grammatical errors, we introduce a new collection of Spanish translations of the JFLEG corpus (§2.3). 2.1 Grammar Error Correction Corpora To our knowledge, there are five publicly available corpora of non-native English that are annotated with corrections, which have been widely used for research in Grammar Error Correction (GEC). The NUS Corpus of Learner English (NUCLE) contains essays written by"
N19-1311,W17-3204,0,0.0418515,"rrors. 1 Introduction Neural Machine Translation (NMT) is undeniably a success story: public benchmarks (Bojar et al., 2016) are dominated by neural systems, and neural approaches are the de facto option for industrial systems (Wu et al., 2016; Hassan Awadalla et al., 2018; Crego et al., 2016; Hieber et al., 2018). Even under low-resource conditions, neural models were recently shown to outperform traditional statistical approaches (Nguyen and Chiang, 2018). However, there are still several shortcomings of NMT that need to be addressed: a (nonexhaustive) list of six challenges is discussed by Koehn and Knowles (2017), including outof-domain testing, rare word handling, the widebeam problem, and the large amount of data needed for learning. An additional challenge is robustness to noise, both during training and at inference time. In this paper, we study the effect of a specific type of noise in NMT: grammatical errors. We primarily focus on errors that are made by non-native † Equal contribution. Work performed at the University of Notre Dame. source-language speakers (as opposed to dialectal language, SMS or Twitter language). Not only is this linguistically important, but we believe that it would potent"
N19-1311,E17-2004,0,0.0923684,"al. (2017) proposed a noiseintroduction system reminiscent of WER, based on insertions, deletions, and substitutions. An NMT system tested on correct transcriptions achieves a BLEU score of 55 (4 references), but tested on the ASR transcriptions it only achieves a BLEU score of 35.7. By introducing similar noise in the training data, they were able to make the NMT system slightly more robust. Interestingly, they found that the optimal amount of noise on the training data is smaller than the amount of noise on the test data. The notion of linguistically plausible corruption is also explored by Li et al. (2017), who created adversarial examples with syntactic and semantic noise (reordering and word substitutions respecmodel BLEU difference clean mix-all in→with on→for to→on in→ ∅ ∅→for −6.7 −6.0 −2.9 −1.8 −1.6 −1.7 −0.1 −0.5 −1.9 −0.6 ∅→any any→∅ −1.2 −0.8 −0.4 −0.7 Table 7: Effect of selected preposition substitutions in test data (prep) relative to clean test data (clean), for the clean and mix-all models. The mix-all model handles most errors more efficiently. tively). When training with these noisy datasets, they obtained better performance on several text classification tasks. Furthermore, in a"
N19-1311,E17-2037,0,0.0575645,"ingapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus (Yannakoudakis et al., 2011), which is only partially public, the Lang-8 corpus (Tajiri et al., 2012), which was harvested from online corrections, and the AESW 2016 Shared Task corpus, which contains corrections on texts from scientific journals. The last corpus is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). This corpus covers a wider range of English proficiency levels on the source side, and its correction annotations include extended fluency edits rather than just minimal grammatical ones. That way, the corrected sentence is not just grammatical, but also guaranteed to be fluent. 2.2 Synthetic grammar errors Ideally, we would train a translation model to translate grammatically noisy language by training it on parallel data with grammatically noisy language. Since, to our knowledge, no such data exist in the quantities that would be needed, an alError Type art prep nn sva Confusion Set {a, an"
N19-1311,W14-1701,0,0.0664639,"Missing"
N19-1311,W13-3601,0,0.0382148,"rs, we introduce a new collection of Spanish translations of the JFLEG corpus (§2.3). 2.1 Grammar Error Correction Corpora To our knowledge, there are five publicly available corpora of non-native English that are annotated with corrections, which have been widely used for research in Grammar Error Correction (GEC). The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus (Yannakoudakis et al., 2011), which is only partially public, the Lang-8 corpus (Tajiri et al., 2012), which was harvested from online corrections, and the AESW 2016 Shared Task corpus, which contains corrections on texts from scientific journals. The last corpus is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). This corpus covers a wider range of English proficiency levels on the source side, and its correction annotations include extended fluency edits rather than just min"
N19-1311,P17-1070,0,0.0335914,"Missing"
N19-1311,N18-1031,1,0.783989,"l errors. We also present a set of Spanish translations of the JFLEG grammar error correction corpus, which allows for testing NMT robustness to real grammatical errors. 1 Introduction Neural Machine Translation (NMT) is undeniably a success story: public benchmarks (Bojar et al., 2016) are dominated by neural systems, and neural approaches are the de facto option for industrial systems (Wu et al., 2016; Hassan Awadalla et al., 2018; Crego et al., 2016; Hieber et al., 2018). Even under low-resource conditions, neural models were recently shown to outperform traditional statistical approaches (Nguyen and Chiang, 2018). However, there are still several shortcomings of NMT that need to be addressed: a (nonexhaustive) list of six challenges is discussed by Koehn and Knowles (2017), including outof-domain testing, rare word handling, the widebeam problem, and the large amount of data needed for learning. An additional challenge is robustness to noise, both during training and at inference time. In this paper, we study the effect of a specific type of noise in NMT: grammatical errors. We primarily focus on errors that are made by non-native † Equal contribution. Work performed at the University of Notre Dame. s"
N19-1311,D16-1161,0,0.0157284,"clean mix-all cor0 Manual correction cor1 cor2 cor3 avg. No corr. Auto corr. 28.4 27.7 28.8 28.1 28.6 27.8 26.2 26.8 27.0 26.7 29.1 28.1 28.2 27.5 Table 5: BLEU scores on the JFLEG-es dev and test datasets. Our proposed mix-all model is slightly behind the clean model on manually corrected input (cor[0–3]). On noisy input (No corr.) the mix-all outperforms the clean model (26.8 > 26.2). Preprocessing the noisy input with a GEC model (Auto corr.) slightly improves results. tem (column Auto corr of Table 5). We used the publicly available JFLEG outputs of the (almost) state-of-the-art model of Junczys-Dowmunt and Grundkiewicz (2016) as inputs to our NMT system.3 This experiment envisions a pipeline where the noisy source is first automatically corrected and then translated. As expected, this helps the clean model (by +1.1 BLEU), but our mixall training helps even further (by another +0.8 BLEU). Interestingly, the automatic GEC system only helps in the test set, while there are no improvements in the dev set. Naturally, since automatic GEC systems are imperfect, the performance of this pipeline still lags behind translating on clean data. 4 Analysis We attempt an in-depth analysis of the impact of the different source-sid"
N19-1311,P02-1040,0,0.106952,"e.g. the mix-all model will refer to the model trained on the mix-all dataset. 3.1 Implementation Details All data are tokenized, truecased, and split into subwords using Byte Pair Encoding (BPE) with 32,000 operations (Sennrich et al., 2016). We filter the training set to only contain sentences up to 80 words. Results We report the results obtained with the transformer model, as they were consistently better than the LSTM one. All the result tables for the LSTM models can be found in the Appendix. The performance of our systems on the synthetic WMT test sets, as measured by detokenized BLEU (Papineni et al., 2002), is summarized in Table 4. When the system is trained only on clean data (first row) and tested on noisy data, it unsurprisingly exhibits degraded performance. We observe significant drops in the range of 1.0–3.6 BLEU. 3073 clean drop En-Es WMT Test Set art prep nn sva clean 33.0 29.6 31.3 32.0 29.3 32.1 31.2 ± 1.5 drop art prep nn sva 31 31.2 30.4 30.4 31.2 30.2 28.4 27.8 27.9 28.7 30.0 30.8 29.3 28.9 30.2 30.0 30.2 30.3 29.5 30.3 28.3 27.7 27.4 29.8 28.2 30.6 30.8 29.9 29.8 30.9 30.0 ± 0.9 29.8 ± 1.4 29.2 ± 1.3 29.4 ± 0.8 29.9 ± 1.2 clean+drop clean+art clean+prep clean+nn clean+sva 32.9 32"
N19-1311,D14-1162,0,0.0810429,"Missing"
N19-1311,P06-1055,0,0.0446059,"of each training sentence, one without errors and one for each error. we obtain the probability of a noun being replaced with its singular or plural form. For sva errors, the probability that a present tense verb is replaced with its third-person-singular (3SG) or not-3SG form. An additional sva error that we included is the confusion between the appropriate form for the verb ‘to be’ in the past tense (‘was’ and ‘were’). The second step involves applying the noiseinducing transformations using our collected statistics as a prior. We obtained parses for each sentence using the Berkeley parser (Petrov et al., 2006). The parse tree allows us to identify candidate error positions in each sentence (for example, the beginning of a noun phrase without a determiner, were one could be inserted). For each error type we introduced exactly one error per sentence, wherever possible, which we believe matches more realistic scenarios than previous work. It also allows for controlled analysis of the behaviour of the NMT system (see Section 4). For each error and each sentence, we first identify candidate positions (based on the error type and the parse tree) and sample one of them based on the specific error distribu"
N19-1311,N18-1057,0,0.0270225,"Missing"
N19-1311,P11-1019,0,0.0713753,"orpora To our knowledge, there are five publicly available corpora of non-native English that are annotated with corrections, which have been widely used for research in Grammar Error Correction (GEC). The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus (Yannakoudakis et al., 2011), which is only partially public, the Lang-8 corpus (Tajiri et al., 2012), which was harvested from online corrections, and the AESW 2016 Shared Task corpus, which contains corrections on texts from scientific journals. The last corpus is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). This corpus covers a wider range of English proficiency levels on the source side, and its correction annotations include extended fluency edits rather than just minimal grammatical ones. That way, the corrected sentence is not just grammatical, but also guaranteed to be fluent. 2.2 Synthetic"
N19-1311,D10-1094,0,0.224091,"3SG, 2SG-Past, not 2SG-Past} Table 1: Confusion sets for each grammar error type. The art and prep sets include an empty token (∅) allowing for insertions and deletions. SG, PL, 2SG, and 3SG stand for singular, plural, second-person and thirdperson singular respectively. ternative is to add synthetic grammatical noise to clean data. An advantage of this approach is that controlled introduction of errors allows for finegrained analysis. This is a two-step process, similar to the methods used in the GEC literature for creating synthetic data based on confusion matrices (Rozovskaya et al., 2014; Rozovskaya and Roth, 2010; Xie et al., 2016; Sperber et al., 2017). First, we mimic the distribution of errors found in real data, and then introduce errors by applying rulebased transformations on automatic parse trees. The first step involves collecting error statistics on real data. Conveniently, the NUCLE corpus has all corrections annotated with 27 error codes. We focus on five types of errors, with the last four being the most common in the NUCLE corpus: • drop: randomly deleting one character from the sentence.2 • art: article/determiner errors • prep: preposition errors • nn: noun number errors • sva: subject-"
N19-1311,I17-2062,0,0.0405053,"Missing"
N19-1311,E17-3017,0,0.0360748,"Missing"
N19-1311,P16-1162,0,0.117385,"he best performance on the dev set corresponding to the test set. At inference time, we use a beam size of 4 with length normalization (Wu et al., 2016) with a weight of 0.6. Experiments 3.2 In this section, we provide implementation details and the results of our NMT experiments. For convenience, we will refer to each model with the same name as the dataset it was trained on; e.g. the mix-all model will refer to the model trained on the mix-all dataset. 3.1 Implementation Details All data are tokenized, truecased, and split into subwords using Byte Pair Encoding (BPE) with 32,000 operations (Sennrich et al., 2016). We filter the training set to only contain sentences up to 80 words. Results We report the results obtained with the transformer model, as they were consistently better than the LSTM one. All the result tables for the LSTM models can be found in the Appendix. The performance of our systems on the synthetic WMT test sets, as measured by detokenized BLEU (Papineni et al., 2002), is summarized in Table 4. When the system is trained only on clean data (first row) and tested on noisy data, it unsurprisingly exhibits degraded performance. We observe significant drops in the range of 1.0–3.6 BLEU."
N19-1311,P12-2039,0,0.0886316,"English that are annotated with corrections, which have been widely used for research in Grammar Error Correction (GEC). The NUS Corpus of Learner English (NUCLE) contains essays written by students at the National University of Singapore, corrected by two annotators using 27 error codes (Dahlmeier et al., 2013). It has become the main benchmark for GEC, as it was used in the CoNLL GEC Shared Tasks (Ng et al., 2013, 2014). Other corpora include the Cambridge Learner Corpus First Certificate in English FCE corpus (Yannakoudakis et al., 2011), which is only partially public, the Lang-8 corpus (Tajiri et al., 2012), which was harvested from online corrections, and the AESW 2016 Shared Task corpus, which contains corrections on texts from scientific journals. The last corpus is the JHU FLuency-Extended GUG corpus (JFLEG) (Napoles et al., 2017). This corpus covers a wider range of English proficiency levels on the source side, and its correction annotations include extended fluency edits rather than just minimal grammatical ones. That way, the corrected sentence is not just grammatical, but also guaranteed to be fluent. 2.2 Synthetic grammar errors Ideally, we would train a translation model to translate"
P00-1057,E91-1005,0,0.0110272,"nd l , call the set of nodes dominated by one node but not strictly dominated by the other the site-segment hh ; l i.  Removing a site-segment must not deprive a tree of its foot node. That is, no site-segment hh ; l i may contain a foot node unless l is itself the foot node.  If two tree sets adjoin into the same tree, the two site-segments must be simultaneously removable. That is, the two sitesegments must be disjoint, or one must contain the other. Because of the rst restriction, we depict tree sets with the components connected by a dominance link (dotted line), in the manner of (Becker et al., 1991). As written, the above rules only allow tree-local adjunction; we can generalize them to allow set-local adjunction by treating this dominance link like an ordinary arc. But this would increase the weak generative capacity of the system. For present purposes it is sucient just to allow one type of set-local adjunction: adjoin the upper tree to the upper foot, and the lower tree to the lower root (see Figure 5). This does not increase the weak generative capacity, as will be shown in Section 2.3. Observe that the set-local TAG given in Figure 5 obeys the above restrictions. 2.2 2LTAG For the"
P00-1057,W00-2008,1,0.8006,"ed later. Of course, in our case the stack"" has at most one element. The Pop rule does the reverse: every completed elementary tree set must contain a site-segment, and the Pop rule places it back where the site-segment came from, emptying the stack."" The Pop-push rule performs setlocal adjunction: a completed elementary tree set is placed between the two trees of yet another elementary tree set, and the stack"" is unchanged. Pop-push is computationally the most expensive rule; since it involves six indices and three di erent elementary trees, its running time is O(n6 G3 ). It was noted in (Chiang et al., 2000) that for synchronous RF-2LTAG, parse forests could not be transferred in time O(n6 ). This fact turns out to be connected to several properties of RF-TAG (Rogers, 1994).3 3 Thanks to Anoop Sarkar for pointing out the rst The CKY-style parser for regular form TAG described in (Rogers, 1994) essentially keeps track of adjunctions using stacks, and the regular form constraint ensures that the stack depth is bounded. The only kinds of adjunction that can occur to arbitrary depth are root and foot adjunction, which are treated similarly to substitution and do not a ect the stacks. The reader will"
P00-1057,P99-1011,1,0.850223,"node has Gorn address , then its ith child has Gorn address 2 Figure 6: Adjoining into by removing .  An auxiliary tree may adjoin anywhere.  When a tree is adjoined at a node ,  is rewritten as , and the foot of inherits the label of . The tree set of hG; G0 i, T (hG; G0 i), is f G [T (G0)], where f G is the yield function of G and T (G0 ) is the tree set of G0. Thus, the elementary trees of G0 are combined to form a derived tree, which is then interpreted as a derivation tree for G, which gives instructions for combining elementary trees of G into the nal derived tree. It was shown in Dras (1999) that when the meta-level grammar is in the regular form of Rogers (1994) the formalism is weakly equivalent to TAG. 2.3 Reducing restricted R-MCTAG to RF-2LTAG Consider the case of a multicomponent tree set f 1 ; 2 g adjoining into an initial tree (Figure 6). Recall that we de ned a sitesegment of a pair of adjunction sites to be all the nodes which are dominated by the upper site but not the lower site. Imagine that the site-segment is excised from , and that 1 and 2 are fused into a single elementary tree. Now we can simulate the multi-component adjunction by ordinary adjunction: adjoin the"
P00-1057,P95-1021,0,0.3527,"of tree-local. seem : sleep sleep : S does S likely S .. .. . . VP John VP seem VP seem VP likely VP to sleep S likely : Figure 4: SL-MCTAG generable derivation Unfortunately, unrestricted set-local multicomponent TAGs not only have more derivational generative capacity than TAGs, but they also have more weak generative capacity: SL-MCTAGs can generate the quadruple copy language wwww, for example, which does not correspond to any known linguistic phenomenon. Other formalisms aiming to model dependency correctly similarly expand weak generative capacity, notably D-tree Substitution Grammar (Rambow et al., 1995), and consequently end up with much greater parsing complexity. The work in this paper follows another Figure 5: Set-local adjunction. line of research which has focused on squeezing as much strong generative capacity as possible out of weakly TAG-equivalent formalisms. Tree-local multicomponent TAG (Weir, 1988), nondirectional composition (Joshi and Vijay-Shanker, 1999), and segmented adjunction (Kulick, 2000) are examples of this approach, wherein the constraint on weak generative capacity naturally limits the expressivity of these systems. We discuss the relation of the formalism of this pa"
P00-1057,P94-1022,0,0.333071,": Adjoining into by removing .  An auxiliary tree may adjoin anywhere.  When a tree is adjoined at a node ,  is rewritten as , and the foot of inherits the label of . The tree set of hG; G0 i, T (hG; G0 i), is f G [T (G0)], where f G is the yield function of G and T (G0 ) is the tree set of G0. Thus, the elementary trees of G0 are combined to form a derived tree, which is then interpreted as a derivation tree for G, which gives instructions for combining elementary trees of G into the nal derived tree. It was shown in Dras (1999) that when the meta-level grammar is in the regular form of Rogers (1994) the formalism is weakly equivalent to TAG. 2.3 Reducing restricted R-MCTAG to RF-2LTAG Consider the case of a multicomponent tree set f 1 ; 2 g adjoining into an initial tree (Figure 6). Recall that we de ned a sitesegment of a pair of adjunction sites to be all the nodes which are dominated by the upper site but not the lower site. Imagine that the site-segment is excised from , and that 1 and 2 are fused into a single elementary tree. Now we can simulate the multi-component adjunction by ordinary adjunction: adjoin the fused 1 and 2 into what is left of ; then replace by adjoining it betwee"
P00-1057,1985.tmi-1.17,0,0.0253703,"erence between the two is in the kinds of languages that they are able to describe: DSG is both less and more restrictive than R-MCTAG. DSG can generate the language count-k for some arbitrary k (that is, fa1 n a2 n : : : ak n g), which makes it extremely powerful, whereas R-MCTAG can only generate count-4. However, DSG cannot generate the copy language (that is, fww j w 2 g with  some terminal alphabet), whereas R-MCTAG can; this may be problematic for a formalism modeling natural language, given the key role of the copy language in demonstrating that natural language is not context-free (Shieber, 1985). RMCTAG is thus a more constrained relaxation of the notion of immediate dominance in favor of non-immediate dominance than is the case for DSG. Another formalism of particular interest here is the Segmented Adjoining Grammar of (Kulick, 2000). This generalization of TAG is characterized by an extension of the adjoining operation, motivated by evidence in scrambling, clitic climbing and subject-to-subject raising. Most interestingly, this extension to TAG, proposed on empirical grounds, is de ned by a composition operation with constrained non-immediate dominance links that looks quite simila"
P00-1057,W00-2015,0,\N,Missing
P00-1058,1997.iwpt-1.6,0,0.0416624,"time parsable. Rather than coin a new acronym for this particular variant, we will simply refer to it as TAG"" and trust that no confusion will arise. The parameters of a probabilistic TAG (Resnik, 1992; Schabes, 1992) are: Pi ( ) = 1 X X P ( a X P ( j s j ) = 1 ) + Pa (NONE j ) = 1 where ranges over initial trees, over auxiliary trees, over modi er trees, and  over nodes. Pi ( ) is the probability of beginning a derivation with ; Ps ( ) is the probability of substituting at ; Pa ( ) is the probability of adjoining at ; nally, Pa (NONE ) is the probability of nothing adjoining at . (Carroll and Weir, 1997) suggest other parameterizations worth exploring as well. Our variant adds another set of parameters: j j j XP sa ( j ; i; f ) + Psa (STOP j ; i; f ) = 1 This is the probability of sister-adjoining between the ith and i + 1th children of  (as before, allowing for two imaginary children beyond the leftmost and rightmost children). Since multiple modi er trees can adjoin at the same location, Psa ( ) is also conditioned on a ag f which indicates whether is the rst modi er tree (i.e., the one closest to the head) to adjoin at that location. The probability of a derivation can then be expressed"
P00-1058,A00-2018,0,0.0430317,"he desired dependencies (b) easily, using the grammar of Figure 1. A more complex lexicalization scheme for CFG could as well (one which kept track of two heads at a time, for example), but the TAG account is simpler and cleaner. Bilexical dependencies are not the only nonlocal dependencies that can be used to improve parsing accuracy. For example, the attachment of an S depends on the presence or absence of the embedded subject (Collins, 1999); Treebank-style two-level NPs are mismodeled by PCFG (Collins, 1999; Johnson, 1998); the generation of a node depends on the label of its grandparent (Charniak, 2000; Johnson, 1998). In order to capture such dependencies in a PCFG-based model, they must be localized either by transforming the data or modifying the parser. Such changes are not always obvious a priori and often must be devised anew for each language or each corpus. But none of these cases really requires special treatment in a PTAG model, because each composition probability involves not only a bilexical dependency but a iarboreal"" (tree-tree) dependency. That is, PTAG generates an entire elementary tree at once, conditioned on the entire elementary tree being modi ed. Thus dependencies t"
P00-1058,P98-1035,0,0.0811538,"rsing performance. We nd that this induction method is an improvement over the EM-based method of (Hwa, 1998), and that the induced model yields results comparable to lexicalized PCFG. 1 Introduction Why use tree-adjoining grammar for statistical parsing? Given that statistical natural language processing is concerned with the probable rather than the possible, it is not because TAG can describe constructions like arbitrarily large Dutch verb clusters. Rather, what makes TAG useful for statistical parsing are the structural descriptions it assigns to breadand-butter sentences. The approach of Chelba and Jelinek (1998) to language modeling is illustrative: even though the probability estimate of w appearing as the kth word can be conditioned on the entire history w1 ; : : : ; wk,1 , the quantity of available training data limits the usable context to about two words|but which two? A trigram model chooses wk,1 and wk,2 and works quite well; a model which chose wk,7 and wk,11 would probably work less well. But (Chelba and Jelinek, 1998) chooses the lexical heads of the two previous constituents as determined by a shift-reduce parser, and works better than a trigram model. Thus the (virtual) grammar serves to"
P00-1058,2000.iwpt-1.9,0,0.287728,"Missing"
P00-1058,P96-1025,0,0.264049,"Missing"
P00-1058,P97-1003,0,0.807424,"AG generates an entire elementary tree at once, conditioned on the entire elementary tree being modi ed. Thus dependencies that have to be stipulated in a PCFG by tree transformations or parser modi cations are captured for free in a PTAG model. Of course, the price that the PTAG model pays is sparser data; the backo model must therefore be chosen carefully. 4 Inducing a stochastic grammar from the Treebank 4.1 Reconstructing derivations We want to extract from the Penn Treebank an LTAG whose derivations mirror the dependency analysis implicit in the head-percolation rules of (Magerman, 1995; Collins, 1997). For each node , these rules classify exactly one child of  as a head and the rest as either arguments or adjuncts. Using this classi cation we can construct a TAG derivation (including elementary trees) from a derived tree as follows: 1. If  is an adjunct, excise the subtree rooted at  to form a modi er tree. 2. If  is an argument, excise the subtree rooted at  to form an initial tree, leaving behind a substitution node. 3. If  has a right corner  which is an argument with the same label as  (and all intervening nodes are heads), excise the segment from  down to  to form an auxili"
P00-1058,W97-0302,0,0.0327281,"Missing"
P00-1058,P98-1091,0,0.394415,"igure 1: Grammar and derivation for John should leave tomorrow."" model bilexical dependencies was noted early on by (Resnik, 1992). It turns out that there are other pieces of contextual information that need to be explicitly accounted for in a CFG by grammar transformations but come for free in a TAG. We discuss a few such cases in Section 3. In Sections 4 and 5 we describe an experiment to test the parsing accuracy of a probabilistic TAG extracted automatically from the Penn Treebank. We nd that the automatically-extracted grammar gives an improvement over the EM-based induction method of (Hwa, 1998), and that the parser performs comparably to lexicalized PCFG parsers, though certainly with room for improvement. We emphasize that TAG is attractive not because it can do things that CFG cannot, but because it does everything that CFG can, only more cleanly. (This is where the analogy with (Chelba and Jelinek, 1998) breaks down.) Thus certain possibilities which were not apparent in a PCFG framework or prohibitively complicated might become simple to implement in a PTAG framework; we conclude by o ering two such possibilities. 2 The formalism The formalism we use is a variant of lexicalized"
P00-1058,J98-4004,0,0.026514,"dently of each other, so that, for example, P (John leave) > 0.) TAG can produce the desired dependencies (b) easily, using the grammar of Figure 1. A more complex lexicalization scheme for CFG could as well (one which kept track of two heads at a time, for example), but the TAG account is simpler and cleaner. Bilexical dependencies are not the only nonlocal dependencies that can be used to improve parsing accuracy. For example, the attachment of an S depends on the presence or absence of the embedded subject (Collins, 1999); Treebank-style two-level NPs are mismodeled by PCFG (Collins, 1999; Johnson, 1998); the generation of a node depends on the label of its grandparent (Charniak, 2000; Johnson, 1998). In order to capture such dependencies in a PCFG-based model, they must be localized either by transforming the data or modifying the parser. Such changes are not always obvious a priori and often must be devised anew for each language or each corpus. But none of these cases really requires special treatment in a PTAG model, because each composition probability involves not only a bilexical dependency but a iarboreal"" (tree-tree) dependency. That is, PTAG generates an entire elementary tree at"
P00-1058,P95-1037,0,0.92514,"ram model chooses wk,1 and wk,2 and works quite well; a model which chose wk,7 and wk,11 would probably work less well. But (Chelba and Jelinek, 1998) chooses the lexical heads of the two previous constituents as determined by a shift-reduce parser, and works better than a trigram model. Thus the (virtual) grammar serves to structure the history so that the two most useful words can be chosen, even though the structure of the problem itself is entirely linear. Similarly, nothing about the parsing problem requires that we construct any structure other than phrase structure. But beginning with (Magerman, 1995) statistical parsers have used bilexical dependencies with great success. Since these dependencies are not encoded in plain phrase-structure trees, the standard approach has been to let the lexical heads percolate up the tree, so that when one lexical head is immediately dominated by another, it is understood to be dependent on it. E ectively, a dependency structure is made parasitic on the phrase structure so that they can be generated together by a context-free model. However, this solution is not ideal. Aside from cases where context-free derivations are incapable of encoding both constitue"
P00-1058,W98-0131,0,0.0275207,"Psa ( 2 (2); 1; true ) Psa (STOP 2 (2); 1; false ) Psa (STOP 2 (); 0; true ) : : : where (i) is the node of with address i. We want to obtain a maximum-likelihood estimate of these parameters, but cannot estimate them directly from the Treebank, because the sample space of PTAG is the space of TAG derivations, not the derived trees that are found in the Treebank. One approach, taken in (Hwa, 1998), is to choose some grammar general enough to parse the whole corpus and obtain a maximum-likelihood estimate by EM. Another approach, taken in (Magerman, 1995) and others for lexicalized PCFGs and (Neumann, 1998; Xia, 1999; Chen and VijayShanker, 2000) for LTAGs, is to use heuristics to reconstruct the derivations, and directly estimate the PTAG parameters from the reconstructed derivations. We take this approach as well. (One could imagine combining the two approaches, using heuristics to extract a grammar but EM to estimate its parameters.)  j j  j  j   j j   3 Some properties of probabilistic TAG In a lexicalized TAG, because each composition brings together two lexical items, every composition probability involves a bilexical dependency. Given a CFG and headpercolation scheme, an equivalen"
P00-1058,P95-1021,0,0.0146324,"m The formalism we use is a variant of lexicalized tree-insertion grammar (LTIG), which is in turn a restriction of LTAG (Schabes and Waters, 1995). In this variant there are three kinds of elementary tree: initial, (predicative) auxiliary, and modi er, and three composition operations: substitution, adjunction, and sister-adjunction. Auxiliary trees and adjunction are restricted as in TIG: essentially, no wrapping adjunction or anything equivalent to wrapping adjunction is allowed. Sister-adjunction is not an operation found in standard de nitions of TAG, but is borrowed from D-Tree Grammar (Rambow et al., 1995). In sisteradjunction the root of a modi er tree is added as a new daughter to any other node. (Note that as it stands sister-adjunction is completely unconstrained; it will be constrained by the probability model.) We introduce this operation simply so we can derive the at structures found in the Penn Treebank. Following (Schabes and Shieber, 1994), multiple modi er trees can be sister-adjoined at a single site, but only one auxiliary tree may be adjoined at a single node. Figure 1 shows an example grammar and the derivation of the sentence John should leave tomorrow."" The derivation tree en"
P00-1058,W96-0213,0,0.112282,"Missing"
P00-1058,C92-2065,0,0.847931,"ism, because it assigns to each sentence not only a parse tree, which is built out of elementary trees and is interpreted as encoding constituency, but a derivation tree, which records how the various elementary trees were combined together and is commonly intepreted as encoding dependency. The ability of probabilistic LTAG to 2 S NP 1 NNP leave ( 2 ) VP MD VP John should ( 1 ) ( )  S VB NP 2,1 1 VP # 2 NP NP ) NNP NN VP MD John should tomorrow ( ) VP VB NP leave NN tomorrow Figure 1: Grammar and derivation for John should leave tomorrow."" model bilexical dependencies was noted early on by (Resnik, 1992). It turns out that there are other pieces of contextual information that need to be explicitly accounted for in a CFG by grammar transformations but come for free in a TAG. We discuss a few such cases in Section 3. In Sections 4 and 5 we describe an experiment to test the parsing accuracy of a probabilistic TAG extracted automatically from the Penn Treebank. We nd that the automatically-extracted grammar gives an improvement over the EM-based induction method of (Hwa, 1998), and that the parser performs comparably to lexicalized PCFG parsers, though certainly with room for improvement. We emp"
P00-1058,J94-1004,0,0.14923,"y trees and adjunction are restricted as in TIG: essentially, no wrapping adjunction or anything equivalent to wrapping adjunction is allowed. Sister-adjunction is not an operation found in standard de nitions of TAG, but is borrowed from D-Tree Grammar (Rambow et al., 1995). In sisteradjunction the root of a modi er tree is added as a new daughter to any other node. (Note that as it stands sister-adjunction is completely unconstrained; it will be constrained by the probability model.) We introduce this operation simply so we can derive the at structures found in the Penn Treebank. Following (Schabes and Shieber, 1994), multiple modi er trees can be sister-adjoined at a single site, but only one auxiliary tree may be adjoined at a single node. Figure 1 shows an example grammar and the derivation of the sentence John should leave tomorrow."" The derivation tree encodes this process, with each arc corresponding to a composition operation. Arcs corresponding to substitution and adjunction are labeled with the Gorn address1 of the substitution or adA Gorn address is a list of integers: the root of a tree has address , and the j th child of the node with 1 junction site. An arc corresponding to the sister-adjun"
P00-1058,J95-4002,0,0.57165,"though certainly with room for improvement. We emphasize that TAG is attractive not because it can do things that CFG cannot, but because it does everything that CFG can, only more cleanly. (This is where the analogy with (Chelba and Jelinek, 1998) breaks down.) Thus certain possibilities which were not apparent in a PCFG framework or prohibitively complicated might become simple to implement in a PTAG framework; we conclude by o ering two such possibilities. 2 The formalism The formalism we use is a variant of lexicalized tree-insertion grammar (LTIG), which is in turn a restriction of LTAG (Schabes and Waters, 1995). In this variant there are three kinds of elementary tree: initial, (predicative) auxiliary, and modi er, and three composition operations: substitution, adjunction, and sister-adjunction. Auxiliary trees and adjunction are restricted as in TIG: essentially, no wrapping adjunction or anything equivalent to wrapping adjunction is allowed. Sister-adjunction is not an operation found in standard de nitions of TAG, but is borrowed from D-Tree Grammar (Rambow et al., 1995). In sisteradjunction the root of a modi er tree is added as a new daughter to any other node. (Note that as it stands sister-a"
P00-1058,C92-2066,0,0.517355,"Missing"
P00-1058,P00-1008,0,\N,Missing
P00-1058,W00-1201,1,\N,Missing
P00-1058,J03-4003,0,\N,Missing
P00-1058,P01-1010,0,\N,Missing
P00-1058,C98-1035,0,\N,Missing
P00-1058,C98-1088,0,\N,Missing
P00-1058,N01-1023,0,\N,Missing
P00-1058,H01-1026,1,\N,Missing
P00-1058,A97-1029,0,\N,Missing
P01-1018,C88-1001,0,0.637881,"ystem. Proposition 3 For any CFRS F 0 , a grammar G from a (possibly different) CFRS is simulable by a grammar in F 0 if and only if it is trivially simulable by a grammar in F 0 ◦ RF-MMTAG. The “only if” direction (⇒) follows from the fact that the MMTAG constructed in the proof of Proposition 2 generates the same derived trees as the CFG. The “if” direction (⇐) is a little trickier because the constructed CFG inserts and relabels nodes. 4 Multicomponent multifoot TAG 4.1 Definitions MMTAG resembles a cross between set-local multicomponent TAG (Joshi, 1987) and ranked node rewriting grammar (Abe, 1988), a variant of TAG in which auxiliary trees may have multiple foot nodes. It also has much in common with dtree substitution grammar (Rambow et al., 1995). Definition 8 An elementary tree set α ~ is a finite set of trees (called the components of α ~ ) with the following properties: 1. Zero or more frontier nodes are designated foot nodes, which lack labels (following Abe), but are marked with the diacritic ∗; 2. Zero or more (non-foot) nodes are designated adjunction nodes, which are partitioned into one or more disjoint sets called adjunction sites. We notate this by assigning an index i to"
P01-1018,P99-1011,0,0.0129977,"tion 4.2 we prove the following result: Proposition 2 A grammar G from a CFRS is simulable by a CFG if and only if it is trivially simulable by an MMTAG in regular form. The “if” direction (⇐) implies (because simulability is reflexive) that RF-MMTAG is simulable by a CFG, and therefore cubic-time parsable. (The proof below does give an effective procedure for constructing a simulating CFG for any RF-MMTAG.) The “only if” direction (⇒) shows that, in the sense we have defined, RF-MMTAG is the most powerful such formalism. We can generalize this result using the notion of a meta-level grammar (Dras, 1999). Definition 7 If F1 and F2 are two CFRSs, F2 ◦ F1 is the CFRS characterized by the interpretation function ~·F2 ◦F1 = ~·F2 ◦ ~·F1 . F1 is the meta-level formalism, which generates derivations for F2 . Obviously F1 must be a treerewriting system. Proposition 3 For any CFRS F 0 , a grammar G from a (possibly different) CFRS is simulable by a grammar in F 0 if and only if it is trivially simulable by a grammar in F 0 ◦ RF-MMTAG. The “only if” direction (⇒) follows from the fact that the MMTAG constructed in the proof of Proposition 2 generates the same derived trees as the CFG. The “if” direc"
P01-1018,W00-2015,0,0.0201788,"hiang@cis.upenn.edu Abstract We consider the question “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” and propose some theoretical and practical constraints on this problem. We then introduce a formalism which, under these constraints, maximally squeezes strong generative power out of context-free grammar. Finally, we generalize this result to formalisms beyond CFG. 1 Introduction “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” This question, posed by Joshi (2000), is important for both linguistic description and natural language processing. The extension of tree adjoining grammar (TAG) to tree-local multicomponent TAG (Joshi, 1987), or the extension of context free grammar (CFG) to tree insertion grammar (Schabes and Waters, 1993) or regular form TAG (Rogers, 1994) can be seen as steps toward answering this question. But this question is difficult to answer with much finality unless we pin its terms down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sen"
P01-1018,P95-1021,0,0.0348673,"ally simulable by a grammar in F 0 ◦ RF-MMTAG. The “only if” direction (⇒) follows from the fact that the MMTAG constructed in the proof of Proposition 2 generates the same derived trees as the CFG. The “if” direction (⇐) is a little trickier because the constructed CFG inserts and relabels nodes. 4 Multicomponent multifoot TAG 4.1 Definitions MMTAG resembles a cross between set-local multicomponent TAG (Joshi, 1987) and ranked node rewriting grammar (Abe, 1988), a variant of TAG in which auxiliary trees may have multiple foot nodes. It also has much in common with dtree substitution grammar (Rambow et al., 1995). Definition 8 An elementary tree set α ~ is a finite set of trees (called the components of α ~ ) with the following properties: 1. Zero or more frontier nodes are designated foot nodes, which lack labels (following Abe), but are marked with the diacritic ∗; 2. Zero or more (non-foot) nodes are designated adjunction nodes, which are partitioned into one or more disjoint sets called adjunction sites. We notate this by assigning an index i to each adjunction site and marking each node of site i with the diacritic i . 3. Each component is associated with a symbol called its type. This is analogo"
P01-1018,P94-1022,0,0.0217141,"maximally squeezes strong generative power out of context-free grammar. Finally, we generalize this result to formalisms beyond CFG. 1 Introduction “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” This question, posed by Joshi (2000), is important for both linguistic description and natural language processing. The extension of tree adjoining grammar (TAG) to tree-local multicomponent TAG (Joshi, 1987), or the extension of context free grammar (CFG) to tree insertion grammar (Schabes and Waters, 1993) or regular form TAG (Rogers, 1994) can be seen as steps toward answering this question. But this question is difficult to answer with much finality unless we pin its terms down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions Σ(G); the strong generative capacity of a formalism F is then {Σ(G) | F provides G}. There is some vagueness in the literature, however, over what structural descriptions are and how they can reasonably be compared across theories (Miller ("
P01-1018,P87-1015,0,0.0990017,"down more precisely. First, what is meant by strong generative power? In the standard definition (Chomsky, 1965) a grammar G weakly generates a set of sentences L(G) and strongly generates a set of structural descriptions Σ(G); the strong generative capacity of a formalism F is then {Σ(G) | F provides G}. There is some vagueness in the literature, however, over what structural descriptions are and how they can reasonably be compared across theories (Miller (1999) gives a good synopsis). (a) X  X X X∗ a X∗ b XNA (b) a X b b X∗ a Figure 1: Example of weakly context-free TAG. The approach that Vijay-Shanker et al. (1987) and Weir (1988) take, elaborated on by Becker et al. (1992), is to identify a very general class of formalisms, which they call linear contextfree rewriting systems (CFRSs), and define for this class a large space of structural descriptions which serves as a common ground in which the strong generative capacities of these formalisms can be compared. Similarly, if we want to talk about squeezing strong generative power out of a formal system, we need to do so in the context of some larger space of structural descriptions. Second, why is preservation of weak generative power important? If we in"
P01-1018,P93-1017,0,\N,Missing
P05-1033,W02-1018,0,0.0763459,"are thus a simple and powerful mechanism for machine translation. The basic phrase-based model is an instance of the noisy-channel approach (Brown et al., 1993),1 in which the translation of a French sentence f into an 1 Throughout this paper, we follow the convention of Brown et al. of designating the source and target languages as “French” and “English,” respectively. The variables f and e stand for source and target sentences; fi j stands for the substring of f from position i to position j inclusive, and similarly for eij . Other phrase-based models model the joint distribution P(e, f ) (Marcu and Wong, 2002) or made P(e) and P( f |e) into features of a log-linear model (Och and Ney, 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reor"
P05-1033,P00-1056,0,0.333427,"0 ∈ [i0 , j0 ]. Next, we form all possible differences of phrase pairs: Definition 2. The set of rules of h f, e, ∼i is the smallest set satisfying the following: j j0 1. If h fi , ei0 i is an initial phrase pair, then Training The training process begins with a word-aligned corpus: a set of triples h f, e, ∼i, where f is a French sentence, e is an English sentence, and ∼ is a (manyto-many) binary relation between positions of f and positions of e. We obtain the word alignments using the method of Koehn et al. (2003), which is based on that of Och and Ney (2004). This involves running GIZA++ (Och and Ney, 2000) on the corpus in both directions, and applying refinement rules (the variant they designate “final-and”) to obtain a single many-to-many word alignment for each sentence. Then, following Och and others, we use heuristics to hypothesize a distribution of possible derivations of each training example, and then estimate 266 j j0 X → h fi , ei0 i is a rule. j j0 2. If r = X → hγ, αi is a rule and h fi , ei0 i is an j initial phrase pair such that γ = γ1 fi γ2 and α = 0 j α1 ei0 α2 , then X → hγ1 X k γ2 , α1 X k α2 i is a rule, where k is an index not used in r. The above scheme generates a very l"
P05-1033,P02-1038,0,0.940545,"ased model is an instance of the noisy-channel approach (Brown et al., 1993),1 in which the translation of a French sentence f into an 1 Throughout this paper, we follow the convention of Brown et al. of designating the source and target languages as “French” and “English,” respectively. The variables f and e stand for source and target sentences; fi j stands for the substring of f from position i to position j inclusive, and similarly for eij . Other phrase-based models model the joint distribution P(e, f ) (Marcu and Wong, 2002) or made P(e) and P( f |e) into features of a log-linear model (Och and Ney, 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003"
P05-1033,J04-4002,0,0.805424,"log-linear model (Och and Ney, 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003), or not at all (Zens and Ney, 2004; Kumar et al., 2005). But it is often desirable to capture translations whose scope is larger than a few consecutive words. 263 Proceedings of the 43rd Annual Meeting of the ACL, pages 263–270, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics Consider the following Mandarin example and its English translation: (3) 2 / é ¦¤ Aozhou shi yu Bei Han you bangjiao Australia is with North Korea have dipl. rels. „ p ý¶ K de shaoshu guojia zhiyi that few countries one of ‘Australia is one of the few countries that have diplo"
P05-1033,W00-1201,1,0.0527945,"e pair which is not a syntactic phrase pair, and report that favoring syntactic phrases does not improve accuracy. But in our model, the rule (19) X → hes gibt X 1 , there is X 1 i would indeed respect syntactic phrases, because it builds a pair of Ss out of a pair of NPs. Thus, favoring subtrees in our model that are syntactic phrases might provide a fairer way of testing the hypothesis that syntactic phrases are better phrases. This feature adds a factor to (17),  j   1 if fi is a constituent (20) c(i, j) =   0 otherwise as determined by a statistical tree-substitutiongrammar parser (Bikel and Chiang, 2000), trained on the Penn Chinese Treebank, version 3 (250k words). Note that the parser was run only on the test data and not the (much larger) training data. Rerunning the minimum-error-rate trainer with the new feature yielded the feature weights shown in Table 2. Although the feature improved accuracy on the development set (from 0.314 to 0.322), it gave no statistically significant improvement on the test set. 269 Conclusion Hierarchical phrase pairs, which can be learned without any syntactically-annotated training data, improve translation accuracy significantly compared with a state-of-the"
P05-1033,P03-1021,0,0.225071,"4, and to use the shortest (as opposed to nearest) reference sentence for the brevity penalty. The results of the experiments are summarized in Table 1. 5.1 Baseline The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004a), as publicly distributed. We used the default feature set: language model (same as above), p( f¯ |e¯ ), p(¯e |f¯), lexical weighting (both directions), distortion model, word penalty, and phrase penalty. We ran the trainer with its default settings (maximum phrase length 7), and then used Koehn’s implementation of minimumerror-rate training (Och, 2003) to tune the feature weights to maximize the system’s BLEU score on our development set, yielding the values shown in Table 2. Finally, we ran the decoder on the test set, pruning the phrase table with b = 100, pruning the chart with b = 100, β = 10−5 , and limiting distortions to 4. These are the default settings, except for the phrase table’s b, which was raised from 20, and the distortion limit. Both of these changes, made by Koehn’s minimum-error-rate trainer by default, improve performance on the development set. 268 Rank 1 3 14 23 577 735 763 1201 1240 2091 3253 10508 28426 47015 1752457"
P05-1033,J93-2003,0,0.078767,"aining data. Introduction The alignment template translation model (Och and Ney, 2004) and related phrase-based models advanced the previous state of the art by moving from words to phrases as the basic unit of translation. Phrases, which can be any substring and not necessarily phrases in any syntactic theory, allow these models to learn local reorderings, translation of short idioms, or insertions and deletions that are sensitive to local context. They are thus a simple and powerful mechanism for machine translation. The basic phrase-based model is an instance of the noisy-channel approach (Brown et al., 1993),1 in which the translation of a French sentence f into an 1 Throughout this paper, we follow the convention of Brown et al. of designating the source and target languages as “French” and “English,” respectively. The variables f and e stand for source and target sentences; fi j stands for the substring of f from position i to position j inclusive, and similarly for eij . Other phrase-based models model the joint distribution P(e, f ) (Marcu and Wong, 2002) or made P(e) and P( f |e) into features of a log-linear model (Och and Ney, 2002). But the basic architecture of phrase segmentation (or ge"
P05-1033,P02-1040,0,0.139889,"aseline system, the state-of-the-art phrase-based system Pharaoh (Koehn et al., 2003; Koehn, 2004a), against our system. For all three systems we trained the translation model on the FBIS corpus (7.2M+9.2M words); for the language model, we used the SRI Language Modeling Toolkit to train a trigram model with modified Kneser-Ney smoothing (Chen and Goodman, 1998) on 155M words of English newswire text, mostly from the Xinhua portion of the Gigaword corpus. We used the 2002 NIST MT evaluation test set as our development set, and the 2003 test set as our test set. Our evaluation metric was BLEU (Papineni et al., 2002), as calculated by the NIST script (version 11a) with its default settings, which is to perform case-insensitive matching of n-grams up to n = 4, and to use the shortest (as opposed to nearest) reference sentence for the brevity penalty. The results of the experiments are summarized in Table 1. 5.1 Baseline The baseline system we used for comparison was Pharaoh (Koehn et al., 2003; Koehn, 2004a), as publicly distributed. We used the default feature set: language model (same as above), p( f¯ |e¯ ), p(¯e |f¯), lexical weighting (both directions), distortion model, word penalty, and phrase penalt"
P05-1033,N03-1017,0,0.341155,"iables f and e stand for source and target sentences; fi j stands for the substring of f from position i to position j inclusive, and similarly for eij . Other phrase-based models model the joint distribution P(e, f ) (Marcu and Wong, 2002) or made P(e) and P( f |e) into features of a log-linear model (Och and Ney, 2002). But the basic architecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003), or not at all (Zens and Ney, 2004; Kumar et al., 2005). But it is often desirable to capture translations whose scope is larger than a few consecutive words. 263 Proceedings of the 43rd Annual Meeting of the ACL, pages 263–270, c Ann Arbor, June 2005. 2005 Association for Computational Linguis"
P05-1033,J97-3002,0,0.418463,"nformed by syntactic theory (via the Penn Treebank). Our system is formally syntaxbased in that it uses synchronous CFG, but not necessarily linguistically syntax-based, because it induces a grammar from a parallel text without relying on any linguistic annotations or assumptions; the result sometimes resembles a syntactician’s grammar but often does not. In this respect it resembles Wu’s (5) hyu 1 you 2 , have 2 with 1 i where 1 and 2 are placeholders for subphrases. This would capture the fact that Chinese PPs almost always modify VP on the left, whereas English PPs 264 bilingual bracketer (Wu, 1997), but ours uses a different extraction method that allows more than one lexical item in a rule, in keeping with the phrasebased philosophy. Our extraction method is basically the same as that of Block (2000), except we allow more than one nonterminal symbol in a rule, and use a more sophisticated probability model. In this paper we describe the design and implementation of our hierarchical phrase-based model, and report on experiments that demonstrate that hierarchical phrases indeed improve translation. These give the model the option to build only partial translations using hierarchical phra"
P05-1033,P01-1067,0,0.524773,"is: The system we describe below uses rules like this, and in fact is able to learn them automatically from a bitext without syntactic annotation. It translates the above example almost exactly as we have shown, the only error being that it omits the word ‘that’ from (6) and therefore (8). These hierarchical phrase pairs are formally productions of a synchronous context-free grammar (defined below). A move to synchronous CFG can be seen as a move towards syntax-based MT; however, we make a distinction here between formally syntax-based and linguistically syntax-based MT. A system like that of Yamada and Knight (2001) is both formally and linguistically syntax-based: formally because it uses synchronous CFG, linguistically because the structures it is defined over are (on the English side) informed by syntactic theory (via the Penn Treebank). Our system is formally syntaxbased in that it uses synchronous CFG, but not necessarily linguistically syntax-based, because it induces a grammar from a parallel text without relying on any linguistic annotations or assumptions; the result sometimes resembles a syntactician’s grammar but often does not. In this respect it resembles Wu’s (5) hyu 1 you 2 , have 2 with 1"
P05-1033,koen-2004-pharaoh,0,0.605468,"Ann Arbor, June 2005. 2005 Association for Computational Linguistics Consider the following Mandarin example and its English translation: (3) 2 / é ¦¤ Aozhou shi yu Bei Han you bangjiao Australia is with North Korea have dipl. rels. „ p ý¶ K de shaoshu guojia zhiyi that few countries one of ‘Australia is one of the few countries that have diplomatic relations with North Korea’ If we count zhiyi, lit. ‘of-one,’ as a single token, then translating this sentence correctly into English requires reversing a sequence of five elements. When we run a phrase-based system, Pharaoh (Koehn et al., 2003; Koehn, 2004a), on this sentence (using the experimental setup described below), we get the following phrases with translations: (4) [Aozhou] [shi] [yu] [Bei Han] [bangjiao]1 [de shaoshu guojia zhiyi] [you] usually modify VP on the right. Because it generalizes over possible prepositional objects and direct objects, it acts both as a discontinuous phrase pair and as a phrase-reordering rule. Thus it is considerably more powerful than a conventional phrase pair. Similarly, (6) h 1 de 2 , the 2 that 1 i would capture the fact that Chinese relative clauses modify NPs on the left, whereas English relative cla"
P05-1033,N04-1033,0,0.0370346,"chitecture of phrase segmentation (or generation), phrase reordering, and phrase translation remains the same. Phrase-based models can robustly perform translations that are localized to substrings that are common enough to have been observed in training. But Koehn et al. (2003) find that phrases longer than three words improve performance little, suggesting that data sparseness takes over for longer phrases. Above the phrase level, these models typically have a simple distortion model that reorders phrases independently of their content (Och and Ney, 2004; Koehn et al., 2003), or not at all (Zens and Ney, 2004; Kumar et al., 2005). But it is often desirable to capture translations whose scope is larger than a few consecutive words. 263 Proceedings of the 43rd Annual Meeting of the ACL, pages 263–270, c Ann Arbor, June 2005. 2005 Association for Computational Linguistics Consider the following Mandarin example and its English translation: (3) 2 / é ¦¤ Aozhou shi yu Bei Han you bangjiao Australia is with North Korea have dipl. rels. „ p ý¶ K de shaoshu guojia zhiyi that few countries one of ‘Australia is one of the few countries that have diplomatic relations with North Korea’ If we count zhiyi, lit"
P05-1033,W04-3250,0,0.641762,"Ann Arbor, June 2005. 2005 Association for Computational Linguistics Consider the following Mandarin example and its English translation: (3) 2 / é ¦¤ Aozhou shi yu Bei Han you bangjiao Australia is with North Korea have dipl. rels. „ p ý¶ K de shaoshu guojia zhiyi that few countries one of ‘Australia is one of the few countries that have diplomatic relations with North Korea’ If we count zhiyi, lit. ‘of-one,’ as a single token, then translating this sentence correctly into English requires reversing a sequence of five elements. When we run a phrase-based system, Pharaoh (Koehn et al., 2003; Koehn, 2004a), on this sentence (using the experimental setup described below), we get the following phrases with translations: (4) [Aozhou] [shi] [yu] [Bei Han] [bangjiao]1 [de shaoshu guojia zhiyi] [you] usually modify VP on the right. Because it generalizes over possible prepositional objects and direct objects, it acts both as a discontinuous phrase pair and as a phrase-reordering rule. Thus it is considerably more powerful than a conventional phrase pair. Similarly, (6) h 1 de 2 , the 2 that 1 i would capture the fact that Chinese relative clauses modify NPs on the left, whereas English relative cla"
P05-1033,zhang-etal-2004-interpreting,0,0.538898,"Missing"
P05-1033,C04-1030,0,\N,Missing
P07-1005,P05-1033,1,0.670826,"not be used by state-of-the-art statistical MT systems. 33 David Chiang Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using Engl"
P07-1005,J07-2003,1,0.188328,"o uses a general log-linear model (Och and Ney, 2002) where the weight of a derivation D for a particular source sentence and its translation is Y w(D) = φi (D)λi (3) i where φi is a feature function and λi is the weight for feature φi . To ensure efficient decoding, the φi are subject to certain locality restrictions. Essentially, they should be defined as products of functions defined on isolated synchronous CGF rules; however, it is possible to extend the domain of locality of the features somewhat. A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed. In principle we could add features that depend on arbitrary source-side context. 3.1 New Features in Hiero for WSD To incorporate WSD into Hiero, we use the translations proposed by the WSD system to help Hiero obtain a better or more probable derivation during the translation of each source sentence. To achieve this, when a grammar rule R is considered during decoding, a"
P07-1005,P05-1066,0,0.0538264,"iero+WSD Results We then added the WSD features of Section 3.1 into Hiero and reran the experiment. The weights obtained by MERT are shown in the row Hiero+WSD of Table 2. We note that a negative weight is learnt for P tywsd . This means that in general, the model prefers grammar rules having chunks that matches WSD translations. This matches our intuition. Using the weights obtained, we translated the test sentences and obtained a BLEU score of 30.30, as shown in the row Hiero+WSD of Table 1. The improvement of 0.57 is statistically significant at p < 0.05 using the sign-test as described by Collins et al. (2005), with 374 (+1), 318 (−1) and 227 (0). Using the bootstrap-sampling test described in (Koehn, 2004b), the improvement is statistically significant at p < 0.05. Though the improvement is modest, it is statistically significant and this positive result is important in view of the negative findings in (Carpuat and Wu, 2005) that WSD does not help MT. Furthermore, note that Hiero+WSD has higher n-gram precisions than Hiero. 7 Analysis Ideally, the WSD system should be suggesting highquality translations which are frequently part of the reference sentences. To determine this, we note the set of gra"
P07-1005,N03-1010,0,0.00474864,"izing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as the WSD model’s predicted translation. They did not incorporate their WSD model or its predictions into their translation model; rather, they used the WSD predictions either to constrain the options available to their decoder, or to postedit the output of their decoder. They reported the negative result that WSD decreased the performance of MT based on their experiments. In another work (Vickr"
P07-1005,N03-1017,0,0.11125,"ystem will be able to help an MT system to determine the correct translation for an ambiguous word. To determine the correct sense of a word, WSD systems typically use a wide array of features that are not limited to the local context of w, and some of these features may not be used by state-of-the-art statistical MT systems. 33 David Chiang Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT sys"
P07-1005,koen-2004-pharaoh,0,0.320543,"Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as th"
P07-1005,W04-3250,0,0.533954,"Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as th"
P07-1005,W02-1006,1,0.648396,"next section, we describe our WSD system. 34 Then, in Section 3, we describe the Hiero MT system and introduce the two new features used to integrate the WSD system into Hiero. In Section 4, we describe the training data used by the WSD system. In Section 5, we describe how the WSD translations provided are used by the decoder of the MT system. In Section 6 and 7, we present and analyze our experimental results, before concluding in Section 8. 2 Word Sense Disambiguation Prior research has shown that using Support Vector Machines (SVM) as the learning algorithm for WSD achieves good results (Lee and Ng, 2002). For our experiments, we use the SVM implementation of (Chang and Lin, 2001) as it is able to work on multiclass problems to output the classification probability for each class. Our implemented WSD classifier uses the knowledge sources of local collocations, parts-of-speech (POS), and surrounding words, following the successful approach of (Lee and Ng, 2002). For local collocations, we use 3 features, w−1 w+1 , w−1 , and w+1 , where w−1 (w+1 ) is the token immediately to the left (right) of the current ambiguous word occurrence w. For parts-of-speech, we use 3 features, P−1 , P0 , and P+1 ,"
P07-1005,W02-1018,0,0.0157549,"nse ambiguity, a WSD system will be able to help an MT system to determine the correct translation for an ambiguous word. To determine the correct sense of a word, WSD systems typically use a wide array of features that are not limited to the local context of w, and some of these features may not be used by state-of-the-art statistical MT systems. 33 David Chiang Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the perf"
P07-1005,P05-1048,0,0.48789,"and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as the WSD model’s predicted translation. They did not incorporate their WSD model or its predictions into their translation model; rather, they used the WSD predictions either to constrain the options availabl"
P07-1005,P00-1056,0,0.186151,".1611 Table 2: Weights for each feature obtained by MERT training. The first eight features are those used by Hiero in (Chiang, 2005). the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus, we trained a trigram language model using the SRI Language Modelling Toolkit (Stolcke, 2002). Following (Chiang, 2005), we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores (Papineni et al., 2002) based on case-insensitive ngram matching, where n is up to 4. First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. The word alignments of both directions are then combined into a single set of alignments using the “diag-and” method of Koehn et al. (2003). Based on these alignments, synchronous CFG rules are then extracted from the corpus. While Hiero is extracting grammar rules, we gathered WSD training data by following the procedure described in section 4. 6.1 Hiero Results Using the MT 2002 test set, we ran the minimumerror rate training (MERT) (Och, 2003) with the decoder to tune the weights for each feature. The weights obtained are shown in the row Hiero of Table 2. Using these w"
P07-1005,P02-1038,0,0.0683074,"there is a one-to-one correspondence between the non-terminals in γ and α indicated by co-indexation. Hence, γ and α always have the same number of non-terminal symbols. For instance, we could have the following grammar rule: X → hd d d X 1 , go to X 1 every month toi (2) where boxed indices represent the correspondences between non-terminal symbols. Hiero extracts the synchronous CFG rules automatically from a word-aligned parallel corpus. To translate a source sentence, the goal is to find its most probable derivation using the extracted grammar rules. Hiero uses a general log-linear model (Och and Ney, 2002) where the weight of a derivation D for a particular source sentence and its translation is Y w(D) = φi (D)λi (3) i where φi is a feature function and λi is the weight for feature φi . To ensure efficient decoding, the φi are subject to certain locality restrictions. Essentially, they should be defined as products of functions defined on isolated synchronous CGF rules; however, it is possible to extend the domain of locality of the features somewhat. A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult bu"
P07-1005,J04-4002,0,0.020172,"o help an MT system to determine the correct translation for an ambiguous word. To determine the correct sense of a word, WSD systems typically use a wide array of features that are not limited to the local context of w, and some of these features may not be used by state-of-the-art statistical MT systems. 33 David Chiang Information Sciences Institute University of Southern California 4676 Admiralty Way, Suite 1001 Marina del Rey, CA 90292, USA chiang@isi.edu To perform translation, state-of-the-art MT systems use a statistical phrase-based approach (Marcu and Wong, 2002; Koehn et al., 2003; Och and Ney, 2004) by treating phrases as the basic units of translation. In this approach, a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu"
P07-1005,P03-1021,0,0.0215816,"nsitive ngram matching, where n is up to 4. First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. The word alignments of both directions are then combined into a single set of alignments using the “diag-and” method of Koehn et al. (2003). Based on these alignments, synchronous CFG rules are then extracted from the corpus. While Hiero is extracting grammar rules, we gathered WSD training data by following the procedure described in section 4. 6.1 Hiero Results Using the MT 2002 test set, we ran the minimumerror rate training (MERT) (Och, 2003) with the decoder to tune the weights for each feature. The weights obtained are shown in the row Hiero of Table 2. Using these weights, we run Hiero’s decoder to perform the actual translation of the MT 2003 test sentences and obtained a BLEU score of 29.73, as shown in the row Hiero of Table 1. This is higher than the score of 28.77 reported in (Chiang, 2005), perhaps due to differences in word segmentation, etc. Note that comparing with the MT systems used in (Carpuat and Wu, 2005) and (Cabezas and Resnik, 2005), the Hiero system we are using represents a much stronger baseline MT system up"
P07-1005,P02-1040,0,0.104543,"666 0.1124 Pw (γ|α) 0.0393 0.0487 Features Pw (α|γ) P typhr 0.1357 0.0665 0.0380 0.0988 Glue −0.0582 −0.0305 P tyword −0.4806 −0.1747 Pwsd (t|s) 0.1051 P tywsd −0.1611 Table 2: Weights for each feature obtained by MERT training. The first eight features are those used by Hiero in (Chiang, 2005). the English portion of the FBIS corpus and the Xinhua portion of the Gigaword corpus, we trained a trigram language model using the SRI Language Modelling Toolkit (Stolcke, 2002). Following (Chiang, 2005), we used the version 11a NIST BLEU script with its default settings to calculate the BLEU scores (Papineni et al., 2002) based on case-insensitive ngram matching, where n is up to 4. First, we performed word alignment on the FBIS parallel corpus using GIZA++ (Och and Ney, 2000) in both directions. The word alignments of both directions are then combined into a single set of alignments using the “diag-and” method of Koehn et al. (2003). Based on these alignments, synchronous CFG rules are then extracted from the corpus. While Hiero is extracting grammar rules, we gathered WSD training data by following the procedure described in section 4. 6.1 Hiero Results Using the MT 2002 test set, we ran the minimumerror rat"
P07-1005,W04-0822,0,0.10486,"a phrase can be any sequence of consecutive words and is not necessarily linguistically meaningful. Capitalizing on the strength of the phrase-based approach, Chiang (2005) introduced a hierarchical phrase-based statistical MT system, Hiero, which achieves significantly better translation performance than Pharaoh (Koehn, 2004a), which is a state-of-the-art phrasebased statistical MT system. Recently, some researchers investigated whether performing WSD will help to improve the performance of an MT system. Carpuat and Wu (2005) integrated the translation predictions from a Chinese WSD system (Carpuat et al., 2004) into a ChineseEnglish word-based statistical MT system using the ISI ReWrite decoder (Germann, 2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as the WSD model’s predicted translation. They did not incorporate their WSD model or its predictions into their translation model; rather, they used the WSD predictions either to constrain the options available to their decoder, or to postedit the output of their decoder. They reported the negati"
P07-1005,H05-1097,0,0.763143,"2003). Though they acknowledged that directly using English translations as word senses would be ideal, they instead predicted the HowNet sense of a word and then used the English gloss of the HowNet sense as the WSD model’s predicted translation. They did not incorporate their WSD model or its predictions into their translation model; rather, they used the WSD predictions either to constrain the options available to their decoder, or to postedit the output of their decoder. They reported the negative result that WSD decreased the performance of MT based on their experiments. In another work (Vickrey et al., 2005), the WSD problem was recast as a word translation task. The Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics, pages 33–40, c Prague, Czech Republic, June 2007. 2007 Association for Computational Linguistics translation choices for a word w were defined as the set of words or phrases aligned to w, as gathered from a word-aligned parallel corpus. The authors showed that they were able to improve their model’s accuracy on two simplified translation tasks: word translation and blank-filling. Recently, Cabezas and Resnik (2005) experimented with incorporating"
P07-1005,P96-1021,0,0.0796472,"ules. Hiero uses a general log-linear model (Och and Ney, 2002) where the weight of a derivation D for a particular source sentence and its translation is Y w(D) = φi (D)λi (3) i where φi is a feature function and λi is the weight for feature φi . To ensure efficient decoding, the φi are subject to certain locality restrictions. Essentially, they should be defined as products of functions defined on isolated synchronous CGF rules; however, it is possible to extend the domain of locality of the features somewhat. A n-gram language model adds a dependence on (n−1) neighboring target-side words (Wu, 1996; Chiang, 2007), making decoding much more difficult but still polynomial; in this paper, we add features that depend on the neighboring source-side words, which does not affect decoding complexity at all because the source string is fixed. In principle we could add features that depend on arbitrary source-side context. 3.1 New Features in Hiero for WSD To incorporate WSD into Hiero, we use the translations proposed by the WSD system to help Hiero obtain a better or more probable derivation during the translation of each source sentence. To achieve this, when a grammar rule R is considered dur"
P07-1019,W05-1506,1,0.657272,"Missing"
P07-1019,P05-1033,1,0.799037,"good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively t"
P07-1019,J07-2003,1,0.669096,"uage boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it to a reasonable size. A much simpler alternative method to incorporate the LM is rescoring: we first decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number of search errors since the true best translation (taking LM into account) is often outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al.,"
P07-1019,P97-1003,0,0.0193947,"2006). For instance, the following rule translates an English passive construction into Chinese: VP VBD was VP-C x1 :VBN IN → b`ei x2 x1 PP x2 :NP-C by Our tree-to-string system performs slightly better than the state-of-the-art phrase-based system Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its 150 derivation trees remain context-free and the search space is still a hypergraph, where we can adapt the methods presented in Sections 3 and 4. The data set is same as in Section 5.1, except that we also parsed the English-side using a variant of the Collins (1997) parser, and then extracted 24.7M tree-to-string rules using the algorithm of (Galley et al., 2006). Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007). All the three +LM decoding methods to be compared below take these binarized forests as input. For cube growing, we use a non-duplicate k-best method (Huang et al., 2006) to get 100-best unique translations according to −LM to estimate the lower-bound heuristics.4 This preprocessing step takes on average 0.12 seconds per sentence, which is negligible in"
P07-1019,P06-1121,0,0.106096,"on quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it to a reaso"
P07-1019,2006.amta-papers.8,1,0.690898,"(Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al., 2006). • We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and Chiang, 2005) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root. ∗ The authors would like to thank Dan Gildea, Jonathan Graehl, Mark Johnson, Kevin Knight, Daniel Marcu, Bob Moore and Hao Zhang. L. H. was partially supported by NSF ITR grants IIS-0428020 while visiting USC/ISI and EIA0205456 at UPenn. D. C. was partially supported under the GALE/DARPA program, contract HR0011-06-C-0022. Cube pruning and c"
P07-1019,W07-0405,1,0.699746,"ystem Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its 150 derivation trees remain context-free and the search space is still a hypergraph, where we can adapt the methods presented in Sections 3 and 4. The data set is same as in Section 5.1, except that we also parsed the English-side using a variant of the Collins (1997) parser, and then extracted 24.7M tree-to-string rules using the algorithm of (Galley et al., 2006). Since our tree-to-string rules may have many variables, we first binarize each hyperedge in the forest on the target projection (Huang, 2007). All the three +LM decoding methods to be compared below take these binarized forests as input. For cube growing, we use a non-duplicate k-best method (Huang et al., 2006) to get 100-best unique translations according to −LM to estimate the lower-bound heuristics.4 This preprocessing step takes on average 0.12 seconds per sentence, which is negligible in comparison to the +LM decoding time. Figure 8(a) compares cube growing and cube pruning against full-integration under various beam settings in the same fashion of Figure 7(a). At the lowest level of search error, the relative speed-up from c"
P07-1019,J99-4005,0,0.310507,"the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it to a reasonable size. A much simpler alternative method to incorporate the LM is rescoring: we first decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number"
P07-1019,koen-2004-pharaoh,0,0.421362,"outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and full-integration: it rescores k subtranslations at each node of the forest, rather than only at the root node as in pure rescoring. By adapting the k-best parsing Algorithm 2 of Huang and Chiang (2005), it achieves significant speed-up over full-integration on Chiang’s Hiero system. We push the idea behind this method further and make the following contributions in this paper: • We generalize cube pruning and adapt it to two systems very different from Hiero: a phrasebased system similar to Pharaoh (Koehn, 2004) and a tree-to-string system (Huang et al., 2006). • We also devise a faster variant of cube pruning, called cube growing, which uses a lazy version of k-best parsing (Huang and Chiang, 2005) that tries to reduce k to the minimum needed at each node to obtain the desired number of hypotheses at the root. ∗ The authors would like to thank Dan Gildea, Jonathan Graehl, Mark Johnson, Kevin Knight, Daniel Marcu, Bob Moore and Hao Zhang. L. H. was partially supported by NSF ITR grants IIS-0428020 while visiting USC/ISI and EIA0205456 at UPenn. D. C. was partially supported under the GALE/DARPA progr"
P07-1019,P06-1077,0,0.056508,"or of 10. The speed-up at the lowest searcherror level is a factor of 32. Figure 7(b) makes a similar comparison but measures search quality by BLEU, which shows an even larger relative speed-up for a given BLEU score, because translations with very different model costs might have similar BLEU scores. It also shows that our full-integration implementation in Cubit faithfully reproduces Pharaoh’s performance. Fixing the stack size to 100 and varying the threshold yielded a similar result. 5.2 Tree-to-string Decoding In tree-to-string (also called syntax-directed) decoding (Huang et al., 2006; Liu et al., 2006), the source string is first parsed into a tree, which is then recursively converted into a target string according to transfer rules in a synchronous grammar (Galley et al., 2006). For instance, the following rule translates an English passive construction into Chinese: VP VBD was VP-C x1 :VBN IN → b`ei x2 x1 PP x2 :NP-C by Our tree-to-string system performs slightly better than the state-of-the-art phrase-based system Pharaoh on the above data set. Although different from the SCFG-based systems in Section 2, its 150 derivation trees remain context-free and the search space is still a hypergr"
P07-1019,J04-4002,0,0.686321,"uage model which is essential for achieving good translation quality. We develop faster approaches for this problem based on k-best parsing algorithms and demonstrate their effectiveness on both phrase-based and syntax-based MT systems. In both cases, our methods achieve significant speed improvements, often by more than a factor of ten, over the conventional beam-search method at the same levels of search error and translation accuracy. 1 Introduction Recent efforts in statistical machine translation (MT) have seen promising improvements in output quality, especially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must"
P07-1019,W05-0636,0,0.0127953,"Missing"
P07-1019,P96-1021,0,0.274875,"ecially the phrase-based models (Och and Ney, 2004) and syntax-based models (Chiang, 2005; Galley et al., 2006). However, efficient decoding under these paradigms, especially with integrated language models (LMs), remains a difficult problem. Part of the complexity arises from the expressive power of the translation model: for example, a phrase- or word-based model with full reordering has exponential complexity (Knight, 1999). The language model also, if fully integrated into the decoder, introduces an expensive overhead for maintaining target-language boundary words for dynamic programming (Wu, 1996; Och and Ney, 2004). In practice, one must prune the search space aggressively to reduce it to a reasonable size. A much simpler alternative method to incorporate the LM is rescoring: we first decode without the LM (henceforth −LM decoding) to produce a k-best list of candidate translations, and then rerank the k-best list using the LM. This method runs much faster in practice but often produces a considerable number of search errors since the true best translation (taking LM into account) is often outside of the k-best list. Cube pruning (Chiang, 2007) is a compromise between rescoring and f"
P09-1064,N04-1022,0,0.861309,"n statistical machine translation, output translations are evaluated by their similarity to human reference translations, where similarity is most often measured by BLEU (Papineni et al., 2002). A decoding objective specifies how to derive final translations from a system’s underlying statistical model. The Bayes optimal decoding objective is to minimize risk based on the similarity measure used for evaluation. The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system’s translations relative to the model’s distribution over possible translations (Kumar and Byrne, 2004). Unfortunately, with a non-linear similarity measure like BLEU, we must resort to approximating the expected loss using a k-best list, which accounts for only a tiny fraction of a model’s full posterior distribution. In this paper, we introduce a variant of the MBR decoding procedure that applies efficiently to translation forests. Instead of maximizing expected similarity, we express similarity in terms of features of sentences, and choose translations that are similar to expected feature values. 567 Proceedings of the 47th Annual Meeting of the ACL and the 4th IJCNLP of the AFNLP, pages 567"
P09-1064,W07-0734,0,0.0159683,"s original intent. One could imagine different feature-based expressions that also produce BLEU scores for real sentences, but produce different values for fractional features. Some care must be taken to define S(e; φ(e0 )) to extend naturally from integer-valued to real-valued features. Second, while any similarity measure can in principle be expressed as S(e; φ(e0 )) for a sufficiently rich feature space, fast consensus decoding will not apply effectively to all functions. For instance, we cannot naturally use functions that include alignments or matchings between e and e0 , such as METEOR (Agarwal and Lavie, 2007) and TER (Snover et al., 2006). Though these functions can in principle be expressed in terms of features of e0 (for instance with indicator features for whole sentences), fast consensus decoding will only be effective if different sentences share many features, so that the feature expectations effectively capture trends in the underlying distribution. 3 the ... telescope 0.6 “saw the” h = 0.4 · 1 + (0.6 · 1.0) · 1 Figure 2: This translation forest for a Spanish sentence encodes two English parse trees. Hyper-edges (boxes) are annotated with normalized transition probabilities, as well as the"
P09-1064,P09-1067,0,0.302909,"ttice or forest, whereas fast consensus decoding restricts this search to a k-best list. However, Tromble et al. (2008) showed that most of the improvement from lattice-based consensus decoding comes from lattice-based expectations, not search: searching over lattices instead of k-best lists did not change results for two language pairs, and improved a third language pair by 0.3 BLEU. Thus, we do not consider our use of k-best lists to be a substantial liability of our approach. Fast consensus decoding is also similar in character to the concurrently developed variational decoding approach of Li et al. (2009). Using BLEU, both approaches choose outputs that match expected n-gram counts from forests, though differ in the details. It is possible to define a similarity measure under which the two approaches are equivalent.5 Experimental Results We evaluate these consensus decoding techniques on two different full-scale state-of-the-art hierarchical machine translation systems. Both systems were trained for 2008 GALE evaluations, in which they outperformed a phrase-based system trained on identical data. 4.1 Hiero: a Hierarchical MT Pipeline Hiero is a hierarchical system that expresses its translatio"
P09-1064,D08-1024,1,0.377021,"r word-aligned sentence pairs provides rule frequency counts, which are normalized to estimate features on rules. The grammar rules of Hiero all share a single non-terminal symbol X, and have at most two non-terminals and six total items (non-terminals and lexical items), for example: my X2 ’s X1 → X1 de mi X2 We extracted the grammar from training data using standard parameters. Rules were allowed to span at most 15 words in the training data. The log-linear model weights were trained using MIRA, a margin-based optimization procedure that accommodates many features (Crammer and Singer, 2003; Chiang et al., 2008). In addition to standard rule frequency features, we included the distortion and syntactic features described in Chiang et al. (2008). 4.2 SBMT: a Syntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: ("
P09-1064,W06-1606,1,0.591075,"yntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decoding with h ic(e,t) Q c(e0 ,t) the similarity measure B(e; e0 ) = t∈T2 c(e , 0 ,h(t)) where h(t) is the unigram prefix of bigram t. 572 Arabic-English Objective Hiero Min. Bayes Risk (Alg 1) 2h 47m 5m 49s Fast Consensus (Alg 3) Speed Ratio 29 Chinese-English Objective Hiero Min. Bayes Risk (Alg 1) 10h 24m Fast Co"
P09-1064,P08-1023,0,0.0827768,"r this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets of derivations and arise naturally in chart-based decoding for a wide variety of hierarchical translation systems (Chiang, 2007; Galley et al., 2006; Mi et al., 2008; Venugopal et al., 2007). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (Tromble et al., 2008). The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts. In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists. We also show that using forests outperfo"
P09-1064,P03-1021,0,0.0977732,"syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decoding with h ic(e,t) Q c(e0 ,t) the similarity measure B(e; e0 ) = t∈T2 c(e , 0 ,h(t)) where h(t) is the unigram prefix of bigram t. 572 Arabic-English Objective Hiero Min. Bayes Risk (Alg 1) 2h 47m 5m 49s Fast Consensus (Alg 3) Speed Ratio 29 Chinese-English Objective Hiero Min. Bayes Risk (Alg 1) 10h 24m Fast Consensus (Alg 3) 4m 52s Speed Ratio 128 Arabic-English Similarity Hiero 52.0 BLEU"
P09-1064,D07-1079,1,0.132411,"(2008). 4.2 SBMT: a Syntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decoding with h ic(e,t) Q c(e0 ,t) the similarity measure B(e; e0 ) = t∈T2 c(e , 0 ,h(t)) where h(t) is the unigram prefix of bigram t. 572 Arabic-English Objective Hiero Min. Bayes Risk (Alg 1) 2h 47m 5m 49s Fast Consensus (Alg 3) Speed Ratio 29 Chinese-English Objective Hiero Min. Bayes Risk (A"
P09-1064,P02-1040,0,0.12146,"lists. Furthermore, our fast decoding procedure can select output sentences based on distributions over entire forests of translations, in addition to k-best lists. We evaluate our procedure on translation forests from two large-scale, state-of-the-art hierarchical machine translation systems. Our forest-based decoding objective consistently outperforms k-best list MBR, giving improvements of up to 1.0 BLEU. 1 Introduction In statistical machine translation, output translations are evaluated by their similarity to human reference translations, where similarity is most often measured by BLEU (Papineni et al., 2002). A decoding objective specifies how to derive final translations from a system’s underlying statistical model. The Bayes optimal decoding objective is to minimize risk based on the similarity measure used for evaluation. The corresponding minimum Bayes risk (MBR) procedure maximizes the expected similarity score of a system’s translations relative to the model’s distribution over possible translations (Kumar and Byrne, 2004). Unfortunately, with a non-linear similarity measure like BLEU, we must resort to approximating the expected loss using a k-best list, which accounts for only a tiny frac"
P09-1064,P07-2026,0,0.185314,"ind e∗ = arg maxe λ · θ(f, e). For MBR decoding, we instead leverage a similarity measure S(e; e0 ) to choose a translation using the model’s probability distribution P(e|f ), which has support over a set of possible translations E. The Viterbi derivation e∗ is the mode of this distribution. MBR is meant to choose a translation that will be similar, on expectation, to any possible reference translation. To this end, MBR chooses e˜ that maximizes expected similarity to the sentences in E under P(e|f ):1 We can sometimes exit the inner for loop early, whenever Ae can never become larger than A (Ehling et al., 2007). Even with this shortcut, the running time of Algorithm 1 is O(k 2 · n), where n is the maximum sentence length, assuming that S(e; e0 ) can be computed in O(n) time. 2.2 Minimum Bayes Risk over Features We now consider the case when S(e; e0 ) is a linear function of sentenceP features. Let S(e; e0 ) be a function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be i"
P09-1064,D07-1014,0,0.0079072,"(e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once for each e. Algorithm 2 proceduralizes this idea: lines 1-4 compute feature expectations, and li"
P09-1064,N04-1035,1,0.103795,"g-linear model weights were trained using MIRA, a margin-based optimization procedure that accommodates many features (Crammer and Singer, 2003; Chiang et al., 2008). In addition to standard rule frequency features, we included the distortion and syntactic features described in Chiang et al. (2008). 4.2 SBMT: a Syntax-Based MT Pipeline SBMT is a string-to-tree translation system with rich target-side syntactic information encoded in the translation model. The synchronous grammar rules are extracted from word aligned sentence pairs where the target sentence is annotated with a syntactic parse (Galley et al., 2004). Rules map source-side strings to target-side parse tree fragments, and non-terminal symbols correspond to target-side grammatical categories: (NP (NP (PRP$ my) NN2 (POS ’s)) NNS1 ) → NNS1 de mi NN2 We extracted the grammar via an array of criteria (Galley et al., 2006; DeNeefe et al., 2007; Marcu et al., 2006). The model was trained using minimum error rate training for Arabic (Och, 2003) and MIRA for Chinese (Chiang et al., 2008). 5 For example, decoding under a variational approximation to the model’s posterior that decomposes over bigram probabilities is equivalent to fast consensus decod"
P09-1064,2006.amta-papers.25,0,0.0235326,"ne different feature-based expressions that also produce BLEU scores for real sentences, but produce different values for fractional features. Some care must be taken to define S(e; φ(e0 )) to extend naturally from integer-valued to real-valued features. Second, while any similarity measure can in principle be expressed as S(e; φ(e0 )) for a sufficiently rich feature space, fast consensus decoding will not apply effectively to all functions. For instance, we cannot naturally use functions that include alignments or matchings between e and e0 , such as METEOR (Agarwal and Lavie, 2007) and TER (Snover et al., 2006). Though these functions can in principle be expressed in terms of features of e0 (for instance with indicator features for whole sentences), fast consensus decoding will only be effective if different sentences share many features, so that the feature expectations effectively capture trends in the underlying distribution. 3 the ... telescope 0.6 “saw the” h = 0.4 · 1 + (0.6 · 1.0) · 1 Figure 2: This translation forest for a Spanish sentence encodes two English parse trees. Hyper-edges (boxes) are annotated with normalized transition probabilities, as well as the bigrams produced by each rule"
P09-1064,P06-1121,1,0.537803,"BR does not apply. For this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets of derivations and arise naturally in chart-based decoding for a wide variety of hierarchical translation systems (Chiang, 2007; Galley et al., 2006; Mi et al., 2008; Venugopal et al., 2007). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (Tromble et al., 2008). The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts. In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists. We also show that using"
P09-1064,W06-1666,0,0.0163604,"function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once for each e. Algorithm 2 proceduralizes this idea: lines 1-4 compute feat"
P09-1064,D08-1065,0,0.317373,"University of California, Berkeley University of Southern California denero@cs.berkeley.edu {chiang, knight}@isi.edu Abstract Our exposition begins with algorithms over kbest lists. A na¨ıve algorithm for finding MBR translations computes the similarity between every pair of k sentences, entailing O(k 2 ) comparisons. We show that if the similarity measure is linear in features of a sentence, then computing expected similarity for all k sentences requires only k similarity evaluations. Specific instances of this general algorithm have recently been proposed for two linear similarity measures (Tromble et al., 2008; Zhang and Gildea, 2008). However, the sentence similarity measures we want to optimize in MT are not linear functions, and so this fast algorithm for MBR does not apply. For this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distribution"
P09-1064,P96-1024,0,0.036875,"a linear function of sentenceP features. Let S(e; e0 ) be a function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once"
P09-1064,N07-1063,0,0.0177076,"propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets of derivations and arise naturally in chart-based decoding for a wide variety of hierarchical translation systems (Chiang, 2007; Galley et al., 2006; Mi et al., 2008; Venugopal et al., 2007). The resulting forest-based decoding procedure compares favorably in both complexity and performance to the recently proposed latticebased MBR (Tromble et al., 2008). The contributions of this paper include a lineartime algorithm for MBR using linear similarities, a linear-time alternative to MBR using non-linear similarity measures, and a forest-based extension to this procedure for similarities based on n-gram counts. In experiments, we show that our fast procedure is on average 80 times faster than MBR using 1000-best lists. We also show that using forests outperforms using k-best lists co"
P09-1064,2006.amta-papers.8,1,0.218007,"tside algorithm, while the denominator is the inside score of the root node. Note that many possible derivations of f are pruned from the forest during decoding, and so this posterior is approximate. The expected n-gram count vector for a hyperedge is E[φ(h)] = P(h|f ) · φ(h). Hence, after computing P (h|f ) for every h, we need only sum P(h|f ) · φ(h) for all h to compute E[φ(e)]. This entire procedure is a linear-time computation in the number of hyper-edges in the forest. To complete forest-based fast consensus decoding, we then extract a k-best list of unique translations from the forest (Huang et al., 2006) and continue Algorithm 3 from line 5, which chooses the e˜ from the k-best list that maximizes BLEU(e; E[φ(e0 )]). 4 The log-BLEU function must be modified slightly to yield a linear Taylor approximation: Tromble et al. (2008) replace the clipped n-gram count with the product of an ngram count and an n-gram indicator function. 571 4 generated already by the decoder of a syntactic translation system. Second, rather than use BLEU as a sentencelevel similarity measure directly, Tromble et al. (2008) approximate corpus BLEU with G above. The parameters θ of the approximation must be estimated on"
P09-1064,P08-1025,0,0.390763,"ia, Berkeley University of Southern California denero@cs.berkeley.edu {chiang, knight}@isi.edu Abstract Our exposition begins with algorithms over kbest lists. A na¨ıve algorithm for finding MBR translations computes the similarity between every pair of k sentences, entailing O(k 2 ) comparisons. We show that if the similarity measure is linear in features of a sentence, then computing expected similarity for all k sentences requires only k similarity evaluations. Specific instances of this general algorithm have recently been proposed for two linear similarity measures (Tromble et al., 2008; Zhang and Gildea, 2008). However, the sentence similarity measures we want to optimize in MT are not linear functions, and so this fast algorithm for MBR does not apply. For this reason, we propose a new objective that retains the benefits of MBR, but can be optimized efficiently, even for non-linear similarity measures. In experiments using BLEU over 1000best lists, we found that our objective provided benefits very similar to MBR, only much faster. This same decoding objective can also be computed efficiently from forest-based expectations. Translation forests compactly encode distributions over much larger sets o"
P09-1064,P08-1067,0,0.0158513,"a, 2008; Tromble et al., 2008). In this section, we consider BLEU in particular, for which the relevant features φ(e) are n-gram counts up to length n = 4. We show how to compute expectations of these counts efficiently from translation forests. 3.1 Translation Forests Translation forests compactly encode an exponential number of output translations for an input sentence, along with their model scores. Forests arise naturally in chart-based decoding procedures for many hierarchical translation systems (Chiang, 2007). Exploiting forests has proven a fruitful avenue of research in both parsing (Huang, 2008) and machine translation (Mi et al., 2008). Formally, translation forests are weighted acyclic hyper-graphs. The nodes are states in the decoding process that include the span (i, j) of the sentence to be translated, the grammar symbol s over that span, and the left and right context words of the translation relevant for computing n-gram language model scores.3 Each hyper-edge h represents the application of a synchronous rule r that combines nodes corresponding to non-terminals in Computing Feature Expectations We now turn our focus to efficiently computing feature expectations, in service of"
P09-1064,W02-1019,0,0.0552059,"res. Let S(e; e0 ) be a function of the form j ωj (e) · φj (e0 ), where φj (e0 ) are real-valued features of e0 , and ωj (e) are sentence-specific weights on those features. Then, the MBR objective can be re-written as   e˜ = arg maxe EP(e0 |f ) S(e; e0 ) X = arg maxe P(e0 |f ) · S(e; e0 ) e0 ∈E MBR can also be interpreted as a consensus decoding procedure: it chooses a translation similar to other high-posterior translations. Minimizing risk has been shown to improve performance for MT (Kumar and Byrne, 2004), as well as other language processing tasks (Goodman, 1996; Goel and Byrne, 2000; Kumar and Byrne, 2002; Titov and Henderson, 2006; Smith and Smith, 2007). The distribution P(e|f ) can be induced from a translation system’s features and weights by exponentiating with base b to form a log-linear model:   arg maxe∈E EP(e0 |f ) S(e; e0 ) = arg maxe X P (e0 |f ) · X e0 ∈E ωj (e) · φj (e0 ) j "" = arg maxe X j = arg maxe X ωj (e) # X 0 0 P (e |f ) · φj (e ) e0 ∈E   ωj (e) · EP(e0 |f ) φj (e0 ) . (1) j bλ·θ(f,e) λ·θ(f,e0 ) e0 ∈E b Equation 1 implies that we can find MBR translations by first computing all feature expectations, then applying S only once for each e. Algorithm 2 proceduralizes this i"
P09-1064,D08-1022,0,\N,Missing
P09-1064,J07-2003,1,\N,Missing
P10-1146,2008.amta-srw.1,0,0.0724859,"recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways (Zhang et al., 2008; Liu et al., 2009; Zollmann and Venugopal, 2006), we compare several methods for extracting a synchronous grammar from tree-to-tree data. One confounding factor in such a comparison is that some methods generate many new synt"
P10-1146,N09-1025,1,0.649334,"because the English-side counterpart of IP/VP has the simple category NP. 3 Decoding In decoding, the rules extracted during training must be reassembled to form a derivation whose source side matches the input sentence. In the exact tree-to-tree approach, whenever substitution is performed, the root labels of the substituted trees must match the labels of the substitution nodes—call this the matching constraint. Because this constraint must be satisﬁed on both the source and target side, it can become difﬁcult to generalize well from training examples to new input sentences. Venugopal et al. (2009), in the string-to-tree setting, attempt to soften the data-fragmentation effect of the matching constraint: instead of trying to ﬁnd the single derivation with the highest probability, they sum over derivations that differ only in their nonterminal labels and try to ﬁnd the single derivation-class with the highest probability. Still, only derivations that satisfy the matching constraint are included in the summation. But in some cases we may want to soften the matching constraint itself. Some syntactic categories are similar enough to be considered compatible: for example, if a rule rooted in"
P10-1146,P05-1033,1,0.684491,"Missing"
P10-1146,J07-2003,1,0.488971,"zes of the parallel texts used are shown in Table 2. We word-aligned the Chinese-English parallel text using GIZA++ followed by link deletion (Fossum et al., 2008), and the Arabic-English parallel text using a combination of GIZA++ and LEAF (Fraser and Marcu, 2007). We parsed the source sides of both parallel texts using the Berkeley parser (Petrov et al., 2006), trained on the Chinese Treebank 6 and Arabic Treebank parts 1–3, and the English sides using a reimplementation of the Collins parser (Collins, 1997). For string-to-string extraction, we used the same constraints as in previous work (Chiang, 2007), with differences shown in Table 2. Rules with nonterminals were extracted from a subset of the data (labeled “Core” in Table 2), and rules without nonterminals were extracted from the full parallel text. Fuzzy tree-to-tree extraction was performed using analogous constraints. For exact tree-to-tree extraction, we used simpler settings: no limit on initial phrase size or unaligned words, and a maximum of 7 frontier nodes on the source side. All systems used the glue rule (Chiang, 2005), which allows the decoder, working bottom-up, to stop building hierarchical structure and instead concatenat"
P10-1146,P97-1003,0,0.205918,"slation, we ran experiments on both Chinese-English and ArabicEnglish translation. 4.1 Setup The sizes of the parallel texts used are shown in Table 2. We word-aligned the Chinese-English parallel text using GIZA++ followed by link deletion (Fossum et al., 2008), and the Arabic-English parallel text using a combination of GIZA++ and LEAF (Fraser and Marcu, 2007). We parsed the source sides of both parallel texts using the Berkeley parser (Petrov et al., 2006), trained on the Chinese Treebank 6 and Arabic Treebank parts 1–3, and the English sides using a reimplementation of the Collins parser (Collins, 1997). For string-to-string extraction, we used the same constraints as in previous work (Chiang, 2007), with differences shown in Table 2. Rules with nonterminals were extracted from a subset of the data (labeled “Core” in Table 2), and rules without nonterminals were extracted from the full parallel text. Fuzzy tree-to-tree extraction was performed using analogous constraints. For exact tree-to-tree extraction, we used simpler settings: no limit on initial phrase size or unaligned words, and a maximum of 7 frontier nodes on the source side. All systems used the glue rule (Chiang, 2005), which all"
P10-1146,P03-2041,0,0.832968,"Missing"
P10-1146,W08-0306,0,0.0432538,"used for calculating feature vectors only and not for checking well-formedness of derivations. This additional state does increase the search space of the decoder, but we did not change any pruning settings. 3 Thanks to Adam Pauls for suggesting this feature class. 4 Experiments To compare the methods described above with hierarchical string-to-string translation, we ran experiments on both Chinese-English and ArabicEnglish translation. 4.1 Setup The sizes of the parallel texts used are shown in Table 2. We word-aligned the Chinese-English parallel text using GIZA++ followed by link deletion (Fossum et al., 2008), and the Arabic-English parallel text using a combination of GIZA++ and LEAF (Fraser and Marcu, 2007). We parsed the source sides of both parallel texts using the Berkeley parser (Petrov et al., 2006), trained on the Chinese Treebank 6 and Arabic Treebank parts 1–3, and the English sides using a reimplementation of the Collins parser (Collins, 1997). For string-to-string extraction, we used the same constraints as in previous work (Chiang, 2007), with differences shown in Table 2. Rules with nonterminals were extracted from a subset of the data (labeled “Core” in Table 2), and rules without n"
P10-1146,D07-1006,0,0.00753526,"additional state does increase the search space of the decoder, but we did not change any pruning settings. 3 Thanks to Adam Pauls for suggesting this feature class. 4 Experiments To compare the methods described above with hierarchical string-to-string translation, we ran experiments on both Chinese-English and ArabicEnglish translation. 4.1 Setup The sizes of the parallel texts used are shown in Table 2. We word-aligned the Chinese-English parallel text using GIZA++ followed by link deletion (Fossum et al., 2008), and the Arabic-English parallel text using a combination of GIZA++ and LEAF (Fraser and Marcu, 2007). We parsed the source sides of both parallel texts using the Berkeley parser (Petrov et al., 2006), trained on the Chinese Treebank 6 and Arabic Treebank parts 1–3, and the English sides using a reimplementation of the Collins parser (Collins, 1997). For string-to-string extraction, we used the same constraints as in previous work (Chiang, 2007), with differences shown in Table 2. Rules with nonterminals were extracted from a subset of the data (labeled “Core” in Table 2), and rules without nonterminals were extracted from the full parallel text. Fuzzy tree-to-tree extraction was performed us"
P10-1146,N04-1035,0,0.763667,"lenge, and show how some old and new ideas can be combined into a simple approach that uses both source and target syntax for signiﬁcant improvements in translation accuracy. 1 Introduction Statistical translation models that use synchronous context-free grammars (SCFGs) or related formalisms to try to capture the recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts"
P10-1146,P06-1121,0,0.491021,"ome old and new ideas can be combined into a simple approach that uses both source and target syntax for signiﬁcant improvements in translation accuracy. 1 Introduction Statistical translation models that use synchronous context-free grammars (SCFGs) or related formalisms to try to capture the recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic co"
P10-1146,2003.mtsummit-papers.22,0,0.0476073,"Missing"
P10-1146,2006.amta-papers.8,0,0.466295,"Missing"
P10-1146,W08-0411,0,0.228374,"Missing"
P10-1146,P06-1077,0,0.7062,"o a simple approach that uses both source and target syntax for signiﬁcant improvements in translation accuracy. 1 Introduction Statistical translation models that use synchronous context-free grammars (SCFGs) or related formalisms to try to capture the recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction i"
P10-1146,P09-1063,0,0.479803,"try to capture the recursive structure of language have been widely adopted over the last few years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways (Zhang et al., 2008; Liu et al., 2009; Zollmann and Venugopal, 2006), we compare several methods for extracting a synchronous grammar from tree-to-tree data. One confounding factor in such a comparison is that some metho"
P10-1146,P04-1084,0,0.0258337,"d out. However, even on human-annotated data, treeto-tree extraction misses many rules, and many such rules would seem to be useful. For example, in Figure 2, the whole English phrase “Taiwan’s. . .shores” is an NP, but its Chinese counterpart is not a constituent. Furthermore, neither “surplus. . .shores” nor its Chinese counterpart are constituents. But both rules are arguably useful for translation. Wellington et al. therefore argue that in order to extract as many rules as possible, a more powerful formalism than synchronous CFG/TSG is required: for example, generalized multitext grammar (Melamed et al., 2004), which is equivalent to synchronous set-local multicomponent CFG/TSG (Weir, 1988). But the problem illustrated in Figure 2 does not reﬂect a very deep fact about syntax or crosslingual divergences, but rather choices in annotation style that interact badly with the exact treeto-tree extraction heuristic. On the Chinese side, the IP is too ﬂat (because 台湾/Táiwān has been analyzed as a topic), whereas the more articulated structure (1) [NP Táiwān [NP [PP zaì . . .] shùnchā]] would also be quite reasonable. On the English side, the high attachment of the PP disagrees with the corresponding Chine"
P10-1146,P06-1055,0,0.0374665,"tings. 3 Thanks to Adam Pauls for suggesting this feature class. 4 Experiments To compare the methods described above with hierarchical string-to-string translation, we ran experiments on both Chinese-English and ArabicEnglish translation. 4.1 Setup The sizes of the parallel texts used are shown in Table 2. We word-aligned the Chinese-English parallel text using GIZA++ followed by link deletion (Fossum et al., 2008), and the Arabic-English parallel text using a combination of GIZA++ and LEAF (Fraser and Marcu, 2007). We parsed the source sides of both parallel texts using the Berkeley parser (Petrov et al., 2006), trained on the Chinese Treebank 6 and Arabic Treebank parts 1–3, and the English sides using a reimplementation of the Collins parser (Collins, 1997). For string-to-string extraction, we used the same constraints as in previous work (Chiang, 2007), with differences shown in Table 2. Rules with nonterminals were extracted from a subset of the data (labeled “Core” in Table 2), and rules without nonterminals were extracted from the full parallel text. Fuzzy tree-to-tree extraction was performed using analogous constraints. For exact tree-to-tree extraction, we used simpler settings: no limit on"
P10-1146,C00-2092,0,0.117075,"Missing"
P10-1146,P08-1058,0,0.0147404,"Missing"
P10-1146,N09-1027,0,0.411041,"es structure (a) because the English-side counterpart of IP/VP has the simple category NP. 3 Decoding In decoding, the rules extracted during training must be reassembled to form a derivation whose source side matches the input sentence. In the exact tree-to-tree approach, whenever substitution is performed, the root labels of the substituted trees must match the labels of the substitution nodes—call this the matching constraint. Because this constraint must be satisﬁed on both the source and target side, it can become difﬁcult to generalize well from training examples to new input sentences. Venugopal et al. (2009), in the string-to-tree setting, attempt to soften the data-fragmentation effect of the matching constraint: instead of trying to ﬁnd the single derivation with the highest probability, they sum over derivations that differ only in their nonterminal labels and try to ﬁnd the single derivation-class with the highest probability. Still, only derivations that satisfy the matching constraint are included in the summation. But in some cases we may want to soften the matching constraint itself. Some syntactic categories are similar enough to be considered compatible: for example, if a rule rooted in"
P10-1146,P06-1123,0,0.0874621,"Missing"
P10-1146,P08-1064,0,0.488014,"w years. The simplest of these (Chiang, 2005) make no use of information from syntactic theories or syntactic annotations, whereas others have successfully incorporated syntactic information on the target side (Galley et al., 2004; Galley et al., 2006) or the source side (Liu et al., 2006; Huang et al., 2006). The next obvious step is toward models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways (Zhang et al., 2008; Liu et al., 2009; Zollmann and Venugopal, 2006), we compare several methods for extracting a synchronous grammar from tree-to-tree data. One confounding factor in such a comparison is that some methods generate many new syntactic categories, making it more difﬁcult to satisfy syntactic constr"
P10-1146,W06-3119,0,0.860623,"models that make full use of syntactic information on both sides. But the natural generalization to this setting has been found to underperform phrasebased models (Liu et al., 2009; Ambati and Lavie, 2008), and researchers have begun to explore solutions (Zhang et al., 2008; Liu et al., 2009). In this paper, we explore the reasons why treeto-tree translation has been challenging, and how source syntax and target syntax might be used together. Drawing on previous successful attempts to relax syntactic constraints during grammar extraction in various ways (Zhang et al., 2008; Liu et al., 2009; Zollmann and Venugopal, 2006), we compare several methods for extracting a synchronous grammar from tree-to-tree data. One confounding factor in such a comparison is that some methods generate many new syntactic categories, making it more difﬁcult to satisfy syntactic constraints at decoding time. We therefore propose to move these constraints from the formalism into the model, implemented as features in the hierarchical phrasebased model Hiero (Chiang, 2005). This augmented model is able to learn from data whether to rely on syntax or not, or to revert back to monotone phrase-based translation. In experiments on Chinese-"
P10-2039,N10-1068,1,0.8887,"Missing"
P10-2039,P08-1085,0,0.170623,"Missing"
P10-2039,P07-1094,0,0.76321,"nique to other problems. In this paper, inspired by the MDL principle, we develop an objective function for generative models that captures both the description of the data by the model (log-likelihood) and the description of the model (model size). By using a simple prior that encourages sparsity, we cast our problem as a search for the maximum a posteriori (MAP) hypothesis and present a variant of EM to approximately search for the minimumdescription-length model. Applying our approach to the POS tagging problem, we obtain higher accuracies than both EM and Bayesian inference as reported by Goldwater and Griffiths (2007). On a Italian POS tagging task, we obtain even larger improvements. We find that our objective function correlates well with accuracy, suggesting that this technique might be useful for other problems. The Minimum Description Length (MDL) principle is a method for model selection that trades off between the explanation of the data by the model and the complexity of the model itself. Inspired by the MDL principle, we develop an objective function for generative models that captures the description of the data by the model (log-likelihood) and the description of the model (model size). We also"
P10-2039,J94-2001,0,0.641864,"mizing the model size in a Hidden Markov Model (HMM) for part-of-speech (POS) tagging leads to higher accuracies than simply running the ExpectationMaximization (EM) algorithm (Dempster et al., 1977). Goldwater and Griffiths (2007) employ a Bayesian approach to POS tagging and use sparse Dirichlet priors to minimize model size. More re2.1 MAP EM with Sparse Priors Objective function In the unsupervised POS tagging task, we are given a word sequence w = w1 , . . . , wN and want to find the best tagging t = t1 , . . . , tN , where ti ∈ T , the tag vocabulary. We adopt the problem formulation of Merialdo (1994), in which we are given a dictionary of possible tags for each word type. We define a bigram HMM P(w, t |θ) = N Y P(w, t |θ) · P(ti |ti−1 ) (1) i=1 In maximum likelihood estimation, the goal is to 209 Proceedings of the ACL 2010 Conference Short Papers, pages 209–214, c Uppsala, Sweden, 11-16 July 2010. 2010 Association for Computational Linguistics find parameter estimates θ 1 0.8 (3) t The EM algorithm can be used to find a solution. However, we would like to maximize likelihood and minimize the size of the model simultaneously. We define the size of a model as the number of non-zero probabi"
P10-2039,P09-1057,0,0.220883,"parameter vector. Let θ1 , . . . , θn be the components of θ. We would like to find  θˆ = arg min − log P(w |θ) + αkθk0 θ 0.6 0.4 0.2 0 0 (10) Substituting (8) into (10) and ignoring the constant term log Z, we get our objective function (6) again. We can exercise finer control over the sparsity of the tag-bigram and channel probability distributions by using a different α for each: arg max log P(w |θ) + θ αc X w,t e −P(w|t) β + αt X e −P(t0 |t) β ! (11) t,t0 In our experiments, we set αc = 0 since previous work has shown that minimizing the number of tag n-gram parameters is more important (Ravi and Knight, 2009; Goldwater and Griffiths, 2007). A common method for preferring smaller modP els is minimizing the L1 norm, i |θi |. However, for a model which is a product of multinomial distributions, the L1 norm is a constant. X X |θi |= θi i i   X X X   0  P(w |t) + P(t |t) = (7) t (8) w t0 = 2|T | Therefore, we cannot use the L1 norm as part of the size term as the result will be the same as the EM algorithm. −θi β where Z = dθ exp α i e is a normalization constant. Then our goal is to find the maximum P  θ i R (9) = arg max log P(w |θ) + log P(θ) −θi β Z X −θi log P(θ) = α · e β − log Z"
P10-2039,P05-1044,0,0.344172,"Missing"
P11-1086,E03-1005,0,0.0230277,"scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003). To our knowledge, there has been no direct comparison between a history-based PCFG approach and DOP approach in the parsing literature. 6 Conclusion In this paper, we have investigated whether we can eliminate composed rules without any loss in translation quality. We have developed a rule Markov model that captures vertical bigrams and trigrams of minimal rules, and tested it in the framework of treeto-string translation. We draw three main conclusions from our experiments. First, our rule Markov models dramatically improve a grammar of minimal rules, giving an improvement of 2.3 Bleu. Seco"
P11-1086,P05-1067,0,0.0482976,"us settings.2 In the second line (2.9 million rules), the drop in Bleu score resulting from adding the rule Markov model is not statistically significant. 5 Related Work Besides the Quirk and Menezes (2006) work discussed in Section 1, there are two other previous 2 For these experiments, a beam size of 100 was used. 863 efforts both using a rule bigram model in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and g"
P11-1086,W08-0306,0,0.0127101,"V is the target-language vocabulary, and g is the order of the n-gram language model (Huang and Mi, 2010). However, if one were to use rule Markov models with a conventional CKY-style 861 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained usin"
P11-1086,N04-1035,0,0.687748,"ause a combinatorial explosion in the number of rules. To avoid this, ad-hoc limits are placed during composition, like upper bounds on the number of nodes in the composed rule, or the height of the rule. Under such limits, the grammar size is manageable, but still much larger than the minimal-rule grammar. Second, due to large grammars, the decoder has to consider many more hypothesis translations, which slows it down. Nevertheless, the advantages outweigh the disadvantages, and to our knowledge, all top-performing systems, both phrase-based and syntax-based, use composed rules. For example, Galley et al. (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk"
P11-1086,P06-1121,0,0.121012,"n, like upper bounds on the number of nodes in the composed rule, or the height of the rule. Under such limits, the grammar size is manageable, but still much larger than the minimal-rule grammar. Second, due to large grammars, the decoder has to consider many more hypothesis translations, which slows it down. Nevertheless, the advantages outweigh the disadvantages, and to our knowledge, all top-performing systems, both phrase-based and syntax-based, use composed rules. For example, Galley et al. (2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk and Menezes (2006) takes such an approach within the framework of dependency translation. However, their study"
P11-1086,D10-1027,1,0.903437,"rts at step 1, where we predict rule r1 (the shaded rule) with probability P(r1 |ǫ) and push its English side onto the stack, with variables replaced by the corresponding tree nodes: x1 becomes NP@1 and x2 becomes VP@2 . This gives us the following stack: In this paper, we use our rule Markov model framework in the context of tree-to-string translation. Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) have gained popularity in recent years due to their speed and simplicity. The input to the translation system is a source parse tree and the output is the target string. Huang and Mi (2010) have recently introduced an efficient incremental decoding algorithm for tree-to-string translation. The decoder operates top-down and maintains a derivation history of translation rules encountered. The history s = [ NP@1 VP@2 ] is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model. This The dot () indicates the next symbol to process in 859 stack hyp. 0 [<s>  IP@ǫ 1 [<s>  IP </s> 2 [<s>  IP@ǫ </s> 3 [<s>  IP@ǫ </s> 4 [<s>  IP@ǫ </s> 5 [<s>  IP@ǫ </s> 6 [<s>  IP@ǫ </s> 7 [<s>  IP@ǫ </s> @1 @2 <s> P(r1 |ǫ) ] [ NP@1 VP@2 ] [ Bush] <s> P(r"
P11-1086,2006.amta-papers.8,1,0.944897,"is above another threshold. 3 Tree-to-string decoding with rule Markov models IP@ǫ VP@2 NP@1 B`ush´ı PP@2.1 VP@2.2 P@2.1.1 NP@2.1.2 VV@2.2.1 AS@2.2.2 NP@2.2.3 yˇu Sh¯al´ong jˇux´ıng le hu`ıt´an Figure 3: Example input parse tree with tree addresses. makes incremental decoding a natural fit with our generative story. In this section, we describe how to integrate our rule Markov model into this incremental decoding algorithm. Note that it is also possible to integrate our rule Markov model with other decoding algorithms, for example, the more common non-incremental top-down/bottom-up approach (Huang et al., 2006), but it would involve a non-trivial change to the decoding algorithms to keep track of the vertical derivation history, which would result in significant overhead. Algorithm Given the input parse tree in Figure 3, Figure 4 illustrates the search process of the incremental decoder with the grammar of Figure 1. We write X @η for a tree node with label X at tree address η (Shieber et al., 1995). The root node has address ǫ, and the ith child of node η has address η.i. At each step, the decoder maintains a stack of active rules, which are rules that have not been completed yet, and the rightmost"
P11-1086,J98-4004,0,0.0855515,"el in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which is analogous to the Data Oriented Parsing (DOP) approach in parsing (Bod, 2003). To our knowledge, there has been no direct comparison between a history-based PCFG approach and DOP approach in the parsing literature. 6 Conclusion In this paper, we have"
P11-1086,W04-3250,0,0.135584,".3+0.5 4.9+7.6 0.3+0.6 176.8 1.3 17.5 1.6 448.7 3.3 448.7+7.6 3.3+1.0 Bleu test 24.2 25.7 26.5 26.5 26.4 27.5 28.0 time (sec/sent) 1.2 1.8 2.0 2.9 2.2 6.8 9.2 Table 1: Main results. Our trigram rule Markov model strongly outperforms minimal rules, and performs at the same level as composed and vertically composed rules, but is smaller and faster. The number of parameters is shown for both the full model and the model filtered for the concatenation of the development and test sets (dev+test). These gains are statistically significant with p < 0.01, using bootstrap resampling with 1000 samples (Koehn, 2004). We find that by just using bigram context, we are able to get at least 1 Bleu point higher than the minimal rule grammar. It is interesting to see that using just bigram rule interactions can give us a reasonable boost. We get our highest gains from using trigram context where our best performing rule Markov model gives us 2.3 Bleu points over minimal rules. This suggests that using longer contexts helps the decoder to find better translations. We also compared rule Markov models against composed rules. Since our models are currently limited to conditioning on vertical context, the closest c"
P11-1086,W08-0308,0,0.0492019,"he rule Markov model is not statistically significant. 5 Related Work Besides the Quirk and Menezes (2006) work discussed in Section 1, there are two other previous 2 For these experiments, a beam size of 100 was used. 863 efforts both using a rule bigram model in machine translation, that is, the probability of the current rule only depends on the immediate previous rule in the vertical context, whereas our rule Markov model can condition on longer and sparser derivation histories. Among them, Ding and Palmer (2005) also use a dependency treelet model similar to Quirk and Menezes (2006), and Liu and Gildea (2008) use a tree-to-string model more like ours. Neither compared to the scenario with composed rules. Outside of machine translation, the idea of weakening independence assumptions by modeling the derivation history is also found in parsing (Johnson, 1998), where rule probabilities are conditioned on parent and grand-parent nonterminals. However, besides the difference between parsing and translation, there are still two major differences. First, our work conditions rule probabilities on parent and grandparent rules, not just nonterminals. Second, we compare against a composed-rule system, which i"
P11-1086,P06-1077,0,0.337664,"f the decoder. The last column in the figure shows the rule Markov model probabilities with the conditioning context. In this example, we use a trigram rule Markov model. After initialization, the process starts at step 1, where we predict rule r1 (the shaded rule) with probability P(r1 |ǫ) and push its English side onto the stack, with variables replaced by the corresponding tree nodes: x1 becomes NP@1 and x2 becomes VP@2 . This gives us the following stack: In this paper, we use our rule Markov model framework in the context of tree-to-string translation. Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) have gained popularity in recent years due to their speed and simplicity. The input to the translation system is a source parse tree and the output is the target string. Huang and Mi (2010) have recently introduced an efficient incremental decoding algorithm for tree-to-string translation. The decoder operates top-down and maintains a derivation history of translation rules encountered. The history s = [ NP@1 VP@2 ] is exactly the vertical chain of ancestors corresponding to the contexts in our rule Markov model. This The dot () indicates the next symbol to process in 8"
P11-1086,P08-1023,1,0.888401,"es). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule patternmatching (Mi et al., 2008). We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the script mteval-v13a.pl. 4.2 Results Table 1 presents the main results of our paper. We used grammars of minimal rules and composed rules of maximum height 3 as our baseline"
P11-1086,P03-1021,0,0.0444013,"entences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development set. At decoding time, we again parse the input sentences using the Berkeley parser, and convert them into translation forests using rule patternmatching (Mi et al., 2008). We evaluate translation quality using case-insensitive IBM Bleu-4, calculated by the script mteval-v13a.pl. 4.2 Results Table 1 presents the main results of our paper. We used grammars of minimal rules and composed rules of maximum height 3 as our baselines. For decoding, we used a beam size of 50. Using the best bigram rule Markov models and the minimal rule grammar gives us a"
P11-1086,N07-1051,0,0.0208804,"uang and Mi, 2010). However, if one were to use rule Markov models with a conventional CKY-style 861 Experiments and results 4.1 Setup The training corpus consists of 1.5M sentence pairs with 38M/32M words of Chinese/English, respectively. Our development set is the newswire portion of the 2006 NIST MT Evaluation test set (616 sentences), and our test set is the newswire portion of the 2008 NIST MT Evaluation test set (691 sentences). We word-aligned the training data using GIZA++ followed by link deletion (Fossum et al., 2008), and then parsed the Chinese sentences using the Berkeley parser (Petrov and Klein, 2007). To extract tree-to-string translation rules, we applied the algorithm of Galley et al. (2004). We trained our rule Markov model on derivations of minimal rules as described above. Our trigram word language model was trained on the target side of the training corpus using the SRILM toolkit (Stolcke, 2002) with modified Kneser-Ney smoothing. The base feature set for all systems is similar to the set used in Mi et al. (2008). The features are combined into a standard log-linear model, which we trained using minimum error-rate training (Och, 2003) to maximize the Bleu score on the development se"
P11-1086,N06-1002,0,0.516674,"2004) initially built a syntax-based system using only minimal rules, and subsequently reported (Galley et al., 2006) that composing rules improves Bleu by 3.6 points, while increasing grammar size 60-fold and decoding time 15-fold. The alternative we propose is to replace composed rules with a rule Markov model that generates rules conditioned on their context. In this work, we restrict a rule’s context to the vertical chain of ancestors of the rule. This ancestral context would play the same role as the context formerly provided by rule composition. The dependency treelet model developed by Quirk and Menezes (2006) takes such an approach within the framework of dependency translation. However, their study leaves unanswered whether a rule Markov model can take the place of composed rules. In this work, we investigate the use of rule Markov models in the context of treeProceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 856–864, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics to-string translation (Liu et al., 2006; Huang et al., 2006). We make three new contributions. First, we carry out a detailed comparison of rule Markov m"
P11-2037,H91-1060,0,0.495299,". *. .to. NP . VBZ . . . who S. SQ/WHNP . VP/WHNP/NP . VBN . . is. NNP . . NP John . believed . . . S/WHNP/NP . *. VP . . VB TO . . ϵ. .to. NP . -NONE. admire . . (a) VP/WHNP . *T* . VP/WHNP . . VB NP/WHNP . admire . . *T* . (b) ϵ. Figure 2: English parse tree with empty elements marked. (a) As annotated in the Penn Treebank. (b) With empty elements reconﬁgured and slash categories added. where “items” are deﬁned differently for each metric, as follows. Deﬁne a nonterminal node, for present purposes, to be a node which is neither a terminal nor preterminal node. The standard PARSEVAL metric (Black et al., 1991) counts labeled nonempty brackets: items are (X, i, j) for each nonempty nonterminal node, where X is its label and i, j are the start and end positions of its span. Yang and Xue (2010) simply count unlabeled empty elements: items are (i, i) for each empty element, where i is its position. If multiple empty elements occur at the same position, they only count the last one. The metric originally proposed by Johnson (2002) counts labeled empty brackets: items are (X/t, i, i) for each empty nonterminal node, where X is its label and t is the type of the empty element it dominates, but also (t, i,"
P11-2037,P04-1082,0,0.624876,"ed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality im"
P11-2037,D10-1062,0,0.538829,"O* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality improves. Our method makes use of a strong syntactic model, the PCFGs with latent annotation of Petrov et al. (2006), which we extend to predict empty cateProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shor"
P11-2037,W03-1005,0,0.661306,"l., 2005) contain detailed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the pa"
P11-2037,N06-1024,0,0.61951,"f empty elements. Yet most parsing work based on these resources has ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality improves. Our method make"
P11-2037,P11-2124,1,0.842048,"ng allowing it to parse a wordlattice instead of a predetermined list of terminals. Lattice parsing adds a layer of ﬂexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1 Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). 213 processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modiﬁed version of the Berkeley parser which allows handling lattices as input.2 The modiﬁcation is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and the initialization procedure of the CKY chart is changed to allow lexical items of spans greater than 1. We then make the necessary adjustments to the parsing algorithm to support this change: trying rules involving prete"
P11-2037,P08-1043,1,0.833705,"a generalization of CKY parsing allowing it to parse a wordlattice instead of a predetermined list of terminals. Lattice parsing adds a layer of ﬂexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1 Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). 213 processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modiﬁed version of the Berkeley parser which allows handling lattices as input.2 The modiﬁcation is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and the initialization procedure of the CKY chart is changed to allow lexical items of spans greater than 1. We then make the necessary adjustments to the parsing algorithm to support this change:"
P11-2037,C10-1045,0,0.00584234,"stead of a predetermined list of terminals. Lattice parsing adds a layer of ﬂexibility to existing parsing technology, and allows parsing in situations where the yield of the tree is not known in advance. Lattice parsing originated in the speech 1 Unfortunately, not enough information was available to carry out comparison with the method of Chung and Gildea (2010). 213 processing community (Hall, 2005; Chappelier et al., 1999), and was recently applied to the task of joint clitic-segmentation and syntactic-parsing in Hebrew (Goldberg and Tsarfaty, 2008; Goldberg and Elhadad, 2011) and Arabic (Green and Manning, 2010). Here, we use lattice parsing for emptyelement recovery. We use a modiﬁed version of the Berkeley parser which allows handling lattices as input.2 The modiﬁcation is fairly straightforward: Each lattice arc correspond to a lexical item. Lexical items are now indexed by their start and end states rather than by their sentence position, and the initialization procedure of the CKY chart is changed to allow lexical items of spans greater than 1. We then make the necessary adjustments to the parsing algorithm to support this change: trying rules involving preterminals even when the span is greater"
P11-2037,P02-1018,0,0.893331,"dependent VP (shíshī fǎlǜ tiáowén). The Penn Treebanks (Marcus et al., 1993; Xue et al., 2005) contain detailed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with"
P11-2037,J93-2004,0,0.0399322,"ted (John was believed to admire who?). Empty elements exist in many languages and serve different purposes. In languages such as Chinese and Korean, where subjects and objects can be dropped to avoid duplication, empty elements are particularly important, as they indicate the position of dropped arguments. Figure 1 gives an example of a Chinese parse tree with empty elements. The ﬁrst empty element (*pro*) marks the subject of the whole sentence, a pronoun inferable from context. The second empty element (*PRO*) marks the subject of the dependent VP (shíshī fǎlǜ tiáowén). The Penn Treebanks (Marcus et al., 1993; Xue et al., 2005) contain detailed annotations of empty elements. Yet most parsing work based on these resources has ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by severa"
P11-2037,P06-1055,0,0.0312248,"empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality improves. Our method makes use of a strong syntactic model, the PCFGs with latent annotation of Petrov et al. (2006), which we extend to predict empty cateProceedings of the 49th Annual Meeting of the Association for Computational Linguistics:shortpapers, pages 212–216, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguistics gories by the use of lattice parsing. The method is language-independent and performs very well on both languages we tested it on: for English, it outperforms the best published method we are aware of (Schmid, 2006), and for Chinese, it outperforms the method of Yang and Xue (2010).1 2 Method Our method is fairly simple. We take a state-of-theart parsing mod"
P11-2037,roark-etal-2006-sparseval,0,0.0134392,"Missing"
P11-2037,P06-1023,0,0.743656,"s ignored empty elements, with some 212 NP . VP . -NONE. ADVP . *pro* . AD . . VP . VV . . 终止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality improves. Our method makes use of a strong syntactic model, the PCFGs with latent an"
P11-2037,C10-2158,0,0.744237,"止 . 暂时 . NP zànshí zhōngzhǐ . for now suspend -NONE. . . .*PRO* IP. VP . . VV NP . . NN . 实施 . NN shíshī 条文 . . implement 法律 . fǎlǜ tiáowén law clause . Figure 1: Chinese parse tree with empty elements marked. The meaning of the sentence is, “Implementation of the law is temporarily suspended.” notable exceptions. Johnson (2002) studied emptyelement recovery in English, followed by several others (Dienes and Dubey, 2003; Campbell, 2004; Gabbard et al., 2006); the best results we are aware of are due to Schmid (2006). Recently, empty-element recovery for Chinese has begun to receive attention: Yang and Xue (2010) treat it as classiﬁcation problem, while Chung and Gildea (2010) pursue several approaches for both Korean and Chinese, and explore applications to machine translation. Our intuition motivating this work is that empty elements are an integral part of syntactic structure, and should be constructed jointly with it, not added in afterwards. Moreover, we expect empty-element recovery to improve as the parsing quality improves. Our method makes use of a strong syntactic model, the PCFGs with latent annotation of Petrov et al. (2006), which we extend to predict empty cateProceedings of the 49th Ann"
P11-2056,J09-2001,0,0.0843537,"Missing"
P11-2056,N10-1083,0,0.0588888,"Missing"
P11-2056,P07-1005,1,0.835466,"sts an average of 9.76 senses for each of the 34 most frequent English prepositions, while nouns usually have around two (WordNet nouns average about 1.2 senses, 2.7 if monosemous nouns are excluded (Fellbaum, 1998)). Disambiguating prepositions is thus a challenging and interesting task in itself (as exemplified by the SemEval 2007 task, (Litkowski and Hargraves, 2007)), and holds promise for NLP applications such as Information Extraction or Machine Translation.1 Given a sentence such as the following: In the morning, he shopped in Rome we ultimately want to be able to annotate it as 1 See (Chan et al., 2007) for how using WSD can help MT. Here, the preposition in has two distinct meanings, namely a temporal and a locative one. These meanings are context-dependent. Ultimately, we want to disambiguate prepositions not by and for themselves, but in the context of sequential semantic labeling. This should also improve disambiguation of the words linked by the prepositions (here, morning, shopped, and Rome). We propose using unsupervised methods in order to leverage unlabeled data, since, to our knowledge, there are no annotated data sets that include both preposition and argument senses. In this pape"
P11-2056,W02-0102,0,0.418239,"Missing"
P11-2056,C10-2052,1,0.904044,"onstrain the argument senses, we construct a dictionary that lists for each word all the possible lexicographer senses according to WordNet. The set of lexicographer senses (45) is a higher level abstraction which is sufficiently coarse to allow for a good generalization. Unknown words are assumed to have all possible senses applicable to their 324 b) h! p! o! h p o h! p! o! h c) h! h o p! o! o Figure 1: Graphical Models. a) 1st order HMM. b) variant used in experiments (one model/preposition, thus no conditioning on p). c) incorporates further constraints on variables As shown by Hovy et al. (2010), preposition senses can be accurately disambiguated using only the head word and object of the PP. We exploit this property of prepositional constructions to represent the constraints between h, p, and o in a graphical model. We define a good model as one that reasonably constrains the choices, but is still tractable in terms of the number of parameters being estimated. As a starting point, we choose the standard firstorder Hidden Markov Model as depicted in Figure 1a. Since we train a separate model for each preposition, we can omit all arcs to p. This results in model 1b. The joint distribu"
P11-2056,D07-1031,0,0.0815467,"nitialized all models uniformly, and trained until the perplexity rate stopped increasing or a predefined number of iterations was reached. Note that MAPEM and Bayesian Inference require tuning of some hyper-parameters on held-out data, and are thus not fully unsupervised. 5.1 EM We use the EM algorithm (Dempster et al., 1977) as a baseline. It is relatively easy to implement with existing toolkits like Carmel (Graehl, 1997). However, EM has a tendency to assume equal importance for each parameter. It thus prefers “general” solutions, assigning part of the probability mass to unlikely states (Johnson, 2007). We ran EM on each model for 100 iterations, or until the perplexity stopped decreasing below a threshold of 10−6 . 5.2 2002). Smoothing helps to prevent overfitting. Repeated random restarts help escape unfavorable initializations that lead to local maxima. Carmel provides options for both smoothing and restarts. 5.3 MAP-EM with L0 Norm Since we want to encourage sparsity in our models, we use the MDL-inspired technique introduced by Vaswani et al. (2010). Here, the goal is to increase the data likelihood while keeping the number of parameters small. The authors use a smoothed L0 prior, whic"
P11-2056,S07-1005,0,0.363796,"Missing"
P11-2056,W03-0411,0,0.221531,"Missing"
P11-2056,J09-2002,0,0.428983,"Missing"
P11-2056,N09-3017,1,0.700377,"Missing"
P11-2056,P10-2039,1,0.79241,"Missing"
P11-2056,S07-1051,0,0.43851,"Missing"
P11-2056,W10-0900,0,\N,Missing
P11-2056,N10-1068,1,\N,Missing
P11-2080,D08-1024,1,0.191835,"Missing"
P11-2080,N09-1025,1,0.39032,"rained on 190+220 million words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Both baseline also include a variety of other features (Chiang et al., 2008; Chiang et al., 2009; Chiang, 2010). Both systems were trained using MIRA (Crammer et al., 2006; Watanabe et al., 2007; Chiang et al., 2008) on a held-out set, then tested on two more sets (Dev and Test) disjoint from the data used for rule extraction and for MIRA training. These datasets have roughly 1000–3000 sentences (30,000–70,000 words) and are drawn from test sets from the NIST MT evaluation and development sets from the GALE program. smoothed system generated the improved translation (8) above, and generally gave a small improvement: task features Dev Chi-Eng nw baseline 28.7 morph 29.1 We then tested the"
P11-2080,P05-1033,1,0.132296,"26.4 23.0 26.0 29.8 27.1 23.4 26.6 Table 4: Our variations on lexical weighting improve translation quality signiﬁcantly across 16 different test conditions. All improvements are signiﬁcant at the p &lt; 0.01 level, except where marked with an asterisk (∗ ), indicating p &lt; 0.05. In order to address the space problem, we use the following heuristic: for any given rule, if the absolute value of one of these features is less than log 2, we discard it for that rule. 5 Experiments Setup We tested these features on two machine translation systems: a hierarchical phrasebased (string-to-string) system (Chiang, 2005) and a syntax-based (string-to-tree) system (Galley et al., 2004; Galley et al., 2006). For Arabic-English translation, both systems were trained on 190+220 million words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance f"
P11-2080,P10-1146,1,0.845055,"lion words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Both baseline also include a variety of other features (Chiang et al., 2008; Chiang et al., 2009; Chiang, 2010). Both systems were trained using MIRA (Crammer et al., 2006; Watanabe et al., 2007; Chiang et al., 2008) on a held-out set, then tested on two more sets (Dev and Test) disjoint from the data used for rule extraction and for MIRA training. These datasets have roughly 1000–3000 sentences (30,000–70,000 words) and are drawn from test sets from the NIST MT evaluation and development sets from the GALE program. smoothed system generated the improved translation (8) above, and generally gave a small improvement: task features Dev Chi-Eng nw baseline 28.7 morph 29.1 We then tested the provenance-con"
P11-2080,N04-1035,0,0.005896,"s on lexical weighting improve translation quality signiﬁcantly across 16 different test conditions. All improvements are signiﬁcant at the p &lt; 0.01 level, except where marked with an asterisk (∗ ), indicating p &lt; 0.05. In order to address the space problem, we use the following heuristic: for any given rule, if the absolute value of one of these features is less than log 2, we discard it for that rule. 5 Experiments Setup We tested these features on two machine translation systems: a hierarchical phrasebased (string-to-string) system (Chiang, 2005) and a syntax-based (string-to-tree) system (Galley et al., 2004; Galley et al., 2006). For Arabic-English translation, both systems were trained on 190+220 million words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Bo"
P11-2080,P06-1121,1,0.218151,"g improve translation quality signiﬁcantly across 16 different test conditions. All improvements are signiﬁcant at the p &lt; 0.01 level, except where marked with an asterisk (∗ ), indicating p &lt; 0.05. In order to address the space problem, we use the following heuristic: for any given rule, if the absolute value of one of these features is less than log 2, we discard it for that rule. 5 Experiments Setup We tested these features on two machine translation systems: a hierarchical phrasebased (string-to-string) system (Chiang, 2005) and a syntax-based (string-to-tree) system (Galley et al., 2004; Galley et al., 2006). For Arabic-English translation, both systems were trained on 190+220 million words of parallel data; for Chinese-English, the string-to-string system was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Both baseline also inclu"
P11-2080,N03-1017,0,0.0104374,"1001 Marina del Rey, CA 90292 {chiang,sdeneefe,pust}@isi.edu Abstract We introduce two simple improvements to the lexical weighting features of Koehn, Och, and Marcu (2003) for machine translation: one which smooths the probability of translating word f to word e by simplifying English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems. 1 Introduction Lexical weighting features (Koehn et al., 2003) estimate the probability of a phrase pair or translation rule word-by-word. In this paper, we introduce two simple improvements to these features: one which smooths the probability of translating word f to word e using English morphology, and one which conditions it on the kind of training data that f and e co-occurred in. These new variations lead to improvements of up to +0.8 BLEU, with an average improvement of +0.6 BLEU across two language pairs, two genres, and two translation systems. 2 Background each i, let 1 |ai | c(NULL, ei ) ← c(NULL, ei ) + 1 c( f j , ei ) ← c( f j , ei ) + for j"
P11-2080,D09-1074,0,0.0305841,"e proposed ruling systems. (12) MT (baseline): This may be one of the most important differences between pictures of the proposed ruling regimes. (13) MT (better): Perhaps this is one of the most important differences between the images of the proposed regimes. The Arabic word  وﻟﻌﻞcan be translated as may or perhaps (among others), with the latter more common according to t(e |f ), as shown in Table 3. But some genres favor perhaps more or less strongly. Thus, both translations (12) and (13) are good, but the latter uses a slightly more informal register appropriate to the genre. Following Matsoukas et al. (2009), we assign each training sentence pair a set of binary features which we call s-features: 457 f وﻟﻌﻞ وﻟﻌﻞ e may perhaps t(e |f ) – 0.13 0.20 nw 0.12 0.23 t s (e | web 0.16 0.32 f) bn 0.09 0.42 un 0.13 0.19 Table 3: Different genres have different preferences for word translations. Key: nw = newswire, web = Web, bn = broadcast news, un = United Nations proceedings. • Whether the sentence pair came from a particular genre, for example, newswire or web • Whether the sentence pair came from a particular collection, for example, FBIS or UN Matsoukas et al. (2009) use these s-features to comput"
P11-2080,J05-4003,0,0.0102609,"Missing"
P11-2080,D07-1080,0,0.0128622,"em was trained on 240+260 million words of parallel data, and the string-to-tree system, 58+65 million words. Both used two language models, one trained on the combined English sides of the Arabic-English and Chinese-English data, and one trained on 4 billion words of English data. The baseline string-to-string system already incorporates some simple provenance features: for each s-feature s, there is a feature P(s |rule). Both baseline also include a variety of other features (Chiang et al., 2008; Chiang et al., 2009; Chiang, 2010). Both systems were trained using MIRA (Crammer et al., 2006; Watanabe et al., 2007; Chiang et al., 2008) on a held-out set, then tested on two more sets (Dev and Test) disjoint from the data used for rule extraction and for MIRA training. These datasets have roughly 1000–3000 sentences (30,000–70,000 words) and are drawn from test sets from the NIST MT evaluation and development sets from the GALE program. smoothed system generated the improved translation (8) above, and generally gave a small improvement: task features Dev Chi-Eng nw baseline 28.7 morph 29.1 We then tested the provenance-conditioned features on both Arabic-English and Chinese-English, again using the strin"
P12-1033,W09-1804,0,0.0332356,"o investigate the impact of hyperparameter tuning on translation quality. For the smaller Arabic-English corpus, we symmetrized all combinations of the two top-scoring alignments (according to F1) in each direction, yielding four sets of alignments. Table 4 shows Bleu scores for translation models learned from these alignments. Unfortunately, we find that optimizing F1 is not optimal for Bleu—using the second-best alignments yields a further improvement of 0.5 Bleu on the NIST 2009 data, which is statistically significant (p < 0.05). 4 Related Work Schoenemann (2011a), taking inspiration from Bodrumlu et al. (2009), uses integer linear programming to optimize IBM Model 1–2 and the HMM with the ℓ0 -norm. This method, however, does not outperform GIZA++. In later work, Schoenemann (2011b) used projected gradient descent for the ℓ1 norm. Here, we have adopted his use of projected gradient descent, but using a smoothed ℓ0 -norm. Liang et al. (2006) show how to train IBM models in both directions simultaneously by adding a term to the log-likelihood that measures the agreement between the two directions. Grac¸a et al. (2010) explore modifications to the HMM model that encourage bijectivity and symmetry. The"
P12-1033,bojar-prokopova-2006-czech,0,0.0655725,"Missing"
P12-1033,J93-2003,0,0.142348,", in experiments on Czech, Arabic, Chinese, and Urdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu). 1 Introduction Automatic word alignment is a vital component of nearly all current statistical translation pipelines. Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar"
P12-1033,D08-1024,1,0.676974,"Missing"
P12-1033,J07-2003,1,0.0816809,"Missing"
P12-1033,P11-1042,0,0.241029,"Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an ℓ0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previous work on unsupervised part-ofspeech tag"
P12-1033,J07-3002,0,0.0950827,"and β = 0.05. We did not choose a large α, as the dataset was small, and we chose a conservative value for β. We ran word alignment in both directions and symmetrized using grow-diag-final (Koehn et al., 2003). For models with the smoothed ℓ0 prior, we tuned α and β separately in each direction. 3.2 Alignment First, we evaluated alignment accuracy directly by comparing against gold-standard word alignments. 4 This data is available at http://statmt.org/wmt10. The results are shown in the alignment F1 column of Table 1. We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007). Following Dyer et al. (2011), we also measured the average fertility, φ˜ sing. , of once-seen source words in the symmetrized alignments. Our alignments show smaller fertility for once-seen words, suggesting that they suffer from “garbage collection” effects less than the baseline alignments do. The fact that we had to use hand-aligned data to tune the hyperparameters α and β means that our method is no longer completely unsupervised. However, our observation is that alignment accuracy is actually fairly robust to the choice of these hyperparameters, as shown in Table 2. As we will see below"
P12-1033,J10-3007,0,0.119524,"Missing"
P12-1033,N03-1017,0,0.00539255,"s.4 We set the hyperparameters α and β by tuning on gold-standard word alignments (to maximize F1) when possible. For Arabic-English and ChineseEnglish, we used 346 and 184 hand-aligned sentences from LDC2006E86 and LDC2006E93. Similarly, for Czech-English, 515 hand-aligned sentences were available (Bojar and Prokopov´a, 2006). But for Urdu-English, since we did not have any gold alignments, we used α = 10 and β = 0.05. We did not choose a large α, as the dataset was small, and we chose a conservative value for β. We ran word alignment in both directions and symmetrized using grow-diag-final (Koehn et al., 2003). For models with the smoothed ℓ0 prior, we tuned α and β separately in each direction. 3.2 Alignment First, we evaluated alignment accuracy directly by comparing against gold-standard word alignments. 4 This data is available at http://statmt.org/wmt10. The results are shown in the alignment F1 column of Table 1. We used balanced F-measure rather than alignment error rate as our metric (Fraser and Marcu, 2007). Following Dyer et al. (2011), we also measured the average fertility, φ˜ sing. , of once-seen source words in the symmetrized alignments. Our alignments show smaller fertility for once"
P12-1033,W04-3250,0,0.0242181,"e used for word alignment. The development data that were used for discriminative training were: for Chinese-English and Arabic-English, data from the NIST 2004 and NIST 2006 test sets, plus newsgroup data from the GALE program (LDC2006E92); for Urdu-English, half of the NIST 2008 test set; for Czech-English, a training set of 2051 sentences provided by the WMT10 translation workshop. The results are shown in the Bleu column of Table 1. We used case-insensitive IBM Bleu (closest reference length) as our metric. Significance testing was carried out using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). All of the tests showed significant improvements (p < 0.01), ranging from +0.4 Bleu to +1.4 Bleu. For Urdu, even though we didn’t have manual alignments to tune hyperparameters, we got significant gains over a good baseline. This is promising for languages that do not have any manually aligned data. Ideally, one would want to tune α and β to maximize Bleu. However, this is prohibitively expensive, especially if we must tune them separately in each alignment direction before symmetrization. We ran some contrastive experiments to investigate the impact of hyperparameter tu"
P12-1033,N06-1014,0,0.728874,"nated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an ℓ0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previo"
P12-1033,P11-2032,0,0.432532,"Missing"
P12-1033,H05-1011,0,0.058597,"(Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM"
P12-1033,J04-4002,0,0.0987219,"nent of nearly all current statistical translation pipelines. Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011)"
P12-1033,P10-1017,0,0.0398842,"the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an ℓ0 prior, inspired"
P12-1033,W11-0320,0,0.0721951,"the constraints (8). The count C(e, f ) is the number of times that f occurs aligned to e. For MAP-EM, it is: θˆ = arg min − θ X E[C(e, f )] log t( f |e) − e, f α X e, f −t( f |e) exp β ! (10) This optimization problem is non-convex, and we do not know of a closed-form solution. Previously (Vaswani et al., 2010), we used ALGENCAN, a nonlinear optimization toolkit, but this solution does not scale well to the number of parameters involved in word alignment models. Instead, we use a simpler and more scalable method which we describe in the next section. 2.3 Projected gradient descent Following Schoenemann (2011b), we use projected gradient descent (PGD) to solve the M-step (but with the ℓ0 -norm instead of the ℓ1 -norm). Gradient projection methods are attractive solutions to constrained optimization problems, particularly when the constraints on the parameters are simple (Bertsekas, 1999). Let F(θ) be the objective function in (11) This moves θ in the direction of steepest descent (∇F) with step size s, and then the function [·]∆ projects the resulting point onto the simplex; that is, it finds the nearest point that satisfies the constraints (8). The gradient ∇F(θk ) is E[C( f, e)] α −t( f |e) ∂F ="
P12-1033,I11-1147,0,0.282807,"the constraints (8). The count C(e, f ) is the number of times that f occurs aligned to e. For MAP-EM, it is: θˆ = arg min − θ X E[C(e, f )] log t( f |e) − e, f α X e, f −t( f |e) exp β ! (10) This optimization problem is non-convex, and we do not know of a closed-form solution. Previously (Vaswani et al., 2010), we used ALGENCAN, a nonlinear optimization toolkit, but this solution does not scale well to the number of parameters involved in word alignment models. Instead, we use a simpler and more scalable method which we describe in the next section. 2.3 Projected gradient descent Following Schoenemann (2011b), we use projected gradient descent (PGD) to solve the M-step (but with the ℓ0 -norm instead of the ℓ1 -norm). Gradient projection methods are attractive solutions to constrained optimization problems, particularly when the constraints on the parameters are simple (Bertsekas, 1999). Let F(θ) be the objective function in (11) This moves θ in the direction of steepest descent (∇F) with step size s, and then the function [·]∆ projects the resulting point onto the simplex; that is, it finds the nearest point that satisfies the constraints (8). The gradient ∇F(θk ) is E[C( f, e)] α −t( f |e) ∂F ="
P12-1033,H05-1010,0,0.106405,", 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although manually-aligned data is very valuable, it is only available for a small number of language pairs. Other models are unsupervised like the IBM models (Liang et al., 2006; Grac¸a et al., 2010; Dyer et al., 2011), but have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporati"
P12-1033,P10-2039,1,0.934169,"have not been as widely adopted as GIZA++ has. In this paper, we propose a simple extension to the IBM/HMM models that is unsupervised like the IBM models, is as scalable as GIZA++ because it is implemented on top of GIZA++, and provides significant improvements in both alignment and translation quality. It extends the IBM/HMM models by incorporating an ℓ0 prior, inspired by the principle of minimum description length (Barron et al., 1998), to encourage sparsity in the word-to-word translation model (Section 2.2). This extension follows our previous work on unsupervised part-ofspeech tagging (Vaswani et al., 2010), but enables it to scale to the large datasets typical in word alignment, using an efficient training method based on projected gradient descent (Section 2.3). Experiments on Czech-, Arabic-, Chinese- and UrduEnglish translation (Section 3) demonstrate consistent significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu). Our implementation has been released as a simple modification to the GIZA++ toolkit that can be used as a drop-in replacement for GIZA++ in any existing MT pipeline. 2 Method We start with a brief review of the"
P12-1033,C96-2141,0,0.953728,"rdu to English translation, significant improvements over IBM Model 4 in both word alignment (up to +6.7 F1) and translation quality (up to +1.4 Bleu). 1 Introduction Automatic word alignment is a vital component of nearly all current statistical translation pipelines. Although state-of-the-art translation models use rules that operate on units bigger than words (like phrases or tree fragments), they nearly always use word alignments to drive extraction of those translation rules. The dominant approach to word alignment has been the IBM models (Brown et al., 1993) together with the HMM model (Vogel et al., 1996). These models are unsupervised, making them applicable to any language pair for which parallel text is available. Moreover, they are widely disseminated in the open-source GIZA++ toolkit (Och and Ney, 2004). These properties make them the default choice for most statistical MT systems. In the decades since their invention, many models have surpassed them in accuracy, but none has supplanted them in practice. Some of these models are partially supervised, combining unlabeled parallel text with manually-aligned parallel text (Moore, 2005; Taskar et al., 2005; Riesa and Marcu, 2010). Although ma"
P12-1033,zhang-etal-2004-interpreting,0,0.0273054,"rd alignment. The development data that were used for discriminative training were: for Chinese-English and Arabic-English, data from the NIST 2004 and NIST 2006 test sets, plus newsgroup data from the GALE program (LDC2006E92); for Urdu-English, half of the NIST 2008 test set; for Czech-English, a training set of 2051 sentences provided by the WMT10 translation workshop. The results are shown in the Bleu column of Table 1. We used case-insensitive IBM Bleu (closest reference length) as our metric. Significance testing was carried out using bootstrap resampling with 1000 samples (Koehn, 2004; Zhang et al., 2004). All of the tests showed significant improvements (p < 0.01), ranging from +0.4 Bleu to +1.4 Bleu. For Urdu, even though we didn’t have manual alignments to tune hyperparameters, we got significant gains over a good baseline. This is promising for languages that do not have any manually aligned data. Ideally, one would want to tune α and β to maximize Bleu. However, this is prohibitively expensive, especially if we must tune them separately in each alignment direction before symmetrization. We ran some contrastive experiments to investigate the impact of hyperparameter tuning on translation q"
P12-1033,P04-1066,0,\N,Missing
P12-2062,W00-1201,1,0.602868,"Missing"
P12-2062,C92-3126,0,0.0876445,"increase with the language model size, with the largest language model yielding a net improvement of 0.82 over the baseline parser. To test further the importance of bilingual information, we compared against a system built only from the Chinese side of the parallel text (with each word aligned to itself). We removed all features that use bilingual information, retaining only the parser probability and the phrase penalty. In their place we added a new feature, the probability of a rule’s source side tree given its root label, which is essentially the same model used in Data-Oriented Parsing (Bod, 1992). Table 3c shows that this system still outperforms the original parser. In other words, part of the gain is not attributable to translation, but additional source-side context and data that the translation model happens to capture. Finally, we varied the size of the parallel text (keeping a maximum rule height of 5 and the largest language model) and found that, as expected, parsing performance correlates with parallel data size (Table 3d). 5 Conclusion We set out to investigate why forest-to-string translation outperforms tree-to-string translation. By comparing their performance as Chinese"
P12-2062,D08-1092,0,0.0245198,"ts corresponding Chinese parse. (a) fff parser −−−−→ source string (b) fff parser −−−−→ source string decoder . fff source tree −−−−−→ . fff source forest −−−−−→ eeee target string decoder eeee target string Figure 1: (a) In tree-to-string translation, the parser generates a single tree which the decoder must use to generate a translation. (b) In forest-to-string translation, the parser generates a forest of possible trees, any of which the decoder can use to generate a translation. Previous work has shown that an observed targetlanguage translation can improve parsing of sourcelanguage text (Burkett and Klein, 2008; Huang et al., 2009), but to our knowledge, only Chen et al. (2011) have explored the case where the target-language translation is unobserved. Below, we carry out experiments to test these two hypotheses. We measure the accuracy (using labeled-bracket F1) of the parses that the translation model selects, and find that they are worse than the parses selected by the parser. Our basic conclusion, then, is that the parses that help translation (according to Bleu) are, on average, worse parses. That is, forest-to-string translation hurts parsing. But there is a twist. Neither labeled-bracket F1 n"
P12-2062,A00-2018,0,0.254382,"Missing"
P12-2062,W05-1506,1,0.799583,"d results (Mi et al., 2008; Zhang et al., 2009). However, we also find that the trees selected by the forest-to-string system score much lower according to labeled-bracket F1. This suggests that the reason the forest-to-string system is able to generate better translations is that it can soften the constraints imposed by the syntax of the source language. 4.1 Setup MER training with labeled-bracket F1 as an objective function is straightforward. At each iteration of MER training, we run the parser and decoder over the CTB dev set to generate an n-best list of possible translation derivations (Huang and Chiang, 2005). For each derivation, we extract its Chinese parse tree and compute the number of brackets guessed and the number matched against the gold-standard parse tree. A trivial modification of the MER trainer then optimizes the feature weights to maximize labeledbracket F1. A technical challenge that arises is ensuring diversity in the n-best lists. The MER trainer requires that each list contain enough unique translations (when maximizing Bleu) or source trees (when maximizing labeled-bracket F1). However, because one source tree may lead to many translation derivations, the n-best list may contain"
P12-2062,2006.amta-papers.8,0,0.0674651,"Missing"
P12-2062,D09-1127,0,0.0505221,"Missing"
P12-2062,P03-1056,0,0.0911656,"Missing"
P12-2062,P06-1077,0,0.552593,"nia Information Sciences Institute chiang@isi.edu encounters the following correct tree (only partial bracketing shown): (1) [NP j¯ıngj`ı z¯engzhˇang] de s`ud`u economy growth DE rate ‘economic growth rate’ Suppose further that the model has never seen this phrase before, although it has seen the subphrase z¯engzhˇang de s`ud`u ‘growth rate’. Because this subphrase is not a syntactic unit in sentence (1), the system will be unable to translate it. But a forest-tostring system would be free to choose another (incorrect but plausible) bracketing: Introduction Tree-to-string translation systems (Liu et al., 2006; Huang et al., 2006) typically employ a pipeline of two stages: a syntactic parser for the source language, and a decoder that translates source-language trees into target-language strings. Originally, the output of the parser stage was a single parse tree, and this type of system has been shown to outperform phrase-based translation on, for instance, Chineseto-English translation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest, that is, a representation of a whole distribution over possible parse trees"
P12-2062,P07-1089,0,0.0213934,"eseto-English translation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest, that is, a representation of a whole distribution over possible parse trees (Mi et al., 2008). In this paper, we investigate two hypotheses to explain why. One hypothesis is that forest-to-string translation selects worse parses. Although syntax often helps translation, there may be situations where syntax, or at least syntax in the way that our models use it, can impose constraints that are too rigid for good-quality translation (Liu et al., 2007; Zhang et al., 2008). For example, suppose that a tree-to-string system (2) j¯ıngj`ı [NP z¯engzhˇang de s`ud`u] economy growth DE rate and successfully translate it using rules learned from observed data. The other hypothesis is that forest-to-string translation selects better parses. For example, if a Chinese parser is given the input c¯anji¯a biˇaojiˇe de h¯unlˇı, it might consider two structures: (3) [VP c¯anji¯a biˇaojiˇe] de h¯unlˇı attend cousin DE wedding ‘wedding that attends a cousin’ (4) c¯anji¯a [NP biˇaojiˇe de h¯unlˇı] attend cousin DE wedding ‘attend a cousin’s wedding’ The two"
P12-2062,D08-1022,0,0.0417473,"Missing"
P12-2062,P08-1023,0,0.574103,"Huang et al., 2006) typically employ a pipeline of two stages: a syntactic parser for the source language, and a decoder that translates source-language trees into target-language strings. Originally, the output of the parser stage was a single parse tree, and this type of system has been shown to outperform phrase-based translation on, for instance, Chineseto-English translation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest, that is, a representation of a whole distribution over possible parse trees (Mi et al., 2008). In this paper, we investigate two hypotheses to explain why. One hypothesis is that forest-to-string translation selects worse parses. Although syntax often helps translation, there may be situations where syntax, or at least syntax in the way that our models use it, can impose constraints that are too rigid for good-quality translation (Liu et al., 2007; Zhang et al., 2008). For example, suppose that a tree-to-string system (2) j¯ıngj`ı [NP z¯engzhˇang de s`ud`u] economy growth DE rate and successfully translate it using rules learned from observed data. The other hypothesis is that forest-"
P12-2062,P03-1021,0,0.0611219,"Missing"
P12-2062,P06-1055,0,0.0214874,"Missing"
P12-2062,P08-1064,0,0.0157974,"slation (Liu et al., 2006). More recent work has shown that translation quality is improved further if the parser outputs a weighted parse forest, that is, a representation of a whole distribution over possible parse trees (Mi et al., 2008). In this paper, we investigate two hypotheses to explain why. One hypothesis is that forest-to-string translation selects worse parses. Although syntax often helps translation, there may be situations where syntax, or at least syntax in the way that our models use it, can impose constraints that are too rigid for good-quality translation (Liu et al., 2007; Zhang et al., 2008). For example, suppose that a tree-to-string system (2) j¯ıngj`ı [NP z¯engzhˇang de s`ud`u] economy growth DE rate and successfully translate it using rules learned from observed data. The other hypothesis is that forest-to-string translation selects better parses. For example, if a Chinese parser is given the input c¯anji¯a biˇaojiˇe de h¯unlˇı, it might consider two structures: (3) [VP c¯anji¯a biˇaojiˇe] de h¯unlˇı attend cousin DE wedding ‘wedding that attends a cousin’ (4) c¯anji¯a [NP biˇaojiˇe de h¯unlˇı] attend cousin DE wedding ‘attend a cousin’s wedding’ The two structures have two d"
P12-2062,P09-1020,1,0.771487,"e (MER) training to optimize the feature weights (Och, 2003) to maximize Bleu. At decoding time, we select the best derivation and extract its source tree. In principle, we ought to sum over all derivations for each source tree; but the approximations that we tried (n-best list crunching, max-rule decoding, minimum Bayes risk) did not appear to help. 3.2 Results Table 2 shows the main results of our experiments. In the second and third line, we see that the forestto-string system outperforms the tree-to-string system by 1.53 Bleu, consistent with previously published results (Mi et al., 2008; Zhang et al., 2009). However, we also find that the trees selected by the forest-to-string system score much lower according to labeled-bracket F1. This suggests that the reason the forest-to-string system is able to generate better translations is that it can soften the constraints imposed by the syntax of the source language. 4.1 Setup MER training with labeled-bracket F1 as an objective function is straightforward. At each iteration of MER training, we run the parser and decoder over the CTB dev set to generate an n-best list of possible translation derivations (Huang and Chiang, 2005). For each derivation, w"
P12-2062,D11-1007,0,\N,Missing
P13-1091,C12-1083,1,0.377003,"on of contextfree grammars, and some important implementation details, resulting in an algorithm that is practical for natural-language applications. The algorithm is part of Bolinas, a new software toolkit for HRG processing. 1 Introduction Hyperedge replacement grammar (HRG) is a context-free rewriting formalism for generating graphs (Drewes et al., 1997), and its synchronous counterpart can be used for transforming graphs to/from other graphs or trees. As such, it has great potential for applications in natural language understanding and generation, and semantics-based machine translation (Jones et al., 2012). Figure 1 shows some examples of graphs for naturallanguage semantics. A polynomial-time recognition algorithm for HRGs was described by Lautemann (1990), building on the work of Rozenberg and Welzl (1986) on boundary node label controlled grammars, and others have presented polynomial-time algorithms as well (Mazanek and Minas, 2008; Moot, 2008). Although Lautemann’s algorithm is correct and tractable, its presentation is prefaced with the remark: “As we are only interested in distinguishing polynomial time from non-polynomial time, the analysis will be rather crude, and implementation detai"
P13-1091,W08-2309,0,0.0254182,"7), and its synchronous counterpart can be used for transforming graphs to/from other graphs or trees. As such, it has great potential for applications in natural language understanding and generation, and semantics-based machine translation (Jones et al., 2012). Figure 1 shows some examples of graphs for naturallanguage semantics. A polynomial-time recognition algorithm for HRGs was described by Lautemann (1990), building on the work of Rozenberg and Welzl (1986) on boundary node label controlled grammars, and others have presented polynomial-time algorithms as well (Mazanek and Minas, 2008; Moot, 2008). Although Lautemann’s algorithm is correct and tractable, its presentation is prefaced with the remark: “As we are only interested in distinguishing polynomial time from non-polynomial time, the analysis will be rather crude, and implementation details will be explicated as little as possible.” Indeed, the key step of the algorithm, which matches a rule against the input graph, is described at a very high level, so that it is not obvious (for a non-expert in graph algorithms) how to implement it. More importantly, this step as described leads to a time complexity that is polynomial, but poten"
P13-1091,J11-1008,0,\N,Missing
P14-1072,D11-1033,0,0.0260781,"el spoken language, then we should train on spoken language data. If we train on newswire, then a spoken sentence might be regarded as ill-formed, because the distribution of sentences in these two domains are very diﬀerent. In practice, we often have limited-size training data from a specific domain, and large amounts of data consisting of language from a variety of domains (we call this general-domain data). How can we utilize the large general-domain dataset to help us train a model on a specific domain? Many methods (Lin et al., 1997; Gao et al., 2002; Klakow, 2000; Moore and Lewis, 2010; Axelrod et al., 2011) rank sentences in the generaldomain data according to their similarity to the in-domain data and select only those with score higher than some threshold. Such methods are effective and widely used. However, sometimes it is hard to say whether a sentence is totally in-domain or out-of-domain; for example, quoted speech in a news report might be partly in-domain if the domain of interest is broadcast conversation. Here, we propose to assign each sentence a probability to indicate how likely it is to belong to the domain of interest, and train a language model using expected KN smoothing. We sho"
P14-1072,J93-2003,0,0.0460073,"Missing"
P14-1072,N04-1039,0,0.0713946,"cable before: one in language model adaptation, and the other in word alignment. In both cases, our method improves performance significantly. 1 Introduction In speech and language processing, smoothing is essential to reduce overfitting, and Kneser-Ney (KN) smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999) has consistently proven to be among the best-performing and most widely used methods. However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights. Such cases have been noted for language modeling (Goodman, 2001; Goodman, 2004), domain adaptation (Tam and Schultz, 2008), grapheme-tophoneme conversion (Bisani and Ney, 2008), and phrase-based translation (Andr´es-Ferrer, 2010; Wuebker et al., 2012). For example, in Expectation-Maximization (Dempster et al., 1977), the Expectation (E) step computes the posterior distribution over possible completions of the data, and the Maximization (M) step reestimates the model parameters as 2 Smoothing on integral counts Before presenting our method, we review KN smoothing on integer counts as applied to language models, although, as we will demonstrate in Section 7, KN smoothing i"
P14-1072,W12-3158,0,0.152053,"ch and language processing, smoothing is essential to reduce overfitting, and Kneser-Ney (KN) smoothing (Kneser and Ney, 1995; Chen and Goodman, 1999) has consistently proven to be among the best-performing and most widely used methods. However, KN smoothing assumes integer counts, whereas in many NLP tasks, training instances appear with possibly fractional weights. Such cases have been noted for language modeling (Goodman, 2001; Goodman, 2004), domain adaptation (Tam and Schultz, 2008), grapheme-tophoneme conversion (Bisani and Ney, 2008), and phrase-based translation (Andr´es-Ferrer, 2010; Wuebker et al., 2012). For example, in Expectation-Maximization (Dempster et al., 1977), the Expectation (E) step computes the posterior distribution over possible completions of the data, and the Maximization (M) step reestimates the model parameters as 2 Smoothing on integral counts Before presenting our method, we review KN smoothing on integer counts as applied to language models, although, as we will demonstrate in Section 7, KN smoothing is applicable to other tasks as well. 2.1 Maximum likelihood estimation Let uw stand for an n-gram, where u stands for the (n − 1) context words and w, the predicted word. L"
P14-1072,N03-1017,0,0.0133557,"zero Alignment F1 Ara-Eng Cze-Eng 66.5 67.2 65.7 65.5 60.1 63.7 60.8 66.5 60.8 65.2 67.7 70.2 69.7 71.9 69.4 71.3 69.2 71.9 Bleu Ara-Eng Cze-Eng 37.0 16.6 36.5 16.6 – – 37.8 16.9 – – 37.2 16.5 38.2 17.0 – – – – Table 1: Expected KN (interpolating with the unigram distribution) consistently outperforms all other methods. For variational Bayes, we followed Riley and Gildea (2012) in setting α to zero (so that the choice of p′ is irrelevant). For fractional KN, we chose D to maximize F1 (see Figure 2). and English-to-foreign directions and then used the grow-diag-final method to symmetrize them (Koehn et al., 2003), and evaluated the alignments using F-measure against gold word alignments. As shown in Table 1, for KN smoothing, interpolation with the unigram distribution performs the best, while for WB smoothing, interestingly, interpolation with the uniform distribution performs the best. The diﬀerence can be explained by the way the two smoothing methods estimate p′ . Consider again a training example with a word e that occurs nowhere else in the training data. In WB smoothing, p′ ( f ) is the empirical unigram distribution. If f contains a word that is much more frequent than the correct translation"
P14-1072,P07-2045,0,0.0343305,"Missing"
P14-1072,P10-2041,0,0.288494,"mple, if we want to model spoken language, then we should train on spoken language data. If we train on newswire, then a spoken sentence might be regarded as ill-formed, because the distribution of sentences in these two domains are very diﬀerent. In practice, we often have limited-size training data from a specific domain, and large amounts of data consisting of language from a variety of domains (we call this general-domain data). How can we utilize the large general-domain dataset to help us train a model on a specific domain? Many methods (Lin et al., 1997; Gao et al., 2002; Klakow, 2000; Moore and Lewis, 2010; Axelrod et al., 2011) rank sentences in the generaldomain data according to their similarity to the in-domain data and select only those with score higher than some threshold. Such methods are effective and widely used. However, sometimes it is hard to say whether a sentence is totally in-domain or out-of-domain; for example, quoted speech in a news report might be partly in-domain if the domain of interest is broadcast conversation. Here, we propose to assign each sentence a probability to indicate how likely it is to belong to the domain of interest, and train a language model using expect"
P14-1072,J03-1002,0,0.0374789,"M word alignment models (Brown et al., 1993). This illustrates both how to use expected KN inside EM and how to use it beyond language modeling. Of course, expected KN can be applied to other instances of EM besides word alignment. 7.1 Problem Given a French sentence f = f1 f2 · · · fm and its English translation e = e1 e2 · · · en , an alignment a is a sequence a1 , a2 , . . . , am , where ai is the index of the English word which generates the French word fi , or NULL. As is common, we assume that each French word can only be generated from one English word or from NULL (Brown et al., 1993; Och and Ney, 2003; Vogel et al., 1996). The IBM models and related models define probability distributions p(a, f |e, θ), which model how likely a French sentence f is to be generated from an English sentence e with word alignment a. Diﬀerent models parameterize this probability distribution in diﬀerent ways. For example, Model 1 only models the lexical translation probabilities: p(a, f |e, θ) ∝ m ∏ 7.3 Alignment experiments We modified GIZA++ (Och and Ney, 2003) to perform expected KN smoothing as described above. Smoothing is enabled or disabled with a command-line switch, making direct comparisons simple. O"
P14-1072,P03-1021,0,0.0128624,"Missing"
P14-1072,P12-2060,0,0.0119746,"f and e are observed, while a is a latent variable. Normally, in the E step, we collect expected 771 Smoothing none (baseline) variational Bayes fractional WB fractional KN expected KN p′ – uniform unigram uniform zero unigram unigram uniform zero Alignment F1 Ara-Eng Cze-Eng 66.5 67.2 65.7 65.5 60.1 63.7 60.8 66.5 60.8 65.2 67.7 70.2 69.7 71.9 69.4 71.3 69.2 71.9 Bleu Ara-Eng Cze-Eng 37.0 16.6 36.5 16.6 – – 37.8 16.9 – – 37.2 16.5 38.2 17.0 – – – – Table 1: Expected KN (interpolating with the unigram distribution) consistently outperforms all other methods. For variational Bayes, we followed Riley and Gildea (2012) in setting α to zero (so that the choice of p′ is irrelevant). For fractional KN, we chose D to maximize F1 (see Figure 2). and English-to-foreign directions and then used the grow-diag-final method to symmetrize them (Koehn et al., 2003), and evaluated the alignments using F-measure against gold word alignments. As shown in Table 1, for KN smoothing, interpolation with the unigram distribution performs the best, while for WB smoothing, interestingly, interpolation with the uniform distribution performs the best. The diﬀerence can be explained by the way the two smoothing methods estimate p′"
P14-1072,P06-1124,0,0.0246326,"[n4 ] ≈ 3 − 4Y E[n3 ] Y= The computational complexity of expected KN is almost identical to KN on integral counts. The main addition is computing and storing the count distributions. Using the dynamic program in Section 3.2, computing the distributions for each r is linear in the number of n-gram types, and we only need to compute the distributions up to r = 2 (or r = 4 for modified KN), and store them for r = 0 (or up to r = 2 for modified KN). (13) E[n1 ] . E[n1 ] + 2E[n2 ] 5 Related Work One side-eﬀect of this change is that (6) is no longer the correct solution to the marginal constraint (Teh, 2006; Sundermeyer et al., 2011). Although this problem can be fixed, standard implementations simply use (6). Witten-Bell (WB) smoothing is somewhat easier than KN to adapt to fractional counts. The SRILM toolkit (Stolcke, 2002) implements a method which we call fractional WB: p(w |u) = λ(u)pmle (w |u) + (1 − λ(u))p′ (w |u′ ) E[c(u)] , λ(u) = E[c(u)] + n1+ (u•) 4.5.3 Recursive smoothing In the original KN method, the lower-order model p′ was estimated using (6); recursive KN smoothing applies KN smoothing to p′ . To do this, we need to reconstruct counts whose MLE is (6). On integral counts, this"
P14-1072,C96-2141,0,0.529388,"dels (Brown et al., 1993). This illustrates both how to use expected KN inside EM and how to use it beyond language modeling. Of course, expected KN can be applied to other instances of EM besides word alignment. 7.1 Problem Given a French sentence f = f1 f2 · · · fm and its English translation e = e1 e2 · · · en , an alignment a is a sequence a1 , a2 , . . . , am , where ai is the index of the English word which generates the French word fi , or NULL. As is common, we assume that each French word can only be generated from one English word or from NULL (Brown et al., 1993; Och and Ney, 2003; Vogel et al., 1996). The IBM models and related models define probability distributions p(a, f |e, θ), which model how likely a French sentence f is to be generated from an English sentence e with word alignment a. Diﬀerent models parameterize this probability distribution in diﬀerent ways. For example, Model 1 only models the lexical translation probabilities: p(a, f |e, θ) ∝ m ∏ 7.3 Alignment experiments We modified GIZA++ (Och and Ney, 2003) to perform expected KN smoothing as described above. Smoothing is enabled or disabled with a command-line switch, making direct comparisons simple. Our implementation is"
P14-1072,bojar-prokopova-2006-czech,0,\N,Missing
P17-1177,J07-2003,1,0.560214,"2014b) for a definition. The annotation of each source word xi is obtained by concatenating the forward and backward hidden states: → − ← →  hi  hi = ← − . hi The whole sequence of these annotations is used by the decoder. 2.2 Decoder Model The decoder is a forward GRU predicting the translation y word by word. The probability of generating the j-th word y j is: 3 Tree Structure Enhanced Neural Machine Translation Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al. (2016)) process a sentence only as a sequence of words, and do not explicitly exploit the inherent structure of natural language sentences. In this section, we present models which directly incorporate source syntactic trees into the encoder-decoder framework. 3.1 Preliminaries where GRU(·) is extended to more than two arguments by first concatenating all arguments except the first. The attention mechanism computes the context vector ci as a weighted sum of the source annotations, I X ← → cj = α j,i hi (5) Like Erigu"
P17-1177,W14-4012,0,0.0741933,"Missing"
P17-1177,D14-1179,0,0.0848773,"Missing"
P17-1177,P16-1078,0,0.656145,"017). With explicit syntactic structure, the decoder can generate the translation more in line with the source syntactic structure. For example, when translating the phrase zhu manila dashiguan in Figure 1, the tree structure indicates that zhu ‘in’ and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incorrect translation like “in embassy of manila” 2 . In this paper, we propose a novel encoderdecoder model that makes use of a precomputed source-side syntactic tree in both the encoder and decoder. In the encoder (§3.3), we improve the tree encoder of Eriguchi et al. (2016) by introducing a bidirectional tree encoder. For each source tree node (including the source words), we generate a representation containing information both from below (as with the original bottom-up encoder) and from above (using a top-down encoder). Thus, the annotation of each node summarizes the surrounding sequential context, as well as the entire syntactic context. In the decoder (§3.4), we incorporate source syntactic tree structure into the attention model via an extension of the coverage model of Tu et al. (2016). With this tree-coverage model, we can better guide the generation pha"
P17-1177,C16-1290,0,0.0606452,"inese phrase zhu manila is translated twice because the model attends to the node spanning zhu manila even though both words have already been translated; there is no mechanism to prevent this. (b) + Tree-Coverage Model Figure 4: The attention heapmap plotting the attention weights during different translation steps, for translating the sentence in Figure 1(a). The nodes [7]-[11] correspond to non-leaf nodes indexed in Figure 3. Incorporating Tree-Coverage Model produces more concentrated alignments and alleviates the over-translation problem. Inspired by the approaches of Cohn et al. (2016), Feng et al. (2016), Tu et al. (2016) and Mi et al. (2016), we propose to use prior knowledge to control the attention mechanism. In our case, the prior knowledge is the source syntactic information. In particular, we build our model on top of the word coverage model proposed by Tu et al. (2016), which alleviate the problems of over-translation and under-translation (failing to translate part of a sentence). The word coverage model makes the attention at a given time step j dependent on the attention at previous time steps via coverage vectors: 1940 C j,i = GRU(C j−1,i , α j,i , d j−1 , hi ). (11) Data LDC MT02"
P17-1177,W02-1039,0,0.307969,"onal Linguistics, pages 1936–1945 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1177 use syntactic information to guide its reordering decisions better (especially for language pairs with significant reordering, like Chinese-English). Although the attention model (Bahdanau et al., 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017). With explicit syntactic structure, the decoder can generate the translation more in line with the source syntactic structure. For example, when translating the phrase zhu manila dashiguan in Figure 1, the tree structure indicates that zhu ‘in’ and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incorrect translation like “in embassy of manila” 2 . In this paper, we propose a novel encoderdecoder model that makes use of a precomputed source-side syntactic tree in both the encoder and decoder. In the encoder (§3.3), we improve the t"
P17-1177,N03-1017,0,0.0640388,"nt unit; see the paper by Cho et al. (2014b) for a definition. The annotation of each source word xi is obtained by concatenating the forward and backward hidden states: → − ← →  hi  hi = ← − . hi The whole sequence of these annotations is used by the decoder. 2.2 Decoder Model The decoder is a forward GRU predicting the translation y word by word. The probability of generating the j-th word y j is: 3 Tree Structure Enhanced Neural Machine Translation Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al. (2016)) process a sentence only as a sequence of words, and do not explicitly exploit the inherent structure of natural language sentences. In this section, we present models which directly incorporate source syntactic trees into the encoder-decoder framework. 3.1 Preliminaries where GRU(·) is extended to more than two arguments by first concatenating all arguments except the first. The attention mechanism computes the context vector ci as a weighted sum of the source annotations, I X"
P17-1177,E17-2093,0,0.0513556,"ce the weight matrices W r , U r , W z , U z , W and U in the standard GRU with PrD , QrD , PzD , QzD , PD , and QD , respectively. The subscript D is either L or R depending on whether node k is a left or right child, respectively. Finally, the annotation of each node is obtained by concatenating its bottom-up hidden state and top-down hidden state: hlk  ↑ h  =  k↓  . hk (a) Tree-GRU Encoder This allows the tree structure information flow from the root to the leaves (words). Thus, all the annotations are based on the full context of word sequence and syntactic tree structure. Kokkinos and Potamianos (2017) propose a similar bidirectional Tree-GRU for sentiment analysis, which differs from ours in several respects: in the bottom-up encoder, we use separate reset/update gates for left and right children, analogous to Tree-LSTMs (Tai et al., 2015); in the topdown encoder, we use separate weights for left and right children. Teng and Zhang (2016) also propose a bidirectional Tree-LSTM encoder for classification tasks. They use a more complex head-lexicalization scheme to feed the top-down encoder. We will compare their model with ours in the experiments. 3.4 Tree-Coverage Model We also extend the d"
P17-1177,D15-1278,0,0.0429885,"Missing"
P17-1177,P06-1077,0,0.0158632,"er by Cho et al. (2014b) for a definition. The annotation of each source word xi is obtained by concatenating the forward and backward hidden states: → − ← →  hi  hi = ← − . hi The whole sequence of these annotations is used by the decoder. 2.2 Decoder Model The decoder is a forward GRU predicting the translation y word by word. The probability of generating the j-th word y j is: 3 Tree Structure Enhanced Neural Machine Translation Although syntax has shown its effectiveness in non-neural statistical machine translation (SMT) systems (Yamada and Knight, 2001; Koehn et al., 2003; Liu et al., 2006; Chiang, 2007), most proposed NMT models (a notable exception being that of Eriguchi et al. (2016)) process a sentence only as a sequence of words, and do not explicitly exploit the inherent structure of natural language sentences. In this section, we present models which directly incorporate source syntactic trees into the encoder-decoder framework. 3.1 Preliminaries where GRU(·) is extended to more than two arguments by first concatenating all arguments except the first. The attention mechanism computes the context vector ci as a weighted sum of the source annotations, I X ← → cj = α j,i hi"
P17-1177,D16-1096,0,0.0900282,"hich is especially important for the translation of long sentences. Second, it becomes possible for the decoder to 1936 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1936–1945 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1177 use syntactic information to guide its reordering decisions better (especially for language pairs with significant reordering, like Chinese-English). Although the attention model (Bahdanau et al., 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017). With explicit syntactic structure, the decoder can generate the translation more in line with the source syntactic structure. For example, when translating the phrase zhu manila dashiguan in Figure 1, the tree structure indicates that zhu ‘in’ and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incorrect translation like “in embassy of manila” 2 . In"
P17-1177,P02-1040,0,0.104515,"erage model, respectively. # 120 130 System Seq-LSTM SeqTree-LSTM Coverage no no MT02 34.98 35.28 MT03 32.81 33.56 MT04 34.08 34.94 MT05 31.39 32.64 MT06 28.03 29.26 Average 31.58(+0.82) 32.60(+1.84) Table 3: BLEU scores of different systems based on LSTM. “Seq-LSTM” denotes both the encoder and decoder parts for the sequential model are based on LSTM; “SeqTree-LSTM” means using Tree-LSTM encoder on top of “Seq-LSTM”. We use Adadelta (Zeiler, 2012) for optimization using a mini-batch size of 32. All other settings are the same as in Bahdanau et al. (2015). We use case insensitive 4-gram BLEU (Papineni et al., 2002) for evaluation, as calculated by multi-bleu.perl in the Moses toolkit.7 4.3 Tree Encoders This set of experiments evaluates the effectiveness of our proposed tree encoders. Table 2, row 2 confirms the finding of Eriguchi et al. (2016) that a Tree-LSTM encoder helps, and row 3 shows that our Tree-GRU encoder gets a better result (+0.87 BLEU, v.s. row 2). To verify our assumption that model consistency is important for performance, we also conduct experiments to compare TreeLSTM and Tree-GRU on top of LSTM-based encoder-decoder settings. Tree-Lstm with LSTM based sequential model can obtain 1.0"
P17-1177,N07-1051,0,0.0167327,"Missing"
P17-1177,D16-1159,0,0.0797839,"Missing"
P17-1177,P15-1150,0,0.0925457,"Missing"
P17-1177,P16-1008,0,0.3108,"epresentations, which is especially important for the translation of long sentences. Second, it becomes possible for the decoder to 1936 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1936–1945 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1177 use syntactic information to guide its reordering decisions better (especially for language pairs with significant reordering, like Chinese-English). Although the attention model (Bahdanau et al., 2015) and the coverage model (Tu et al., 2016; Mi et al., 2016) provide effective mechanisms to control the generation of translation, these mechanisms work at the word level and cannot capture phrasal cohesion between the two languages (Fox, 2002; Kim et al., 2017). With explicit syntactic structure, the decoder can generate the translation more in line with the source syntactic structure. For example, when translating the phrase zhu manila dashiguan in Figure 1, the tree structure indicates that zhu ‘in’ and manila form a syntactic unit, so that the model can avoid breaking this unit up to make an incorrect translation like “in embassy"
P17-1177,W09-3825,0,0.044999,"Missing"
P17-1177,P01-1067,0,\N,Missing
P18-1251,E17-1098,1,0.534633,"hash function is simply h(a, c, r1 r2 ) = a + c|Σ|. In more detail, H actually maps from hashes to integers. Clearing H (line 8) actually just increments a counter i; storing a hash k is implemented as H[k] ← i, so we can test whether k is a member by testing whether H[k] = i. An atomic operation (atomicExch) is used to consistently check H since several threads update this variable asynchronously. 4 Experiments We tested the performance of our implementation by constructing several FSTs of varying sizes and comparing our implementation against other baselines. 4.1 Setup In our previous work (Argueta and Chiang, 2017), we created transducers for a toy translation task. We trained a bigram language model (as in Figure 3a) and a one-state translation model (as in Figure 3) with probabilities estimated from 1 https://thrust.github.io/ 1 l 0 ga ta 0.8 a/ e /0 .6 un :th la /1. 0 a:t h e /0 .4 3 un a/ 0. 2 ga 2 (a) 0 /1. ta gata:cat/1 (b) Figure 3: The transducers used for testing were obtained by pre-composing: (a) a language model and (b) a translation model. These two composed together form a transducer that can translate an input sequence from one language (here, Spanish) into another language (here, English"
P18-1251,J97-2003,0,0.378591,"introduce the first (to our knowledge) GPU implementation of the FST composition operation, and we also discuss the optimizations used to achieve the best performance on this architecture. We show that our approach obtains speedups of up to 6× over our serial implementation and 4.5× over OpenFST. 1 Introduction Finite-state transducers (FSTs) and their algorithms (Mohri, 2009) are widely used in speech and language processing for problems such as grapheme-to-phoneme conversion, morphological analysis, part-of-speech tagging, chunking, named entity recognition, and others (Mohri et al., 2002; Mohri, 1997). Hidden Markov models (Baum et al., 1970), conditional random fields (Lafferty et al., 2001) and connectionist temporal classification (Graves et al., 2006) can also be thought of as finite-state transducers. Composition is one of the most important operations on FSTs, because it allows complex FSTs to be built up from many simpler building blocks, but it is also one of the most expensive. Much work has been done on speeding up composition on a single CPU processor (Pereira and Riley, 1997; Hori and Nakamura, 2005; Dixon et al., 2007; Allauzen and Mohri, 2008; Allauzen et al., 2009; Ladner an"
P18-1251,W13-1813,0,0.0356124,"Missing"
P19-1626,E17-1098,1,0.824544,"e` ← atomicMax(C[`][k], e` ) for n ← 1, . . . , N do c0 ← atomicMax(D0 [`][n], y) if c0 < y then y ← c0 syncthreads() d` += exp(C[`][k] − e` ) syncthreads() for n ← 1, . . . , N do x, i ← unpack(D0 [`][n]) D[`][n] ← exp(x)/d` E[`][n] ← i return D imum between it and a second value y. The larger is stored back into D0 [`][n], and the original value of D0 [`][n] is returned as c0 . This operation is performed as one atomic transaction. The following two lines (13-14) set y to the smaller of the two values. Our algorithm recovers the original column indices (m) with a simple extension following Argueta and Chiang (2017). We pack each probability as well as its original column index into a single 64-bit integer before the sorting step (line 5), with the probability in the upper 32 bits and the column index in the lower 32 bits. This representation preserves the ordering of probabilities, so a single atomicMax operation on the packed representation will atomically update both the probability and the index. The final aspect to consider is the configuration of the kernel calls from the host CPU. The grid layout must be configured correctly to use this method. The top-N routine relies on specific ker6219 (a) Tesl"
P19-1626,D18-1332,0,0.0304793,"alized knowledge about the targeted architectures. The disadvantage of hardware agnostic APIs is the lack of optimizations for a set of task-specific functions. Adapting parallel neural operations to a specific hardware platform is required to obtain optimal speed. Since matrix operations are used heavily in deep learning, much research has been done on optimizing them on GPUs (Chetlur et al., 2014; Gupta et al., 2015). Recently, some efforts have been made to other kinds of operations: serial operations running on the GPU (Povey et al., 2016), operations not involving matrix multiplications (Bogoychev et al., 2018), and models using sparse structures (Zhang et al., 2016). In this paper, we focus on sparse operations running exclusively on the GPU architecture. Much recent work in High Performance Computing (HPC) and Natural Language Processing (NLP) focuses on an expensive step of a model or models and optimizes it for a specific architecture. The lookup operation used in the input layer and the softmax function used in the output are two examples seen in machine translation, language modeling, and other tasks. Previous work has accelerated the softmax step by skipping it entirely (Devlin et al., 2014),"
P19-1626,Q16-1026,0,0.0164661,"can be represented as a L× K matrix A whose rows are one-hot vectors, so that the product C = AB is a matrix whose rows are the embeddings of the words in the minibatch. Deep learning toolkits (Neubig et al., 2017; Jia et al., 2014) do not perform a full matrix multiplication; typically, they implement a specialized operation to do this. A problem arises, however, when the input vector is not a one-hot vector, but an “N-hot” vector. For example, we might use additional dimensions of the vector to represent subword or partof-speech tag information (Niehues et al., 2011; Collobert et al., 2011; Chiu and Nichols, 2016). In this case, it would be appropriate to use a sparse matrix library like cuSPARSE, but we show below that we can do better. Beam search and top-N selection Some applications in deep learning require additional computations after the softmax function. During NMT decoding, the top-N probabilities from softmax(z) are chosen at every time-step t and used as an input to the next search step t + 1. It is common practice to obtain the top-N elements after the softmax operation. Naively, we can do this by sorting the probabilities and then taking the first N elements, as shown in Algorithm 1. This"
P19-1626,W14-4012,0,0.0487493,"Missing"
P19-1626,P14-1129,0,0.108508,"Missing"
P19-1626,P14-1020,0,0.0247395,"the first layer of a neural network. We accelerate the first matrix multiplication using batched sparse vectors as input. The second operation is the computation of the softmax used for beam search. We combine the softmax and the top-N selection into one operation obtaining a speedup over a parallel stateof-the-art baseline. We show that our fused topN selection and sparse lookups achieve speedups of 7× and 50× relative to other parallel NVIDIA baselines. 2 Graphics Processing Units GPUs are widely used to accelerate a variety of non-neural tasks such as search (Garcia et al., 2008), parsing (Hall et al., 2014), and sorting (Sintorn and Assarsson, 2008). Applications adapted to the GPU spot different architectural properties of the graphics card to obtain the best performance. This section provides a short overview of the architectural features targeted for this work. 2.1 CUDA execution model CPUs call special functions, also called kernels, to execute a set of instructions in parallel using multiple threads on the GPU. Kernels can be configured to create and execute an arbitrary number of threads. The threads in a kernel are grouped into different thread blocks (also called cooperative thread array"
P19-1626,W18-2714,0,0.0305819,"imizes it for a specific architecture. The lookup operation used in the input layer and the softmax function used in the output are two examples seen in machine translation, language modeling, and other tasks. Previous work has accelerated the softmax step by skipping it entirely (Devlin et al., 2014), or approximating it (Shim et al., 2017; Grave et al., 2017). Another strategy is to fuse multiple tasks into a single step. This approach increases the room for parallelism. Recent efforts have fused the softmax and top-N operations to accelerate beam search on the GPU using similar approaches (Hoang et al., 2018; Milakov and Gimelshein, 2018). Our approach differs from former methods in the following aspects: We deliver a novel method tailored towards scenarios seen in Neural Machine Translation (NMT), we introduce a new GPU-specific method to obtain the top-N elements from a list of hypotheses using a different sorting mechanism, and we introduce a sparse lookup method 6215 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 6215–6224 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics for GPUs. NMT uses beam search d"
P19-1626,W17-3204,0,0.0263205,"parallel. For our experiments, we process 32 (warp size) rows and columns in parallel for the input matrices. We cannot use a stride size larger than 32, since certain GPU architectures do not allow a 2 dimensional block larger than 32 × 32 (or a block containing more than 1024 threads total). Although this method is fairly straightforward, we will see below that it outperforms other methods when N is small, as we expect it to be. 4.2 Fused softmax and top-N The beam size, or top-N, used in NMT is usually small, with the most commonly used values ranging from 1 to 75 (Sutskever et al., 2014; Koehn and Knowles, 2017). Because of this, we base our implementation on insertion sort, which is O(K 2 ), where K is the number of elements to be sorted, but is reasonably efficient for small arrays. It can be easily modified into a top-N selection algorithm that runs in O(KN) time (Algorithm 3). Unlike in6218 Algorithm 2 Sparse matrix multiplication using the CSR format. Input Ar ∈ RL , Ac ∈ RLN , Av ∈ RLN , B ∈ RK×M Output C ∈ RL×M 1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: parfor m ← 1, . . . , M do . Block level parfor ` ← 1, . . . , L do . Block level x←0 k start ← Ar [m] kend ← Ar [m + 1] for k ← k start , . . . , ken"
P19-1626,W11-2124,0,0.037362,"word embeddings. Then, a minibatch of L words can be represented as a L× K matrix A whose rows are one-hot vectors, so that the product C = AB is a matrix whose rows are the embeddings of the words in the minibatch. Deep learning toolkits (Neubig et al., 2017; Jia et al., 2014) do not perform a full matrix multiplication; typically, they implement a specialized operation to do this. A problem arises, however, when the input vector is not a one-hot vector, but an “N-hot” vector. For example, we might use additional dimensions of the vector to represent subword or partof-speech tag information (Niehues et al., 2011; Collobert et al., 2011; Chiu and Nichols, 2016). In this case, it would be appropriate to use a sparse matrix library like cuSPARSE, but we show below that we can do better. Beam search and top-N selection Some applications in deep learning require additional computations after the softmax function. During NMT decoding, the top-N probabilities from softmax(z) are chosen at every time-step t and used as an input to the next search step t + 1. It is common practice to obtain the top-N elements after the softmax operation. Naively, we can do this by sorting the probabilities and then taking the"
W00-1201,A97-1029,1,0.448713,"ic features of the English Treebank) in favor of the ""bigrams on nonterminals"" model. 2.2.2 M o d e l P a r a m e t e r s This section briefly describes the top-level parameters used in the BBN parsing model. We use p to denote the unlexicalized nonterminal corresponding to P in (1), and similarly for li, ri and h. We now present the toplevel generation probabilities, along with examples from Figure 1. For brevity, we omit the smoothing details of BBN's model (see (Miller et al., 1998) for a complete description); we note that all smoothing weights are computed via the technique described in (Bikel et al., 1997). The probability of generating p as the root label is predicted conditioning on only + T O P + , which is the hidden root of all parse trees: P (Pl + T O P + ) , e.g., P ( S I + T O P + ) . 2.2 B B N M o d e l 2.2.1 O v e r v i e w The BBN model is also of the lexicalized PCFG variety. In the BB.N model, as with Model 2 of (Collins, 1997), modifying nonterminals are generated conditioning both on the parent P and its head child H. Unlike Model 2 of (Collins, 1997), they are also generated conditioning on the previously generated modifying nonterminal, L/-1 or Pq-1, (2) The probability of gene"
W00-1201,A88-1019,0,0.231498,"Missing"
W00-1201,P97-1003,0,0.815998,"ch were originally designed for English parsing, on parsing Chinese, using the newly-available Chinese Treebank. We will show that the languagedependent components of these parsers are quite compact, and that with little effort they can be adapted to produce promising results for Chinese parsing. We also discuss directions for future work. 2 Models and Modifications We will briefly describe the two parsing models employed (for a full description of the BBN model, see (Miller et al., 1998) and also (Bikel, 2000); for a full description Of the TAG model, see (Chiang, 2000)). 2.1 M o d e l 2 of (Collins, 1997) Both parsing models discussed in this paper inherit a great deal from this model, so we briefly describe its ""progenitive"" features here, describing only how each of the two models of this paper differ in the subsequent two sections. The lexicalized PCFG that sits behind Model 2 of (Collins, 1997) has rules of the form P ~ LnLn-I""'"" L I H R I "" "" .Rn-IRn (1) S(will-MD) NP(AppI,~NNP) NNP I Apple VP(wilI-MD) MD VP (buy-VB) VB I buy PRT(out-RP) [ RP I out NP(Microsoft--NNP) I NNP I Microsoft Figure 1: A sample sentence with parse tree. where P, Li, R/ and H are all lexicalized nonterminals, and"
W00-1201,J93-2004,0,0.0499324,"ficacy in many tasks, but also to their engineering effiCacy. Many machine learning approaches let the data speak for itself (data ipsa loquuntur), as it were, allowing the modeler to focus on what features of the data are important, rather than on the complicated interaction of such features, as had often been the case with hand-crafted NLP systems. The success of statistical methods in particular has been quite evident in the area of syntactic parsing, most recently with the outstanding results of (Charniak, 2000) and (Colhns, 2000) on the now-standard English test set of the Penn Treebank (Marcus et al., 1993). A significant trend in parsing models has been the incorporation of linguistically-motivated features; however, it is important to note that ""linguistically-motivated"" does not necessarily mean ""language-dependent""---often, it means just the opposite. For example, almost all statistical parsers make use of lexicalized nonterminals in some way, which allows lexical items' indiosyncratic parsing preferences to be modeled, but the paring between head words and their parent nonterminals is determined almost entirely by the training data, thereby making this feature--which models preferences of p"
W00-1201,C92-2065,0,0.035707,"conditioning on the modifier constituent li or r/, the tag of the head constituent, th, and the word of the head constituent, Wh P(tl, [li, th, Wh) and P(tr~ [ri, th, Wh). (7) The head word of the entire sentence, Wh, is predicted conditioning only on the top-most symbol p and th: P(whlth,p). (8) Head words of modifier constituents, w h and Wry, are predicted conditioning on all the context used for predicting parts of speech in (7), as well as the parts of speech themsleves P(wt, [tl,, li, th, Wh) and P(wri [try, ri, th, Wh). TAG Model The model of (Chiang, 2000) is based on stochastic TAG (Resnik, 1992; Schabes, 1992). In this model a parse tree is built up not out of lexicalized phrase-structure rules but by tree fragments (called elementary trees) which are texicalized in the sense that each fragment contains exactly one lexical item (its anchor). In the variant of TAG we use, there are three kinds of elementary tree: initial, (predicative) auxiliary, and modifier, and three composition operations: substitution, adjunction, and sister-adjunction. Figure 2 illustrates all three of these operations, c~i is an initial tree which substitutes at the leftmost node labeled NP$;/~ is an auxiliary"
W00-1201,J94-1004,0,0.015422,"three composition operations: substitution, adjunction, and sister-adjunction. Figure 2 illustrates all three of these operations, c~i is an initial tree which substitutes at the leftmost node labeled NP$;/~ is an auxiliary tree which adjoins at the node labeled VP. See (Joshi and Schabes, 1997) for a more detailed explanation. Sister-adjunction is not a standard TAG operation, but borrowed from D-Tree Grammar (Rainbow et al., 1995). In Figure 2 the modifier tree V is sister adjoined between the nodes labeled VB and NP$. Multiple modifier trees can adjoin at the same place, in the spirit of (Schabes and Shieber, 1994). In stochastic TAG, the probability of generating an elementary tree depends on the elementary tree itself and the elementary tree it attaches to. The parameters are as follows: (9) The original English model also included a word feature to heIp reduce part-of-speech ambiguity for unknown words, but this component of the model was removed for Chinese, as it was language-dependent. The probability of an entire parse tree is the product of the probabilities of generating all of the elements of that parse tree, I + P (NONE I = i = i # 1The hidden nonterminal +BEGIN+ is used to provide a convenie"
W00-1201,A00-2018,0,\N,Missing
W00-1201,C92-2066,0,\N,Missing
W00-1201,W00-1320,1,\N,Missing
W00-1201,P00-1058,1,\N,Missing
W00-1201,P95-1021,0,\N,Missing
W00-2008,P99-1011,1,0.572462,"Missing"
W00-2008,P94-1022,0,0.528315,"Missing"
W00-2008,E93-1045,0,0.08135,"Missing"
W00-2008,P99-1012,1,\N,Missing
W00-2008,P96-1016,0,\N,Missing
W02-2202,P00-1058,1,0.815466,"Pennsylvania 1. Introduction A number of formalisms have been proposed in order to restrict tree adjoining grammar (TAG) to be weakly equivalent to context free grammar (CFG): for example, tree substitution grammar (TSG), tree insertion grammar (TIG), and regular-form TAG (RF-TAG); in the other direction, tree-local multicomponent TAG (TL-MCTAG) has been proposed as an extension to TAG which is weakly equivalent to TAG. These formalisms have been put to use in various applications. For example, Kroch and Joshi (1987) and others use TL-MCTAG for linguistic description; Bod (1992) uses TSG, and Chiang (2000) uses TIG, for statistical parsing; Shieber (1994) proposes to use TSG for generating synchronous TAG derivations for translation, and Dras (1999) uses RF-TAG for the same purpose. Although it is understood that these formalisms are useful because they have greater strong generative capacity (henceforth SGC) than their weakly-equivalent relatives, it is not always made clear what this actually means: sometimes it is understood in terms of phrase structures, sometimes in terms of a broader notion of structural descriptions (so Chomsky (1963)). We take the latter view, and follow Miller (1999) i"
W02-2202,W00-2008,1,0.859325,"Missing"
W02-2202,P99-1011,0,0.013597,"context free grammar (CFG): for example, tree substitution grammar (TSG), tree insertion grammar (TIG), and regular-form TAG (RF-TAG); in the other direction, tree-local multicomponent TAG (TL-MCTAG) has been proposed as an extension to TAG which is weakly equivalent to TAG. These formalisms have been put to use in various applications. For example, Kroch and Joshi (1987) and others use TL-MCTAG for linguistic description; Bod (1992) uses TSG, and Chiang (2000) uses TIG, for statistical parsing; Shieber (1994) proposes to use TSG for generating synchronous TAG derivations for translation, and Dras (1999) uses RF-TAG for the same purpose. Although it is understood that these formalisms are useful because they have greater strong generative capacity (henceforth SGC) than their weakly-equivalent relatives, it is not always made clear what this actually means: sometimes it is understood in terms of phrase structures, sometimes in terms of a broader notion of structural descriptions (so Chomsky (1963)). We take the latter view, and follow Miller (1999) in seeing phrase structures as just one of many possible interpretations of structural descriptions (alongside, for example, dependency structures)"
W02-2202,W00-2015,0,0.0179303,"ous TAG G 0 of order r which generates the language {(w1 · · · w2n : wπ5n (1) · · · wπ5n (2n) ) |wi ∈ {ci }∗ }, where n = 4r + 1 and πkn is as above. This case is more difficult because unlike a CFG nonterminal, a TAG auxiliary tree has a hole in its span created by its foot node. But it can be shown that any elementary tree pair which covers three of the ci , such that none of them are separated by a foot node in either the domain or the range, must cover all the ci except perhaps one. Given our choice of n, this suffices to show that G 0 cannot exist.  18 Proceedings of TAG+6 7. Conclusion Joshi (2000) poses the question, “How much strong generative power can be squeezed out of a formal system without increasing its weak generative power?” The shifting relations between these four formalisms under different interpretations (see Figure 1) show that there is more than one way to answer this question. First, there is more than one way to measure strong generative power. That TIG generates more tree sets than RF-TAG or CL-SCG but fewer indexed string sets demonstrates a point noted by Becker et al. (1992): that SGC (in the sense of phrase structures) and DGC are orthogonal notions. This is beca"
W02-2202,P94-1022,0,0.112748,"s, 1995) is a TAG in which all auxiliary trees are either left or right auxiliary trees, and adjunction is constrained so that: • no left (right) auxiliary tree can be adjoined on any node that is on the spine of a right (left) auxiliary tree, and • no adjunction is permitted on a node that is to the right (left) of the spine of a left (right) auxiliary tree. c 2002 David Chiang. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 11–18. Universit´a di Venezia. 12 Proceedings of TAG+6 Definition 2. We say that a TAG is in regular form (Rogers, 1994) if there exists some partial ordering  over nonterminal symbols such that if β is an auxiliary tree whose root and foot nodes are labeled X, and η is a node labeled Y on β’s spine where adjunction is allowed, then X  Y, and X = Y only if η is a foot node. Thus adjunction at the foot node is allowed freely, adjunction at the middle of a spine is allowed only to a bounded depth, and adjunction at the root is not allowed at all.1 Next we define CL-SCG, first introducing the notion of an indexed string, which we will make use of in several places in this paper. Definition 3. An indexed string i"
W02-2202,J95-4002,0,0.315898,"mal grammars for various applications. string sets CFG = TIG = RF-TAG = CL-SCG TIG tree sets (modulo projection) CFG = RF-TAG = CL-SCG RF-TAG CL-SCG indexed string sets string relations CFG = TIG Figure 1: Summary of results. Edges denote strict inclusion (lower ⊂ higher); = denotes equality. 2. Definitions We assume familiarity with CFGs and TAGs, and proceed to define two restrictions on TAGs: Definition 1. A left (or right) auxiliary tree is an auxiliary tree in which every frontier node to the right (resp., left) of the foot node is labeled with the empty string. A tree insertion grammar (Schabes and Waters, 1995) is a TAG in which all auxiliary trees are either left or right auxiliary trees, and adjunction is constrained so that: • no left (right) auxiliary tree can be adjoined on any node that is on the spine of a right (left) auxiliary tree, and • no adjunction is permitted on a node that is to the right (left) of the spine of a left (right) auxiliary tree. c 2002 David Chiang. Proceedings of the Sixth International Workshop on Tree Adjoining Grammar and Related Frameworks (TAG+6), pp. 11–18. Universit´a di Venezia. 12 Proceedings of TAG+6 Definition 2. We say that a TAG is in regular form (Rogers,"
W04-3302,W01-1807,0,0.0199885,"nsures that in the derivation of a string w, all variables are instantiated only to substrings of w. (The standard definition of RCG does not have this requirement, because its variables range not over strings but pairs of string positions of w. The definition here is closer to that of simple literal movement grammars (Groenink, 1997).) The language defined by an RCG is the set of all strings w such that S (w) is provable, where S is a distinguished start predicate. The class of range-concatenation languages (RCLs) is exactly the set of languages recognizable in deterministic polynomial time (Bertsch and Nederhof, 2001). Moreover, RCL, unlike CFL, is closed under intersection. The proof is very simple: given two grammars, we rename apart their predicate symbols; let S 1 and S 2 be the renamed start symbols and S be a new start symbol, and add the new clause Figure 3: Simplified version of Boullier’s grammar. The function h maps from nouns to the verbs which take them as arguments. These three predicates T, N0 , and V0 , in turn, check that each word is properly connected to the syntactic dependency structure. But as in Brown and Wilson’s pseudoknot analysis, these three predicates rely on nonexistent informa"
W04-3302,E99-1008,0,0.0294489,"n this analysis’ orchestration of its constraints would break down: several verbs might claim a single noun as an argument, or a verb might claim a noun which claims a different verb. S (x) D S 1 (x), S 2 (x). Because the conjunction operator (comma) is part of the formalism, it can be used not only to intersect whole languages, but the yields of subderivations. This means that RCG gives finer control over weak parallelism, allowing us to localize it and use it in concert with other mechanisms in the grammar. The caveat from the previous section still applies, however, as we illustrate below. Boullier (1999) explores possible uses of the extra power of RCG, and applies it to the phenomenon of German scrambling, in which the arguments of a verb cluster may appear in any order (see Figure 4). If we assume that an arbitrary number of verbs is allowed and arbitrary permutations of arguments is allowed, then scrambling can be shown to be beyond the power of linear context-free rewriting systems (Becker et al., 1992). Boullier gives an RCG that he claims models German scrambling (Figure 3). The predicates S, N, and V use intersection to call the predicate T on every word, N0 on every noun, and V0 on ev"
W04-3302,2000.iwpt-1.8,0,0.166868,"in (2), D1 and D2 are correlated only by their yields; they do not directly constrain each other at all. Thus, from the point of view of strong generative capacity, language intersection is better thought of as adding a constraint to the tail end of otherwise independent parallel processes. We call this type of parallelism weak parallelism and argue that for real applications it is easy to overestimate how much control this kind of parallelism offers. We illustrate this in our first example, which uses CFGs for RNA pseudoknots. We then consider the range-concatenation grammar (RCG) formalism (Boullier, 2000), which includes an intersection operation, allowing it to integrate weak parallelism more tightly into the operation of the grammar. However, weak parallelism is still susceptible to the caveat from above, which we illustrate with a second example, an analysis of German scrambling. Finally, we analyze more carefully the different kinds of parallelism available in an RCG and illustrate how they can be combined to model protein β-sheets. 2 Brown and Wilson’s intersected-CFL analysis of RNA pseudoknots Our first example comes from the RNA structure prediction literature. An RNA molecule can be t"
W05-1506,J04-4004,0,0.471205,"t (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005). 2 Previous Work As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming. The simplest method is to abandon dynamic programming and rely on aggressive pruning to maintain tractability, as is used in (Collins, 2000; Bikel, 2004). But this approach is prohibitively slow, and produces rather lowquality k-best lists (see Sec. 5.1.2). Gildea and Jurafsky (2002) described an O(k2 )-overhead extension for the CKY alg"
W05-1506,C00-1011,0,0.0172235,"l the metric will not be compatible with the parsing algorithm, the k-best lists can be used to approximate the full distribution f 0 . A similar situation occurs when the parser can produce multiple derivations that are regarded as equivalent (e.g., multiple lexicalized parse trees corresponding to the same unlexicalized parse tree); if we want the maximum a posteriori parse, we have to sum over equivalent derivations. Again, the equivalence relation will in general not be compatible with the parsing algorithm, so the k-best lists can be used to approximate f 0 , as in Data Oriented Parsing (Bod, 2000) and in speech recognition (Mohri and Riley, 2002). Another instance of this k-best approach is cascaded optimization. NLP systems are often cascades of modules, where we want to optimize the modules’ objective functions jointly. However, often a module is incompatible with the packed representation of the previous module due to factors like non-local dependencies. So we might want to postpone some disambiguation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (Gildea and Jurafsky, 2002; Sutton and McCallum, 2005), information extraction and cor"
W05-1506,A00-2018,0,0.399259,"lgorithm turns out to be a special case of our Algorithm 0 (Sec. 4.1), and is reported to also be prohibitively slow. Since the original design of the algorithm described below, we have become aware of two efforts that are very closely related to ours, one by Jim´enez and Marzal (2000) and another done in parallel to ours by Charniak and Johnson (2005). Jim´enez and Marzal present an algorithm very similar to our Algorithm 3 (Sec. 4.4) while Charniak and Johnson propose using an algorithm similar to our Algorithm 0, but with multiple passes to improve efficiency. They apply this method to the Charniak (2000) parser to get 50-best lists for reranking, yielding an improvement in parsing accuracy. Our work differs from Jim´enez and Marzal’s in the following three respects. First, we formulate the parsing problem in the more general framework of hypergraphs (Klein and Manning, 2001), making it applicable to a very wide variety of parsing algorithms, whereas Jim´enez and Marzal define their algorithm as an extension of CKY, for CFGs in Chomsky Normal Form (CNF) only. This generalization is not only of theoretical importance, but also critical in the application to state-of-theart parsers such as (Coll"
W05-1506,P05-1033,1,0.45832,"solution (Wellner et al., 2004), and formal semantics of TAG (Joshi and Vijay-Shanker, 1999). Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition function (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation. For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other metrics) of the model on the data. Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Ri"
W05-1506,J03-4003,0,0.838742,"for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005). 2 Previous Work As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming. The simplest method is to abandon dynamic programming and rely on aggressive pruning to maintain tractability, as is used in (Collins, 2000; Bikel, 2004). But this approach is prohibitively slow, and produces rather lowquality k-best lists (see Sec. 5.1.2). Gildea and Jurafsky (2002) described an O(k2 )-overhead extension for the CKY algorithm and reimplemented Collins’ Model 1 to obtain k-best"
W05-1506,W01-1812,0,0.700021,"maximize BLEU. For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005). 2 Previous Work As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming. The si"
W05-1506,N04-1022,0,0.0508739,"ter Studies University of Maryland 3161 AV Williams College Park, MD 20742 dchiang@umiacs.umd.edu f 0 to produce a k-best list (the top k candidates under f 0 ), which serves as an approximation to the full set. Then, in the second phase, optimize f over all the analyses in the k-best list. A typical example is discriminative reranking on k-best lists from a generative module, such as (Collins, 2000) for parsing and (Shen et al., 2004) for translation, where the reranking model has nonlocal features that cannot be computed during parsing proper. Another example is minimum-Bayes-risk decoding (Kumar and Byrne, 2004; Goodman, 1998),where, assuming f 0 defines a probability distribution over all candidates, one seeks the candidate with the highest expected score according to an arbitrary metric (e.g., PARSEVAL or BLEU); since in general the metric will not be compatible with the parsing algorithm, the k-best lists can be used to approximate the full distribution f 0 . A similar situation occurs when the parser can produce multiple derivations that are regarded as equivalent (e.g., multiple lexicalized parse trees corresponding to the same unlexicalized parse tree); if we want the maximum a posteriori pars"
W05-1506,J93-2004,0,0.0295307,"wer 10−3 beam and further applied a cell limit of 100,7 but, as we will show below, this has a detrimental effect on the quality of the output. We therefore omit this method from our speed comparisons, and use our implementation of Algorithm 0 (na¨ıve) as the baseline. We implemented our k-best Algorithms 0, 1, and 3 on top of Bikel’s parser and conducted experiments on a 2.4 GHz 64-bit AMD Opteron with 32 GB memory. The program is written in Java 1.5 running on the Sun JVM in server mode with a maximum heap size of 5 GB. For this experiment, we used sections 02–21 of the Penn Treebank (PTB) (Marcus et al., 1993) as the training data and section 23 (2416 sentences) for evaluation, as is now standard. We ran Bikel’s parser using its settings to emulate Model 2 of (Collins, 2003). 5.1.1 Efficiency We tested our algorithms under various conditions. We first did a comparison of the average parsing time per 6 In beam search, or threshold pruning, each cell in the chart (typically containing all the items corresponding to a span [i, j]) is reduced by discarding all items that are worse than β times the score of the best item in the cell. This β is known as the beam width. 7 In this type of pruning, also kno"
W05-1506,P05-1012,0,0.429744,"shortest hyperpath problem and Nielsen et al. (2005) extend it to k shortest hyperpath. Our work differes from (Nielsen et al., 2005) in two aspects. First, we solve the problem of k-best derivations (i.e., trees), not the k-best hyperpaths, although in many cases they coincide (see Sec. 3 for further discussions). Second, their work assumes non-negative costs (or probabilities ≤ 1) so that they can apply Dijkstra-like algorithms. Although generative models, being probabilitybased, do not suffer from this problem, more general models (e.g., log-linear models) may require negative edge costs (McDonald et al., 2005; Taskar et al., 2004). Our work, based on the Viterbi algorithm, is still applicable as long as the hypergraph is acyclic, and is used by McDonald et al. (2005) to get the k-best parses. 3 Formulation Following Klein and Manning (2001), we use weighted directed hypergraphs (Gallo et al., 1993) as an abstraction of the probabilistic parsing problem. Definition 1. An ordered hypergraph (henceforth hypergraph) H is a tuple hV, E, t, Ri, where V is a finite set of vertices, E is a finite set of hyperarcs, and R is the set of weights. Each hyperarc e ∈ E is a triple e = hT (e), h(e), f (e)i, where"
W05-1506,J02-3001,0,0.186032,"k-best lists can be used to approximate f 0 , as in Data Oriented Parsing (Bod, 2000) and in speech recognition (Mohri and Riley, 2002). Another instance of this k-best approach is cascaded optimization. NLP systems are often cascades of modules, where we want to optimize the modules’ objective functions jointly. However, often a module is incompatible with the packed representation of the previous module due to factors like non-local dependencies. So we might want to postpone some disambiguation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (Gildea and Jurafsky, 2002; Sutton and McCallum, 2005), information extraction and coreference resolution (Wellner et al., 2004), and formal semantics of TAG (Joshi and Vijay-Shanker, 1999). Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition function (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation. For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the"
W05-1506,J99-4004,0,0.543574,"anslation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005). 2 Previous Work As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming. The simplest method is to abandon dynamic programming and rely on aggressive pruning to maintain tractability, as is used in (Collins, 2000; Bikel, 2004). But this approach is prohibitively slow, and produces"
W05-1506,J03-1006,0,0.628235,"ers (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In our experiments, we apply the algorithms to the lexicalized PCFG parser of Bikel (2004), which is very similar to Collins’ Model 2 (Collins, 2003), and to a synchronous CFG based machine translation system (Chiang, 2005). 2 Previous Work As pointed out by Charniak and Johnson (2005), the major difficulty in k-best parsing is dynamic programming. The simplest method is to abandon dynamic programming and rely on aggressive pruning to maintain tractability, as is used in (Collins, 2000; Bikel, 2004). But this approach is prohibitively slow, and produces rather lowqualit"
W05-1506,P03-1021,0,0.147152,"uation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (Gildea and Jurafsky, 2002; Sutton and McCallum, 2005), information extraction and coreference resolution (Wellner et al., 2004), and formal semantics of TAG (Joshi and Vijay-Shanker, 1999). Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition function (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation. For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other metrics) of the model on the data. Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and N"
W05-1506,J04-4002,0,0.0505172,"ch (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other metrics) of the model on the data. Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deductive system (Shieber et al., 1995; Goodman, 1999; Nederhof, 2003) falls into this class. In o"
W05-1506,W97-0301,0,0.285053,"model by optimizing some metric incompatible with the packed representation. For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other metrics) of the model on the data. Similarly, Chiang (2005) uses the k-best parsing algorithm described below in a CFG-based log-linear translation model in order to learn feature weights which maximize BLEU. For algorithms whose packed representations are graphs, such as Hidden Markov Models and other finitestate methods, Ratnaparkhi’s MXPARSE parser (Ratnaparkhi, 1997), and many stack-based machine translation decoders (Brown et al., 1995; Och and Ney, 2004), the k-best paths problem is well-studied in both pure algorithmic context (see (Eppstein, 2001) and (Brander and Sinclair, 1995) for surveys) and NLP/Speech community (Mohri, 2002; Mohri and Riley, 2002). This paper, however, aims at the k-best tree algorithms whose packed representations are hypergraphs (Gallo et al., 1993; Klein and Manning, 2001) (equivalently, and/or graphs or packed forests), which includes most parsers and parsing-based MT decoders. Any algorithm expressible as a weighted deducti"
W05-1506,N04-1023,0,0.0458131,"avid A. Smith and Jonathan May for pointing it out. In the actual implementions, an earlier version is used which has the correct behavior. ∗ David Chiang Inst. for Advanced Computer Studies University of Maryland 3161 AV Williams College Park, MD 20742 dchiang@umiacs.umd.edu f 0 to produce a k-best list (the top k candidates under f 0 ), which serves as an approximation to the full set. Then, in the second phase, optimize f over all the analyses in the k-best list. A typical example is discriminative reranking on k-best lists from a generative module, such as (Collins, 2000) for parsing and (Shen et al., 2004) for translation, where the reranking model has nonlocal features that cannot be computed during parsing proper. Another example is minimum-Bayes-risk decoding (Kumar and Byrne, 2004; Goodman, 1998),where, assuming f 0 defines a probability distribution over all candidates, one seeks the candidate with the highest expected score according to an arbitrary metric (e.g., PARSEVAL or BLEU); since in general the metric will not be compatible with the parsing algorithm, the k-best lists can be used to approximate the full distribution f 0 . A similar situation occurs when the parser can produce mult"
W05-1506,W05-0636,0,0.0227026,"approximate f 0 , as in Data Oriented Parsing (Bod, 2000) and in speech recognition (Mohri and Riley, 2002). Another instance of this k-best approach is cascaded optimization. NLP systems are often cascades of modules, where we want to optimize the modules’ objective functions jointly. However, often a module is incompatible with the packed representation of the previous module due to factors like non-local dependencies. So we might want to postpone some disambiguation by propagating k-best lists to subsequent phases, as in joint parsing and semantic role labeling (Gildea and Jurafsky, 2002; Sutton and McCallum, 2005), information extraction and coreference resolution (Wellner et al., 2004), and formal semantics of TAG (Joshi and Vijay-Shanker, 1999). Moreover, much recent work on discriminative training uses k-best lists; they are sometimes used to approximate the normalization constant or partition function (which would otherwise be intractable), or to train a model by optimizing some metric incompatible with the packed representation. For example, Och (2003) shows how to train a log-linear translation model not by maximizing the likelihood of training data, but maximizing the BLEU score (among other met"
W05-1506,W04-3201,0,0.0297109,"blem and Nielsen et al. (2005) extend it to k shortest hyperpath. Our work differes from (Nielsen et al., 2005) in two aspects. First, we solve the problem of k-best derivations (i.e., trees), not the k-best hyperpaths, although in many cases they coincide (see Sec. 3 for further discussions). Second, their work assumes non-negative costs (or probabilities ≤ 1) so that they can apply Dijkstra-like algorithms. Although generative models, being probabilitybased, do not suffer from this problem, more general models (e.g., log-linear models) may require negative edge costs (McDonald et al., 2005; Taskar et al., 2004). Our work, based on the Viterbi algorithm, is still applicable as long as the hypergraph is acyclic, and is used by McDonald et al. (2005) to get the k-best parses. 3 Formulation Following Klein and Manning (2001), we use weighted directed hypergraphs (Gallo et al., 1993) as an abstraction of the probabilistic parsing problem. Definition 1. An ordered hypergraph (henceforth hypergraph) H is a tuple hV, E, t, Ri, where V is a finite set of vertices, E is a finite set of hyperarcs, and R is the set of weights. Each hyperarc e ∈ E is a triple e = hT (e), h(e), f (e)i, where h(e) ∈ V is its head"
W05-1506,P05-1022,0,\N,Missing
W06-1501,C02-1126,1,0.849314,"monolingual parser, is simply set to 1 if an estimate is available for that level, so that it completely overrides the further backed-off models. The initial estimates for the Pt1i are set by hand. The availability of three backoff models makes it easy to specify the initial guesses at an appropriate level of detail: for example, one might give a general probability of some α ¯ mapping to α ¯ 0 using Pt13 , but then make special exceptions for particular lexical anchors using Pt11 or Pt12 . Finally Pt2 is reestimated by EM on some heldout unannotated sentences of L0 , using the same method as Chiang and Bikel (2002) but on the syntactic transfer probabilities instead of the monolingual parsing model. Another difference is that, following Bikel (2004), we do not recalculate the λi at each iteration, but use the initial values throughout. Ps (α, α0 |η, η 0 ) ≈ Ps (α |η)Pt (α0 |α) (5) 5 A Synchronous TSG-SA for Dialectal Arabic Parameter estimation and smoothing Ps and Psa are the parameters of a monolingual TSG+SA and can be learned from a monolingual Just as the probability model discussed in the preceding section factored the rewriting probabilities 5 into three parts, we create a synchronous TSG-SA and"
W06-1501,J94-1004,0,0.050562,"pendency trees for the MSA and LA sentences are isomorphic, as shown in Figure 2. They differ only in the node labels. 4 Model 4.1 The synchronous TSG+SA formalism Our parser (Chiang, 2000) is based on synchronous tree-substitution grammar with sisteradjunction (TSG+SA). Tree-substitution grammar (Schabes, 1990) is TAG without auxiliary trees or adjunction; instead we include a weaker composition operation, sister-adjunction (Rambow et al., 2001), in which an initial tree is inserted between two sister nodes (see Figure 4). We allow multiple sister-adjunctions at the same site, similar to how Schabes and Shieber (1994) allow multiple adjunctions of modifier auxiliary trees. 3 S S NP VP NP NNP VP NP Qintex VBD NNS sold assets ⇒ VP NNP VBD PRT NP Qintex sold RP NNS off assets PRT RP off Figure 4: Sister-adjunction, with inserted material shown with shaded background                 S NPi ↓ 1 S VP VP , V   ‘like’  NP NP↓ 2 V &+' ti ‘like’ NP↓ 1 NP↓ 2                Figure 5: Example elementary tree pair of a synchronous TSG: the SVO transformation (LA on left, MSA on right) 4 hη, η 0 i is Ps (α, α0 |η, η 0 ), and the probability of sister-adjoining hα, α0 i at a sister"
W06-1501,E06-1047,1,0.800615,"while in MSA, the VSO order is more common. Second, we see that the demonstrative determiner follows the noun in LA, but precedes it in MSA. Finally, we see that the negation marker follows the verb in LA, while it precedes the verb in MSA. (Levantine also has other negation markers that precede the verb, as well as the circumfix m- -$.) The two phrase structure trees are shown in Figure 1 in the convention of the Linguistic Data Consortium (Maamouri et al., 2004). Unlike the phrase 2 Related Work This paper is part of a larger investigation into parsing Arabic dialects (Rambow et al., 2005; Chiang et al., 2006). In that investigation, we examined three different approaches: • Sentence transduction, in which a dialect sentence is roughly translated into one or more MSA sentences and then parsed by an MSA parser. • Treebank transduction, in which the MSA treebank is transduced into an approximation of a LA treebank, on which a LA parer is then trained. • Grammar transduction, which is the name given in the overview papers to the approach discussed in this paper. The present paper provides for the first time a complete technical presentation of this approach. Overall, grammar transduction outperformed"
W06-1501,P00-1058,1,0.843629,"us (except for development and testing), nor of a parallel LA-MSA corpus. The approach described in this paper uses a special parameterization of stochastic synchronous TAG (Shieber, 1994) which we call a “hidden TAG model.” This model couples a model of MSA trees, learned from the Arabic Treebank, with a model of MSA-LA translation, which is initialized by hand and then trained in an unsupervised fashion. Parsing new LA sentences then entails simultaneously building a forest of MSA trees and the corresponding forest of LA trees. Our implementation uses an extension of our monolingual parser (Chiang, 2000) based on tree-substitution Abstract This paper discusses a novel probabilistic synchronous TAG formalism, synchronous Tree Substitution Grammar with sister adjunction (TSG+SA). We use it to parse a language for which there is no training data, by leveraging off a second, related language for which there is abundant training data. The grammar for the resource-rich side is automatically extracted from a treebank; the grammar on the resource-poor side and the synchronization are created by handwritten rules. Our approach thus represents a combination of grammar-based and empirical natural langua"
W06-1501,W04-3207,0,0.0319903,"e or more MSA sentences and then parsed by an MSA parser. • Treebank transduction, in which the MSA treebank is transduced into an approximation of a LA treebank, on which a LA parer is then trained. • Grammar transduction, which is the name given in the overview papers to the approach discussed in this paper. The present paper provides for the first time a complete technical presentation of this approach. Overall, grammar transduction outperformed the other two approaches. In other work, there has been a fair amount of interest in parsing one language using another language, see for example (Smith and Smith, 2004; 2 S S NP-TPC VP VP    ) ‘men’i V NEG  NP-SBJ NP-OBJ ‘like’ ti N ‘not’ DET    ‘work’ ‘this’ NEG V NP-SBJ ( &+'    ‘not’ ‘like’ ‘men’ NP-OBJ DET N # * ‘this’ ‘work’ Figure 1: LDC-style left-to-right phrase structure trees for LA (left) and MSA (right) for sentence (1)   !    ) ‘work’ ‘men’ $ &+' ’like’ ‘not’ *    ) ( ‘work’ ‘men’ ‘not’ # ‘this’ ‘like’ ‘this’ Figure 2: Unordered dependency trees for LA (left) and MSA (right) for sentence (1) NP NNP Qintex S NP VP VBD NP PRT NP RP NNS off assets (α3 ) (α4 ) A synchronous TSG+SA"
W06-1501,levy-andrew-2006-tregex,0,0.0122304,"rect) tag. The results are shown in Table 1. The baseline is simply the application of a pure MSA Chiang parser to LA. We see that important improvements are obtained using the lexical mapping. Adding the SVO transformation does not improve the results, but the NEG and BD transformations help slightly, and their effect is (partly) Syntactic Mapping Because of the underlying syntactic similarity between the two varieties of Arabic, we assume that every tree in the MSA grammar extracted from the MSA treebank is also a LA tree. In addition, we define tree transformations in the Tsurgeon package (Levy and Andrew, 2006). These consist of a pattern which matches MSA elementary trees in the extracted grammar, and a transformation which produces a LA elementary tree. We perform the following tree transformations on all elementary trees which match the underlying MSA pattern. Thus, each MSA tree corresponds to at least two LA trees: the original one and the transformed one. If several transformations apply, we obtain multiple transformed trees. • Negation (NEG): we insert a $ negation marker immediately following each verb. 6                S S VP N PRP$ VP S0 ↓ 1  ‘want’  , V S0 ↓ 1 ' ‘"
W06-1501,W00-1307,0,0.0330982,"ubject-VerbObject (SVO) constructions occur in MSA and LA treebanks. But pure VSO constructions (without pro-drop) occur in the LA corpus only 10ordering in MSA. Hence, the goal is to skew the distributions of the SVO constructions in the MSA data. Therefore, VSO constructions are replicated and converted to SVO constructions. One possible resulting pair of trees is shown in Figure 5. • Ps and Psa are the parameters of a monolingual TSG+SA for MSA. We extract a grammar for the resource-rich language (MSA) from the Penn Arabic Treebank in a process described by Chiang and others (Chiang, 2000; Xia et al., 2000; Chen, 2001). • For the lexical transfer model Pt2 , we create by hand a probabilistic mapping between (word, POS tag) pairs in the two languages. • For the syntactic transfer model P t1 , we created by hand a grammar for the resource-poor language and a mapping between elementary trees in the two grammars, along with initial guesses for the mapping probabilities. • The bd construction (BD): bd is a LA noun that means ‘want’. It acts like a verb in verbal constructions yielding VP constructions headed by NN. It is typically followed by an enclitic possessive pronoun. Accordingly, we defined a"
W06-1501,maamouri-etal-2006-developing,1,0.825899,"shown in Figure 6. We discuss the hand-crafted lexicon and synchronous grammar in the following subsections. 5.1 Lexical Mapping We used a small, hand-crafted lexicon of 100 words which mapped all LA function words and some of the most common open-class words to MSA. We assigned uniform probabilities to the mapping. All other MSA words were assumed to also be LA words. Unknown LA words were handled using the standard unknown word mechanism. 5.2 6 Experimental Results While our approach does not rely on any annotated corpus for LA, nor on a parallel corpus MSALA, we use a small treebank of LA (Maamouri et al., 2006) to analyze and test our approach. The LA treebank is divided into a development corpus and a test corpus, each about 11,000 tokens (using the same tokenization scheme as employed in the MSA treebank). We first use the development corpus to determine which of the transformations are useful. We use two conditions. In the first, the input text is not tagged, and the parser hypothesizes tags. In the second, the input text is tagged with the gold (correct) tag. The results are shown in Table 1. The baseline is simply the application of a pure MSA Chiang parser to LA. We see that important improvem"
W06-1501,J01-1004,1,\N,Missing
W06-1504,W04-3307,0,0.0134534,"odel even some very simple RNA secondary structures. Therefore they propose a more powerful version of linear TAGs, extended simple linear TAGs (ESL-TAGs), which generate a class of languages that include the context-free languages and can be recognized in O(n5 ) time. Satta and Schuler (1998), working within the application domain of natural language syntax, define another restriction on TAG which is also recognizable in O(n5 ) time. Despite being less powerful than full TAG, it is still able to generate languages like the copy language {ww} and Dutch cross-serial dependencies (Joshi, 1985). Kato et al. (2004) conjecture that this restricted TAG is in fact equivalent to ESL-TAG. In this paper we prove their conjecture, and also prove that adding substitution to ESL-TAG does not increase its weak generative capacity, whereas adding substitution to SL-TAG makes it weakly equivalent to ESL-TAG. Thus these four for2 Definitions We assume a standard definition of TAG, with or without substitution, in which adjunction is not allowed at foot nodes, and other nodes can have noadjunction (NA) constraints, obligatory-adjunction (OA), or selective-adjunction constraints. We use the symbols η, η1 , η2 , etc. t"
W06-1504,P94-1022,0,0.0545203,"itself. The segment of a tree from η1 to η2 (where η1 dominates η2 ) is the set of all nodes in the subtree of η 1 but not in the subtree of η2 . A segment can be excised, which means removing the nodes of the segment and making η2 replace η1 as the child of its parent. We also assume a standard definition of TAG derivation trees. We use the symbols h, h 1 , h2 , etc. to range over nodes of derivation trees. The sub1 Adjunction at root and foot nodes is another operation that by itself will not take a formalism beyond context-free power, a fact which is exploited in Rogers’ regular-form TAG (Rogers, 1994). But allowing this in a linear TAG would circumvent the linearity constraint. ∗ This research was primarily carried out while the author was at the University of Pennsylvania. 25 Proceedings of the 8th International Workshop on Tree Adjoining Grammar and Related Formalisms, pages 25–32, c Sydney, July 2006. 2006 Association for Computational Linguistics derivation of h is the subtree of h in the derivation tree. When we cut up derivations into subderivations or segments and recombine them, the edge labels (indicating addresses of adjunctions and substitutions) stay with the node above, not th"
W06-1504,P98-2192,0,0.02795,"f languages incommensurate with the context-free languages, and can be recognized in O(n 4 ) time. Working within the application domain of modeling of RNA secondary structures, they find that SL-TAGs are too restrictive—they can model RNA pseudoknots but because they cannot generate all the context-free languages, they cannot model even some very simple RNA secondary structures. Therefore they propose a more powerful version of linear TAGs, extended simple linear TAGs (ESL-TAGs), which generate a class of languages that include the context-free languages and can be recognized in O(n5 ) time. Satta and Schuler (1998), working within the application domain of natural language syntax, define another restriction on TAG which is also recognizable in O(n5 ) time. Despite being less powerful than full TAG, it is still able to generate languages like the copy language {ww} and Dutch cross-serial dependencies (Joshi, 1985). Kato et al. (2004) conjecture that this restricted TAG is in fact equivalent to ESL-TAG. In this paper we prove their conjecture, and also prove that adding substitution to ESL-TAG does not increase its weak generative capacity, whereas adding substitution to SL-TAG makes it weakly equivalent"
W06-1504,J95-4002,0,0.268675,"roperties We now review several old results and prove a few new results relating the weak generative capacity of these formalisms to one another and to (linear) CFG and TAG. These results are summarized in Figure 1. 3.1 Weak equivalence L(SSL-TAG) ⊆ L(SL-TAG + substitution): We deal first with the left and right auxiliary trees, and then with off-spine adjunction. First, we eliminate the left and right auxiliary trees. Since these only insert material to the left or right of a node, just as in tree-insertion grammars (TIGs), we may apply the conversion from TIGs to tree-substitution grammars (Schabes and Waters, 1995), used in the proof of the context-freeness of Previous results Proposition 1 (Uemura et al. 1999). Linear CFL ( SL-TAL 26 .. . X .. . (Step 1a) ⇒ X∗ Y ⇒ LX ↓ X NA .. . X NA RX ↓ .. . X∗ ⇒ .. . X NA LX ↓ X NA RX ↓ .. . RX Y Y LX X Y .. . X NA RX X (Step 1b) .. . X .. . .. . X NA RX ↓ LX LX ↓ Y Y Figure 2: Elimination of left/right auxiliary trees. TIG.2 (Step 1a) For each active node X that is not the root of a left or right auxiliary tree, we create four copies of the containing elementary tree with X altered in the following ways: first, leave X unchanged; then, add a copy of X above it, mak"
W06-1504,C98-2187,0,\N,Missing
W06-1504,J97-3002,0,\N,Missing
W08-2303,W06-1509,1,0.525871,"ee-locality of an otherwise non-local derivation. First, it has been employed in (Joshi et al., 2003) to derive quantifier-scope restrictions in nested quantifications such as: (1) Two politicians spy on someone from every city. (Joshi et al., 2003, ex. (6)) Tatjana Scheffler Department of Linguistics 619 Williams Hall University of Pennsylvania Philadelphia, PA 19104-6305 USA tatjana@ling.upenn.edu Other applications of flexible composition include the modelling of complex noun phrases in pied-piping and stranding of wh-phrases (Kallmeyer and Scheffler, 2004), an analysis of anaphor binding (Ryant and Scheffler, 2006), discourse semantics (Forbes-Riley et al., 2006), and scrambling patterns (Chen-Main and Joshi, 2007). With the proposal of unification-based semantics for TAG, noun phrase quantifiers have been analysed as multi-component sets, where one component is the lexical quantifier and the other is just an S-node carrying the scopal information for the quantifier. But this kind of analysis can be problematic for tree-local MCTAG, since the two components will in general attach to different elementary trees. For example, see Figure 2a for the sentence (2) Whom does John like a picture of? (Kallmeyer a"
W08-2303,P99-1011,0,0.0712118,"Missing"
W08-2303,W04-3305,1,0.939125,"analyses of various linguistic phenomena in order to preserve tree-locality of an otherwise non-local derivation. First, it has been employed in (Joshi et al., 2003) to derive quantifier-scope restrictions in nested quantifications such as: (1) Two politicians spy on someone from every city. (Joshi et al., 2003, ex. (6)) Tatjana Scheffler Department of Linguistics 619 Williams Hall University of Pennsylvania Philadelphia, PA 19104-6305 USA tatjana@ling.upenn.edu Other applications of flexible composition include the modelling of complex noun phrases in pied-piping and stranding of wh-phrases (Kallmeyer and Scheffler, 2004), an analysis of anaphor binding (Ryant and Scheffler, 2006), discourse semantics (Forbes-Riley et al., 2006), and scrambling patterns (Chen-Main and Joshi, 2007). With the proposal of unification-based semantics for TAG, noun phrase quantifiers have been analysed as multi-component sets, where one component is the lexical quantifier and the other is just an S-node carrying the scopal information for the quantifier. But this kind of analysis can be problematic for tree-local MCTAG, since the two components will in general attach to different elementary trees. For example, see Figure 2a for the"
W08-2303,W04-3324,0,\N,Missing
W08-2303,W02-2217,0,\N,Missing
W17-0123,D16-1263,0,0.23046,"Missing"
W17-0123,D16-1133,1,0.903874,"d corpus interpretable for future studies. New technologies are being developed to facilitate collection of translations (Bird et al., 2014), and there already exist recent examples of parallel speech collection efforts focused on endangered languages (Blachon et al., 2016; Adda et al., 2016). 2 Methodology As a proof-of-concept, we work on the language pair Griko-Italian, for which there exists a sentence-aligned parallel corpus of source-language speech and target-language text (Lekakou et al., 2013). Griko is an endangered minority language spoken in the south of Italy. Using the method of Anastasopoulos et al. (2016), we also obtain speech-to-translation word-level alignments. The corpus that we work on already provides gold-standard transcriptions and speech-totranslation alignments, so it is suitable for conducting a case study that will examine the potential effect of providing the alignments on the crowdsourced transcriptions, as we will be able to compare directly against the gold standard. 170 Proceedings of the 2nd Workshop on the Use of Computational Methods in the Study of Endangered Languages, pages 170–178, c Honolulu, Hawai‘i, USA, March 6–7, 2017. 2017 Association for Computational Linguistic"
W17-0123,C14-1096,0,0.0296799,"ranscribing it, often at a phonetic level, as most of these languages do not have a writing system. This, however, is a costly and slow process, as it could take up to 1 hour for a trained linguist to transcribe the phonemes of 1 minute of speech (Thi-Ngoc-Diep Do and Castelli, 2014). Therefore, speech is more likely to be annotated with translations than with transcriptions. This translated speech is a potentially valuable source of information as it will make the collected corpus interpretable for future studies. New technologies are being developed to facilitate collection of translations (Bird et al., 2014), and there already exist recent examples of parallel speech collection efforts focused on endangered languages (Blachon et al., 2016; Adda et al., 2016). 2 Methodology As a proof-of-concept, we work on the language pair Griko-Italian, for which there exists a sentence-aligned parallel corpus of source-language speech and target-language text (Lekakou et al., 2013). Griko is an endangered minority language spoken in the south of Italy. Using the method of Anastasopoulos et al. (2016), we also obtain speech-to-translation word-level alignments. The corpus that we work on already provides gold-s"
W17-0123,N16-1109,1,0.888718,"Missing"
W17-4607,D16-1263,0,0.0546957,"Missing"
W17-4607,D16-1133,1,0.879375,"Lopez♦ David Chiang♠ ♠ Department of Computer Science and Engineering, University of Notre Dame ♦ School of Informatics, University of Edinburgh Abstract native speakers themselves (Bird et al., 2014; Blachon et al., 2016; Adda et al., 2016). Nevertheless, even translation takes time and language knowledge, so there may still be little translated data relative to the amount of recorded audio. An important goal, then, is to bootstrap language technology from this small parallel corpus in order to provide tools to annotate more data or make the data more searchable. We build on the approach of Anastasopoulos et al. (2016), who developed a system that performs joint inference to identify recurring segments of audio and cluster them while aligning them to words in a text translation. Here, we extend the method to be able to search for new instances of the latent clusters within the unlabeled audio, effectively providing keyword translations for some of the unlabeled speech. We evaluate our method on a Spanish-English corpus used in previous work, and on two datasets from endangered languages (narratives in Arapaho and Ainu). No previous computational methods have been tested on the latter data, to our knowledge."
W18-6322,W17-3204,0,0.481709,"l machine translation (NMT) systems continue to be plagued by a number of problems. We focus on two here: the beam problem and the brevity problem. First, machine translation systems rely on heuristics to search through the intractably large space of possible translations. Most commonly, beam search is used during the decoding process. Traditional statistical machine translation systems often rely on large beams to find good translations. However, in neural machine translation, increasing the beam size has been shown to degrade performance. This is the last of the six challenges identified by Koehn and Knowles (2017). The second problem, noted by several authors, is that NMT tends to generate translations that are too short. Jean et al. (2015) and Koehn and Knowles address this by dividing translation scores by their length, inspired by work on audio chords (Boulanger-Lewandowski et al., 2013). A similar method is also used by Google’s production system (Wu et al., 2016). A third simple method used by various authors (Och and Ney, 2002; He et al., 2016; Neubig, 2016) is a tunable 2 Problem Current neural machine translation models are examples of locally normalized models, which estimate the probability o"
W18-6322,P16-1231,0,0.0420106,"ch time step, a “budget” for the total remaining log-probability. In this example sentence, “The British women won Olymp ic gold in p airs row ing,” the empty translation has initial position 622 in the beam. Already by the third step of decoding, the correct translation has a lower score than the empty translation. However, using greedy search, a nonempty translation would be returned. 3 exp s0 (e∗ ) L = − log P 0 e exp s (e) where e∗ is the reference translation. However, optimizing this is expensive, as it requires performing inference on every training example or heuristic approximations (Andor et al., 2016; Shen et al., 2016). Alternatively, we can adopt a two-tiered model, familiar from phrase-based translation (Och and Ney, 2002), first training s and then training s0 while keeping the parameters of s fixed, possibly on a smaller dataset. A variety of methods, like minimum error rate training (Och, 2003; He et al., 2016), are possible, but keeping with the globallynormalized negative log-likelihood, we obtain, for the constant word reward, the gradient: Correcting Length To address the brevity problem, many designers of NMT systems add corrections to the model. These corrections are often pre"
W18-6322,W16-4610,0,0.0141081,"translation, increasing the beam size has been shown to degrade performance. This is the last of the six challenges identified by Koehn and Knowles (2017). The second problem, noted by several authors, is that NMT tends to generate translations that are too short. Jean et al. (2015) and Koehn and Knowles address this by dividing translation scores by their length, inspired by work on audio chords (Boulanger-Lewandowski et al., 2013). A similar method is also used by Google’s production system (Wu et al., 2016). A third simple method used by various authors (Och and Ney, 2002; He et al., 2016; Neubig, 2016) is a tunable 2 Problem Current neural machine translation models are examples of locally normalized models, which estimate the probability of generating an output sequence e = e1:m as P(e1:m ) = m Y i=1 P(ei |e1:i−1 ). For any partial output sequence e1:i , let us call P(e0 |e1:i ), where e0 ranges over all possible completions of e1:i , the suffix distribution of e1:i . The suffix distribution must sum to one, so if the model overestimates P(e1:i ), there is no way for the suffix distribution to downgrade it. This is known as label bias (Bottou, 1991; Lafferty et al., 2001). 212 Proceedings"
W18-6322,P03-1021,0,0.0916319,"on. However, using greedy search, a nonempty translation would be returned. 3 exp s0 (e∗ ) L = − log P 0 e exp s (e) where e∗ is the reference translation. However, optimizing this is expensive, as it requires performing inference on every training example or heuristic approximations (Andor et al., 2016; Shen et al., 2016). Alternatively, we can adopt a two-tiered model, familiar from phrase-based translation (Och and Ney, 2002), first training s and then training s0 while keeping the parameters of s fixed, possibly on a smaller dataset. A variety of methods, like minimum error rate training (Och, 2003; He et al., 2016), are possible, but keeping with the globallynormalized negative log-likelihood, we obtain, for the constant word reward, the gradient: Correcting Length To address the brevity problem, many designers of NMT systems add corrections to the model. These corrections are often presented as modifications to the search procedure. But, in our view, the brevity problem is essentially a modeling problem, and these corrections should be seen as modifications to the model (Section 3.1). Furthermore, since the root of the problem is local normalization, our view is that these modificatio"
W18-6322,P02-1038,0,0.231928,"lations. However, in neural machine translation, increasing the beam size has been shown to degrade performance. This is the last of the six challenges identified by Koehn and Knowles (2017). The second problem, noted by several authors, is that NMT tends to generate translations that are too short. Jean et al. (2015) and Koehn and Knowles address this by dividing translation scores by their length, inspired by work on audio chords (Boulanger-Lewandowski et al., 2013). A similar method is also used by Google’s production system (Wu et al., 2016). A third simple method used by various authors (Och and Ney, 2002; He et al., 2016; Neubig, 2016) is a tunable 2 Problem Current neural machine translation models are examples of locally normalized models, which estimate the probability of generating an output sequence e = e1:m as P(e1:m ) = m Y i=1 P(ei |e1:i−1 ). For any partial output sequence e1:i , let us call P(e0 |e1:i ), where e0 ranges over all possible completions of e1:i , the suffix distribution of e1:i . The suffix distribution must sum to one, so if the model overestimates P(e1:i ), there is no way for the suffix distribution to downgrade it. This is known as label bias (Bottou, 1991; Lafferty"
W18-6322,P02-1040,0,0.100737,"d on the task, language pair, training data size, as well as the beam size. These values can affect performance strongly. 4.1 4.2 Data and settings Solving the length problem solves the beam problem Here, we first show that the beam problem is indeed the brevity problem. We then demonstrate that solving the length problem does solve the beam problem. Tables 1, 2, and 3 show the results of our German–English, Russian–English, and French–English systems respectively. Each table looks at the impact on BLEU, METEOR, and the ratio of the lengths of generated sentences compared to the gold lengths (Papineni et al., 2002; Denkowski and Lavie, 2014). The baseline method is a standard model without any length correction. The reward method is the tuned constant word reward discussed in the previous section. Norm refers to the normalization method, where a hypothesis’ score is divided by its length. Most of the experimental settings below follow the recommendations of Denkowski and Neubig (2017). Our high-resource, German–English data is from the 2016 WMT shared task (Bojar et al., 2016). We use a bidirectional encoder-decoder model with attention (Bahdanau et al., 2015).1 Our word representation layer has 512 hi"
W18-6322,W14-3348,0,0.0185007,"pair, training data size, as well as the beam size. These values can affect performance strongly. 4.1 4.2 Data and settings Solving the length problem solves the beam problem Here, we first show that the beam problem is indeed the brevity problem. We then demonstrate that solving the length problem does solve the beam problem. Tables 1, 2, and 3 show the results of our German–English, Russian–English, and French–English systems respectively. Each table looks at the impact on BLEU, METEOR, and the ratio of the lengths of generated sentences compared to the gold lengths (Papineni et al., 2002; Denkowski and Lavie, 2014). The baseline method is a standard model without any length correction. The reward method is the tuned constant word reward discussed in the previous section. Norm refers to the normalization method, where a hypothesis’ score is divided by its length. Most of the experimental settings below follow the recommendations of Denkowski and Neubig (2017). Our high-resource, German–English data is from the 2016 WMT shared task (Bojar et al., 2016). We use a bidirectional encoder-decoder model with attention (Bahdanau et al., 2015).1 Our word representation layer has 512 hidden units, while other hidd"
W18-6322,P16-1162,0,0.0306973,"thod, where a hypothesis’ score is divided by its length. Most of the experimental settings below follow the recommendations of Denkowski and Neubig (2017). Our high-resource, German–English data is from the 2016 WMT shared task (Bojar et al., 2016). We use a bidirectional encoder-decoder model with attention (Bahdanau et al., 2015).1 Our word representation layer has 512 hidden units, while other hidden layers have 1024 nodes. Our model is trained using Adam with a learning rate of 0.0002. We use 32k byte-pair encoding (BPE) operations learned on the combined source and target training data (Sennrich et al., 2016). We train on minibatches of size 2012 words and validate every 100k sentences, selecting the final model based on development perplexity. Our medium-resource, Russian–English system uses data from the 2017 WMT translation task, which consists of roughly 1 million training sentences (Bojar et al., 2017). We use the same architecture as our German–English system, but only have 512 nodes in all layers. We use 16k BPE operations and dropout of 0.2. We train on mini4.2.1 Baseline The top sections of Tables 1, 2, 3 illustrate the brevity and beam problems in the baseline models. As beam size increa"
W18-6322,W17-3203,0,0.019283,"show the results of our German–English, Russian–English, and French–English systems respectively. Each table looks at the impact on BLEU, METEOR, and the ratio of the lengths of generated sentences compared to the gold lengths (Papineni et al., 2002; Denkowski and Lavie, 2014). The baseline method is a standard model without any length correction. The reward method is the tuned constant word reward discussed in the previous section. Norm refers to the normalization method, where a hypothesis’ score is divided by its length. Most of the experimental settings below follow the recommendations of Denkowski and Neubig (2017). Our high-resource, German–English data is from the 2016 WMT shared task (Bojar et al., 2016). We use a bidirectional encoder-decoder model with attention (Bahdanau et al., 2015).1 Our word representation layer has 512 hidden units, while other hidden layers have 1024 nodes. Our model is trained using Adam with a learning rate of 0.0002. We use 32k byte-pair encoding (BPE) operations learned on the combined source and target training data (Sennrich et al., 2016). We train on minibatches of size 2012 words and validate every 100k sentences, selecting the final model based on development perple"
W18-6322,P16-1159,0,0.314488,"get” for the total remaining log-probability. In this example sentence, “The British women won Olymp ic gold in p airs row ing,” the empty translation has initial position 622 in the beam. Already by the third step of decoding, the correct translation has a lower score than the empty translation. However, using greedy search, a nonempty translation would be returned. 3 exp s0 (e∗ ) L = − log P 0 e exp s (e) where e∗ is the reference translation. However, optimizing this is expensive, as it requires performing inference on every training example or heuristic approximations (Andor et al., 2016; Shen et al., 2016). Alternatively, we can adopt a two-tiered model, familiar from phrase-based translation (Och and Ney, 2002), first training s and then training s0 while keeping the parameters of s fixed, possibly on a smaller dataset. A variety of methods, like minimum error rate training (Och, 2003; He et al., 2016), are possible, but keeping with the globallynormalized negative log-likelihood, we obtain, for the constant word reward, the gradient: Correcting Length To address the brevity problem, many designers of NMT systems add corrections to the model. These corrections are often presented as modificati"
W18-6322,D17-1227,0,0.0931315,"Missing"
W18-6322,D16-1137,0,0.0675763,"the brevity problem, which results from the locally-normalized structure of the model. We compared two corrections to the model and introduced a method to learn the parameters of these corrections. Because this method is helpful and easy, we hope to see it included to make stronger baseline NMT systems. We have argued that the brevity problem is an example of label bias, and that the solution is a very limited form of globally-normalized model. These can be seen as the simplest case of the more general problem of label bias and the more general solution of globally-normalized models for NMT (Wiseman and Rush, 2016; Venkatraman et al., 2015; Ranzato et al., 2015; Shen et al., 2016). Some questions for future research are: Word reward vs. length normalization Tuning the word reward score generally had higher METEOR scores than length normalization across all of our settings. With BLEU, length normalization beat the word reward on GermanEnglish and French–English, but tied on EnglishFrench and lost on Russian–English. For the largest beam of 1000, the tuned word reward had a higher BLEU than length normalization. Overall, the two methods have relatively similar performance, but the tuned word reward has t"
W18-6322,W15-3014,0,0.271627,"ity problem. First, machine translation systems rely on heuristics to search through the intractably large space of possible translations. Most commonly, beam search is used during the decoding process. Traditional statistical machine translation systems often rely on large beams to find good translations. However, in neural machine translation, increasing the beam size has been shown to degrade performance. This is the last of the six challenges identified by Koehn and Knowles (2017). The second problem, noted by several authors, is that NMT tends to generate translations that are too short. Jean et al. (2015) and Koehn and Knowles address this by dividing translation scores by their length, inspired by work on audio chords (Boulanger-Lewandowski et al., 2013). A similar method is also used by Google’s production system (Wu et al., 2016). A third simple method used by various authors (Och and Ney, 2002; He et al., 2016; Neubig, 2016) is a tunable 2 Problem Current neural machine translation models are examples of locally normalized models, which estimate the probability of generating an output sequence e = e1:m as P(e1:m ) = m Y i=1 P(ei |e1:i−1 ). For any partial output sequence e1:i , let us call"
W18-6322,D18-1342,0,0.266066,"Missing"
W18-6322,C12-2136,0,0.0773961,"Missing"
W18-6322,2010.iwslt-evaluation.1,0,\N,Missing
