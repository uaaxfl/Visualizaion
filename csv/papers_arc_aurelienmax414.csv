W18-3814,Construction of a Multilingual Corpus Annotated with Translation Relations,2018,-1,-1,2,0,17661,yuming zhai,Proceedings of the First Workshop on Linguistic Resources for Natural Language Processing,0,"Translation relations, which distinguish literal translation from other translation techniques, constitute an important subject of study for human translators (Chuquet and Paillard, 1989). However, automatic processing techniques based on interlingual relations, such as machine translation or paraphrase generation exploiting translational equivalence, have not exploited these relations explicitly until now. In this work, we present a categorisation of translation relations and annotate them in a parallel multilingual (English, French, Chinese) corpus of oral presentations, the TED Talks. Our long term objective will be to automatically detect these relations in order to integrate them as important characteristics for the search of monolingual segments in relation of equivalence (paraphrases) or of entailment. The annotated corpus resulting from our work will be made available to the community."
W16-2337,{LIMSI}{'}s Contribution to the {WMT}{'}16 Biomedical Translation Task,2016,30,3,2,0,10027,julia ive,"Proceedings of the First Conference on Machine Translation: Volume 2, Shared Task Papers",0,"The article describes LIMSIxe2x80x99s submission to the first WMTxe2x80x9916 shared biomedical translation task, focusing on the sole English-French translation direction. Our main submission is the output of a MOSES-based statistical machine translation (SMT) system, rescored with Structured OUtput Layer (SOUL) neural network models. We also present an attempt to circumvent syntactic complexity: our proposal combines the outputs of PBSMT systems trained either to translate entire source sentences or specific syntactic constructs extracted from those sentences. The approach is implemented using Confusion Network (CN) decoding. The quality of the combined output is comparable to the quality of our main system."
P15-2091,Multi-Pass Decoding With Complex Feature Guidance for Statistical Machine Translation,2015,15,2,2,1,8610,benjamin marie,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"In Statistical Machine Translation, some complex features are still difficult to integrate during decoding and usually used through the reranking of the k-best hypotheses produced by the decoder. We propose a translation table partitioning method that exploits the result of this reranking to iteratively guide the decoder in order to produce a new k-best list more relevant to some complex features. We report experiments on two translation domains and two translations directions which yield improvements of up to 1.4 BLEU over the reranking baseline using the same set of complex features. On a practical viewpoint, our approach allows SMT system developers to easily integrate complex features into decoding rather than being limited to their use in one-time k-best list reranking."
D15-1120,Touch-Based Pre-Post-Editing of Machine Translation Output,2015,18,11,2,1,8610,benjamin marie,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"We introduce pre-post-editing, possibly the most basic form of interactive translation, as a touch-based interaction with iteratively improved translation hypotheses prior to classical post-editing. We report simulated experiments that yield very large improvements on classical evaluation metrics (up to 21 BLEU) as well as on a parameterized variant of the TER metric that takes into account the cost of matching/touching tokens, confirming the promising prospects of the novel translation scenarios offered by our approach."
2015.lilt-12.6,Sentence alignment for literary texts: The state-of-the-art and beyond,2015,-1,-1,2,0,34613,yong xu,"Linguistic Issues in Language Technology, Volume 12, 2015 - Literature Lifts up Computational Linguistics",0,"Literary works are becoming increasingly available in electronic formats, thus quickly transforming editorial processes and reading habits. In the context of the global enthusiasm for multilingualism, the rapid spread of e-book readers, such as Amazon Kindle R or Kobo Touch R , fosters the development of a new generation of reading tools for bilingual books. In particular, literary works, when available in several languages, offer an attractive perspective for self-development or everyday leisure reading, but also for activities such as language learning, translation or literary studies. An important issue in the automatic processing of multilingual e-books is the alignment between textual units. Alignment could help identify corresponding text units in different languages, which would be particularly beneficial to bilingual readers and translation professionals. Computing automatic alignments for literary works, however, is a task more challenging than in the case of better behaved corpora such as parliamentary proceedings or technical manuals. In this paper, we revisit the problem of computing high-quality. alignment for literary works. We first perform a large-scale evaluation of automatic alignment for literary texts, which provides a fair assessment of the actual difficulty of this task. We then introduce a two-pass approach, based on a maximum entropy model. Experimental results for novels available in English and French or in English and Spanish demonstrate the effectiveness of our method."
W14-3330,{LIMSI} @ {WMT}{'}14 Medical Translation Task,2014,-1,-1,9,0.625,36855,nicolas pecheux,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,None
F14-3011,(Much) Faster Construction of {SMT} Phrase Tables from Large-scale Parallel Corpora (Construction (tr{\\`e}s) rapide de tables de traduction {\\`a} partir de grands bi-textes) [in {F}rench],2014,0,0,2,1,26548,li gong,Proceedings of TALN 2014 (Volume 3: System Demonstrations),0,None
F14-2002,Towards a More Efficient Development of Statistical Machine Translation Systems (Vers un d{\\'e}veloppement plus efficace des syst{\\`e}mes de traduction statistique : un peu de vert dans un monde de {BLEU}) [in {F}rench],2014,0,0,2,1,26548,li gong,Proceedings of TALN 2014 (Volume 2: Short Papers),0,None
D14-1133,Confidence-based Rewriting of Machine Translation Output,2014,38,2,2,1,8610,benjamin marie,Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP}),0,"Numerous works in Statistical Machine Translation (SMT) have attempted to identify better translation hypotheses obtained by an initial decoding using an improved, but more costly scoring function. In this work, we introduce an approach that takes the hypotheses produced by a state-ofthe-art, reranked phrase-based SMT system, and explores new parts of the search space by applying rewriting rules selected on the basis of posterior phraselevel confidence. In the medical domain, we obtain a 1.9 BLEU improvement over a reranked baseline exploiting the same scoring function, corresponding to a 5.4 BLEU improvement over the original Moses baseline. We show that if an indication of which phrases require rewriting is provided, our automatic rewriting procedure yields an additional improvement of 1.5 BLEU. Various analyses, including a manual error analysis, further illustrate the good performance and potential for improvement of our approach in spite of its simplicity."
2014.iwslt-papers.9,Incremental development of statistical machine translation systems,2014,-1,-1,2,1,26548,li gong,Proceedings of the 11th International Workshop on Spoken Language Translation: Papers,0,"Statistical Machine Translation produces results that make it a competitive option in most machine-assisted translation scenarios. However, these good results often come at a very high computational cost and correspond to training regimes which are unfit to many practical contexts, where the ability to adapt to users and domains and to continuously integrate new data (eg. in post-edition contexts) are of primary importance. In this article, we show how these requirements can be met using a strategy for on-demand word alignment and model estimation. Most remarkably, our incremental system development framework is shown to deliver top quality translation performance even in the absence of tuning, and to surpass a strong baseline when performing online tuning. All these results obtained with great computational savings as compared to conventional systems."
W13-2204,{LIMSI} @ {WMT}13,2013,27,0,6,0,38567,alexander allauzen,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"This paper describes LIMSIxe2x80x99s submissions to the shared WMTxe2x80x9913 translation task. We report results for French-English, German-English and Spanish-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams, and continuous space models in a post-processing step. The main novelties of this yearxe2x80x99s participation are the following: our first participation to the Spanish-English task; experiments with source pre-ordering; a tighter integration of continuous space language models using artificial text generation (for German); and the use of different tuning sets according to the original language of the text to be translated."
W13-1733,{LIMSI}{'}s participation to the 2013 shared task on Native Language Identification,2013,9,1,3,0.714286,8590,thomas lavergne,Proceedings of the Eighth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"This paper describes LIMSIxe2x80x99s participation to the first shared task on Native Language Identification. Our submission uses a Maximum Entropy classifier, using as features character and chunk n-grams, spelling and grammatical mistakes, and lexical preferences. Performance was slightly improved by using a twostep classifier to better distinguish otherwise easily confused native languages."
2013.iwslt-papers.7,Improving bilingual sub-sentential alignment by sampling-based transpotting,2013,-1,-1,2,1,26548,li gong,Proceedings of the 10th International Workshop on Spoken Language Translation: Papers,0,"In this article, we present a sampling-based approach to improve bilingual sub-sentential alignment in parallel corpora. This approach can be used to align parallel sentences on an as needed basis, and is able to accurately align newly available sentences. We evaluate the resulting alignments on several Machine Translation tasks. Results show that for the tasks considered here, our approach performs on par with the state-of-the-art statistical alignment pipeline giza++/Moses, and obtains superior results in a number of configurations, notably when aligning additional parallel sentence pairs carefully selected to match the test input."
2013.iwslt-papers.16,A study in greedy oracle improvement of translation hypotheses,2013,-1,-1,2,1,8610,benjamin marie,Proceedings of the 10th International Workshop on Spoken Language Translation: Papers,0,"This paper describes a study of translation hypotheses that can be obtained by iterative, greedy oracle improvement from the best hypothesis of a state-of-the-art phrase-based Statistical Machine Translation system. The factors that we consider include the influence of the rewriting operations, target languages, and training data sizes. Analysis of our results provide new insights into some previously unanswered questions, which include the reachability of previously unreachable hypotheses via indirect translation (thanks to the introduction of a rewrite operation on the source text), and the potential translation performance of systems relying on pruned phrase tables."
W12-4201,{WSD} for n-best reranking and local language modeling in {SMT},2012,22,6,4,0,2673,marianna apidianaki,"Proceedings of the Sixth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"We integrate semantic information at two stages of the translation process of a state-of-the-art SMT system. A Word Sense Disambiguation (WSD) classifier produces a probability distribution over the translation candidates of source words which is exploited in two ways. First, the probabilities serve to rerank a list of n-best translations produced by the system. Second, the WSD predictions are used to build a supplementary language model for each sentence, aimed to favor translations that seem more adequate in this specific sentential context. Both approaches lead to significant improvements in translation performance, highlighting the usefulness of source side disambiguation for SMT."
W12-3141,{LIMSI} @ {WMT}12,2012,23,1,6,1,40944,haison le,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes LIMSI's submissions to the shared translation task. We report results for French-English and German-English in both directions. Our submissions use n-code, an open source system based on bilingual n-grams. In this approach, both the translation and target language models are estimated as conventional smoothed n-gram models; an approach we extend here by estimating the translation probabilities in a continuous space using neural networks. Experimental results show a significant and consistent BLEU improvement of approximately 1 point for all conditions. We also report preliminary experiments using an on-the-fly translation model."
W12-2505,Aligning Bilingual Literary Works: a Pilot Study,2012,16,1,2,0,20569,qian yu,Proceedings of the {NAACL}-{HLT} 2012 Workshop on Computational Linguistics for Literature,0,"Electronic versions of literary works abound on the Internet and the rapid dissemination of electronic readers will make electronic books more and more common. It is often the case that literary works exist in more than one language, suggesting that, if properly aligned, they could be turned into useful resources for many practical applications, such as writing and language learning aids, translation studies, or data-based machine translation. To be of any use, these bilingual works need to be aligned as precisely as possible, a notoriously difficult task. In this paper, we revisit the problem of sentence alignment for literary works and explore the performance of a new, multi-pass, approach based on a combination of systems. Experiments conducted on excerpts of ten masterpieces of the French and English literature show that our approach significantly outperforms two open source tools."
bouamor-etal-2012-contrastive,A contrastive review of paraphrase acquisition techniques,2012,20,3,2,1,516,houda bouamor,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"This paper addresses the issue of what approach should be used for building a corpus of sententential paraphrases depending on one's requirements. Six strategies are studied: (1) multiple translations into a single language from another language; (2) multiple translations into a single language from different other languages; (3) multiple descriptions of short videos; (4) multiple subtitles for the same language; (5) headlines for similar news articles; and (6) sub-sentential paraphrasing in the context of a Web-based game. We report results on French for 50 paraphrase pairs collected for all these strategies, where corpora were manually aligned at the finest possible level to define oracle performance in terms of accessible sub-sentential paraphrases. The differences observed will be used as criteria for motivating the choice of a given approach before attempting to build a new paraphrase corpus."
F12-2002,Extraction d{'}information automatique en domaine m{\\'e}dical par projection inter-langue : vers un passage {\\`a} l{'}{\\'e}chelle (Automatic Information Extraction in the Medical Domain by Cross-Lingual Projection) [in {F}rench],2012,0,0,3,0,7146,asma abacha,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
F12-2015,Validation sur le Web de reformulations locales: application {\\`a} la Wikip{\\'e}dia (Assisted Rephrasing for {W}ikipedia Contributors through Web-based Validation) [in {F}rench],2012,0,0,2,1,516,houda bouamor,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
F12-2020,"Une {\\'e}tude en 3{D} de la paraphrase: types de corpus, langues et techniques (A Study of Paraphrase along 3 Dimensions : Corpus Types, Languages and Techniques) [in {F}rench]",2012,-1,-1,2,1,516,houda bouamor,"Proceedings of the Joint Conference JEP-TALN-RECITAL 2012, volume 2: TALN",0,None
E12-1073,Validation of sub-sentential paraphrases acquired from parallel monolingual corpora,2012,28,4,2,1,516,houda bouamor,Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"The task of paraphrase acquisition from related sentences can be tackled by a variety of techniques making use of various types of knowledge. In this work, we make the hypothesis that their performance can be increased if candidate paraphrases can be validated using information that characterizes paraphrases independently of the set of techniques that proposed them. We implement this as a bi-class classification problem (i.e. paraphrase vs. not paraphrase), allowing any paraphrase acquisition technique to be easily integrated into the combination system. We report experiments on two languages, English and French, with 5 individual techniques on parallel monolingual parallel corpora obtained via multiple translation, and a large set of classification features including surface to contextual similarity measures. Relative improvements in F-measure close to 18% are obtained on both languages over the best performing techniques."
D12-1066,Generalizing Sub-sentential Paraphrase Acquisition across Original Signal Type of Text Pairs,2012,39,7,1,1,28247,aurelien max,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"This paper describes a study on the impact of the original signal (text, speech, visual scene, event) of a text pair on the task of both manual and automatic sub-sentential paraphrase acquisition. A corpus of 2,500 annotated sentences in English and French is described, and performance on this corpus is reported for an efficient system combination exploiting a large set of features for paraphrase recognition. A detailed quantified typology of sub-sentential paraphrases found in our corpus types is given."
2012.iwslt-papers.20,Towards contextual adaptation for any-text translation,2012,22,3,2,1,26548,li gong,Proceedings of the 9th International Workshop on Spoken Language Translation: Papers,0,"Adaptation for Machine Translation has been studied in a variety of ways, using an ideal scenario where the training data can be split into {''}out-of-domain{''} and {''}in-domain{''} corpora, on which the adaptation is based. In this paper, we consider a more realistic setting which does not assume the availability of any kind of {''}in-domain{''} data, hence the name {''}any-text translation{''}. In this context, we present a new approach to contextually adapt a translation model onthe-fly, and present several experimental results where this approach outperforms conventionaly trained baselines. We also present a document-level contrastive evaluation whose results can be easily interpreted, even by non-specialists."
W11-2135,{LIMSI} @ {WMT}11,2011,19,17,4,0.512821,5598,alexandre allauzen,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes LIMSI's submissions to the Sixth Workshop on Statistical Machine Translation. We report results for the French-English and German-English shared translation tasks in both directions. Our systems use n-code, an open source Statistical Machine Translation system based on bilingual n-grams. For the French-English task, we focussed on finding efficient ways to take advantage of the large and heterogeneous training parallel data. In particular, using a simple filtering strategy helped to improve both processing time and translation quality. To translate from English to French and German, we also investigated the use of the SOUL language model in Machine Translation and showed significant improvements with a 10-gram SOUL model. We also briefly report experiments with several alternatives to the standard n-best MERT procedure, leading to a significant speed-up."
W11-1602,Web-based Validation for Contextual Targeted Paraphrasing,2011,39,4,2,1,516,houda bouamor,Proceedings of the Workshop on Monolingual Text-To-Text Generation,0,"In this work, we present a scenario where contextual targeted paraphrasing of sub-sentential phrases is performed automatically to support the task of text revision. Candidate paraphrases are obtained from a preexisting repertoire and validated in the context of the original sentence using information derived from the Web. We report on experiments on French, where the original sentences to be rewritten are taken from a rewriting memory automatically extracted from the edit history of Wikipedia."
P11-2069,Monolingual Alignment by Edit Rate Computation on Sentential Paraphrase Pairs,2011,17,6,2,1,516,houda bouamor,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"In this paper, we present a novel way of tackling the monolingual alignment problem on pairs of sentential paraphrases by means of edit rate computation. In order to inform the edit rate, information in the form of subsentential paraphrases is provided by a range of techniques built for different purposes. We show that the tunable TER-PLUS metric from Machine Translation evaluation can achieve good performance on this task and that it can effectively exploit information coming from complementary sources."
2011.jeptalnrecital-long.33,Paraphrases et modifications locales dans l{'}historique des r{\\'e}visions de Wikip{\\'e}dia (Paraphrases and local changes in the revision history of {W}ikipedia),2011,-1,-1,4,0,35957,camille dutrey,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous analysons les modifications locales disponibles dans l{'}historique des r{\'e}visions de la version fran{\c{c}}aise de Wikip{\'e}dia. Nous d{\'e}finissons tout d{'}abord une typologie des modifications fond{\'e}e sur une {\'e}tude d{\'e}taill{\'e}e d{'}un large corpus de modifications. Puis, nous d{\'e}taillons l{'}annotation manuelle d{'}une partie de ce corpus afin d{'}{\'e}valuer le degr{\'e} de complexit{\'e} de la t{\^a}che d{'}identification automatique de paraphrases dans ce genre de corpus. Enfin, nous {\'e}valuons un outil d{'}identification de paraphrases {\`a} base de r{\`e}gles sur un sous-ensemble de notre corpus."
2011.jeptalnrecital-long.38,Combinaison d{'}informations pour l{'}alignement monolingue (Information combination for monolingual alignment),2011,-1,-1,2,1,516,houda bouamor,Actes de la 18e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous d{\'e}crivons une nouvelle m{\'e}thode d{'}alignement automatique de paraphrases d{'}{\'e}nonc{\'e}s. Nous utilisons des m{\'e}thodes d{\'e}velopp{\'e}es pr{\'e}c{\'e}demment afin de produire diff{\'e}rentes approches hybrides (hybridations). Ces diff{\'e}rentes m{\'e}thodes permettent d{'}acqu{\'e}rir des {\'e}quivalences textuelles {\`a} partir d{'}un corpus monolingue parall{\`e}le. L{'}hybridation combine des informations obtenues par diverses techniques : alignements statistiques, approche symbolique, fusion d{'}arbres syntaxiques et alignement bas{\'e} sur des distances d{'}{\'e}dition. Nous avons {\'e}valu{\'e} l{'}ensemble de ces r{\'e}sultats et nous constatons une am{\'e}lioration sur l{'}acquisition de paraphrases sous-phrastiques."
max-etal-2010-contrastive,Contrastive Lexical Evaluation of Machine Translation,2010,15,10,1,1,28247,aurelien max,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"This paper advocates a complementary measure of translation performance that focuses on the constrastive ability of two or more systems or system versions to adequately translate source words. This is motivated by three main reasons : 1) existing automatic metrics sometimes do not show significant differences that can be revealed by fine-grained focussed human evaluation, 2) these metrics are based on direct comparisons between system hypotheses with the corresponding reference translations, thus ignoring the input words that were actually translated, and 3) as these metrics do not take input hypotheses from several systems at once, fine-grained contrastive evaluation can only be done indirectly. This proposal is illustrated on a multi-source Machine Translation scenario where multiple translations of a source text are available. Significant gains (up to +1.3 BLEU point) are achieved on these experiments, and contrastive lexical evaluation is shown to provide new information that can help to better analyse a system's performance."
max-wisniewski-2010-mining,Mining Naturally-occurring Corrections and Paraphrases from {W}ikipedia{'}s Revision History,2010,10,40,1,1,28247,aurelien max,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Naturally-occurring instances of linguistic phenomena are important both for training and for evaluating automatic text processing. When available in large quantities, they also prove interesting material for linguistic studies. In this article, we present WiCoPaCo (Wikipedia Correction and Paraphrase Corpus), a new freely-available resource built by automatically mining WikipediaÂs revision history. The WiCoPaCo corpus focuses on local modifications made by human revisors and include various types of corrections (such as spelling error or typographical corrections) and rewritings, which can be categorized broadly into meaning-preserving and meaning-altering revisions. We present an initial hand-built typology of these revisions, but the resource allows for any possible annotation scheme. We discuss the main motivations for building such a resource and describe the main technical details guiding its construction. We also present applications and data analysis on French and report initial results on spelling error correction and morphosyntactic rewriting. The WiCoPaCo corpus can be freely downloaded from http://wicopaco.limsi.fr."
D10-1064,Example-Based Paraphrasing for Improved Phrase-Based Statistical Machine Translation,2010,30,18,1,1,28247,aurelien max,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"In this article, an original view on how to improve phrase translation estimates is proposed. This proposal is grounded on two main ideas: first, that appropriate examples of a given phrase should participate more in building its translation distribution; second, that paraphrases can be used to better estimate this distribution. Initial experiments provide evidence of the potential of our approach and its implementation for effectively improving translation performance."
C10-1027,Local lexical adaptation in Machine Translation through triangulation: {SMT} helping {SMT},2010,23,11,2,0.341263,836,josep crego,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"We present a framework where auxiliary MT systems are used to provide lexical predictions to a main SMT system. In this work, predictions are obtained by means of pivoting via auxiliary languages, and introduced into the main SMT system in the form of a low order language model, which is estimated on a sentence-by-sentence basis. The linear combination of models implemented by the decoder is thus extended with this additional language model. Experiments are carried out over three different translation tasks using the European Parliament corpus. For each task, nine additional languages are used as auxiliary languages to obtain the triangulated predictions. Translation accuracy results show that improvements in translation quality are obtained, even for large data conditions."
2010.jeptalnrecital-long.13,Recueil et analyse d{'}un corpus {\\'e}cologique de corrections orthographiques extrait des r{\\'e}visions de Wikip{\\'e}dia,2010,-1,-1,2,0.517241,168,guillaume wisniewski,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans cet article, nous introduisons une m{\'e}thode {\`a} base de r{\`e}gles permettant d{'}extraire automatiquement de l{'}historique des {\'e}ditions de l{'}encyclop{\'e}die collaborative Wikip{\'e}dia des corrections orthographiques. Cette m{\'e}thode nous a permis de construire un corpus d{'}erreurs compos{\'e} de 72 483 erreurs lexicales (non-word errors) et 74 100 erreurs grammaticales (real-word errors). Il n{'}existe pas, {\`a} notre connaissance, de plus gros corpus d{'}erreurs {\'e}cologiques librement disponible. En outre, les techniques mises en oeuvre peuvent {\^e}tre facilement transpos{\'e}es {\`a} de nombreuses autres langues. La collecte de ce corpus ouvre de nouvelles perspectives pour l{'}{\'e}tude des erreurs fr{\'e}quentes ainsi que l{'}apprentissage et l{'}{\'e}valuation des correcteurs orthographiques automatiques. Plusieurs exp{\'e}riences illustrant son int{\'e}r{\^e}t sont propos{\'e}es."
2010.jeptalnrecital-court.18,Acquisition de paraphrases sous-phrastiques depuis des paraphrases d{'}{\\'e}nonc{\\'e}s,2010,-1,-1,2,1,516,houda bouamor,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Dans cet article, nous pr{\'e}sentons la t{\^a}che d{'}acquisition de paraphrases sous-phrastiques (impliquant des paires de mots ou de groupes de mots), et d{\'e}crivons plusieurs techniques op{\'e}rant {\`a} diff{\'e}rents niveaux. Nous d{\'e}crivons une {\'e}valuation visant {\`a} comparer ces techniques et leurs combinaisons sur deux corpus de paraphrases d{'}{\'e}nonc{\'e}s obtenus par traduction multiple. Les conclusions que nous tirons peuvent servir de guide pour am{\'e}liorer des techniques existantes."
2010.iwslt-papers.12,Multi-pivot translation by system combination,2010,28,10,2,0,29312,gregor leusch,Proceedings of the 7th International Workshop on Spoken Language Translation: Papers,0,"This paper describes a technique to exploit multiple pivot languages when using machine translation (MT) on language pairs with scarce bilingual resources, or where no translation system for a language pair is available. The principal idea is to generate intermediate translations in several pivot languages, translate them separately into the target language, and generate a consensus translation out of these using MT system combination techniques. Our technique can also be applied when a translation system for a language pair is available, but is limited in its translation accuracy because of scarce resources. Using statistical MT systems for the 11 different languages of Europarl, we show experimentally that a direct translation system can be replaced by this pivot approach without a loss in translation quality if about six pivot languages are available. Furthermore, we can already improve an existing MT system by adding two pivot systems to it. The maximum improvement was found to be 1.4{\%} abs. in BLEU in our experiments for 8 or more pivot languages."
W09-2503,Sub-sentencial Paraphrasing by Contextual Pivot Translation,2009,26,13,1,1,28247,aurelien max,Proceedings of the 2009 Workshop on Applied Textual Inference ({T}ext{I}nfer),0,"The ability to generate or to recognize paraphrases is key to the vast majority of NLP applications. As correctly exploiting context during translation has been shown to be successful, using context information for paraphrasing could also lead to improved performance. In this article, we adopt the pivot approach based on parallel multilingual corpora proposed by (Bannard and Callison-Burch, 2005), which finds short paraphrases by finding appropriate pivot phrases in one or several auxiliary languages and back-translating these pivot phrases into the original language. We show how context can be exploited both when attempting to find pivot phrases, and when looking for the most appropriate paraphrase in the original subsentential envelope. This framework allows the use of paraphrasing units ranging from words to large sub-sentential fragments for which context information from the sentence can be successfully exploited. We report experiments on a text revision task, and show that in these experiments our contextual sub-sentential paraphrasing system outperforms a strong baseline system."
W09-0417,{LIMSI}{`}s Statistical Translation Systems for {WMT}{`}09,2009,18,3,3,0.444444,5598,alexandre allauzen,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"This paper describes our Statistical Machine Translation systems for the WMT09 (en:fr) shared task. For this evaluation, we have developed four systems, using two different MT Toolkits: our primary submission, in both directions, is based on Moses, boosted with contextual information on phrases, and is contrasted with a conventional Moses-based system. Additional contrasts are based on the Ncode toolkit, one of which uses (part of) the English/French GigaWord parallel corpus."
2009.jeptalnrecital-long.16,Prise en compte de d{\\'e}pendances syntaxiques pour la traduction contextuelle de segments,2009,-1,-1,1,1,28247,aurelien max,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Dans un syst{\`e}me standard de traduction statistique bas{\'e} sur les segments, le score attribu{\'e} aux diff{\'e}rentes traductions d{'}un segment ne d{\'e}pend pas du contexte dans lequel il appara{\^\i}t. Plusieurs travaux r{\'e}cents tendent {\`a} montrer l{'}int{\'e}r{\^e}t de prendre en compte le contexte source lors de la traduction, mais ces {\'e}tudes portent sur des syst{\`e}mes traduisant vers l{'}anglais, une langue faiblement fl{\'e}chie. Dans cet article, nous d{\'e}crivons nos exp{\'e}riences sur la prise en compte du contexte source dans un syst{\`e}me statistique traduisant de l{'}anglais vers le fran{\c{c}}ais, bas{\'e} sur l{'}approche propos{\'e}e par Stroppa et al. (2007). Nous {\'e}tudions l{'}impact de diff{\'e}rents types d{'}indices capturant l{'}information contextuelle, dont des d{\'e}pendances syntaxiques typ{\'e}es. Si les mesures automatiques d{'}{\'e}valuation de la qualit{\'e} d{'}une traduction ne r{\'e}v{\`e}lent pas de gains significatifs de notre syst{\`e}me par rapport {\`a} un syst{\`e}me {\`a} l{'}{\'e}tat de l{'}art ne faisant pas usage du contexte, une {\'e}valuation manuelle conduite sur 100 phrases choisies al{\'e}atoirement est en faveur de notre syst{\`e}me. Cette {\'e}valuation fait {\'e}galement ressortir que la prise en compte de certaines d{\'e}pendances syntaxiques est b{\'e}n{\'e}fique {\`a} notre syst{\`e}me."
2009.jeptalnrecital-demonstration.2,Amener des utilisateurs {\\`a} cr{\\'e}er et {\\'e}valuer des paraphrases par le jeu,2009,-1,-1,2,1,516,houda bouamor,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,"Dans cet article, nous pr{\'e}sentons une application sur le web pour l{'}acquisition de paraphrases phrastiques et sous-phrastiques sous forme de jeu. L{'}application permet l{'}acquisition {\`a} la fois de paraphrases et de jugements humains multiples sur ces paraphrases, ce qui constitue des donn{\'e}es particuli{\`e}rement utiles pour les applications du TAL bas{\'e}es sur les ph{\'e}nom{\`e}nes paraphrastiques."
2009.jeptalnrecital-court.28,Plusieurs langues (bien choisies) valent mieux qu{'}une : traduction statistique multi-source par renforcement lexical,2009,17,3,2,0.341263,836,josep crego,Actes de la 16{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Les syst{\`e}mes de traduction statistiques int{\`e}grent diff{\'e}rents types de mod{\`e}les dont les pr{\'e}dictions sont combin{\'e}es, lors du d{\'e}codage, afin de produire les meilleures traductions possibles. Traduire correctement des mots polys{\'e}miques, comme, par exemple, le mot avocat du fran{\c{c}}ais vers l{'}anglais (lawyer ou avocado), requiert l{'}utilisation de mod{\`e}les suppl{\'e}mentaires, dont l{'}estimation et l{'}int{\'e}gration s{'}av{\`e}rent complexes. Une alternative consiste {\`a} tirer parti de l{'}observation selon laquelle les ambigu{\""\i}t{\'e}s li{\'e}es {\`a} la polys{\'e}mie ne sont pas les m{\^e}mes selon les langues source consid{\'e}r{\'e}es. Si l{'}on dispose, par exemple, d{'}une traduction vers l{'}espagnol dans laquelle avocat a {\'e}t{\'e} traduit par aguacate, alors la traduction de ce mot vers l{'}anglais n{'}est plus ambigu{\""e}. Ainsi, la connaissance d{'}une traduction fran{\c{c}}ais!espagnol permet de renforcer la s{\'e}lection de la traduction avocado pour le syst{\`e}me fran{\c{c}}ais!anglais. Dans cet article, nous proposons d{'}utiliser des documents en plusieurs langues pour renforcer les choix lexicaux effectu{\'e}s par un syst{\`e}me de traduction automatique. En particulier, nous montrons une am{\'e}lioration des performances sur plusieurs m{\'e}triques lorsque les traductions auxiliaires utilis{\'e}es sont obtenues manuellement."
W08-1911,Looking up phrase rephrasings via a pivot language,2008,21,3,1,1,28247,aurelien max,Coling 2008: Proceedings of the Workshop on Cognitive Aspects of the Lexicon ({COGALEX} 2008),0,"Rephrasing text spans is a common task when revising a text. However, traditional dictionaries often cannot provide direct assistance to writers in performing this task. In this article, we describe an approach to obtain a monolingual phrase lexicon using techniques used in Statistical Machine Translation. A part to be rephrased is first translated into a pivot language, and then translated back into the original language. Models for assessing fluency, meaning preservation and lexical divergence are used to rank possible rephrasings, and their relative weight can be tuned by the user so as to better address her needs. An evaluation shows that these models can be used successfully to select rephrasings that are likely to be useful to a writer."
toney-etal-2008-evaluation,An Evaluation of Spoken and Textual Interaction in the {RITEL} Interactive Question Answering System,2008,6,12,3,0,48266,dave toney,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The RITEL project aims to integrate a spoken language dialogue system and an open-domain information retrieval system in order to enable human users to ask a general question and to refine their search for information interactively. This type of system is often referred to as an Interactive Question Answering (IQA) system. In this paper, we present an evaluation of how the performance of the RITEL system differs when users interact with it using spoken versus textual input and output. Our results indicate that while users do not perceive the two versions to perform significantly differently, many more questions are asked in a typical text-based dialogue."
2008.jeptalnrecital-long.23,G{\\'e}n{\\'e}ration de reformulations locales par pivot pour l{'}aide {\\`a} la r{\\'e}vision,2008,-1,-1,1,1,28247,aurelien max,Actes de la 15{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles longs,0,"Cet article pr{\'e}sente une approche pour obtenir des paraphrases pour de courts segments de texte qui peuvent aider un r{\'e}dacteur {\`a} reformuler localement des textes. La ressource principale utilis{\'e}e est une table d{'}alignements bilingues de segments d{'}un syst{\`e}me de traduction automatique statistique. Un segment marqu{\'e} par le r{\'e}dacteur est tout d{'}abord traduit dans une langue pivot avant d{'}{\^e}tre traduit {\`a} nouveau dans la langue d{'}origine, ce qui est permis par la nature m{\^e}me de la ressource bilingue utilis{\'e}e sans avoir recours {\`a} un processus de traduction complet. Le cadre propos{\'e} permet l{'}int{\'e}gration et la combinaison de diff{\'e}rents mod{\`e}les d{'}estimation de la qualit{\'e} des paraphrases. Des mod{\`e}les linguistiques tentant de prendre en compte des caract{\'e}ristiques des paraphrases de courts segments de textes sont propos{\'e}s, et une {\'e}valuation est d{\'e}crite et ses r{\'e}sultats analys{\'e}s. Les domaines d{'}application possibles incluent, outre l{'}aide {\`a} la reformulation, le r{\'e}sum{\'e} et la r{\'e}{\'e}criture des textes pour r{\'e}pondre {\`a} des conventions ou {\`a} des pr{\'e}f{\'e}rences stylistiques. L{'}approche est critiqu{\'e}e et des perspectives d{'}am{\'e}lioration sont propos{\'e}es."
2008.eamt-1.17,Explorations in using grammatical dependencies for contextual phrase translation disambiguation,2008,9,9,1,1,28247,aurelien max,Proceedings of the 12th Annual conference of the European Association for Machine Translation,0,"Recent research has shown the importance of using source context information to disambiguate source phrases in phrase-based Sta- tistical Machine Translation. Although encouraging results have been obtained, those studies mostly focus on translating into a less inflected target language. In this article, we present an attempt at using source context information to translate from English into French. In addition to information extracted from the immediate context of a source phrase, we also exploit grammatical dependencies information. While automatic evaluation does not exhibit a significant dierence, a manual evaluation we conducted provides evidence that our context-aware translation en- gine outperforms its context-insensitive counterpart."
2005.jeptalnrecital-court.15,Simplification interactive pour la production de textes adapt{\\'e}s aux personnes souffrant de troubles de la compr{\\'e}hension,2005,-1,-1,1,1,28247,aurelien max,Actes de la 12{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. Articles courts,0,"Cet article traite du probl{\`e}me de la compr{\'e}hensibilit{\'e} des textes et en particulier du besoin de simplifier la complexit{\'e} syntaxique des phrases pour des lecteurs souffrant de troubles de la compr{\'e}hension. Nous pr{\'e}sentons une approche {\`a} base de r{\`e}gles de simplification d{\'e}velopp{\'e}es manuellement et son int{\'e}gration dans un traitement de texte. Cette int{\'e}gration permet la validation interactive de simplifications candidates produites par le syst{\`e}me, et lie la t{\^a}che de cr{\'e}ation de texte simplifi{\'e} {\`a} celle de r{\'e}daction."
W04-1710,he Syntax Student{'}s Companion: an e{L}earning Tool designed for (Computational) Linguistics Students,2004,8,1,1,1,28247,aurelien max,Proceedings of the Workshop on e{L}earning for Computational Linguistics and Computational Linguistics for e{L}earning,0,"This paper advocates the use of free and easily accessible computer programs in teaching. The motivating reasons for a particular program supporting the learning of syntax are given, and a first version of the program is presented and illustrated. Initial evaluation results led to additional specifications and to the development of a new version of the program that is introduced. Finally, several perspectives for such a support tool are drawn."
W04-0915,Interpreting communicative goals in constrained domains using generation and interactive negotiation,2004,13,1,1,1,28247,aurelien max,Proceedings of the 2nd Workshop on Text Meaning and Interpretation,0,"This article presents an approach to interpret the content of documents in constrained domains at the level of communicative goals. The kind of knowledge used contains descriptions of well-formed document contents and texts that can be produced from them. The automatic analysis of text content is followed by an interactive negotiation phase involving an expert of the class of documents. Motivating reasons are given for an application of this approach, document normalization, and an implemented system is briefly introduced."
C04-1166,From Controlled Document Authoring to Interactive Document Normalization,2004,9,2,1,1,28247,aurelien max,{COLING} 2004: Proceedings of the 20th International Conference on Computational Linguistics,0,"This paper presents an approach to normalize documents in constrained domains. This approach reuses resources developed for controlled document authoring and is decomposed into three phases. First, candidate content representations for an input document are automatically built. Then, the content representation that best corresponds to the document according to an expert of the class of documents is identified. This content representation is finally used to generate the normalized version of the document. The current version of our prototype system is presented, and its limitations are discussed."
W03-2204,Multi-language Machine Translation through Interactive Document Normalization,2003,14,3,1,1,28247,aurelien max,"Proceedings of the 7th International {EAMT} workshop on {MT} and other language technology tools, Improving {MT} through other language technology tools, Resource and tools for building {MT} at {EACL} 2003",0,"Document normalization is an interactive process that transforms raw legacy documents into semantically well-formed and linguistically controlled documents with the same communicative intention content. A paradigm for content analysis has been implemented to select candidate semantic representations of the communicative content of an input document. This implementation reuses the formal content specification of a multilingual controlled authoring system. As a consequence, a candidate semantic representation can not only be associated with a text in the language of the input document, but also in all the languages supported by the system. This paper presents how multilingual versions of an input legacy document can be obtained interactively with a proposed implementation, and discusses the advantages and limitations of this kind of normalizing translation."
P03-2017,Towards Interactive Text Understanding,2003,16,4,2,0,26373,marc dymetman,The Companion Volume to the Proceedings of 41st Annual Meeting of the Association for Computational Linguistics,0,This position paper argues for an interactive approach to text understanding. The proposed model extends an existing semantics-based text authoring system by using the input text as a source of information to assist the user in re-authoring its content. The approach permits a reliable deep semantic analysis by combining automatic information extraction with a minimal amount of human intervention.
E03-3006,Reversing Controlled Document Authoring to Normalize Documents,2003,13,5,1,1,28247,aurelien max,Student Research Workshop,0,"This paper introduces document normalization, and addresses the issue of whether controlled document authoring systems can be used in a reverse mode to normalize legacy documents. A paradigm for deep content analysis using such a system is proposed, and an architecture for a document normalization system is described."
2002.jeptalnrecital-recital.8,Normalisation de documents par analyse du contenu {\\`a} l{'}aide d{'}un mod{\\`e}le s{\\'e}mantique et d{'}un g{\\'e}n{\\'e}rateur,2002,-1,-1,1,1,28247,aurelien max,Actes de la 9{\\`e}me conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. REncontres jeunes Chercheurs en Informatique pour le Traitement Automatique des Langues,0,La probl{\'e}matique de la normalisation de documents est introduite et illustr{\'e}e par des exemples issus de notices pharmaceutiques. Un paradigme pour l{'}analyse du contenu des documents est propos{\'e}. Ce paradigme se base sur la sp{\'e}cification formelle de la s{\'e}mantique des documents et utilise une notion de similarit{\'e} floue entre les pr{\'e}dictions textuelles d{'}un g{\'e}n{\'e}rateur de texte et le texte du document {\`a} analyser. Une impl{\'e}mentation initiale du paradigme est pr{\'e}sent{\'e}e.
