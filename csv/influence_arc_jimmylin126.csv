2020.acl-main.204,N19-1423,0,0.10578,"l quality. Further analyses show different behaviors in the BERT transformer layers and also reveal their redundancy. Our work provides new ideas to efficiently apply deep transformer-based models to downstream tasks. Code is available at https://github.com/castorini/ DeeBERT. 1 Figure 1: DeeBERT model overview. Grey blocks are transformer layers, orange circles are classification layers (off-ramps), and blue arrows represent inference samples exiting at different layers. Introduction Large-scale pre-trained language models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2019), BERT (Devlin et al., 2019), XLNet (Yang et al., 2019), and RoBERTa (Liu et al., 2019) have brought significant improvements to natural language processing (NLP) applications. Despite their power, they are notorious for being enormous in size and slow in both training and inference. Their long inference latencies present challenges to deployment in real-time applications and hardwareconstrained edge devices such as mobile phones and smart watches. To accelerate inference for BERT, we propose DeeBERT: Dynamic early exiting for BERT. The inspiration comes from a well-known observation in the computer vision community: in"
2020.acl-main.204,2021.ccl-1.108,0,0.210481,"Missing"
2020.acl-main.204,N18-1202,0,0.172503,"Missing"
2020.acl-main.204,D19-6122,1,0.819035,"tion. Michel et al. (2019) and Voita et al. (2019) analyze redundancy 2246 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 2246–2251 c July 5 - 10, 2020. 2020 Association for Computational Linguistics in attention heads. Q-BERT (Shen et al., 2019) uses quantization to compress BERT, and LayerDrop (Fan et al., 2019) uses group regularization to enable structured pruning at inference time. On the knowledge distillation side, TinyBERT (Jiao et al., 2019) and DistilBERT (Sanh et al., 2019) both distill BERT into a smaller transformer-based model, and Tang et al. (2019) distill BERT into even smaller non-transformer-based models. Our work is inspired by Cambazoglu et al. (2010), Teerapittayanon et al. (2017), and Huang et al. (2018), but mainly differs from previous work in that we focus on improving model efficiency with minimal quality degradation. 3 Early Exit for BERT inference DeeBERT modifies fine-tuning and inference of BERT models, leaving pre-training unchanged. It adds one off-ramp for each transformer layer. An inference sample can exit earlier at an off-ramp, without going through the rest of the transformer layers. The last off-ramp is the class"
2020.acl-main.204,P19-1580,0,0.0631466,"Missing"
2020.acl-main.204,W18-5446,0,0.177163,"Missing"
2020.acl-main.246,N19-1408,1,0.830565,"true parameter and draw millions of samples. To maintain the realism of our study, we apply kernel density estimation to actual results, using the resulting probability density (or discretized mass) function as the ground truth distribution. Specifically, we examine the experimental results of the following neural networks: Document classification. We first conduct hyperparameter search over neural networks for document classification, namely a multilayer perceptron (MLP) and a long short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) model representing state of the art (for LSTMs) from Adhikari et al. (2019). For our dataset and evaluation metric, we choose Reuters (Apt´e et al., 1994) and the F1 score, respectively. Next, we fit discretized kernel density estimators to the results—see the appendix for experimental details. We name the distributions after their models, MLP and LSTM. Sentiment analysis. Similar to Dodge et al. (2019), on the task of sentiment analysis, we tune the hyperparameters of two LSTMs—one ingesting embeddings from language models (ELMo; Peters et al., 2018), the other shallow word vectors (GloVe; Pennington et al., 2014). We choose the binary Stanford Sentiment Treebank (S"
2020.acl-main.246,Q18-1018,0,0.0300378,"We find that the estimator favors negative errors and yields poor bootstrapped confidence intervals. We derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation. Our codebase is at https://github.com/ castorini/meanmax. 1 Introduction Questionable answers and irreproducible results represent a formidable beast in natural language processing research. Worryingly, countless experimental papers lack empirical rigor, disregarding necessities such as the reporting of statistical significance tests (Dror et al., 2018) and computational environments (Crane, 2018). As Forde and Paganini (2019) concisely lament, explorimentation, the act of tinkering with metaparameters and praying for success, while helpful in brainstorming, does not constitute a rigorous scientific effort. Against the crashing wave of explorimentation, though, a few brave souls have resisted the urge to feed the beast. Reimers and Gurevych (2017) argue for the reporting of neural network score distributions. Gorman and Bedrick (2019) demonstrate that deterministic dataset splits yield less robust results than random ones for neural networks. Dodge et al. (2019) advocate for reporting"
2020.acl-main.246,D19-1224,0,0.24484,"nd computational environments (Crane, 2018). As Forde and Paganini (2019) concisely lament, explorimentation, the act of tinkering with metaparameters and praying for success, while helpful in brainstorming, does not constitute a rigorous scientific effort. Against the crashing wave of explorimentation, though, a few brave souls have resisted the urge to feed the beast. Reimers and Gurevych (2017) argue for the reporting of neural network score distributions. Gorman and Bedrick (2019) demonstrate that deterministic dataset splits yield less robust results than random ones for neural networks. Dodge et al. (2019) advocate for reporting the expected validation quality as a function of the computation budget used for hyperparameter tuning, which is paramount to robust conclusions. But carefully tread we must. Papers that advocate for scientific rigor must be held to the very same standards that they espouse, lest they birth a new beast altogether. In this work, we critically examine one such paper from Dodge et al. (2019). We acknowledge the validity of their technical contribution, but we find several notable caveats, as far as statistical generalizability is concerned. Analytically, we show that their"
2020.acl-main.246,D14-1162,0,0.0848023,"Missing"
2020.acl-main.246,N18-1202,0,0.0101884,"ong short-term memory (LSTM; Hochreiter and Schmidhuber, 1997) model representing state of the art (for LSTMs) from Adhikari et al. (2019). For our dataset and evaluation metric, we choose Reuters (Apt´e et al., 1994) and the F1 score, respectively. Next, we fit discretized kernel density estimators to the results—see the appendix for experimental details. We name the distributions after their models, MLP and LSTM. Sentiment analysis. Similar to Dodge et al. (2019), on the task of sentiment analysis, we tune the hyperparameters of two LSTMs—one ingesting embeddings from language models (ELMo; Peters et al., 2018), the other shallow word vectors (GloVe; Pennington et al., 2014). We choose the binary Stanford Sentiment Treebank (Socher et al., 2013) dataset and apply the same kernel density estimation method. We denote the distributions by their embedding types, GloVe and ELMo. 4.2 Experimental Test Battery False conclusion probing. To assess the impact of the estimator bias, we measure the probability of researchers falsely concluding that one method underperforms its true value for a given n. The unbiased estimator has an expectation of 0.5, preferring neither underestimates nor overestimates. Concret"
2020.acl-main.246,D17-1035,0,0.0210887,"a formidable beast in natural language processing research. Worryingly, countless experimental papers lack empirical rigor, disregarding necessities such as the reporting of statistical significance tests (Dror et al., 2018) and computational environments (Crane, 2018). As Forde and Paganini (2019) concisely lament, explorimentation, the act of tinkering with metaparameters and praying for success, while helpful in brainstorming, does not constitute a rigorous scientific effort. Against the crashing wave of explorimentation, though, a few brave souls have resisted the urge to feed the beast. Reimers and Gurevych (2017) argue for the reporting of neural network score distributions. Gorman and Bedrick (2019) demonstrate that deterministic dataset splits yield less robust results than random ones for neural networks. Dodge et al. (2019) advocate for reporting the expected validation quality as a function of the computation budget used for hyperparameter tuning, which is paramount to robust conclusions. But carefully tread we must. Papers that advocate for scientific rigor must be held to the very same standards that they espouse, lest they birth a new beast altogether. In this work, we critically examine one s"
2020.acl-main.246,D13-1170,0,0.00472683,"). For our dataset and evaluation metric, we choose Reuters (Apt´e et al., 1994) and the F1 score, respectively. Next, we fit discretized kernel density estimators to the results—see the appendix for experimental details. We name the distributions after their models, MLP and LSTM. Sentiment analysis. Similar to Dodge et al. (2019), on the task of sentiment analysis, we tune the hyperparameters of two LSTMs—one ingesting embeddings from language models (ELMo; Peters et al., 2018), the other shallow word vectors (GloVe; Pennington et al., 2014). We choose the binary Stanford Sentiment Treebank (Socher et al., 2013) dataset and apply the same kernel density estimation method. We denote the distributions by their embedding types, GloVe and ELMo. 4.2 Experimental Test Battery False conclusion probing. To assess the impact of the estimator bias, we measure the probability of researchers falsely concluding that one method underperforms its true value for a given n. The unbiased estimator has an expectation of 0.5, preferring neither underestimates nor overestimates. Concretely, denote the true n-run expected maxima of the method as θn and the estimator as θˆn . We iterate n = 1, . . . , 50 and report the pro"
2020.acl-main.246,P18-1128,0,0.0250587,"timator is biased and uses error-prone assumptions. We find that the estimator favors negative errors and yields poor bootstrapped confidence intervals. We derive an unbiased alternative and bolster our claims with empirical evidence from statistical simulation. Our codebase is at https://github.com/ castorini/meanmax. 1 Introduction Questionable answers and irreproducible results represent a formidable beast in natural language processing research. Worryingly, countless experimental papers lack empirical rigor, disregarding necessities such as the reporting of statistical significance tests (Dror et al., 2018) and computational environments (Crane, 2018). As Forde and Paganini (2019) concisely lament, explorimentation, the act of tinkering with metaparameters and praying for success, while helpful in brainstorming, does not constitute a rigorous scientific effort. Against the crashing wave of explorimentation, though, a few brave souls have resisted the urge to feed the beast. Reimers and Gurevych (2017) argue for the reporting of neural network score distributions. Gorman and Bedrick (2019) demonstrate that deterministic dataset splits yield less robust results than random ones for neural networks"
2020.acl-main.246,P19-1267,0,0.0172398,"ntal papers lack empirical rigor, disregarding necessities such as the reporting of statistical significance tests (Dror et al., 2018) and computational environments (Crane, 2018). As Forde and Paganini (2019) concisely lament, explorimentation, the act of tinkering with metaparameters and praying for success, while helpful in brainstorming, does not constitute a rigorous scientific effort. Against the crashing wave of explorimentation, though, a few brave souls have resisted the urge to feed the beast. Reimers and Gurevych (2017) argue for the reporting of neural network score distributions. Gorman and Bedrick (2019) demonstrate that deterministic dataset splits yield less robust results than random ones for neural networks. Dodge et al. (2019) advocate for reporting the expected validation quality as a function of the computation budget used for hyperparameter tuning, which is paramount to robust conclusions. But carefully tread we must. Papers that advocate for scientific rigor must be held to the very same standards that they espouse, lest they birth a new beast altogether. In this work, we critically examine one such paper from Dodge et al. (2019). We acknowledge the validity of their technical contri"
2020.acl-main.246,P82-1020,0,0.789152,"Missing"
2020.acl-main.355,W05-0909,0,0.0480845,"ialized with pre-trained 300-dimensional GloVe embeddings (Pennington et al., 2014), which are frozen during training. We train the model up to 20 epochs with five different seeds and a batch size of 50. We further employ dropout with a probability of 0.1 and 0.3 for data split-1 and split-2, respectively. Moreover, we use the vocabulary set released by Song et al. (2018) for both the encoder and the decoder. During decoding, we perform beam search with a beam size of 20 and a length penalty weight of 1.75. 4.3 Evaluation Following previous work, we use BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-4, and ROUGE-L (Lin, 2004) to evaluate the performance of our model. BLEU and METEOR were originally designed to evaluate machine translation systems, and ROUGE was designed to evaluate text summarization systems. 5 Results and Discussion In this section, we present our experimental results for both neural table-to-text generation and NQG. We report the mean and standard deviation of each metric across multiple seeds to ensure robustness against potentially spurious conclusions (Crane, 2018). In Tables 2 and 3, we compare previous work with our results for NQG and neural table-totext g"
2020.acl-main.355,Q18-1018,0,0.0170396,"Evaluation Following previous work, we use BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-4, and ROUGE-L (Lin, 2004) to evaluate the performance of our model. BLEU and METEOR were originally designed to evaluate machine translation systems, and ROUGE was designed to evaluate text summarization systems. 5 Results and Discussion In this section, we present our experimental results for both neural table-to-text generation and NQG. We report the mean and standard deviation of each metric across multiple seeds to ensure robustness against potentially spurious conclusions (Crane, 2018). In Tables 2 and 3, we compare previous work with our results for NQG and neural table-totext generation, respectively. All results are copied from the original papers except for Liu et al. (2018) in Table 3, where Repl. refers to scores from experiments that we conducted using the source code released by the authors, and Orig. refers to scores taken from the original paper. It is noteworthy that a similar version of our model has served as a baseline in previous papers (Liu et al., 2018; Kim et al., 2019; Liu et al., 2019a). However, the distinctions discussed in Section 2, especially the EM"
2020.acl-main.355,P17-1123,0,0.0707997,"semantic representations of the tables. In this paper, similar to Lebret et al. (2016), we use both content and field information to represent a table by concatenating the field and position embeddings with the word embedding. Unlike Liu et al. (2018), we don’t separate local and global addressing by using specific modules for each, but rather adopt the EMA technique and let the bidirectional model accomplish this implicitly, exploiting the natural advantages of the model. 2.2 Neural Question Generation Previous NQG models can be classified into rulebased and neural-network-based approaches. Du et al. (2017) propose a seq2seq model that is able to achieve better results than previous rule-based systems without taking the target answer into consideration. Zhou et al. (2017) concatenate answer position indicators with the word embeddings to make the model aware of the target answer. They also use lexical features (e.g., POS and NER tags) to enrich their model’s encoder. In addition, Song et al. (2018) suggest using a multi-perspective context matching algorithm to further leverage information from explicit interactions between the passage and the target answer. More recently, Kim et al. (2019) use"
2020.acl-main.355,P16-1154,0,0.373462,"widely used SQuAD dataset (Rajpurkar et al., 2016) for this task. Table 1 presents a sample (passage, answer, question) triple from this dataset. Prior work has made remarkable progress on both of these tasks. However, the proposed models utilize complex neural architectures to capture necessary information from the input(s). In this paper, we question the need for such sophisticated NN models for text generation from inputs comprising structured and unstructured data. Specifically, we adopt a bi-directional, attentionbased seq2seq model (Bahdanau et al., 2015) equipped with a copy mechanism (Gu et al., 2016) for both tasks. We demonstrate that this model, together with the exponential moving average (EMA) technique, achieves the state of the art in both neural table-to-text generation and NQG. Interestingly, our model is able to achieve this result even without using any linguistic features. Our contributions are two-fold: First, we propose a unified NN model for text generation from structured and unstructured data and show that training this model with the EMA technique leads to the state of the art in neural table-to-text generation as well as NQG. Second, because our model is, in essence, the"
2020.acl-main.355,P13-2121,0,0.0606384,"Missing"
2020.acl-main.355,N10-1086,0,0.117695,"Missing"
2020.acl-main.355,D16-1128,0,0.107083,"Missing"
2020.acl-main.355,W04-1013,0,0.0197759,"beddings (Pennington et al., 2014), which are frozen during training. We train the model up to 20 epochs with five different seeds and a batch size of 50. We further employ dropout with a probability of 0.1 and 0.3 for data split-1 and split-2, respectively. Moreover, we use the vocabulary set released by Song et al. (2018) for both the encoder and the decoder. During decoding, we perform beam search with a beam size of 20 and a length penalty weight of 1.75. 4.3 Evaluation Following previous work, we use BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-4, and ROUGE-L (Lin, 2004) to evaluate the performance of our model. BLEU and METEOR were originally designed to evaluate machine translation systems, and ROUGE was designed to evaluate text summarization systems. 5 Results and Discussion In this section, we present our experimental results for both neural table-to-text generation and NQG. We report the mean and standard deviation of each metric across multiple seeds to ensure robustness against potentially spurious conclusions (Crane, 2018). In Tables 2 and 3, we compare previous work with our results for NQG and neural table-totext generation, respectively. All resul"
2020.acl-main.355,N18-2047,1,0.845382,"g target output description. Passage: Hydrogen is commonly used in power stations as a coolant in generators due to a number of favorable properties that are a direct result of its light diatomic molecules. Answer: as a coolant in generators Question: How is hydrogen used at power stations? Table 1: A sample (passage, answer, question) triple from the SQuAD dataset. Introduction Recent NLP literature can be characterized as increasingly complex neural network architectures that eke out progressively smaller gains over previous models. Following a previous line of research (Melis et al., 2018; Mohammed et al., 2018; Adhikari et al., 2019), we investigate the necessity of such complicated neural architectures. In this work, our focus is on text generation from structured and unstructured data by considering description generation from a table and question generation from a passage and a target answer. More specifically, the goal of the neural tableto-text generation task is to generate biographies based on Wikipedia infoboxes (structured data). An infobox is a factual table with a number of fields (e.g., name, nationality, and occupation) describing a person. For this task, we use the W IKI B IO dataset"
2020.acl-main.355,P16-1170,0,0.0212313,"estion from a natural language text and a target answer within it (unstructured data). This is a crucial yet challenging task in NLP that has received growing attention due to its application in improving question answering systems (Duan et al., 2017; Tang et al., 2017, 2018), providing material for educational purposes (Heilman and 3864 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3864–3870 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Smith, 2010), and helping conversational systems to start and continue a conversation (Mostafazadeh et al., 2016). We adopt the widely used SQuAD dataset (Rajpurkar et al., 2016) for this task. Table 1 presents a sample (passage, answer, question) triple from this dataset. Prior work has made remarkable progress on both of these tasks. However, the proposed models utilize complex neural architectures to capture necessary information from the input(s). In this paper, we question the need for such sophisticated NN models for text generation from inputs comprising structured and unstructured data. Specifically, we adopt a bi-directional, attentionbased seq2seq model (Bahdanau et al., 2015) equipped with a c"
2020.acl-main.355,P02-1040,0,0.1091,"5. The word embeddings are initialized with pre-trained 300-dimensional GloVe embeddings (Pennington et al., 2014), which are frozen during training. We train the model up to 20 epochs with five different seeds and a batch size of 50. We further employ dropout with a probability of 0.1 and 0.3 for data split-1 and split-2, respectively. Moreover, we use the vocabulary set released by Song et al. (2018) for both the encoder and the decoder. During decoding, we perform beam search with a beam size of 20 and a length penalty weight of 1.75. 4.3 Evaluation Following previous work, we use BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-4, and ROUGE-L (Lin, 2004) to evaluate the performance of our model. BLEU and METEOR were originally designed to evaluate machine translation systems, and ROUGE was designed to evaluate text summarization systems. 5 Results and Discussion In this section, we present our experimental results for both neural table-to-text generation and NQG. We report the mean and standard deviation of each metric across multiple seeds to ensure robustness against potentially spurious conclusions (Crane, 2018). In Tables 2 and 3, we compare previous work with our result"
2020.acl-main.355,D14-1162,0,0.0908013,"ed as word vocabulary and field vocabulary, respectively, for both the encoder and the decoder. Ultimately, we conduct greedy search to decode a description for a given input table. For the NQG task, we use a two-layer BiLSTM for the encoder and a single-layer LSTM for the decoder. We set the dimension of the LSTM hidden states to 350 and 512 for split-1 and split-2, respectively. Optimization is performed using the AdaGrad optimizer with a learning rate of 0.3 and gradient clipping when its norm exceeds 5. The word embeddings are initialized with pre-trained 300-dimensional GloVe embeddings (Pennington et al., 2014), which are frozen during training. We train the model up to 20 epochs with five different seeds and a batch size of 50. We further employ dropout with a probability of 0.1 and 0.3 for data split-1 and split-2, respectively. Moreover, we use the vocabulary set released by Song et al. (2018) for both the encoder and the decoder. During decoding, we perform beam search with a beam size of 20 and a length penalty weight of 1.75. 4.3 Evaluation Following previous work, we use BLEU-4 (Papineni et al., 2002), METEOR (Banerjee and Lavie, 2005), ROUGE-4, and ROUGE-L (Lin, 2004) to evaluate the perform"
2020.acl-main.355,D16-1264,0,0.0307414,"nstructured data). This is a crucial yet challenging task in NLP that has received growing attention due to its application in improving question answering systems (Duan et al., 2017; Tang et al., 2017, 2018), providing material for educational purposes (Heilman and 3864 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3864–3870 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Smith, 2010), and helping conversational systems to start and continue a conversation (Mostafazadeh et al., 2016). We adopt the widely used SQuAD dataset (Rajpurkar et al., 2016) for this task. Table 1 presents a sample (passage, answer, question) triple from this dataset. Prior work has made remarkable progress on both of these tasks. However, the proposed models utilize complex neural architectures to capture necessary information from the input(s). In this paper, we question the need for such sophisticated NN models for text generation from inputs comprising structured and unstructured data. Specifically, we adopt a bi-directional, attentionbased seq2seq model (Bahdanau et al., 2015) equipped with a copy mechanism (Gu et al., 2016) for both tasks. We demonstrate th"
2020.acl-main.355,N18-2090,0,0.144853,"model accomplish this implicitly, exploiting the natural advantages of the model. 2.2 Neural Question Generation Previous NQG models can be classified into rulebased and neural-network-based approaches. Du et al. (2017) propose a seq2seq model that is able to achieve better results than previous rule-based systems without taking the target answer into consideration. Zhou et al. (2017) concatenate answer position indicators with the word embeddings to make the model aware of the target answer. They also use lexical features (e.g., POS and NER tags) to enrich their model’s encoder. In addition, Song et al. (2018) suggest using a multi-perspective context matching algorithm to further leverage information from explicit interactions between the passage and the target answer. More recently, Kim et al. (2019) use answerseparated seq2seq, which replaces the target answer in the passage with a unique token to avoid using the answer words in the generated question. They also make use of a module called keywordnet to extract critical information from the target answer. Similarly, Liu et al. (2019a) propose using a clue word predictor by adopting graph convolution networks to highlight the imperative aspects o"
2020.acl-main.355,D18-1427,0,0.0361257,"Missing"
2020.acl-main.355,D17-1090,0,0.0182613,"ality, and occupation) describing a person. For this task, we use the W IKI B IO dataset (Lebret et al., 2016) as the benchmark dataset. Figure 1 shows an example of a biographic infobox as well as the target output textual description. Automatic question generation aims to generate a syntactically correct, semantically meaningful and relevant question from a natural language text and a target answer within it (unstructured data). This is a crucial yet challenging task in NLP that has received growing attention due to its application in improving question answering systems (Duan et al., 2017; Tang et al., 2017, 2018), providing material for educational purposes (Heilman and 3864 Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 3864–3870 c July 5 - 10, 2020. 2020 Association for Computational Linguistics Smith, 2010), and helping conversational systems to start and continue a conversation (Mostafazadeh et al., 2016). We adopt the widely used SQuAD dataset (Rajpurkar et al., 2016) for this task. Table 1 presents a sample (passage, answer, question) triple from this dataset. Prior work has made remarkable progress on both of these tasks. However, the propo"
2020.acl-main.355,N18-1141,0,0.0227987,"Missing"
2020.acl-main.355,D18-1424,0,0.0505757,"Missing"
2020.coling-main.307,N19-1423,0,0.163264,"r disambiguation. Furthermore, we verify that our proposed template can be easily extended to other MC tasks with contexts such as supporting facts in open-book question answering settings. Starting from the MC task, this work initiates further research to find generic natural language templates that can effectively leverage stored knowledge in pretrained models. 1 Introduction Scaling and unifying deep neural models via building task-agnostic architectures and pretraining objectives have brought about significant progress in performing various downstream natural language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lewis et al., 2019; Raffel et al., 2020). According to recent findings, impressive performance can be attributed to a sort of internalized implicit “knowledge base” contained in pretrained models (Petroni et al., 2019; Jiang et al., 2019). Since such knowledge is drawn from unstructured and unlabeled text via the design of neural architectures and pretraining objectives, it is non-trivial to identify an effective way to explicitly elicit such stored knowledge for each NLP task. Challenging NLP tasks that require commonsense reasoning abilities (or backgro"
2020.coling-main.307,P19-1475,0,0.0380931,"Missing"
2020.coling-main.307,D18-1260,0,0.0939317,"or T5. (T5) (Raffel et al., 2020) for commonsense reasoning. Specifically, we find that T5 performs surprisingly well by framing multiple-choice (MC) QA problems as ranking multiple text sequences associated with two predefined (and also pretrained) output tokens. Empirically, we demonstrate that without fine-tuning, the pretrained sequence-to-sequence model achieves better-than-random performance on commonsense reasoning tasks. With fine-tuning, our approach yields state-of-theart performance (at the time it was proposed) on three MC datasets: WinoGrande (Sakaguchi et al., 2019), OpenBookQA (Mihaylov et al., 2018), and ARC-Easy (Clark et al., 2018). 2 Approach An approach based on natural language templates enables various options in formulating a verbal reasoning task as a text-to-text problem with T5. Our proposed novel formulation builds on T5’s MNLI template, composed of a hypothesis and a premise.1 Consider a concrete example in WinoGrande: He never comes to my home, but I always go to his house because the _ is smaller. Option1: home; Option2: house In this case, the correct replacement for _ is Option1. We decompose the above problem into two source–target training examples, where _ is replaced"
2020.coling-main.307,2020.findings-emnlp.63,1,0.914866,"ther “contradiction”, (2) one produces “entailment” or “contradiction” and the other some other token, (3) both produce some other tokens, and (4) both produce the same token, either “entailment” or “contradiction.” Ideally, T5 would produce contrastive tokens for each input pair, as in case (1), which allows us to unambiguously select the final answer. However, the model might produce the same tokens for each input, or even tokens not in the predefined set, as in cases (2) to (4). To deal with these cases, we apply a softmax over the logits of the pair of predefined target tokens, similar to Nogueira et al. (2020). From this, we compute the probabilities of the predefined target tokens (in the case of Table 1, “entailment” and “contradiction”). Then, we compare the probabilities of both input instances, and in cases (2) to (4), we select the instance that has a higher probability as the correct answer. 3 Experiments In our experiments, we fine-tune T5-3B on Google Colab’s TPU v2 with a batch size of 16, a learning rate of 2 × 10−4 , save model checkpoints every 5000 steps and choose the checkpoints 1 See appendix D.3 in (Raffel et al., 2020) for more details. 3450 Condition Condition Target token #1 #2"
2020.coling-main.307,D19-1250,0,0.0289058,"o find generic natural language templates that can effectively leverage stored knowledge in pretrained models. 1 Introduction Scaling and unifying deep neural models via building task-agnostic architectures and pretraining objectives have brought about significant progress in performing various downstream natural language understanding tasks (Devlin et al., 2019; Yang et al., 2019; Liu et al., 2019; Lewis et al., 2019; Raffel et al., 2020). According to recent findings, impressive performance can be attributed to a sort of internalized implicit “knowledge base” contained in pretrained models (Petroni et al., 2019; Jiang et al., 2019). Since such knowledge is drawn from unstructured and unlabeled text via the design of neural architectures and pretraining objectives, it is non-trivial to identify an effective way to explicitly elicit such stored knowledge for each NLP task. Challenging NLP tasks that require commonsense reasoning abilities (or background knowledge) could benefit from implicit knowledge captured in pretrained models. Various research lines have explored techniques that effectively leverage reasoning abilities based on BERT (Devlin et al., 2019). For example, Sakaguchi et al. (2019) use"
2020.coling-main.307,D19-1256,0,0.0201996,"tured in pretrained models. Various research lines have explored techniques that effectively leverage reasoning abilities based on BERT (Devlin et al., 2019). For example, Sakaguchi et al. (2019) use a classification head capped on pretrained BERT variants to tackle commonsense reasoning. Zhu et al. (2020) propose to perturb token embeddings to improve model performance and robustness. Alternatively, many researchers recast the multiple-choice (MC) problem setting, which is frequently used in verbal reasoning benchmarks, as a ranking problem regarding statement possibilities (Li et al., 2019; Pirtoaca et al., 2019). Tamborrino et al. (2020) further present a hybrid method that combines full-text inputs, pretrained token representations, and learning-to-rank objectives. Finally, Khashabi et al. (2020) propose to unify and fine-tune pretrained models on many QA tasks jointly. As an alternative to designing task-specific architectures on top of pretrained transformers, we propose a new approach to downstream NLP tasks based on task-specific templates exploiting existing pretrained sequence-to-sequence models. We aim to identify a text-to-text approach that effectively elicits implicit knowledge embedded in"
2020.coling-main.307,2020.acl-main.357,0,0.01095,"ls. Various research lines have explored techniques that effectively leverage reasoning abilities based on BERT (Devlin et al., 2019). For example, Sakaguchi et al. (2019) use a classification head capped on pretrained BERT variants to tackle commonsense reasoning. Zhu et al. (2020) propose to perturb token embeddings to improve model performance and robustness. Alternatively, many researchers recast the multiple-choice (MC) problem setting, which is frequently used in verbal reasoning benchmarks, as a ranking problem regarding statement possibilities (Li et al., 2019; Pirtoaca et al., 2019). Tamborrino et al. (2020) further present a hybrid method that combines full-text inputs, pretrained token representations, and learning-to-rank objectives. Finally, Khashabi et al. (2020) propose to unify and fine-tune pretrained models on many QA tasks jointly. As an alternative to designing task-specific architectures on top of pretrained transformers, we propose a new approach to downstream NLP tasks based on task-specific templates exploiting existing pretrained sequence-to-sequence models. We aim to identify a text-to-text approach that effectively elicits implicit knowledge embedded in the pretrained text-to-te"
2020.coling-main.307,H89-1033,0,0.158711,"in the dataset that have nothing to do with the task; in contrast, the WinoGrande dataset has devoted considerable effort to reducing such biases, which may allow models to (inadvertently) “cheat” (for example, using simple statistical associations). While it is certainly true that datasets over-estimate the reasoning capabilities of modern models (Sakaguchi et al., 2019), there are alternative and complementary explanations as well. It has been a fundamental assumption of the research community that commonsense reasoning is difficult because it comprises tacit rather than explicit knowledge (Winograd, 1972). That is, commonsense knowledge—like water is wet and that a tuba is usually too big to fit in a backpack—is not written down anywhere (unlike, say, factual knowledge, which can be modeled in a knowledge graph). As a result—the reasoning goes—data-driven techniques (even neural models) will be of limited use due to the paucity of relevant corpora. Yet, previous encoder-only architectures (Devlin et al., 2019; Liu et al., 2019) that exploit a language modeling objective (that is, relying only on explicit textual knowledge) can clearly make headway in language reasoning tasks, and we can furthe"
2020.findings-emnlp.249,D19-1352,1,0.94404,"by Google suggests that the company is exploring this approach to improving web search across a number of languages.1 We are inspired by the work of Wu and Dredze (2019), who explored the cross-lingual potential of multi-lingual BERT as a zero-shot language transfer model for NLP tasks such as named-entity recognition and parsing. Mono-lingual BERT models (Devlin et al., 2019) have also proven effective in document retrieval (Dai and Callan, 2019; 1 https://www.blog. google/products/search/ search-language-understanding-bert/ MacAvaney et al., 2019; Li et al., 2020). In particular, Akkalyoncu Yilmaz et al. (2019) demonstrated that BERT models fine-tuned with passage-level relevance data can transfer across domains: surprisingly, fine-tuning on social media data is effective for relevance classification on newswire documents without any additional modifications. Building on these results, we wondered if multi-lingual BERT could enable cross-lingual training of neural document ranking models as well. The contribution of this work is to explore diverse methods to train neural document ranking models cross-lingually. While we are aware of two previous papers along these lines (Shi and Lin, 2019; MacAvaney"
2020.findings-emnlp.249,N19-1423,0,0.015929,"refer to as cross-lingual training. Success in this task would make it easier to develop effective search engines in multiple (potentially lowresource) languages, without gathering expensive relevance judgments in each language. A blog post by Google suggests that the company is exploring this approach to improving web search across a number of languages.1 We are inspired by the work of Wu and Dredze (2019), who explored the cross-lingual potential of multi-lingual BERT as a zero-shot language transfer model for NLP tasks such as named-entity recognition and parsing. Mono-lingual BERT models (Devlin et al., 2019) have also proven effective in document retrieval (Dai and Callan, 2019; 1 https://www.blog. google/products/search/ search-language-understanding-bert/ MacAvaney et al., 2019; Li et al., 2020). In particular, Akkalyoncu Yilmaz et al. (2019) demonstrated that BERT models fine-tuned with passage-level relevance data can transfer across domains: surprisingly, fine-tuning on social media data is effective for relevance classification on newswire documents without any additional modifications. Building on these results, we wondered if multi-lingual BERT could enable cross-lingual training of neura"
2020.findings-emnlp.249,D19-1672,0,0.0543022,"Missing"
2020.findings-emnlp.249,D18-1330,0,0.05525,"Missing"
2020.findings-emnlp.249,2020.emnlp-main.194,0,0.0824726,"Missing"
2020.findings-emnlp.249,D19-1077,0,0.130371,"on This work proposes techniques for leveraging relevance judgments in a source language (English) to train neural models for mono-lingual document retrieval in multiple target (non-English) languages, what we refer to as cross-lingual training. Success in this task would make it easier to develop effective search engines in multiple (potentially lowresource) languages, without gathering expensive relevance judgments in each language. A blog post by Google suggests that the company is exploring this approach to improving web search across a number of languages.1 We are inspired by the work of Wu and Dredze (2019), who explored the cross-lingual potential of multi-lingual BERT as a zero-shot language transfer model for NLP tasks such as named-entity recognition and parsing. Mono-lingual BERT models (Devlin et al., 2019) have also proven effective in document retrieval (Dai and Callan, 2019; 1 https://www.blog. google/products/search/ search-language-understanding-bert/ MacAvaney et al., 2019; Li et al., 2020). In particular, Akkalyoncu Yilmaz et al. (2019) demonstrated that BERT models fine-tuned with passage-level relevance data can transfer across domains: surprisingly, fine-tuning on social media da"
2020.findings-emnlp.63,N19-1423,0,0.0559564,"d based on the probability that each item belongs to the desired class. Applied to the document ranking problem in information retrieval—where given a query, the system’s task is to return a ranked list of documents from a large corpus that maximizes some ranking ∗ Equal contribution. metric such as average precision or nDCG—the simplest formulation is to deploy a classifier that estimates the probability each document belongs to the “relevant” class, and then sort all the candidates by these estimates. Deep transformer models pretrained with language modeling objectives, exemplified by BERT (Devlin et al., 2019), have proven highly effective in a variety of classification and sequence labeling tasks in NLP; Nogueira and Cho (2019) are the first to demonstrate their effectiveness in ranking tasks. Since it is impractical to apply inference to every document in a corpus with respect to a query, these techniques are typically applied to rerank a list of candidates. In a typical end-to-end system, these candidates are taken from the results of a keyword search based on a “classic” IR scoring function such as BM25 (Robertson et al., 1994). This leads to the standard multi-stage pipeline architecture where"
2020.findings-emnlp.63,D19-1224,0,0.0171798,"rations (approx. ten epochs) with class-balanced batches of size 128. We are not able to conduct experiments with T5-11B due to its computational cost. To simplify our training procedure (and related hyperparameters) as well as to eliminate the need for convergence checks, we simply train for a fixed number of iterations, selected based on the computational demands of our largest model and the (self-allotted) time for running experiments. We report results using the model state at the final checkpoint. This procedure is consistent with the advice of Kaplan et al. (2020) and recommendations by Dodge et al. (2019), since we quantify effectiveness for a particular computational budget. We use a maximum of 512 input tokens and two output tokens (one for the target token and another for the end-of-sequence token). In the MS MARCO passage dataset, none of the inputs exceed this length limitation. Training T5 base, large, and 3B take approximately 12, 48, and 160 hours overall, respectively, on a single Google TPU v3-8. For inference, we adopt greedy decoding. Since we only use the logits of the first decoding step, beam search and top-k random sampling (Fan et al., 2018) would give the same results. Becaus"
2020.findings-emnlp.63,D18-2012,0,0.0339711,"t pair. Each candidate document from first-stage retrieval is independently fed to the model, and the final document ranking is simply a permutation of the initial candidate documents based on these estimated probabilities in descending order. Although this trick may seem obvious in retrospect, we are quite certain of its novelty—a lead author of the T5 paper (Raffel), in personal communication, affirmed that the authors never tried anything along these lines before because there was no need for the tasks that they were tackling. Note that T5 tokenizes sequences using the SentencePiece model (Kudo and Richardson, 2018), which might split a word into subwords. We choose target tokens (“true” and “false”) that are represented as single words; thus, each class is represented by a single logit. In the case where target tokens are split in multiple subwords, we would need a method to aggregate their logits into a single score; we thought it best to avoid this complexity. Our formulation naturally begs the question: Why “true” and “false” as the target tokens? We 709 discuss this question in Section 5.4. However, as a preview, we find that the choice of target tokens has a large impact on effectiveness in some ci"
2020.findings-emnlp.63,2020.acl-main.703,0,0.0358778,"re the transfer capabilities of T5 (recall, the model name stands for Text-to-Text Transfer Transformer) by experimenting with zeroshot document ranking on different datasets. To summarize, we fine-tune the model on the MS MARCO passage dataset and directly apply it on three other test collections commonly used by the information retrieval community. This requires a modification to rank long documents at inference time, which we describe below. Finally, while our experiments only examine T5, we note that our method can be used with any other pretrained sequence-to-sequence model such as BART (Lewis et al., 2020), MASS (Song et al., 2019), UniLM (Dong et al., 2019), and Pegasus (Zhang et al., 2020). We leave explorations of these models for future work. 3 3.1 Experimental Setup Datasets We use the following datasets in our experiments: MS MARCO passage (Bajaj et al., 2016) is a ranking dataset with 8.8M passages obtained from Bing search engine results with around 1M natural language questions. Note that for terminological consistency, we refer to each “unit” in the corpus as a document, even though they are in reality paragraph-length passages. The training set contains approximately 530K (query, rel"
2020.findings-emnlp.63,D19-1352,1,0.812404,"pplied to rerank a list of candidates. In a typical end-to-end system, these candidates are taken from the results of a keyword search based on a “classic” IR scoring function such as BM25 (Robertson et al., 1994). This leads to the standard multi-stage pipeline architecture where first-stage retrieval is followed by reranking using one or more machine learning models (Asadi and Lin, 2013; Nogueira et al., 2019a). This architecture underlies nearly all transformerbased approaches to document retrieval today, for example, CEDR (MacAvaney et al., 2019), BERT– MaxP (Dai and Callan, 2019), Birch (Yilmaz et al., 2019), and PARADE (Li et al., 2020). Applying BERT (and its variants) to document ranking can be characterized as a classificationbased encoder-only approach. In contrast, we explore the use of a sequence-to-sequence encoder– decoder architecture—specifically, T5 (Raffel et al., 2020)—to ranking, which requires a trick to coax relevance probabilities out of model-generated “target tokens”. We show that in a data-rich setting, with sufficient training examples, our approach outperforms a classification-based encoder-only model. However, our sequence-to-sequence model appears to be far more data-effi"
2020.findings-emnlp.63,P18-1082,0,\N,Missing
2020.nlpcovid19-acl.2,2020.nlpcovid19-acl.2,1,0.0609097,"aset Edwin Zhang,1 Nikhil Gupta,1 Rodrigo Nogueira,1 Kyunghyun Cho,2,3,4 and Jimmy Lin1 1 David R. Cheriton School of Computer Science, University of Waterloo 2 Courant Institute of Mathematical Sciences, New York University 3 Center for Data Science, New York University 4 CIFAR Associate Fellow This extended abstract represents an abridged version of Zhang et al. (2020a), posted on arXiv April 10, 2020 and concurrently submitted to this workshop. We have intentionally decided for this short piece to reflect the state of our work at that time. The latest updates on our project can be found in Zhang et al. (2020b). The Neural Covidex is a search engine that exploits the latest neural ranking architectures to provide information access to the COVID-19 Open Research Dataset (CORD-19) curated by the Allen Institute for AI (Wang et al., 2020). It exists as part of a suite of tools we have developed to help domain experts tackle the ongoing global pandemic. We hope that improved information access capabilities to the scientific literature can inform evidencebased decision making and insight generation. The first version of CORD-19 was released on March 13, 2020. Within a couple of weeks, our team was able"
2020.nlpcovid19-acl.2,N19-1423,0,\N,Missing
2020.nlpcovid19-acl.2,N19-4013,1,\N,Missing
2020.nlpcovid19-acl.2,D19-1352,1,\N,Missing
2020.nlpcovid19-acl.2,2020.findings-emnlp.63,1,\N,Missing
2020.nlposs-1.9,D19-3016,1,0.899834,"ccurate evaluation of the false positive rate. Next, users (optionally) select which augmentation modules to use, and they train a model with the provided hyperparameters on the selected dataset, which is first processed into log-Mel frames with zero mean and unit variance, as is standard. This training process should take less than a few hours on a GPU-capable device for most use cases, including ours. Finally, users may run the model in the included command line interface demo or deploy it to the browser using Honkling, our inbrowser keyword spotting (KWS) system, if the model is supported (Lee et al., 2019). 3.3 Dev/Test # Par. interprets them as recorded noise for data augmentation, which covers popular noise datasets such as MUSAN (Snyder et al., 2015) and Microsoft SNSD (Reddy et al., 2019). For modeling, Howl provides implementations of convolutional neural networks (CNNs) and recurrent neural networks (RNNs) for wake word detection. These models are from the existing literature, such as residual CNNs (Tang and Lin, 2018), a modified listen–attend–spell (LAS) encoder (Chan et al., 2015; Park et al., 2019), and MobileNetv2 (Sandler et al., 2018). Most of the models are lightweight since the e"
2020.repl4nlp-1.10,P19-1355,0,0.0276868,"the limits of knowledge distillation as we distill BERT’s knowledge all the way down to linear models—a relevant baseline for the task. We report substantial improvement in effectiveness for even the simplest models, as they capture the knowledge learnt by BERT. 1 Introduction Transformer-based (Vaswani et al., 2017) pretrained contextual word embedding models such as BERT (Devlin et al., 2019) and XLNet (Yang et al., 2019) currently power many of the state-of-the-art models across various natural language processing (NLP) tasks. However, these models consume immense computational resources (Strubell et al., 2019). With the surge of such pre-trained models being developed in quick succession, there is a need for effective compression techniques for their inexpensive deployment. Knowledge distillation (KD; Hinton et al. 2015; Ba and Caruana 2014) has been shown to be a fairly straightforward and effective model-agnostic compression method, which transfers knowledge learnt by huge models into more efficient models. In this work, we investigate if BERT-level effectiveness can be achieved by more efficient models using 1. We develop and release* a fine-tuned BERT model (DocBERT), which achieves state-of-th"
2020.repl4nlp-1.10,C18-1330,0,0.0177308,"ly. Document Classification Models Neural network-based models. In recent years neural network-based architectures have dominated the task of document classification. Many researchers (Kim, 2014; Conneau et al., 2017; Johnson and Zhang, 2017) show convolutional neural networks to be effective for classifying single-label short texts. Furthermore, Liu et al. (2017) develop a variant of the popular KimCNN (Kim, 2014), XML-CNN, for addressing the multi-label nature of document classification, which they call extreme classification. Alternatively, others (Yang et al., 2016; Adhikari et al., 2019; Yang et al., 2018) show effective use of recurrent neural networks to exploit semantic representations by treating documents as a sequence of words or sentences for classification. In this work, we explore several neural baseline models and use both LSTM (Hochreiter and Schmidhuber, 1997) and KimCNN architectures for knowledge distillation experiments. Non-neural models. Logistic regression (LR) and support vector machines (SVM) trained on tf–idf vectors form efficient and effective baselines for document classification. Adhikari et al. (2019) show LR and SVM surpass most of the neural baselines on multiple dat"
2020.repl4nlp-1.10,E17-1104,0,0.0164428,"loped to solve the task and highlight the methods that we compare and build upon in this work. 2.1 C N W S Reuters AAPD IMDB Yelp 2014 90 54 10 5 10,789 55,840 135,669 1,125,386 144.3 167.3 393.8 148.8 6.6 1.0 14.4 9.1 Table 1: Summary of the datasets. C denotes the number of classes in the dataset, N the number of samples, and W and S the average number of words and sentences per document, respectively. Document Classification Models Neural network-based models. In recent years neural network-based architectures have dominated the task of document classification. Many researchers (Kim, 2014; Conneau et al., 2017; Johnson and Zhang, 2017) show convolutional neural networks to be effective for classifying single-label short texts. Furthermore, Liu et al. (2017) develop a variant of the popular KimCNN (Kim, 2014), XML-CNN, for addressing the multi-label nature of document classification, which they call extreme classification. Alternatively, others (Yang et al., 2016; Adhikari et al., 2019; Yang et al., 2018) show effective use of recurrent neural networks to exploit semantic representations by treating documents as a sequence of words or sentences for classification. In this work, we explore several ne"
2020.repl4nlp-1.10,N19-1423,0,0.48774,"architectures for knowledge distillation experiments. Non-neural models. Logistic regression (LR) and support vector machines (SVM) trained on tf–idf vectors form efficient and effective baselines for document classification. Adhikari et al. (2019) show LR and SVM surpass most of the neural baselines on multiple datasets, questioning the need for employing neural networks to model syntactic structure for document classification. Here, we explore both LR and SVMs, and we perform knowledge distillation experiments using an LR model. Large-scale pre-training. Recent work (Howard and Ruder, 2018; Devlin et al., 2019; Yang et al., 2019) has demonstrated the effectiveness of largescale pre-training for NLP tasks. In this work, we use BERT as a representative of this approach and demonstrate the power of fine-tuned BERT on document classification (termed DocBERT). 2.2 Dataset approach to model compression, where an efficient student model captures the knowledge learnt by privileged but cumbersome teacher model(s). The knowledge transfer takes place by forcing the student to mimic the soft target probabilities of the teacher. Hinton et al. (2015) highlight that it is in the interest of the generalizability o"
2020.repl4nlp-1.10,N16-1174,0,0.0445421,"rds and sentences per document, respectively. Document Classification Models Neural network-based models. In recent years neural network-based architectures have dominated the task of document classification. Many researchers (Kim, 2014; Conneau et al., 2017; Johnson and Zhang, 2017) show convolutional neural networks to be effective for classifying single-label short texts. Furthermore, Liu et al. (2017) develop a variant of the popular KimCNN (Kim, 2014), XML-CNN, for addressing the multi-label nature of document classification, which they call extreme classification. Alternatively, others (Yang et al., 2016; Adhikari et al., 2019; Yang et al., 2018) show effective use of recurrent neural networks to exploit semantic representations by treating documents as a sequence of words or sentences for classification. In this work, we explore several neural baseline models and use both LSTM (Hochreiter and Schmidhuber, 1997) and KimCNN architectures for knowledge distillation experiments. Non-neural models. Logistic regression (LR) and support vector machines (SVM) trained on tf–idf vectors form efficient and effective baselines for document classification. Adhikari et al. (2019) show LR and SVM surpass m"
2020.repl4nlp-1.10,P18-1031,0,0.0225782,"huber, 1997) and KimCNN architectures for knowledge distillation experiments. Non-neural models. Logistic regression (LR) and support vector machines (SVM) trained on tf–idf vectors form efficient and effective baselines for document classification. Adhikari et al. (2019) show LR and SVM surpass most of the neural baselines on multiple datasets, questioning the need for employing neural networks to model syntactic structure for document classification. Here, we explore both LR and SVMs, and we perform knowledge distillation experiments using an LR model. Large-scale pre-training. Recent work (Howard and Ruder, 2018; Devlin et al., 2019; Yang et al., 2019) has demonstrated the effectiveness of largescale pre-training for NLP tasks. In this work, we use BERT as a representative of this approach and demonstrate the power of fine-tuned BERT on document classification (termed DocBERT). 2.2 Dataset approach to model compression, where an efficient student model captures the knowledge learnt by privileged but cumbersome teacher model(s). The knowledge transfer takes place by forcing the student to mimic the soft target probabilities of the teacher. Hinton et al. (2015) highlight that it is in the interest of t"
2020.repl4nlp-1.10,P17-1052,0,0.0146532,"k and highlight the methods that we compare and build upon in this work. 2.1 C N W S Reuters AAPD IMDB Yelp 2014 90 54 10 5 10,789 55,840 135,669 1,125,386 144.3 167.3 393.8 148.8 6.6 1.0 14.4 9.1 Table 1: Summary of the datasets. C denotes the number of classes in the dataset, N the number of samples, and W and S the average number of words and sentences per document, respectively. Document Classification Models Neural network-based models. In recent years neural network-based architectures have dominated the task of document classification. Many researchers (Kim, 2014; Conneau et al., 2017; Johnson and Zhang, 2017) show convolutional neural networks to be effective for classifying single-label short texts. Furthermore, Liu et al. (2017) develop a variant of the popular KimCNN (Kim, 2014), XML-CNN, for addressing the multi-label nature of document classification, which they call extreme classification. Alternatively, others (Yang et al., 2016; Adhikari et al., 2019; Yang et al., 2018) show effective use of recurrent neural networks to exploit semantic representations by treating documents as a sequence of words or sentences for classification. In this work, we explore several neural baseline models and u"
2020.repl4nlp-1.10,D14-1181,0,0.0376807,"architectural choice have been questioned owing to the effectiveness of simple bag-of-words baselines (Adhikari et al., 2019). We first confirm that a fine-tuned BERT model leads to state-of-the-art model quality by a substantial margin on standard document classification benchmarks. Following this, we investigate the extent to which BERT-level effectiveness can be obtained by various different baselines, combined with KD. We demonstrate, quite surprisingly, that it is possible to apply KD successfully on impoverished student models, such as a single-layer convolutional neural network (CNN) (Kim, 2014) and even linear models. The key contributions of this work are as follows: Fine-tuned variants of BERT are able to achieve state-of-the-art accuracy on many natural language processing tasks, although at significant computational costs. In this paper, we verify BERT’s effectiveness for document classification and investigate the extent to which BERT-level effectiveness can be obtained by different baselines, combined with knowledge distillation—a popular model compression method. The results show that BERTlevel effectiveness can be achieved by a singlelayer LSTM with at least 40× fewer FLOPS"
2020.sdp-1.19,N18-1022,0,0.0435168,"Missing"
2020.sdp-1.19,N19-1423,0,0.00869123,"e language of choice for applied machine learning today in part due to its diverse and mature ecosystem. Pyserini (Yilmaz et al., 2020)2 bridges the gap between the JVM and Python by providing a Python interface to Anserini. Together, Anserini and Pyserini provide basic keyword search capabilities to arbitrary corpora, which include tools to fetch raw document texts as well as utilities to access various term statistics. PyGaggle. As part of Covidex, Zhang et al. (2020b) built PyGaggle,3 a Python library for neural text ranking designed to work with Pyserini. Although the application of BERT (Devlin et al., 2019) to text ranking is well known (Nogueira and Cho, 2019), PyGaggle was designed to showcase models that adopt a novel sequence-to-sequence formulation for ranking (Nogueira et al., 2020b), specifically using T5 (Raffel et al., 2020). Deployed as a relevance classifier that reranks BM25 results from Pyserini, the model is fed a query q and each candidate document d in turn. The model is fine-tuned to produce either “true” or “false” depending on whether the document is relevant or not to the query. At inference time, a softmax is applied to the logits of the “true” and “false” tokens, and the re"
2020.sdp-1.19,2020.acl-main.740,0,0.0301653,"Missing"
2020.sdp-1.19,2020.acl-demos.27,0,0.03331,"t of scholars. Our project does not aspire to be a comprehensive guide to the literature like Google Scholar or AI2’s Semantic Scholar. Rather, our niche is domain-specific “verticals” that are manageable from the scale perspective, yet sufficiently large to support interesting analyses. Our two illustrative case studies of the ACL Anthology and hydrology abstracts fit this bill exactly: our software stack can run with only modest resources, and both these corpora are sufficiently rich and self-contained to answer interesting questions—see, for example, recent analyses of the ACL Anthology by Mohammad (2020) and the work by Rahman et al. (2020) on the collection of hydrology abstracts. ysis. We hope that our open-source approach provides infrastructure that may be useful for other researchers as well. Acknowledgments This research was supported in part by the Natural Sciences and Engineering Research Council (NSERC) of Canada. References 5 Ongoing Work and Conclusions There are two main directions we are currently pursuing as part of ongoing work on Cydex. The first is expansion of the platform to new domains, in particular examining the scalability of our underlying infrastructure. Both the ACL"
2020.sdp-1.19,2020.findings-emnlp.63,1,0.886812,"providing a Python interface to Anserini. Together, Anserini and Pyserini provide basic keyword search capabilities to arbitrary corpora, which include tools to fetch raw document texts as well as utilities to access various term statistics. PyGaggle. As part of Covidex, Zhang et al. (2020b) built PyGaggle,3 a Python library for neural text ranking designed to work with Pyserini. Although the application of BERT (Devlin et al., 2019) to text ranking is well known (Nogueira and Cho, 2019), PyGaggle was designed to showcase models that adopt a novel sequence-to-sequence formulation for ranking (Nogueira et al., 2020b), specifically using T5 (Raffel et al., 2020). Deployed as a relevance classifier that reranks BM25 results from Pyserini, the model is fed a query q and each candidate document d in turn. The model is fine-tuned to produce either “true” or “false” depending on whether the document is relevant or not to the query. At inference time, a softmax is applied to the logits of the “true” and “false” tokens, and the resulting probability of the “true” token is used as the relevance score of d. Candidate documents are then reranked using their relevance scores. Given the lack of COVID-19 training dat"
2020.sdp-1.19,D19-1352,1,0.834482,"iled in Zhang et al. (2020b), the Covidex team submitted the best automatic runs 170 Figure 1: The Cydex search interface on top of the ACL Anthology. On the left, we display all configured facets based on the provided schema, while the right side displays search results using a customizable layout component. in the final two rounds using a combination of techniques that included ranking with T5. Note that the Covidex ranking model operated in a zeroshot setting, and there is independent evidence that transformer-based ranking models have powerful cross-domain relevance transfer capabilities (Yilmaz et al., 2019; MacAvaney et al., 2020). Thus, we hope that the ranking models would generalize to the NLP and hydrology domains as well. Despite the lack of domain-specific evaluation data, we performed our own informal evaluation of Cydex on the ACL Anthology. Our evaluation can be characterized as informal “hallway usability testing”, primarily as a sanity check. We asked four colleagues (who were not the co-authors) to compare the top five results of five different natural language questions between Cydex and the ACL Anthology’s current site-specific Google search. All of our colleagues are familiar wit"
2020.sdp-1.19,2020.nlpcovid19-acl.2,1,0.669563,"g/10.18653/v1/P17 TensorFlow (Abadi et al., 2016), the two most popular neural network toolkits today, and more broadly, Python has emerged as the language of choice for applied machine learning today in part due to its diverse and mature ecosystem. Pyserini (Yilmaz et al., 2020)2 bridges the gap between the JVM and Python by providing a Python interface to Anserini. Together, Anserini and Pyserini provide basic keyword search capabilities to arbitrary corpora, which include tools to fetch raw document texts as well as utilities to access various term statistics. PyGaggle. As part of Covidex, Zhang et al. (2020b) built PyGaggle,3 a Python library for neural text ranking designed to work with Pyserini. Although the application of BERT (Devlin et al., 2019) to text ranking is well known (Nogueira and Cho, 2019), PyGaggle was designed to showcase models that adopt a novel sequence-to-sequence formulation for ranking (Nogueira et al., 2020b), specifically using T5 (Raffel et al., 2020). Deployed as a relevance classifier that reranks BM25 results from Pyserini, the model is fed a query q and each candidate document d in turn. The model is fine-tuned to produce either “true” or “false” depending on wheth"
2020.sdp-1.19,2020.sdp-1.5,1,0.884381,"g/10.18653/v1/P17 TensorFlow (Abadi et al., 2016), the two most popular neural network toolkits today, and more broadly, Python has emerged as the language of choice for applied machine learning today in part due to its diverse and mature ecosystem. Pyserini (Yilmaz et al., 2020)2 bridges the gap between the JVM and Python by providing a Python interface to Anserini. Together, Anserini and Pyserini provide basic keyword search capabilities to arbitrary corpora, which include tools to fetch raw document texts as well as utilities to access various term statistics. PyGaggle. As part of Covidex, Zhang et al. (2020b) built PyGaggle,3 a Python library for neural text ranking designed to work with Pyserini. Although the application of BERT (Devlin et al., 2019) to text ranking is well known (Nogueira and Cho, 2019), PyGaggle was designed to showcase models that adopt a novel sequence-to-sequence formulation for ranking (Nogueira et al., 2020b), specifically using T5 (Raffel et al., 2020). Deployed as a relevance classifier that reranks BM25 results from Pyserini, the model is fed a query q and each candidate document d in turn. The model is fine-tuned to produce either “true” or “false” depending on wheth"
2020.sdp-1.5,W19-5034,0,0.0333516,"ovidex.r5.d2q.1s (= expando + monoT5) r5.fusion2 r5.fusion1 Table 1: Selected TREC-COVID results. Our submissions are under teams “covidex” and “anserini”. All runs notated with † incorporate our infrastructure components in some way. Note that the metrics used in the first three rounds are different from those used in the final two rounds. 37 tion technique proved to be effective: when constructing the keyword query for a given topic, we take the non-stopwords from the query field and further expand them with terms belonging to named entities extracted from the question field using ScispaCy (Neumann et al., 2019). demonstrated our impact not only in developing effective ranking models, but also our service to the community in providing infrastructure. As another point of comparison, UIowaSRun3 (2c) represented a fusion of two traditional (i.e., term-based) relevance feedback runs, and did not use any neural networks. Interestingly, its effectiveness is not very far behind SparseDenseSciBert (2b), the best run in the feedback category (which does take advantage of BERT). It seems that BERTbased methods for exploiting relevance judgments yielded only modest improvements, likely due to the paucity of rel"
2020.sdp-1.5,2020.findings-emnlp.63,1,0.823597,"Missing"
2020.sdp-1.5,D19-1352,1,0.925726,"as follows: each full-text article was segmented into paragraphs and for each paragraph, we created a “document” comprising the title, abstract, and that paragraph. The title and abstract alone comprised an additional “document”. Thus, a fulltext article with n paragraphs yielded n + 1 separate retrieval units in the index. Keyword Search In our design, initial retrieval is performed by the Anserini IR toolkit (Yang et al., 2017, 2018),1 which we have been developing for several years and powers a number of our previous systems that incorporate various neural architectures (Yang et al., 2019; Yilmaz et al., 2019). Anserini represents an effort to better align real-world search applications with academic information retrieval research: under the covers, it builds on the popular and widely-deployed open-source Lucene search library, on top of which we provide a number of 1 To be consistent with standard IR parlance, we call each of these retrieval units a “document”, in a generic sense, despite their composite structure. In addition to the above indexing schemes, we considered three more based on our doc2query document expansion technique (Nogueira et al., 2019b; Nogueira and Lin, 2019). The idea behind"
2020.sdp-1.5,2020.nlpcovid19-acl.2,1,0.236165,"scientific articles (as of October, 2020), including most with full text, about COVID-19 and coronavirus-related research more broadly (for example, SARS and MERS). These articles are gathered from a variety of sources, including PubMed, a curated list of articles from the WHO, as well as preprints from arXiv, bioRxiv, and medRxiv. The goal of the effort is “to mobilize researchers to apply recent advances in natural language process3. Finally, we package the previous two components into Covidex, an end-to-end search engine and browsing interface deployed at covidex.ai, initially described in Zhang et al. (2020a). All three efforts have been successful. In the TRECCOVID challenge, our infrastructure and baselines have been adopted by many teams, which in some 31 Proceedings of the First Workshop on Scholarly Document Processing, pages 31–41 c Online, November 19, 2020. 2020 Association for Computational Linguistics https://doi.org/10.18653/v1/P17 cases have submitted runs that scored higher than our own submissions. This illustrates the success of our infrastructure-building efforts (1). In round 3, we submitted the highest-scoring run that took advantage of previous training data and the secondhigh"
2020.sdp-1.5,N19-4013,1,0.878427,"Missing"
2020.sustainlp-1.11,2020.acl-main.537,0,0.0360741,"(Mitra et al., 2017), KNRM (Xiong et al., 2017), Co-PACRR (Hui et al., 2018) — just to name a few. In recent years, large-scale pretrained language models, especially those based on the transformer architecture (Vaswani et al., 2017), have been applied to IR tasks and have pushed the state of the art even further (Nogueira and Cho, 2019; Dai and Callan, 2019; Yilmaz et al., 2019; Li et al., 2020). The idea of early exiting for neural networks originates from BranchyNet (Teerapittayanon et al., 2017), and is also applied to NLP tasks in several papers (Xin et al., 2020; Schwartz et al., 2020; Liu et al., 2020; Zhou et al., 2020). Our work differs from them by using an early exiting strategy that specializes for document ranking. Another related work that focuses on retrieval is Cascade Transformer (Soldaini and Moschitti, 2020), where a fixed proportion of samples are dropped after each layer. In contrast, our work drops samples based on their scores, and empirically we are able to achieve higher inference speedups. 3 3.2 Our model, early exiting MonoBERT, is a multioutput variant of BERT which enables early exiting. Similar to MonoBERT, we start with a pre-trained BERTBASE model with n transforme"
2020.sustainlp-1.11,2021.ccl-1.108,0,0.145743,"Missing"
2020.sustainlp-1.11,N18-1202,0,0.0106945,"cument ranking are intrinsically different, and it is natural to allocate more computational resources for positive samples. We conduct experiments on BERTBASE with two document ranking datasets, MS MARCO passage (Bajaj et al., 2016) and ASNQ (Garg et al., 2019). We compare against Cascade Transformer (Soldaini and Moschitti, 2020), a recently proposed technique to accelerate inference in BERT-based document ranking. Results show that our method can reduce inference latency by up to 2.5× with minimal effectiveness degradation. Introduction Large scale pre-trained language models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2019), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al., 2019) have brought impressive improvements to natural language processing (NLP) and information retrieval (IR) applications. However, these large-scale models bring to our community not only exciting results, but also concerns about intensive computation demands and high inference latency, especially in real-world deployments. In this paper, we study how to accelerate inference of BERT-based IR models. We follow the framework of MonoBERT (Nogueira and Cho, 2019), which performs binary"
2020.sustainlp-1.11,2020.acl-main.593,0,0.0703644,"Guo et al., 2016), DUET (Mitra et al., 2017), KNRM (Xiong et al., 2017), Co-PACRR (Hui et al., 2018) — just to name a few. In recent years, large-scale pretrained language models, especially those based on the transformer architecture (Vaswani et al., 2017), have been applied to IR tasks and have pushed the state of the art even further (Nogueira and Cho, 2019; Dai and Callan, 2019; Yilmaz et al., 2019; Li et al., 2020). The idea of early exiting for neural networks originates from BranchyNet (Teerapittayanon et al., 2017), and is also applied to NLP tasks in several papers (Xin et al., 2020; Schwartz et al., 2020; Liu et al., 2020; Zhou et al., 2020). Our work differs from them by using an early exiting strategy that specializes for document ranking. Another related work that focuses on retrieval is Cascade Transformer (Soldaini and Moschitti, 2020), where a fixed proportion of samples are dropped after each layer. In contrast, our work drops samples based on their scores, and empirically we are able to achieve higher inference speedups. 3 3.2 Our model, early exiting MonoBERT, is a multioutput variant of BERT which enables early exiting. Similar to MonoBERT, we start with a pre-trained BERTBASE model"
2020.sustainlp-1.11,2020.acl-main.504,0,0.371059,"early; otherwise, the next transformer layer proceeds with the computation. Different from DeeBERT, which treats all classes equally, we use asymmetric early exiting for document ranking: the exiting threshold for positive predictions is higher than for negative ones, since the two classes in document ranking are intrinsically different, and it is natural to allocate more computational resources for positive samples. We conduct experiments on BERTBASE with two document ranking datasets, MS MARCO passage (Bajaj et al., 2016) and ASNQ (Garg et al., 2019). We compare against Cascade Transformer (Soldaini and Moschitti, 2020), a recently proposed technique to accelerate inference in BERT-based document ranking. Results show that our method can reduce inference latency by up to 2.5× with minimal effectiveness degradation. Introduction Large scale pre-trained language models such as ELMo (Peters et al., 2018), GPT (Radford et al., 2019), BERT (Devlin et al., 2019), RoBERTa (Liu et al., 2019), and ALBERT (Lan et al., 2019) have brought impressive improvements to natural language processing (NLP) and information retrieval (IR) applications. However, these large-scale models bring to our community not only exciting res"
2020.sustainlp-1.11,2020.acl-main.204,1,0.793275,"to natural language processing (NLP) and information retrieval (IR) applications. However, these large-scale models bring to our community not only exciting results, but also concerns about intensive computation demands and high inference latency, especially in real-world deployments. In this paper, we study how to accelerate inference of BERT-based IR models. We follow the framework of MonoBERT (Nogueira and Cho, 2019), which performs binary classification on query–document pairs into relevant/non-relevant. To accelerate inference for BERT, we employ the idea of early exiting as in DeeBERT (Xin et al., 2020). In DeeBERT, extra classification layers 83 Proceedings of SustaiNLP: Workshop on Simple and Efficient Natural Language Processing, pages 83–88 c Online, November 20, 2020. 2020 Association for Computational Linguistics 2 Related Work The task of MonoBERT is binary classification: it produces a probability distribution over two classes, relevant and non-relevant. MonoBERT is initialized with a pre-trained BERT model (or other models with a similar architecture such as RoBERTa). A classifier, which is typically a single-layer fully-connected network, is attached to the last transformer layer o"
2020.sustainlp-1.11,D19-1352,1,0.857519,"ments to IR tasks. Throughout this paper, we refer to candidates to be retrieved as documents, although they may actually be passages, answers to a question, etc. Representative neural ranking models include DRMM (Guo et al., 2016), DUET (Mitra et al., 2017), KNRM (Xiong et al., 2017), Co-PACRR (Hui et al., 2018) — just to name a few. In recent years, large-scale pretrained language models, especially those based on the transformer architecture (Vaswani et al., 2017), have been applied to IR tasks and have pushed the state of the art even further (Nogueira and Cho, 2019; Dai and Callan, 2019; Yilmaz et al., 2019; Li et al., 2020). The idea of early exiting for neural networks originates from BranchyNet (Teerapittayanon et al., 2017), and is also applied to NLP tasks in several papers (Xin et al., 2020; Schwartz et al., 2020; Liu et al., 2020; Zhou et al., 2020). Our work differs from them by using an early exiting strategy that specializes for document ranking. Another related work that focuses on retrieval is Cascade Transformer (Soldaini and Moschitti, 2020), where a fixed proportion of samples are dropped after each layer. In contrast, our work drops samples based on their scores, and empirically"
2020.sustainlp-1.14,N19-1423,0,0.0408739,"fine-tuned with the large MS MARCO passage retrieval dataset. We arrive at the surprising and novel finding that “some” labeled in-domain data can be worse than none at all. 1 Introduction Given a corpus C comprised of an arbitrary number of texts, the goal of the retrieval task is to generate a ranked list of k results for a user query q that maximizes some metric of interest. Texts can differ in length: if the corpus is comprised of paragraphlength segments, the task is referred to as passage retrieval. Otherwise, information retrieval (IR) researchers use the term document retrieval. BERT (Devlin et al., 2019) has been successfully applied to the passage retrieval task by using it as a relevance classifier that reranks an initial list of candidate results (Nogueira and Cho, 2019), which are retrieved using bag-of-words queries and efficient exact-match scoring techniques such as BM25. As passages are usually shorter than the 512 token input length limit of BERT, this solution is straightforward. Even in cases where the candidate text exceeds this length limitation, Dai and Callan (2019) showed that simply taking the bestscoring passage from a longer document as a proxy for the document score is an"
2020.sustainlp-1.14,D19-6109,0,0.0518871,"ata are needed to obtain further improvements. Nogueira et al. (2020) demonstrated this on the MS MARCO passage ranking dataset. However, to the best of our knowledge, no previous work has investigated the effect of training data size for traditional TREC-style datasets on BERT-based models, which are smaller than MS MARCO by orders of magnitude. Beyond ranking tasks, methods for tackling the limited labeled data issue using transfer learning have also been investigated. Rietzler et al. (2019) conducted supervised learning on the source dataset and unsupervised learning on the target dataset. Ma et al. (2019) employed adversarial learning to generate pseudo-labels for target datasets. Interestingly, both papers reported that directly transferring knowledge learned from a supervised out-of-domain dataset or unsupervised in-domain dataset to a target domain consistently underperforms supervised in-domain training without the intervention of special techniques (e.g., adversarial regularization). 3 Methodology In order to analyze the impact of fine-tuning a BERT ranking model with limited training data, we sample standard benchmark datasets to simulate having less data available. Rather than proposing"
2020.sustainlp-1.14,2020.findings-emnlp.63,1,0.878588,"s the question of how to construct “passage-level” relevance labels from document-level labels. Dai and Callan (2019) proposed the simple strategy of giving all passages the same label as the document (at training time) and aggregating passage scores (at inference time). Their most effective approach, BERT–MaxP, uses the maximum passage score as the document score at inference (ranking) time. Alternatively, this obstacle can be entirely avoided with a zero-shot approach: the model is fine-tuned on a passage retrieval dataset and then directly applied to the target corpus (Yilmaz et al., 2019; Nogueira et al., 2020). For example, Yilmaz et al. (2019) found that when BERT was fine-tuned on a combination of (out-of-domain) datasets, the model exhibited state-of-the-art effectiveness (at the time) on Robust04. Nogueira et al. (2020) confirmed this finding and further improved zero-shot effectiveness on Robust04 by fine-tuning T5 (Raffel et al., 2020) on the MS MARCO passage dataset (Bajaj et al., 2018). Cohen et al. (2018) investigated the use of adversarial regularization to prevent pre-BERT neural models from learning representations closely tied to a specific domain. They found that training on a dataset"
2020.sustainlp-1.14,P19-3007,0,0.0150653,"same. In other words, it is not the case that we regularly reach peak validation effectiveness earlier when fine-tuning with fewer judgments. 110 6 Figure 2: Attention visualization from three BERT models, where “w/o MS” and “w/ MS” indicate without and with MS MARCO fine-tuning, respectively. Colors refer to different attention heads. Deeper colors indicate stronger attention. All attention connections are from layer 8 to layer 9. Attention visualizations. To investigate how MS MARCO increases effectiveness in other domains, we visualize attention from three models using the BertViz toolkit (Vig, 2019): a zero-shot model, a model fine-tuned on GOV2 directly, and a model fine-tuned on GOV2 after first fine-tuning on MS MARCO. Figure 2 compares the attention that the query term “reclamation” received in each model when predicting the relevance between the query “abandoned mine reclamation” and the text “healinglandwater site navigation healing the land and water pennsylvania’s abandoned mine reclamation program”. Both the query and the text snippet are from GOV2. The attention map visualizes interactions from layer 8 to layer 9 in the models. Conclusions This paper shows that, on two TREC col"
2020.sustainlp-1.14,D19-1352,1,0.835359,"his immediately raises the question of how to construct “passage-level” relevance labels from document-level labels. Dai and Callan (2019) proposed the simple strategy of giving all passages the same label as the document (at training time) and aggregating passage scores (at inference time). Their most effective approach, BERT–MaxP, uses the maximum passage score as the document score at inference (ranking) time. Alternatively, this obstacle can be entirely avoided with a zero-shot approach: the model is fine-tuned on a passage retrieval dataset and then directly applied to the target corpus (Yilmaz et al., 2019; Nogueira et al., 2020). For example, Yilmaz et al. (2019) found that when BERT was fine-tuned on a combination of (out-of-domain) datasets, the model exhibited state-of-the-art effectiveness (at the time) on Robust04. Nogueira et al. (2020) confirmed this finding and further improved zero-shot effectiveness on Robust04 by fine-tuning T5 (Raffel et al., 2020) on the MS MARCO passage dataset (Bajaj et al., 2018). Cohen et al. (2018) investigated the use of adversarial regularization to prevent pre-BERT neural models from learning representations closely tied to a specific domain. They found th"
2021.acl-long.84,2020.argmining-1.7,0,0.0164587,"r they are always reliable, as pointed out by Guo et al. (2017) that modern neural networks, while having better accuracy, tend to be overconfident compared to simple networks from 20 years ago. In this paper, we study the problem of selective prediction (Geifman and El-Yaniv, 2017) in NLP. Under the setting of selective prediction, a model is allowed to abstain from making predictions on uncertain examples (Figure 1) and thereby reduce the error rate. This is a practical setting in a lot of realistic scenarios, such as making entailment judgments for breaking news articles in search engines (Carlebach et al., 2020) and making critical predictions in medical and legal documents (Zhang et al., 2019). In these cases, it is totally acceptable, if not desirable, for the models to admit their uncertainty and call for help from humans or better (but more costly) models. Under the selective prediction setting, we construct a selective classifier by pairing a standard classifier with a confidence estimator. The confidence estimator measures how confident the model is for a certain example, and instructs the classifier to abstain on uncertain ones. Naturally, a good confidence estimator should have higher confide"
2021.acl-long.84,P18-1009,0,0.0159755,"fore, we can conclude only that the error regularization trick improves selective prediction, but the best specific method varies. We leave this exploration for future work. 5.4 The No-Answer Problem In this section, we conduct experiments to see how selective classifiers perform on datasets that either allow abstention or, equivalently, provide the no-answer label. This no-answer problem occurs whenever a trained classifier encounters an example whose label is unseen in training, which is common in practice. For example, in the setting of ultrafine entity typing with more than 10,000 labels (Choi et al., 2018), it is unsurprising to encounter examples with unseen types. Ideally, in this case, the classifier should choose the no-answer label. This setting is important yet often neglected, and there exist few classification datasets with the no-answer label. We therefore build our own datasets, binarized MNLI and SST-5 (bMNLI and bSST-5), to evaluate different models in this setting (Table 2). The MNLI dataset is for sentence entailment classification. Given a pair of sentences, the goal is to predict the relationship between them, among three labels: entailment, contradiction, and neutral. The SST-5"
2021.acl-long.84,2020.emnlp-main.21,0,0.0172259,"citly estimate confidence in semantic parsing. Kamath et al. (2020) introduce selective prediction for OOD question answering, where abstention is allowed for OOD and difficult questions. However, selective prediction for broader NLP applications has yet to be explored, and we hope to draw the attention of the NLP community to this problem. There are two notable related topics, confidence calibration and unanswerable questions, but the difference between them and selective prediction is still nontrivial. Calibration (Guo et al., 2017; Jiang et al., 2018; Kumar et al., 2018; Wang et al., 2020; Desai and Durrett, 2020) focuses on adjusting the overall confidence level of a model, while selective prediction is based on relative confidence among the examples. For example, the most widely used calibration technique, temperature scaling (Platt, 1999), globally increases or decreases the model’s confidence on all examples, but the ranking of all examples’ confidence is unchanged. Unanswerable questions are considered in previous datasets, e.g., SQuAD2.0 (Rajpurkar et al., 2018). The unanswerable questions are impossible to answer even for humans, while abstention in selective prediction is due to model uncertain"
2021.acl-long.84,N19-1423,0,0.0961348,"en many different methods for confidence estimation. Bayesian methods such as Markov Chain Monte Carlo (Geyer, 1992) and Variational Inference (Hinton and Van Camp, 1993; Graves, 2011) assume a prior distribution over model parameters and obtain confidence estimates through the posterior. Ensemble-based methods (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Geifman et al., 2019) estimate confidence based on statistics of the ensemble model’s output. These methods, however, are computationally practical for small models only. Current large-scale pre-trained NLP models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), are too expensive to run multiple times of inference, and therefore require lightweight confidence estimation. Previously, selective prediction and confidence estimation have been studied in limited NLP scenarios. Dong et al. (2018) train a separate confidence scoring model to explicitly estimate confidence in semantic parsing. Kamath et al. (2020) introduce selective prediction for OOD question answering, where abstention is allowed for OOD and difficult questions. However, selective prediction for broader NLP applications has yet to be explored, and we hope t"
2021.acl-long.84,I05-5002,0,0.0155227,"s; (2) ALBERT-base (Lan et al., 2020), a variant of BERT featuring parameter sharing and memory efficiency; (3) Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997), the popular pre-transformer model that is lightweight and fast. In this section, we compare the performance of selective prediction of these models, demonstrate the effectiveness of the proposed error regularization, and show the application of selective prediction in two interesting scenarios—the no-answer problem and the classifier cascades. 5.1 Experiment Setups We conduct experiments mainly on three datasets: MRPC (Dolan and Brockett, 2005), QNLI (Wang et al., 2018), and MNLI (Williams et al., 2018). In Section 5.4, we will need an additional non-binary dataset SST-5 (Socher et al., 2013). Statistics of these datasets can be found in Table 2. Following the setting of the GLUE benchmark (Wang et al., 2018), we use the training set for training/finetuning and the development set for evaluation (the test set’s labels are not publicly available); MNLI’s development set has two parts, matched and mismatched (m/mm). These datasets include semantic equivalence judgments, entailment classification, and sentiment analysis, which are impo"
2021.acl-long.84,P82-1020,0,0.661256,"Missing"
2021.acl-long.84,P18-1069,0,0.0274317,"through the posterior. Ensemble-based methods (Gal and Ghahramani, 2016; Lakshminarayanan et al., 2017; Geifman et al., 2019) estimate confidence based on statistics of the ensemble model’s output. These methods, however, are computationally practical for small models only. Current large-scale pre-trained NLP models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), are too expensive to run multiple times of inference, and therefore require lightweight confidence estimation. Previously, selective prediction and confidence estimation have been studied in limited NLP scenarios. Dong et al. (2018) train a separate confidence scoring model to explicitly estimate confidence in semantic parsing. Kamath et al. (2020) introduce selective prediction for OOD question answering, where abstention is allowed for OOD and difficult questions. However, selective prediction for broader NLP applications has yet to be explored, and we hope to draw the attention of the NLP community to this problem. There are two notable related topics, confidence calibration and unanswerable questions, but the difference between them and selective prediction is still nontrivial. Calibration (Guo et al., 2017; Jiang et"
2021.acl-long.84,2020.acl-main.503,0,0.182918,"l., 2019) estimate confidence based on statistics of the ensemble model’s output. These methods, however, are computationally practical for small models only. Current large-scale pre-trained NLP models, such as BERT (Devlin et al., 2019) and RoBERTa (Liu et al., 2019), are too expensive to run multiple times of inference, and therefore require lightweight confidence estimation. Previously, selective prediction and confidence estimation have been studied in limited NLP scenarios. Dong et al. (2018) train a separate confidence scoring model to explicitly estimate confidence in semantic parsing. Kamath et al. (2020) introduce selective prediction for OOD question answering, where abstention is allowed for OOD and difficult questions. However, selective prediction for broader NLP applications has yet to be explored, and we hope to draw the attention of the NLP community to this problem. There are two notable related topics, confidence calibration and unanswerable questions, but the difference between them and selective prediction is still nontrivial. Calibration (Guo et al., 2017; Jiang et al., 2018; Kumar et al., 2018; Wang et al., 2020; Desai and Durrett, 2020) focuses on adjusting the overall confidenc"
2021.acl-long.84,2021.ccl-1.108,0,0.0337254,"Missing"
2021.acl-long.84,P18-2124,0,0.0166096,"and selective prediction is still nontrivial. Calibration (Guo et al., 2017; Jiang et al., 2018; Kumar et al., 2018; Wang et al., 2020; Desai and Durrett, 2020) focuses on adjusting the overall confidence level of a model, while selective prediction is based on relative confidence among the examples. For example, the most widely used calibration technique, temperature scaling (Platt, 1999), globally increases or decreases the model’s confidence on all examples, but the ranking of all examples’ confidence is unchanged. Unanswerable questions are considered in previous datasets, e.g., SQuAD2.0 (Rajpurkar et al., 2018). The unanswerable questions are impossible to answer even for humans, while abstention in selective prediction is due to model uncertainty rather than modelagnostic data uncertainty. 3 Background We introduce relevant concepts about selective prediction and confidence estimators, using multiclass classification as an example. 3.1 Selective Prediction Given a feature space X and a set of labels Y, a standard classifier f is a function f : X → Y. A selective classifier is another function h : X → Y ∪ {⊥}, where ⊥ is a special label indicating the abstention of prediction. Normally, the selectiv"
2021.acl-long.84,2020.acl-main.593,0,0.0255807,"Missing"
2021.acl-long.84,D13-1170,0,0.00286485,"Schmidhuber, 1997), the popular pre-transformer model that is lightweight and fast. In this section, we compare the performance of selective prediction of these models, demonstrate the effectiveness of the proposed error regularization, and show the application of selective prediction in two interesting scenarios—the no-answer problem and the classifier cascades. 5.1 Experiment Setups We conduct experiments mainly on three datasets: MRPC (Dolan and Brockett, 2005), QNLI (Wang et al., 2018), and MNLI (Williams et al., 2018). In Section 5.4, we will need an additional non-binary dataset SST-5 (Socher et al., 2013). Statistics of these datasets can be found in Table 2. Following the setting of the GLUE benchmark (Wang et al., 2018), we use the training set for training/finetuning and the development set for evaluation (the test set’s labels are not publicly available); MNLI’s development set has two parts, matched and mismatched (m/mm). These datasets include semantic equivalence judgments, entailment classification, and sentiment analysis, which are important application scenarios for selective prediction as discussed in Section 1. The implementation is based on PyTorch (Paszke et al., 2019) and the Hu"
2021.acl-long.84,W18-5446,0,0.0224338,"Missing"
2021.acl-long.84,2020.acl-main.278,0,0.0283734,"ring model to explicitly estimate confidence in semantic parsing. Kamath et al. (2020) introduce selective prediction for OOD question answering, where abstention is allowed for OOD and difficult questions. However, selective prediction for broader NLP applications has yet to be explored, and we hope to draw the attention of the NLP community to this problem. There are two notable related topics, confidence calibration and unanswerable questions, but the difference between them and selective prediction is still nontrivial. Calibration (Guo et al., 2017; Jiang et al., 2018; Kumar et al., 2018; Wang et al., 2020; Desai and Durrett, 2020) focuses on adjusting the overall confidence level of a model, while selective prediction is based on relative confidence among the examples. For example, the most widely used calibration technique, temperature scaling (Platt, 1999), globally increases or decreases the model’s confidence on all examples, but the ranking of all examples’ confidence is unchanged. Unanswerable questions are considered in previous datasets, e.g., SQuAD2.0 (Rajpurkar et al., 2018). The unanswerable questions are impossible to answer even for humans, while abstention in selective prediction"
2021.acl-long.84,N18-1101,0,0.0112747,"ing parameter sharing and memory efficiency; (3) Long Short-Term Memory (LSTM; Hochreiter and Schmidhuber, 1997), the popular pre-transformer model that is lightweight and fast. In this section, we compare the performance of selective prediction of these models, demonstrate the effectiveness of the proposed error regularization, and show the application of selective prediction in two interesting scenarios—the no-answer problem and the classifier cascades. 5.1 Experiment Setups We conduct experiments mainly on three datasets: MRPC (Dolan and Brockett, 2005), QNLI (Wang et al., 2018), and MNLI (Williams et al., 2018). In Section 5.4, we will need an additional non-binary dataset SST-5 (Socher et al., 2013). Statistics of these datasets can be found in Table 2. Following the setting of the GLUE benchmark (Wang et al., 2018), we use the training set for training/finetuning and the development set for evaluation (the test set’s labels are not publicly available); MNLI’s development set has two parts, matched and mismatched (m/mm). These datasets include semantic equivalence judgments, entailment classification, and sentiment analysis, which are important application scenarios for selective prediction as disc"
2021.acl-long.84,2021.eacl-main.8,1,0.805779,"Missing"
2021.acl-long.84,2020.acl-main.204,1,0.883105,"Missing"
2021.acl-long.84,N19-1316,0,0.028038,"rks, while having better accuracy, tend to be overconfident compared to simple networks from 20 years ago. In this paper, we study the problem of selective prediction (Geifman and El-Yaniv, 2017) in NLP. Under the setting of selective prediction, a model is allowed to abstain from making predictions on uncertain examples (Figure 1) and thereby reduce the error rate. This is a practical setting in a lot of realistic scenarios, such as making entailment judgments for breaking news articles in search engines (Carlebach et al., 2020) and making critical predictions in medical and legal documents (Zhang et al., 2019). In these cases, it is totally acceptable, if not desirable, for the models to admit their uncertainty and call for help from humans or better (but more costly) models. Under the selective prediction setting, we construct a selective classifier by pairing a standard classifier with a confidence estimator. The confidence estimator measures how confident the model is for a certain example, and instructs the classifier to abstain on uncertain ones. Naturally, a good confidence estimator should have higher confidence for correctly classified examples than incorrect ones. We consider two choices o"
2021.acl-short.51,D19-1352,1,0.857247,"Missing"
2021.acl-short.51,W18-5521,0,0.0339137,"Missing"
2021.acl-short.51,P17-1171,0,0.0619387,"Missing"
2021.acl-short.51,P17-1152,0,0.0880552,"Missing"
2021.acl-short.51,W18-5516,0,0.0919058,"a in April and May 1992. Evidence 2 (wiki/Los Angeles County): Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA. Label: S UPPORTS 3.1 Figure 1: An example claim and its corresponding evidence and label from the FEVER dataset. By construction, the veracity of each claim is determined by the (candidate) supporting sentences, taken together. One simple and popular approach to fact extraction and verification is to consider the veracity of the claim with respect to each candidate independently (i.e., classification), and then aggregate the evidence (Hanselowski et al., 2018; Zhou et al., 2019; Soleimani et al., 2019; Liu et al., 2020; Pradeep et al., 2021b). For convenience, we refer to these as “pointwise approaches”, borrowing from the learning to rank literature (Li, 2011). As an alternative, researchers have proposed approaches that consider multiple candidates at once to jointly arrive at a veracity prediction (Thorne et al., 2018; Nie et al., 2019a; Zhou et al., 2019; Stammbach and Neumann, 2019; Pradeep et al., 2021a). For convenience, we refer to these as “listwise approaches”, also borrowing from the learning to rank literature (Li, 2011). Such listwise"
2021.acl-short.51,D19-1258,0,0.0590758,"Missing"
2021.acl-short.51,2020.findings-emnlp.63,1,0.921438,"lternating through the two ranked lists of documents, skipping duplicates and keeping the top K unique documents. 3.2 Sentence Selection ˆ Given a claim q and retrieved documents D(q), the next stage in the pipeline selects the top ˆ = L most relevant evidence sentences S(q) {sk1 ,i1 , ..., skL ,iL }, where sk,i is the i-th sentence from document dk . Similar to how Soleimani et al. (2019) and Subramanian and Lee (2020) frame this stage as a semantic matching problem using BERT-based models, we use T5 to rank the similarities between the claim and the sentences in each document. Introduced by Nogueira et al. (2020), like Pradeep et al. (2021a), we use T5 (Raffel et al., 2020) as a pointwise reranker, which they dub monoT5. Empirically, T5 has been found to be more effective at ranking than BERT-based models across a wide variety of domains. As a sequence-to-sequence model, ranking is performed using the following input template: Query: q Document: sk,i Relevant: where q and sk,i are the claim and evidence sentence, respectively. To provide a broader context and to resolve ambiguities, we prepend each sentence sk,i with the title of document dk . We fine-tune the model to generate the token “true” if sk,"
2021.acl-short.51,2021.louhi-1.11,1,0.89417,"cially the County of Los Angeles, is the most populous county in the USA. Label: S UPPORTS 3.1 Figure 1: An example claim and its corresponding evidence and label from the FEVER dataset. By construction, the veracity of each claim is determined by the (candidate) supporting sentences, taken together. One simple and popular approach to fact extraction and verification is to consider the veracity of the claim with respect to each candidate independently (i.e., classification), and then aggregate the evidence (Hanselowski et al., 2018; Zhou et al., 2019; Soleimani et al., 2019; Liu et al., 2020; Pradeep et al., 2021b). For convenience, we refer to these as “pointwise approaches”, borrowing from the learning to rank literature (Li, 2011). As an alternative, researchers have proposed approaches that consider multiple candidates at once to jointly arrive at a veracity prediction (Thorne et al., 2018; Nie et al., 2019a; Zhou et al., 2019; Stammbach and Neumann, 2019; Pradeep et al., 2021a). For convenience, we refer to these as “listwise approaches”, also borrowing from the learning to rank literature (Li, 2011). Such listwise approaches have also been used for information aggregation in other NLP tasks such"
2021.acl-short.51,2020.acl-main.655,0,0.0242282,"geles County, officially the County of Los Angeles, is the most populous county in the USA. Label: S UPPORTS 3.1 Figure 1: An example claim and its corresponding evidence and label from the FEVER dataset. By construction, the veracity of each claim is determined by the (candidate) supporting sentences, taken together. One simple and popular approach to fact extraction and verification is to consider the veracity of the claim with respect to each candidate independently (i.e., classification), and then aggregate the evidence (Hanselowski et al., 2018; Zhou et al., 2019; Soleimani et al., 2019; Liu et al., 2020; Pradeep et al., 2021b). For convenience, we refer to these as “pointwise approaches”, borrowing from the learning to rank literature (Li, 2011). As an alternative, researchers have proposed approaches that consider multiple candidates at once to jointly arrive at a veracity prediction (Thorne et al., 2018; Nie et al., 2019a; Zhou et al., 2019; Stammbach and Neumann, 2019; Pradeep et al., 2021a). For convenience, we refer to these as “listwise approaches”, also borrowing from the learning to rank literature (Li, 2011). Such listwise approaches have also been used for information aggregation i"
2021.acl-short.51,N18-1074,0,0.0987791,"Missing"
2021.acl-short.51,2020.emnlp-main.582,0,0.0246478,"Missing"
2021.acl-short.51,W18-5515,0,0.0378376,"Missing"
2021.acl-short.51,2020.sdp-1.5,1,0.850534,"Missing"
2021.acl-short.51,2020.acl-main.549,0,0.0564519,"Missing"
2021.acl-short.51,P19-1085,0,0.0758933,"vidence 2 (wiki/Los Angeles County): Los Angeles County, officially the County of Los Angeles, is the most populous county in the USA. Label: S UPPORTS 3.1 Figure 1: An example claim and its corresponding evidence and label from the FEVER dataset. By construction, the veracity of each claim is determined by the (candidate) supporting sentences, taken together. One simple and popular approach to fact extraction and verification is to consider the veracity of the claim with respect to each candidate independently (i.e., classification), and then aggregate the evidence (Hanselowski et al., 2018; Zhou et al., 2019; Soleimani et al., 2019; Liu et al., 2020; Pradeep et al., 2021b). For convenience, we refer to these as “pointwise approaches”, borrowing from the learning to rank literature (Li, 2011). As an alternative, researchers have proposed approaches that consider multiple candidates at once to jointly arrive at a veracity prediction (Thorne et al., 2018; Nie et al., 2019a; Zhou et al., 2019; Stammbach and Neumann, 2019; Pradeep et al., 2021a). For convenience, we refer to these as “listwise approaches”, also borrowing from the learning to rank literature (Li, 2011). Such listwise approaches have al"
2021.acl-srw.16,K19-1079,0,0.138852,"NL) in the vocabulary. Thus, the NL can be treated as the end-of-paragraph during fine-tuning (Mao et al., 2019). Experimental results show that learning to end the paragraph can benefit the word/token perplexity, BLUE score, E OS perplexity, and human evaluated ending quality score. Introduction Large-pretrained neural models such as GPT (Radford, 2018) and BERT (Devlin et al., 2019) have achieved the state-of-the-art on many NLP tasks. Among these models, OpenAI’s GPT2 (Radford et al., 2019), for example, has shown to be capable of generating long fluent text in many areas, such as stories (See et al., 2019), recipes (H. Lee et al., 2020), patent claims (Lee and Hsiang, 2019), and news (Zellers et al., 2019). However, the semantics of a text goes beyond what’s written to what’s not written: When to break paragraphs and when to end. We wish to experiment on this issue: How much do E OP and E OS markers affect our ability to generate texts with GPT2. To study the strength of GPT2 as a language generator, See et al. (2019) conduct experiments in the context of story generation with the WritingPrompts (Fan et al., 2018) dataset. They find that 1 https://github.com/rsvp-ai/semantic_ unwritten Our cont"
2021.acl-srw.16,P15-1152,0,0.0252162,"er, not every paragraph ends with NL, and during the pre-training, not every NL represents the paragraph separator (S EP) . A better option is to append a new specific token E OP to indicate the end of the paragraph: w1:T = p,1 , ..., p,n−1 , p,n , E OS where p,i = {wbi :ei , E OP}. Then, each paragraph can end with the E OP and the transformer-based language model can learn this feature with every paragraph in the training data, without distinguishing when to generate E OP and when not to. It is well known that greedy decoding and beam search usually lead to repetitive and degenerate outputs(Shang et al., 2015; Massarelli et al., 2019). Sampling-based decoding methods have shown a strong ability in generating diversity, fluency and repetitiveness of the generation with pre-trained language models, such as top-k and top-p sampling. In this work, we choose the top-p sampling decoding algorithm and set the p equals to 0.95. 3 3.1 Dataset Language #Train samples #Test samples #Validation samples #Avg. words per sample #Avg. paragraphs per sample Background Experiments Datasets Story Generation. The story generation dataset is the WritingPrompts, collected by Fan et al. (2018) Story English 199,083 11,0"
2021.blackboxnlp-1.39,D19-1445,0,0.104712,"he prototypical, most-interpreted trans- regard to prediction. However, Sixt et al. (2020) show that most of the modified back-propagation former model. As this work specifically explores methods fail a basic sanity check: invariance to passage reranking, we also provide the necessary parameter randomization and label randomization. literature about recent progress. LIME (Ribeiro et al., 2016) is not even limited to 2.1 BERT specific differentiable models. They use interpretable modA number of works investigate the inner mecha- els like decision trees to approximate deep neural nisms of BERT. Kovaleva et al. (2019); Clark et al. networks, and thus can theoretically interpret any (2019) carefully analyze BERT’s attention heads, classifier. However, empirically, LIME’s high demand on memory may worsen its quality compared noting positive correlation between attention heads to other methods, as we will see in the later section. and linguistic features, as well as special tokens. Looking at attention, Voita et al. (2018) find Information-theoretic methods are often unconthat BERT captures anaphora and dependence on strained by tasks and models as well, while adposition and length in machine translation. Poi"
2021.blackboxnlp-1.39,2021.naacl-tutorials.1,1,0.815292,"Missing"
2021.blackboxnlp-1.39,N19-1112,0,0.0607683,"Missing"
2021.blackboxnlp-1.39,2020.tacl-1.54,0,0.0458001,"Missing"
2021.blackboxnlp-1.39,P19-1282,0,0.061447,"Missing"
2021.eacl-main.307,W04-1013,0,0.0643547,"apability. That is, no previous work has addressed this problem—and thus, points of comparison are limited. Further note that the point of our technique is not to establish state-of-the-art performance on these various datasets, but rather to illustrate our tagging feature. Nevertheless, it is worth noticing that our model outperforms the previous stateof-the-art paraphrase generation model (Fu et al., 2019) on MSCOCO and QQP datasets in terms of ROUGE scores. We only present the results of ROUGE 2-gram scores for brevity, but the results of other ROUGE scores are consistent. We choose ROUGE (Lin, 2004) instead of BLEU (Papineni et al., 2002) as our evaluation metrics for both tagger performance and diversity evaluation because of the diversity loss that we introduced. In an ideal dataset, the references of a source sentence should be able to cover all possible paraphrases. In this case, precision-based metrics like BLEU should be an even more suitable evaluation metric of generated paraphrases. However, the datasets we experimented on only contain a handful of references, far from covering all possibilities. The substituted words and diversified descriptions encouraged by our minimized mutu"
2021.eacl-main.307,2020.tacl-1.47,0,0.0190766,"entirely language agnostic, and we report experiments in both English and Chinese. Additionally, we modify the loss during fine-tuning to explicitly encourage diversity in the paraphrase generation process. 1 This is a feature requested by many of our customers. 3522 Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics, pages 3522–3527 April 19 - 23, 2021. ©2021 Association for Computational Linguistics 2 Paraphrase Generator 2.2 We treat paraphrase generation as a standard supervised sequence-to-sequence task by fine-tuning mBART-large (Liu et al., 2020). Although our technique can be applied to any sequence-to-sequence framework, we selected mBART because a multilingual model is widely available. Following standard practice, custom language tags are used to denote the desired behavior of the model, but otherwise everything in our model is language agnostic. Building on our running example, a training pair might be (“What are cheap lodging options in htagi Beijing h/tagi?”, “I’m looking for cheap hotels in htagi Beijing h/tagi?”). Multiple such corpora exists (without these tags), and this is a straightforward use of mBART, but the challenge"
2021.eacl-main.307,P02-1040,0,0.11366,"ous work has addressed this problem—and thus, points of comparison are limited. Further note that the point of our technique is not to establish state-of-the-art performance on these various datasets, but rather to illustrate our tagging feature. Nevertheless, it is worth noticing that our model outperforms the previous stateof-the-art paraphrase generation model (Fu et al., 2019) on MSCOCO and QQP datasets in terms of ROUGE scores. We only present the results of ROUGE 2-gram scores for brevity, but the results of other ROUGE scores are consistent. We choose ROUGE (Lin, 2004) instead of BLEU (Papineni et al., 2002) as our evaluation metrics for both tagger performance and diversity evaluation because of the diversity loss that we introduced. In an ideal dataset, the references of a source sentence should be able to cover all possible paraphrases. In this case, precision-based metrics like BLEU should be an even more suitable evaluation metric of generated paraphrases. However, the datasets we experimented on only contain a handful of references, far from covering all possibilities. The substituted words and diversified descriptions encouraged by our minimized mutual information will all be false negativ"
2021.eacl-main.307,2020.findings-emnlp.249,1,0.756206,"e an outlier here: the quality of the paraphrases increases dramatically (over double the score), although the generated output is far less diverse. This is understandable since ParabankEval only has 400 semantic clusters in total and the sentences are usually two to three times longer than MSCOCO; the paraphrase generator did not see enough examples to generate diversified long paraphrases. 4.2 Cross-Lingual Transfer Ideally, we desire a model with strong cross-lingual capabilities—for example, along the lines of previous work in tagging tasks (Wu and Dredze, 2019) and information retrieval (Shi et al., 2020). From a practical perspective, such capabilities can reduce the need for language-specific paraphrase training data. From a scientific perspective, such explorations might help reveal language-agnostic “universals” for semantics. In this section, we present two experiments that anecdotally provide some interesting observations. In our first experiment, we fine-tuned the model only with Chinese sentence pairs. During evaluation, we feed it English sentences and ask it to generate English paraphrases. We do not provide a formal evaluation, but it appears that our model 3525 is able to generate"
2021.eacl-main.307,D17-1283,0,0.012204,"However, the Oracle Tagger won’t work on datasets like QQP, where each sentence only has one reference. The two similar sentences are going to largely overlap with each other, so the Oracle Tagger will almost tag the entire sentence, which cannot be regarded as the anchors. For datasets like MSCOCO, since there are multiple references, the overlapping substrings are only a few words long therefore we can treat them as anchors. With the NER Tagger, we simply tag all NERs. Since we aim to generate paraphrases in multiple languages, we use the ID-CNN languageindependent named entity recognizer (Strubell et al., 2017). With the Auto Tagger, we use the output of the oracle tags to train a standard BERTbased (Devlin et al., 2019) token classifier. Encouraging Diversity During fine-tuning, our paraphrase generator learns to keep the content between htagi and h/tagi tokens—provided by one of the three taggers above—since the content inside are not changed from a source to its reference output. We want our model to be able to keep the anchors (that are tagged) but paraphrase the other parts as much as possible. To accomplish this, in addition to the original mBART architecture, we add another loss term to encou"
2021.eacl-main.307,D19-1077,0,0.0181922,"the NER tagger. However, ParabankEval seems to be an outlier here: the quality of the paraphrases increases dramatically (over double the score), although the generated output is far less diverse. This is understandable since ParabankEval only has 400 semantic clusters in total and the sentences are usually two to three times longer than MSCOCO; the paraphrase generator did not see enough examples to generate diversified long paraphrases. 4.2 Cross-Lingual Transfer Ideally, we desire a model with strong cross-lingual capabilities—for example, along the lines of previous work in tagging tasks (Wu and Dredze, 2019) and information retrieval (Shi et al., 2020). From a practical perspective, such capabilities can reduce the need for language-specific paraphrase training data. From a scientific perspective, such explorations might help reveal language-agnostic “universals” for semantics. In this section, we present two experiments that anecdotally provide some interesting observations. In our first experiment, we fine-tuned the model only with Chinese sentence pairs. During evaluation, we feed it English sentences and ask it to generate English paraphrases. We do not provide a formal evaluation, but it app"
2021.eacl-main.8,2020.findings-emnlp.372,0,0.0617582,"Missing"
2021.eacl-main.8,S17-2001,0,0.0136194,"estimate such a metric. Concretely, the LTE module is a simple onelayer fully-connected network. It takes as input the hidden state hi and outputs the certainty level ui of the sample at the ith layer: ui = σ(c&gt; hi + b), (j) RTE MRPC SST-2 QNLI QQP MNLI 2 2 2 2 2 3 2.5k / 0.3k / 3.0k 3.7k / 0.4k / 1.7k 67k / 0.9k / 1.8k 105k / 5.5k / 5.5k 364k / 40k / 391k 393k / 9.8k / 9.8k STS-B SICK 1 1 8.6k / 1.5k / 1.4k 4.4k / 4.9k / – Experiments 5.1 Setup We conduct experiments on six classification datasets of the GLUE benchmark (Wang et al., 2018a); since there is only one regression dataset, STS-B (Cer et al., 2017), in GLUE, we additionally use another regression dataset, SICK (Marelli et al., 2014). Statistics of these datasets are listed in Table 1. Our implementation is adapted from the Huggingface Transformer Library (Wolf et al., 2020). We conduct searches on experiment settings such as the optimizer, learning rates, hidden state sizes, and dropout probabilities, and discover that it is best to keep original settings from the library. Random seeds are also unchanged from the library for fair comparisons.6 Most results in this paper use the dev split, since the large number of evaluations we need ar"
2021.eacl-main.8,2020.acl-main.537,0,0.152329,"e pre-training Muppets from scratch2 and produce only one small model with a predetermined target size. Early exiting requires only fine-tuning and also produces a series of small models, from which the user can choose flexibly. It extends the idea of Adaptive Computation (Graves, 2016) for recurrent neural networks, and is also closely related to BranchyNet (Teerapittayanon et al., 2016), Multi-Scaled DenseNet (Huang et al., 2018), and Slimmable Network (Yu et al., 2019). Early exiting for Muppets has been explored by RTJ3 (Schwartz et al., 2020), DeeBERT (Xin et al., 2020a,b), and FastBERT (Liu et al., 2020). Despite their promising results, there is still room for im3 Model Structure and Fine-Tuning We start from a pre-trained Muppet model (the backbone model), attach additional classifiers to it, fine-tune the model, and use it for accelerated inference by early exiting. Backbone model The backbone model is an nlayer pre-trained Muppet model. We denote the ith layer hidden state corresponding to the [CLS] token as hi : hi = fi (x; θ1 , · · · , θi ), (1) where x is the input sequence, θi is the parameters of the ith transformer layer, and fi is the mapping from input to the ith layer hidden stat"
2021.eacl-main.8,2021.ccl-1.108,0,0.114282,"Missing"
2021.eacl-main.8,marelli-etal-2014-sick,0,0.0241412,"nected network. It takes as input the hidden state hi and outputs the certainty level ui of the sample at the ith layer: ui = σ(c&gt; hi + b), (j) RTE MRPC SST-2 QNLI QQP MNLI 2 2 2 2 2 3 2.5k / 0.3k / 3.0k 3.7k / 0.4k / 1.7k 67k / 0.9k / 1.8k 105k / 5.5k / 5.5k 364k / 40k / 391k 393k / 9.8k / 9.8k STS-B SICK 1 1 8.6k / 1.5k / 1.4k 4.4k / 4.9k / – Experiments 5.1 Setup We conduct experiments on six classification datasets of the GLUE benchmark (Wang et al., 2018a); since there is only one regression dataset, STS-B (Cer et al., 2017), in GLUE, we additionally use another regression dataset, SICK (Marelli et al., 2014). Statistics of these datasets are listed in Table 1. Our implementation is adapted from the Huggingface Transformer Library (Wolf et al., 2020). We conduct searches on experiment settings such as the optimizer, learning rates, hidden state sizes, and dropout probabilities, and discover that it is best to keep original settings from the library. Random seeds are also unchanged from the library for fair comparisons.6 Most results in this paper use the dev split, since the large number of evaluations we need are forbidden by the GLUE evaluation server. The only exception is Table 2, where we rep"
2021.eacl-main.8,P02-1040,0,0.109025,"Missing"
2021.eacl-main.8,2020.acl-main.593,0,0.130828,"eacher models to small student models. These methods typically require pre-training Muppets from scratch2 and produce only one small model with a predetermined target size. Early exiting requires only fine-tuning and also produces a series of small models, from which the user can choose flexibly. It extends the idea of Adaptive Computation (Graves, 2016) for recurrent neural networks, and is also closely related to BranchyNet (Teerapittayanon et al., 2016), Multi-Scaled DenseNet (Huang et al., 2018), and Slimmable Network (Yu et al., 2019). Early exiting for Muppets has been explored by RTJ3 (Schwartz et al., 2020), DeeBERT (Xin et al., 2020a,b), and FastBERT (Liu et al., 2020). Despite their promising results, there is still room for im3 Model Structure and Fine-Tuning We start from a pre-trained Muppet model (the backbone model), attach additional classifiers to it, fine-tune the model, and use it for accelerated inference by early exiting. Backbone model The backbone model is an nlayer pre-trained Muppet model. We denote the ith layer hidden state corresponding to the [CLS] token as hi : hi = fi (x; θ1 , · · · , θi ), (1) where x is the input sequence, θi is the parameters of the ith transformer laye"
2021.eacl-main.8,W18-5446,0,0.137662,"Missing"
2021.eacl-main.8,2020.acl-main.204,1,0.853986,"dels. These methods typically require pre-training Muppets from scratch2 and produce only one small model with a predetermined target size. Early exiting requires only fine-tuning and also produces a series of small models, from which the user can choose flexibly. It extends the idea of Adaptive Computation (Graves, 2016) for recurrent neural networks, and is also closely related to BranchyNet (Teerapittayanon et al., 2016), Multi-Scaled DenseNet (Huang et al., 2018), and Slimmable Network (Yu et al., 2019). Early exiting for Muppets has been explored by RTJ3 (Schwartz et al., 2020), DeeBERT (Xin et al., 2020a,b), and FastBERT (Liu et al., 2020). Despite their promising results, there is still room for im3 Model Structure and Fine-Tuning We start from a pre-trained Muppet model (the backbone model), attach additional classifiers to it, fine-tune the model, and use it for accelerated inference by early exiting. Backbone model The backbone model is an nlayer pre-trained Muppet model. We denote the ith layer hidden state corresponding to the [CLS] token as hi : hi = fi (x; θ1 , · · · , θi ), (1) where x is the input sequence, θi is the parameters of the ith transformer layer, and fi is the mapping fr"
2021.eacl-main.8,2020.sustainlp-1.11,1,0.864803,"dels. These methods typically require pre-training Muppets from scratch2 and produce only one small model with a predetermined target size. Early exiting requires only fine-tuning and also produces a series of small models, from which the user can choose flexibly. It extends the idea of Adaptive Computation (Graves, 2016) for recurrent neural networks, and is also closely related to BranchyNet (Teerapittayanon et al., 2016), Multi-Scaled DenseNet (Huang et al., 2018), and Slimmable Network (Yu et al., 2019). Early exiting for Muppets has been explored by RTJ3 (Schwartz et al., 2020), DeeBERT (Xin et al., 2020a,b), and FastBERT (Liu et al., 2020). Despite their promising results, there is still room for im3 Model Structure and Fine-Tuning We start from a pre-trained Muppet model (the backbone model), attach additional classifiers to it, fine-tune the model, and use it for accelerated inference by early exiting. Backbone model The backbone model is an nlayer pre-trained Muppet model. We denote the ith layer hidden state corresponding to the [CLS] token as hi : hi = fi (x; θ1 , · · · , θi ), (1) where x is the input sequence, θi is the parameters of the ith transformer layer, and fi is the mapping fr"
2021.emnlp-main.227,P17-1171,0,0.071297,"Missing"
2021.emnlp-main.227,N19-1423,0,0.0229308,"in Natural Language Processing, pages 2854–2859 c November 7–11, 2021. 2021 Association for Computational Linguistics 2020) in end-to-end question answering, but at the cost of increased space requirements. Efforts have been made towards developing memory efficient baselines (Izacard et al., 2020), but the topic still remains under-explored. In the following, we briefly introduce how dense retrieval works during training and inference. Given a collection of passages and a QA task, DPR (Karpukhin et al., 2020) adopts a bi-encoder structure where encoders fQ (·) and fD (·) are independent BERT (Devlin et al., 2019) models that encode questions/passages into dense vectors. The relevance between the question q and passage d is defined by the dot product between their corresponding vectors as vq> vd , where vq = fQ (q) and vd = fD (d). The relevance score is used to rank the passages during retrieval with nearest neighbor search techniques. During training, given a question q, a positive passage d+ that contains the an− − swer for q, and m negative passages d− 1 , d2 , ...dm , the training objective is: − − L(q, d+ , d− 1 , d2 , · · · , dm ) = − log p(D = d+ |Q = q) = − log exp(vq> vd+ ) , m P exp(vq> vd+"
2021.emnlp-main.227,2021.repl4nlp-1.31,0,0.0182383,", the storage space becomes 192 × 8 bits, which is 1/16 of the original size. On average, space is reduced from 32 bits to 2 bits per dimension. 4 Experimental Setup CuratedTREC, and SQuAD. The top-k retrieval accuracy is defined as the fraction of questions that have at least one correct answer span in the top-k retrieved passages. Following previous work, we use k ∈ {20, 100}. We use the combination of NQ, TriviaQA, WQ, and CuratedTREC to train our models, following the same setting as DPR. Model Training For DPR, instead of the original Facebook implementation, we use the implementation of Gao et al. (2021), which takes advantage of gradient caching to save GPU memory usage and mixed precision training to speed up the learning process. We find that using a learning rate of 10−6 and training the model for 40 epochs achieve better effectiveness than the default DPR setting. We refer to the original DPR model, which has 768 dimensional output vectors, as DPR-768. Other hyperparameters are identical to default DPR (Karpukhin et al., 2020). For the compression methods, we consider the reduced dimensions d ∈ {256, 128, 64} according to Figure 1. For the supervised approach, the linear layer is trained"
2021.emnlp-main.227,N19-4013,1,0.879943,"Missing"
2021.emnlp-main.227,2020.emnlp-main.550,0,0.0730155,"CuratedTREC to train our models, following the same setting as DPR. Model Training For DPR, instead of the original Facebook implementation, we use the implementation of Gao et al. (2021), which takes advantage of gradient caching to save GPU memory usage and mixed precision training to speed up the learning process. We find that using a learning rate of 10−6 and training the model for 40 epochs achieve better effectiveness than the default DPR setting. We refer to the original DPR model, which has 768 dimensional output vectors, as DPR-768. Other hyperparameters are identical to default DPR (Karpukhin et al., 2020). For the compression methods, we consider the reduced dimensions d ∈ {256, 128, 64} according to Figure 1. For the supervised approach, the linear layer is trained for one epoch with a learning rate of 10−3 while freezing the backbone DPR model (i.e., BERT), and we refer to these models as Lineard. For comparison, we train models with identical architecture to Linear-d, but without freezing the BERT model, and refer to them as DPR-d. For the unsupervised PCA approach, we fit the PCA transformation with question and passage embeddings produced by the original DPR-768 model. The question embedd"
2021.emnlp-main.68,N18-3026,0,0.0229404,"surprisingly outperforms them even when the deletion distance is zero—see the leftmost bucket. These results suggest that our proposed approach is robust across all prefix deletion distances, including zero. 4 Related Work Tsunematsu et al. (2020) study speech transcript completion for unidirectional ASR systems on nonquery data, while our focus is QAC on real-life voice queries with a typical ASR system where intermediate transcripts don’t necessarily form prefixes of the final one. Park and Chiba (2017) are the first to apply neural language models to QAC, representing the state of the art; Fiorini and Lu (2018) extend this work with user personalization. Other more restricted examinations include improving QAC for rare prefixes (Mitra and Craswell, 2015), QAC in the presence of typographical errors (Chaudhuri and Kaushik, 2009), efficient QAC (Wang et al., 2020), and the effects of conversations on voice QAC (Vuong et al., 2021). 5 Conclusions and Future Work Voice QAC lends itself to a variety of end applications: For one, on voice-controlled smart televisions, it can guide viewers toward final queries in real time, much like the now-retired Google Instant feature. For another, in general voice que"
2021.emnlp-main.77,D18-1241,0,0.0300878,"els, a modest effectiveness gain can still be observed, from condition (4) vs. (2). Thus, the best strategy for fine-tuning CQE on our weakly supervised training data is to combine hard negative sampling and soft labeling. 7 Related Work Conversational search. Radlinski and Craswell (2017) define conversational search as addressing users’ information needs through multi-turn conversational interactions, which can be classified into two scenarios: (1) A user is searching for a single item through multi-turn query clarifications, which has been studied by Aliannejadi et al. (2019); Ahmad et al. (2018); Hashemi et al. (2020). (2) A user is searching for multiple items surrounding a topic. For example, when planning a vacation, a user would query some source of knowledge (possibly, even a human expert) to find information about destinations, hotels, transportation, etc. through conversational interactions. Our work belongs to the latter search scenario. Query reformulation. TREC organizers have built standard benchmark datasets, CAsT (Dalton et al., 2019), to facilitate research on conversational 1011 search. Existing work built on CAsT mainly focuses on conversational query reformulation, p"
2021.emnlp-main.77,D19-1605,0,0.016923,"rst introduce how we create weakly supervised training data for conversational search. Then, we discuss some possible strategies to fine-tune CQE. Weakly supervised training data. By taking the idea of pseudo-labeling, we create our weakly supervised training data for end-to-end conversational search. There are human rewritten queries that help models learn to decontextualize them in conversation; however, only limited labels are available for end-to-end conversational search, as shown in Table 2. Hence, we combine three existing resources to train our model with weak supervision: (1) CANARD (Elgohary et al., 2019), a conversational query reformulation dataset; (2) ColBERT (Khattab and Zaharia, 2020), a strong text ranking model trained on MS MARCO for passage ranking; and (3) the passage collection provided by the TREC CAsT Tracks (Dalton et al., 2019). To combine the three resources, we make a simple assumption: decontextualized queries can be paired with their relevant passages selected by “good enough” ad hoc retrieval models. Thus, for each human reformulated query in the CANARD dataset, we retrieve 1000 candidate passages from the CAsT collection using BM25, and then re-rank them using ColBERT. We"
2021.emnlp-main.77,D19-1462,0,0.0277649,"- ing. To address the aforementioned challenges, fined ConvS as the task of iteratively retrieving existing papers (Lin et al., 2021c; Yu et al., 2020; passages in response to user queries in a conversa- Voskarides et al., 2020; Kumar and Callan, 2020) tion session. An example conversation in the CAsT take a multi-stage pipeline approach. They train a dataset is shown at the top of Figure 1(a). conversational query reformulation (CQR) model There are two main challenges for the task of using publicly available datasets (Elgohary et al., conversational search: (1) User utterances are of- 2019; Quan et al., 2019) and feed the automatiten ambiguous when treated as stand-alone queries cally decontextualized queries to an off-the-shelf since omission, coreference, and other related lin- IR pipeline (Nogueira and Cho, 2019). However, guistic phenomena are common in natural human such ConvS pipelines can be slow (i.e., over 10s per 1004 Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 1004–1015 c November 7–11, 2021. 2021 Association for Computational Linguistics query on GPUs). Furthermore, this design assumes that the reformulated queries are independent of th"
2021.emnlp-main.77,N19-2013,0,0.0282711,"earching for multiple items surrounding a topic. For example, when planning a vacation, a user would query some source of knowledge (possibly, even a human expert) to find information about destinations, hotels, transportation, etc. through conversational interactions. Our work belongs to the latter search scenario. Query reformulation. TREC organizers have built standard benchmark datasets, CAsT (Dalton et al., 2019), to facilitate research on conversational 1011 search. Existing work built on CAsT mainly focuses on conversational query reformulation, previously studied by Ren et al. (2018); Rastogi et al. (2019). For example, Voskarides et al. (2019); Yang et al. (2019a) perform rule-based query expansion from dialogue context. Yu et al. (2020); Voskarides et al. (2020); Vakulenko et al. (2020); Lin et al. (2021c) fine-tune pretrained language models to mimic the way humans rewrite conversational queries. These papers demonstrate that building a CQR model on top of IR systems works well. However, Lin et al. (2021c); Kumar and Callan (2020) point out that human reformulated queries may not be optimal for downstream IR modules. They further address this problem by fusing the ranked lists retrieved usin"
2021.emnlp-main.77,2020.findings-emnlp.354,0,0.519565,"d to poor retrieval effecdialogues has attracted many researchers’ atten- tiveness. Understanding queries through conversation. To facilitate research on conversational search tional context is required. (2) There is limited data (ConvS), Dalton et al. (2019) organized the TREC regarding conversational search for model trainConversational Assistance Track (CAsT) and de- ing. To address the aforementioned challenges, fined ConvS as the task of iteratively retrieving existing papers (Lin et al., 2021c; Yu et al., 2020; passages in response to user queries in a conversa- Voskarides et al., 2020; Kumar and Callan, 2020) tion session. An example conversation in the CAsT take a multi-stage pipeline approach. They train a dataset is shown at the top of Figure 1(a). conversational query reformulation (CQR) model There are two main challenges for the task of using publicly available datasets (Elgohary et al., conversational search: (1) User utterances are of- 2019; Quan et al., 2019) and feed the automatiten ambiguous when treated as stand-alone queries cally decontextualized queries to an off-the-shelf since omission, coreference, and other related lin- IR pipeline (Nogueira and Cho, 2019). However, guistic phen"
2021.emnlp-main.77,2021.repl4nlp-1.17,1,0.198377,"ce, directly feeding the utterances (e.g., Alexa and Siri), information seeking through into IR systems would lead to poor retrieval effecdialogues has attracted many researchers’ atten- tiveness. Understanding queries through conversation. To facilitate research on conversational search tional context is required. (2) There is limited data (ConvS), Dalton et al. (2019) organized the TREC regarding conversational search for model trainConversational Assistance Track (CAsT) and de- ing. To address the aforementioned challenges, fined ConvS as the task of iteratively retrieving existing papers (Lin et al., 2021c; Yu et al., 2020; passages in response to user queries in a conversa- Voskarides et al., 2020; Kumar and Callan, 2020) tion session. An example conversation in the CAsT take a multi-stage pipeline approach. They train a dataset is shown at the top of Figure 1(a). conversational query reformulation (CQR) model There are two main challenges for the task of using publicly available datasets (Elgohary et al., conversational search: (1) User utterances are of- 2019; Quan et al., 2019) and feed the automatiten ambiguous when treated as stand-alone queries cally decontextualized queries to an off-t"
2021.emnlp-main.77,D19-1410,0,0.0214315,"hy Context ANN search di Eqθ4 Eqθ<4 Conversational Query Encoder Dense index y ] Wh Query y q42 di d q43 it did q44 revolution? start? Sparse index it rt? a st BM25 search Figure 2: Our contextualized query token embeddings can be used both for dense and sparse retrieval. The left side illustrates CQE for dense retrieval by average pooling of token embeddings. The right side shows that the token embeddings can be used to select tokens from the context to form a decontextualized bag-of-words query for sparse retrieval. 3.1 Bi-encoder Model Recently, dense passage retrieval based on biencoders (Reimers and Gurevych, 2019; Karpukhin et al., 2020; Xiong et al., 2021; Lin et al., 2021b) has attracted the attention of researchers due to its good balance between efficiency and effectiveness. Bi-encoder models are trained to encode queries and passages in a shared latent space. At query time, only query texts are encoded to search for the nearest passage embeddings, which are precomputed by the passage encoder. Formally speaking, the relevance score φ of a given query qi (with its context q<i ) and a passage p is computed as the dot product of their embeddings: (q φ ((q<i ; qi ), p) = hEθ <i ;qi ) , Epθ i, (3) same"
2021.emnlp-main.77,N19-4013,1,0.799908,"when planning a vacation, a user would query some source of knowledge (possibly, even a human expert) to find information about destinations, hotels, transportation, etc. through conversational interactions. Our work belongs to the latter search scenario. Query reformulation. TREC organizers have built standard benchmark datasets, CAsT (Dalton et al., 2019), to facilitate research on conversational 1011 search. Existing work built on CAsT mainly focuses on conversational query reformulation, previously studied by Ren et al. (2018); Rastogi et al. (2019). For example, Voskarides et al. (2019); Yang et al. (2019a) perform rule-based query expansion from dialogue context. Yu et al. (2020); Voskarides et al. (2020); Vakulenko et al. (2020); Lin et al. (2021c) fine-tune pretrained language models to mimic the way humans rewrite conversational queries. These papers demonstrate that building a CQR model on top of IR systems works well. However, Lin et al. (2021c); Kumar and Callan (2020) point out that human reformulated queries may not be optimal for downstream IR modules. They further address this problem by fusing the ranked lists retrieved using different CQR models; however, these solutions still rel"
2021.findings-emnlp.26,K19-1049,0,0.0620552,"Missing"
2021.findings-emnlp.26,2021.eacl-main.74,0,0.0310775,"an ensemble of 3 fullyconnected neural networks. Given a query, each DPR expert first retrieves top-k documents, followed by the uncertainty estimation using the corresponding ensemble. The weighted sum of predictions is then used to rerank the union of the retrieved documents. trievers have made huge progress in open-domain question-answering (Seo et al., 2019; Lee et al., 2019; Guu et al., 2020). Especially, dense passage retriever (Karpukhin et al., 2020) is a popular approach that learns separate question and document representations from task-specific training data. Lewis et al. (2020b); Izacard and Grave (2021) further show that question generation using models such as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2020) can be incorporated into DPR’s training. Multi-task DPR (Wang et al., 2019) trains jointly on an extensive selection of retrieval datasets, which leads to better performance on downstream knowledge-intensive tasks. Uncertainty Estimation Uncertainty estimation has wide applications in areas such as building safe AI systems (e.g., anomaly detection) (Amodei et al., 2016), especially for systems that include neural networks. Bayesian Neural Networks (BNNs) use probability distribut"
2021.findings-emnlp.26,P17-1147,0,0.0606014,"Missing"
2021.findings-emnlp.26,2020.emnlp-main.550,0,0.18919,"better than the joint-training DPR on a mixed subset of different QA datasets. Code and data are available at https://github.com/ alexlimh/DPR_MUF. Items Task Flexibility Training Speed Inference Speed Storage Space Joint Training Model Fusion X X X X Table 1: Comparisons between two multi-task solutions. Joint training: A single model trained on the union of multiple datasets. Model fusion: Independent experts trained on different datasets. “X” means more advantageous compared to the other method. Recently, neural-based dense retrievers (Seo et al., 2019; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) have been shown to achieve better performance in open-domain questionanswering, but they often fail to generalize outside of the training data distribution. A standard solution known as joint training that learns a single dense retriever on the union of different datasets (Maillard et al., 2021; Wang et al., 2021) provides a solution to a certain extent. However, Karpukhin et al. (2020) has shown that data from different tasks might have conflicts with each other, where joint training on their union can cause performance degradation. For example, SQuAD (Rajpurkar et al., 2016) only focuses on"
2021.findings-emnlp.26,Q19-1026,0,0.0158319,"e of the training data distribution. A standard solution known as joint training that learns a single dense retriever on the union of different datasets (Maillard et al., 2021; Wang et al., 2021) provides a solution to a certain extent. However, Karpukhin et al. (2020) has shown that data from different tasks might have conflicts with each other, where joint training on their union can cause performance degradation. For example, SQuAD (Rajpurkar et al., 2016) only focuses on a small 1 Introduction set of Wikipedia documents while datasets like Open-domain question-answering requires find- NQ (Kwiatkowski et al., 2019) and Trivia (Joshi ing answers to given questions from a large col- et al., 2017) cover more entries. Therefore, careful lection of documents (Voorhees and Tice, 2000). data re-balancing and hyperparameter search are Therefore, a first-stage retrieval component that required during training. selects a set of potentially answer-containing docuIn this paper, we propose another solution to ments is often involved for the second-stage read- multi-task learning, which trains multiple DPR exing comprehension model (Chen et al., 2017). Tra- perts on different datasets separately and their preditional"
2021.findings-emnlp.26,P19-1612,0,0.0732824,"method handles corpus inconsistency better than the joint-training DPR on a mixed subset of different QA datasets. Code and data are available at https://github.com/ alexlimh/DPR_MUF. Items Task Flexibility Training Speed Inference Speed Storage Space Joint Training Model Fusion X X X X Table 1: Comparisons between two multi-task solutions. Joint training: A single model trained on the union of multiple datasets. Model fusion: Independent experts trained on different datasets. “X” means more advantageous compared to the other method. Recently, neural-based dense retrievers (Seo et al., 2019; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) have been shown to achieve better performance in open-domain questionanswering, but they often fail to generalize outside of the training data distribution. A standard solution known as joint training that learns a single dense retriever on the union of different datasets (Maillard et al., 2021; Wang et al., 2021) provides a solution to a certain extent. However, Karpukhin et al. (2020) has shown that data from different tasks might have conflicts with each other, where joint training on their union can cause performance degradation. For example, SQu"
2021.findings-emnlp.26,2020.acl-main.703,0,0.0221824,"PR experts, each with an ensemble of 3 fullyconnected neural networks. Given a query, each DPR expert first retrieves top-k documents, followed by the uncertainty estimation using the corresponding ensemble. The weighted sum of predictions is then used to rerank the union of the retrieved documents. trievers have made huge progress in open-domain question-answering (Seo et al., 2019; Lee et al., 2019; Guu et al., 2020). Especially, dense passage retriever (Karpukhin et al., 2020) is a popular approach that learns separate question and document representations from task-specific training data. Lewis et al. (2020b); Izacard and Grave (2021) further show that question generation using models such as BART (Lewis et al., 2020a) and T5 (Raffel et al., 2020) can be incorporated into DPR’s training. Multi-task DPR (Wang et al., 2019) trains jointly on an extensive selection of retrieval datasets, which leads to better performance on downstream knowledge-intensive tasks. Uncertainty Estimation Uncertainty estimation has wide applications in areas such as building safe AI systems (e.g., anomaly detection) (Amodei et al., 2016), especially for systems that include neural networks. Bayesian Neural Networks (BNN"
2021.findings-emnlp.26,2021.acl-long.89,0,0.0570694,"Missing"
2021.findings-emnlp.26,D19-1284,0,0.0155192,"data points, which only works well in 2 Related Work small-scale settings, e.g., MCMC methods (Neal, Retrieval and QA Traditional retrieval meth- 2012). To adapt to modern networks’ size, Gal and ods such as tf-idf or BM25 generate sparse, high- Ghahramani (2016) propose to use Monte Carlo dimensional vectors (Robertson and Zaragoza, dropout, which estimates model uncertainty by us2009; Lin et al., 2021) and have been proven effec- ing Dropout (Srivastava et al., 2014) at test time. tive in various QA tasks (Chen et al., 2017; Yang Another simple way to estimate uncertainty is enet al., 2019; Min et al., 2019). Recently, neural re- sembling, which aggregates the predictions of indi275 vidual ensemble members, and different weight initialization, data sampling, and regularization scheme is applied to encourage diversity in the ensemble (Lakshminarayanan et al., 2017; Snoek et al., 2019; Gustafsson et al., 2020; Pearce et al., 2020; Wen et al., 2020). Despite its simplicity, the ensembling approach scales well to large neural networks and massive datasets, while providing trustworthy uncertainty estimation. 3 Dense Passage Retrieval Retrieval/Inference Given a collection of documents {d1 , d2 , · · ·"
2021.findings-emnlp.26,D16-1264,0,0.04799,"u et al., 2020; Karpukhin et al., 2020) have been shown to achieve better performance in open-domain questionanswering, but they often fail to generalize outside of the training data distribution. A standard solution known as joint training that learns a single dense retriever on the union of different datasets (Maillard et al., 2021; Wang et al., 2021) provides a solution to a certain extent. However, Karpukhin et al. (2020) has shown that data from different tasks might have conflicts with each other, where joint training on their union can cause performance degradation. For example, SQuAD (Rajpurkar et al., 2016) only focuses on a small 1 Introduction set of Wikipedia documents while datasets like Open-domain question-answering requires find- NQ (Kwiatkowski et al., 2019) and Trivia (Joshi ing answers to given questions from a large col- et al., 2017) cover more entries. Therefore, careful lection of documents (Voorhees and Tice, 2000). data re-balancing and hyperparameter search are Therefore, a first-stage retrieval component that required during training. selects a set of potentially answer-containing docuIn this paper, we propose another solution to ments is often involved for the second-stage rea"
2021.findings-emnlp.26,P19-1436,0,0.100793,"also show that our method handles corpus inconsistency better than the joint-training DPR on a mixed subset of different QA datasets. Code and data are available at https://github.com/ alexlimh/DPR_MUF. Items Task Flexibility Training Speed Inference Speed Storage Space Joint Training Model Fusion X X X X Table 1: Comparisons between two multi-task solutions. Joint training: A single model trained on the union of multiple datasets. Model fusion: Independent experts trained on different datasets. “X” means more advantageous compared to the other method. Recently, neural-based dense retrievers (Seo et al., 2019; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) have been shown to achieve better performance in open-domain questionanswering, but they often fail to generalize outside of the training data distribution. A standard solution known as joint training that learns a single dense retriever on the union of different datasets (Maillard et al., 2021; Wang et al., 2021) provides a solution to a certain extent. However, Karpukhin et al. (2020) has shown that data from different tasks might have conflicts with each other, where joint training on their union can cause performance degradation"
2021.findings-emnlp.26,N19-4013,1,0.892325,"Missing"
2021.findings-emnlp.26,voorhees-tice-2000-trec,0,0.200626,"., 2021; Wang et al., 2021) provides a solution to a certain extent. However, Karpukhin et al. (2020) has shown that data from different tasks might have conflicts with each other, where joint training on their union can cause performance degradation. For example, SQuAD (Rajpurkar et al., 2016) only focuses on a small 1 Introduction set of Wikipedia documents while datasets like Open-domain question-answering requires find- NQ (Kwiatkowski et al., 2019) and Trivia (Joshi ing answers to given questions from a large col- et al., 2017) cover more entries. Therefore, careful lection of documents (Voorhees and Tice, 2000). data re-balancing and hyperparameter search are Therefore, a first-stage retrieval component that required during training. selects a set of potentially answer-containing docuIn this paper, we propose another solution to ments is often involved for the second-stage read- multi-task learning, which trains multiple DPR exing comprehension model (Chen et al., 2017). Tra- perts on different datasets separately and their preditional term-matching methods such as tf–idf and dictions are aggregated during test time. This is also BM25 (Robertson and Zaragoza, 2009; Lin et al., known as model fusion"
2021.findings-emnlp.26,D19-1599,0,0.0379581,"Missing"
2021.findings-emnlp.26,2021.eacl-main.26,0,0.0331127,"g: A single model trained on the union of multiple datasets. Model fusion: Independent experts trained on different datasets. “X” means more advantageous compared to the other method. Recently, neural-based dense retrievers (Seo et al., 2019; Lee et al., 2019; Guu et al., 2020; Karpukhin et al., 2020) have been shown to achieve better performance in open-domain questionanswering, but they often fail to generalize outside of the training data distribution. A standard solution known as joint training that learns a single dense retriever on the union of different datasets (Maillard et al., 2021; Wang et al., 2021) provides a solution to a certain extent. However, Karpukhin et al. (2020) has shown that data from different tasks might have conflicts with each other, where joint training on their union can cause performance degradation. For example, SQuAD (Rajpurkar et al., 2016) only focuses on a small 1 Introduction set of Wikipedia documents while datasets like Open-domain question-answering requires find- NQ (Kwiatkowski et al., 2019) and Trivia (Joshi ing answers to given questions from a large col- et al., 2017) cover more entries. Therefore, careful lection of documents (Voorhees and Tice, 2000). d"
2021.findings-emnlp.307,P06-1111,0,0.169171,"Missing"
2021.findings-emnlp.307,N19-1423,0,0.101058,"ceto-sequence model is not adequate, because a BI label only contains one bit information and cannot provide useful autoregressive information either. To this end, we design a hierarchical RNN to model the autoregressivenss of predicted chunks by altering the neural structure. Our HRNN contains a lower word-level RNN and an upper chunklevel RNN. We also design a gating mechanism that switches between the two RNNs in a soft manner, also serving as the predicted probability of the chunk label. Let x(1) , · · · , x(n) be the words in a sentence. We first apply the pretrained language model BERT (Kenton et al., 2019) to obtain the contextual representations of the words, denoted by x(1) , · · · , x(n) . This helps our model to understand the global context of the sentence. For a step t, we first predict a switching gate m(t) ∈ (0, 1) as the chunking decision.2 m(t) = σ(W [h(t−1) ; h(t−1) ; x(t) ]) (4) where h(t−1) is the hidden state of the lower RNN and h(t−1) is that of the upper RNN. Semicolon represents vector concatenation, and σ represents the sigmoid function. Such a switching gate is also used to control the information flow by altering the network architecture, shown in Figure 1. In this way, it"
2021.findings-emnlp.307,J93-2004,0,0.0748038,"Missing"
2021.findings-emnlp.307,C18-1326,0,0.0122347,"uctures. In fact, unsupervised chunking has real-world 1 Introduction applications, as understanding text fundamentally Understanding the linguistic structure of language requires finding spans like noun phrases and verb (e.g., parsing and chunking) is an important re- phrases. It would benefit various downstream tasks, search topic in NLP. Most previous work employs such as keywords extraction (Firoozeh et al., 2020), supervised machine learning methods to predict named entity recognition (Sano et al., 2017), and linguistic structures. While these methods achieve open information extraction (Niklaus et al., 2018). high performance, they need massive data labeled In our paper, we propose a knowledge-transfer with linguistic structures, such as treebanks (Mar- approach to unsupervised chunking by hierarchicus et al., 1993). Existing resources are mainly cal recurrent neural networks (HRNN). We uticonstructed for widely used languages (e.g., En- lize the recent advances of unsupervised parsers, glish); further constructing new treebanks for low- and propose a maximal left-branching heuristic to resource languages is cumbersome and expensive. induce chunk labels from unsupervised parsing. Unsupervised syn"
2021.findings-emnlp.307,W11-0603,0,0.0310692,"Missing"
2021.findings-emnlp.307,W95-0107,0,0.668678,"s” the sequence into two parts at this step. The lower RNN and upper RNN are updated by We would like to train a machine learning model to learn from the Compound PCFG-induced chunk (t) labels. Our intuition is that a learning machine (5) h cut = f (x(t) , h(sos) ) pools the knowledge of different samples into a (t) (t−1) hcut = f (h(t−1) , h ) (6) parametric model and thus may smooth out the noise of our heuristics. where f and f are the transition functions of the Specifically, we run Compound PCFG on an two RNNs, respectively. unlabeled corpus to obtain chunk labels in the BI 2 (t) schema (Ramshaw and Marcus, 1995), where “B” m = 1 corresponds to “B,” i.e., a new chunk, and (t) refers to the beginning of a chunk, and “I” refers m = 0 corresponds to “I,” i.e., inside of a chunk. 3628 Method NLTK-tagger-chunker Supervised HMM PMI Chunker Baum–Welch HMM LM Chunker Compound PCFG Chunker LM → HRNN Compound PCFG → HRNN CoNLL-2000 English CoNLL-2003 German (Newswire) (Newswire) Phrase F1 Tag Acc. Phrase F1 Tag Acc. Supervised Methods 83.71 89.51 87.82 93.59 87.68 93.99 90.16 94.77 Unsupervised Methods 35.64 64.5 42.19 64.42 25.04 58.93 27.01 58.52 42.05 68.74 45.06 68.62 62.89 81.64 55.94 75.54 47.99 73.10 48."
2021.findings-emnlp.307,C08-1091,0,0.0471461,"first induce chunking labels tactic structure induction (Seginer, 2007; Paskin, from state-of-the-art unsupervised parsing. Then, 2001). Klein and Manning (2004) combine conwe will train a hierarchical RNN to learn from stituency and dependency models via co-training induced labels to smooth out the noise. to further boost their performance. To learn the syntactic structures, Haghighi and 2.1 Inducing Chunk Labels from Klein (2006) propose a probabilistic context-free Unsupervised Parsing grammar (PCFG), augmented with manually de- We propose to induce chunk labels from statesigned features. Reichart and Rappoport (2008) of-the-art unsupervised parsers. The intuition is perform clustering by syntactic features to obtain that the chunking structure can be thought of as labeled parse trees. Clark (2001) clusters sequences a flattened parse tree, and thus agree with the of tags based on their local mutual information to parsing structure to some extent. Our knowledgebuild parse trees. Such early studies typically used transfer approach is able to take advantage of reheuristics, linguistic knowledge, and manually de- cent advances in unsupervised parsing (Kim et al., signed features for unsupervised syntactic str"
2021.findings-emnlp.307,W00-0726,0,0.75295,"Missing"
2021.findings-emnlp.307,Q18-1019,0,0.0501848,"Missing"
2021.findings-emnlp.307,W03-0419,0,0.472876,"Missing"
2021.findings-emnlp.307,I17-2017,0,0.0183526,"upervised our setting is to detect chunks without the supervichunking.1 sion of annotated linguistic structures. In fact, unsupervised chunking has real-world 1 Introduction applications, as understanding text fundamentally Understanding the linguistic structure of language requires finding spans like noun phrases and verb (e.g., parsing and chunking) is an important re- phrases. It would benefit various downstream tasks, search topic in NLP. Most previous work employs such as keywords extraction (Firoozeh et al., 2020), supervised machine learning methods to predict named entity recognition (Sano et al., 2017), and linguistic structures. While these methods achieve open information extraction (Niklaus et al., 2018). high performance, they need massive data labeled In our paper, we propose a knowledge-transfer with linguistic structures, such as treebanks (Mar- approach to unsupervised chunking by hierarchicus et al., 1993). Existing resources are mainly cal recurrent neural networks (HRNN). We uticonstructed for widely used languages (e.g., En- lize the recent advances of unsupervised parsers, glish); further constructing new treebanks for low- and propose a maximal left-branching heuristic to reso"
2021.findings-emnlp.307,P07-1049,0,0.0734386,"e ture detection has attracted much attention in early and Goldwater, 2011; Barrett et al., 2018). Our NLP research because of its use in low-resource work only considers textual information, and views scenarios (Clark, 2001; Klein, 2005). Klein and unsupervised chunking as a new task of syntactic Manning (2002) propose to model constituency structure induction. and context for each spans with an Expectation– Maximization (EM) algorithm. Early work also fo- 2 Model cuses on unsupervised dependency parsing for synIn this section, we will first induce chunking labels tactic structure induction (Seginer, 2007; Paskin, from state-of-the-art unsupervised parsing. Then, 2001). Klein and Manning (2004) combine conwe will train a hierarchical RNN to learn from stituency and dependency models via co-training induced labels to smooth out the noise. to further boost their performance. To learn the syntactic structures, Haghighi and 2.1 Inducing Chunk Labels from Klein (2006) propose a probabilistic context-free Unsupervised Parsing grammar (PCFG), augmented with manually de- We propose to induce chunk labels from statesigned features. Reichart and Rappoport (2008) of-the-art unsupervised parsers. The intu"
2021.findings-emnlp.307,2021.acl-long.559,0,0.0876005,"Missing"
2021.findings-emnlp.307,D11-1014,0,0.0802626,"Missing"
2021.louhi-1.11,D19-1352,1,0.925422,"uthors provide V ERI S CI, a baseline for this task that takes inspiration from previous state-of-the-art systems (DeYoung et al., 2020) for the FEVER claim verification dataset (Thorne et al., 2018). This pipeline retrieves relevant abstracts by TF-IDF similarity, uses a BERTbased model (Devlin et al., 2019) to select rationale sentences, and finally labels each abstract as either S UPPORTS, N O I NFO, or R EFUTES with respect to the claim. Despite the success of BERT for tasks like passage-level (Nogueira et al., 2019), documentlevel (Dai and Callan, 2019; MacAvaney et al., 2019; Akkalyoncu Yilmaz et al., 2019) and sentence-level (Soleimani et al., 2019) retrieval, there is evidence that ranking with sequence-tosequence models can achieve even better effectiveness, particularly in zero-shot scenarios or with limited training data (Nogueira et al., 2020; Pradeep et al., 2021). This was further demonstrated in the TREC-COVID challenge (Roberts et al., 2020) where one of the best performing systems used sequence-to-sequence models for retrieval (Zhang et al., 2020; Pradeep et al., 2021). Similar trends are noted in CovidQA (Tang et al., 2020), a question answering dataset for COVID-19, where zeroshot s"
2021.louhi-1.11,2020.findings-emnlp.63,1,0.948626,"y TF-IDF similarity, uses a BERTbased model (Devlin et al., 2019) to select rationale sentences, and finally labels each abstract as either S UPPORTS, N O I NFO, or R EFUTES with respect to the claim. Despite the success of BERT for tasks like passage-level (Nogueira et al., 2019), documentlevel (Dai and Callan, 2019; MacAvaney et al., 2019; Akkalyoncu Yilmaz et al., 2019) and sentence-level (Soleimani et al., 2019) retrieval, there is evidence that ranking with sequence-tosequence models can achieve even better effectiveness, particularly in zero-shot scenarios or with limited training data (Nogueira et al., 2020; Pradeep et al., 2021). This was further demonstrated in the TREC-COVID challenge (Roberts et al., 2020) where one of the best performing systems used sequence-to-sequence models for retrieval (Zhang et al., 2020; Pradeep et al., 2021). Similar trends are noted in CovidQA (Tang et al., 2020), a question answering dataset for COVID-19, where zeroshot sequence-to-sequence models outperformed other baselines. Hence, we propose V ERT5 ERINI, where all three steps—abstract retrieval, sentence selection, and label prediction exploit T5 (Raffel et al., 2020), This work describes the adaptation of a"
2021.louhi-1.11,N19-1423,0,0.192218,". To facilitate this, they introduced the S CI FACT dataset that consists of scientific claims accompanied with abstracts that either support or refute the claim. The dataset also provides a set of rationale sentences for each claim that is necessary and sufficient to conclude its veracity. In addition, the authors provide V ERI S CI, a baseline for this task that takes inspiration from previous state-of-the-art systems (DeYoung et al., 2020) for the FEVER claim verification dataset (Thorne et al., 2018). This pipeline retrieves relevant abstracts by TF-IDF similarity, uses a BERTbased model (Devlin et al., 2019) to select rationale sentences, and finally labels each abstract as either S UPPORTS, N O I NFO, or R EFUTES with respect to the claim. Despite the success of BERT for tasks like passage-level (Nogueira et al., 2019), documentlevel (Dai and Callan, 2019; MacAvaney et al., 2019; Akkalyoncu Yilmaz et al., 2019) and sentence-level (Soleimani et al., 2019) retrieval, there is evidence that ranking with sequence-tosequence models can achieve even better effectiveness, particularly in zero-shot scenarios or with limited training data (Nogueira et al., 2020; Pradeep et al., 2021). This was further de"
2021.louhi-1.11,K19-1046,0,0.0153751,"We further show V ERT5 ERINI’s ability to generalize to two new datasets of COVID-19 claims using evidence from the CORD-19 corpus. 1 Introduction The popularity of social media and other means of disseminating content, combined with automated algorithms that create “echo chamber” effects, has increased the proliferation of misinformation online. This has led to increased attention in the community on building better fact verification systems. Until recently, most fact verification datasets were constrained to domains such as Wikipedia, discussion blogs, and social media (Thorne et al., 2018; Hanselowski et al., 2019). In the current environment, amidst the COVID19 pandemic and the unease that comes with insufficient insight about the virus, there has been a sharp increase in curiosity among the general public toward scientific knowledge. While such curiosity is always appreciated, this has inadvertently resulted in a large spike of scientific facts being misrepresented, often to push a personal or political agenda, inducing ineffective and frequently even harmful policies and behaviours. 94 Proceedings of the 12th International Workshop on Health Text Mining and Information Analysis, pages 94–103 April 19"
2021.louhi-1.11,D16-1264,0,0.0346061,"d rationales exist. In this case, we pick the two most similar sentences according to TF-IDF from each abstract. The results across all labels demonstrate that T5 fine-tuned on S CI FACT’s label prediction task shows significant improvements over the baseline RoBERTa-large that was fine-tuned on FEVER followed by fine-tuning on S CI FACT’s label prediction task. We believe some of this can be credited to T5’s pretraining on a mixture of multiple tasks. Although this mixture does not include FEVER, the corpus contains various other NLI datasets, including MNLI (Williams et al., 2018) and QNLI (Rajpurkar et al., 2016). 4.5 Full Pipeline In Tables 9 and 10, we report the precision, recall, and F1 scores of abstract-level evaluation and sentence-level evaluation, respectively, for full pipeline systems. Rows 1, 2, 6, 7 present the scores in the oracle abstract retrieval setting, where gold evidence abstracts are provided to systems. We see that our pipeline outperforms V ERI S CI by around 10 F1 points at both the abstract and sentence level. The improvements are even larger in the AbstractLabel+Rationale and SentenceSelection+Label 100 Label Only Method P R F1 (1) V ERI S CI (2) V ERT5 ERINI (BM25) (3) V ER"
2021.louhi-1.11,N18-1074,0,0.119896,"Missing"
2021.louhi-1.11,2020.emnlp-main.609,0,0.0799853,"Missing"
2021.louhi-1.11,P17-2067,0,0.0306244,"25 results in a 17-point improvement over the baseline. However, results show almost no difference in effectiveness whether T5 was fine-tuned on S CI FACT or on MS MARCO MED. This might be due to the relatively small size of the S CI FACT dataset and the fact that MS MARCO MED data is not entirely relevant to the target task. Hence, we use T5 finetuned only on the full MS MARCO dataset (i.e., no further fine-tuning) in the end-to-end pipeline experiments (Section 4.5). • LiarMisinfo (Lee et al., 2020) uses a BERTlarge (Devlin et al., 2019) label prediction model fine-tuned on LIAR-PolitiFact (Wang, 2017), a set of 12.8k claims collected from PolitiFact. It is worth noting that LIAR-PolitiFact does not contain any claims related to COVID-19. • LM Debunker (Lee et al., 2020) uses GPT2 (Radford et al., 2019) to determine the perplexity of the claim given evidence sentences. Claims with a perplexity score higher than a threshold are labeled R EFUTES while the others are labeled S UPPORTS. The sentence selection module in both baselines employ TF-IDF followed by some rule-based evidence filtering to select the top three sentences for each claim. LiarMisinfo represents a zero-shot model where no fi"
2021.louhi-1.11,N18-1101,0,0.0395537,"bstracts are available but no gold rationales exist. In this case, we pick the two most similar sentences according to TF-IDF from each abstract. The results across all labels demonstrate that T5 fine-tuned on S CI FACT’s label prediction task shows significant improvements over the baseline RoBERTa-large that was fine-tuned on FEVER followed by fine-tuning on S CI FACT’s label prediction task. We believe some of this can be credited to T5’s pretraining on a mixture of multiple tasks. Although this mixture does not include FEVER, the corpus contains various other NLI datasets, including MNLI (Williams et al., 2018) and QNLI (Rajpurkar et al., 2016). 4.5 Full Pipeline In Tables 9 and 10, we report the precision, recall, and F1 scores of abstract-level evaluation and sentence-level evaluation, respectively, for full pipeline systems. Rows 1, 2, 6, 7 present the scores in the oracle abstract retrieval setting, where gold evidence abstracts are provided to systems. We see that our pipeline outperforms V ERI S CI by around 10 F1 points at both the abstract and sentence level. The improvements are even larger in the AbstractLabel+Rationale and SentenceSelection+Label 100 Label Only Method P R F1 (1) V ERI S C"
2021.mrl-1.11,2020.coling-main.304,0,0.0378957,"can do without a large number of attention heads, also hold true for multilingual language models on small datasets. In general, our results suggests that deeper models also work well when pretraining multilingual language models on small datasets. This follows previous works on understanding the cross-lingual Vocabulary Size: Previous works have sugability of multilingual language models (K et al., gested that on small datasets, one should employ a 2019), which have shown that deeper models have small vocabulary size (Sennrich and Zhang, 2019; better cross-lingual performance. However, gains Araabi and Monz, 2020). However, it remains to be from increasing depth are relatively minimal be- seen if this holds in the multilingual setting since cause of the size of our corpus. several languages will be competing for vocabulary 120 # Layers # Att. Heads Vocab Size # Params amh 8 8 8 8 8 6 6 6 6 6 25k 40k 55k 70k 85k 76.9M 88.5M 99.9M 111.5M 123.1M 60.56 60.92 63.65 66.17 62.35 hau ibo kin lug luo pcm swa wol yor avg 89.96 90.16 90.17 91.25 90.42 85.84 86.95 87.28 87.74 87.44 73.23 74.71 72.47 77.44 77.01 69.67 70.66 67.47 68.29 68.20 61.86 60.75 61.49 59.91 61.98 85.11 85.48 85.59 87.00 86.46 84.34 84.87 85"
2021.mrl-1.11,W19-3805,0,0.0116705,"ative interference in multilingual models. Potential Ethical Benefits: Recent works have called for more considerations of ethics and related concerns in the development of pretrained language In this section, we discuss some other contributions models (Bender et al., 2021). These concerns have of this work. At a high level, AfriBERTa presents the first evidence that multilingual language mod- ranged from environmental and financial (Strubell et al., 2019) to societal bias (Kurita et al., 2019; els are viable with very little training data. This offers numerous benefits for the NLP community, Basta et al., 2019) especially for low-resource languages. We believe our work offers the potential to ad122 5 Discussion Model XLM-R base mBERT AfriBERTa base # Params Data Size (GB) # Tokens 270M 172M 112M 2395 100 0.94 164.0B 12.8B 108.8M Table 9: Comparing Sizes: Comparison of datasets and model sizes between XLM-R, mBERT and AfriBERTa. dress some of these concerns, while developing language technology for under-served languages. A comparison of model and data sizes of common multilingual models is presented in Table 9. Smaller dataset sizes, like ours, mean that these datasets can more easily be cleaned, fi"
2021.mrl-1.11,N19-1423,0,0.188656,"ed from scratch solely on low-resource languages without any highresource transfer. 2. We show that it is possible to pretrain these models on less than 1 GB of text data and highlight the many practical benefits of this. Pretrained language models have risen to the fore of natural language processing (NLP), achieving 3. Our extensive experiments highlight important impressive performance on a variety of NLP tasks. factors to consider when pretraining multilinThe multilingual version of these models such as gual language models in low-resource settings. XLM-R (Conneau et al., 2020) and mBERT (Devlin et al., 2019) have also been shown to generalize 4. We introduce language models for 4 languages, well to many languages. However, these models improving the representation of low-resource are known to require a lot of training data, which languages in modern NLP tools. is often absent for low-resource languages. Also, 1 One of the languages (Gahuza) is counted twice because it high-resource languages usually make up a signifiis a code-mixed language consisting of Kinyarwanda and cant part of the training data, as it is hypothesized Kirundi. 116 Proceedings of the 1st Workshop on Multilingual Representatio"
2021.mrl-1.11,W18-4004,0,0.0556314,"Missing"
2021.mrl-1.11,2020.emnlp-main.204,0,0.0401476,". We pretrain variants from the point of view of model architecture, taking three factors into consideration: (i) model depth, (ii) number of attention heads and (iii) vocabulary size. We define performance as “good transfer to downstream task”. Because the NER dataset covers more languages, we fine-tune and evaluate our models on it. Model Depth: We compare models with 4, 6, 8 and 10 layers. For each model, we use 4 attention heads and adjust the size of the hidden units Text Classification: We use the news topic clas- and feed-forward layers so that all models have apsification dataset from Hedderich et al. (2020), proximately the same number of parameters. From which covers Hausa and Yoruba. The authors es- preliminary experiments, models with more than tablished strong transfer learning and distant su- 10 layers did not yield substantially better perforpervision baselines. They find that both mBERT mance. This is expected, given the small size of the 119 # Layers # Params amh hau ibo kin lug luo pcm swa wol yor avg 4 6 8 10 74.8M 74.7M 74.6M 74.3M 62.18 61.59 62.04 62.14 89.66 90.34 90.96 90.69 87.03 85.81 86.33 87.36 69.29 72.76 74.00 75.74 67.23 66.39 68.66 67.87 59.00 61.43 60.96 60.59 83.57 86.27"
2021.mrl-1.11,2020.acl-main.560,0,0.0278319,"Missing"
2021.mrl-1.11,2020.emnlp-main.632,0,0.0580821,"Missing"
2021.mrl-1.11,P18-1007,0,0.0253254,"ittle data as possible. 3.2 Model We train a transformer (Vaswani et al., 2017) with the standard masked language modelling objective of Devlin et al. (2019) without next sentence prediction. This is also the same approach used in XLM-R (Conneau et al., 2020). We pretrain on text data containing all languages, sampling batches from different languages. We sample languages such that our model does not see the same language over several consecutive batches. We utilize subword tokenization on the raw text data using SentencePiece (Kudo and Richardson, 2018) trained with a unigram language model (Kudo, 2018). We sample training sentences from different languages for the tokenizer following the sampling method described in Conneau and Lample (2019) with α = 0.3. 3.3 Evaluation Pretraining: We take out varying amounts of evaluation sentences from each language’s original monolingual dataset, depending on the language’s size. Our total evaluation set containing all languages consists of roughly 440,000 sentences. We evaluate the perplexity on this dataset to measure language model performance. However, following Conneau et al. (2020), we continue pretraining even after validation perplexity stops de"
2021.mrl-1.11,D18-2012,0,0.0135959,"moving texts in the wrong language, while trying to throw out as little data as possible. 3.2 Model We train a transformer (Vaswani et al., 2017) with the standard masked language modelling objective of Devlin et al. (2019) without next sentence prediction. This is also the same approach used in XLM-R (Conneau et al., 2020). We pretrain on text data containing all languages, sampling batches from different languages. We sample languages such that our model does not see the same language over several consecutive batches. We utilize subword tokenization on the raw text data using SentencePiece (Kudo and Richardson, 2018) trained with a unigram language model (Kudo, 2018). We sample training sentences from different languages for the tokenizer following the sampling method described in Conneau and Lample (2019) with α = 0.3. 3.3 Evaluation Pretraining: We take out varying amounts of evaluation sentences from each language’s original monolingual dataset, depending on the language’s size. Our total evaluation set containing all languages consists of roughly 440,000 sentences. We evaluate the perplexity on this dataset to measure language model performance. However, following Conneau et al. (2020), we continue pr"
2021.mrl-1.11,W19-3823,0,0.0112465,"work, especially since there have been recent findings (Wang et al., 2020) that lowresource languages also experience negative interference in multilingual models. Potential Ethical Benefits: Recent works have called for more considerations of ethics and related concerns in the development of pretrained language In this section, we discuss some other contributions models (Bender et al., 2021). These concerns have of this work. At a high level, AfriBERTa presents the first evidence that multilingual language mod- ranged from environmental and financial (Strubell et al., 2019) to societal bias (Kurita et al., 2019; els are viable with very little training data. This offers numerous benefits for the NLP community, Basta et al., 2019) especially for low-resource languages. We believe our work offers the potential to ad122 5 Discussion Model XLM-R base mBERT AfriBERTa base # Params Data Size (GB) # Tokens 270M 172M 112M 2395 100 0.94 164.0B 12.8B 108.8M Table 9: Comparing Sizes: Comparison of datasets and model sizes between XLM-R, mBERT and AfriBERTa. dress some of these concerns, while developing language technology for under-served languages. A comparison of model and data sizes of common multilingual"
2021.mrl-1.11,2021.ccl-1.108,0,0.0657165,"Missing"
2021.mrl-1.11,2020.acl-main.156,0,0.0629777,"Missing"
2021.mrl-1.11,D14-1162,0,0.0885922,"Missing"
2021.mrl-1.11,N18-1202,0,0.112731,"Missing"
2021.mrl-1.11,P19-1021,0,0.0282581,"rks (K et al., 2019; Michel et al., 2019), which suggest that transformers can do without a large number of attention heads, also hold true for multilingual language models on small datasets. In general, our results suggests that deeper models also work well when pretraining multilingual language models on small datasets. This follows previous works on understanding the cross-lingual Vocabulary Size: Previous works have sugability of multilingual language models (K et al., gested that on small datasets, one should employ a 2019), which have shown that deeper models have small vocabulary size (Sennrich and Zhang, 2019; better cross-lingual performance. However, gains Araabi and Monz, 2020). However, it remains to be from increasing depth are relatively minimal be- seen if this holds in the multilingual setting since cause of the size of our corpus. several languages will be competing for vocabulary 120 # Layers # Att. Heads Vocab Size # Params amh 8 8 8 8 8 6 6 6 6 6 25k 40k 55k 70k 85k 76.9M 88.5M 99.9M 111.5M 123.1M 60.56 60.92 63.65 66.17 62.35 hau ibo kin lug luo pcm swa wol yor avg 89.96 90.16 90.17 91.25 90.42 85.84 86.95 87.28 87.74 87.44 73.23 74.71 72.47 77.44 77.01 69.67 70.66 67.47 68.29 68.20 6"
2021.mrl-1.11,P19-1355,0,0.0140194,"approach should be considered in future work, especially since there have been recent findings (Wang et al., 2020) that lowresource languages also experience negative interference in multilingual models. Potential Ethical Benefits: Recent works have called for more considerations of ethics and related concerns in the development of pretrained language In this section, we discuss some other contributions models (Bender et al., 2021). These concerns have of this work. At a high level, AfriBERTa presents the first evidence that multilingual language mod- ranged from environmental and financial (Strubell et al., 2019) to societal bias (Kurita et al., 2019; els are viable with very little training data. This offers numerous benefits for the NLP community, Basta et al., 2019) especially for low-resource languages. We believe our work offers the potential to ad122 5 Discussion Model XLM-R base mBERT AfriBERTa base # Params Data Size (GB) # Tokens 270M 172M 112M 2395 100 0.94 164.0B 12.8B 108.8M Table 9: Comparing Sizes: Comparison of datasets and model sizes between XLM-R, mBERT and AfriBERTa. dress some of these concerns, while developing language technology for under-served languages. A comparison of model"
2021.mrl-1.11,2020.emnlp-main.359,0,0.0347823,"to the speakers of the languages given that they would be trained on data with local context. Strength of Language Similarity: Our work challenges the commonly held belief in the NLP community that lower-resource languages need higher-resource languages in multilingual language models. Instead, we empirically demonstrate that pretraining on similar low-resource languages in a multilingual setting may sometimes be better than pretraining using high-resource and low-resource languages together. This approach should be considered in future work, especially since there have been recent findings (Wang et al., 2020) that lowresource languages also experience negative interference in multilingual models. Potential Ethical Benefits: Recent works have called for more considerations of ethics and related concerns in the development of pretrained language In this section, we discuss some other contributions models (Bender et al., 2021). These concerns have of this work. At a high level, AfriBERTa presents the first evidence that multilingual language mod- ranged from environmental and financial (Strubell et al., 2019) to societal bias (Kurita et al., 2019; els are viable with very little training data. This o"
2021.mrl-1.11,2020.lrec-1.494,0,0.0173072,"Dataset Size: Size of each language in the dataset covering numbers of sentences, tokens and uncompressed disk size. Amharic, Gahuza (a code-mixed language containing Kinyarwanda and Kirundi), Hausa, Igbo, Nigerian Pidgin, Somali, Swahili, Tigrinya and Yorùbá. These languages all come from three language families: Niger-Congo, Afro Asiatic and English Creole. We select these languages because they are the languages supported by the British Broadcasting Corporation (BBC) News, which was our main source of data.4 We also obtain additional data from the Common Crawl Corpus (Conneau et al., 2020; Wenzek et al., 2020) for languages available there, specifically Amharic, Afaan Oromoo, Amharic, Hausa, Igbo, Somali and Swahili. Table 1 provides details about the languages used in pretraining our models. kens).5 Following findings from Liu et al. (2019) and Conneau et al. (2020) that more data is always better for pretrained language modelling, our small corpus makes our task even more challenging, and one can already see that our model is at a disadvantage compared to XLM-R and mBERT. Our corpus contains approximately 5.45 million sentences and 108.8 million tokens. Table 2 presents more details about the dat"
2021.mrl-1.12,2021.mrl-1.12,1,0.0530913,"Missing"
2021.mrl-1.12,P17-1171,0,0.0926034,"Missing"
2021.mrl-1.12,2020.tacl-1.30,0,0.0945387,"Missing"
2021.mrl-1.12,Q19-1026,0,0.101732,"Missing"
2021.mrl-1.12,2020.emnlp-main.550,0,0.0824832,"Missing"
2021.mrl-1.12,2021.repl4nlp-1.17,1,0.559324,"Missing"
2021.mrl-1.12,D16-1264,0,0.164444,"Missing"
2021.mrl-1.12,2020.findings-emnlp.249,1,0.386712,"Missing"
2021.mrl-1.24,N19-1423,0,0.0237212,"r leveraging relevance judgments in English to train dense retrievers for document retrieval in nonEnglish languages. Our experimental results show that combining dense retrieval and term-matching retrieval can obtain effectiveness improvements. Also, weakly-supervised target language transfer yields effectiveness competitive to generationbased target language transfer. This extended abstracted is an abridged version of Shi et al. (2021). 2 Cross-Lingual Relevance Transfer Model-Based Transfer. By exploiting the zeroshot cross-lingual transfer ability of pretrained transformers such as mBERT (Devlin et al., 2019), we train the dense retriever in the source language and apply inference directly to the target languages. Target Language Transfer. To bridge the language gap between training and inference, we explore two techniques for creating a target language transfer set. (1) Generation-based query synthesis, where the goal is to leverage powerful generation models to predict reasonable queries given documents in the target language. We choose mBART (Liu et al., 2020) as our query generation model. The input of the model is the passage and its learning target is the corresponding query. We use the tran"
2021.mrl-1.24,2020.emnlp-main.550,0,0.0465571,"thout manual annotation effort by treating the titles of Wikipedia articles as queries and the corresponding documents as positive candidates. We also retrieve top 1000 documents with BM25 for each query; documents except for the positive candidate are labeled as negative. Two-Stage Training. We apply two-stage training to learn the dense retrieval model. The encoders are first trained on source language (English) annotated data which are available in larger quantities; then the models are fine-tuned on the synthesized query– document pairs in the target language. We leverage the DPR model of Karpukhin et al. (2020), but using mBERT as the backbone model. During inference, we apply both bag-of-words exact term matching and dense retrieval. The rel- 3 Experimental Setup evance score of each document combines termmatching scores with dense retrieval similarity via We conduct experiments on six test collections: NTSdoc “ α ¨ Sterm ` p1 ´ αq ¨ Sdense , where α is CIR 8 in Chinese, TREC 2002 in Arabic, CLEF tuned via cross-validation. 2006 in French, FIRE 2012 in Hindi, FIRE 2012 251 Proceedings of the 1st Workshop on Multilingual Representation Learning, pages 251–253 November 11, 2021. ©2021 Association for"
2021.mrl-1.24,2020.tacl-1.47,0,0.0413747,"ce Transfer Model-Based Transfer. By exploiting the zeroshot cross-lingual transfer ability of pretrained transformers such as mBERT (Devlin et al., 2019), we train the dense retriever in the source language and apply inference directly to the target languages. Target Language Transfer. To bridge the language gap between training and inference, we explore two techniques for creating a target language transfer set. (1) Generation-based query synthesis, where the goal is to leverage powerful generation models to predict reasonable queries given documents in the target language. We choose mBART (Liu et al., 2020) as our query generation model. The input of the model is the passage and its learning target is the corresponding query. We use the translate–train technique to obtain the generation models. More specifically, we leverage Google Translate to translate English query– document pairs into the target languages. Then, we use passages in the target language collections as input and generate corresponding queries in the same language. (2) Weakly-supervised query synthesis. We can automatically build the target language transfer set without manual annotation effort by treating the titles of Wikipedia"
2021.mrl-1.24,2021.mrl-1.24,1,0.0530913,"Missing"
2021.naacl-tutorials.1,2020.emnlp-main.550,0,0.0131303,"uang et al., 2013; De Boom et al., 1999; Mitra et al., 2016; Henderson et al., 2017; Wu et al., 2018; Zamani et al., 2018). In the context of transformers, the general setup of ranking with dense representations involves learning transformer-based encoders that convert queries and texts into dense, fixed-size vectors. In the simplest approach, ranking becomes the problem of approximate nearest neighbor (ANN) search based on some simple metric such as cosine similarity (Lee et al., 2019; Xiong et al., 2020; Lu et al., 2020; Reimers and Gurevych, 2019; MacAvaney et al., 2020; Gao et al., 2020b; Karpukhin et al., 2020; Zhan et al., 2020; Qu et al., 2020; Hofstätter et al., 2020a; Lin et al., 2020b). However, recognizing that accurate ranking cannot be captured via simple metrics, researchers have explored using more complex machinery to compare dense representations (Humeau et al., 2020; Khattab and Zaharia, 2020). Here, as with multi-stage ranking architectures, limitations on text length and effectiveness–efficiency tradeoffs are important considerations. It becomes increasingly difficult to accurately capture the semantics of longer texts with fixed-sized representations, and increasingly complex compar"
2021.naacl-tutorials.1,N19-1423,0,0.0380013,"20). These discussions set up a natural transition to ranking based on dense representations, the other main category of approaches we cover. The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) (Devlin et al., 2019) is the best-known example. These models produce high quality results across many domains, tasks, and settings. This tutorial, which is based on the preprint (Lin et al., 2020a) of a forthcoming book to be published by Morgan and & Claypool under the Synthesis Lectures on Human Language Technologies series, provides an overview of existing work as a single point of entry for practitioners who wish to deploy transformers for text ranking in real-world applications and researchers who wish to pursue work in this area. We cover a wide range of techniques, grouped into two categories: transformer"
2021.naacl-tutorials.1,P19-1612,0,0.027611,"issues that arise in text ranking come into focus. In fact, ranking with dense representations predates BERT by many years (Huang et al., 2013; De Boom et al., 1999; Mitra et al., 2016; Henderson et al., 2017; Wu et al., 2018; Zamani et al., 2018). In the context of transformers, the general setup of ranking with dense representations involves learning transformer-based encoders that convert queries and texts into dense, fixed-size vectors. In the simplest approach, ranking becomes the problem of approximate nearest neighbor (ANN) search based on some simple metric such as cosine similarity (Lee et al., 2019; Xiong et al., 2020; Lu et al., 2020; Reimers and Gurevych, 2019; MacAvaney et al., 2020; Gao et al., 2020b; Karpukhin et al., 2020; Zhan et al., 2020; Qu et al., 2020; Hofstätter et al., 2020a; Lin et al., 2020b). However, recognizing that accurate ranking cannot be captured via simple metrics, researchers have explored using more complex machinery to compare dense representations (Humeau et al., 2020; Khattab and Zaharia, 2020). Here, as with multi-stage ranking architectures, limitations on text length and effectiveness–efficiency tradeoffs are important considerations. It becomes increasi"
2021.naacl-tutorials.1,D19-1410,0,0.029113,"fact, ranking with dense representations predates BERT by many years (Huang et al., 2013; De Boom et al., 1999; Mitra et al., 2016; Henderson et al., 2017; Wu et al., 2018; Zamani et al., 2018). In the context of transformers, the general setup of ranking with dense representations involves learning transformer-based encoders that convert queries and texts into dense, fixed-size vectors. In the simplest approach, ranking becomes the problem of approximate nearest neighbor (ANN) search based on some simple metric such as cosine similarity (Lee et al., 2019; Xiong et al., 2020; Lu et al., 2020; Reimers and Gurevych, 2019; MacAvaney et al., 2020; Gao et al., 2020b; Karpukhin et al., 2020; Zhan et al., 2020; Qu et al., 2020; Hofstätter et al., 2020a; Lin et al., 2020b). However, recognizing that accurate ranking cannot be captured via simple metrics, researchers have explored using more complex machinery to compare dense representations (Humeau et al., 2020; Khattab and Zaharia, 2020). Here, as with multi-stage ranking architectures, limitations on text length and effectiveness–efficiency tradeoffs are important considerations. It becomes increasingly difficult to accurately capture the semantics of longer text"
2021.naacl-tutorials.1,2020.emnlp-main.134,0,0.0264157,"yond BERT?” We describe efforts to build ranking models that are faster (i.e., lower inference latency), that are better (i.e., higher ranking effectiveness), or that manifest interesting tradeoffs between effectiveness and efficiency. These include ranking models that leverage BERT variants (Li et al., 2020), exploit knowledge distillation to train more compact student models (Gao et al., 2020a), and other transformer architectures, including groundup redesign efforts (Hofstätter et al., 2020b; Mitra et al., 2020) and adapting pretrained sequence-tosequence models (Nogueira et al., 2020; dos Santos et al., 2020). These discussions set up a natural transition to ranking based on dense representations, the other main category of approaches we cover. The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representations from Transformers) (Dev"
2021.naacl-tutorials.1,2020.acl-main.504,0,0.0227522,"e for Informatics David R. Cheriton School of Computer Science, University of Waterloo Overview to handle long input sequences and hence difficulty in ranking texts beyond a certain length (e.g., “full-length” documents such as news articles). This limitation is addressed by a number of models (Nogueira and Cho, 2019; Akkalyoncu Yilmaz et al., 2019; Dai and Callan, 2019b; MacAvaney et al., 2019; Wu et al., 2020; Li et al., 2020), and a simple retrieve-then-rerank approach can be elaborated into a multi-stage architecture with reranker pipelines (Nogueira et al., 2019a; Matsubara et al., 2020; Soldaini and Moschitti, 2020) that balance effectiveness and efficiency. On top of multi-stage ranking architectures, researchers have proposed additional innovations, including query expansion (Zheng et al., 2020), document expansion (Nogueira et al., 2019b; Nogueira and Lin, 2019) and term importance prediction (Dai and Callan, 2019a, 2020). A natural question that arises is, “What’s beyond BERT?” We describe efforts to build ranking models that are faster (i.e., lower inference latency), that are better (i.e., higher ranking effectiveness), or that manifest interesting tradeoffs between effectiveness and efficiency. Th"
2021.naacl-tutorials.1,2020.findings-emnlp.63,1,0.845091,"that arises is, “What’s beyond BERT?” We describe efforts to build ranking models that are faster (i.e., lower inference latency), that are better (i.e., higher ranking effectiveness), or that manifest interesting tradeoffs between effectiveness and efficiency. These include ranking models that leverage BERT variants (Li et al., 2020), exploit knowledge distillation to train more compact student models (Gao et al., 2020a), and other transformer architectures, including groundup redesign efforts (Hofstätter et al., 2020b; Mitra et al., 2020) and adapting pretrained sequence-tosequence models (Nogueira et al., 2020; dos Santos et al., 2020). These discussions set up a natural transition to ranking based on dense representations, the other main category of approaches we cover. The goal of text ranking is to generate an ordered list of texts retrieved from a corpus in response to a query for a particular task. Although the most common formulation of text ranking is search, instances of the task can also be found in many text processing applications. This tutorial provides an overview of text ranking with neural network architectures known as transformers, of which BERT (Bidirectional Encoder Representatio"
2021.naacl-tutorials.1,2020.findings-emnlp.424,1,0.85427,"Missing"
2021.nlp4prog-1.10,2021.ccl-1.108,0,0.0867301,"Missing"
2021.repl4nlp-1.17,N19-1423,0,0.0370386,"2019 Deep Learning Track demonstrate that our approach helps models learn robust representations for dense retrieval effectively and efficiently. 1 Introduction For well over half a century, solutions to the ad hoc retrieval problem—where the system’s task is return a list of top k texts from an arbitrarily large corpus D that maximizes some metric of quality such as average precision or NDCG—has been dominated by sparse vector representations, for example, bag-of-words BM25. Even in modern multi-stage ranking architectures, which take advantage of large pretrained transformers such as BERT (Devlin et al., 2019), the models are deployed as rerankers over initial candidates retrieved based on sparse vector representations; this is sometimes called “first-stage retrieval”. One well-known example of this design is the BERT-based reranker of Nogueira and Cho (2019); see Lin et al. (2020) for a recent survey. ∗ Contributed equally. The standard reranker architecture, while effective, exhibits high query latency, on the order of seconds per query (Hofstätter and Hanbury, 2019; Khattab and Zaharia, 2020) because expensive neural inference must be applied at query time on query–passage pairs. This design is"
2021.repl4nlp-1.17,2020.emnlp-main.550,0,0.0491438,"Missing"
2021.repl4nlp-1.17,P19-1612,0,0.135694,") for a recent survey. ∗ Contributed equally. The standard reranker architecture, while effective, exhibits high query latency, on the order of seconds per query (Hofstätter and Hanbury, 2019; Khattab and Zaharia, 2020) because expensive neural inference must be applied at query time on query–passage pairs. This design is known as a cross-encoder (Humeau et al., 2020), which exploits query–passage attention interactions across all transformer layers. As an alternative, a biencoder design provides an approach to ranking with dense representations that is far more efficient than cross-encoders (Lee et al., 2019; Reimers and Gurevych, 2019; Khattab and Zaharia, 2020; Karpukhin et al., 2020; Luan et al., 2021; Xiong et al., 2021; Qu et al., 2020; Hofstätter et al., 2021). Prior to retrieval, the vector representations can be precomputed for each of the texts in a corpus. When retrieving texts in response to a given query, computationally expensive transformer inference is replaced by much faster approximate nearest neighbor (ANN) search (Liu et al., 2004; Malkov and Yashunin, 2020). Recently, researchers have proposed bi-encoders that produce multiple vectors to represent a query (or a passage) (Humea"
2021.repl4nlp-1.17,D19-1410,0,0.0328358,"vey. ∗ Contributed equally. The standard reranker architecture, while effective, exhibits high query latency, on the order of seconds per query (Hofstätter and Hanbury, 2019; Khattab and Zaharia, 2020) because expensive neural inference must be applied at query time on query–passage pairs. This design is known as a cross-encoder (Humeau et al., 2020), which exploits query–passage attention interactions across all transformer layers. As an alternative, a biencoder design provides an approach to ranking with dense representations that is far more efficient than cross-encoders (Lee et al., 2019; Reimers and Gurevych, 2019; Khattab and Zaharia, 2020; Karpukhin et al., 2020; Luan et al., 2021; Xiong et al., 2021; Qu et al., 2020; Hofstätter et al., 2021). Prior to retrieval, the vector representations can be precomputed for each of the texts in a corpus. When retrieving texts in response to a given query, computationally expensive transformer inference is replaced by much faster approximate nearest neighbor (ANN) search (Liu et al., 2004; Malkov and Yashunin, 2020). Recently, researchers have proposed bi-encoders that produce multiple vectors to represent a query (or a passage) (Humeau et al., 2020; Luan et al.,"
2021.sustainlp-1.8,D19-1352,1,0.89724,"Missing"
2021.sustainlp-1.8,2020.findings-emnlp.412,0,0.0333139,"in the Age of Muppets: Effectiveness–Efficiency Tradeoffs in Multi-Stage Ranking Yue Zhang,1∗ Chengcheng Hu,2∗ Yuqi Liu,2∗ Hui Fang,1 Jimmy Lin2 1 Department of Electrical and Computer Engineering, University of Delaware 2 David R. Cheriton School of Computer Science, University of Waterloo {zhangyue, hfang}@udel.edu, {c45hu, y899liu, jimmylin}@uwaterloo.ca Abstract 2019), and this realization has compelled the field to explore other approaches, for example, simplified models (Hofstätter et al., 2020; Soldaini and Moschitti, 2020; Mitra et al., 2020; MacAvaney et al., 2020; Gao et al., 2020; Jiang et al., 2020) and learned dense representations (Xiong et al., 2020; Lin et al., 2020b). We are also motivated by the desire to reduce the computational costs of ranking with transformers, but from a different perspective. Based on the observation that neural networks in general (and transformers in particular) have largely supplanted feature-based learning to rank (LTR) in modern information retrieval, we ask the question: What, if anything, does “traditional” feature-based learning to rank have to offer in the age of muppets?1 The subtext of this question is that we, as a field, should not forget our own"
2021.sustainlp-1.8,N19-1423,0,0.0203672,"Applied to the MS MARCO passage and document ranking tasks, we are able to achieve the same level of effectiveness, but with up to 18× increase in efficiency. Furthermore, our techniques are orthogonal to other methods focused on accelerating transformer inference, and thus can be combined for even greater efficiency gains. A higher-level message from our work is that, even though pretrained transformers dominate the modern IR landscape, there are still important roles for “traditional” LTR techniques, and that we should not forget history. 1 Introduction Pretrained transformers such as BERT (Devlin et al., 2019) have dramatically increased retrieval effectiveness in many tasks across a multitude of domains (Lin et al., 2020a). Nevertheless, in a standard “retrieve-then-rerank” setup, the application of pretrained transformer-based rerankers incurs large computational costs and long query latencies, making those rerankers unrealistic for many real-world applications. For example, according to the ColBERT paper (Khattab and Zaharia, 2020), reranking 1000 hits from the MS MARCO passage dataset takes 32.9 seconds per query. Other researchers have noted the computational costs of transformer-based rankers"
2021.sustainlp-1.8,2020.sustainlp-1.11,1,0.821882,"n be reduced in various ways. We can accelerate inference using smaller or simpler models. Gao et al. (2020) use distillation to transfer knowledge captured in a larger model into a smaller model, achieving substantial speedups with minimal effectiveness loss. Hofstätter et al. (2020) propose a simpler transformer model to capture contextual information that trades effectiveness for much faster inference. Additional examples of this approach include Mitra et al. (2020) and MacAvaney et al. (2020). An alternative is to introduce early-exit optimizations, as in Soldaini and Moschitti (2020) and Xin et al. (2020). Further speedups can be gained by making modifications to the backbone transformer model, as in Sanh et al. (2020). The key point is that our 6 Conclusions The “retrieve-then-rerank” approach with transformers has been demonstrated to be effective in many IR tasks, but poor efficiency makes it less attractive for real-world applications. Our goal is to increase the efficiency of the entire pipeline but at the same time maintain the same level of effectiveness: this is achieved by a feature-based learning-to-rank module that filters candidates prior to neural reranking. On the MS MARCO passag"
2021.sustainlp-1.8,2020.findings-emnlp.63,1,0.826021,"0.382 0.382 0.443 0.445 0.444 5.60s 0.92s 0.46s (6×) (12×) BoWd2q (1k) + BERT BoWd2q (10k) + LTRd2q (50) + BERT 1000 50 0.389 0.389 0.454 0.454 9.63s 0.83s (12×) BoWd2q (1k) + T5 BoWd2q (10k) + LTRd2q (50) + T5 1000 50 0.386 0.388 0.453 0.454 5.60s 0.63s (9×) Table 2: The effectiveness and efficiency of different pipeline configurations on the MS MARCO passage ranking task. The effectiveness of the pipelines with additional LTR modules are statistically indistinguishable from the baselines without the LTR modules. on final-stage neural reranking. Previous evaluations (Nogueira and Cho, 2019; Nogueira et al., 2020; Pradeep et al., 2021) have already verified that these two models serve as competitive baselines. We pad all the token sequences in the batch to have the same length and truncate them if their lengths exceed 512 tokens. can reach MRR@10 parity with the baseline. We conduct two-tailed paired t-tests to confirm that there are no significant effectiveness differences between results before and after inserting LTR as the filtering stage. Depending on the pipeline setup, we only need to perform neural inference on 20 to 100 candidates—precisely because of our LTR filtering. We report the per-quer"
2021.sustainlp-1.8,2020.acl-main.504,0,0.0217153,"achieve a good balance between effectiveness and efficiency in end-to-end retrieval. Motivated by the observation, dating back more than a decade, that effective techniques are often computationally expensive, multi-stage retrieval architectures control latency by applying expensive techniques over only the most promising candidates (Wang et al., 2011). This is often operationalized as optimizing for recall in the earlier stages of the pipeline. Specifically in the context of transformers, multi-stage neural pipelines have been explored in the past by many researchers (Nogueira et al., 2019a; Soldaini and Moschitti, 2020; Matsubara et al., 2020; Pradeep et al., 2021). The key difference in our work is the (re-)introduction of “traditional” feature-based learning-to-rank approaches alongside neural models. This aligns with our broader goal of investigating how learning to rank might contribute to modern retrieval approaches dominated by neural models. The computational costs associated with ranking using pretrained transformers can be reduced in various ways. We can accelerate inference using smaller or simpler models. Gao et al. (2020) use distillation to transfer knowledge captured in a larger model into a s"
C12-1164,P05-1033,0,0.0157314,"and (3). Within the #weight structure, terms follow their probabilities, which correspond to the P rtoken values in these equations. Notice that the translation distribution for the source token leave is uninformed by the context maternity leave, therefore the candidates laisser (Eng. let go, allow) and quitter (Eng. quit) have higher probabilities than congé (Eng. vacation, day off) in this model. 2.2 Machine Translation for Cross-Language IR State-of-the-art statistical MT systems typically use hierarchical phrase-based translation models based on a Synchronous Context-Free Grammar (SCFG) (Chiang, 2005). In an SCFG, the rule [X] ||α ||β ||A ||ℓ(α → β) indicates that the context free expansion X → α in the source language occurs synchronously with X → β in the target language, with a likelihood of ℓ(α → β).1 In this case, we call α the Left-Hand Side (LHS) of the rule, and β the Right-Hand Side (RHS) of the rule. We use indexed nonterminals (e.g., [X,1]) since in principle more than one nonterminal can appear on the right side. A sequence of token alignments A indicates which token in α is aligned to which target token in β. Consider the following four rules from an SCFG: R1 .[S] ||[S,1] ||[S"
C12-1164,J07-2003,0,0.327277,"of s, computed by: t (1) = arg max[ max ℓ(t, D|s)] = arg max[ max TM(t, D|s)LM(t)] t t D∈D(s,t) D∈D(s,t) Y   = arg max LM(t) max ℓ(r) t D∈D(s,t) (5) r∈D where TM and LM correspond to the translation and language model scores, and D(s, t) is the set of possible derivations that generates the pair of sentences (s, t) (e.g., the sequence of four rules that translate the example query in Section 2.2 is one such derivation). The likelihood of each grammar rule r, ℓ(r), is learned as part of the training process of the translation model, by generalizing from token alignments on the training data (Chiang, 2007). Decoders produce a set of candidate sentence translations in the process of computing equation (5), so we can generalize our model to consider the n candidates with the highest likelihoods, for some n > 1. We start by preprocessing the source query s and each candidate translation t (k) . For each source token s j , we use the derivation output to determine which grammar rules were used to produce t (k) , and the token alignments in these rules to determine which target tokens are associated with s j in the derivation. By doing this for each translation candidate t (k) , we construct a proba"
C12-1164,P10-4002,1,0.900468,"-Free Grammar (SCFG), in which probabilistic rules describe the translation of larger units of text. Finally, the translation grammar is combined with a language model to produce translations of entire sentences. As the whole process is statistically generated, it is at any point able to produce a ranked list of the highest scoring translations rather than only the one best choice. Although it is desirable to exploit these internal representations when performing retrieval, one possible disadvantage of using such a complex translation model is efficiency. However, modern decoders, e.g., cdec (Dyer et al., 2010), use pruning methods to efficiently search for the most likely translations of a given text. In this paper, we describe two ways to exploit these internal representations and construct context-sensitive term translation probabilities. One method is to extract a context-aware portion of the SCFG by selecting only the grammar rules that apply to a given query. Using token alignments within each rule, a probability distribution can be constructed to represent the translation candidates for each query token, an approach that we refer to as “phrase-based.” Another solution is to perform translatio"
C12-1164,D12-1061,0,0.0108839,"Missing"
C12-1164,N03-1017,0,0.0176968,"rnel, ...) #weight(1.0 cong) #weight(1.0 europ)) The overfitting issue is partially mitigated by using the n-best translation derivations, as opposed to the “1-best translation” approach, which treats the MT system as a black box. However, the lack of textual variety in the n most probable derivations is a known issue, caused by the fact that statistical MT systems identify the most probable derivations (not the most probable strings), many of which can correspond to the same surface form. This phenomenon is called “spurious ambiguity” in the MT literature, and it occurs in both phrase-based (Koehn et al., 2003) and hierarchical phrase-based MT systems (Chiang, 2007). For instance, according to Li et al. (2009), a string has an average of 115 distinct derivations in Chiang’s Hiero system. Researchers have proposed several ways to cope with this situation, and we plan to integrate some of these in our future work. However, an alternative approach is to exploit grammar rules directly: this allows us to increase variety without introducing noisy translations, and we discuss this approach next. 3.2 Probabilities from the Translation Grammar An alternative approach to exploit the MT system is to learn con"
C12-1164,J03-3003,0,0.0243537,"Missing"
C12-1164,P09-1067,0,0.0130128,"he n-best translation derivations, as opposed to the “1-best translation” approach, which treats the MT system as a black box. However, the lack of textual variety in the n most probable derivations is a known issue, caused by the fact that statistical MT systems identify the most probable derivations (not the most probable strings), many of which can correspond to the same surface form. This phenomenon is called “spurious ambiguity” in the MT literature, and it occurs in both phrase-based (Koehn et al., 2003) and hierarchical phrase-based MT systems (Chiang, 2007). For instance, according to Li et al. (2009), a string has an average of 115 distinct derivations in Chiang’s Hiero system. Researchers have proposed several ways to cope with this situation, and we plan to integrate some of these in our future work. However, an alternative approach is to exploit grammar rules directly: this allows us to increase variety without introducing noisy translations, and we discuss this approach next. 3.2 Probabilities from the Translation Grammar An alternative approach to exploit the MT system is to learn context-sensitive translation probabilities directly from the translation grammar. Hierarchical phrase-b"
C12-1164,D07-1104,0,0.0379392,"ut introducing noisy translations, and we discuss this approach next. 3.2 Probabilities from the Translation Grammar An alternative approach to exploit the MT system is to learn context-sensitive translation probabilities directly from the translation grammar. Hierarchical phrase-based MT systems use suffix arrays to extract all rules in an SCFG which apply to a given source text, requiring a 3 Since a source token may be aligned to multiple target tokens in the same query translation, we still need to normalize the final likelihood values. 2690 smaller memory footprint in the decoding phase (Lopez, 2007). We can use this feature to learn a token translation probability mapping that is a middle point between P rtoken and P rnbest in terms of context-aware choices and providing a varied set of translation alternatives. We propose the following method to construct a probability distribution from a set of SCFG rules: For each grammar rule, we use the token alignments to determine which source token translates to which target token(s) in the phrase pair. Going over all grammar rules that apply to a given query, we construct a probability distribution for each token that appears on the LHS. More sp"
C12-1164,P99-1027,0,0.443368,"cal Papers, pages 2685–2702, COLING 2012, Mumbai, December 2012. 2685 1 Introduction Cross-Language Information Retrieval (CLIR) is the problem of retrieving documents relevant to a query written in a different language. There are two main approaches to tackle this problem: translating the query into the document language, or translating documents into the query language. Query translation has become the more popular approach for experimental work due to the computational feasibility of trying different system variants without repeatedly translating the entire document collection (Oard, 1998; McCarley, 1999). Query translation approaches for CLIR can be pursued either by applying a Machine Translation (MT) system or by using a token-to-token bilingual mapping, with or without translation probabilities. These approaches have complementary strengths: MT makes good use of context but at the cost of typically producing only one-best results, while token-to-token mappings can produce n-best token translations but without leveraging available contextual clues. This has led to a small cottage industry of what we might refer to as “context recovery” in which postprocessing techniques are used to select o"
C12-1164,E12-1012,0,0.19374,"Missing"
C12-1164,oard-1998-comparative,1,0.792241,"2012: Technical Papers, pages 2685–2702, COLING 2012, Mumbai, December 2012. 2685 1 Introduction Cross-Language Information Retrieval (CLIR) is the problem of retrieving documents relevant to a query written in a different language. There are two main approaches to tackle this problem: translating the query into the document language, or translating documents into the query language. Query translation has become the more popular approach for experimental work due to the computational feasibility of trying different system variants without repeatedly translating the entire document collection (Oard, 1998; McCarley, 1999). Query translation approaches for CLIR can be pursued either by applying a Machine Translation (MT) system or by using a token-to-token bilingual mapping, with or without translation probabilities. These approaches have complementary strengths: MT makes good use of context but at the cost of typically producing only one-best results, while token-to-token mappings can produce n-best token translations but without leveraging available contextual clues. This has led to a small cottage industry of what we might refer to as “context recovery” in which postprocessing techniques are"
C12-1164,J03-1002,0,0.014117,"ocessing. The collections contain 383,872, 388,589 and 177,452 documents, and 50, 50, and 73 topics, respectively. We learned our English-to-Arabic translation model using 3.4 million aligned sentence pairs from the GALE 2010 evaluation. Our English-to-Chinese translation model was trained on 302,996 aligned sentence pairs from the FBIS parallel text collection. We trained an Englishto-French translation model using 2.2 million aligned sentence pairs from the latest Europarl corpus (version 7) that was built from the European parliament proceedings.4 Token alignments were learned with GIZA++ (Och and Ney, 2003), using 5 Model 1 and 5 HMM iterations. An SCFG serves as the basis for the translation model (Chiang, 2007), which was extracted from these token alignments using a suffix array (Lopez, 2007). We used cdec for decoding, due to its support for SCFG-based models and its efficient C-based implementation, making it faster than most of the other state-of-the-art systems (Dyer et al., 2010). A 3-gram language model was trained from the target side of the training data for Chinese and Arabic, using the SRILM toolkit (Stolcke, 2002). For French, we trained a 5-gram LM from the monolingual dataset pro"
C12-1164,I05-3027,0,\N,Missing
C18-1178,D13-1160,0,0.0980794,"rk The development and continual advance of question answering techniques over knowledge graphs require benchmark datasets that cover different aspects of the task. Quite obviously, each dataset has to target one (or more) knowledge graphs, which means that the structure of the answers are dictated by the conceptual organization of the particular knowledge graph. Over the years, researchers have built a number of datasets based on Freebase (Bollacker et al., 2008). For instance, F REE 917 (Cai and Yates, 2013) contains 917 questions involving 635 distinct Freebase predicates. W EB Q UESTIONS (Berant et al., 2013) contains 5,810 question-answer pairs collected using the Google Suggest API and manually answered using Amazon Mechanical Turk (AMT). Both contain answers that require complex, multi-hop traversals of the knowledge graph. In contrast, the S IMPLE Q UESTIONS dataset focuses on questions that can be answered via the lookup of a single fact (i.e., triple). Due to its much larger size and thus support for data-hungry machine learning techniques, this dataset has gained great popularity with researchers. Unfortunately, Google shut down Freebase in 2015; a final snapshot of the knowledge graph is s"
C18-1178,P13-1042,0,0.0273732,"esearchers honest” in guarding against model overfitting on a single dataset. 2 Background and Related Work The development and continual advance of question answering techniques over knowledge graphs require benchmark datasets that cover different aspects of the task. Quite obviously, each dataset has to target one (or more) knowledge graphs, which means that the structure of the answers are dictated by the conceptual organization of the particular knowledge graph. Over the years, researchers have built a number of datasets based on Freebase (Bollacker et al., 2008). For instance, F REE 917 (Cai and Yates, 2013) contains 917 questions involving 635 distinct Freebase predicates. W EB Q UESTIONS (Berant et al., 2013) contains 5,810 question-answer pairs collected using the Google Suggest API and manually answered using Amazon Mechanical Turk (AMT). Both contain answers that require complex, multi-hop traversals of the knowledge graph. In contrast, the S IMPLE Q UESTIONS dataset focuses on questions that can be answered via the lookup of a single fact (i.e., triple). Due to its much larger size and thus support for data-hungry machine learning techniques, this dataset has gained great popularity with re"
C18-1178,P16-1076,0,0.0198131,"le research communities, with many commercial deployments. To ensure continued progress, it is important that open and relevant benchmarks are available to support the comparison of various techniques. In this paper, we focus on the class of questions that can be answered by a single triple (i.e., fact) from a knowledge graph. For example, the question “What type of music is on the album Phenomenon?” can be answered via the lookup of a simple fact—in this case, the “genre” property of the entity “Phenomenon”. Analysis of an existing benchmark dataset (Yao, 2015) and real-world user questions (Dai et al., 2016; Ture and Jojic, 2017) show that such questions cover a broad range of users’ needs. The S IMPLE Q UESTIONS dataset (Bordes et al., 2015) has emerged as the de facto benchmark for evaluating these simple questions over knowledge graphs. However, there is one major deficiency with this resource: the answers draw from Freebase. Unfortunately, Freebase is defunct and no longer maintained. This creates a number of insurmountable challenges: First, because the knowledge graph is stale, it is no longer possible to build a “real-world” operational QA system using models trained on S IMPLE Q UESTIONS"
C18-1178,N18-2047,1,0.893362,"er to DBpedia. 2099 Freebase Predicate DBpedia Predicate Directionality Type Constraint fb:architecture/structure/architect dbo:architect forward backward backward backward backward - fb:location/location/contains dbo:country fb:baseball/baseball position/players dbo:position fb:music/album release type/albums dbo:type fb:book/author/works written dbo:author dbo:BaseballPlayer dbo:Album dbo:WrittenWork Table 4: Examples of predicate mapping rules. 5 Question Answering Baseline To lay the foundation for future work on our new dataset, we provide simple yet strong baselines using recent work by Mohammed et al. (2018), who applied techniques with and without neural networks to S IMPLE Q UESTIONS. In this paper, we used their open-source code5 to generate the experimental results reported here. We briefly describe their approach, which decomposes into four tasks: • Entity Detection: Given a question, the task is to identify the topic entity of the question. For this task, we examined bidirectional LSTMs and Conditional Random Fields (CRFs). • Entity Linking: Detected entities (text strings) need to be linked to entities in the knowledge graph (e.g., URI from DBpedia in our case). This is formulated as a str"
C18-1178,D17-1307,0,0.0211142,"ities, with many commercial deployments. To ensure continued progress, it is important that open and relevant benchmarks are available to support the comparison of various techniques. In this paper, we focus on the class of questions that can be answered by a single triple (i.e., fact) from a knowledge graph. For example, the question “What type of music is on the album Phenomenon?” can be answered via the lookup of a simple fact—in this case, the “genre” property of the entity “Phenomenon”. Analysis of an existing benchmark dataset (Yao, 2015) and real-world user questions (Dai et al., 2016; Ture and Jojic, 2017) show that such questions cover a broad range of users’ needs. The S IMPLE Q UESTIONS dataset (Bordes et al., 2015) has emerged as the de facto benchmark for evaluating these simple questions over knowledge graphs. However, there is one major deficiency with this resource: the answers draw from Freebase. Unfortunately, Freebase is defunct and no longer maintained. This creates a number of insurmountable challenges: First, because the knowledge graph is stale, it is no longer possible to build a “real-world” operational QA system using models trained on S IMPLE Q UESTIONS. Second, a defunct kno"
C18-1178,N15-3014,0,0.0178381,"tant problem at the intersection of multiple research communities, with many commercial deployments. To ensure continued progress, it is important that open and relevant benchmarks are available to support the comparison of various techniques. In this paper, we focus on the class of questions that can be answered by a single triple (i.e., fact) from a knowledge graph. For example, the question “What type of music is on the album Phenomenon?” can be answered via the lookup of a simple fact—in this case, the “genre” property of the entity “Phenomenon”. Analysis of an existing benchmark dataset (Yao, 2015) and real-world user questions (Dai et al., 2016; Ture and Jojic, 2017) show that such questions cover a broad range of users’ needs. The S IMPLE Q UESTIONS dataset (Bordes et al., 2015) has emerged as the de facto benchmark for evaluating these simple questions over knowledge graphs. However, there is one major deficiency with this resource: the answers draw from Freebase. Unfortunately, Freebase is defunct and no longer maintained. This creates a number of insurmountable challenges: First, because the knowledge graph is stale, it is no longer possible to build a “real-world” operational QA s"
D08-1044,P01-1005,0,0.0610597,"any academic research groups. This paper illustrates these points with a case study in building word cooccurrence matrices from large corpora. I conclude with an analysis of an alternative computing model based on renting instead of buying computer clusters. 1 Introduction Over the past couple of decades, the field of computational linguistics (and more broadly, human language technologies) has seen the emergence and later dominance of empirical techniques and datadriven research. Concomitant with this trend is a coherent research thread that focuses on exploiting increasingly-large datasets. Banko and Brill (2001) were among the first to demonstrate the importance of dataset size as a significant factor governing prediction accuracy in a supervised machine learning Challenges in scaling algorithms to increasinglylarge datasets have become a serious issue for researchers. It is clear that datasets readily available today and the types of analyses that researchers wish to conduct have outgrown the capabilities of individual computers. The only practical recourse is to distribute the computation across multiple cores, processors, or machines. The consequences of failing to scale include misleading general"
D08-1044,D07-1090,0,0.601969,"s with MapReduce Jimmy Lin The iSchool, University of Maryland National Center for Biotechnology Information, National Library of Medicine jimmylin@umd.edu Abstract task. In fact, they argued that size of training set was perhaps more important than the choice of machine learning algorithm itself. Similarly, experiments in question answering have shown the effectiveness of simple pattern-matching techniques when applied to large quantities of data (Brill et al., 2001; Dumais et al., 2002). More recently, this line of argumentation has been echoed in experiments with Web-scale language models. Brants et al. (2007) showed that for statistical machine translation, a simple smoothing technique (dubbed Stupid Backoff) approaches the quality of the Kneser-Ney algorithm as the amount of training data increases, and with the simple method one can process significantly more data. This paper explores the challenge of scaling up language processing algorithms to increasingly large datasets. While cluster computing has been available in commercial environments for several years, academic researchers have fallen behind in their ability to work on large datasets. I discuss two barriers contributing to this problem:"
D08-1044,J90-1003,0,0.0501066,"f a corpus is a square N × N matrix where N corresponds to the number of unique words in the corpus. A cell mij contains the number of times word wi co-occurs with word wj within a specific context—a natural unit such as a sentence or a certain window of m words (where m is an application-dependent parameter). Note that the upper and lower triangles of the matrix are identical since co-occurrence is a symmetric relation. This task is quite common in corpus linguistics and provides the starting point to many other algorithms, e.g., for computing statistics such as pointwise mutual information (Church and Hanks, 1990), for unsupervised sense clustering (Sch¨utze, 1998), and more generally, a large body of work in lexical semantics based on distributional profiles, dating back to Firth (1957) and Harris (1968). The task also has applications in information retrieval, e.g., (Sch¨utze and Pedersen, 1998; Xu and Croft, 1998), and other related fields as well. More generally, this problem relates to the task of estimating distributions of discrete events from a large number of observations (more on this in Section 7). It is obvious that the space requirement for this problem is O(N 2 ), where N is the size of t"
D08-1044,W08-0333,1,0.740511,"a network, combiners allow a programmer to aggregate partial results, thus reducing network traffic. In cases where an operation is both associative and commutative, reducers can directly serve as combiners. Google’s proprietary implementation of MapReduce is in C++ and not available to the public. However, the existence of Hadoop, an open-source implementation in Java spearheaded by Yahoo, allows anyone to take advantage of MapReduce. The growing popularity of this technology has stimulated a flurry of recent work, on applications in machine learning (Chu et al., 2006), machine translation (Dyer et al., 2008), and document retrieval (Elsayed et al., 2008). 421 Word Co-occurrence Matrices To illustrate the arguments outlined above, I present a case study using MapReduce to build word cooccurrence matrices from large corpora, a common task in natural language processing. Formally, the co-occurrence matrix of a corpus is a square N × N matrix where N corresponds to the number of unique words in the corpus. A cell mij contains the number of times word wi co-occurs with word wj within a specific context—a natural unit such as a sentence or a certain window of m words (where m is an application-dependen"
D08-1044,P08-2067,1,0.822049,"Missing"
D08-1044,W06-1605,0,0.0152163,"it in memory, a naive implementation can be very slow as memory is paged to disk. For large corpora, one needs to optimize disk access and avoid costly seeks. As illustrated in the next section, MapReduce handles exactly these issues transparently, allowing the programmer to express the algorithm in a straightforward manner. A bit more discussion of the task before moving on: in many applications, researchers have discovered that building the complete word cooccurrence matrix may not be necessary. For example, Sch¨utze (1998) discusses feature selection techniques in defining context vectors; Mohammad and Hirst (2006) present evidence that conceptual distance is better captured via distributional profiles mediated by thesaurus categories. These objections, however, miss the point—the focus of this paper is on practical cluster computing for academic researchers; this particular task serves merely as an illustrative example. In addition, for rapid prototyping, it may be useful to start with the complete co-occurrence matrix (especially if it can be built efficiently), and then explore how algorithms can be optimized for specific applications and tasks. procedure M AP1 (n, d) for all w ∈ d do 3: for all u ∈"
D08-1044,J98-1004,0,0.284823,"Missing"
D15-1181,S12-1051,0,0.148195,"eling and similarity measurement. For sentence modeling, we use a convolutional neural network featuring convolution filters with multiple granularities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4). For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine distance, Euclidean distance, and element-wise difference (Sec. 5). We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks (Agirre et al., 2012; Marelli et al., 2014), and highly competitive performance on the Microsoft Research paraphrase (MSRP) identification task (Dolan et al., 2004). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long ShortTerm Memory (LSTM) neural networks of Tai et al. (2015) without using parsers or part-ofspeech taggers. On the MSRP task, we outperform the recently-proposed convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonst"
D15-1181,S12-1059,0,0.0108059,"Missing"
D15-1181,P14-1114,0,0.027304,"Missing"
D15-1181,S14-2114,0,0.0352034,"Missing"
D15-1181,D12-1050,0,0.0152731,"Missing"
D15-1181,P09-1053,0,0.0209443,"(1) string-based, including n-gram overlap features on both the word and character levels (Wan et al., 2006) and features based on machine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional"
D15-1181,C04-1051,0,0.305727,"larities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4). For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine distance, Euclidean distance, and element-wise difference (Sec. 5). We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks (Agirre et al., 2012; Marelli et al., 2014), and highly competitive performance on the Microsoft Research paraphrase (MSRP) identification task (Dolan et al., 2004). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long ShortTerm Memory (LSTM) neural networks of Tai et al. (2015) without using parsers or part-ofspeech taggers. On the MSRP task, we outperform the recently-proposed convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonstrating clear benefits from our use of multiple perspectives both in sentence modeling and structured similarity measurement. 2 Related Work Most"
D15-1181,I05-5003,0,0.00955795,"Missing"
D15-1181,N13-1092,0,0.0723157,"Missing"
D15-1181,P12-1091,0,0.0163219,"hine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner"
D15-1181,D13-1090,0,0.08521,"roduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent parse. Ji and Eisenstein (2013) used matrix factorization techniques to obtain sentence representations, and combined them with fine-tuned sparse features using an SVM classifier for similarity prediction. Both Socher et al. and Ji and Eisenstein incorporated sparse features to improve performance, which we do not use in this work. Hu et al. (2014) used convolutional neural networks that combine hierarchical sentence modeling with layer-by-layer composition and pooling. While they performed comparisons directly over entire sentence representations, we instead develop a structured similarity measurement layer to compare loca"
D15-1181,S14-2131,0,0.122905,"Missing"
D15-1181,P14-1062,0,0.0453703,"Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent par"
D15-1181,D14-1181,0,0.0249844,"veloped a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooling to better model inputs of varying sizes. Kim (2014) proposed several modifications to the convolutional neural network architecture of Collobert and Weston (2008), including the use of both fixed and learned word vectors and varying window sizes of the convolution filters. For the MSRP task, Socher et al. (2011) used a recursive neural network to model each sentence, recursively computing the representation for the sentence from the representations of its constituents in a binarized constituent parse. Ji and Eisenstein (2013) used matrix factorization techniques to obtain sentence representations, and combined them with fine-tuned sparse featu"
D15-1181,Q14-1017,0,0.106757,"Missing"
D15-1181,S14-2055,0,0.079445,"Missing"
D15-1181,N12-1019,0,0.0261517,"in Natural Language Processing, pages 1576–1586, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtain features (Hassan, 2011; Guo and Diab, 2012). Several strongly-performing approaches used system combination (Das and Smith, 2009; Madnani et al., 2012) or multi-task learning. Xu et al. (2014) developed a feature-rich multi-instance learning model that jointly learns paraphrase relations between word and sentence pairs. Recent work has moved away from handcrafted features and towards modeling with distributed representations and neural network architectures. Collobert and Weston (2008) used convolutional neural networks in a multitask setting, where their model is trained jointly for multiple NLP tasks with shared weights. Kalchbrenner et al. (2014) introduced a convolutional neural network for sentence modeling that uses dynamic k-max pooli"
D15-1181,P14-5010,0,0.00337417,"We conduct experiments with ws values in the range [1, 3] as well as ws = ∞ (no convolution). We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags. We use the Dim g = 300dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dim k = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dim p = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS embeddings is expected to retain syntactic information which is reported to be effective for paraphrase identification (Das and Smith, 2009). We use POS embeddings only for the MSRP task. Therefore for MSRP, we concatenate all word and POS embeddings and obtain Dim = Dim g + Dim p + Dim k = 525-dimension vectors for each input word; fo"
D15-1181,S14-2001,0,0.125109,"measurement. For sentence modeling, we use a convolutional neural network featuring convolution filters with multiple granularities and window sizes, followed by multiple types of pooling. We experiment with two types of word embeddings as well as partof-speech tag embeddings (Sec. 4). For similarity measurement, we compare pairs of local regions of the sentence representations, using multiple distance functions: cosine distance, Euclidean distance, and element-wise difference (Sec. 5). We demonstrate state-of-the-art performance on two SemEval semantic relatedness tasks (Agirre et al., 2012; Marelli et al., 2014), and highly competitive performance on the Microsoft Research paraphrase (MSRP) identification task (Dolan et al., 2004). On the SemEval-2014 task, we match the state-of-the-art dependency tree Long ShortTerm Memory (LSTM) neural networks of Tai et al. (2015) without using parsers or part-ofspeech taggers. On the MSRP task, we outperform the recently-proposed convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonstrating clear benefits f"
D15-1181,D14-1162,0,0.113318,"bθk + ||θ||22 (6) m 2 m loss(θ) = k=1 where fbθ is the predicted distribution with model weight vector θ, f is the ground truth, m is the number of training examples, and λ is the regularization parameter. Note that we use the same KL-loss function and same sparse target distribution technique as Tai et al. (2015). 6.3 Experiment Settings We conduct experiments with ws values in the range [1, 3] as well as ws = ∞ (no convolution). We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags. We use the Dim g = 300dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dim k = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dim p = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS"
D15-1181,S12-1100,0,0.0426687,"Missing"
D15-1181,S12-1060,0,0.0122495,"Missing"
D15-1181,P15-1150,0,0.059497,"Missing"
D15-1181,U06-1019,0,0.0215901,"convolutional neural network model of Yin and Sch¨utze (2015) without any pretraining. In addition, we perform ablation experiments to show the contribution of our modeling decisions for all three datasets, demonstrating clear benefits from our use of multiple perspectives both in sentence modeling and structured similarity measurement. 2 Related Work Most previous work on modeling sentence similarity has focused on feature engineering. Several types of sparse features have been found useful, including: (1) string-based, including n-gram overlap features on both the word and character levels (Wan et al., 2006) and features based on machine translation evaluation metrics (Madnani 1576 Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing, pages 1576–1586, c Lisbon, Portugal, 17-21 September 2015. 2015 Association for Computational Linguistics. et al., 2012); (2) knowledge-based, using external lexical resources such as WordNet (Fellbaum, 1998; Fern and Stevenson, 2008); (3) syntaxbased, e.g., modeling divergence of dependency syntax between the two sentences (Das and Smith, 2009); (4) corpus-based, using distributional models such as latent semantic analysis to obtai"
D15-1181,S12-1096,0,0.0315849,"Missing"
D15-1181,Q15-1025,1,0.0633854,", f is the ground truth, m is the number of training examples, and λ is the regularization parameter. Note that we use the same KL-loss function and same sparse target distribution technique as Tai et al. (2015). 6.3 Experiment Settings We conduct experiments with ws values in the range [1, 3] as well as ws = ∞ (no convolution). We use multiple kinds of embeddings to represent each sentence, both on words and part-ofspeech (POS) tags. We use the Dim g = 300dimensional GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens. We use Dim k = 25-dimensional PARAGRAM vectors (Wieting et al., 2015) only on the MSRP task since they were developed for paraphrase tasks, having been trained on word pairs from the Paraphrase Database (Ganitkevitch et al., 2013). For POS embeddings, we run the Stanford POS tagger (Manning et al., 2014) on the English side of the Xinhua machine translation parallel corpus, which consists of Xinhua news articles with approximately 25 million words. We then train Dim p = 200-dimensional POS embeddings using the word2vec toolkit (Mikolov et al., 2013). Adding POS embeddings is expected to retain syntactic information which is reported to be effective for paraphra"
D15-1181,N15-1091,0,0.529578,"Missing"
D15-1181,S14-2044,0,0.212805,"Missing"
D15-1181,Q14-1034,0,\N,Missing
D17-1285,D14-1179,0,0.0107995,"Missing"
D17-1285,D15-1181,1,0.879973,"Missing"
D17-1285,N16-1108,1,0.839608,"agannatha and Yu (2016) adopted an LSTM model for medical entity detection given patient EHR records. There are recent work with the use of deep reinforcement learning on healthcare study (Li, 2017). Our approach is inspired by recent embedding learning work to jointly represent texts and knowledge base (Toutanova et al., 2015, 2016), previous work on embedding transfer learning (Bordes et al., 2013) and noisecontrastive estimation (Rao et al., 2016). Lastly our work models insight extraction as a similarity measurement problem, and is inspired by similarity measurement work (He et al., 2016; He and Lin, 2016) on pairwise word interaction modeling with deep neural networks. 3 System Overview We provide a recipe to build a system for biomedical insight extraction and use it as a guide for the remainder of this paper (Algorithm 1). To make our discussion concrete, we will use a sample biomedical article in Example 1. Given the text, at line 4 of Algorithm 1 we firstly look for all named entities using a shallow parser and public medical dictionaries (see details in Section 4). Many named entities could be found, for example, “clinical study”, “sleep disturbances in middleaged men” and “diabetes”. Nex"
D17-1285,S16-1170,1,0.837756,"many NLP tasks. Jagannatha and Yu (2016) adopted an LSTM model for medical entity detection given patient EHR records. There are recent work with the use of deep reinforcement learning on healthcare study (Li, 2017). Our approach is inspired by recent embedding learning work to jointly represent texts and knowledge base (Toutanova et al., 2015, 2016), previous work on embedding transfer learning (Bordes et al., 2013) and noisecontrastive estimation (Rao et al., 2016). Lastly our work models insight extraction as a similarity measurement problem, and is inspired by similarity measurement work (He et al., 2016; He and Lin, 2016) on pairwise word interaction modeling with deep neural networks. 3 System Overview We provide a recipe to build a system for biomedical insight extraction and use it as a guide for the remainder of this paper (Algorithm 1). To make our discussion concrete, we will use a sample biomedical article in Example 1. Given the text, at line 4 of Algorithm 1 we firstly look for all named entities using a shallow parser and public medical dictionaries (see details in Section 4). Many named entities could be found, for example, “clinical study”, “sleep disturbances in middleaged men”"
D17-1285,W09-2415,0,0.0546815,"Missing"
D17-1285,P82-1020,0,0.808678,"Missing"
D17-1285,N16-1056,0,0.0275769,"the input do 4: Identify all possible named entities ~ B) ~ do 5: for each named entity pair (A, 6: if causality/correlation holds then ~ B) ~ 7: Extract and Score (A, 8: end if 9: end for 10: end for ~ B) ~ pairs 11: Rank all extracted (A, 12: return top ranked entity pairs knowledge-based features from medical dictionaries and word position features. Our work instead propose neural network models that do not require sparse features as in most previous work. Recent shift from feature engineering to model engineering with neural networks has significantly improved accuracy on many NLP tasks. Jagannatha and Yu (2016) adopted an LSTM model for medical entity detection given patient EHR records. There are recent work with the use of deep reinforcement learning on healthcare study (Li, 2017). Our approach is inspired by recent embedding learning work to jointly represent texts and knowledge base (Toutanova et al., 2015, 2016), previous work on embedding transfer learning (Bordes et al., 2013) and noisecontrastive estimation (Rao et al., 2016). Lastly our work models insight extraction as a similarity measurement problem, and is inspired by similarity measurement work (He et al., 2016; He and Lin, 2016) on pa"
D17-1285,P14-5010,0,0.00471093,"ter tailored for ranking tasks empirically (Verga et al., 2016): lossrelationSim (w, x, R~+ , R~− ) = X − log(σ(fw0 (x, R~+ ) − fw0 (x, R~− ))) (11) R~− where σ is the sigmoid function, function ~ represents the relational similarity fw0 (x, R) model with BiLSTM, and outputs a similarity score for ranking purpose (Sec. 5.2). In all experiments, we perform optimization using RMSProp (Tieleman and Hinton, 2012) with backpropagation (Bottou, 1998) and a learning rate fixed to 10−4 and a momentum parameter 0.9. Settings and Preprocessing. We preprocess both datasets with Stanford CoreNLP toolkit (Manning et al., 2014). We tokenize, lowercase, sentence split and dependency parse all words of both datasets. We set LSTM hidden state dim = 500. Two sets of d = 300-dimension word embeddings are utilized. The first one is 300-dimension GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens; for better biomedical/health domain adaptation, we also train second word embeddings using the GloVe toolkit on biomedical research articles with over 1 billion tokens. We do not update word embeddings in all experiments. During system deployment, we only initialize input words with the medical word emb"
D17-1285,P16-1105,0,0.0553314,"Missing"
D17-1285,D14-1162,0,0.0803531,"M, and outputs a similarity score for ranking purpose (Sec. 5.2). In all experiments, we perform optimization using RMSProp (Tieleman and Hinton, 2012) with backpropagation (Bottou, 1998) and a learning rate fixed to 10−4 and a momentum parameter 0.9. Settings and Preprocessing. We preprocess both datasets with Stanford CoreNLP toolkit (Manning et al., 2014). We tokenize, lowercase, sentence split and dependency parse all words of both datasets. We set LSTM hidden state dim = 500. Two sets of d = 300-dimension word embeddings are utilized. The first one is 300-dimension GloVe word embeddings (Pennington et al., 2014) trained on 840 billion tokens; for better biomedical/health domain adaptation, we also train second word embeddings using the GloVe toolkit on biomedical research articles with over 1 billion tokens. We do not update word embeddings in all experiments. During system deployment, we only initialize input words with the medical word embeddings if they do not exist in GloVe embeddings’ vocabulary. We also concatenate embeddings of both input words and their head words on dependency trees as input for relation extraction models. We follow the task settings and compute F1-score with the official ev"
D17-1285,N10-1123,0,0.0324289,"n acceptance accuracy, and on a SemEval task evaluation its causality/correlation relation extraction compares favorably against previous state-of-the-art work. 1.1 Contributions 1. We build an end-to-end system to extract insights from biomedical literature. 2. We innovate in similarity measurement modeling with deep neural networks for better causality/correlation relation extraction. 3. Our human evaluation show our system can achieve competitive acceptance accuracy. 2 Related Work Most previous work in BioNLP focused on extraction of biomedical concepts (Craven, 1999; Finkel et al., 2005; Poon and Vanderwende, 2010; Simpson and Demner-Fushman, 2012; Liu, 2016), such as drug or protein names. We also conduct relation extraction on general named entities, such as “smoking” or “sleep quality”. Kabiljo et al. (2009) compared pattern-matching techniques against a baseline regular expression approach for gene/protein entity extraction. But existing tools for relation extraction are not as comprehensive as entity recognition tools. Medical dictionaries and resources are heavily utilized by previous work. For instance, Chen et al. (2008) extracted disease-drug relation pairs with MedLEE (Friedman et al., 2004)"
D17-1285,S10-1057,0,0.067663,"Missing"
D17-1285,D15-1174,0,0.0140958,"medical dictionaries and word position features. Our work instead propose neural network models that do not require sparse features as in most previous work. Recent shift from feature engineering to model engineering with neural networks has significantly improved accuracy on many NLP tasks. Jagannatha and Yu (2016) adopted an LSTM model for medical entity detection given patient EHR records. There are recent work with the use of deep reinforcement learning on healthcare study (Li, 2017). Our approach is inspired by recent embedding learning work to jointly represent texts and knowledge base (Toutanova et al., 2015, 2016), previous work on embedding transfer learning (Bordes et al., 2013) and noisecontrastive estimation (Rao et al., 2016). Lastly our work models insight extraction as a similarity measurement problem, and is inspired by similarity measurement work (He et al., 2016; He and Lin, 2016) on pairwise word interaction modeling with deep neural networks. 3 System Overview We provide a recipe to build a system for biomedical insight extraction and use it as a guide for the remainder of this paper (Algorithm 1). To make our discussion concrete, we will use a sample biomedical article in Example 1."
D17-1285,P16-1136,0,0.0279357,"Missing"
D17-1285,S10-1049,0,0.0494285,"Missing"
D17-1285,S10-1047,0,0.0367543,"Missing"
D17-1285,N16-1103,0,0.0147702,"label of the relational similarity model is binary because the BPR loss ranks positive inputs above negative inputs, thereby requiring the supervision signal to distinguish positives from negatives. Due to BPR loss’s ranking nature, each training instance of the relational similarity model include one positive input (x, R~+ ) and one negative input (x, R~− ). Given a positive correlation/causality input (R~+ ), we generate negative training examples by matching the input x with each of the negative relation labels (R~− ). BPR loss is shown to be better tailored for ranking tasks empirically (Verga et al., 2016): lossrelationSim (w, x, R~+ , R~− ) = X − log(σ(fw0 (x, R~+ ) − fw0 (x, R~− ))) (11) R~− where σ is the sigmoid function, function ~ represents the relational similarity fw0 (x, R) model with BiLSTM, and outputs a similarity score for ranking purpose (Sec. 5.2). In all experiments, we perform optimization using RMSProp (Tieleman and Hinton, 2012) with backpropagation (Bottou, 1998) and a learning rate fixed to 10−4 and a momentum parameter 0.9. Settings and Preprocessing. We preprocess both datasets with Stanford CoreNLP toolkit (Manning et al., 2014). We tokenize, lowercase, sentence split a"
D19-1114,D15-1075,0,0.0695342,"Missing"
D19-1114,D14-1082,0,0.326346,"etworks have been proposed for textual similarity modeling. These models share three main components: (1) sequential sentence encoders, which incorporate word context and sentence order for better sentence representations, e.g., by using recurrent neural networks (RNNs; Mikolov et al., 2010; Seo et al., 2016), (2) interaction and attention mechanisms, which use the encoding outputs of sentences to calculate or reweight salient word pair interactions (He and Lin, 2016; Chen et al., 2017), and (3) incorporating syntactic parsing information as an intuitive structure prior for sentence modeling (Chen and Manning, 2014; Zhao et al., 2016; Chen et al., 2017). Our work is inspired by the recent reproducibility study by Lan and Xu (2018), which examines many neural network architectures for semantic similarity modeling through extensive evaluations on multiple benchmark datasets. Their results suggest that syntactic structure information captured by a TreeLSTM encoder either provides few benefits or even hurts performance. Structure information has often been overlooked in recent semantic modeling methods, such as InferSent (Conneau et al., 2017), DecAtt (Parikh et al., 2016), and BiMPM (Wang et al., 2017). It"
D19-1114,P17-1152,0,0.591893,"(He et al., 2015; Rao et al., 2017; Wang et al., 2018) and query ranking (Mitra and Craswell, 2019). Recently, various neural networks have been proposed for textual similarity modeling. These models share three main components: (1) sequential sentence encoders, which incorporate word context and sentence order for better sentence representations, e.g., by using recurrent neural networks (RNNs; Mikolov et al., 2010; Seo et al., 2016), (2) interaction and attention mechanisms, which use the encoding outputs of sentences to calculate or reweight salient word pair interactions (He and Lin, 2016; Chen et al., 2017), and (3) incorporating syntactic parsing information as an intuitive structure prior for sentence modeling (Chen and Manning, 2014; Zhao et al., 2016; Chen et al., 2017). Our work is inspired by the recent reproducibility study by Lan and Xu (2018), which examines many neural network architectures for semantic similarity modeling through extensive evaluations on multiple benchmark datasets. Their results suggest that syntactic structure information captured by a TreeLSTM encoder either provides few benefits or even hurts performance. Structure information has often been overlooked in recent s"
D19-1114,D17-1070,0,0.0808031,"ormation as an intuitive structure prior for sentence modeling (Chen and Manning, 2014; Zhao et al., 2016; Chen et al., 2017). Our work is inspired by the recent reproducibility study by Lan and Xu (2018), which examines many neural network architectures for semantic similarity modeling through extensive evaluations on multiple benchmark datasets. Their results suggest that syntactic structure information captured by a TreeLSTM encoder either provides few benefits or even hurts performance. Structure information has often been overlooked in recent semantic modeling methods, such as InferSent (Conneau et al., 2017), DecAtt (Parikh et al., 2016), and BiMPM (Wang et al., 2017). It is not yet clear whether the syntactic structures implicitly captured by sequential modeling of texts from large annotated data or existing structure modeling techniques (Tai et al., 2015; Kipf and Welling, 2017) are effective in learning tree representations. To further explore the effects of tree structures in sentence modeling, we start with the Pairwise Word Interaction Model (PWIM) of He and Lin (2016) as our base architecture, which has shown strong performance on various datasets from Lan and Xu (2018). In summary, PWIM u"
D19-1114,D15-1181,1,0.820466,"a recent reproducibility study, then introduce components for modeling context and structure using multi-layer BiLSTMs and TreeLSTMs. In addition, we introduce residual connections to the deep convolutional neural network component of the model. Extensive evaluations on eight benchmark datasets show that incorporating structural information contributes to consistent improvements over strong baselines. 1 Introduction Modeling the semantic similarity between a pair of sentences is a fundamental task in natural language processing. It is the core problem of many tasks such as question answering (He et al., 2015; Rao et al., 2017; Wang et al., 2018) and query ranking (Mitra and Craswell, 2019). Recently, various neural networks have been proposed for textual similarity modeling. These models share three main components: (1) sequential sentence encoders, which incorporate word context and sentence order for better sentence representations, e.g., by using recurrent neural networks (RNNs; Mikolov et al., 2010; Seo et al., 2016), (2) interaction and attention mechanisms, which use the encoding outputs of sentences to calculate or reweight salient word pair interactions (He and Lin, 2016; Chen et al., 201"
D19-1114,S14-2010,0,0.0425834,"Missing"
D19-1114,D14-1108,0,0.0369867,"Missing"
D19-1114,D17-1126,0,0.0240074,"Missing"
D19-1114,C18-1328,0,0.143648,"ce encoders, which incorporate word context and sentence order for better sentence representations, e.g., by using recurrent neural networks (RNNs; Mikolov et al., 2010; Seo et al., 2016), (2) interaction and attention mechanisms, which use the encoding outputs of sentences to calculate or reweight salient word pair interactions (He and Lin, 2016; Chen et al., 2017), and (3) incorporating syntactic parsing information as an intuitive structure prior for sentence modeling (Chen and Manning, 2014; Zhao et al., 2016; Chen et al., 2017). Our work is inspired by the recent reproducibility study by Lan and Xu (2018), which examines many neural network architectures for semantic similarity modeling through extensive evaluations on multiple benchmark datasets. Their results suggest that syntactic structure information captured by a TreeLSTM encoder either provides few benefits or even hurts performance. Structure information has often been overlooked in recent semantic modeling methods, such as InferSent (Conneau et al., 2017), DecAtt (Parikh et al., 2016), and BiMPM (Wang et al., 2017). It is not yet clear whether the syntactic structures implicitly captured by sequential modeling of texts from large anno"
D19-1114,S14-2001,0,0.0367219,"Lan et al., 2017) is a paraphrase corpus with 50k sentence pairs. • PIT-2015 (Xu et al., 2015) is a paraphrase dataset that comes from SemEval2015 Task 1. • STS-2014 (Agirre et al., 2014) comes from SemEval-2014 Task 10 and each pair of sentences has a similarity score ∈ [0, 5]. • WikiQA (Yang et al., 2015) is an opendomain question-answering dataset. After applying the same pre-processing methods in He and Lin (2016), it contains 12k question-answer pairs with binary labels. • TrecQA (Wang et al., 2007) is from the Text Retrieval Conferences and consists of 56k question-answer pairs. • SICK (Marelli et al., 2014) comes from SemEval-2014 Task 1 with 10k annotated sentence pairs. Each pair has a similarity score ∈ [1, 5]. The first seven datasets are the same as the ones examined in Lan and Xu (2018), except for MNLI (Williams et al., 2018), since SNLI is much larger than MNLI for the task of natural language inference. We also add the SICK dataset (Marelli et al., 2014), which is unexplored in Lan and Xu (2018). Across multiple tasks and domains, we systematically compare our proposed models with state-of-the-art neural models: InferSent (Conneau et al., 2017), Shortcut-stacked Sentence Encoder (SSE; N"
D19-1114,W17-5308,0,0.0538337,"ections Since He and Lin (2016) phrase the similarity measurement problem as a pattern recognition (image processing) problem and apply deep convolutional neural networks, we explore the addition of residual connections (He et al., 2016) to deal with the potential vanishing gradient problems in deep networks. A building block is defined as y = f (x, Wi ) + x, where x, y are the input and output of the layer considered, and f (x, Wi ) is the learned residual mapping. 1 Multi-layer BiLSTM Sentence Encoders We use multiple stacked, bi-directional LSTM layers with shortcut connections, similar to Nie and Bansal (2017). In this architecture, the input sequences of the ith BiLSTM layer are the concatenated outputs of all the previous layers and the initial word embedding sequences. Let W = {w1 , w2 , ..., wn } represent the word embeddings associated with each word in the source sentence. Define the output of the ith BiLSTM layer at time t as hit = BiLSTMi (xit ). Then, the input of the ith BiLSTM layer at time t is: The Pairwise Word Interaction Model The Pairwise Word Interaction model (He and Lin, 2016) captures fine-grained word-level information to measure textual similarity. They use a BiLSTM for conte"
D19-1114,D16-1244,0,0.0851385,"Missing"
D19-1114,D14-1162,0,0.100513,"t neural models: InferSent (Conneau et al., 2017), Shortcut-stacked Sentence Encoder (SSE; Nie and Bansal, 2017), Decomposable Attention Model (DecAtt; Parikh et al., 2016), and Enhanced Sequential Inference Model (ESIM; Chen et al., 2017). For our experiments on SNLI, Quora, TwitterURL, PIT-2015, WikiQA and TrecQA, the training objective is to minimize the NLL loss. For STS-2014 and SICK datasets, we use the KL divergence loss. Following He and Lin (2016), for all cases, we use the RMSProp optimizer (Tieleman and Hinton, 2012). Our word representations use 300-dimensional GloVe word vectors (Pennington et al., 2014), which we make static in all experiments. We produce dependency parse trees for each sentence using the Stanford Neural Network Dependency Parser (Chen and Manning, 2014). The TreeLSTM then models sentence representations over each sentence’s parse tree. 4 Results and Analysis Table 1 shows the results of our models on different datasets. The first block of the table contains figures copied directly from Lan and Xu (2018); note that they do not use SICK. PWIMour refers to our own implementation. Note that there are at least three independent open-source implementations of the PWIM base model"
D19-1114,D07-1003,0,0.0617532,"d from the Quora website, with binary labels indicating if they are duplicates of each other. • Twitter-URL (Lan et al., 2017) is a paraphrase corpus with 50k sentence pairs. • PIT-2015 (Xu et al., 2015) is a paraphrase dataset that comes from SemEval2015 Task 1. • STS-2014 (Agirre et al., 2014) comes from SemEval-2014 Task 10 and each pair of sentences has a similarity score ∈ [0, 5]. • WikiQA (Yang et al., 2015) is an opendomain question-answering dataset. After applying the same pre-processing methods in He and Lin (2016), it contains 12k question-answer pairs with binary labels. • TrecQA (Wang et al., 2007) is from the Text Retrieval Conferences and consists of 56k question-answer pairs. • SICK (Marelli et al., 2014) comes from SemEval-2014 Task 1 with 10k annotated sentence pairs. Each pair has a similarity score ∈ [1, 5]. The first seven datasets are the same as the ones examined in Lan and Xu (2018), except for MNLI (Williams et al., 2018), since SNLI is much larger than MNLI for the task of natural language inference. We also add the SICK dataset (Marelli et al., 2014), which is unexplored in Lan and Xu (2018). Across multiple tasks and domains, we systematically compare our proposed models"
D19-1114,N18-1101,0,0.0141852,"pair of sentences has a similarity score ∈ [0, 5]. • WikiQA (Yang et al., 2015) is an opendomain question-answering dataset. After applying the same pre-processing methods in He and Lin (2016), it contains 12k question-answer pairs with binary labels. • TrecQA (Wang et al., 2007) is from the Text Retrieval Conferences and consists of 56k question-answer pairs. • SICK (Marelli et al., 2014) comes from SemEval-2014 Task 1 with 10k annotated sentence pairs. Each pair has a similarity score ∈ [1, 5]. The first seven datasets are the same as the ones examined in Lan and Xu (2018), except for MNLI (Williams et al., 2018), since SNLI is much larger than MNLI for the task of natural language inference. We also add the SICK dataset (Marelli et al., 2014), which is unexplored in Lan and Xu (2018). Across multiple tasks and domains, we systematically compare our proposed models with state-of-the-art neural models: InferSent (Conneau et al., 2017), Shortcut-stacked Sentence Encoder (SSE; Nie and Bansal, 2017), Decomposable Attention Model (DecAtt; Parikh et al., 2016), and Enhanced Sequential Inference Model (ESIM; Chen et al., 2017). For our experiments on SNLI, Quora, TwitterURL, PIT-2015, WikiQA and TrecQA, the"
D19-1114,S15-2001,0,0.0607429,"Missing"
D19-1114,D15-1237,0,0.0516605,"Missing"
D19-1114,C16-1212,0,0.0154552,"ed for textual similarity modeling. These models share three main components: (1) sequential sentence encoders, which incorporate word context and sentence order for better sentence representations, e.g., by using recurrent neural networks (RNNs; Mikolov et al., 2010; Seo et al., 2016), (2) interaction and attention mechanisms, which use the encoding outputs of sentences to calculate or reweight salient word pair interactions (He and Lin, 2016; Chen et al., 2017), and (3) incorporating syntactic parsing information as an intuitive structure prior for sentence modeling (Chen and Manning, 2014; Zhao et al., 2016; Chen et al., 2017). Our work is inspired by the recent reproducibility study by Lan and Xu (2018), which examines many neural network architectures for semantic similarity modeling through extensive evaluations on multiple benchmark datasets. Their results suggest that syntactic structure information captured by a TreeLSTM encoder either provides few benefits or even hurts performance. Structure information has often been overlooked in recent semantic modeling methods, such as InferSent (Conneau et al., 2017), DecAtt (Parikh et al., 2016), and BiMPM (Wang et al., 2017). It is not yet clear w"
D19-1114,P15-1150,0,0.209719,"Missing"
D19-1352,N19-1423,0,0.0667955,"lines that inflate the merits of an approach are not new problems in information retrieval (Armstrong et al., 2009), and researchers have in fact observed similar issues in the recommender systems literature as well (Rendle et al., 2019; Dacrema et al., 2019). Having placed evaluation on more solid footing with respect to well-tuned baselines by building on previous work, this paper examines how we might make neural approaches “work” for document retrieval. One promising recent innovation is models that exploit massive pre-training (Peters et al., 2018; Radford et al., 2018), leading to BERT (Devlin et al., 2019) as the most popular example today. Researchers have applied BERT to a broad range of NLP tasks with impressive gains: most relevant to our document ranking task, these include BERTserini (Yang et al., 2019b) for question answering and Nogueira and Cho (2019) for passage reranking. Extending our own previous work (Yang et al., 2019c), the main contribution of this paper is a successful application of BERT to yield large improvements in ad hoc document retrieval. We introduce two simple yet effective innovations: First, we focus on integrating sentence-level evidence for document ranking to add"
D19-1352,D18-1478,0,0.0788624,"Missing"
D19-1352,D18-1211,0,0.0230724,"ching technique. Despite the plethora of neural models that have been proposed for document ranking (Mitra and Craswell, 2019), there has recently been some skepticism about whether they have truly advanced the state of the art (Lin, 2018), at least in the absence of large amounts of behavioral log data only available to search engine companies. In a meta-analysis of over 100 papers that report results on the dataset from the Robust Track at TREC 2004 (Robust04), Yang et al. (2019a) found that most neural approaches do not compare against competitive baselines. To provide two recent examples, McDonald et al. (2018) report a best AP score of 0.272 and Li et al. (2018) 0.290, compared to a simple bag-of-words query expansion baseline that achieves 0.299 (Lin, 2018). Further experiments by Yang et al. (2019a) achieve 0.315 under more rigorous experimental conditions with a neural ranking model, but this is still pretty far from the best-known score of 0.3686 on this dataset (Cormack et al., 2009). Although Sculley et al. (2018) remind us that the goal of science is not wins, but knowledge, the latter requires first establishing strong baselines that accurately quantify proposed contributions. Comparisons t"
D19-1352,N18-1202,0,0.0241366,"ely quantify proposed contributions. Comparisons to weak baselines that inflate the merits of an approach are not new problems in information retrieval (Armstrong et al., 2009), and researchers have in fact observed similar issues in the recommender systems literature as well (Rendle et al., 2019; Dacrema et al., 2019). Having placed evaluation on more solid footing with respect to well-tuned baselines by building on previous work, this paper examines how we might make neural approaches “work” for document retrieval. One promising recent innovation is models that exploit massive pre-training (Peters et al., 2018; Radford et al., 2018), leading to BERT (Devlin et al., 2019) as the most popular example today. Researchers have applied BERT to a broad range of NLP tasks with impressive gains: most relevant to our document ranking task, these include BERTserini (Yang et al., 2019b) for question answering and Nogueira and Cho (2019) for passage reranking. Extending our own previous work (Yang et al., 2019c), the main contribution of this paper is a successful application of BERT to yield large improvements in ad hoc document retrieval. We introduce two simple yet effective innovations: First, we focus on i"
D19-1352,N19-4013,1,0.872926,"Missing"
D19-1352,P79-1022,0,0.279815,"Missing"
D19-1352,D19-3004,1,0.855108,"† 0.5015† 0.5014† 0.3103† 0.3140† 0.3143† 0.5830 0.5830 0.5830 0.4758 0.4817† 0.4807 0.3385† 0.3386† 0.3382† 0.4860 0.4810 0.4830 0.4785 0.4755 0.4731 1S: BERT(MS MARCO → MB) 2S: BERT(MS MARCO → MB) 3S: BERT(MS MARCO → MB) 0.3676† 0.3697† 0.3691† 0.4610† 0.4657† 0.4669† 0.5239† 0.5324† 0.5325† 0.3292† 0.3323† 0.3314† 0.6080† 0.6170† 0.6200† 0.5061† 0.5092† 0.5070† 0.3486† 0.3496† 0.3522† 0.4920 0.4830 0.4850 0.4953† 0.4899† 0.4899† Table 1: Ranking effectiveness on Robust04, Core17, and Core18 in terms of AP, P@20, and NDCG@20. details about the technical design of our system are presented in Yilmaz et al. (2019), a companion demonstration paper. 4 Results and Discussion Our main results are shown in Table 1. The top row shows the BM25+RM3 query expansion baseline using default Anserini parameters.3 The remaining blocks display the ranking effectiveness of our models on Robust04, Core17, and Core18. In parentheses we describe the fine-tuning procedure: for instance, MSMARCO → MB refers to a model that was first fine-tuned on MS MARCO and then on MB. The nS preceding the model name indicates that inference was performed using the top n scoring sentences from each document. Table 1 also includes results"
D19-1451,D18-1269,0,0.0277625,"ultilingual entity and description embeddings. Wang et al. (2018) applied GCNs with the connectivity matrix defined on relations to embed entities from multilingual KGs into a unified low-dimensional space. In this work, we also employ GCNs. However, in contrast to Wang et al. (2018), we regard relation features as input to our models. In addition, we investigate two different ways to capture relation and attribute features. Multilingual Sentence Representations. Another line of research related to this work is aligning sentences in multiple languages. Recent works (Hermann and Blunsom, 2014; Conneau et al., 2018; Eriguchi et al., 2018) studied crosslingual sentence classification via zero-shot learning. Johnson et al. (2017) proposed a sequenceto-sequence multilingual machine translation system where the encoder can be used to produce cross-lingual sentence embeddings (Artetxe and Schwenk, 2018). Recently, BERT (Devlin et al., 2019) has advanced the state-of-the-art on multiple natural language understanding tasks. Specifically, multilingual BERT enables learning representations of sentences under multilingual settings. We adopt BERT to produce cross-lingual representations of entity literal descript"
D19-1451,P13-1153,0,0.0780832,"Missing"
D19-1451,N19-1423,0,0.480151,"entities (i.e., topological connections, relations and attributes, as well as literal descriptions) remains under-explored. In this work, we propose a novel approach to learn cross-lingual entity embeddings by using all aforementioned aspects of information in KGs. To be specific, we propose two variants of GCNbased models, namely M AN and H MAN, that incorporate multi-aspect features, including topological features, relation types, and attributes into cross-lingual entity embeddings. To capture semantic relatedness of literal descriptions, we finetune the pretrained multilingual BERT model (Devlin et al., 2019) to bridge cross-lingual gaps. We design two strategies to combine GCN-based and BERT-based modules to make alignment decisions. Experiments show that our method achieves new state-of-the-art results on two benchmark datasets. Source code for our models is publicly available at https://github.com/ h324yang/HMAN. 2 Problem Definition In a multilingual knowledge graph G, we use L to denote the set of languages that G contains and Gi = {Ei , Ri , Ai , Vi , Di } to represent the language-specific knowledge graph in language Li ∈ L. Ei , Ri , Ai , Vi and Di are sets of entities, relations, attribut"
D19-1451,P19-1024,1,0.845187,"duce two GCN-based models, namely M AN and H MAN, that learn entity embeddings from the graph structures. Second, we discuss two uses of a multilingual pretrained BERT model to learn cross-lingual embeddings of entity descriptions: P OINTWISE B ERT and PAIRWISE B ERT. Finally, we investigate two strategies to integrate the GCN-based and the BERT-based modules. 3.1 Cross-Lingual Graph Embeddings Graph convolutional networks (GCNs) (Kipf and Welling, 2017) are variants of convolutional networks that have proven effective in capturing information from graph structures, such as dependency graphs (Guo et al., 2019b), abstract meaning representation graphs (Guo et al., 2019a), and knowledge graphs (Wang et al., 2018). In practice, multi-layer GCNs are stacked to collect evidence from multi-hop neighbors. Formally, the l-th GCN layer takes as input feature representations H (l−1) and outputs H (l) :   (l) − 12 ˜ ˜ − 12 (l−1) (l) ˜ H = φ D AD H W (1) where A˜ = A + I is the adjacency matrix, I is ˜ is the diagonal node degree the identity matrix, D ˜ matrix of A, φ(·) is ReLU function, and W (l) represents learnable parameters in the l-th layer. H (0) is the initial input. GCNs can iteratively update th"
D19-1451,Q19-1019,1,0.822025,"duce two GCN-based models, namely M AN and H MAN, that learn entity embeddings from the graph structures. Second, we discuss two uses of a multilingual pretrained BERT model to learn cross-lingual embeddings of entity descriptions: P OINTWISE B ERT and PAIRWISE B ERT. Finally, we investigate two strategies to integrate the GCN-based and the BERT-based modules. 3.1 Cross-Lingual Graph Embeddings Graph convolutional networks (GCNs) (Kipf and Welling, 2017) are variants of convolutional networks that have proven effective in capturing information from graph structures, such as dependency graphs (Guo et al., 2019b), abstract meaning representation graphs (Guo et al., 2019a), and knowledge graphs (Wang et al., 2018). In practice, multi-layer GCNs are stacked to collect evidence from multi-hop neighbors. Formally, the l-th GCN layer takes as input feature representations H (l−1) and outputs H (l) :   (l) − 12 ˜ ˜ − 12 (l−1) (l) ˜ H = φ D AD H W (1) where A˜ = A + I is the adjacency matrix, I is ˜ is the diagonal node degree the identity matrix, D ˜ matrix of A, φ(·) is ReLU function, and W (l) represents learnable parameters in the l-th layer. H (0) is the initial input. GCNs can iteratively update th"
D19-1451,P14-1006,0,0.0259027,"ithm to alternately learn multilingual entity and description embeddings. Wang et al. (2018) applied GCNs with the connectivity matrix defined on relations to embed entities from multilingual KGs into a unified low-dimensional space. In this work, we also employ GCNs. However, in contrast to Wang et al. (2018), we regard relation features as input to our models. In addition, we investigate two different ways to capture relation and attribute features. Multilingual Sentence Representations. Another line of research related to this work is aligning sentences in multiple languages. Recent works (Hermann and Blunsom, 2014; Conneau et al., 2018; Eriguchi et al., 2018) studied crosslingual sentence classification via zero-shot learning. Johnson et al. (2017) proposed a sequenceto-sequence multilingual machine translation system where the encoder can be used to produce cross-lingual sentence embeddings (Artetxe and Schwenk, 2018). Recently, BERT (Devlin et al., 2019) has advanced the state-of-the-art on multiple natural language understanding tasks. Specifically, multilingual BERT enables learning representations of sentences under multilingual settings. We adopt BERT to produce cross-lingual representations of e"
D19-1451,D14-1005,0,0.0346512,"t Network (M AN) to capture the three aspects of entity features. Specifically, three l-layer GCNs take as inputs the tripleaspect features (i.e., Xt , Xr , and Xa ) and produce (l) (l) (l) the representations Ht , Hr , and Ha according to Equation 1, respectively. Finally, the multiaspect entity embedding is: (l) Hm = [Ht ⊕ Ha(l) ⊕ Hr(l) ] (2) where ⊕ denotes vector concatenation. Hm can then feed into alignment decisions. Such fusion through concatenation is also known as Scoring Level Fusion, which has been proven simple but effective for capturing multimodal semantics (Bruni et al., 2014; Kiela and Bottou, 2014; Collell et al., 2017). It is worth noting that the main differences between M AN and the work of Wang et al. (2018) are two fold: First, we use the same approach as in Kipf and Welling (2017) to construct the adjacency matrix, while Wang et al. (2018) designed a new connectivity matrix as the adjacency matrix for the GCNs. Second, M AN explicitly regards the relation type features as model input, while Wang et al. (2018) incorporated such relation information into the connectivity matrix. H MAN. Note that M AN propagates relation and attribute information through the graph structure. However"
D19-1451,N19-1229,1,0.801615,"rview of P OINTWISE B ERT (left) and PAIRWISE B ERT (right). from which the final hidden state is used as the sequence representation, and [SEP] is the special token for separating token sequences, and produces the probability of classifying the pair as equivalent entities. The probability is then used to rank all candidate entity pairs, i.e., ranking score. We denote this model as P OINTWISE B ERT, shown in Figure 3 (left). This approach is computationally expensive, since for each entity we need to consider all candidate entities in the target language. One solution, inspired by the work of Shi et al. (2019), is to reduce the search space for each entity with a reranking strategy (see Section 3.3). PAIRWISE B ERT. Due to the heavy computational cost of P OINTWISE B ERT, semantic matching between all entity pairs is very expensive. Instead of producing ranking scores for description pairs, we propose PAIRWISE B ERT to encode the entity literal descriptions as cross-lingual textual embeddings, where distances between entity pairs can be directly measured using these embeddings. The PAIRWISE B ERT model consists of two components, each of which takes as input the description of one entity (from the"
D19-1451,D18-1032,0,0.160471,"essing, pages 4431–4441, c Hong Kong, China, November 3–7, 2019. 2019 Association for Computational Linguistics including topological connections, relation types, attributes, and literal descriptions expressed in different languages (Bizer et al., 2009; Xie et al., 2016), as shown in Figure 1 (bottom). The key challenge of addressing such a task thus is how to better model and use provided multi-aspect information of entities to bridge cross-lingual gaps and find more equivalent entities (i.e., ILLs). Recently, embedding-based solutions (Chen et al., 2017b; Sun et al., 2017; Zhu et al., 2017; Wang et al., 2018; Chen et al., 2018) have been proposed to unify multilingual KGs into the same low-dimensional vector space where equivalent entities are close to each other. Such methods only make use of one or two aspects of the aforementioned information. For example, Zhu et al. (2017) relied only on topological features while Sun et al. (2017) and Wang et al. (2018) exploited both topological and attribute features. Chen et al. (2018) proposed a co-training algorithm to combine topological features and literal descriptions of entities. However, combining these multi-aspect information of entities (i.e.,"
D19-1451,D18-1244,0,0.0428449,"ice, multi-layer GCNs are stacked to collect evidence from multi-hop neighbors. Formally, the l-th GCN layer takes as input feature representations H (l−1) and outputs H (l) :   (l) − 12 ˜ ˜ − 12 (l−1) (l) ˜ H = φ D AD H W (1) where A˜ = A + I is the adjacency matrix, I is ˜ is the diagonal node degree the identity matrix, D ˜ matrix of A, φ(·) is ReLU function, and W (l) represents learnable parameters in the l-th layer. H (0) is the initial input. GCNs can iteratively update the representation of each entity node via a propagation mechanism through the graph. Inspired by previous studies (Zhang et al., 2018; Wang et al., 2018), we also 4432 adopt GCNs in this work to collect evidence from multilingual KG structures and to learn crosslingual embeddings of entities. The primary assumptions are: (1) equivalent entities tend to be neighbored by equivalent entities via the same types of relations; (2) equivalent entities tend to share similar or even the same attributes. Multi-Aspect Entity Features. Existing KGs (Bizer et al., 2009; Suchanek et al., 2008; Rebele et al., 2016) provide multi-aspect information of entities. In this section, we mainly focus on the following three aspects: topological co"
D19-1540,P17-1171,0,0.24467,"multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-ofthe-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying encoders. 1 Introduction Neural networks have achieved great success in many NLP tasks, such as question answering (Rao et al., 2016; Chen et al., 2017a), paraphrase detection (Wang et al., 2017), and textual semantic similarity modeling (He and Lin, 2016). Many of these tasks can be treated as variants of a semantic matching (SM) problem, where two pieces of texts are jointly modeled through distributed representations for similarity learning. Various neural network architectures, e.g., Siamese networks (He et al., 2016) and attention (Seo et al., 2017; Tay et al., 2019b), have been proposed to model semantic similarity using diverse techniques. A core problem of information retrieval (IR) is relevance matching (RM), where the goal is to ra"
D19-1540,P17-1152,0,0.53916,"multiple granularities, and (3) a semantic matching module with co-attention mechanisms that capture context-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-ofthe-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying encoders. 1 Introduction Neural networks have achieved great success in many NLP tasks, such as question answering (Rao et al., 2016; Chen et al., 2017a), paraphrase detection (Wang et al., 2017), and textual semantic similarity modeling (He and Lin, 2016). Many of these tasks can be treated as variants of a semantic matching (SM) problem, where two pieces of texts are jointly modeled through distributed representations for similarity learning. Various neural network architectures, e.g., Siamese networks (He et al., 2016) and attention (Seo et al., 2017; Tay et al., 2019b), have been proposed to model semantic similarity using diverse techniques. A core problem of information retrieval (IR) is relevance matching (RM), where the goal is to ra"
D19-1540,D17-1070,0,0.103087,"uery. We use the TREC Microblog 2013–2014 datasets (Lin and Efron, 2013; Lin et al., 2014), as prepared by Rao et al. (2019), where each dataset contains around 50 queries and 40k query-tweet pairs. We report MAP and precision at rank 30 (P@30). Experimental Setup Benchmarks and Metrics We evaluated our proposed HCAN model on three NLP tasks and two IR datasets, as follows: Answer Selection. This task is to rank candidate answer sentences based on their similarity to the For the answer selection, paraphrase identification, and STS tasks, we compared against the following baselines: InferSent (Conneau et al., 2017), ESIM (Chen et al., 2017b), DecAtt (Parikh et al., 2016), and PWIM (He and Lin, 2016). Additionally, we report state-of-the-arts results on each dataset from published literature. We also include the current state-of-the-art BERT (Devlin et al., 2019) results on each dataset. For the tweet search task, we mostly follow the experimental setting in Rao et al. (2019). Baselines include the classic query likelihood (QL) method, RM3 query expansion (Abdul-Jaleel et al., 2004), learning to rank (L2R), as well as a number of neural ranking models: DRMM (Guo et al., 2016), DUET (Mitra et al., 2017),"
D19-1540,N19-1423,0,0.0190793,"ental Setup Benchmarks and Metrics We evaluated our proposed HCAN model on three NLP tasks and two IR datasets, as follows: Answer Selection. This task is to rank candidate answer sentences based on their similarity to the For the answer selection, paraphrase identification, and STS tasks, we compared against the following baselines: InferSent (Conneau et al., 2017), ESIM (Chen et al., 2017b), DecAtt (Parikh et al., 2016), and PWIM (He and Lin, 2016). Additionally, we report state-of-the-arts results on each dataset from published literature. We also include the current state-of-the-art BERT (Devlin et al., 2019) results on each dataset. For the tweet search task, we mostly follow the experimental setting in Rao et al. (2019). Baselines include the classic query likelihood (QL) method, RM3 query expansion (Abdul-Jaleel et al., 2004), learning to rank (L2R), as well as a number of neural ranking models: DRMM (Guo et al., 2016), DUET (Mitra et al., 2017), KNRM (Xiong et al., 2017b), and PACRR (Hui et al., 2017). For the neural baselines, we used implementations in MatchZoo.2 For L2R, we used LambdaMART (Burges, 2010) on the same feature sets as Rao et al. (2019): text-based, URL1 The leaderboard can be"
D19-1540,N16-1108,1,0.871034,"text-aware semantic relatedness. Evaluations on multiple IR and NLP benchmarks demonstrate state-ofthe-art effectiveness compared to approaches that do not exploit pretraining on external data. Extensive ablation studies suggest that relevance and semantic matching signals are complementary across many problem settings, regardless of the choice of underlying encoders. 1 Introduction Neural networks have achieved great success in many NLP tasks, such as question answering (Rao et al., 2016; Chen et al., 2017a), paraphrase detection (Wang et al., 2017), and textual semantic similarity modeling (He and Lin, 2016). Many of these tasks can be treated as variants of a semantic matching (SM) problem, where two pieces of texts are jointly modeled through distributed representations for similarity learning. Various neural network architectures, e.g., Siamese networks (He et al., 2016) and attention (Seo et al., 2017; Tay et al., 2019b), have been proposed to model semantic similarity using diverse techniques. A core problem of information retrieval (IR) is relevance matching (RM), where the goal is to rank documents by relevance to a user’s query. Though at a high level semantic and relevance matching both"
D19-1540,S16-1170,1,0.852975,"s are complementary across many problem settings, regardless of the choice of underlying encoders. 1 Introduction Neural networks have achieved great success in many NLP tasks, such as question answering (Rao et al., 2016; Chen et al., 2017a), paraphrase detection (Wang et al., 2017), and textual semantic similarity modeling (He and Lin, 2016). Many of these tasks can be treated as variants of a semantic matching (SM) problem, where two pieces of texts are jointly modeled through distributed representations for similarity learning. Various neural network architectures, e.g., Siamese networks (He et al., 2016) and attention (Seo et al., 2017; Tay et al., 2019b), have been proposed to model semantic similarity using diverse techniques. A core problem of information retrieval (IR) is relevance matching (RM), where the goal is to rank documents by relevance to a user’s query. Though at a high level semantic and relevance matching both require modeling similarities in pairs of texts, there are fundamental differences. Semantic matching emphasizes “meaning” correspondences by exploiting lexical information (e.g., words, phrases, entities) and compositional structures (e.g., dependency trees), while rele"
D19-1540,N18-4017,0,0.0174319,"ere the goal is to rank documents by relevance to a user’s query. Though at a high level semantic and relevance matching both require modeling similarities in pairs of texts, there are fundamental differences. Semantic matching emphasizes “meaning” correspondences by exploiting lexical information (e.g., words, phrases, entities) and compositional structures (e.g., dependency trees), while relevance matching focuses on keyword matching. It has been observed that existing approaches for textual similarity modeling in NLP can produce poor results for IR tasks (Guo et al., 2016), and vice versa (Htut et al., 2018). Specifically, Guo et al. (2016) point out three distinguishing characteristics of relevance matching: exact match signals, query term importance, and diverse matching requirements. In particular, exact match signals play a critical role in relevance matching, more so than the role of term matching in, for example, paraphrase detection. Furthermore, in document ranking there is an asymmetry between queries and documents in terms of length and the richness of signals that can be extracted; thus, symmetric models such as Siamese architectures may not be entirely appropriate. To better demonstra"
D19-1540,D17-1110,0,0.0554917,"Missing"
D19-1540,D17-1126,0,0.0289615,"Missing"
D19-1540,C18-1328,0,0.0336884,"Missing"
D19-1540,D16-1244,0,0.0918108,"Missing"
D19-1540,P19-1486,1,0.916478,"regardless of the choice of underlying encoders. 1 Introduction Neural networks have achieved great success in many NLP tasks, such as question answering (Rao et al., 2016; Chen et al., 2017a), paraphrase detection (Wang et al., 2017), and textual semantic similarity modeling (He and Lin, 2016). Many of these tasks can be treated as variants of a semantic matching (SM) problem, where two pieces of texts are jointly modeled through distributed representations for similarity learning. Various neural network architectures, e.g., Siamese networks (He et al., 2016) and attention (Seo et al., 2017; Tay et al., 2019b), have been proposed to model semantic similarity using diverse techniques. A core problem of information retrieval (IR) is relevance matching (RM), where the goal is to rank documents by relevance to a user’s query. Though at a high level semantic and relevance matching both require modeling similarities in pairs of texts, there are fundamental differences. Semantic matching emphasizes “meaning” correspondences by exploiting lexical information (e.g., words, phrases, entities) and compositional structures (e.g., dependency trees), while relevance matching focuses on keyword matching. It has"
D19-1540,P19-1145,1,0.918765,"regardless of the choice of underlying encoders. 1 Introduction Neural networks have achieved great success in many NLP tasks, such as question answering (Rao et al., 2016; Chen et al., 2017a), paraphrase detection (Wang et al., 2017), and textual semantic similarity modeling (He and Lin, 2016). Many of these tasks can be treated as variants of a semantic matching (SM) problem, where two pieces of texts are jointly modeled through distributed representations for similarity learning. Various neural network architectures, e.g., Siamese networks (He et al., 2016) and attention (Seo et al., 2017; Tay et al., 2019b), have been proposed to model semantic similarity using diverse techniques. A core problem of information retrieval (IR) is relevance matching (RM), where the goal is to rank documents by relevance to a user’s query. Though at a high level semantic and relevance matching both require modeling similarities in pairs of texts, there are fundamental differences. Semantic matching emphasizes “meaning” correspondences by exploiting lexical information (e.g., words, phrases, entities) and compositional structures (e.g., dependency trees), while relevance matching focuses on keyword matching. It has"
D19-1540,D07-1003,0,0.167947,"R (2) Final Classification o =softmax(MLP({OlRM , OlSM })), l =1, 2, ..., N and o ∈ Rkclassk X L=− log oi [yi ], (oi ,yi ) where N is the number of encoder layers. 3.1 3.2 Baselines and Implementations d Given the relevance and semantic matching features {OlRM , OlSM } (from Equations 1 and 2) learned at each encoder layer l, we concatenate them together and use a two-layer fully-connected layer with ReLU activation to generate the final prediction vector o. During training, we minimize the negative log likelihood loss L summed over all samples (oi , yi ) below: 3 question. We use the TrecQA (Wang et al., 2007) dataset (raw version)1 with 56k question-answer pairs. We report mean average precision (MAP) and mean reciprocal rank (MRR). Paraphrase Identification. This task is to identify whether two sentences are paraphrases of each other. We use the TwitterURL (Lan et al., 2017) dataset with 50k sentence pairs. We report the unweighted average of F1 scores on the positive and negative classes (macro-F1). Semantic Textual Similarity (STS). This task is to measure the degree of semantic equivalence between pairs of texts. We use the Quora (Iyer et al., 2017) dataset with 400k question pairs collected f"
D19-1540,P79-1022,0,0.485594,"Missing"
D19-1540,Q16-1019,0,0.0463851,"Missing"
D19-1591,Q16-1026,0,0.0123496,"nts Model Selection Named-entity recognition is a sequence labeling task. The input of the model is a sequence of words x(t) , t = 1, 2, · · · . Each input word has a corresponding label z (t) ∈ L, where L is the set of all labels {lj }, j = 1, 2, · · · , m. The label indicates whether the word is an entity or not, and if yes, which kind of entity it is. A typical modern NER model consists of a bidirectional LSTM and a conditional random field (CRF) on top of the LSTM (Collobert et al., 2011; Huang et al., 2015). Sometimes there is also a convolutional neural network (CNN) (Ma and Hovy, 2016; Chiu and Nichols, 2016). However, the goal of this paper is not to achieve state-of-the-art performance on this task, but rather we are trying to understand the mechanisms of LSTMs. Therefore, we choose a relatively simple model (see Figure 1) &gt; is the transpose of the j-th column where W:,j vector of the matrix W. The final prediction z˜(t) is chosen as the label with greatest probability. 3.2 Experiment Setup The model is trained on the CoNLL2003 (Sang and De Meulder, 2003) training dataset. Development and test sets of CoNLL2003 will be used in experiments in Section 4. In this dataset there are nine labels in to"
D19-1591,W09-1119,0,0.0650586,"et al., 1986). Biological neural systems consist of a huge number of neurons, and can react to the environment in complicated ways. Biologists start with analyzing basic components of reactions, what stimuli trigger them, and which neurons are excited during the process. To verify the relationship between stimuli, neurons, and reactions, biologists further dissect neurons which are correlated with specific basic reactions, and see if the reaction still occurs for the same stimuli. We adopt the same methodology to study LSTMs, using a representative task in NLP: named-entity recognition (NER) (Ratinov and Roth, 2009; Lample et al., 2016). Even though the output of a neural network may be complicated, we focus on basic components of the output: whether a label is predicted or not. We feed into the neural model various input instances, and analyze the relationship between the value of each LSTM neuron and the predicted label. We quantify the sensitivity of neurons to each label, and study how label-specific information is distributed among all neurons. We discover that each individual neuron is specialized to carry information for a subset of labels, and the information of each label is only carried by a s"
D19-1591,D16-1159,0,0.0764823,"Missing"
D19-1591,C18-1327,0,0.0173959,"44 17 3 37 16 48 30 9 14 2 30 28 38 10 16 13 31 39 31 9 36 29 29 42 11 32 44 47 3 45 42 29 30 38 45 44 12 28 45 46 36 21 25 32 43 30 26 14 16 9 41 6 33 19 14 2 40 23 2 21 47 21 1 O 0.10 -0.01 -0.03 0.09 2.40 -0.03 1.35 0.13 0.56 0.57 Figure 2: Sensitivity of the top ten neurons for all labels; red for positive values, blue for negative ones, and deeper colors stand for larger absolute values. based on the best F1 score on the development set. The chosen checkpoint has an F1 score of 86.4. For comparison, the F1 score obtained by the same toolkit using a bi-directional LSTM and a CRF is 89.5 (Yang et al., 2018). 4 Analyzing Neuron-Label Affinity In this section, we first identify important neurons by quantifying the sensitivity of a neuron to a label, and then verify the quantification by neuron ablation experiments. 4.1 Identifying Important Neurons A neuron of an LSTM corresponds to an entry (dimension) of h(t) . For a certain label lj , we try to identify neurons that are important for its prediction in the following way. We define the contribution of the i-th neuron to the j-th label at timestep t as (t) (t) ui,j = Wi,j hi . (4) Note that contribution is defined with the number after multiplied"
D19-1591,P18-4013,0,0.0172088,"of LSTMs. Therefore, we choose a relatively simple model (see Figure 1) &gt; is the transpose of the j-th column where W:,j vector of the matrix W. The final prediction z˜(t) is chosen as the label with greatest probability. 3.2 Experiment Setup The model is trained on the CoNLL2003 (Sang and De Meulder, 2003) training dataset. Development and test sets of CoNLL2003 will be used in experiments in Section 4. In this dataset there are nine labels in total, under the BIO tagging schema. See the first row of Figure 3 for the complete set of labels. Code for this paper is adopted from the toolkit by Yang and Zhang (2018). We set the hidden size of the LSTM to 50, since a larger hidden size does not significantly improve the results. Other hyperparameters, such as learning rate, batch size, and drop out rate, are kept unchanged. The model is trained for 10 epochs, and we pick the checkpoint 5824 cell 7 10 13 17 19 24 27 34 37 48 B-PER 0.10 0.10 0.72 0.03 0.07 -0.04 -0.23 -0.04 1.23 0.17 I-PER -0.01 0.18 1.81 0.08 0.25 0.00 0.06 0.26 0.11 0.30 B-LOC I-LOC B-ORG I-ORG B-MISC I-MISC 0.02 0.01 0.08 0.01 -0.08 0.00 0.06 0.02 0.07 0.13 1.05 -0.01 0.09 0.00 0.00 0.05 -0.01 0.00 1.17 0.13 0.08 0.15 -0.01 1.29 0.27 -0."
D19-3004,D16-1264,0,0.04366,"to-understand manner. 1 2 Integration Challenges The problem we are trying to solve, and the focus of this work, is how to bridge the worlds of information retrieval and natural language processing from a software engineering perspective, for applications to document retrieval. Following the standard formulation, we assume a (potentially large) corpus D that users wish to search. For a keyword query Q, the system’s task is to return a ranked list of documents that maximizes a retrieval metric such as average precision (AP). This stands in contrast to reading comprehension tasks such as SQuAD (Rajpurkar et al., 2016) and many formulations of question answering today such as WikiQA (Yang et al., 2015) and the MS MARCO QA Task (Bajaj et al., 2018), where there is no (or minimal) retrieval component. These are better characterized as “selection” tasks on (pre-determined) text passages. Within the information retrieval community, there exists a disconnect between academic researchers and industry practitioners. Outside of a few large organizations that deploy custom infrastructure (mostly commercial search engine companies), Lucene (along with the closely-related Introduction The information retrieval communi"
D19-3004,N19-1423,0,0.044498,"ormation retrieval community, much like the natural language processing community, has witnessed the growing dominance of approaches based on neural networks. Applications of neural networks to document ranking usually involve multi-stage architectures, beginning with a traditional term-matching technique (e.g., BM25) over a standard inverted index, followed by a reranker that rescores the candidate list of documents (Asadi and Lin, 2013). Researchers have developed a panoply of neural ranking models—see Mitra and Craswell (2019) for a recent overview—but there is emerging evidence that BERT (Devlin et al., 2019) outperforms previous approaches to document retrieval (Yang et al., 2019c; MacAvaney et al., 2019) as well as search-related tasks such as question answering (Nogueira and Cho, 2019; Yang et al., 2019b). 1 2 http://birchir.io/ http://anserini.io/ 19 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 19–24 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics projects Solr and Elasticsearch) has become the de facto platform for building real-world search applications, deployed at Twitter, Netflix, eBay, and numerous other organiza"
D19-3004,N19-4013,1,0.857603,"unity, has witnessed the growing dominance of approaches based on neural networks. Applications of neural networks to document ranking usually involve multi-stage architectures, beginning with a traditional term-matching technique (e.g., BM25) over a standard inverted index, followed by a reranker that rescores the candidate list of documents (Asadi and Lin, 2013). Researchers have developed a panoply of neural ranking models—see Mitra and Craswell (2019) for a recent overview—but there is emerging evidence that BERT (Devlin et al., 2019) outperforms previous approaches to document retrieval (Yang et al., 2019c; MacAvaney et al., 2019) as well as search-related tasks such as question answering (Nogueira and Cho, 2019; Yang et al., 2019b). 1 2 http://birchir.io/ http://anserini.io/ 19 Proceedings of the 2019 EMNLP and the 9th IJCNLP (System Demonstrations), pages 19–24 c Hong Kong, China, November 3 – 7, 2019. 2019 Association for Computational Linguistics projects Solr and Elasticsearch) has become the de facto platform for building real-world search applications, deployed at Twitter, Netflix, eBay, and numerous other organizations. However, many researchers still rely on academic systems such as I"
D19-3004,D15-1237,0,0.06896,"he focus of this work, is how to bridge the worlds of information retrieval and natural language processing from a software engineering perspective, for applications to document retrieval. Following the standard formulation, we assume a (potentially large) corpus D that users wish to search. For a keyword query Q, the system’s task is to return a ranked list of documents that maximizes a retrieval metric such as average precision (AP). This stands in contrast to reading comprehension tasks such as SQuAD (Rajpurkar et al., 2016) and many formulations of question answering today such as WikiQA (Yang et al., 2015) and the MS MARCO QA Task (Bajaj et al., 2018), where there is no (or minimal) retrieval component. These are better characterized as “selection” tasks on (pre-determined) text passages. Within the information retrieval community, there exists a disconnect between academic researchers and industry practitioners. Outside of a few large organizations that deploy custom infrastructure (mostly commercial search engine companies), Lucene (along with the closely-related Introduction The information retrieval community, much like the natural language processing community, has witnessed the growing do"
D19-3004,D19-1352,1,0.939002,"in the community (Lin et al., 2016). Already, Anserini has proven to be effective and has gained some traction: For example, Nogueira and Cho (2019) used Anserini for generating candidate documents before applying BERT to ranking passages in the TREC Complex Answer Retrieval (CAR) task (Dietz et al., 2017), which led to a large increase in effectiveness. Yang et al. (2019b) also combined Anserini and BERT to demonstrate large improvements in open-domain question answering directly on Wikipedia. 3.2 Code Entry Point Query Birch 3.1 (JVM) 3.3 Models Our document ranking approach is detailed in Yilmaz et al. (2019) and Yang et al. (2019c). We follow Nogueira and Cho (2019) in adapting BERT for binary (specifically, relevance) classification over text. Candidate documents from Anserini https://www.lemurproject.org/ http://terrier.org/ 5 20 https://pyjnius.readthedocs.io/ 2011 2012 2013 2014 Model AP P@30 AP P@30 AP P@30 AP P@30 QL RM3 MP-HCNN (Rao et al., 2019) BiCNN (Shi et al., 2018) 0.3576 0.3824 0.4043 0.4293 0.4000 0.4211 0.4293 0.4728 0.2091 0.2342 0.2460 0.2621 0.3311 0.3452 0.3791 0.4147 0.2532 0.2766 0.2896 0.2990 0.4450 0.4733 0.5294 0.5367 0.3924 0.4480 0.4420 0.4563 0.6182 0.6339 0.6394 0.680"
D19-3004,H93-1012,0,0.72094,"Missing"
D19-6122,I05-5002,0,0.035415,"et al., 2019) likely offer greater benefits to the student model, but BERT is widely used and sufficient for the point of this paper. We follow the same experimental procedure in Devlin et al. (2018) and fine-tune BERT end-to-end for each task, varying only the final classifier layer for the desired number of classes. tence similarity, and paraphrasing: Stanford Sentiment Treebank-2 (SST-2; Socher et al., 2013), the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SST-2 is a binary polarity dataset of single-sentence movie reviews. CoLA is a single-sentence grammaticality task, with expertly annotated binary judgements. STS-B comprises sentence pairs labeled with realvalued similarity between 1 and 5. Lastly, MRPC has sentence pairs with binary labels denoting semantic equivalence. We pick these four tasks from the General Language Understanding Evaluation (GLUE; Wang et al., 2018) benchmark, and submit results to their public evaluation server.2 Language modeling. For creating the transfer set, we apply two public, state-of-the-art language models: t"
D19-6122,D14-1181,0,0.0082641,"o match the single-sentence SST-2, we break paragraphs into individual linguistic sentences and, hence, multiple transfer set examples. To confirm that this is domain sensitive, we also apply it to the out-of-domain CoLA task in linguistic acceptability. We are unable to find a suitable unlabeled set for our other tasks—by construction, most sentence-pair datasets require manual balancing to prevent an overabundance of a single class, e.g., dissimilar examples in sentence similarity. We call this method TSIMDb . 4.2 izing out-of-vocabulary (OOV) vectors from U NI FORM [−0.25, 0.25], following Kim (2014), along with multichannel embeddings. To fine-tune our pretrained language models, we use Adam (Kingma and Ba, 2014) with a learning rate (LR) linear warmup proportion of 0.1, linearly decaying the LR afterwards. We choose a batch size of eight and one fine-tuning epoch, which is sufficient for convergence. We tune the LR from {1, 5} × 10−5 based on word-level perplexity on the development set. 5 Results and Discussion We present our results in Table 1. As an initial sanity check, we confirm that our BiLSTM (row 11) is acceptably similar to the previous best reported BiLSTM (row 5). We also ve"
D19-6122,2021.ccl-1.108,0,0.255879,"Missing"
D19-6122,S17-2001,0,0.027771,"d pretrained models like XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019) likely offer greater benefits to the student model, but BERT is widely used and sufficient for the point of this paper. We follow the same experimental procedure in Devlin et al. (2018) and fine-tune BERT end-to-end for each task, varying only the final classifier layer for the desired number of classes. tence similarity, and paraphrasing: Stanford Sentiment Treebank-2 (SST-2; Socher et al., 2013), the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SST-2 is a binary polarity dataset of single-sentence movie reviews. CoLA is a single-sentence grammaticality task, with expertly annotated binary judgements. STS-B comprises sentence pairs labeled with realvalued similarity between 1 and 5. Lastly, MRPC has sentence pairs with binary labels denoting semantic equivalence. We pick these four tasks from the General Language Understanding Evaluation (GLUE; Wang et al., 2018) benchmark, and submit results to their public evaluation server.2 Language modeling. For creating"
D19-6122,N18-1202,0,0.0665708,"Missing"
D19-6122,D13-1170,0,0.0111114,"RT, a deep pretrained language representation model that achieves close to state of the art (SOTA) on our tasks. Extremely recent, improved pretrained models like XLNet (Yang et al., 2019) and RoBERTa (Liu et al., 2019) likely offer greater benefits to the student model, but BERT is widely used and sufficient for the point of this paper. We follow the same experimental procedure in Devlin et al. (2018) and fine-tune BERT end-to-end for each task, varying only the final classifier layer for the desired number of classes. tence similarity, and paraphrasing: Stanford Sentiment Treebank-2 (SST-2; Socher et al., 2013), the Corpus of Linguistic Acceptability (CoLA; Warstadt et al., 2018), Semantic Textual Similarity Benchmark (STS-B; Cer et al., 2017), and Microsoft Research Paraphrase Corpus (MRPC; Dolan and Brockett, 2005). SST-2 is a binary polarity dataset of single-sentence movie reviews. CoLA is a single-sentence grammaticality task, with expertly annotated binary judgements. STS-B comprises sentence pairs labeled with realvalued similarity between 1 and 5. Lastly, MRPC has sentence pairs with binary labels denoting semantic equivalence. We pick these four tasks from the General Language Understanding"
D19-6122,W18-5446,0,0.295506,"a single contiguous sentence. 3.1 Model Architecture For simplicity and efficient inference, our student models use the same single-layer BiLSTM models from Tang et al. (2019)—see Figures 1 and 2. First, we map an input sequence of words to their corresponding word2vec embeddings, trained on Google News. Next, for single-sentence tasks, these embeddings are fed into a single-layer BiLSTM encoder to yield concatenated forward and backward states h = [hf ; hb ]. For sentencepair tasks, we encode each sentence separately using a BiLSTM to yield h1 and h2 . To produce a single vector h, following Wang et al. (2018), we compute h = [h1 ; h2 ; δ(h1 , h2 ); h1 · h2 ], where · denotes elementwise multiplication and δ denotes elementwise absolute difference. Finally, for both single- and paired-sentence tasks, h is passed through a multilayer perceptron (MLP) with one hidden layer that uses a rectified linear unit (ReLU) activation. For classification, the fi203 a b c e j f i 1 h 2 i j g h f ... ... e n g b a d c d Input #1 Figure 1: Illustration of the single-sentence BiLSTM, copied from Tang et al. (2019). The labels are as follows: (a) word embeddings (b) BiLSTM layer (c) final forward hidden state (d) fi"
D19-6607,P15-1034,0,0.0579064,"ame that contains the document id, a mention id, and a list of key–value pairs containing the mention class, mention text, and character offsets of the mention in the source document. Subsequent occurrences of the same mention in the same document are mapped to an existing mention so that character offsets can later be consolidated into an array. In this manner, we retain accurate provenance that allows us to trace back an extracted mention to its source. Relation Extraction. CoreNLP provides two different annotators for relation extraction: the Open Information Extraction (OpenIE) annotator (Angeli et al., 2015) and the Knowledge Base Population (KBP) annotator (Surdeanu et al., 2012). The OpenIE annotator provides open-class relations based on the provided text while the KBP annotator fills slots for a fixed set of 45 relations, such as org:city of headquarters. We use the latter, as it is more appropriate for our task. As 41 with the entities, each extracted relation corresponds to a row in a Spark DataFrame with the document id, the subject mention, the relation, the object mention, and a confidence score. knowledge graph (Wikidata). This is accomplished by first manually defining a mapping from C"
D19-6607,P05-1045,0,0.0141671,"tity recognition, etc.) that can be chained together in a pipeline. While dstlr is extractor agnostic and we have explored a number of different systems, we have found CoreNLP to be the most straightforward package to deploy from an engineering perspective. One of the contributions of our platform is the infrastructure to scale out the CoreNLP toolkit using Spark to process large document collections in an scalable manner. Named Entity Extraction. We use CoreNLP’s NERClassifierCombiner annotator to extract entity mentions of 20 different types, such as persons, organizations, locations, etc. (Finkel et al., 2005). Each mention corresponds to a row in a Spark DataFrame that contains the document id, a mention id, and a list of key–value pairs containing the mention class, mention text, and character offsets of the mention in the source document. Subsequent occurrences of the same mention in the same document are mapped to an existing mention so that character offsets can later be consolidated into an array. In this manner, we retain accurate provenance that allows us to trace back an extracted mention to its source. Relation Extraction. CoreNLP provides two different annotators for relation extraction:"
D19-6607,D12-1042,0,0.0424117,"pairs containing the mention class, mention text, and character offsets of the mention in the source document. Subsequent occurrences of the same mention in the same document are mapped to an existing mention so that character offsets can later be consolidated into an array. In this manner, we retain accurate provenance that allows us to trace back an extracted mention to its source. Relation Extraction. CoreNLP provides two different annotators for relation extraction: the Open Information Extraction (OpenIE) annotator (Angeli et al., 2015) and the Knowledge Base Population (KBP) annotator (Surdeanu et al., 2012). The OpenIE annotator provides open-class relations based on the provided text while the KBP annotator fills slots for a fixed set of 45 relations, such as org:city of headquarters. We use the latter, as it is more appropriate for our task. As 41 with the entities, each extracted relation corresponds to a row in a Spark DataFrame with the document id, the subject mention, the relation, the object mention, and a confidence score. knowledge graph (Wikidata). This is accomplished by first manually defining a mapping from CoreNLP relations to Wikidata properties. For example, the “headquarters” r"
D19-6607,L18-1245,0,0.0228558,"ns extracted from unstructured text and facts in the external knowledge graph that give rise to interesting applications in fact verification and related tasks. Currently, we use Wikidata as a stand-in, as our platform is designed to be enterprise and domain agnostic. Of course, relation extraction and complementary tasks such as entity linking, coreference resolution, predicate mapping, etc. have been studied for decades. Notable efforts include the Knowledge Base Population (KBP) and Knowledge Base Acceleration (KBA) tasks in the Text Analysis Conference (TAC) series (Ji and Grishman, 2011; Getman et al., 2018), NELL (Never-Ending Language Learning) (Mitchell et al., 2015), open information extractors such as Ollie (Mausam et al., 2012), and approaches based on weak supervision such as Snorkel (Ratner et al., 2017). Our focus, however, is very different as we wish to build an end-to-end platform that not only supports extraction, but the entire data management lifecycle, as discussed in the introduction. Part of this effort is the development of various applications that exploit knowledge graphs (for example, fact verification). In this sense, our work is complementary to all these abovementioned sy"
D19-6607,C18-1283,0,0.0187258,"in Section 3.2. This ingestion takes only a few seconds. 5 In our case study, the “product” of dstlr is a knowledge graph constructed from a corpus of unstructured text (Washington Post articles) that has been enriched with high-quality facts extracted from an external knowledge graph (Wikidata). The knowledge graph, stored in Neo4j, can then be manipulated by different applications using Neo4j’s declarative query language called Cypher. We describe a query-driven approach to align extracted relations from CoreNLP to external facts from Wikidata. This, in essence, performs fact verification (Thorne and Vlachos, 2018) against an external knowledge source that is presumed to have high-quality facts. While fact verification using external knowledge sources is not novel (Vlachos and Riedel, 2015), the contribution of our particular case study is to illustrate how it can be recast into a query-driven subgraph alignment problem. Within this framework, fact verification, locating textual support for asserted facts, updating incorrectly-asserted facts, asserting newlydiscovered facts, and data augmentation via distant supervision can all be viewed as different aspects of the same underlying task. As an illustrati"
D19-6607,P11-1115,0,0.0227381,"terplay between relations extracted from unstructured text and facts in the external knowledge graph that give rise to interesting applications in fact verification and related tasks. Currently, we use Wikidata as a stand-in, as our platform is designed to be enterprise and domain agnostic. Of course, relation extraction and complementary tasks such as entity linking, coreference resolution, predicate mapping, etc. have been studied for decades. Notable efforts include the Knowledge Base Population (KBP) and Knowledge Base Acceleration (KBA) tasks in the Text Analysis Conference (TAC) series (Ji and Grishman, 2011; Getman et al., 2018), NELL (Never-Ending Language Learning) (Mitchell et al., 2015), open information extractors such as Ollie (Mausam et al., 2012), and approaches based on weak supervision such as Snorkel (Ratner et al., 2017). Our focus, however, is very different as we wish to build an end-to-end platform that not only supports extraction, but the entire data management lifecycle, as discussed in the introduction. Part of this effort is the development of various applications that exploit knowledge graphs (for example, fact verification). In this sense, our work is complementary to all t"
D19-6607,D15-1312,0,0.0146279,"on Post articles) that has been enriched with high-quality facts extracted from an external knowledge graph (Wikidata). The knowledge graph, stored in Neo4j, can then be manipulated by different applications using Neo4j’s declarative query language called Cypher. We describe a query-driven approach to align extracted relations from CoreNLP to external facts from Wikidata. This, in essence, performs fact verification (Thorne and Vlachos, 2018) against an external knowledge source that is presumed to have high-quality facts. While fact verification using external knowledge sources is not novel (Vlachos and Riedel, 2015), the contribution of our particular case study is to illustrate how it can be recast into a query-driven subgraph alignment problem. Within this framework, fact verification, locating textual support for asserted facts, updating incorrectly-asserted facts, asserting newlydiscovered facts, and data augmentation via distant supervision can all be viewed as different aspects of the same underlying task. As an illustration of these ideas, we consider the city of headquarters relation identified by CoreNLP. Figure 2 shows a Cypher query that performs one possible subgraph alignment. A typical Cyph"
D19-6607,P14-5010,0,0.0040182,"nder different index architectures. The execution layer, which relies on Apache Spark, coordinates the two major phases of knowledge graph construction: extraction and enrichment. The knowledge graph is held in the popular graph database Neo4j. Applications built on top of the dstlr platform take advantage of a declarative query language called Cypher to manipulate the contents of the knowledge graph. The extraction phase is responsible for populating the raw knowledge graph with mentions, entities, and relations identified from unstructured text. Currently, we use Stanford’s CoreNLP toolkit (Manning et al., 2014) for the JVM due to its support for many common language analysis tasks (i.e., tokenization, part-of-speech tagging, named entity recognition, etc.) that can be chained together in a pipeline. While dstlr is extractor agnostic and we have explored a number of different systems, we have found CoreNLP to be the most straightforward package to deploy from an engineering perspective. One of the contributions of our platform is the infrastructure to scale out the CoreNLP toolkit using Spark to process large document collections in an scalable manner. Named Entity Extraction. We use CoreNLP’s NERCla"
D19-6607,D12-1048,0,0.0410777,"verification and related tasks. Currently, we use Wikidata as a stand-in, as our platform is designed to be enterprise and domain agnostic. Of course, relation extraction and complementary tasks such as entity linking, coreference resolution, predicate mapping, etc. have been studied for decades. Notable efforts include the Knowledge Base Population (KBP) and Knowledge Base Acceleration (KBA) tasks in the Text Analysis Conference (TAC) series (Ji and Grishman, 2011; Getman et al., 2018), NELL (Never-Ending Language Learning) (Mitchell et al., 2015), open information extractors such as Ollie (Mausam et al., 2012), and approaches based on weak supervision such as Snorkel (Ratner et al., 2017). Our focus, however, is very different as we wish to build an end-to-end platform that not only supports extraction, but the entire data management lifecycle, as discussed in the introduction. Part of this effort is the development of various applications that exploit knowledge graphs (for example, fact verification). In this sense, our work is complementary to all these abovementioned systems and techniques, as the dstlr platform is sufficiently general to incorporate their extraction results. tracting distantly-"
H05-1117,N04-1007,1,0.562328,"itute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA jimmylin@umd.edu, demner@cs.umd.edu Abstract Actually a misnomer, definition questions can be better paraphrased as “Tell me interesting things about X.”, where X can be a person, an organization, a common noun, etc. Taken another way, definition questions might be viewed as simultaneously asking a whole series of factoid questions about the same entity (e.g., “When was he born?”, “What was his occupation?”, “Where did he live?”, etc.), except that these questions are not known in advance; see Prager et al. (2004) for an implementation based on this view of definition questions. Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called P OURPRE, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system’s response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address"
H05-1117,N03-1020,0,0.0267362,"ragments from documents (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 3 Previous Work The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the B LEU metric for machine translation (Papineni et al., 2002). Since then, the basic method for scoring translation quality has been improved upon by others, e.g., (Babych and Hartley, 2004; Lin and Och, 2004). The basic idea has been extended to evaluating document summarization with ROUGE (Lin and Hovy, 2003). Recently, Soricut and Brill (2004) employed ngram co-occurrences to evaluate question answering in a FAQ domain; unfortunately, the task differs from definition question answering, making their results not directly applicable. Xu et al. (2004) applied ROUGE to automatically evaluate answers to definition questions, viewing the task as a variation of document summarization. Because TREC answer nuggets were terse phrases, the authors found it necessary to rephrase them—two humans were asked to manually create “reference answers” based on the assessors’ nuggets and IR results, which was a labor"
H05-1117,C04-1072,0,0.0194165,"Official definition of F-measure. in a system response, given that they were usually extracted text fragments from documents (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 3 Previous Work The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the B LEU metric for machine translation (Papineni et al., 2002). Since then, the basic method for scoring translation quality has been improved upon by others, e.g., (Babych and Hartley, 2004; Lin and Och, 2004). The basic idea has been extended to evaluating document summarization with ROUGE (Lin and Hovy, 2003). Recently, Soricut and Brill (2004) employed ngram co-occurrences to evaluate question answering in a FAQ domain; unfortunately, the task differs from definition question answering, making their results not directly applicable. Xu et al. (2004) applied ROUGE to automatically evaluate answers to definition questions, viewing the task as a variation of document summarization. Because TREC answer nuggets were terse phrases, the authors found it necessary to rephrase them—two humans were asked t"
H05-1117,P02-1040,0,0.128799,"R = 100 ( × (r + a) 1 if l &lt; α = 1 − l−α otherwise l (β 2 + 1) × P × R β2 × P + R β = 5 in TREC 2003, β = 3 in TREC 2004. Finally, the F (β) = Figure 2: Official definition of F-measure. in a system response, given that they were usually extracted text fragments from documents (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 3 Previous Work The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the B LEU metric for machine translation (Papineni et al., 2002). Since then, the basic method for scoring translation quality has been improved upon by others, e.g., (Babych and Hartley, 2004; Lin and Och, 2004). The basic idea has been extended to evaluating document summarization with ROUGE (Lin and Hovy, 2003). Recently, Soricut and Brill (2004) employed ngram co-occurrences to evaluate question answering in a FAQ domain; unfortunately, the task differs from definition question answering, making their results not directly applicable. Xu et al. (2004) applied ROUGE to automatically evaluate answers to definition questions, viewing the task as a variatio"
H05-1117,P04-1073,0,0.027592,"Science 3 Institute for Advanced Computer Studies University of Maryland College Park, MD 20742, USA jimmylin@umd.edu, demner@cs.umd.edu Abstract Actually a misnomer, definition questions can be better paraphrased as “Tell me interesting things about X.”, where X can be a person, an organization, a common noun, etc. Taken another way, definition questions might be viewed as simultaneously asking a whole series of factoid questions about the same entity (e.g., “When was he born?”, “What was his occupation?”, “Where did he live?”, etc.), except that these questions are not known in advance; see Prager et al. (2004) for an implementation based on this view of definition questions. Following recent developments in the automatic evaluation of machine translation and document summarization, we present a similar approach, implemented in a measure called P OURPRE, for automatically evaluating answers to definition questions. Until now, the only way to assess the correctness of answers to such questions involves manual determination of whether an information nugget appears in a system’s response. The lack of automatic methods for scoring system output is an impediment to progress in the field, which we address"
H05-1117,P04-1078,0,0.0379643,"es, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 3 Previous Work The idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs was first successfully implemented in the B LEU metric for machine translation (Papineni et al., 2002). Since then, the basic method for scoring translation quality has been improved upon by others, e.g., (Babych and Hartley, 2004; Lin and Och, 2004). The basic idea has been extended to evaluating document summarization with ROUGE (Lin and Hovy, 2003). Recently, Soricut and Brill (2004) employed ngram co-occurrences to evaluate question answering in a FAQ domain; unfortunately, the task differs from definition question answering, making their results not directly applicable. Xu et al. (2004) applied ROUGE to automatically evaluate answers to definition questions, viewing the task as a variation of document summarization. Because TREC answer nuggets were terse phrases, the authors found it necessary to rephrase them—two humans were asked to manually create “reference answers” based on the assessors’ nuggets and IR results, which was a laborintensive process. Furthermore, Xu e"
H05-1117,P04-1077,0,\N,Missing
H05-1117,P04-1079,0,\N,Missing
J07-1005,P06-1106,1,0.474642,"mputational linguistics—redundancy detection for multi-document summarization—seems easy by comparison. Furthermore, it is unclear if textual strings make “good answers.” Perhaps a graphical rendering of the semantic predicates present in relevant abstracts might more effectively convey the desired information; see, for example, Fiszman, Rindflesch, and Kilicoglu (2004). Perhaps some variation of multi-level bulleted lists, appropriately integrated with interface elements for expanding and hiding items, might provide physicians a better overview of the information landscape; see, for example, Demner-Fushman and Lin (2006). Recognizing this complex set of issues, we decided to take a simple extractive approach to answer generation. For each abstract in our reranked list of citations, our system produces an answer by combining the title of the abstract and the top three outcome sentences (in the order they appeared in the abstract). We employed the outcome scores generated by the regression model. No attempt was made to synthesize information from multiple citations. A formal evaluation of this simple approach to answer generation is presented in the next section. 10. Evaluation of Clinical Answers Evaluation of"
J07-1005,W04-2611,0,0.232825,"Missing"
J07-1005,N04-1007,1,0.324843,"Missing"
J07-1005,W04-3103,0,0.0235487,"Missing"
J07-1005,H05-1117,1,0.471269,"an directly act on. Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences. The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion—it need not be repeated unless the physician wishes to “drill down”; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization (Lin and Demner-Fushman 2005b), but these features are also beyond the capabilities of current summarization systems. It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objective"
J07-1005,W05-0906,1,0.867725,"an directly act on. Ideally, answers should integrate information from multiple clinical studies, pointing out both similarities and differences. The system should collate concurrences, that is, if multiple abstracts arrive at the same conclusion—it need not be repeated unless the physician wishes to “drill down”; the system should reconcile contradictions, for example, if two abstracts disagree on a particular treatment because they studied different patient populations. We have noted that many of these desiderata make complex question answering quite similar to multi-document summarization (Lin and Demner-Fushman 2005b), but these features are also beyond the capabilities of current summarization systems. It is clear that the type of answers desired by physicians require a level of semantic analysis that is beyond the current state of the art, even with the aid of existing medical ontologies. For example, even the seemingly straightforward task of identifying similarities and differences in outcome statements is rendered exceedingly complex by the tremendous amount of background medical knowledge that must be brought to bear in interpreting clinical results and subtle differences in study design, objective"
J07-1005,N06-1049,1,0.2699,"re exist relatively few points of comparison to our own work, as the research space is sparsely populated. In this section, however, we will attempt to draw connections to other clinical information systems (although not necessarily for question answering) and related domain-specific question-answering systems. For an overview of systems designed to answer open-domain factoid questions, the TREC QA track overview papers are a good place to start (Voorhees and Tice 1999). In addition, there has been much work on the application of linguistic and semantic knowledge to information retrieval; see Lin and Demner-Fushman (2006a) for a brief overview. The idea that clinical information systems should be sensitive to the practice of evidence-based medicine is not new. Based on analyses of 4,000 MEDLINE citations, Mendonc¸a and Cimino (2001) have studied MeSH terms associated with the four basic clinical tasks of therapy, diagnosis, prognosis, and etiology. The goal was to automatically classify citations for task-specific retrieval, similar in spirit to the Hedges Project (Haynes et al. 1994; Wilczynski, McKibbon, and Haynes 2001). Cimino and Mendonc¸a reported good performance for etiology, diagnosis, and in particu"
J07-1005,W06-3309,1,0.515262,"and demonstrates that respectable performance is possible with a feature-combination approach. 8 Although note that answer generation from the PubMed results also requires the use of the outcome extractor. 97 Computational Linguistics Volume 33, Number 1 The literature also contains work on sentence-level classification of MEDLINE abstracts for non-clinical purposes. For example, McKnight and Srinivasan (2003) describe a machine learning approach to automatically label sentences as belonging to introduction, methods, results, or conclusion using structured abstracts as training data (see also Lin et al. 2006). Tbahriti et al. (2006) have demonstrated that differential weighting of automatically labeled sections can lead to improved retrieval performance. Note, however, that such labels are orthogonal to PICO frame elements, and hence are not directly relevant to knowledge extraction for clinical question answering. In a similar vein, Light, Qiu, and Srinivasan (2004) report on the identification of speculative statements in MEDLINE abstracts, but once again, this work is not directly applicable to clinical question answering. In addition to question answering, multi-document summarization provides"
J07-1005,N04-1019,0,0.0125339,"Missing"
J07-1005,W04-0509,0,0.386645,"s a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision. PICO-based querying in information retrieval is merely an instance of faceted querying, which has been widely used by librarians since the introduction of automated retrieval systems (e.g., Meadow et al. 1989). The work of Hearst (1996) demonstrates that faceted queries can be converted into simple filtering constraints to boost precision. The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004). Their study also illustrates the importance of semantic classes and relations. However, extraction of outcome statements from secondary sources (meta-analyses, in this case) differs from extraction of outcomes from MEDLINE citations because secondary sources represent knowledge that has already been distilled by humans (which may limit its scope). Because secondary sources are often more consistently organized, it is possible to depend on certain surface cues for reliable extraction (which is not possible for MEDLINE abstracts in general). Our study tackles outcome identification in primary"
J07-1005,W04-0508,0,0.0410269,"Missing"
lin-2002-web,J98-3005,0,\N,Missing
lin-2002-web,C00-1043,0,\N,Missing
lin-2002-web,W01-1203,0,\N,Missing
lin-2002-web,W00-1102,0,\N,Missing
lin-2002-web,W01-1201,0,\N,Missing
lin-2002-web,P01-1005,0,\N,Missing
lin-2002-web,A00-1041,0,\N,Missing
N04-1007,P03-1001,0,0.0117006,"equire a system to find as many relevant nuggets as possible, making recall very important. To boost recall, we employed an alternative strategy: by applying the set of surface patterns offline, we were able to “precompile” from the AQUAINT corpus a list of nuggets about every entity mentioned within it. In essence, we have automatically constructed an immense relational database containing nuggets distilled from every article in the corpus. The task of answering definition questions then becomes a simple lookup for the relevant term. This approach is similar in spirit to the work reported by Fleischman et al. (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. Our surface patterns operated both at the word and part-of-speech level. Rudimentary chunking, such as marking the boundaries of noun phrases, was performed by grouping words based on their part-of-speech tags. In total, we applied eleven surface patterns over the entire corpus—these are detailed in Table 1, with examples in Table 2. Typically, surface patterns identify nuggets on the order of a few words. In answering definition questions, however, we decided to retur"
N04-1007,W02-1111,0,0.0124288,"ny relevant nuggets as possible, making recall very important. To boost recall, we employed an alternative strategy: by applying the set of surface patterns offline, we were able to “precompile” from the AQUAINT corpus a list of nuggets about every entity mentioned within it. In essence, we have automatically constructed an immense relational database containing nuggets distilled from every article in the corpus. The task of answering definition questions then becomes a simple lookup for the relevant term. This approach is similar in spirit to the work reported by Fleischman et al. (2003) and Mann (2002), except that our system benefits from a greater variety of patterns and answers a broader range of questions. Our surface patterns operated both at the word and part-of-speech level. Rudimentary chunking, such as marking the boundaries of noun phrases, was performed by grouping words based on their part-of-speech tags. In total, we applied eleven surface patterns over the entire corpus—these are detailed in Table 1, with examples in Table 2. Typically, surface patterns identify nuggets on the order of a few words. In answering definition questions, however, we decided to return responses that"
N04-2004,P98-1013,0,0.0339704,"yntactic parsing techniques (Collins, 1997; Charniak, 2001), attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences. A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots). Verbs are viewed as simple predicates over their arguments. This approach has its roots in Fillmore’s Case Grammar (1968), and serves as the foundation for two current large-scale semantic annotation projects: FrameNet (Baker et al., 1998) and PropBank (Kingsbury et al., 2002). Underlying the semantic roles approach is a lexicalist assumption, that is, each verb’s lexical entry completely encodes (more formally, projects) its syntactic and semantic structures. Alternations in argument structure are usually attributed to multiple lexical entries (i.e., verb senses). Under the lexicalist approach, the semantics of the verb break might look something like this: (1) break(agent, theme) agent: subject theme: object break(agent, theme, instrument) agent: subject theme: object instrument: oblique(with) break(theme) theme: subject ..."
N04-2004,P01-1017,0,0.0233009,"pread the “semantic load” of lexical entries to other morphemes not typically taken to bear semantic content. This approach follows current trends in linguistic theory, and more perspicuously accounts for alternations in argument structure. I demonstrate how such a framework can be computationally realized with a feature-based, agenda-driven chart parser for the Minimalist Program. 1 Introduction The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content. Due to advances in statistical syntactic parsing techniques (Collins, 1997; Charniak, 2001), attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences. A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots). Verbs are viewed as simple predicates over their arguments. This approach has its roots in Fillmore’s Case Grammar (1968), and serves as the foundation for two current large-scale semantic annotation projects: FrameNet (Baker et al., 1998) and PropBank (Kingsbury et al., 2002)."
N04-2004,P97-1003,0,0.0234457,"olution is to spread the “semantic load” of lexical entries to other morphemes not typically taken to bear semantic content. This approach follows current trends in linguistic theory, and more perspicuously accounts for alternations in argument structure. I demonstrate how such a framework can be computationally realized with a feature-based, agenda-driven chart parser for the Minimalist Program. 1 Introduction The understanding of natural language text includes not only analysis of syntactic structure, but also of semantic content. Due to advances in statistical syntactic parsing techniques (Collins, 1997; Charniak, 2001), attention has recently shifted towards the harder question of analyzing the meaning of natural language sentences. A common lexical semantic representation in the computational linguistics literature is a frame-based model where syntactic arguments are associated with various semantic roles (essentially frame slots). Verbs are viewed as simple predicates over their arguments. This approach has its roots in Fillmore’s Case Grammar (1968), and serves as the foundation for two current large-scale semantic annotation projects: FrameNet (Baker et al., 1998) and PropBank (Kingsbur"
N04-2004,2000.iwpt-1.13,0,0.0139968,"roke: vδ P DP window vδ vδ vB E √ break A RGδ (window, e) ∧ B ECOME(B E([state break]), e) The structure denotes an event where an entity undergoes a change of state to the end state specified by the root. vδ P can be optionally embedded as the complement 5 Minimalist Derivations My theory of verbal argument structure can be implemented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing. The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky’s Minimalist Program (Stabler, 1997; Harkema, 2000; Niyogi, 2001). Lexical entries in the system are minimally specified, each consisting of a phonetic form, a list of relevant features, and semantics in the form of a λ expression. The basic structure building operation, M ERGE, takes two items and creates a larger item. In the process, compatible features are canceled and one of the items projects. Simultaneously, the λ expression associated with the licensor is applied to the λ expression associated with the licensee (in theoretical linguistic terms, SpellOut). The most basic feature is the =x licensor feature, which cancels out a correspon"
N04-2004,W01-1814,0,0.168602,"indow vδ vδ vB E √ break A RGδ (window, e) ∧ B ECOME(B E([state break]), e) The structure denotes an event where an entity undergoes a change of state to the end state specified by the root. vδ P can be optionally embedded as the complement 5 Minimalist Derivations My theory of verbal argument structure can be implemented in a unified morpho-syntactic parsing model that interleaves syntactic and semantic parsing. The system is in the form of an agenda-driven chart-based parser whose foundation is similar to previous formalizations of Chomsky’s Minimalist Program (Stabler, 1997; Harkema, 2000; Niyogi, 2001). Lexical entries in the system are minimally specified, each consisting of a phonetic form, a list of relevant features, and semantics in the form of a λ expression. The basic structure building operation, M ERGE, takes two items and creates a larger item. In the process, compatible features are canceled and one of the items projects. Simultaneously, the λ expression associated with the licensor is applied to the λ expression associated with the licensee (in theoretical linguistic terms, SpellOut). The most basic feature is the =x licensor feature, which cancels out a corresponding x licensee"
N04-2004,J91-4003,0,0.627306,"oblem; see (Levin and Rappaport Hovav, 1996). Fixed roles are too coarsegrained to account for certain semantic distinctions—the only recourse, to expand the inventory of roles, comes with the price of increased complexity, e.g., in the syntaxto-semantics mapping. There is a general consensus among theoretical linguists that the proper representation of verbal argument structure is event structure—representations grounded in a theory of events that decompose semantic roles in terms of primitive predicates representing concepts such as causality and inchoativity (Dowty, 1979; Jackendoff, 1983; Pustejovsky, 1991b; Rappaport Hovav and Levin, 1998). Consider the following example: (2) He sweeps the floor clean. [ [ DO(he, sweeps(the floor)) ] CAUSE [ BECOME [ clean(the floor) ] ] ] Dowty breaks the event described by (2) into two subevents, the activity of sweeping the floor and its result, the state of the floor being clean. A more recent approach, advocated by Rappaport Hovav and Levin (1998), describes a basic set of event templates corresponding to Vendler’s event classes (Vendler, 1957): (3) [ x ACT&lt;MANNER&gt; ] (activity) [ x &lt;STATE&gt; ] (state) [ BECOME [ x &lt;STATE&gt; ] ] (achievement) [ x CAUSE [ BECOM"
N04-2004,C98-1013,0,\N,Missing
N06-1049,P04-1027,0,0.0620038,"Missing"
N06-1049,N04-1007,1,0.1571,"esearch & education 0.1 okay Receives millions for product endorsements 0.1 okay Receives millions from product endorsements 0.0 okay Abbreviated name to attract boomers Table 5: Answer nuggets for the target “AARP” with weights derived from the nugget pyramid building process. tively stable. We believe that around five assessors yield the smallest nugget pyramid that confers the advantages of the methodology. The idea of building “nugget pyramids” is an extension of a similarly-named evaluation scheme in document summarization, although there are important differences. Nenkova and Passonneau (2004) call for multiple assessors to annotate SCUs, which is much more involved than the methodology presented here, where the nuggets are fixed and assessors only provide additional judgments about their importance. This obviously has the advantage of streamlining the assessment process, but has the potential to miss other important nuggets that were not identified in the first place. Our experimental results, however, suggest that this is a worthwhile tradeoff. The explicit goal of this work was to develop scoring models for nugget-based evaluation that would address shortcomings of the present a"
N06-1049,H05-1117,1,0.852196,"tational Linguistics 2 Evaluation of Complex Questions vital To date, NIST has conducted three large-scale evaluations of complex questions using a nugget-based evaluation methodology: “definition” questions in TREC 2003, “other” questions in TREC 2004 and TREC 2005, and “relationship” questions in TREC 2005. Since relatively few teams participated in the 2005 evaluation of “relationship” questions, this work focuses on the three years’ worth of “definition/other” questions. The nugget-based paradigm has been previously detailed in a number of papers (Voorhees, 2003; Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005a); here, we present only a short summary. System responses to complex questions consist of an unordered set of passages. To evaluate answers, NIST pools answer strings from all participants, removes their association with the runs that produced them, and presents them to a human assessor. Using these responses and research performed during the original development of the question, the assessor creates an “answer key” comprised of a list of “nuggets”—essentially, facts about the target. According to TREC guidelines, a nugget is defined as a fact for which the assessor could make a binary decis"
N06-1049,W05-0906,1,0.944099,"tational Linguistics 2 Evaluation of Complex Questions vital To date, NIST has conducted three large-scale evaluations of complex questions using a nugget-based evaluation methodology: “definition” questions in TREC 2003, “other” questions in TREC 2004 and TREC 2005, and “relationship” questions in TREC 2005. Since relatively few teams participated in the 2005 evaluation of “relationship” questions, this work focuses on the three years’ worth of “definition/other” questions. The nugget-based paradigm has been previously detailed in a number of papers (Voorhees, 2003; Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005a); here, we present only a short summary. System responses to complex questions consist of an unordered set of passages. To evaluate answers, NIST pools answer strings from all participants, removes their association with the runs that produced them, and presents them to a human assessor. Using these responses and research performed during the original development of the question, the assessor creates an “answer key” comprised of a list of “nuggets”—essentially, facts about the target. According to TREC guidelines, a nugget is defined as a fact for which the assessor could make a binary decis"
N06-1049,N03-1020,0,0.0300988,"eeds from complementary perspectives; see, for example, the recent DUC task of query-focused multi-document summarization (Amig´o et al., 2004; Dang, 2005). From an evaluation point of view, this provides opportunities for cross-fertilization and exchange of fresh ideas. As an example of this intellectual discourse, the recently-developed P OURPRE metric for automatically evaluating answers to complex questions (Lin and Demner-Fushman, 2005a) employs n-gram overlap to compare system responses to reference output, an idea originally implemented in the ROUGE metric for summarization evaluation (Lin and Hovy, 2003). Drawing additional inspiration from research on summarization evaluation, we adapt the pyramid evaluation scheme (Nenkova and Passonneau, 2004) to address the shortcomings of the vital/okay distinction in the nugget-based evaluation methodology. The basic intuition behind the pyramid scheme (Nenkova and Passonneau, 2004) is simple: the importance of a fact is directly related to the number of people that recognize it as such (i.e., its popularity). The evaluation methodology calls for assessors to annotate Semantic Content Units (SCUs) found within model reference summaries. The weight assig"
N06-1049,N04-1019,0,0.204067,"., 2004; Dang, 2005). From an evaluation point of view, this provides opportunities for cross-fertilization and exchange of fresh ideas. As an example of this intellectual discourse, the recently-developed P OURPRE metric for automatically evaluating answers to complex questions (Lin and Demner-Fushman, 2005a) employs n-gram overlap to compare system responses to reference output, an idea originally implemented in the ROUGE metric for summarization evaluation (Lin and Hovy, 2003). Drawing additional inspiration from research on summarization evaluation, we adapt the pyramid evaluation scheme (Nenkova and Passonneau, 2004) to address the shortcomings of the vital/okay distinction in the nugget-based evaluation methodology. The basic intuition behind the pyramid scheme (Nenkova and Passonneau, 2004) is simple: the importance of a fact is directly related to the number of people that recognize it as such (i.e., its popularity). The evaluation methodology calls for assessors to annotate Semantic Content Units (SCUs) found within model reference summaries. The weight assigned to an SCU is equal to the number of annotators that have marked the particular unit. These SCUs can be arranged in a pyramid, with the highes"
N06-1049,H05-1038,0,0.119658,"cross all submitted runs is zero: 22 in TREC 2003, 41 in TREC 2004, and 44 in TREC 2005. An evaluation in which the median score for many questions is zero has many shortcomings. For one, it is difficult to tell if a particular run is “better” than another—even though they may be very different in other salient properties such as length, for example. The discriminative power of the present F-score measure is called into question: are present systems 385 that bad, or is the current scoring model insufficient to discriminate between different (poorly performing) systems? Also, as pointed out by Voorhees (2005), a score distribution heavily skewed towards zero makes meta-analysis of evaluation stability hard to perform. Since such studies depend on variability in scores, evaluations would appear more stable than they really are. While there are obviously shortcomings to the current scheme of labeling nuggets as either “vital” or “okay”, the distinction does start to capture the intuition that “not all nuggets are created equal”. Some nuggets are inherently more important than others, and this should be reflected in the evaluation methodology. The solution, we believe, is to solicit judgments from mu"
N07-1027,W04-2509,0,0.0352983,"ions (Voorhees, 2003). We believe that QA evaluation methodology has lagged behind these developments and does not adequately characterize the performance of current systems. In the next section, we present an evaluation framework that takes into account users’ desire for context and the structure of more complex QA tasks. Focusing on question series, we compare the performance of top TREC systems to a baseline IR engine using this evaluation framework. 3 An Evaluation Framework Question series in TREC represent an attempt at modeling information-seeking dialogues between a user and a system (Kato et al., 2004). Primarily because dialogue systems are difficult to evaluate, NIST has adopted a setup in which individual questions are evaluated in isolation—this implicitly models a user who types in a question, receives an answer, and then moves on to the next question in the series. Component scores are aggregated using a weighted average, and no attempt is made to capture dependencies across different question types. Simultaneously acknowledging the challenges in evaluating dialogue systems and recognizing the similarities between complex QA and query-focused summarization, we propose an alternative f"
N07-1027,H05-1117,1,0.790834,"5 “Other” Questions Our second set of experiments examine the performance of TREC systems on “other” questions. Once again, we selected the top-ranked, second-ranked, third-ranked, and median runs from TREC 2004 and TREC 2005. Since system submissions were already passages, no additional processing was necessary. The IR baseline was exactly the same as the run used in the previous experiment. Below, we describe the evaluation methodology and results. 5.1 Evaluation Methodology The evaluation of “other” questions closely mirrors the procedure developed for factoid series. We employed P OURPRE (Lin and Demner-Fushman, 2005), a recently developed method for automatically evaluating answers to complex questions. The metric relies on n-gram overlap as a surrogate for manual nugget matching, and has been shown to correlate well with official human judgments. We modified the P OURPRE scoring script to return only the nugget recall (of vital nuggets only). Formally, systems’ responses to “other” questions consist of unordered sets of answer strings. We de216 cided to break each system’s response into individual answer strings and compute nugget recall on a string-by-string basis. Since these answer strings are for the"
N07-1027,N06-1049,1,0.81155,"ht), combining both factoid and “other” questions. performance between the 2004 and 2005 test sets can be attributed to the nature of the targets. In TREC 2005, allowable semantic categories of targets were expanded to include events such as “Miss Universe 2000 crowned”, which by their very nature are narrower in scope. This, combined with many highly-specific targets, meant that the corpus contained fewer topically-relevant documents for each target to begin with. As a result, an IR-based sentence extraction approach performs quite well—this explanation is consistent with the observations of Lin and Demner-Fushman (2006). 6 Combining Question Types In the previous two sections, factoid and “other” questions were examined in isolation, which ignores their complementary role in supplying information about a target. To provide a more complete picture of system performance, we devised a method by which both question types can be evaluated together. At the conceptual level, there is little difference between factoid and “other” questions. The first type asks for explicit facts, while the second type asks for facts that the user didn’t know enough to ask about directly. We can unify the evaluation of both types by"
N07-1027,N04-1019,0,0.0398779,"e. Instead of generating individual answers to each question, a system might alternatively produce a segment of text (i.e., a summary) that attempts to answer all the questions. This slightly different conception of QA brings it into better alignment with recent trends in multidocument summarization, which may yield previously untapped synergies (see Section 7). To assess the quality of system responses, we adopt the nugget-based methodology used previously for many types of complex questions (Voorhees, 2003), which shares similarities with the pyramid evaluation scheme used in summarization (Nenkova and Passonneau, 2004). A nugget can be described as an “atomic fact” that addresses an aspect of an information need. Instead of the standard nugget F-score, which hides important tradeoffs between precision and recall, we propose to measure nugget recall as a function of response length. The goal is to quantify the number of relevant facts that a user will have encountered after reading a particular amount of text. Intuitively, we wish to model how quickly a hypothetical user could “learn” about a topic by reading system responses. Within this framework, we compared existing TREC QA systems against an IR baseline"
N07-1027,C04-1189,0,0.0577809,"Missing"
N07-1027,P99-1071,0,0.0151697,"omet was named after its two observers—two amateur astronomers in the United States who discovered it on July 22, 1995. Its visit to the solar system—just once every 4,200 years, will give millions of people a rare heavenly treat when it reaches its full brightness next year. Since projected sentences are simply concatenated, the responses often exhibit readability problems (although by chance this particular response is relatively coherent). Nevertheless, one might imagine that such output forms the basis for generating coherent query-focused summaries with sentencerewrite techniques, e.g., (Barzilay et al., 1999). In this work, we set aside problems with fluency since our evaluation framework is unable to measure this (desirable) characteristic. System responses were prepared for four runs from TREC 2004 and four runs from TREC 2005 in the manner described above. As a baseline, we employed Lucene to retrieve the top 100 documents from the AQUAINT corpus using the target as the query (in our example, “Hale Bopp comet”). From the result set, we retained all sentences that contain at least a term from the target. Sentence order within each document and across the ranked list was preserved. Answer respons"
N07-1027,H05-1038,0,0.0158314,"ably easier than pinpointing the exact answer. Thus, real-world user preferences may erode the advantage that QA has over IR techniques such as passage retrieval, e.g., (Zobel et al., 1995; Tellex et al., 2003). Second, the focus of question answering research has shifted away from isolated factoid questions to more complex information needs embedded within a broader context (e.g., a user scenario). Since 2004, the main task at the TREC QA tracks has consisted of question series organized around topics (called “targets”)—which can be people, organizations, entities, or events (Voorhees, 2004; Voorhees, 2005). Questions in a series inquire about different facets of a target, but are themselves either factoid or list questions. In addition, each series contains an explicit “other” question (always the last one), which can be paraphrased as “Tell me other interesting things about this target that I don’t know enough to ask directly.” See Table 1 for examples of question series. Separately, NIST has been exploring other types of complex information needs, 213 for example, the relationship task in TREC 2005 and the ciQA (complex, interactive Question Answering) task in TREC 2006 (Dang et al., 2006). O"
N09-4001,W08-0207,1,0.821425,"Lin is an assistant professor in the iSchool at the University of Maryland, College Park. He joined the faculty in 2004 after completing his Ph.D. in Electrical Engineering and Computer Science at MIT. Dr. Lin’s research interests lie at the intersection of natural language processing and information retrieval. 1 Proceedings of NAACL HLT 2009: Tutorials, pages 1–2, c Boulder, Colorado, June 2009. 2009 Association for Computational Linguistics He leads the University of Maryland’s effort in the Google/IBM Academic Cloud Computing Initiative. Dr. Lin has taught two semester‐long Hadoop courses [2] and has given numerous talks about MapReduce to a wide audience. Chris Dyer is a Ph.D. student at the University of Maryland, College Park, in the Department of Linguistics. His current research interests include statistical machine translation, machine learning, and the relationship between artificial language processing systems and the human linguistic processing system. He has served on program committees for AMTA, ACL, COLING, EACL, EMNLP, NAACL, ISWLT, and the ACL Workshops on Machine translation, and is one of the developers of the Moses open source machine translation toolkit. He has p"
N10-1041,N06-1049,1,0.83045,"a pre-arranged period of time shortly thereafter, each assessor was given five minutes to interact with the participant’s system, live over the web. After this interaction period, participants submitted a final run, which had presumably gained the benefit of user interaction. By comparing initial and final runs, it was possible to quantify the effect of the interaction. The target corpus was AQUAINT-2, which consists of around 970k documents totaling 2.5 GB. System responses consisted of multi-line answers and were evaluated using the “nugget” methodology with the “nugget pyramid” extension (Lin and Demner-Fushman, 2006). 3 Experiment Design This section describes our experiments for the TREC 2007 ciQA task. In summary: the initial run was generated automatically using standard MMR. The web-based interactions consisted of iterations of interactive MMR, where the user selected the best candidate extract at each step. The final run consisted of the output of interactive MMR padded with automatically-generated output. Sentence extracts were used as the basic response unit. For each topic, the top 100 documents were retrieved from the AQUAINT-2 collection with Lucene, using the topic template verbatim as the quer"
N10-1041,N07-1027,1,0.902114,"Missing"
N10-4001,W08-0207,1,0.519078,"Missing"
N10-4001,J90-1003,0,\N,Missing
N10-4001,W09-3209,0,\N,Missing
N10-4001,J93-2003,0,\N,Missing
N10-4001,C96-2141,0,\N,Missing
N10-4001,N09-4002,0,\N,Missing
N10-4001,A92-1018,0,\N,Missing
N10-4001,W02-2018,0,\N,Missing
N10-4001,D07-1090,0,\N,Missing
N10-4001,W09-0401,0,\N,Missing
N10-4001,W08-0333,1,\N,Missing
N10-4001,J98-1004,0,\N,Missing
N10-4001,N10-1021,0,\N,Missing
N10-4001,D08-1044,1,\N,Missing
N10-4001,N03-1017,0,\N,Missing
N10-4001,N03-1028,0,\N,Missing
N10-4001,J03-1002,0,\N,Missing
N10-4001,P01-1005,0,\N,Missing
N10-4001,D09-1079,0,\N,Missing
N10-4001,N10-1062,0,\N,Missing
N12-1079,D07-1090,0,0.0140684,"cs. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case). 1 Introduction It has been repeatedly shown that “throwing more data at the problem” is effective in increasing SMT output quality, both for translation modeling (Dyer et al., 2008) and for language modeling (Brants et al., 2007). In this paper, we bring together two related research threads to gather parallel sentences for improved translation modeling: cross-lingual pairwise similarity to mine comparable documents and classification to identify sentence pairs that are mutual translations. Unlike most previous work, which sidesteps the computationally-intensive task of pairwise comparisons to mine comparable documents and instead relies on heuristics, we tackle the challenge head on. This paper describes a fully open-source, scalable MapReduce-based processing pipeline that is able to automatically extract large quan"
N12-1079,J07-2003,0,0.0320452,"e effect on translation quality (this condition is called S1 hereafter). The positive results of the first classifier was then processed by the second classifier (this twolevel approach is called S2 hereafter). Candidate generation was completed in 2.4 hours on our cluster with 96 cores. These candidates went through the MapReduce shuffle-and-sort process in 0.75 hours, which were then classified in 4 hours. Processing by the more complex classifier in S2 took an additional 0.52 hours. 23.5 End-to-End MT Experiments In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al., 2006), cdec for decoding (Dyer et al., 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. The baseline system was trained on the German-English WMT10 training data, consisting of 3.1m sentence pairs. For development and testing, we used the newswire datasets provided for WMT10, including 2525 sentences for tuning and 2489 sentences for testing. Our baseline system includes all standard features, including phrase translation probabilities in both directions, word and arity penaltie"
N12-1079,W08-0333,1,0.844616,"lingual documents and instead rely on heuristics. In contrast, we confront this challenge head on using the MapReduce framework. On a modest cluster, our scalable end-to-end processing pipeline was able to automatically gather 5.8m parallel sentence pairs from English and German Wikipedia. Augmenting existing bitext with these data yielded significant improvements over a state-of-the-art baseline (2.39 BLEU points in the best case). 1 Introduction It has been repeatedly shown that “throwing more data at the problem” is effective in increasing SMT output quality, both for translation modeling (Dyer et al., 2008) and for language modeling (Brants et al., 2007). In this paper, we bring together two related research threads to gather parallel sentences for improved translation modeling: cross-lingual pairwise similarity to mine comparable documents and classification to identify sentence pairs that are mutual translations. Unlike most previous work, which sidesteps the computationally-intensive task of pairwise comparisons to mine comparable documents and instead relies on heuristics, we tackle the challenge head on. This paper describes a fully open-source, scalable MapReduce-based processing pipeline"
N12-1079,P10-4002,1,0.845074,"was then processed by the second classifier (this twolevel approach is called S2 hereafter). Candidate generation was completed in 2.4 hours on our cluster with 96 cores. These candidates went through the MapReduce shuffle-and-sort process in 0.75 hours, which were then classified in 4 hours. Processing by the more complex classifier in S2 took an additional 0.52 hours. 23.5 End-to-End MT Experiments In all experiments, our MT system learned a synchronous context-free grammar (Chiang, 2007), using GIZA++ for word alignments, MIRA for parameter tuning (Crammer et al., 2006), cdec for decoding (Dyer et al., 2010), a 5-gram SRILM for language modeling, and single-reference BLEU for evaluation. The baseline system was trained on the German-English WMT10 training data, consisting of 3.1m sentence pairs. For development and testing, we used the newswire datasets provided for WMT10, including 2525 sentences for tuning and 2489 sentences for testing. Our baseline system includes all standard features, including phrase translation probabilities in both directions, word and arity penalties, and language model scores. It achieves a BLEU score of 21.37 on the test set, which would place it 5th out of 9 systems"
N12-1079,J05-4003,0,0.728022,"process: 1. identify comparable documents and generate candidate sentence pairs, and 2. filter candidate pairs to retain parallel sentences. The general solution to the first step involves computing pairwise similarities across multi-lingual corpora. As this is computationally intensive, most 626 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 626–630, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting “inter-wiki” links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). In contrast, we adopt a more exhaustive approach by directly tackling the cross-lingual pairwise similarity problem, using MapReduce on a modest cluster. We perform experiments on German and English Wikipedia (two largest available), but our technique is general and does not depend on sparse, manually-created interwiki links. Thus, compared to those approaches, we achieve much higher recall. The second step (filtering candidate sentence pairs) is relativel"
N12-1079,J03-3002,0,0.398166,"The general solution to the first step involves computing pairwise similarities across multi-lingual corpora. As this is computationally intensive, most 626 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 626–630, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting “inter-wiki” links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). In contrast, we adopt a more exhaustive approach by directly tackling the cross-lingual pairwise similarity problem, using MapReduce on a modest cluster. We perform experiments on German and English Wikipedia (two largest available), but our technique is general and does not depend on sparse, manually-created interwiki links. Thus, compared to those approaches, we achieve much higher recall. The second step (filtering candidate sentence pairs) is relatively straightforward, and we adopt the classification approach of Munteanu and Marcu (2005). However, unlike in previous work, we need to cla"
N12-1079,N10-1063,0,0.211535,"ce pairs, and 2. filter candidate pairs to retain parallel sentences. The general solution to the first step involves computing pairwise similarities across multi-lingual corpora. As this is computationally intensive, most 626 2012 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 626–630, c Montr´eal, Canada, June 3-8, 2012. 2012 Association for Computational Linguistics studies fall back to heuristics, e.g., comparing news articles close in time (Munteanu and Marcu, 2005), exploiting “inter-wiki” links in Wikipedia (Smith et al., 2010), or bootstrapping off an existing search engine (Resnik and Smith, 2003). In contrast, we adopt a more exhaustive approach by directly tackling the cross-lingual pairwise similarity problem, using MapReduce on a modest cluster. We perform experiments on German and English Wikipedia (two largest available), but our technique is general and does not depend on sparse, manually-created interwiki links. Thus, compared to those approaches, we achieve much higher recall. The second step (filtering candidate sentence pairs) is relatively straightforward, and we adopt the classification approach of Mu"
N12-1079,C10-1124,0,0.375811,"hieve much higher recall. The second step (filtering candidate sentence pairs) is relatively straightforward, and we adopt the classification approach of Munteanu and Marcu (2005). However, unlike in previous work, we need to classify large volumes of data (due to higher recall in the first step). Therefore, we care about the relationship between classification accuracy and the speed of the classifier. Our two-stage approach gives us both high effectiveness (accuracy) and efficiency (speed). A recent study from Google describes a general solution to our problem that scales to web collections (Uszkoreit et al., 2010). The authors translate all documents from one language into another, thus transforming the problem into identifying similar mono-lingual document pairs. Nevertheless, our approach makes several additional contributions. First, we explore the effect of dataset size on results. Our conclusions are more nuanced than simply “more data is better”, since there is a tradeoff between quality and quantity. Our experiments involve orders of magnitude less data, but we nevertheless observe significant gains over a strong baseline. Overall, our approach requires far less computational resources and thus"
N12-1079,W10-1709,0,\N,Missing
N12-1079,W09-0426,0,\N,Missing
N12-1079,N10-4001,1,\N,Missing
N13-1033,brown-2004-modified,0,0.0344034,"a source sentence) and their translations are numerous, both phrase lookup and extraction are performance bottlenecks. Despite considerable research and the use of efficient indexes like suffix arrays (Manber and Myers, 1990), this problem remains not fully solved. Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres We show how to exploit the massive parallelism offered by modern general purpose graphics processing units (GPUs) to eliminate the computational bottlenecks associated with “on the fly” phrase extraction. GPUs have previously been applied to DNA sequence matching using suffix trees (Schatz et al., 2007) and suffix arrays (Gharaibeh and Ripeanu, 2010). Building on this work, we present two novel contributions: Firs"
N13-1033,P05-1032,0,0.022765,"teger identifier; with 32-bit integers the total number of bytes is 16|T |. As we will show, this turns out to be quite modest, even for large parallel corpora (§6). 3.2 Suffix Array Efficiency Tricks Previous work on translation by pattern matching using suffix arrays on serial architectures has produced a number of efficiency optimizations: 1. Binary search bounds for longer substrings are initialized to the bounds of their longest prefix. Substrings are queried only if their longest prefix string was matched in the text. 2. In addition to conditioning on the longest prefix, Zhang and Vogel (2005) and Lopez (2007) condition on a successful query for the longest proper suffix. 3. Lopez (2007) queries each unique substring of a sentence exactly once, regardless of how many times it appears in an input sentence. 4. Lopez (2007) directly indexes one-word substrings with a small auxiliary array, so that their positions in the suffix array can be found in constant time. For longer substrings, this optimization reduces the log |T |term of query complexity to log(count(a)), where a is the first word of the query string. Although these efficiency tricks are important in the serial algorithms th"
N13-1033,J07-2003,0,0.0736494,"ications latency is also important. One current limitation of our work is that large batch sizes are necessary to fully utilize the available processing power of the GPU. This and other properties of the GPU, such as the high latency involved in transferring data from main memory to GPU memory, make low-latency processing a challenge, which we hope to address. Another broad future direction is to “GPU-ify” other machine translation models and other com333 ponents in the machine translation pipeline. An obvious next step is to extend our work to the hierarchical phrase-based translation model (Chiang, 2007), which would involve extracting “gappy” phrases. Lopez (2008a) has tackled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for spe"
N13-1033,D11-1047,0,0.025636,"Missing"
N13-1033,P10-4002,1,0.838649,"has 448 CUDA cores with a peak memory bandwidth 144 GB/s. Note that the GPU was released in early 2010 and represents previous generation technology. NVIDIA’s current GPUs (Kepler) boasts raw processing power in the 1.3 TFlops (double precision) range, which is approximately three times the GPU we used. Our CPU is a 3.33 GHz Intel Xeon X5260 processor, which has two cores. As a baseline, we compared against the publicly available implementation of the CPU-based algorithms described by Lopez (2008a) found in the pycdec (Chahuneau et al., 2012) extension of the cdec machine translation system (Dyer et al., 2010). Note that we only tested grammar extraction for continuous pairs of phrases, and we did not test the slower and more complex queries for hierarchical Input Sentences Number of Words With Sampling (s300 ) No Sampling (s∞ ) With Sampling (s300 ) No Sampling (s∞ ) 2,000 4,000 6,000 8,000 57,868 117,854 161,883 214,246 Xinhua 3811 4723 5496 6391 GPU (words/second) (21.9) (20.4) (32.1) (29.7) CPU (words/second) 200 (1.5) Speedup 19× 24× 27× 32× 1917 2859 3496 4171 GPU (words/second) (8.5) (11.1) (19.9) (23.2) CPU (words/second) 1.13 (0.02) Speedup 1690× 2520× 3082× 3677× Xinhua + UN 2021 2558 293"
N13-1033,D07-1104,1,0.943616,"e considerable research and the use of efficient indexes like suffix arrays (Manber and Myers, 1990), this problem remains not fully solved. Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres We show how to exploit the massive parallelism offered by modern general purpose graphics processing units (GPUs) to eliminate the computational bottlenecks associated with “on the fly” phrase extraction. GPUs have previously been applied to DNA sequence matching using suffix trees (Schatz et al., 2007) and suffix arrays (Gharaibeh and Ripeanu, 2010). Building on this work, we present two novel contributions: First, we describe improved GPU algorithms for suffix array queries that achieve greater parallelism (§3). Second, we propose no"
N13-1033,C08-1064,1,0.90296,"ur work is that large batch sizes are necessary to fully utilize the available processing power of the GPU. This and other properties of the GPU, such as the high latency involved in transferring data from main memory to GPU memory, make low-latency processing a challenge, which we hope to address. Another broad future direction is to “GPU-ify” other machine translation models and other com333 ponents in the machine translation pipeline. An obvious next step is to extend our work to the hierarchical phrase-based translation model (Chiang, 2007), which would involve extracting “gappy” phrases. Lopez (2008a) has tackled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for speech recognition (Chong et al., 2008; Chong et al., 2009) and"
N13-1033,W99-0604,0,0.283873,"esident occupancy. To fully take advantage of the processing power, we process multiple input sentences in parallel. Compared with previous algorithms, our two-pass approach and our strategy of thread assignment to increase the amount of parallelism represent novel contributions. 4 Extracting Aligned Target Phrases The problem at line 5 of Algorithm 1 is to extract the target phrase aligned to each matching source phrase instance. Efficiency is crucial since some source phrases occur hundreds of thousands of times. Phrase extraction from word alignments typically uses the consistency check of Och et al. (1999). A consistent phrase is one for which no words inside the phrase pair are aligned to words outside the phrase pair. Usually, consistent pairs are computed offline via dynamic programming over the alignment grid, from which we extract all consistent phrase pairs up to a heuristic bound on phrase length. The online extraction algorithm of Lopez (2008a) checks for consistent phrases in a different manner. Rather than finding all consistent phrase pairs in a sentence, the algorithm asks: given a specific source phrase, is there a consistent phrase pair 328 Figure 1: Source phrase f2 f3 f4 and tar"
N13-1033,W11-2921,0,0.127121,"ckled this problem on the CPU, but it is unclear to what extent the same types of algorithms he proposed can execute efficiently in the GPU environment. Beyond phrase extraction, it might be possible to perform decoding itself in the GPU—not only will this exploit massive amounts of parallelism, but also reduce costs in moving data to and from the GPU memory. 9 Conclusion GPU parallelism offers many promises for practical and efficient implementations of language processing systems. This promise has been demonstrated for speech recognition (Chong et al., 2008; Chong et al., 2009) and parsing (Yi et al., 2011), and we have demonstrated here that it extends to machine translation as well. We believe that explorations of modern parallel hardware architectures is a fertile area of research: the field has only begun to examine the possibilities and there remain many more interesting questions to tackle. Parallelism is critical not only from the perspective of building real-world applications, but for overcoming fundamental computational bottlenecks associated with models that researchers are developing today. Acknowledgments This research was supported in part by the BOLT program of the Defense Advance"
N13-1033,2005.eamt-1.39,0,0.597781,"on are performance bottlenecks. Despite considerable research and the use of efficient indexes like suffix arrays (Manber and Myers, 1990), this problem remains not fully solved. Introduction Efficiently handling large translation models is a perennial problem in statistical machine translation. One particularly promising solution (§2) is to use the parallel text itself as an implicit representation of the translation model and extract translation units “on the fly” when they are needed to decode new input (Brown, 2004). This idea has been applied to phrase-based (Callison-Burch et al., 2005; Zhang and Vogel, 2005), hierarchical (Lopez, 2007; Lopez, 2008b; Lopez, 2008a), and syntax-based (Cromieres We show how to exploit the massive parallelism offered by modern general purpose graphics processing units (GPUs) to eliminate the computational bottlenecks associated with “on the fly” phrase extraction. GPUs have previously been applied to DNA sequence matching using suffix trees (Schatz et al., 2007) and suffix arrays (Gharaibeh and Ripeanu, 2010). Building on this work, we present two novel contributions: First, we describe improved GPU algorithms for suffix array queries that achieve greater parallelism"
N16-1108,S12-1051,0,0.130183,"sent2 , a careful reader might look for corresponding semantic units, which we operationalize in our pairwise word interaction modeling technique (Sec. 5). Second, based on the pairwise word interactions, we describe a novel similarity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement. Since not all words are created equal, important words that can make more contributions deserve extra “focus” from the model (Sec. 6). We conducted thorough evaluations on ten test sets from three SemEval STS competitions (Agirre et al., 2012; Marelli et al., 2014; Agirre et al., 2014) and two answer selection tasks (Yang et al., 2015; Wang et al., 2007). We outperform the recent multiperspective convolutional neural networks of He et al. (2015) and demonstrate state-of-the-art accuracy on all five tasks. In addition, we conducted ablation 937 Proceedings of NAACL-HLT 2016, pages 937–948, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement. 2 4. 19-Layer Deep ConvNet b"
N16-1108,S14-2010,0,0.179698,"responding semantic units, which we operationalize in our pairwise word interaction modeling technique (Sec. 5). Second, based on the pairwise word interactions, we describe a novel similarity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement. Since not all words are created equal, important words that can make more contributions deserve extra “focus” from the model (Sec. 6). We conducted thorough evaluations on ten test sets from three SemEval STS competitions (Agirre et al., 2012; Marelli et al., 2014; Agirre et al., 2014) and two answer selection tasks (Yang et al., 2015; Wang et al., 2007). We outperform the recent multiperspective convolutional neural networks of He et al. (2015) and demonstrate state-of-the-art accuracy on all five tasks. In addition, we conducted ablation 937 Proceedings of NAACL-HLT 2016, pages 937–948, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement. 2 4. 19-Layer Deep ConvNet b b b Related Work Feature engineering was th"
N16-1108,S12-1059,0,0.0115553,"Missing"
N16-1108,P14-1114,0,0.0224221,"Missing"
N16-1108,S14-2114,0,0.0604704,"Missing"
N16-1108,P09-1053,0,0.00883646,"e conducted ablation 937 Proceedings of NAACL-HLT 2016, pages 937–948, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement. 2 4. 19-Layer Deep ConvNet b b b Related Work Feature engineering was the dominant approach in most previous work; different types of sparse features were explored and found useful. For example, n-gram overlap features at the word and character levels (Madnani et al., 2012; Wan et al., 2006), syntax features (Das and Smith, 2009; Xu et al., 2014), knowledge-based features using WordNet (Fellbaum, 1998; Fern and Stevenson, 2008) and word-alignment features (Sultan et al., 2014). The recent shift from sparse feature engineering to neural network model engineering has significantly improved accuracy on STS datasets. Most previous work use sentence modeling with a “Siamese” structure (Bromley et al., 1993). For example, Hu et al. (2014) used convolutional neural networks that combine hierarchical structures with layer-by-layer composition and pooling. Tai et al. (2015) and Zhu et al. (2015) concurrently proposed tree-str"
N16-1108,D15-1042,0,0.0275886,"Missing"
N16-1108,N13-1092,0,0.0199032,"Missing"
N16-1108,D15-1181,1,0.547858,"their semantic textual similarity (STS) remains a fundamental problem in language research and lies at the core of many language processing tasks, including question answering (Lin, 2007), query ranking (Burges et al., 2005), and paraphrase generation (Xu, 2014). Traditional NLP approaches, e.g., developing hand-crafted features, suffer from sparsity because of language ambiguity and the limited amount of annotated data available. Neural networks and distributed representations can alleviate such sparsity, thus neural network-based models are widely used by recent systems for the STS problem (He et al., 2015; Tai et al., 2015; Yin and Sch¨utze, 2015). However, most previous neural network approaches are based on sentence modeling, which first maps each input sentence into a fixed-length In contrast, we focus on capturing fine-grained word-level information directly. Our contribution is twofold: First, instead of using sentence modeling, we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences. This is inspired by our own intuitions of how people recognize textual similarity: given two sentences sent1 and sent2 , a careful reader might look"
N16-1108,N10-1145,0,0.0537016,"same data. The neural network models in the table, paragraph vector (PV) (Le and Mikolov, 2014), CNN (Yu et al., 2014), and PV-Cnt/CNN-Cnt with word matching features (Yang et al., 2015), are mostly based on sentence modeling. Our model outperforms them all. TrecQA Results (Table 7). This is the largest dataset in our experiments, with over 55,000 question-answer pairs. Only recently have neural network approaches (Yu et al., 2014) started to show promising results on this decade-old dataset. Previous approaches with probabilistic tree-edit techniques or tree kernels (Wang and Manning, 2010; Heilman and Smith, 2010; Yao et al., 2013) have been successful since tree structure information per944 MRR 0.4924 0.5132 0.5160 0.6058 0.6086 0.6281 0.6652 0.7090 0.7234 Table 6: Test results on WikiQA data. Model Cui et al. (2005) Wang et al. (2007) Heilman and Smith (2010) Wang and Manning (2010) Yao et al. (2013) Severyn and Moschitti (2013) Yih et al. (2013) Wang and Nyberg (2015) Severyn and Moschitti (2015) This work Table 5: Test results on all six test sets in STS2014. We show results of the top three participating systems at the competition in Pearson’s r scores. ficial measure) of Pearson’s r scores calcu"
N16-1108,S14-2131,0,0.0502671,"Missing"
N16-1108,S14-2072,0,0.0268439,"0.6086 0.6281 0.6652 0.7090 0.7234 Table 6: Test results on WikiQA data. Model Cui et al. (2005) Wang et al. (2007) Heilman and Smith (2010) Wang and Manning (2010) Yao et al. (2013) Severyn and Moschitti (2013) Yih et al. (2013) Wang and Nyberg (2015) Severyn and Moschitti (2015) This work Table 5: Test results on all six test sets in STS2014. We show results of the top three participating systems at the competition in Pearson’s r scores. ficial measure) of Pearson’s r scores calculated based on the number of sentence pairs in each test set. We show the 1st ranked (Sultan et al., 2014), 2nd (Kashyap et al., 2014), 3rd (Lynum et al., 2014) systems in the STS2014 competition, all of which are based on heavy feature engineering. Our model does not use any sparse features, WordNet, or parse trees, but still performs favorably compared to the STS2014 winning system (Sultan et al., 2014). MAP 0.4891 0.5099 0.5110 0.5976 0.5993 0.6190 0.6520 0.6930 0.7090 MAP 0.4271 0.6029 0.6091 0.5951 0.6307 0.6781 0.7092 0.7134 0.7459 0.7588 MRR 0.5259 0.6852 0.6917 0.6951 0.7477 0.7358 0.7700 0.7913 0.8078 0.8219 Table 7: Test results on TrecQA data. mits a fine-grained focus on important words for similarity comparison"
N16-1108,S14-2055,0,0.0374822,"Missing"
N16-1108,S14-2078,0,0.0564521,"Missing"
N16-1108,N12-1019,0,0.0168839,"tate-of-the-art accuracy on all five tasks. In addition, we conducted ablation 937 Proceedings of NAACL-HLT 2016, pages 937–948, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement. 2 4. 19-Layer Deep ConvNet b b b Related Work Feature engineering was the dominant approach in most previous work; different types of sparse features were explored and found useful. For example, n-gram overlap features at the word and character levels (Madnani et al., 2012; Wan et al., 2006), syntax features (Das and Smith, 2009; Xu et al., 2014), knowledge-based features using WordNet (Fellbaum, 1998; Fern and Stevenson, 2008) and word-alignment features (Sultan et al., 2014). The recent shift from sparse feature engineering to neural network model engineering has significantly improved accuracy on STS datasets. Most previous work use sentence modeling with a “Siamese” structure (Bromley et al., 1993). For example, Hu et al. (2014) used convolutional neural networks that combine hierarchical structures with layer-by-layer composition and pooling. Tai et al. (2"
N16-1108,S14-2001,0,0.133317,"der might look for corresponding semantic units, which we operationalize in our pairwise word interaction modeling technique (Sec. 5). Second, based on the pairwise word interactions, we describe a novel similarity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement. Since not all words are created equal, important words that can make more contributions deserve extra “focus” from the model (Sec. 6). We conducted thorough evaluations on ten test sets from three SemEval STS competitions (Agirre et al., 2012; Marelli et al., 2014; Agirre et al., 2014) and two answer selection tasks (Yang et al., 2015; Wang et al., 2007). We outperform the recent multiperspective convolutional neural networks of He et al. (2015) and demonstrate state-of-the-art accuracy on all five tasks. In addition, we conducted ablation 937 Proceedings of NAACL-HLT 2016, pages 937–948, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement. 2 4. 19-Layer Deep ConvNet b b b Related Work Feat"
N16-1108,D14-1162,0,0.123023,"Missing"
N16-1108,D15-1044,0,0.0296698,"and 4) the addition of forward and backward hidden states add hadd 1t and h2s . The output of Algorithm 1 is a similarity cube simCube with size R13·|sent1 |·|sent2 |, where |sent∗ |is the number of words in the sentence sent∗ . The 13 values collected from each word pair (s, t) are: the 12 similarity distances, plus one extra dimension for the padding indicator. Note that all word interactions are modeled over word contexts in Algorithm 1, rather than individual words. Our pairwise word interaction model shares similarities with recent popular neural attention models (Bahdanau et al., 2014; Rush et al., 2015). However, there are important differences: For example, we do not use attention weight vectors or weighted 940 On the Mat There Sit Cats Figure 3: The similarity focus layer helps identify important pairwise word interactions (in black dots) depending on their importance for similarity measurement. representations, which are the core of attention models. The other difference is that attention weights are typically interpreted as soft degrees with which the model attends to particular words; in contrast, our word interaction model directly utilizes multiple similarity metrics, and thus is more"
N16-1108,S12-1060,0,0.0277296,"Missing"
N16-1108,D13-1044,0,0.0638398,"Missing"
N16-1108,Q14-1017,0,0.0631457,"Missing"
N16-1108,S14-2039,0,0.0354457,"inguistics studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement. 2 4. 19-Layer Deep ConvNet b b b Related Work Feature engineering was the dominant approach in most previous work; different types of sparse features were explored and found useful. For example, n-gram overlap features at the word and character levels (Madnani et al., 2012; Wan et al., 2006), syntax features (Das and Smith, 2009; Xu et al., 2014), knowledge-based features using WordNet (Fellbaum, 1998; Fern and Stevenson, 2008) and word-alignment features (Sultan et al., 2014). The recent shift from sparse feature engineering to neural network model engineering has significantly improved accuracy on STS datasets. Most previous work use sentence modeling with a “Siamese” structure (Bromley et al., 1993). For example, Hu et al. (2014) used convolutional neural networks that combine hierarchical structures with layer-by-layer composition and pooling. Tai et al. (2015) and Zhu et al. (2015) concurrently proposed tree-structured long short-term memory networks, which recursively construct sentence representations following their syntactic trees. There are many other exa"
N16-1108,P15-1150,0,0.518474,"xtual similarity (STS) remains a fundamental problem in language research and lies at the core of many language processing tasks, including question answering (Lin, 2007), query ranking (Burges et al., 2005), and paraphrase generation (Xu, 2014). Traditional NLP approaches, e.g., developing hand-crafted features, suffer from sparsity because of language ambiguity and the limited amount of annotated data available. Neural networks and distributed representations can alleviate such sparsity, thus neural network-based models are widely used by recent systems for the STS problem (He et al., 2015; Tai et al., 2015; Yin and Sch¨utze, 2015). However, most previous neural network approaches are based on sentence modeling, which first maps each input sentence into a fixed-length In contrast, we focus on capturing fine-grained word-level information directly. Our contribution is twofold: First, instead of using sentence modeling, we propose pairwise word interaction modeling that encourages explicit word context interactions across sentences. This is inspired by our own intuitions of how people recognize textual similarity: given two sentences sent1 and sent2 , a careful reader might look for corresponding"
N16-1108,U06-1019,0,0.00994954,"cy on all five tasks. In addition, we conducted ablation 937 Proceedings of NAACL-HLT 2016, pages 937–948, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement. 2 4. 19-Layer Deep ConvNet b b b Related Work Feature engineering was the dominant approach in most previous work; different types of sparse features were explored and found useful. For example, n-gram overlap features at the word and character levels (Madnani et al., 2012; Wan et al., 2006), syntax features (Das and Smith, 2009; Xu et al., 2014), knowledge-based features using WordNet (Fellbaum, 1998; Fern and Stevenson, 2008) and word-alignment features (Sultan et al., 2014). The recent shift from sparse feature engineering to neural network model engineering has significantly improved accuracy on STS datasets. Most previous work use sentence modeling with a “Siamese” structure (Bromley et al., 1993). For example, Hu et al. (2014) used convolutional neural networks that combine hierarchical structures with layer-by-layer composition and pooling. Tai et al. (2015) and Zhu et al."
N16-1108,C10-1131,0,0.110977,"Missing"
N16-1108,P15-2116,0,0.0802939,"Missing"
N16-1108,D07-1003,0,0.863064,"interaction modeling technique (Sec. 5). Second, based on the pairwise word interactions, we describe a novel similarity focus layer which helps the model selectively identify important word interactions depending on their importance for similarity measurement. Since not all words are created equal, important words that can make more contributions deserve extra “focus” from the model (Sec. 6). We conducted thorough evaluations on ten test sets from three SemEval STS competitions (Agirre et al., 2012; Marelli et al., 2014; Agirre et al., 2014) and two answer selection tasks (Yang et al., 2015; Wang et al., 2007). We outperform the recent multiperspective convolutional neural networks of He et al. (2015) and demonstrate state-of-the-art accuracy on all five tasks. In addition, we conducted ablation 937 Proceedings of NAACL-HLT 2016, pages 937–948, c San Diego, California, June 12-17, 2016. 2016 Association for Computational Linguistics studies and visualized our models to show the clear benefits of modeling pairwise word interactions for similarity measurement. 2 4. 19-Layer Deep ConvNet b b b Related Work Feature engineering was the dominant approach in most previous work; different types of sparse f"
N16-1108,Q15-1025,0,0.036368,"(x, y 0 ) − fθ (x, ygold )) (12) y 0 6=y gold where ygold is the ground truth label, input x is the pair of sentences x = {S1 , S2 }, θ is the model weight vector, and the function fθ (x, y 0 ) is the output of our model. In all cases, we performed optimization using RMSProp (Tieleman and Hinton, 2012) with backpropagation (Bottou, 1998), with a learning rate fixed to 10−4 . Settings. For the SICK and MSRVID experiments, we used 300-dimension GloVe word embeddings (Pennington et al., 2014). For the STS2014, WikiQA, and TrecQA experiments, we used 300dimension PARAGRAM - SL 999 embeddings from Wieting et al. (2015) and the PARAGRAM - PHRASE embeddings from Wieting et al. (2016), trained on word pairs from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013). We did not update word embeddings in all experiments. We used the SICK development set for tuning and then applied exactly the same hyperparameters to all ten test sets. For the answer selection task (WikiQA and TrecQA), we used the official trec eval scorer to compute the metrics Mean Average Precision (MAP) and Mean Reciprocal Rank (MRR) and 943 r 0.7863 0.7886 0.7993 0.8070 0.8268 0.8414 0.8477 0.8522 0.8411 0.8488 0.8491 0.8676 0.8686 0.87"
N16-1108,D15-1237,0,0.147952,"Missing"
N16-1108,N13-1106,0,0.0727441,"Missing"
N16-1108,P13-1171,0,0.0905755,"Missing"
N16-1108,N15-1091,0,0.062998,"Missing"
N16-1108,S15-2002,0,0.0604804,"Missing"
N16-1108,S14-2044,0,0.0768778,"Missing"
N16-1108,Q14-1034,0,\N,Missing
N18-2047,P14-1133,0,0.0252959,"butes to the effectiveness of a particular NN architecture. In many cases, the lack of rigorous ablation studies further compounds difficulties in interpreting results and credit assignment. To give two related examples: Melis et al. (2017) reported that standard LSTM 2 Related Work The problem of question answering on knowledge graphs dates back at least a decade, but the most relevant recent work in the NLP community comes from Berant et al. (2013). This thread of work focuses on semantic parsing, where a question is mapped to its logical form and then translated to a structured query, cf. (Berant and Liang, 2014; Reddy et al., 2014). However, the more recent S IMPLE Q UESTIONS dataset (Bordes et al., 2015) has emerged as the de facto benchmark for evaluating simple QA over knowledge graphs. The original solution of Bordes et al. (2015) featured memory networks, but over the past several 291 Proceedings of NAACL-HLT 2018, pages 291–296 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics years, researchers have applied many NN architectures for tackling this problem: Golub and He (2016) proposed a character-level attention-based encoder-decoder framework; Dai et a"
N18-2047,D13-1160,0,0.0587754,"ield progressively smaller gains over the previous state of the art (see §2 for more details). Lost in this push, we argue, is an understanding of what exactly contributes to the effectiveness of a particular NN architecture. In many cases, the lack of rigorous ablation studies further compounds difficulties in interpreting results and credit assignment. To give two related examples: Melis et al. (2017) reported that standard LSTM 2 Related Work The problem of question answering on knowledge graphs dates back at least a decade, but the most relevant recent work in the NLP community comes from Berant et al. (2013). This thread of work focuses on semantic parsing, where a question is mapped to its logical form and then translated to a structured query, cf. (Berant and Liang, 2014; Reddy et al., 2014). However, the more recent S IMPLE Q UESTIONS dataset (Bordes et al., 2015) has emerged as the de facto benchmark for evaluating simple QA over knowledge graphs. The original solution of Bordes et al. (2015) featured memory networks, but over the past several 291 Proceedings of NAACL-HLT 2018, pages 291–296 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics years, rese"
N18-2047,Q14-1030,0,0.0341383,"ss of a particular NN architecture. In many cases, the lack of rigorous ablation studies further compounds difficulties in interpreting results and credit assignment. To give two related examples: Melis et al. (2017) reported that standard LSTM 2 Related Work The problem of question answering on knowledge graphs dates back at least a decade, but the most relevant recent work in the NLP community comes from Berant et al. (2013). This thread of work focuses on semantic parsing, where a question is mapped to its logical form and then translated to a structured query, cf. (Berant and Liang, 2014; Reddy et al., 2014). However, the more recent S IMPLE Q UESTIONS dataset (Bordes et al., 2015) has emerged as the de facto benchmark for evaluating simple QA over knowledge graphs. The original solution of Bordes et al. (2015) featured memory networks, but over the past several 291 Proceedings of NAACL-HLT 2018, pages 291–296 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics years, researchers have applied many NN architectures for tackling this problem: Golub and He (2016) proposed a character-level attention-based encoder-decoder framework; Dai et al. (2016) proposed a"
N18-2047,Q18-1018,0,0.0702238,"ginal Bordes et al. (2015) paper. Cast in this light, our results suggest that neural networks have indeed contributed to real and meaningful improvements in the state of the art according to this benchmark dataset, but that the improvements directly attributable to neural networks are far more modest than previous papers may have led readers to believe. One should further keep in mind an important caveat in interpreting the results in Table 3: As Reimers and Gurevych (2017) have discussed, non-determinism associated with training neural networks can yield significant differences in accuracy. Crane (2018) further demonstrated that for answer selection in question answering, a range of mundane issues such as software versions can have a significant impact on accuracy, and these effects can be larger than incremental improvements reported in the literature. We adopt the emerging best practice of reporting results from multiple trials, but this makes comparison to previous single-point results difficult. It is worth emphasizing that all NN models we have examined can be characterized as “Deep Learning 101”: easily within the grasp of a student after taking an intro NLP course. Yet, our strong bas"
N18-2047,D17-1035,0,0.0727101,"/linking yields 73.7, which is only a 1.2 absolute decrease in end-to-end accuracy. Replacing the BiGRU with the CNN for relation prediction has only a tiny effect on accuracy (0.2 decrease at most). Results show that the baselines that don’t use neural networks (CRF + LR) perform surprisingly well: combining LR (GloVe+rel) or LR (td-idf) for relation prediction Results We begin with results on individual components. To alleviate the effects of parameter initialization, we ran experiments with n different random seeds (n = 20 for entity detection and n = 50 for relation prediction). Following Reimers and Gurevych (2017), and due to questions about assumptions of normality, we simply report the mean as well as the minimum and maximum scores achieved in square brackets. For entity detection, on the validation set, the BiLSTM (which outperforms the BiGRU) achieves 93.1 [92.8 93.4] F1, compared to the CRF at 90.2. Entity linking results (R@N ) are shown in Table 1 for both the BiLSTM and the CRF. We see that entity linking using the CRF achieves comparable accuracy, even though the CRF performs slightly worse on entity detection alone; entity linking appears to be the bottleneck. Error analysis shows that there"
N18-2047,P16-1076,0,0.484095,"so perform reasonably well. These results show that gains from sophisticated deep learning techniques proposed in the literature are quite modest and that some previous models exhibit unnecessary complexity. 1 Introduction There has been significant recent interest in simple question answering over knowledge graphs, where a natural language question such as “Where was Sasha Vujacic born?” can be answered via the lookup of a simple fact—in this case, the “place of birth” property of the entity “Sasha Vujacic”. Analysis of an existing benchmark dataset (Yao, 2015) and real-world user questions (Dai et al., 2016; Ture and Jojic, 2017) show that such questions cover a broad range of users’ needs. Most recent work on the simple QA task involves increasingly complex neural network (NN) architectures that yield progressively smaller gains over the previous state of the art (see §2 for more details). Lost in this push, we argue, is an understanding of what exactly contributes to the effectiveness of a particular NN architecture. In many cases, the lack of rigorous ablation studies further compounds difficulties in interpreting results and credit assignment. To give two related examples: Melis et al. (2017"
N18-2047,P05-1045,0,0.141211,"tion of the hidden states from the forward and backward passes. This representation is then passed through a linear layer, followed by batch normalization, ReLU activation, dropout, and a final layer that maps into the tag space. Note that since we’re examining baselines, we do not layer a CRF on top of the BiLSTM (Lample et al., 2016; Ma and Hovy, 2016). Conditional Random Fields (CRFs): Prior to the advent of neural techniques, CRFs represented the state of the art in sequence labeling, and therefore it makes sense to explore how well this method works. We specifically adopt the approach of Finkel et al. (2005), who used features such as word positions, POS tags, character n-grams, etc. 3.2 The output of entity detection is a sequence of tokens representing a candidate entity. This still needs to be linked to an actual node in the knowledge graph. In Freebase, each node is denoted by a Machine Identifier, or MID. Our formulation treats this problem as fuzzy string matching and does not use neural networks. For all the entities in the knowledge graph (Freebase), we pre-built an inverted index over ngrams n ∈ {1, 2, 3} in an entity’s name. At linking time, we generate all corresponding n-grams from th"
N18-2047,D16-1166,0,0.331662,"mapped to its logical form and then translated to a structured query, cf. (Berant and Liang, 2014; Reddy et al., 2014). However, the more recent S IMPLE Q UESTIONS dataset (Bordes et al., 2015) has emerged as the de facto benchmark for evaluating simple QA over knowledge graphs. The original solution of Bordes et al. (2015) featured memory networks, but over the past several 291 Proceedings of NAACL-HLT 2018, pages 291–296 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics years, researchers have applied many NN architectures for tackling this problem: Golub and He (2016) proposed a character-level attention-based encoder-decoder framework; Dai et al. (2016) proposed a conditional probabilistic framework using BiGRUs. Lukovnikov et al. (2017) used a hierarchical word/character-level question encoder and trained a neural network in an end-to-end manner. Yin et al. (2016) applied a character-level CNN for entity linking and a separate word-level CNN with attentive max-pooling for fact selection. Yu et al. (2017) used a hierarchical residual BiLSTM for relation detection, the results of which were combined with entity linking output. These approaches can be chara"
N18-2047,D17-1307,0,0.529594,"bly well. These results show that gains from sophisticated deep learning techniques proposed in the literature are quite modest and that some previous models exhibit unnecessary complexity. 1 Introduction There has been significant recent interest in simple question answering over knowledge graphs, where a natural language question such as “Where was Sasha Vujacic born?” can be answered via the lookup of a simple fact—in this case, the “place of birth” property of the entity “Sasha Vujacic”. Analysis of an existing benchmark dataset (Yao, 2015) and real-world user questions (Dai et al., 2016; Ture and Jojic, 2017) show that such questions cover a broad range of users’ needs. Most recent work on the simple QA task involves increasingly complex neural network (NN) architectures that yield progressively smaller gains over the previous state of the art (see §2 for more details). Lost in this push, we argue, is an understanding of what exactly contributes to the effectiveness of a particular NN architecture. In many cases, the lack of rigorous ablation studies further compounds difficulties in interpreting results and credit assignment. To give two related examples: Melis et al. (2017) reported that standar"
N18-2047,D14-1181,0,0.00866979,"ication decision only on the hidden states (forward and backward passes) of the final token, but otherwise the model architecture is the same as for entity detection. Recurrent Neural Networks (RNNs): The most obvious NN model for this task is to use RNNs; we examined both bi-directional LSTM and GRU variants over an input matrix comprised of word embeddings from the input question. Following 1 Entity Linking http://buboqa.io/ 292 Convolutional Neural Networks (CNNs): Another natural model is to use CNNs, which have been shown to perform well for sentence classification. We adopt the model of Kim (2014), albeit slightly simplified in that we use a single static channel instead of multiple channels. Feature maps of widths two to four are applied over the input matrix comprised of input tokens transformed into word embeddings, followed by max pooling, a fully-connected layer and softmax to output the final prediction. Note this is a “vanilla” CNN without any attention mechanism. are based on the structure of the knowledge graph, as neither of these signals are available from the surface lexical forms of the entities. 4 We conducted evaluations on the S IMPLE Q UES TIONS dataset (Bordes et al.,"
N18-2047,N16-1030,0,0.0339583,"help. We take this one step further and examine techniques that do not involve neural networks. Establishing strong baselines allows us to objectively quantify the contribution of various deep learning techniques. 3 standard practice, the representation of each token is a concatenation of the hidden states from the forward and backward passes. This representation is then passed through a linear layer, followed by batch normalization, ReLU activation, dropout, and a final layer that maps into the tag space. Note that since we’re examining baselines, we do not layer a CRF on top of the BiLSTM (Lample et al., 2016; Ma and Hovy, 2016). Conditional Random Fields (CRFs): Prior to the advent of neural techniques, CRFs represented the state of the art in sequence labeling, and therefore it makes sense to explore how well this method works. We specifically adopt the approach of Finkel et al. (2005), who used features such as word positions, POS tags, character n-grams, etc. 3.2 The output of entity detection is a sequence of tokens representing a candidate entity. This still needs to be linked to an actual node in the knowledge graph. In Freebase, each node is denoted by a Machine Identifier, or MID. Our for"
N18-2047,N15-3014,0,0.0925059,"hniques that do not use neural networks also perform reasonably well. These results show that gains from sophisticated deep learning techniques proposed in the literature are quite modest and that some previous models exhibit unnecessary complexity. 1 Introduction There has been significant recent interest in simple question answering over knowledge graphs, where a natural language question such as “Where was Sasha Vujacic born?” can be answered via the lookup of a simple fact—in this case, the “place of birth” property of the entity “Sasha Vujacic”. Analysis of an existing benchmark dataset (Yao, 2015) and real-world user questions (Dai et al., 2016; Ture and Jojic, 2017) show that such questions cover a broad range of users’ needs. Most recent work on the simple QA task involves increasingly complex neural network (NN) architectures that yield progressively smaller gains over the previous state of the art (see §2 for more details). Lost in this push, we argue, is an understanding of what exactly contributes to the effectiveness of a particular NN architecture. In many cases, the lack of rigorous ablation studies further compounds difficulties in interpreting results and credit assignment."
N18-2047,C16-1164,0,0.309995,"Missing"
N18-2047,P16-1101,0,0.0355874,"ne step further and examine techniques that do not involve neural networks. Establishing strong baselines allows us to objectively quantify the contribution of various deep learning techniques. 3 standard practice, the representation of each token is a concatenation of the hidden states from the forward and backward passes. This representation is then passed through a linear layer, followed by batch normalization, ReLU activation, dropout, and a final layer that maps into the tag space. Note that since we’re examining baselines, we do not layer a CRF on top of the BiLSTM (Lample et al., 2016; Ma and Hovy, 2016). Conditional Random Fields (CRFs): Prior to the advent of neural techniques, CRFs represented the state of the art in sequence labeling, and therefore it makes sense to explore how well this method works. We specifically adopt the approach of Finkel et al. (2005), who used features such as word positions, POS tags, character n-grams, etc. 3.2 The output of entity detection is a sequence of tokens representing a candidate entity. This still needs to be linked to an actual node in the knowledge graph. In Freebase, each node is denoted by a Machine Identifier, or MID. Our formulation treats this"
N18-2047,P17-1053,0,0.301476,"na, June 1 - 6, 2018. 2018 Association for Computational Linguistics years, researchers have applied many NN architectures for tackling this problem: Golub and He (2016) proposed a character-level attention-based encoder-decoder framework; Dai et al. (2016) proposed a conditional probabilistic framework using BiGRUs. Lukovnikov et al. (2017) used a hierarchical word/character-level question encoder and trained a neural network in an end-to-end manner. Yin et al. (2016) applied a character-level CNN for entity linking and a separate word-level CNN with attentive max-pooling for fact selection. Yu et al. (2017) used a hierarchical residual BiLSTM for relation detection, the results of which were combined with entity linking output. These approaches can be characterized as exploiting increasingly sophisticated modeling techniques (e.g., attention, residual learning, etc.). In this push toward complexity, we do not believe that researchers have adequately explored baselines, and thus it is unclear how much various NN techniques actually help. To this end, our work builds on Ture and Jojic (2017), who adopted a straightforward problem decomposition with simple NN models to argue that attentionbased mec"
N18-2047,D14-1162,0,0.0869661,"ry sequence of contiguous E NTITY tags and compute precision, recall, and F1 against the ground truth. For both entity linking and relation prediction, we evaluate recall at N (R@N ), i.e., whether the correct answer appears in the top N results. For end-to-end evaluation, we follow the approach of Bordes et al. (2015) and mark a prediction as correct if both the entity and the relation exactly match the ground truth. The main metric is accuracy, which is equivalent to R@1. Our models were implemented in PyTorch v0.2.0 with CUDA 8.0 running on an NVIDIA GeForce GTX 1080 GPU. GloVe embeddings (Pennington et al., 2014) of size 300 served as the input to our models. We used negative log likelihood loss to optimize model parameters using Adam, with an initial learning rate of 0.0001. We performed random search over hyperparameters, exploring a range that is typical of NNs for NLP applications; the hyperparameters were selected based on the development set. In our final model, all LSTM and GRU hidden states sizes and MLP hidden sizes were set to 300. For the CNNs, we used a size 300 output channel. Dropout rate for the CNNs was 0.5 and 0.3 for the RNNs. For the CRF implementation, we used the Stanford NER tagg"
N18-5002,D14-1181,0,0.00483351,"Missing"
N18-5013,D14-1181,0,0.0155435,"y (e.g., remote locations or developing countries) can take advantage of NN models. Such a deployment also protects user privacy, since user data does not leave the client. Second, the browser has emerged as the dominant platform for information visualization, and JavaScript-based implementations support seamless integration with modern techniques and existing toolkits (e.g., D3.js). This provides opportunities to visually inspect neural networks. We demonstrate a prototype implementation of a convolutional neural network for sentence classification, applied to sentiment analysis—the model of Kim (2014)—in JavaScript, running completely inside a web browser. Not surprisingly, we find that inference performance is significantly slower compared to code running natively, but the browser is nevertheless able to take advantage of GPUs and hardware acceleration on a variety of platforms. Our implementation enables simple visualizations that allow us to gain insights into what semantic n-gram features the model is extracting. This is useful for pedagogy (teaching students about neural networks) as well as research, since understanding a model is critical to improving it. Overall, our visualizations"
N19-1229,D17-1070,0,0.0191392,"ken (here, one kernel for the query token ‘Evernote’ is visualized). In QAtt, the query token embedding is directly “injected” into the kernel via element-wise product (blue dotted arrows). In PAtt, cosine similarity between the query token and tokens in the post within the convolution window are used as attention weights in the kernel. et al., 2016), which we refer to as a General Sentence Encoder in Section 2.1. Further adopting best practices, we incorporate query-aware convolutions with an average aggregation layer in the representation learning process. Recently, a number of researchers (Conneau et al., 2017; Mohammed et al., 2018) have started to reexamine simple baselines and found them to be highly competitive with the state of the art, especially with proper tuning. For example, the InferSent approach (Conneau et al., 2017) uses a simple BiLSTM with max pooling that achieves quite impressive accuracy on several classification benchmarks. Our contribution is along similar lines, where we explore simple yet highly effective models for ranking social media posts, to gain insights into query–post relevance matching using standard neural architectures. Experiments with TREC Microblog datasets show"
N19-1229,S16-1170,1,0.836221,"n alternative, they advocate a bottom-up approach where architectural complexity is gradually increased. We adopt exactly such an approach, focused exclusively on word-level modeling. As shown in Figure 1, we examine variants of a simple, generic architecture that has emerged as “best practices” in the NLP community for tackling modeling problems on two input sequences: a Siamese CNN architecture for learning representations over both inputs (a query and a social media post in our case), followed by fully-connected layers that produce a final relevance prediction (Severyn and Moschitti, 2015; He et al., 2016; Rao 2212 Proceedings of NAACL-HLT 2019, pages 2212–2217 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics Query Query Evernote Evernote hacked hacked ℎ&quot; Your Evernote cosine similarity ℎ&quot; Your Evernote QAtt Kernel password PAtt Kernel password has has been been hacked hacked Social Media Post Social Media Post Figure 2: The Query-Aware Attention (QAtt) architecture on the left and the Position-Aware Attention (PAtt) architecture on the right. In both, we construct F convolutional kernels for each query token (here, one kernel for the query token"
N19-1229,N18-2047,1,0.845737,"for the query token ‘Evernote’ is visualized). In QAtt, the query token embedding is directly “injected” into the kernel via element-wise product (blue dotted arrows). In PAtt, cosine similarity between the query token and tokens in the post within the convolution window are used as attention weights in the kernel. et al., 2016), which we refer to as a General Sentence Encoder in Section 2.1. Further adopting best practices, we incorporate query-aware convolutions with an average aggregation layer in the representation learning process. Recently, a number of researchers (Conneau et al., 2017; Mohammed et al., 2018) have started to reexamine simple baselines and found them to be highly competitive with the state of the art, especially with proper tuning. For example, the InferSent approach (Conneau et al., 2017) uses a simple BiLSTM with max pooling that achieves quite impressive accuracy on several classification benchmarks. Our contribution is along similar lines, where we explore simple yet highly effective models for ranking social media posts, to gain insights into query–post relevance matching using standard neural architectures. Experiments with TREC Microblog datasets show that our best model not"
N19-1229,D14-1162,0,0.0909146,"Missing"
N19-1229,D14-1181,0,0.00547347,"over many previous neural models, e.g., K-NRM (Xiong et al., 2017) and DUET (Mitra et al., 2017), by a significant margin. To the best of our knowledge, Rao et al. (2019) is the most effective neural model to date. We compared against two variants of MP-HCNN; MP-HCNN+QL includes a linear interpolation with QL scores. Results and Discussion Table 3 shows the effectiveness of all variants of our model, compared against previous results copied from Rao et al. (2019). Model 1 illustrates the effectiveness of the basic BiCNN model with a kernel window size of two; combining different window sizes (Kim, 2014) doesn’t yield any improvements. It appears that this model performs worse than the QL baseline. Comparing Model 2 to Model 1, we find that query-aware kernels contribute significant improvements, achieving effectiveness comparable to the QL baseline. With Model 3, which captures positional information with the position-aware encoder, we obtain competitive effectiveness compared to Model 8, the full MP-HCNN model that includes interpolation with QL. Note that Model 8 leverages additional signals, including URL information, character-level encodings, and external term features such as tf–idf. W"
N19-1229,D18-1211,0,0.0243565,"ngs Post Embeddings Figure 1: Our model architecture: a general sentence encoder is applied on query and post embeddings to generate gq and gp ; an attention encoder is applied on post embeddings to generate variable-length queryaware features hi . These features are further aggregated to yield v, which feeds into the final prediction. Despite a large body of work on neural ranking models for “traditional” ad hoc retrieval over web pages and newswire documents (Huang et al., 2013; Shen et al., 2014; Guo et al., 2016; Xiong et al., 2017; Mitra et al., 2017; Pang et al., 2017; Dai et al., 2018; McDonald et al., 2018), there has been surprisingly little work (Rao et al., 2017) on applying neural networks to searching short social media posts such as tweets on Twitter. Rao et al. (2019) identified short document length, informality of language, and heterogeneous relevance signals as main challenges in relevance modeling, and proposed the first neural model specifically designed to handle these characteristics. Evaluation on a number of datasets from the TREC Microblog Tracks demonstrates state-of-the-art effectiveness as well as the necessity of different model components to capture a multitude of relevance"
N19-1229,K17-1028,0,0.0202336,"rst neural model specifically designed to handle these characteristics. Evaluation on a number of datasets from the TREC Microblog Tracks demonstrates state-of-the-art effectiveness as well as the necessity of different model components to capture a multitude of relevance signals. ∗ ? General Sentence Encoder : QAtt or PAtt : Avg Aggregation : Introduction 1 ?# Work done at the University of Maryland, College Park. https://github.com/Impavidity/samCNN In this paper, we also examine the problem of modeling relevance for ranking short social media posts, but from a complementary perspective. As Weissenborn et al. (2017) notes, most systems are built in a top-down process: authors propose a complex architecture and then validate design decisions with ablation experiments. However, such experiments often lack comparisons to strong baselines, which raises the question as to whether model complexity is empirically justified. As an alternative, they advocate a bottom-up approach where architectural complexity is gradually increased. We adopt exactly such an approach, focused exclusively on word-level modeling. As shown in Figure 1, we examine variants of a simple, generic architecture that has emerged as “best pr"
N19-1229,D15-1181,1,\N,Missing
N19-1229,D18-1051,0,\N,Missing
N19-1229,Q18-1018,0,\N,Missing
N19-1408,P15-1017,0,0.0204837,"his model nicely captures the intuition that modeling word sequences in sentences should be handled separately from sentence-level discourse modeling, one wonders if such complex architectures are really necessary, especially given the size of training data available today. An important variant of document classification is the multi-label, multi-class case. Liu et al. (2017) develop XML-CNNs for multi-label text classification, basing the architecture on KimCNN (Kim, 2014) with increased filter sizes and an additional fully-connected layer. They also incorporate dynamic adaptive max-pooling (Chen et al., 2015) instead of the vanilla max-pooling over time in KimCNN. The paper compares with CNN-based approaches for the multi-label task, but only reports precision and disregards recall. Yang et al. (2018) instead adopts encoder–decoder sequence generation models (SGMs) for generating multiple labels for each document. Similar to our critique of HAN, we opine against the high complexity of these multi-label approaches. 2.2 b BiLSTM Model We design our model to be minimalistic: First, we feed the word embeddings w1:n of a document to a single-layer BiLSTM, extracting concatenated forward and backward wo"
N19-1408,Q18-1018,0,0.043009,"om seeds, reporting the mean validation set scores and their corresponding test set results. and data splits.3 As a result, we simply copy the value reported in Yang et al. (2018) in Table 2, row 8, which represents their maximum F1 score. To verify the correctness of our HAN and KimCNN reimplementations, we compare the differences in F1 and accuracy on the appropriate datasets. We attribute the small differences to using different dataset splits (see Section 4.1) and reporting mean values. Toward Robust Baselines. Recently, reproducibility is becoming a growing concern for the NLP community (Crane, 2018). Indeed, very few of the papers that we consider in this study report validation set results, let alone run on multiple seeds. In order to address these issues, we report scores on both validation and test sets for our reimplementations; doing so is good practice, since it reinforces the validity of the experimental results and claims. We also provide the standard deviation of the scores across different seeds to demonstrate the stability of our results. This is in line with previous papers (Zhang and Wallace, 2017; Reimers and Gurevych, 2017; Crane, 2018) that emphasize reporting variance fo"
N19-1408,D14-1181,0,0.0216559,"erarchical attention network (HAN), uses word- and sentence-level attention in classifying documents (Yang et al., 2016). Although this model nicely captures the intuition that modeling word sequences in sentences should be handled separately from sentence-level discourse modeling, one wonders if such complex architectures are really necessary, especially given the size of training data available today. An important variant of document classification is the multi-label, multi-class case. Liu et al. (2017) develop XML-CNNs for multi-label text classification, basing the architecture on KimCNN (Kim, 2014) with increased filter sizes and an additional fully-connected layer. They also incorporate dynamic adaptive max-pooling (Chen et al., 2015) instead of the vanilla max-pooling over time in KimCNN. The paper compares with CNN-based approaches for the multi-label task, but only reports precision and disregards recall. Yang et al. (2018) instead adopts encoder–decoder sequence generation models (SGMs) for generating multiple labels for each document. Similar to our critique of HAN, we opine against the high complexity of these multi-label approaches. 2.2 b BiLSTM Model We design our model to be m"
N19-1408,N18-2047,1,0.891566,"Missing"
N19-1408,N18-1202,0,0.121387,"Missing"
N19-1408,I17-1026,0,0.0285528,"Baselines. Recently, reproducibility is becoming a growing concern for the NLP community (Crane, 2018). Indeed, very few of the papers that we consider in this study report validation set results, let alone run on multiple seeds. In order to address these issues, we report scores on both validation and test sets for our reimplementations; doing so is good practice, since it reinforces the validity of the experimental results and claims. We also provide the standard deviation of the scores across different seeds to demonstrate the stability of our results. This is in line with previous papers (Zhang and Wallace, 2017; Reimers and Gurevych, 2017; Crane, 2018) that emphasize reporting variance for robustness against potentially spurious conclusions. Baseline Comparison. We see that our simple LSTMreg model achieves state of the art on Reuters and IMDB (see Table 2, rows 9 and 10), establishing mean scores of 87.0 and 52.8 for F1 score and accuracy on the test sets of Reuters and IMDB, respectively. This highlights the efficacy of proper regularization and optimization techniques for the task. We observe that LSTMreg consistently improves upon the performance of LSTMbase across all of the tasks—see rows 9 an"
N19-1408,D17-1035,0,0.025549,"oducibility is becoming a growing concern for the NLP community (Crane, 2018). Indeed, very few of the papers that we consider in this study report validation set results, let alone run on multiple seeds. In order to address these issues, we report scores on both validation and test sets for our reimplementations; doing so is good practice, since it reinforces the validity of the experimental results and claims. We also provide the standard deviation of the scores across different seeds to demonstrate the stability of our results. This is in line with previous papers (Zhang and Wallace, 2017; Reimers and Gurevych, 2017; Crane, 2018) that emphasize reporting variance for robustness against potentially spurious conclusions. Baseline Comparison. We see that our simple LSTMreg model achieves state of the art on Reuters and IMDB (see Table 2, rows 9 and 10), establishing mean scores of 87.0 and 52.8 for F1 score and accuracy on the test sets of Reuters and IMDB, respectively. This highlights the efficacy of proper regularization and optimization techniques for the task. We observe that LSTMreg consistently improves upon the performance of LSTMbase across all of the tasks—see rows 9 and 10, where, on average, reg"
N19-1408,D15-1167,0,0.0887185,"4 68.0 ±0.6 – 71.0† 43.1 42.5 42.9 ±0.3 – – 51.8 ±0.3 – – 43.4 42.4 42.7 ±0.4 37.6†† – 51.2 ±0.3 49.4‡ – 61.1 59.7 66.5 ±0.1 – – 68.2 ±0.1 – – 60.9 59.6 66.1 ±0.6 61.0†† – 67.9 ±0.1 70.5‡ – 87.6 ±0.2 84.9 ±0.3 72.1 ±0.4 69.6 ±0.4 52.5 ±0.2 52.1 ±0.3 68.6 ±0.1 68.4 ±0.1 89.1 ±0.8 87.0 ±0.5 73.1 ±0.4 70.5 ±0.5 53.4 ±0.2 52.8 ±0.3 69.0 ±0.1 68.7 ±0.1 Table 2: Results for each model on the validation and test sets; best values are bolded in blue. Repl. reports mean ± SD of five runs from our reimplementations; Orig. refers to point estimates from † Yang et al. (2018), ‡ Yang et al. (2016), and †† Tang et al. (2015). For our optimization objective, we use crossentropy and binary cross-entropy loss for singlelabel and multi-label tasks, respectively. On all datasets and models, we use 300-dimensional word vectors (Mikolov et al., 2013) pre-trained on Google News. We train all neural models for 30 epochs with five random seeds, reporting the mean validation set scores and their corresponding test set results. and data splits.3 As a result, we simply copy the value reported in Yang et al. (2018) in Table 2, row 8, which represents their maximum F1 score. To verify the correctness of our HAN and KimCNN reimp"
N19-1408,C18-1330,0,0.347804,"really necessary, especially given the size of training data available today. An important variant of document classification is the multi-label, multi-class case. Liu et al. (2017) develop XML-CNNs for multi-label text classification, basing the architecture on KimCNN (Kim, 2014) with increased filter sizes and an additional fully-connected layer. They also incorporate dynamic adaptive max-pooling (Chen et al., 2015) instead of the vanilla max-pooling over time in KimCNN. The paper compares with CNN-based approaches for the multi-label task, but only reports precision and disregards recall. Yang et al. (2018) instead adopts encoder–decoder sequence generation models (SGMs) for generating multiple labels for each document. Similar to our critique of HAN, we opine against the high complexity of these multi-label approaches. 2.2 b BiLSTM Model We design our model to be minimalistic: First, we feed the word embeddings w1:n of a document to a single-layer BiLSTM, extracting concatenated forward and backward word-level context vectors h1:n = hf1:n ⊕ hb1:n . Subsequently, we max-pool h1:n across time to yield document vector d—see Figure 1, labels a–f. Finally, we feed d to a sigmoid or a softmax layer o"
N19-1408,N16-1174,0,0.799139,"works that tackle real-world problems in production environments. Like the papers cited above, we question the need for overly complex neural architectures, focusing on the problem of document classification. Starting with a large-scale reproducibility study of several recent neural models, we find that a simple bi-directional LSTM (BiLSTM) architecture with appropriate regularization yields accuracy and F1 that are either competitive or exceed the state of the art on four standard benchmark datasets. As the closest comparison point, we find no benefit to the hierarchical modeling proposed by Yang et al. (2016) and we are able to achieve good classification results without attention mechanisms. While these regularization techniques, borrowed from language modeling, are not novel, we are to our knowledge the first to apply them in this context. Our work provides an opensource platform and the foundation for future work in document classification. 4046 Proceedings of NAACL-HLT 2019, pages 4046–4051 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics 2 Background and Related Work a 1 2.1 c g max-pool There have been attempts to extend dropout (Srivastava et a"
N19-2008,N15-1011,0,0.0141541,"ct to the union of all results after deduplication (so these values are not directly comparable to previous tables). Also, note that while Rtot is useful as a retrospective metric, 5 Related Work and Discussion Detection of complaint escalation intents is straightforwardly formulated as a text classification problem, which of course has been studied for decades. Prior to the neural wave, popular techniques include Naive Bayes (McCallum and Nigam, 1998) and Support Vector Machines (Joachims, 1998) with feature engineering. There is plethora of work based on CNNs (LeCun et al., 1998; Kim, 2014; Johnson and Zhang, 2015; Conneau et al., 2017) and RNNs (Johnson and Zhang, 2016; Zhou et al., 2015; Socher et al., 2013); attention mechanisms have also been found to be effective (Yang et al., 2016; Du et al., 2017; Du and Huang, 2018). We readily concede that there are at best minor modeling advances in this work and thus little novelty from a purely academic perspective. However, our primary contribution is to provide a case study to the broader community of how NLP solutions are deployed in production settings. In this respect, we make two points: First, we feel that many models discussed in the academic litera"
N19-2008,E17-2068,0,0.0409294,"Missing"
N19-2008,D14-1181,0,0.00330811,"with respect to the union of all results after deduplication (so these values are not directly comparable to previous tables). Also, note that while Rtot is useful as a retrospective metric, 5 Related Work and Discussion Detection of complaint escalation intents is straightforwardly formulated as a text classification problem, which of course has been studied for decades. Prior to the neural wave, popular techniques include Naive Bayes (McCallum and Nigam, 1998) and Support Vector Machines (Joachims, 1998) with feature engineering. There is plethora of work based on CNNs (LeCun et al., 1998; Kim, 2014; Johnson and Zhang, 2015; Conneau et al., 2017) and RNNs (Johnson and Zhang, 2016; Zhou et al., 2015; Socher et al., 2013); attention mechanisms have also been found to be effective (Yang et al., 2016; Du et al., 2017; Du and Huang, 2018). We readily concede that there are at best minor modeling advances in this work and thus little novelty from a purely academic perspective. However, our primary contribution is to provide a case study to the broader community of how NLP solutions are deployed in production settings. In this respect, we make two points: First, we feel that many models discuss"
N19-2008,D16-1076,0,0.0301256,"Missing"
N19-2008,Q17-1010,0,0.0110467,"Missing"
N19-2008,E17-1104,0,0.0138031,"sults after deduplication (so these values are not directly comparable to previous tables). Also, note that while Rtot is useful as a retrospective metric, 5 Related Work and Discussion Detection of complaint escalation intents is straightforwardly formulated as a text classification problem, which of course has been studied for decades. Prior to the neural wave, popular techniques include Naive Bayes (McCallum and Nigam, 1998) and Support Vector Machines (Joachims, 1998) with feature engineering. There is plethora of work based on CNNs (LeCun et al., 1998; Kim, 2014; Johnson and Zhang, 2015; Conneau et al., 2017) and RNNs (Johnson and Zhang, 2016; Zhou et al., 2015; Socher et al., 2013); attention mechanisms have also been found to be effective (Yang et al., 2016; Du et al., 2017; Du and Huang, 2018). We readily concede that there are at best minor modeling advances in this work and thus little novelty from a purely academic perspective. However, our primary contribution is to provide a case study to the broader community of how NLP solutions are deployed in production settings. In this respect, we make two points: First, we feel that many models discussed in the academic literature are too complex fo"
N19-2008,pak-paroubek-2010-twitter,0,0.0139139,"s of accuracy. In total, we use nine features, described in Table 1. Features 1 through 6 are self-explanatory and represent counts of various token types and simple statistics. Feature 7 is the sentiment score from a logistic regression classifier that we have separately trained on social media data. The training data contains 6.981 million Chinese microblog messages (Weibo) with at least one emoji or emoticon. The emojis and emoticons are used as (noisy) sentiment labels, e.g., happy face for positive and sad face for negative. This distant supervision method is widely used in social media (Pak and Paroubek, 2010; Lin and Kolcz, 2012). In our task, all emojis and emoticons are removed from the text during training as they only serve as the labels. Features 8 and 9 are counts from two term dictionaries, called TD1 and TD2. TD1 contains 121 terms and was manually gathered by examining customer dialogues. TD2 contains 8, 712 terms and was extracted by computing the pointwise KL-divergence (Tan et al., 2016) between the term distributions of positive vs. negative training examples, and then selecting the top words according to this measure. TD1 is a subset of TD2. All features (except for Feature 7) are n"
N19-2008,D13-1170,0,0.00510664,"evious tables). Also, note that while Rtot is useful as a retrospective metric, 5 Related Work and Discussion Detection of complaint escalation intents is straightforwardly formulated as a text classification problem, which of course has been studied for decades. Prior to the neural wave, popular techniques include Naive Bayes (McCallum and Nigam, 1998) and Support Vector Machines (Joachims, 1998) with feature engineering. There is plethora of work based on CNNs (LeCun et al., 1998; Kim, 2014; Johnson and Zhang, 2015; Conneau et al., 2017) and RNNs (Johnson and Zhang, 2016; Zhou et al., 2015; Socher et al., 2013); attention mechanisms have also been found to be effective (Yang et al., 2016; Du et al., 2017; Du and Huang, 2018). We readily concede that there are at best minor modeling advances in this work and thus little novelty from a purely academic perspective. However, our primary contribution is to provide a case study to the broader community of how NLP solutions are deployed in production settings. In this respect, we make two points: First, we feel that many models discussed in the academic literature are too complex for operational deployment: model complexity increases training time, inferen"
N19-2008,N16-1174,0,0.196984,"by connecting the customer with a specialized service agent to intervene and provide a higher level of attention. Our work makes the following contributions: To our knowledge, we are the first to formalize and examine this problem of identifying complaint escalation. This problem is more challenging than just performing sentiment analysis: plenty of unhappy customers express negative affect in their dialogues without escalating and filing additional grievances. Tackling this challenge requires identifying the intent of the customer. Our explorations began with Hierarchical Attention Networks (Yang et al., 2016), which we have adapted and simplified for our task. As with most real-world systems, our final model integrates manually-engineered features, and we show that such explicit features complement latent representations learned by recurrent neural networks. We evaluated our models both on retrospective data and in a trial online deployment. Controlled ablation studies show the contributions of neural as well as non-neural signals, and confirm that our model outperforms competitive baselines. For the academic audience, we discuss factors that impact production deployment: one important message is"
N19-4013,D18-1053,0,0.316479,"impractical to apply inference exhaustively to all documents in a corpus with current models (mostly based on neural networks), this formulation necessarily requires some type of term-based retrieval technique to restrict the input text under consideration—and hence an architecture quite like the pipelined systems from over a decade ago. Recently, there has been a resurgence of interest in this task, the most notable of which is Dr.QA (Chen et al., 2017). Other recent papers have examined the role of retrieval in this end-to-end formulation (Wang et al., 2017; Kratzwald and Feuerriegel, 2018; Lee et al., 2018), some of which have, in essence, rediscovered ideas from the late 1990s and early 2000s. For a wide range of applications, researchers have recently demonstrated the effectiveness of neural models that have been pretrained on a language modeling task (Peters et al., 2018; Radford et al., 2018); BERT (Devlin et al., 2018) is the latest refinement of this idea. Our work tackles end-to-end question answering by combining BERT with Anserini, an IR toolkit built on top of the popular open-source Lucene search engine. Anserini (Yang et al., 2017, 2018) represents recent efforts by researchers to br"
N19-4013,P18-1160,0,0.0721202,"Missing"
N19-4013,N18-1202,0,0.429397,"QA benchmark datasets today—for example, TrecQA (Yao et al., 2013), WikiQA (Yang et al., 2015), and MSMARCO (Bajaj et al., 2016)—are best characterized as answer selection tasks. That is, the system is given the question as well as a candidate list of sentences to choose from. Of course, those candidates have to come from somewhere, but their source lies outside the problem formulation. Similarly, reading comprehension datasets such as SQuAD (Rajpurkar et al., 2016) Introduction BERT (Devlin et al., 2018), the latest refinement of a series of neural models that make heavy use of pretraining (Peters et al., 2018; Radford et al., 2018), has led to impressive gains in many natural language processing tasks, ranging from sentence classification to question answering to sequence labeling. Most relevant to our task, Nogueira and Cho (2019) showed impressive gains in using BERT for query-based passage reranking. In this demonstration, we integrate BERT with the open-source Anserini IR toolkit to create BERTserini, an end-to-end open-domain question answering (QA) system. Unlike most QA or reading comprehension models, which are best described as rerankers or extractors since they assume as input relatively"
N19-4013,D16-1264,0,0.196563,"phasize various aspects of linguistic analysis. Information retrieval techniques receded into the background and became altogether ignored. Most popular QA benchmark datasets today—for example, TrecQA (Yao et al., 2013), WikiQA (Yang et al., 2015), and MSMARCO (Bajaj et al., 2016)—are best characterized as answer selection tasks. That is, the system is given the question as well as a candidate list of sentences to choose from. Of course, those candidates have to come from somewhere, but their source lies outside the problem formulation. Similarly, reading comprehension datasets such as SQuAD (Rajpurkar et al., 2016) Introduction BERT (Devlin et al., 2018), the latest refinement of a series of neural models that make heavy use of pretraining (Peters et al., 2018; Radford et al., 2018), has led to impressive gains in many natural language processing tasks, ranging from sentence classification to question answering to sequence labeling. Most relevant to our task, Nogueira and Cho (2019) showed impressive gains in using BERT for query-based passage reranking. In this demonstration, we integrate BERT with the open-source Anserini IR toolkit to create BERTserini, an end-to-end open-domain question answering (Q"
N19-4013,P17-1171,0,0.479849,"e document from which to extract answers. In contrast, what we refer to as “end-to-end” question answering begins with a large corpus of documents. Since it is impractical to apply inference exhaustively to all documents in a corpus with current models (mostly based on neural networks), this formulation necessarily requires some type of term-based retrieval technique to restrict the input text under consideration—and hence an architecture quite like the pipelined systems from over a decade ago. Recently, there has been a resurgence of interest in this task, the most notable of which is Dr.QA (Chen et al., 2017). Other recent papers have examined the role of retrieval in this end-to-end formulation (Wang et al., 2017; Kratzwald and Feuerriegel, 2018; Lee et al., 2018), some of which have, in essence, rediscovered ideas from the late 1990s and early 2000s. For a wide range of applications, researchers have recently demonstrated the effectiveness of neural models that have been pretrained on a language modeling task (Peters et al., 2018; Radford et al., 2018); BERT (Devlin et al., 2018) is the latest refinement of this idea. Our work tackles end-to-end question answering by combining BERT with Anserini"
N19-4013,P18-1078,0,0.0559442,"sentences and indexed, where each sentence is treated as a “document”. At inference time, we retrieve k text segments (one of the above conditions) using the question as a “bag of words” query. We use a post-v0.3.0 branch of Anserini,1 with BM25 as the ranking function (Anserini’s default parameters). 3.2 BERT Reader Text segments from the retriever are passed to the BERT reader. We use the model in Devlin et al. (2018), but with one important difference: to allow comparison and aggregation of results from different segments, we remove the final softmax layer over different answer spans; cf. (Clark and Gardner, 2018). Our BERT reader is based on Google’s reference implementation2 (TensorFlow 1.12.0). For System Architecture The architecture of BERTserini is shown in Figure 1 and is comprised of two main modules, the Anserini retriever and the BERT reader. The retriever is responsible for selecting segments of text that contain the answer, which is then passed to the reader to identify an answer span. To facilitate 1 2 73 http://anserini.io/ https://github.com/google-research/bert EM F1 1 R Dr.QA (Chen et al., 2017) Dr.QA + Fine-tune Dr.QA + Multitask R3 (Wang et al., 2017) Kratzwald and Feuerriegel (2018)"
N19-4013,D15-1237,0,0.166301,"Missing"
N19-4013,N13-1106,0,0.0395273,"Missing"
P06-1106,P04-1027,0,0.06751,"Missing"
P06-1106,W04-0509,0,0.151894,"e clinical findings. Since the system by default orders the clusters based on size, it implicitly equates “most popular drug” with “best drug”. Although this assumption is false, we have observed in practice that more-studied drugs are more likely to be beneficial. In contrast with the genomics domain, which has received much attention from both the IR and NLP communities, retrieval systems for the clinical domain represent an underexplored area of research. Although individual components that attempt to operationalize principles of evidencebased medicine do exist (Mendonc¸a and Cimino, 2001; Niu and Hirst, 2004), complete end–to– end clinical question answering systems are difstandard non-parametric test for applications of this type. Due to the relatively small test set (only 25 questions), the increase in cumulative relevance exhibited by the cluster round-robin condition is not statistically significant. However, differences for the oracle conditions were significant. 7 Discussion and Related Work According to two separate evaluations, it appears that our system outperforms the PubMed baseline. However, our approach provides more advantages over a linear result set that are not highlighted in thes"
P06-1106,H05-1038,0,0.0503679,"on Complex information needs can rarely be addressed by single documents, but rather require the integration of knowledge from multiple sources. This suggests that modern information retrieval systems, which excel at producing ranked lists of documents sorted by relevance, may not be sufficient to provide users with a good overview of the “information landscape”. Current question answering systems aspire to address this shortcoming by gathering relevant “facts” from multiple documents in response to information needs. The so-called “definition” or “other” questions at recent TREC evaluations (Voorhees, 2005) serve as good examples: 2 Clinical Information Needs Although the need to answer questions related to patient care has been well documented (Covell et al., 1985; Gorman et al., 1994; Ely et al., 1999), studies have shown that existing search systems, e.g., PubMed, the U.S. National Library of Medicine’s search engine, are often unable to supply physicians with clinically-relevant answers in a timely manner (Gorman et al., 1994; Chambliss and Conley, 1996). Clinical information 841 Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the ACL,"
P06-1106,J07-1005,1,\N,Missing
P06-1119,2006.eamt-1.18,1,0.694848,"Missing"
P06-1119,J93-2003,0,0.00721652,"Missing"
P06-1119,cmejrek-etal-2004-prague,1,0.901336,"Missing"
P06-1119,W05-1010,0,0.0143561,"and word order. 5 Related Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation"
P06-1119,C96-1006,0,0.0262257,"Work Several studies have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward pro"
P06-1119,J03-1002,0,0.00292812,"us translation are geared toward providing domain-specific thesauri to specialists in a particular field, e.g., medical terminology (Déjean, et al., 2005) and agricultural terminology (Chun & Wenlin, 2002). Researchers on these projects are faced with either finding human translators who are specialized enough to manage the domain-particular translations—or applying automatic techniques to large-scale parallel corpora where data sparsity poses a problem for lowfrequency terms. Data sparsity is also an issue for more general state-of-the-art bilingual alignment approaches (Brown, et al., 2000; Och & Ney, 2003; Wantanabe & Sumita, 2003). 6 Conclusion The task of translating large ontologies can be recast as a problem of implementing fast and efficient processes for acquiring task-specific lexical resources. We developed a method for prioritizing keyword phrases from an English thesaurus of concepts and elicited Czech translations for a subset of the keyword phrases. From these, we decomposed phrase elements for reuse in an English-Czech probabilistic dictionary. We then applied the dictionary in machine translation of the rest of the thesaurus. Our results show an overall improvement in machine tra"
P06-1119,P02-1040,0,0.0751386,"rily phrase by phrase/word by word translation. Our evaluation scores below will partially reflect the simplicity of our system. Our system is simple by design. Any improvement or degradation to the input of our system has direct influence on the output. Thus, measures of translation accuracy for our system can be directly interpreted as quality measures for the lexical resources used and the process by which they were developed. 4 Evaluation We performed two different types of evaluation to validate our process. First, we compared our system output to human reference translations using Bleu (Papineni, et al., 2002), a widelyaccepted objective metric for evaluation of machine translations. Second, we showed corrected and uncorrected machine translations to Czech speakers and collected subjective judgments of fluency and accuracy. For evaluation purposes, we selected 418 keyword phrases to be used as target translations. These phrases were selected using a stratified sampling technique so that different levels of thesaurus value would be represented. There was no overlap between these keyword phrases and the 3000 prioritized keyword phrases used to build our lexicon. Prior to machine translation we obtain"
P06-1119,P99-1067,0,0.00918922,"have taken a knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domai"
P06-1119,P03-2025,0,0.0476662,"Missing"
P06-1119,C96-2098,0,0.0202396,"knowledgeacquisition approach to collecting multilingual word pairs. For example, Sadat et al. (2003) automatically extracted bilingual word pairs from comparable corpora. This approach is based on the simple assumption that if two words are mutual translations, then their most frequent collocates are likely to be mutual translations as well. However, the approach requires large comparable corpora, the collection of which presents non-trivial challenges. Others have made similar mutual-translation assumptions for lexical acquisition (Echizen-ya, et al., 2005; Kaji & Aizono, 1996; Rapp, 1999; Tanaka & Iwasaki, 1996). Most make use of either parallel corpora or a bilingual dictionary for the task of bilingual term extraction. Echizen-ya, et al. (2005) avoided using a bilingual dictionary, but required a parallel corpus to achieve their goal; whereas Fung (2000) and others have relied on pre-existing bilingual dictionaries. In either case, large bilingual resources of some kind are required. In addition, these approaches focused on the extraction of single-word pairs, not phrasal units. Many recent approaches to dictionary and thesaurus translation are geared toward providing domain-specific thesauri to sp"
P06-2068,N04-1007,1,0.925838,"India show an increase in cooperation? If so, what are the driving factors behind this increase? Evidence for a relationship includes both the means to influence some entity and the motivation for doing so. Eight types of relationships (“spheres of influence”) were noted: financial, movement of goods, family ties, co-location, common interest, and temporal connection. Relationship questions are significantly different from definition questions, which can be paraphrased as “Tell me interesting things about x.” Definition questions have received significant amounts of attention recently, e.g., (Hildebrandt et al., 2004; Prager et al., 2004; Xu et al., 2004; Cui et al., 2005). Research has shown that certain cue phrases serve as strong indicators for nuggets, and thus an approach based on matching surface patterns (e.g., appositives, parenthetical expressions) works quite well. Unfortunately, such techniques do not generalize to relationship questions because their answers are not usually captured by patterns or marked by surface cues. Unlike answers to factoid questions, answers to relationship questions consist of an unsorted set of passages. For assessing system output, NIST employs the nugget-based evalu"
P06-2068,H05-1117,1,0.868518,"Missing"
P06-2068,N06-1049,1,0.839016,"ed the highest performance in terms of P OURPRE scores, i.e., all but the ten top-ranking documents were discarded. In addition, we built a linear regression model that employed the above features to predict the nugget score of a sentence (the dependent variable). For the training samples, the nugget matching component within P OURPRE was employed to compute the nugget score—this value quantified the “goodness” of a particular sentence in terms of nugget content.1 Due to known issues with the vital/okay distinction (Hildebrandt et al., 2004), it was ignored for this computation; however, see (Lin and Demner-Fushman, 2006b) for recent attempts to address this issue. When presented with a test question, the system ranked all sentences from the top ten retrieved documents using the regression model. Answers were generated by filling a quota of characters, just as in the baseline. Once again, no attempt was made to reduce redundancy. We conducted a five-fold cross validation experiment using all sentences from the top 100 Lucene documents as training samples. After experimenting with different features, we discovered that a regression model with the following performed best: passage match score, document score, a"
P06-2068,N06-1048,0,0.0404438,"Missing"
P06-2068,C04-1100,0,0.0227877,"etc.). Relationship questions are more difficult to process: for one, they are often not phrased as direct wh-questions, but rather as indirect requests for information, statements of doubt, etc. Furthermore, since these complex questions cannot be answered by short noun phrases, existing answer type ontologies are not very useful. For our experiments, we decided to simply use the question verbatim as the query to the IR systems, but undoubtedly performance can be gained by better query formulation strategies. These are difficult challenges, but recent work on applying semantic models to QA (Narayanan and Harabagiu, 2004; Lin and Demner-Fushman, 2006a) provide a promising direction. While our formulation of answering relationship questions as sentence retrieval is productive, it clearly has limitations. The assumption that information nuggets do not span sentence boundaries is false and neglects important work in anaphora resolution and discourse modeling. The current setup of the task, where answers consist of unordered strings, does not place any value on coherence and readability of the responses, which will be important if the answers are intended for human consumption. Clearly, there are ample opportunit"
P06-2068,P04-1073,0,0.0280529,"cooperation? If so, what are the driving factors behind this increase? Evidence for a relationship includes both the means to influence some entity and the motivation for doing so. Eight types of relationships (“spheres of influence”) were noted: financial, movement of goods, family ties, co-location, common interest, and temporal connection. Relationship questions are significantly different from definition questions, which can be paraphrased as “Tell me interesting things about x.” Definition questions have received significant amounts of attention recently, e.g., (Hildebrandt et al., 2004; Prager et al., 2004; Xu et al., 2004; Cui et al., 2005). Research has shown that certain cue phrases serve as strong indicators for nuggets, and thus an approach based on matching surface patterns (e.g., appositives, parenthetical expressions) works quite well. Unfortunately, such techniques do not generalize to relationship questions because their answers are not usually captured by patterns or marked by surface cues. Unlike answers to factoid questions, answers to relationship questions consist of an unsorted set of passages. For assessing system output, NIST employs the nugget-based evaluation methodology ori"
P06-2068,P04-1027,0,0.0414409,"Missing"
P07-1097,N04-1007,1,0.836874,"ted solely on vital nuggets, while nugget precision is approximated by a length allowance based on the number of both vital and okay nuggets returned. In an earlier pilot study, researchers discovered that it was not possible for assessors to consistently enumerate the total set of nuggets contained in an answer, which corresponds to the denominator in a precision calculation (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 2.2 The Pyramid Extension The vital/okay distinction has been identified as a weakness in the TREC nugget-based evaluation methodology (Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005; Lin and DemnerFushman, 2006). There do not appear to be any reliable indicators for predicting nugget importance, which makes it challenging to develop algorithms sensitive to this consideration. Since only vital nuggets affect nugget recall, it is difficult for systems to achieve non-zero scores on topics with few vital nuggets in the answer key. Thus, scores are easily affected by assessor errors and other random variations in evaluation conditions. One direct consequence is that in previous TREC evaluations, the median score for many questions turned out to b"
P07-1097,H91-1061,0,0.127225,"ternative, we propose another method for combining the opinions of multiple assessors: evaluate system responses individually against N sets of binary judgments, and 773 then compute the mean across those scores. We define the macro-averaged binary F-score over a set A = {a1 , ..., aN } of N assessors as: P F= a∈A Fa N Where Fa is the binary F-score according to the vital/okay judgments of assessor a. The differences between the pyramid F-score and the macroaveraged binary F-score correspond to the distinction between micro- and macro-averaging discussed in the context of text classification (Lewis, 1991). In those applications, both measures are meaningful depending on focus: individual instances or entire classes. In tasks where it is important to correctly classify individual instances, microaveraging is more appropriate. In tasks where it is important to correctly identify a class, macroaveraging better quantifies performance. In classification tasks, imbalance in the prevalence of each class can lead to large differences in macro- and micro-averaged scores. Analogizing to our work, the original formulation of nugget pyramids corresponds to micro-averaging (since we focus on individual nug"
P07-1097,H05-1117,1,0.817217,"s, while nugget precision is approximated by a length allowance based on the number of both vital and okay nuggets returned. In an earlier pilot study, researchers discovered that it was not possible for assessors to consistently enumerate the total set of nuggets contained in an answer, which corresponds to the denominator in a precision calculation (Voorhees, 2003). Thus, a penalty for verbosity serves as a surrogate for precision. 2.2 The Pyramid Extension The vital/okay distinction has been identified as a weakness in the TREC nugget-based evaluation methodology (Hildebrandt et al., 2004; Lin and Demner-Fushman, 2005; Lin and DemnerFushman, 2006). There do not appear to be any reliable indicators for predicting nugget importance, which makes it challenging to develop algorithms sensitive to this consideration. Since only vital nuggets affect nugget recall, it is difficult for systems to achieve non-zero scores on topics with few vital nuggets in the answer key. Thus, scores are easily affected by assessor errors and other random variations in evaluation conditions. One direct consequence is that in previous TREC evaluations, the median score for many questions turned out to be zero. A binary distinction o"
P07-1097,N06-1049,1,0.845947,"es University of Maryland College Park, MD 20742 jimmylin@umd.edu on the notion of “information nuggets” to assess answers to complex questions. As it has become the de facto standard for evaluating such systems, the research community stands to benefit from a better understanding of the characteristics of this evaluation methodology. This paper explores recent refinements to the nugget-based evaluation methodology developed by NIST. In particular, we examine the recent so-called “pyramid extension” that incorporates relevance judgments from multiple assessors to improve evaluation stability (Lin and Demner-Fushman, 2006). We organize our discussion as follows: The next section begins by providing a brief overview of nugget-based evaluations and the pyramid extension. Section 3 presents results from the first largescale implementation of nugget pyramids for QA evaluation in TREC 2006. Analysis shows that this extension improves both stability and discriminative power. In Section 4, we discuss an alternative for combining multiple judgments that parallels the distinction between micro- and macro-averaging often seen in classification tasks. Experiments reveal that the methods yield almost exactly the same resul"
P07-1097,H05-1038,0,0.0252343,"ems to achieve non-zero scores on topics with few vital nuggets in the answer key. Thus, scores are easily affected by assessor errors and other random variations in evaluation conditions. One direct consequence is that in previous TREC evaluations, the median score for many questions turned out to be zero. A binary distinction on nugget importance is insufficient to discriminate between the quality of runs that return no vital nuggets but different numbers of okay nuggets. Also, a score distribution heavily skewed towards zero makes meta-analyses of evaluation stability difficult to perform (Voorhees, 2005). The pyramid extension (Lin and DemnerFushman, 2006) was proposed to address the issues mentioned above. The idea was relatively simple: by soliciting vital/okay judgments from multiple assessors (after the list of nuggets has been produced by a primary assessor), it is possible to define nugget importance with greater granularity. Each nugget is assigned a weight between zero and one that is proportional to the number of assessors who judged it to be vital. Nugget recall from Figure 1 can be redefined to incorporate these weights: P wm R = Pm∈A n∈V wn nuggets m and n, respectively.1 The calc"
P13-4034,P96-1041,0,0.0297478,"cted and used as the initial weights for the next iteration; the emitted hypotheses are scored 4 Evaluation We evaluated online learning in Hadoop MapReduce by applying it to German-English machine translation, using our hierarchical phrasebased translation system with cdec as the decoder (Dyer et al., 2010). The parallel training data consist of the Europarl and News Commentary corpora from the WMT12 translation task,6 containing 2.08M sentences. A 5-gram language model was trained on the English side of the bitext along with 750M words of news using SRILM with modified Kneser-Ney smoothing (Chen and Goodman, 1996). We experimented with two feature sets: (1) a small set with standard MT features, including 5 By default, each line is treated as a key-value pair encoded in text, where the key and the value are separated by a &lt;tab&gt;. 6 202 http://www.statmt.org/wmt12/translation-task.html Tuning set size (corpus) (on disk, GB) dev 3.3 5k 7.8 10k 15.2 25k 37.2 50k 74.5 dev 3.3 5k 7.8 10k 15.2 25k 37.2 50k 74.5 Time/iteration (in seconds) 119 289 432 942 1802 232 610 1136 2395 4465 # splits # features Tuning B LEU 120 120 120 300 600 120 120 120 300 600 16 16 16 16 16 85k 159k 200k 200k 200k 22.38 32.60 33.16"
P13-4034,N09-1025,0,0.445855,"ue to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time. This online nature offers several attractive properties, facilitating scaling to large training sets while remaining simple and offering fast convergence. 2 Learning and Inference 2.1 Online Large-Margin Learning MIRA is a popular online large-margin structured learning method for NLP tasks (McDonald et al., 2005; Chiang et al., 2009; Chiang, 2012). The 1 https://github.com/kho/mr-mira 199 Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics, pages 199–204, c Sofia, Bulgaria, August 4-9 2013. 2013 Association for Computational Linguistics main intuition is that we want our model to enforce a margin between the correct and incorrect outputs of a sentence that agrees with our cost function. This is done by making the smallest update we can to our parameters, w, on every sentence, that will ensure that the difference in model scores δfi (y 0 ) = w&gt; (f (xi , y + ) − f (xi , y 0 )) between th"
P13-4034,P10-4002,1,0.895476,"×n i ∗ i aged is defined as w = P ni i . Although averi aging yields different result from running a single learner over the entire data, we have found the difference to be quite small in terms of convergence and quality of tuned weights in practice. After the reducer finishes, the averaged weights are extracted and used as the initial weights for the next iteration; the emitted hypotheses are scored 4 Evaluation We evaluated online learning in Hadoop MapReduce by applying it to German-English machine translation, using our hierarchical phrasebased translation system with cdec as the decoder (Dyer et al., 2010). The parallel training data consist of the Europarl and News Commentary corpora from the WMT12 translation task,6 containing 2.08M sentences. A 5-gram language model was trained on the English side of the bitext along with 750M words of news using SRILM with modified Kneser-Ney smoothing (Chen and Goodman, 1996). We experimented with two feature sets: (1) a small set with standard MT features, including 5 By default, each line is treated as a key-value pair encoded in text, where the key and the value are separated by a &lt;tab&gt;. 6 202 http://www.statmt.org/wmt12/translation-task.html Tuning set"
P13-4034,P13-1031,0,0.046656,"orate `1 /`2 regularization for joint feature selection in order to improve efficiency and counter overfitting effects (Simianer et al., 2012). Furthermore, the PA update has a single learning rate η for all features, which specifies how much the feature weights can change at each update. However, since dense features (e.g., language model) are observed far more frequently than sparse features (e.g., rule id), we may instead want to use a per-feature learning rate that allows larger steps for features that do not have much support. Thus, we allow setting an adaptive per-feature learning rate (Green et al., 2013; Crammer et al., 2009; Duchi et al., 2011). 1 wt+1 = arg min ||w − wt ||2 + Cξi w 2 0 s.t. ∀y ∈ Y(xi ), δfi (y 0 ) ≥ ∆i (y 0 ) − ξi where Y(xi ) is the space of possible structured outputs we are able to produce from xi , and C is a regularization parameter that controls the size of the update. In practice, we can define Y(xi ) to be the k-best output. With a passive-aggressive (PA) update, the ∀y 0 constraint above can be approximated by selecting the single most violated constraint, which maximizes y 0 ← arg maxy∈Y(xi ) w&gt; f (xi , y) + ∆i (y). This optimization problem is attractive because"
P13-4034,W12-3154,0,0.0153843,"in Table 2, in order to test the effectiveness and scalability of our system. First, we used the standard development set from WMT12, consisting of 3,003 sentences from news domain. In order to show the scaling characteristics of our approach, we then used larger portions of the training bitext directly to tune parameters. In order to avoid overfitting, we used a jackknifing method to split the training data into n = 10 folds, and 203 Acknowledgments Therefore, we are actually comparing a smaller indomain tuning set with a larger out-of-domain set. While this domain adaptation is problematic (Haddow and Koehn, 2012), the ability to discriminatively tune on larger sets remains highly desirable. In terms of running time, we observe that the algorithm scales linearly with respect to input size, regardless of the feature set. With more features, running time increases due to a more complex translation model, as well as larger intermediate output (i.e., amount of information passed from mappers to reducers). The scaling characteristics point out the strength of our system: our scalable MIRA implementation allows one to tackle learning problems where there are many parameters, but also many training instances."
P13-4034,W11-2130,0,0.0169494,"that the algorithm scales linearly with respect to input size, regardless of the feature set. With more features, running time increases due to a more complex translation model, as well as larger intermediate output (i.e., amount of information passed from mappers to reducers). The scaling characteristics point out the strength of our system: our scalable MIRA implementation allows one to tackle learning problems where there are many parameters, but also many training instances. Comparing the wall clock time of parallelization with Hadoop to the standard mode of 10–20 learner parallelization (Haddow et al., 2011; Chiang et al., 2009), for the small 25k feature setting, after one iteration, which takes 4625 seconds using 15 learners on our PBS cluster, the tuning score is 19.5 B LEU, while in approximately the same time, we can perform five iterations with Hadoop and obtain 30.98 B LEU. While this is not a completely fair comparison, as the two clusters utilize different resources and the number of learners, it suggests the practical benefits that Hadoop can provide. Although increasing the number of learners on our PBS cluster to the number of mappers used in Hadoop would result in roughly equivalent"
P13-4034,P05-1012,0,0.0685204,"uch as sequence labeling and parsing. 1 Introduction Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP. While batch learning algorithms adapted for structured learning such as CRFs (Lafferty et al., 2001) and structural SVMs (Joachims, 1998) have received much attention, online methods such as the structured perceptron (Collins, 2002) and a family of Passive-Aggressive algorithms (Crammer et al., 2006) have recently gained prominence across many tasks, including part-of-speech tagging (Shen, 2007), parsing (McDonald et al., 2005) and statistical machine translation (SMT) (Chiang, 2012), due to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time. This online nature offers several attractive properties, facilitating scaling to large training sets while remaining simple and offering fast convergence. 2 Learning and Inference 2.1 Online Large-Margin Learning MIRA is a popular online large-margin struc"
P13-4034,P02-1040,0,0.0871422,"lization of MIRA with Hadoop for structured learning. While the current demonstrated application focuses on large-scale discriminative training for machine translation, the learning algorithm is general with respect to the inference algorithm employed. We are able to decouple our learner entirely from the MT decoder, allowing users to specify their own inference procedure through a simple text communication protocol (§2.2). The learner only requires k-best output with feature vectors, as well as the specification of a cost function. Standard automatic evaluation metrics for MT, such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006), have already been implemented. Furthermore, our system can be extended to other structured learning problems with a minimal amount of effort, simply by implementing a task-specific cost function and specifying an appropriate decoder. Through Hadoop streaming, our system can take advantage of commodity clusters to handle large-scale training (§3), while also being capable of running in environments ranging from a single machine to a PBS-managed batch cluster. Experimental results (§4) show that it scales linearly and makes fast parameter tuning on large tuning se"
P13-4034,P07-1096,0,0.02368,"ed learning problems such as sequence labeling and parsing. 1 Introduction Structured learning problems such as sequence labeling or parsing, where the output has a rich internal structure, commonly arise in NLP. While batch learning algorithms adapted for structured learning such as CRFs (Lafferty et al., 2001) and structural SVMs (Joachims, 1998) have received much attention, online methods such as the structured perceptron (Collins, 2002) and a family of Passive-Aggressive algorithms (Crammer et al., 2006) have recently gained prominence across many tasks, including part-of-speech tagging (Shen, 2007), parsing (McDonald et al., 2005) and statistical machine translation (SMT) (Chiang, 2012), due to their ability to deal with large training sets and high-dimensional input representations. Unlike batch learners, which must consider all examples when optimizing the objective, online learners operate in rounds, optimizing using one example or a handful of examples at a time. This online nature offers several attractive properties, facilitating scaling to large training sets while remaining simple and offering fast convergence. 2 Learning and Inference 2.1 Online Large-Margin Learning MIRA is a"
P13-4034,P12-1002,0,0.047525,"odel can produce, to stand in for the correct output in optimization. Our system was developed to handle both cases, with the decoder providing the k-best list to the learner, specifying whether to perform costaugmented selection. Sparse Features While utilizing sparse features is a primary motivation for performing large-scale discriminative training, which features to use and how to learn their weights can have a large impact on the potential benefit. To this end, we incorporate `1 /`2 regularization for joint feature selection in order to improve efficiency and counter overfitting effects (Simianer et al., 2012). Furthermore, the PA update has a single learning rate η for all features, which specifies how much the feature weights can change at each update. However, since dense features (e.g., language model) are observed far more frequently than sparse features (e.g., rule id), we may instead want to use a per-feature learning rate that allows larger steps for features that do not have much support. Thus, we allow setting an adaptive per-feature learning rate (Green et al., 2013; Crammer et al., 2009; Duchi et al., 2011). 1 wt+1 = arg min ||w − wt ||2 + Cξi w 2 0 s.t. ∀y ∈ Y(xi ), δfi (y 0 ) ≥ ∆i (y"
P13-4034,2006.amta-papers.25,0,0.0178616,"structured learning. While the current demonstrated application focuses on large-scale discriminative training for machine translation, the learning algorithm is general with respect to the inference algorithm employed. We are able to decouple our learner entirely from the MT decoder, allowing users to specify their own inference procedure through a simple text communication protocol (§2.2). The learner only requires k-best output with feature vectors, as well as the specification of a cost function. Standard automatic evaluation metrics for MT, such as B LEU (Papineni et al., 2002) and TER (Snover et al., 2006), have already been implemented. Furthermore, our system can be extended to other structured learning problems with a minimal amount of effort, simply by implementing a task-specific cost function and specifying an appropriate decoder. Through Hadoop streaming, our system can take advantage of commodity clusters to handle large-scale training (§3), while also being capable of running in environments ranging from a single machine to a PBS-managed batch cluster. Experimental results (§4) show that it scales linearly and makes fast parameter tuning on large tuning sets for SMT practical. We prese"
P13-4034,P02-1062,0,\N,Missing
Q15-1007,D07-1090,0,0.0910812,"rmance analysis in §4.3 shows that even in grammar extraction there are CPU bottlenecks we need to address and opportunities for further optimization. Beyond grammar extraction, there is a question about whether decoding can be moved to the GPU. Memory is a big hurdle: since accessing data structures off-GPU is costly, it would be preferable to hold all models in GPU memory. We’ve addressed the problem for translation models, but the language models used in machine translation are also large. It might be possible to use lossy compression (Talbot and Osborne, 2007) or batch request strategies (Brants et al., 2007) to solve this problem. If we do, we believe that translation models could be decoded using variants of GPU algorithms for speech (Chong et al., 2009) or parsing (Yi et al., 2011; Canny et al., 2013; Hall et al., 2014), though the latter algorithms exploit properties of latent-variable grammars that may not extend to translation. Thinking beyond decoding, we believe that other problems in computational linguistics might benefit from the massive parallelism offered by GPUs. Acknowledgments This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency,"
Q15-1007,P05-1032,0,0.20276,"rd. We believe that GPU-based extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput. 1 Adam Lopez School of Informatics University of Edinburgh Edinburgh, United Kingdom alopez@inf.ed.ac.uk On-demand extraction can be slow, but for phrase-based models, massive parallelization on general purpose graphics processing units (GPUs) can dramatically accelerate performance. He et al. (2013) demonstrated orders of magnitude speedup in exact pattern matching with suffix arrays, the algorithms at the heart of on-demand extraction (Callison-Burch et al., 2005; Zhang and Vogel, 2005). However, some popular translation models use “gappy” phrases (Chiang, 2007; Simard et al., 2005; Galley and Manning, 2010), and the GPU algorithm of He et al. does not work for these models since it is limited to contiguous phrases. Instead, we need pattern matching and phrase extraction that is able to handle variable-length gaps (Lopez, 2007). Introduction Most machine translation systems extract a large, fixed translation model from parallel text that is accessed from memory or disk. An alternative is to store the indexed parallel text in memory and extract transla"
Q15-1007,D13-1195,0,0.16187,"out whether decoding can be moved to the GPU. Memory is a big hurdle: since accessing data structures off-GPU is costly, it would be preferable to hold all models in GPU memory. We’ve addressed the problem for translation models, but the language models used in machine translation are also large. It might be possible to use lossy compression (Talbot and Osborne, 2007) or batch request strategies (Brants et al., 2007) to solve this problem. If we do, we believe that translation models could be decoded using variants of GPU algorithms for speech (Chong et al., 2009) or parsing (Yi et al., 2011; Canny et al., 2013; Hall et al., 2014), though the latter algorithms exploit properties of latent-variable grammars that may not extend to translation. Thinking beyond decoding, we believe that other problems in computational linguistics might benefit from the massive parallelism offered by GPUs. Acknowledgments This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract No. HR0011-12-C-0015; NSF under award IIS-1218043; and the Human Language Technology Center of Excellence at Johns Hopkins University. Any opinions, findings, conclusions, or recommendation"
Q15-1007,J07-2003,0,0.275671,"applications that demand high throughput. 1 Adam Lopez School of Informatics University of Edinburgh Edinburgh, United Kingdom alopez@inf.ed.ac.uk On-demand extraction can be slow, but for phrase-based models, massive parallelization on general purpose graphics processing units (GPUs) can dramatically accelerate performance. He et al. (2013) demonstrated orders of magnitude speedup in exact pattern matching with suffix arrays, the algorithms at the heart of on-demand extraction (Callison-Burch et al., 2005; Zhang and Vogel, 2005). However, some popular translation models use “gappy” phrases (Chiang, 2007; Simard et al., 2005; Galley and Manning, 2010), and the GPU algorithm of He et al. does not work for these models since it is limited to contiguous phrases. Instead, we need pattern matching and phrase extraction that is able to handle variable-length gaps (Lopez, 2007). Introduction Most machine translation systems extract a large, fixed translation model from parallel text that is accessed from memory or disk. An alternative is to store the indexed parallel text in memory and extract translation units on demand only when they are This paper presents a novel GPU algorithm for on-demand extr"
Q15-1007,P11-2031,0,0.0163268,"d SMT baseline. Phrase tables generated by Moses are essentially the same as the GPU implementation of on-demand extraction for phrase-based translation by He et al. (2013). 4 4.1 Results Translation quality We first verified that our GPU implementation achieves the same translation quality as the corresponding CPU baseline. This is accomplished by comparing system output against the baseline systems, training on Xinhua, tuning on NIST03, and testing on NIST05. In all cases, we used MIRA (Chiang, 2012) to tune parameters. We ran experiments three times and report the average as recommended by Clark et al. (2011). Hierarchical grammars were extracted with sampling at a rate of 300; we also bound source patterns at a length of 5 and matches at a length of 15. For Moses we used default parameters. Our BLEU scores, shown in Table 1, replicate well-known results where hierarchical models outperform pure phrase-based models on this task. The difference in quality is partly because the phrasebased baseline system does not use lexicalized reordering, which provides similar improvements to hierarchical translation (Lopez, 2008b). Such lexicalized reordering models cannot be produced by the GPU-based system of"
Q15-1007,P10-4002,1,0.924694,"action of hierarchical translation models based on matching and extracting gappy phrases. Our experiments examine both grammar extraction and end-to-end translation, comparing quality, speed, and memory use. We compare against the GPU system for phrase-based translation by He et al. (2013) and cdec, a state-of-the-art 87 Transactions of the Association for Computational Linguistics, vol. 3, pp. 87–100, 2015. Action Editor: David Chiang. c Submission batch: 6/2014; Revision batch 12/2014; Published 2/2015. 2015 Association for Computational Linguistics. CPU system for hierarchical translation (Dyer et al., 2010). Our system outperforms the former on translation quality by 2.3 BLEU (replicating previously-known results) and outperforms the latter on speed, improving grammar extraction throughput by at least an order of magnitude on large batches of sentences while maintaining the same level of translation quality. Our contribution is to show, complete with an open-source implementation, how GPUs can vastly increase the speed of hierarchical grammar extraction, particularly for high-throughput MT applications. 2 Algorithms GPU architectures, which are optimized for massively parallel operations on rela"
Q15-1007,E12-1025,0,0.0262498,"Missing"
Q15-1007,P14-1020,0,0.181223,"can be moved to the GPU. Memory is a big hurdle: since accessing data structures off-GPU is costly, it would be preferable to hold all models in GPU memory. We’ve addressed the problem for translation models, but the language models used in machine translation are also large. It might be possible to use lossy compression (Talbot and Osborne, 2007) or batch request strategies (Brants et al., 2007) to solve this problem. If we do, we believe that translation models could be decoded using variants of GPU algorithms for speech (Chong et al., 2009) or parsing (Yi et al., 2011; Canny et al., 2013; Hall et al., 2014), though the latter algorithms exploit properties of latent-variable grammars that may not extend to translation. Thinking beyond decoding, we believe that other problems in computational linguistics might benefit from the massive parallelism offered by GPUs. Acknowledgments This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract No. HR0011-12-C-0015; NSF under award IIS-1218043; and the Human Language Technology Center of Excellence at Johns Hopkins University. Any opinions, findings, conclusions, or recommendations expressed in this"
Q15-1007,N13-1033,1,0.138086,"hput by roughly two thirds on a standard MT evaluation dataset. The GPU necessary to achieve these improvements increases the cost of a server by about a third. We believe that GPU-based extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput. 1 Adam Lopez School of Informatics University of Edinburgh Edinburgh, United Kingdom alopez@inf.ed.ac.uk On-demand extraction can be slow, but for phrase-based models, massive parallelization on general purpose graphics processing units (GPUs) can dramatically accelerate performance. He et al. (2013) demonstrated orders of magnitude speedup in exact pattern matching with suffix arrays, the algorithms at the heart of on-demand extraction (Callison-Burch et al., 2005; Zhang and Vogel, 2005). However, some popular translation models use “gappy” phrases (Chiang, 2007; Simard et al., 2005; Galley and Manning, 2010), and the GPU algorithm of He et al. does not work for these models since it is limited to contiguous phrases. Instead, we need pattern matching and phrase extraction that is able to handle variable-length gaps (Lopez, 2007). Introduction Most machine translation systems extract a la"
Q15-1007,N10-1062,0,0.0326637,"Missing"
Q15-1007,D07-1104,1,0.77137,"d 15 to a separate thread. The thread assigned position 2 scans T for matches of him until the end of sentence at position 9, finding matches (2, 4) and (2, 8). 90 As a second example, consider it ? and. In this case, it has four matches, but and only two. So, we need only two threads, each scanning backwards from matches of and. Since most patterns are infrequent, allocating threads this way minimizes work. However, very large corpora contain one-gap patterns for which both subpatterns are frequent. We simply precompute all matches for these patterns and retrieve them at runtime, as in Lopez (2007). This precomputation is performed once given T and therefore it is a one-time cost. Materializing every match of u ? v would consume substantial memory, so we only emit those for which a translation of the substring matching ? is extractable using the check in §2.3. The success of this check is a prerequisite for extracting the translation of u ? v or any pattern containing it, so pruning in this way conserves GPU memory without affecting the final grammar. Passes 5-7: Finding two-gap patterns (New) We next find all patterns with two gaps of the form u ? v ? w. The search is similar to passes"
Q15-1007,C08-1064,1,0.546959,"r algorithms that use these scans as subroutines, we parallelize the scans themselves in a fine-grained manner to obtain high throughput. The relatively small size of the GPU memory also affects design decisions. Data transfer between the GPU and the CPU has high latency, so we want to avoid shuffling data as much as possible. To accomplish this, we must fit all our data structures into the 5 GB memory available on our particular GPU. As we will show, this requires some tradeoffs in addition to careful design of algorithms and associated data structures. 88 2.1 Translation by Pattern Matching Lopez (2008b) provides a recipe for “translation by pattern matching” that we use as a guide for the remainder of this paper (Algorithm 1). Algorithm 1 Translation by pattern matching 1: for each input sentence do 2: for each phrase in the sentence do 3: Find its occurrences in the source text 4: for each occurrence do 5: Extract any aligned target phrase 6: for each extracted phrase pair do 7: Compute feature values 8: Decode as usual using the scored rules We encounter a computational bottleneck in lines 2–7, since there are many query phrases, matching occurrences, and extracted phrase pairs to proces"
Q15-1007,W99-0604,0,0.214195,"umerated in pass 5, we scan T from position j + |v |+ 1 for matches of w until we reach the end of sentence. As with the one-gap patterns, we apply the extraction check on the second ? of the two-gap patterns u ? v ? w to avoid needlessly materializing matches that will not yield translations. 2.3 Extracting Every Target Phrase In line 5 of Algorithm 1 we must extract the aligned translation of every match of every pattern found in T . Efficiency is crucial since some patterns may occur hundreds of thousands of times. We extract translations from word alignments using the consistency check of Och et al. (1999). A pair of substrings is consistent only if no word in either substring is aligned to any word outside the pair. For example, in Figure 2 the pair (it sets him on, los excita) is consistent. The pair (him on and, los excita y) is not, because excita also aligns to the words it sets. Only consistent pairs can be paraliza los y excita los # it sets him on and it takes him off L0 R0 L 9 10 11 12 13 14 15 16 17 18 2 2 1 2 3 5 5 4 5 R 7 2 2 1 2 3 5 5 4 5 store the sentence-relative positions of the leftmost and rightmost words it aligns to in T 0 , and P [i] stores T [i]’s sentence-relative positi"
Q15-1007,P12-1002,0,0.0126572,"e University of Maryland College Park, Maryland huah@cs.umd.edu Jimmy Lin The iSchool and UMIACS University of Maryland College Park, Maryland jimmylin@umd.edu Abstract needed to decode new input. This architecture has several advantages: It requires only a few gigabytes to represent a model that would otherwise require a terabyte (Lopez, 2008b). It can adapt incrementally to new training data (Levenberg et al., 2010), making it useful for interactive translation (Gonz´alezRubio et al., 2012). It supports rule extraction that is sensitive to the input sentence, enabling leave-oneout training (Simianer et al., 2012) and the use of sentence similarity features (Philips, 2012). Grammars for machine translation can be materialized on demand by finding source phrases in an indexed parallel corpus and extracting their translations. This approach is limited in practical applications by the computational expense of online lookup and extraction. For phrase-based models, recent work has shown that on-demand grammar extraction can be greatly accelerated by parallelization on general purpose graphics processing units (GPUs), but these algorithms do not work for hierarchical models, which require matching patterns t"
Q15-1007,P07-1065,0,0.0251396,"tion pipeline are amenable to GPU algorithms. The performance analysis in §4.3 shows that even in grammar extraction there are CPU bottlenecks we need to address and opportunities for further optimization. Beyond grammar extraction, there is a question about whether decoding can be moved to the GPU. Memory is a big hurdle: since accessing data structures off-GPU is costly, it would be preferable to hold all models in GPU memory. We’ve addressed the problem for translation models, but the language models used in machine translation are also large. It might be possible to use lossy compression (Talbot and Osborne, 2007) or batch request strategies (Brants et al., 2007) to solve this problem. If we do, we believe that translation models could be decoded using variants of GPU algorithms for speech (Chong et al., 2009) or parsing (Yi et al., 2011; Canny et al., 2013; Hall et al., 2014), though the latter algorithms exploit properties of latent-variable grammars that may not extend to translation. Thinking beyond decoding, we believe that other problems in computational linguistics might benefit from the massive parallelism offered by GPUs. Acknowledgments This research was supported in part by the BOLT program"
Q15-1007,W11-2921,0,0.199351,"is a question about whether decoding can be moved to the GPU. Memory is a big hurdle: since accessing data structures off-GPU is costly, it would be preferable to hold all models in GPU memory. We’ve addressed the problem for translation models, but the language models used in machine translation are also large. It might be possible to use lossy compression (Talbot and Osborne, 2007) or batch request strategies (Brants et al., 2007) to solve this problem. If we do, we believe that translation models could be decoded using variants of GPU algorithms for speech (Chong et al., 2009) or parsing (Yi et al., 2011; Canny et al., 2013; Hall et al., 2014), though the latter algorithms exploit properties of latent-variable grammars that may not extend to translation. Thinking beyond decoding, we believe that other problems in computational linguistics might benefit from the massive parallelism offered by GPUs. Acknowledgments This research was supported in part by the BOLT program of the Defense Advanced Research Projects Agency, Contract No. HR0011-12-C-0015; NSF under award IIS-1218043; and the Human Language Technology Center of Excellence at Johns Hopkins University. Any opinions, findings, conclusion"
Q15-1007,2005.eamt-1.39,0,0.0348882,"extraction of hierarchical grammars is an attractive proposition, particularly for MT applications that demand high throughput. 1 Adam Lopez School of Informatics University of Edinburgh Edinburgh, United Kingdom alopez@inf.ed.ac.uk On-demand extraction can be slow, but for phrase-based models, massive parallelization on general purpose graphics processing units (GPUs) can dramatically accelerate performance. He et al. (2013) demonstrated orders of magnitude speedup in exact pattern matching with suffix arrays, the algorithms at the heart of on-demand extraction (Callison-Burch et al., 2005; Zhang and Vogel, 2005). However, some popular translation models use “gappy” phrases (Chiang, 2007; Simard et al., 2005; Galley and Manning, 2010), and the GPU algorithm of He et al. does not work for these models since it is limited to contiguous phrases. Instead, we need pattern matching and phrase extraction that is able to handle variable-length gaps (Lopez, 2007). Introduction Most machine translation systems extract a large, fixed translation model from parallel text that is accessed from memory or disk. An alternative is to store the indexed parallel text in memory and extract translation units on demand onl"
Q15-1007,N10-1140,0,\N,Missing
Q15-1007,H05-1095,0,\N,Missing
Q15-1007,P07-2045,0,\N,Missing
S16-1170,S12-1051,0,0.0526721,"ning entry in STS2015 when evaluated on the STS2015 data. 1 Introduction Measuring the semantic textual similarity (STS) of two pieces of text remains a fundamental problem in language research. It lies at the core of many language processing tasks, including paraphrase detection (Xu et al., 2014), question answering (Lin, 2007), and query ranking (Duh, 2009). The STS problem can be formalized as: given a query sentence S1 and a comparison sentence S2 , the task is to compute their semantic similarity in terms of a similarity score sim(S1 , S2 ). The SemEval Semantic Textual Similarity tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016) are a popular evaluation venue for the STS problem. Over the years the competitions have made more than 15, 000 human annotated sentence pairs publicly available, and have evaluated over 300 system runs. Traditional approaches are based on hand-crafted feature engineering (Wan et al., 2006; Madnani et In this paper, we extend the multi-perspective convolutional neural network (MPCNN) of He et al. (2015). Most previous neural network models, including the MPCNN, treat input sentences separately, and largely ig"
S16-1170,S13-1004,0,0.0254217,"when evaluated on the STS2015 data. 1 Introduction Measuring the semantic textual similarity (STS) of two pieces of text remains a fundamental problem in language research. It lies at the core of many language processing tasks, including paraphrase detection (Xu et al., 2014), question answering (Lin, 2007), and query ranking (Duh, 2009). The STS problem can be formalized as: given a query sentence S1 and a comparison sentence S2 , the task is to compute their semantic similarity in terms of a similarity score sim(S1 , S2 ). The SemEval Semantic Textual Similarity tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016) are a popular evaluation venue for the STS problem. Over the years the competitions have made more than 15, 000 human annotated sentence pairs publicly available, and have evaluated over 300 system runs. Traditional approaches are based on hand-crafted feature engineering (Wan et al., 2006; Madnani et In this paper, we extend the multi-perspective convolutional neural network (MPCNN) of He et al. (2015). Most previous neural network models, including the MPCNN, treat input sentences separately, and largely ignore context-sensitiv"
S16-1170,S14-2010,0,0.0264574,"e STS2015 data. 1 Introduction Measuring the semantic textual similarity (STS) of two pieces of text remains a fundamental problem in language research. It lies at the core of many language processing tasks, including paraphrase detection (Xu et al., 2014), question answering (Lin, 2007), and query ranking (Duh, 2009). The STS problem can be formalized as: given a query sentence S1 and a comparison sentence S2 , the task is to compute their semantic similarity in terms of a similarity score sim(S1 , S2 ). The SemEval Semantic Textual Similarity tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016) are a popular evaluation venue for the STS problem. Over the years the competitions have made more than 15, 000 human annotated sentence pairs publicly available, and have evaluated over 300 system runs. Traditional approaches are based on hand-crafted feature engineering (Wan et al., 2006; Madnani et In this paper, we extend the multi-perspective convolutional neural network (MPCNN) of He et al. (2015). Most previous neural network models, including the MPCNN, treat input sentences separately, and largely ignore context-sensitive interactions betwee"
S16-1170,S16-1081,0,0.0107772,"wieting,kgimpel}@ttic.edu, jimmylin@uwaterloo.ca Abstract al., 2012; Fellbaum, 1998; Fern and Stevenson, 2008; Das and Smith, 2009; Guo and Diab, 2012; Sultan et al., 2014; Kashyap et al., 2014; Lynum et al., 2014). Competitive systems in recent years are mostly based on neural networks (He et al., 2015; Tai et al., 2015; Yin and Sch¨utze, 2015; He and Lin, 2016), which can alleviate data sparseness with pretraining and distributed representations. We describe an attention-based convolutional neural network for the English semantic textual similarity (STS) task in the SemEval2016 competition (Agirre et al., 2016). We develop an attention-based input interaction layer and incorporate it into our multiperspective convolutional neural network (He et al., 2015), using the PARAGRAM - PHRASE word embeddings (Wieting et al., 2016) trained on paraphrase pairs. Without using any sparse features, our final model outperforms the winning entry in STS2015 when evaluated on the STS2015 data. 1 Introduction Measuring the semantic textual similarity (STS) of two pieces of text remains a fundamental problem in language research. It lies at the core of many language processing tasks, including paraphrase detection (Xu"
S16-1170,P09-1053,0,0.0150928,"Missing"
S16-1170,N13-1092,0,0.0663307,"Missing"
S16-1170,P12-1091,0,0.0208662,"Missing"
S16-1170,N16-1108,1,0.380292,"Gimpel2 , Jinfeng Rao1 , and Jimmy Lin3 Department of Computer Science, University of Maryland, College Park 2 Toyota Technological Institute at Chicago 3 David R. Cheriton School of Computer Science, University of Waterloo 1 {huah,jinfeng}@umd.edu, {jwieting,kgimpel}@ttic.edu, jimmylin@uwaterloo.ca Abstract al., 2012; Fellbaum, 1998; Fern and Stevenson, 2008; Das and Smith, 2009; Guo and Diab, 2012; Sultan et al., 2014; Kashyap et al., 2014; Lynum et al., 2014). Competitive systems in recent years are mostly based on neural networks (He et al., 2015; Tai et al., 2015; Yin and Sch¨utze, 2015; He and Lin, 2016), which can alleviate data sparseness with pretraining and distributed representations. We describe an attention-based convolutional neural network for the English semantic textual similarity (STS) task in the SemEval2016 competition (Agirre et al., 2016). We develop an attention-based input interaction layer and incorporate it into our multiperspective convolutional neural network (He et al., 2015), using the PARAGRAM - PHRASE word embeddings (Wieting et al., 2016) trained on paraphrase pairs. Without using any sparse features, our final model outperforms the winning entry in STS2015 when eva"
S16-1170,D15-1181,1,0.645682,"ual Similarity Measurement Hua He1 , John Wieting2 , Kevin Gimpel2 , Jinfeng Rao1 , and Jimmy Lin3 Department of Computer Science, University of Maryland, College Park 2 Toyota Technological Institute at Chicago 3 David R. Cheriton School of Computer Science, University of Waterloo 1 {huah,jinfeng}@umd.edu, {jwieting,kgimpel}@ttic.edu, jimmylin@uwaterloo.ca Abstract al., 2012; Fellbaum, 1998; Fern and Stevenson, 2008; Das and Smith, 2009; Guo and Diab, 2012; Sultan et al., 2014; Kashyap et al., 2014; Lynum et al., 2014). Competitive systems in recent years are mostly based on neural networks (He et al., 2015; Tai et al., 2015; Yin and Sch¨utze, 2015; He and Lin, 2016), which can alleviate data sparseness with pretraining and distributed representations. We describe an attention-based convolutional neural network for the English semantic textual similarity (STS) task in the SemEval2016 competition (Agirre et al., 2016). We develop an attention-based input interaction layer and incorporate it into our multiperspective convolutional neural network (He et al., 2015), using the PARAGRAM - PHRASE word embeddings (Wieting et al., 2016) trained on paraphrase pairs. Without using any sparse features, our"
S16-1170,S14-2072,0,0.0341736,"Missing"
S16-1170,S14-2078,0,0.0273443,"Missing"
S16-1170,N12-1019,0,0.0509841,"Missing"
S16-1170,P14-5010,0,0.00200919,"s yield sentence embeddings (via simple averaging) that perform well across STS tasks without task-specific tuning. Their performance is thought to be due in part to how the vectors for less important words have smaller norms than those for information-bearing words. 5 1st run 0.6607 0.7946 0.8154 0.8094 0.6187 0.7420 Pearson’s r 0.8040 0.7948 0.7622 0.7721 0.8015 Table 3: Ablation study on STS2015 test data. Experiments and Results Datasets. The test data of the SemEval-2016 English STS competition consists of five datasets from different domains. We tokenize all data using Stanford CoreNLP (Manning et al., 2014). Each pair has a similarity score ∈ [0, 5] which increases with similarity. We use training data from previous STS competitions (2012 to 2015). Table 1 provides a brief description. Experimental Settings. We largely follow the same experimental settings as He et al. (2015), e.g., we perform optimization with stochastic gradient descent using a fixed learning rate of 0.01. We use the 300-dimensional PARAGRAM - PHRASE XXL word embeddings (d = 300). Results on STS2016. We provide results of three runs in Table 2. The three runs are from the same system, but with models of different training epoc"
S16-1170,D14-1162,0,0.105816,"Missing"
S16-1170,D15-1044,0,0.0285488,"i words, each with a d-dimensional word embedding vector. S i [a] denotes the embedding vector of the a-th word in S i . We then define an attention matrix D ∈ R`0 ×`1 . Entry (a, b) in the matrix D represents the pairwise word similarity score between the a-th word embedding of S 0 and the b-th word embedding of S 1 . The similarity score uses cosine distance: X attenEmb i [a] = concat(S i [a], Ai [a] S i [a]) where represents element-wise multiplication. Our input interaction layer is inspired by recent work that incorporates attention mechanisms into neural networks (Bahdanau et al., 2014; Rush et al., 2015; Yin et al., 2015; Rockt¨aschel et al., 2016). Many of these add parameters and computational complexity to the model. However, our attentionbased input layer is simpler and more efficient. Moreover, we do not introduce any additional parameters, as we simply use cosine distance to create the attention weights. Nevertheless, adding this attention layer improves performance, as we show in Section 5. 4 Word Embeddings We compare several types of word embeddings to represent the initial sentence matrices (S i ). We use the PARAGRAM - SL 999 embeddings from Wieting et al. (2015) and the PARAGRAM"
S16-1170,S12-1060,0,0.112547,"Missing"
S16-1170,S14-2039,0,0.0124857,"the model identify important input words for improved similarity measurement. We also use the strongly-performing PARAGRAM - PHRASE word embeddings (Wieting et al., 2016) (Sec. 4) trained on phrase pairs from the Paraphrase Database (Ganitkevitch et al., 2013). These components comprise our submission to the SemEval-2016 STS competition (shown in Figure 1): an attention-based multi-perspective convolutional neural network augmented with PARAGRAM - PHRASE word embeddings. We provide details of each component in the following sections. Unlike much previous work in the SemEval ˇ c et al., 2012; Sultan et al., 2014), competitions (Sari´ we do not use sparse features, syntactic parsers, or external resources like WordNet. 1103 Proceedings of SemEval-2016, pages 1103–1108, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics from individual dimensions, while holistic filters can discover broader patterns of contextual information. We use both kinds of filters for a richer representation of the input. Output: Similarity Score Structured Similarity Measurement Layer Multi-Perspective Sentence Model Attention-Based Input Interaction Layer Cats Sit On The Mat bc bc bc Parag"
S16-1170,S15-2027,0,0.0490206,"rules of the STS2015 competition (Agirre et al., 2015). We remove or replace one component at a time from the full system and perform re-training and re-testing. 2 For in-domain evaluation, LSTMs outperformed averaging. 1106 We observe a significant drop when the attentionbased input interaction layer (Sec. 3) is removed. We also find that the PARAGRAM - PHRASE word embeddings are highly beneficial, outperforming both GloVe word embeddings (Pennington et al., 2014) and the PARAGRAM - SL 999 embeddings of Wieting et al. (2015). Our full system performs favorably compared to the winning system (Sultan et al., 2015) at the STS2015 SemEval competition. 6 Conclusion Our submission to the SemEval-2016 STS competition uses our multi-perspective convolutional neural network model as the base model. We develop an attention-based input interaction layer to guide the convolutional neural network to focus on the most important input words. We further improve performance by using the PARAGRAM - PHRASE word embeddings, yielding a result on the 2015 test data that surpasses that of the top system from STS2015. Acknowledgments This work was supported by NSF awards IIS1218043 and CNS-1405688. The views expressed here"
S16-1170,P15-1150,0,0.00822716,"asurement Hua He1 , John Wieting2 , Kevin Gimpel2 , Jinfeng Rao1 , and Jimmy Lin3 Department of Computer Science, University of Maryland, College Park 2 Toyota Technological Institute at Chicago 3 David R. Cheriton School of Computer Science, University of Waterloo 1 {huah,jinfeng}@umd.edu, {jwieting,kgimpel}@ttic.edu, jimmylin@uwaterloo.ca Abstract al., 2012; Fellbaum, 1998; Fern and Stevenson, 2008; Das and Smith, 2009; Guo and Diab, 2012; Sultan et al., 2014; Kashyap et al., 2014; Lynum et al., 2014). Competitive systems in recent years are mostly based on neural networks (He et al., 2015; Tai et al., 2015; Yin and Sch¨utze, 2015; He and Lin, 2016), which can alleviate data sparseness with pretraining and distributed representations. We describe an attention-based convolutional neural network for the English semantic textual similarity (STS) task in the SemEval2016 competition (Agirre et al., 2016). We develop an attention-based input interaction layer and incorporate it into our multiperspective convolutional neural network (He et al., 2015), using the PARAGRAM - PHRASE word embeddings (Wieting et al., 2016) trained on paraphrase pairs. Without using any sparse features, our final model outper"
S16-1170,U06-1019,0,0.0144564,"can be formalized as: given a query sentence S1 and a comparison sentence S2 , the task is to compute their semantic similarity in terms of a similarity score sim(S1 , S2 ). The SemEval Semantic Textual Similarity tasks (Agirre et al., 2012; Agirre et al., 2013; Agirre et al., 2014; Agirre et al., 2015; Agirre et al., 2016) are a popular evaluation venue for the STS problem. Over the years the competitions have made more than 15, 000 human annotated sentence pairs publicly available, and have evaluated over 300 system runs. Traditional approaches are based on hand-crafted feature engineering (Wan et al., 2006; Madnani et In this paper, we extend the multi-perspective convolutional neural network (MPCNN) of He et al. (2015). Most previous neural network models, including the MPCNN, treat input sentences separately, and largely ignore context-sensitive interactions between the input sentences. We address this problem by utilizing an attention mechanism (Bahdanau et al., 2014) to develop an attention-based input interaction layer (Sec. 3). It converts the two independent input sentences into an inter-related sentence pair, which can help the model identify important input words for improved similarit"
S16-1170,Q15-1025,1,0.582861,"ahdanau et al., 2014; Rush et al., 2015; Yin et al., 2015; Rockt¨aschel et al., 2016). Many of these add parameters and computational complexity to the model. However, our attentionbased input layer is simpler and more efficient. Moreover, we do not introduce any additional parameters, as we simply use cosine distance to create the attention weights. Nevertheless, adding this attention layer improves performance, as we show in Section 5. 4 Word Embeddings We compare several types of word embeddings to represent the initial sentence matrices (S i ). We use the PARAGRAM - SL 999 embeddings from Wieting et al. (2015) and the PARAGRAM - PHRASE embeddings from Wieting et al. (2016). These were both constructed from the Paraphrase Database (PPDB) (Ganitkevitch et al., 2013) by training on noisy paraphrase pairs using a hinge-based loss with negative sampling. However, they were trained on two different types of data. The PARAGRAM - SL 999 embeddings were trained on the lexical section of PPDB, which consists of word pairs only. The PARAGRAM - PHRASE embeddings were trained on the phrasal section of PPDB, which consists of phrase pairs. The representations for the phrases were created by simply averaging word"
S16-1170,S15-2045,0,\N,Missing
S16-1170,Q14-1034,0,\N,Missing
S16-1170,N15-1091,0,\N,Missing
W00-1107,M93-1016,0,0.244561,"me manner as single words. The treatment of these representational structures using a restrictive bag-of-words paradigm limits the type of queries that may be formulated. For example, treating adjective/noun pairs ([adj., noun]) as lexical atoms renders it impossible to find the equivalent of ""all big things,"" corresponding to the pair [big, *]. The extraction of these relations from documents has been relatively inefficient and unsystematic. One approach is to first parse the document using a full-text parser, and then extract interesting relations from the resulting parse tree (Fagan, 1987; Grishman and Sterling, 1993; Loper, 2000). This approach is slow and inefficient because full-text parsing is very time-intensive. Due to current limitations of computational tech69 nology, only a small fraction of the information gathered by a full parser can be efficiently indexed. For the most part, relations that can be effectively utilized for information retrieval purposes only occupy a few nodes of a (possibly dense) parse tree; thus, most of the knowledge gathered by the parser is thrown away. Also, extracting non-linguistic relations from parse trees is very difficult; many interesting relations (from an IR poi"
W00-1107,M95-1014,0,0.0380996,"guage. Empirically, the effectiveness of the finitestate language model has been demonstrated in the Message UnderstandingConferences (MUCs), which evaluated information extraction (IE) systerns on a variety of domain-specific tasks. The conferences have shown that superficial parsing using finite-state grammars performs better than deep parsing using context-free grammars (at least under the current constraints of technology). The NYU team switched over from a system that performed full parsing (PROTEUS) in MUC-5 (Grishman and Sterling, 1993) to a regular expression matching parser in MUC-6 (Grishman, 1995). Full parsing was slow and error-prone, and the process of building a full syntactic analysis involved relatively unconstrained search which consumed large amounts of both time and space. The longer debug-cycles that resulted from this translated into fewer iterations with which to tune the system within a given amount of time. Furthermore, the complexity of a full context-free grammar contributed to maintenance problems; complex interactions within the grammar prevented rapid updating of the system to handle new constructions. Finite-state grammars have been used to extract entities such as"
W00-1107,H92-1022,0,0.00710365,"of legal tokens, which are shown in Table 1. In addition, token modifiers (also in Table 1) can alter the meaning of the immediately preceding token. Tokens surrounded by curly braces ({}) are saved as bound variables, which can be later utilized to build relations (ternary expressions). These variables are referenced numerically starting at zero (e.g., the 0th bound variable). 5.2 5 The REXTOR System Using its finite-state language model, the REXTOR System generates a set of ternary expressions that correspond to content of a part-of-speechtagged input document. Currently, the Brill Tagger (Brill, 1992) (with minor postprocessing) is used for the part-of-speech (POS) tagging. The relations construction process consists of two distinct processes, each guided by its own externally specified grammar file. Extraction rules are applied to match arbitrary patterns of text, based either on one of thirty-nine POS tags or on exact words. Whenever an item is extracted, a corresponding relation rule is triggered, which handles the actual generation of the ternary expressions (relations). 4In fact, our first implementation of a ternary expressions indexer used a SQL database. 71 Extraction Rules Relatio"
W00-1107,P97-1004,0,0.0176624,"e reversible syntactic/semantic transformational rules that render explicit the relationship between alternate realizations of the same meaning. For example, a buy expression is semantically equivalent to a sell expression, except the subject and indirect objects are exchanged. Because many verbs can undergo the same alternations, they can in fact be grouped into verb classes, and hence governed by the same S-rules. Thus, S-rules can be viewed as metarules applied over ternary expressions. A similar technique for handling both syntactic and semantic variations can be found in (Grishman, 1995; Jacquemin et al., 1997). Both utilize metarules (e.g., for passive/active transformation) applied over textual patterns in order to generate and handle variations. 74 Below we present a concrete example of how R E X T O R could potentially improve the performance of existing keyword search engines dramatically. W e indexed an electronic version of the Worldbook Encyclopedia at the sentence level using the following two techniques: 1. A simple inverted keyword index. All stopwords are thrown out, and all content words are stemmed. Retrieval was performed b y matching content words in the query with content words in t"
W00-1107,J93-4006,0,0.105463,"Missing"
W00-1107,C88-1065,1,0.0833455,", and thus is unable to equate two sentences that have the same meaning, e.g., (7) and (7'). Although it is certainly possible to manually encode such semantic knowledge as extraction and relation rules, this solution is far from elegant. A potential solution to this semantic variations problem is to borrow the solution employed by START. A ternary expression representation of natural language mimics its syntactic organization, and hence sentences that differ in surface form but are close in meaning will not map into the same structure. In order to solve this problem, START deploys ""S-rules"" (Katz and Levin, 1988), which are reversible syntactic/semantic transformational rules that render explicit the relationship between alternate realizations of the same meaning. For example, a buy expression is semantically equivalent to a sell expression, except the subject and indirect objects are exchanged. Because many verbs can undergo the same alternations, they can in fact be grouped into verb classes, and hence governed by the same S-rules. Thus, S-rules can be viewed as metarules applied over ternary expressions. A similar technique for handling both syntactic and semantic variations can be found in (Grishm"
W00-1107,P91-1032,0,0.0189405,"rtain English constructions, such as center embedding, cannot be described by any finite-state grammar (Chomsky, 1959a; Chomsky, 1959b). However, Church (1980) demonstrated that the finitestate language model is adequate to describe a performance model of language (i.e.,constrained by memory, attention, and other realistic limitations) that approximates competence (i.e.,language abilityunder optimal conditions without resource constraints). M a n y phenomena that cannot be handled by fiuite-state grammars are awkward from a psycholinguistic point of view, and hence rarely seen. More recently, Pereira and Wright (1991) developed formal methods of approximating context-free grammars with finitestate grammars, s Thus, for practical purposes, computationally simple finite-state grammars can be utilized to adequately model natural language. Empirically, the effectiveness of the finitestate language model has been demonstrated in the Message UnderstandingConferences (MUCs), which evaluated information extraction (IE) systerns on a variety of domain-specific tasks. The conferences have shown that superficial parsing using finite-state grammars performs better than deep parsing using context-free grammars (at leas"
W00-1107,X96-1030,0,0.00921938,"will serve as a stepping stone towards a comprehensive system capable of providing users with ""just the right information"" to queries posed in natural language. 3 Previous Work The concept of indexing more than simple keywords is not new; the idea of indexing (parts of) phrases, for example, is more than a decade old (Fagan, 1987). Arampatzis (1998) introduced the phrase retrieval hypothesis, which asserted that phrases are a better indication of document content than keywords. Several researchers have also explored different techniques of linguistic norrealization for information retrieval (Strzalkowski et al., 1996; Zhai et al., 1996; Arampatzis et al., 2000). The performance improvements were neither negligible nor dramatic, but despite the lack of any significant breakthroughs, the authors affirmed the potential value of linguisticallymotivated indexing schemes and the advantages they offer over traditional IR. Previous research in linguistically motivated information retrieval concentrated primarily on noun phrases and their attached prepositional phrases. Techniques that involve head/modifier relations have been tried, e.g., indexing adjective/noun and noun/right adjunct pairs (which normalizes vari"
W00-1107,M91-1028,0,\N,Missing
W01-1009,P93-1016,0,0.0228384,"entire World Wide Web, but is the practical and useful way to apply NL annotation until the transformational rule problem can be solved for unlimited domains. 8 Initial Prototype Webnotator is a prototype test-bed to evaluate the practicality of NL-based annotation and retrieval through Web-based collaboration. It provides eÆcient facilities for retrieving answers already stored within the knowledge base and a scalable framework for ordinary users to contribute knowledge. The system analyzes natural language annotations to produce ternary expressions by postprocessing the results of Minipar (Lin, 1993; Lin, 1994), a fast and robust functional dependency parser that is freely available for non-commercial purposes. The quality of the representational structures depends ultimately on the quality of whatever parser Webnotator is made to access. In the current implementation, ternary expressions are not embedded, elements of ternary expressions are not indexed, and coreference is not detected. Words are stemmed to their root form and morphological information is discarded. The system also implements a version of transformational rules described above as a simple forward-chaining rule-based syst"
W01-1009,C94-1079,0,0.0478415,"ld Wide Web, but is the practical and useful way to apply NL annotation until the transformational rule problem can be solved for unlimited domains. 8 Initial Prototype Webnotator is a prototype test-bed to evaluate the practicality of NL-based annotation and retrieval through Web-based collaboration. It provides eÆcient facilities for retrieving answers already stored within the knowledge base and a scalable framework for ordinary users to contribute knowledge. The system analyzes natural language annotations to produce ternary expressions by postprocessing the results of Minipar (Lin, 1993; Lin, 1994), a fast and robust functional dependency parser that is freely available for non-commercial purposes. The quality of the representational structures depends ultimately on the quality of whatever parser Webnotator is made to access. In the current implementation, ternary expressions are not embedded, elements of ternary expressions are not indexed, and coreference is not detected. Words are stemmed to their root form and morphological information is discarded. The system also implements a version of transformational rules described above as a simple forward-chaining rule-based system. Using a"
W02-1708,C88-1065,1,\N,Missing
W02-1708,W01-1009,1,\N,Missing
W02-1708,lin-2002-web,1,\N,Missing
W03-1608,P97-1004,0,0.0118422,"Missing"
W03-1608,C88-1065,1,0.477704,"over lexical paraphrases, from the capturing of longdistance relationships to the more accurate modeling of context. The paraphrases generated by our approach could prove to be useful in any natural language application where understanding of linguistic variations is important. In particular, we are attempting to apply our results to improve the performance of question answering system, which we will describe in the following section. 6 Paraphrases and Question Answering The ultimate goal of our work on paraphrases is to enable the development of high-precision question answering system (cf. (Katz and Levin, 1988; Soubbotin and Soubbotin, 2001; Hermjakob et al., 2002)). We believe that a knowledge base of paraphrases is the key to overcoming challenges presented by the expressiveness of natural languages. Because the same semantic content can be expressed in many different ways, a question answering system must be able to cope with a variety of alternative phrasings. In particular, an answer stated in a form that differs from the form of the question presents significant problems: When did Colorado become a state? (1a) Colorado became a state in 1876. (1b) Colorado was admitted to the Union in 1876. W"
W03-1608,N01-1009,0,0.052603,"Missing"
W03-1608,P93-1024,0,0.115321,"Missing"
W03-1608,N03-1003,0,0.533607,"Missing"
W03-1608,1993.iwpt-1.22,0,0.0514625,"g claim to it. To test the accuracy of our alignment, we manually aligned 454 sentences from two different versions of Chapter 21 from Twenty Thousand Leagues Under the Sea and compared the results of our automatic alignment algorithm against the manually generated “gold standard.” We obtained a precision of 0.93 and recall of 0.88, which is comparable to the numbers (P.94/R.85) reported by Barzilay and McKeown, who used a different cost function for the alignment process. 3.2 Parsing and Postprocessing The sentence pairs produced by the alignment algorithm are then parsed by the Link Parser (Sleator and Temperly, 1993), a dependency-based parser developed at CMU. The resulting parse structures are post-processed to render the links more consistent: Because the Link Parser does not directly identify the subject of a passive sentence, our postprocessor takes the object of the by-phrase as the subject by default. For our purposes, auxiliary verbs are ignored; the postprocessor connects verbs directly to their subjects, discarding links through any auxiliary verbs. In addition, subjects and objects within relative clauses are appropriately modified so that the linkages remained consistent with subject and objec"
W03-1608,P01-1008,0,0.710223,"ntactic trees that are roughly semantically equivalent, from aligned monolingual corpora. The structural paraphrases produced by our algorithm are similar to the S-rules advocated by Katz and Levin for question answering (1988), except that our paraphrases are automatically generated. Because there is disagreement regarding the exact definition of paraphrases (Dras, 1999), we employ that operating definition that structural paraphrases are roughly interchangeable within the specific configuration of syntactic structures that they specify. Our approach is a synthesis of techniques developed by Barzilay and McKeown (2001) and Lin and Pantel (2001), designed to overcome the limitations of both. In addition to the evaluation of paraphrases generated by our method, we also describe a novel information retrieval system under development that is designed to take advantage of structural paraphrases. 2 Previous Work There has been a rich body of research on automatically deriving paraphrases, including equating morphological and syntactic variants of technical terms (Jacquemin et al., 1997), and identifying equivalent adjective-noun phrases (Lapata, 2001). Unfortunately, both are limited in types of paraphrases that"
W03-1608,P91-1023,0,0.025627,"Missing"
W03-1608,J93-1004,0,\N,Missing
W04-2614,P98-1013,0,0.00987283,"an language faculty. In machine translation, a “good” representation of verbs can straightforwardly capture cross-linguistic divergences in the expression of arguments. In question answering, lexical semantics can be leveraged to bridge the gap between the way a question is asked and the way an answer is stated. This paper explores fine-grained lexical semantic representations—approaches that view a verb as more than a simple predicate of its arguments (e.g., Dang et al., 2000). This contrasts with recent semantic annotation projects such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998). For example, while it is undeniable that throw(John, the ball, Mary), is a valid representation for the sentence “John threw the ball to Mary”, it is widely believed (at least by theoretical linguists) that decomposing verbs in terms of more basic primitives can better capture generalizations about verb meaning and argument realization. I will argue that finer-grained semantics is not only theoretically motivated, but necessary for building applications. I first provide a brief overview of theories of verbal argument structure, and then contrast the typology of Mandarin verbs with that of En"
W04-2614,C00-2148,0,0.181178,"pose facets of meaning relevant to the surface realization of sentential elements, and reveal insights about the organization of the human language faculty. In machine translation, a “good” representation of verbs can straightforwardly capture cross-linguistic divergences in the expression of arguments. In question answering, lexical semantics can be leveraged to bridge the gap between the way a question is asked and the way an answer is stated. This paper explores fine-grained lexical semantic representations—approaches that view a verb as more than a simple predicate of its arguments (e.g., Dang et al., 2000). This contrasts with recent semantic annotation projects such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998). For example, while it is undeniable that throw(John, the ball, Mary), is a valid representation for the sentence “John threw the ball to Mary”, it is widely believed (at least by theoretical linguists) that decomposing verbs in terms of more basic primitives can better capture generalizations about verb meaning and argument realization. I will argue that finer-grained semantics is not only theoretically motivated, but necessary for building applications. I"
W04-2614,kingsbury-palmer-2002-treebank,0,0.0178752,"insights about the organization of the human language faculty. In machine translation, a “good” representation of verbs can straightforwardly capture cross-linguistic divergences in the expression of arguments. In question answering, lexical semantics can be leveraged to bridge the gap between the way a question is asked and the way an answer is stated. This paper explores fine-grained lexical semantic representations—approaches that view a verb as more than a simple predicate of its arguments (e.g., Dang et al., 2000). This contrasts with recent semantic annotation projects such as PropBank (Kingsbury and Palmer, 2002) and FrameNet (Baker et al., 1998). For example, while it is undeniable that throw(John, the ball, Mary), is a valid representation for the sentence “John threw the ball to Mary”, it is widely believed (at least by theoretical linguists) that decomposing verbs in terms of more basic primitives can better capture generalizations about verb meaning and argument realization. I will argue that finer-grained semantics is not only theoretically motivated, but necessary for building applications. I first provide a brief overview of theories of verbal argument structure, and then contrast the typology"
W04-2614,N04-2004,1,0.800467,"verbal categories in Mandarin. Non-causative change of state predicates (achievements) are derived from states with the addition of the particle le. Accomplishments are further derived from achievements through the formation of resultative verb compounds in which the first verb denotes an activity, and the second verb the end state. Traditionally, the particle le that appears post-verbally has been analyzed as an aspectual marker denoting perfectivity (Li and Thompson, 1981). This contrasts with my analysis of it as a signal of inchoativity. How are these two approaches to be reconciled? In (Lin, 2004b), I argue that le is a reflex, rather than an overt realization of the underlying inchoative marker. As generally defined, perfective aspect is not compatible with stative predicates. However, the addition of a covert inchoative functional head, in effect, licenses the perfective aspect. 5 Computational Significance? Why is this peculiar organization of the Mandarin verbal system important for lexical semantic representations designed for language applications? It demonstrates that, at least for languages such as Mandarin Chinese, the verb phrase must be rich in internal structure; a verb ca"
W04-2614,J91-4003,0,0.252839,"es. Recognizing the drawbacks of theories based purely on semantic roles, there is now a general consensus among linguists that argument structure is (to a large extent) predictable from event semantics—hence, patterns of argument realization should be inferable from lexical semantic representations grounded in a theory of events. These event representations typically decompose seman1 see (Dowty, 1991) and (Levin and Rappaport Hovav, 1996) tic roles in terms of primitive predicates representing concepts such as causality, agentivity, inchoativity, and stativity (Dowty, 1979; Jackendoff, 1983; Pustejovsky, 1991b; Rappaport Hovav and Levin, 1998). 3 From Event Types to Event Structure Although Aristotle (Metaphysics 1048b) observed that the meanings of some verbs involve an “end” or a “result”, and other do not, it wasn’t until the twentieth century that philosophers and linguists developed a classification of event types which captures logical entailments and the co-occurrence restrictions between verbs and other syntactic elements such as tenses and adverbials. Vendler’s (1957) classification of events into states, activities, accomplishments, and achievements is groundbreaking in this respect. In"
W04-2614,C98-1013,0,\N,Missing
W05-0906,P04-1027,0,0.296669,"Missing"
W05-0906,P98-1013,0,0.00791267,"text, remains more elusive for a variety of reasons. Finer-grained linguistic analysis at a large scale and sufficiently-rich domain ontologies to support potentially long inference chains are necessary prerequisites—both of which represent open research problems. Furthermore, it is unclear how exactly one would operationalize the evaluation of such capabilities. Nevertheless, we believe that advanced reasoning capabilities based on detailed semantic analyses of text will receive much attention in the future. The recent flurry of work on semantic analysis, based on resources such as FrameNet (Baker et al., 1998) and PropBank (Kingsbury et al., 2002), provide the substrate for reasoning engines. Developments in the automatic construction, adaptation, and merging of ontologies will supply the knowledge necessary to draw inferences. In order to jump-start the knowledge acquisition process, we envision the development of domain-specific question answering systems, the lessons from which will be applied to systems that operate on broader domains. In terms of operationalizing evaluations for these advanced capabilities, the field has already made important first steps, e.g., the Pascal Recognising Textual"
W05-0906,J96-2004,0,0.0343101,"mate average human performance. Fortunately, it appears researchers have realized that “model averaging” may not be the best way to capture the existence of many “equally good” summaries. As an example, the Pyramid Method (Nenkova and Passonneau, 2004), 46 represents a good first attempt at a realistic model of human variations. Second, the view that variations in judgment are an inescapable part of extrinsic evaluations would lead one to conclude that low inter-annotator agreement isn’t necessarily bad. Computational linguistics research generally attaches great value to high kappa measures (Carletta, 1996), which indicate high human agreement on a particular task. Low agreement is seen as a barrier to conducting reproducible research and to drawing generalizable conclusions. However, this is not necessarily true—low agreement in information retrieval has not been a handicap for advancing the state of the art. When dealing with notions such as relevance, low kappa values can most likely be attributed to the nature of the task itself. Attempting to raise agreement by, for example, developing rigid assessment guidelines, may do more harm than good. Prescriptive attempts to define what a good answe"
W05-0906,N04-1019,0,0.361259,"anks. This measure will hopefully spur progress in definition question answering systems. The development of automatic evaluation metrics based on n-gram co-occurrence for question answering is an example of successful knowledge transfer from summarization to question answering evaluation. We believe that there exist many more opportunities for future exploration; as an example, there are remarkable similarities between information nuggets in definition question answering and recently-proposed methods for assessing summaries based on fine-grained semantic units (Teufel and van Halteren, 2004; Nenkova and Passonneau, 2004). Another promising direction of research in definition question answering involves applying the Pyramid Method (Nenkova and Passonneau, 2004) to better model the vital/okay nuggets distinction. As it currently stands, the vital/okay dichotomy is troublesome because there is no way to operationalize such a classification scheme within a system; see Hildebrandt et al. (2004) for more discussion. Yet, the effects on score are significant: a system that returns, for example, all the okay nuggets but none of the vital nuggets would receive a score of zero. In truth, the vital/okay distinction is a"
W05-0906,W02-0404,0,0.0273617,"and topical summaries cannot be generated solely using an extractive approach—sentences are at the wrong level of granularity, a source of problems ranging from dangling anaphoric references to verbose subordinate clauses. Only through more detailed linguistic analysis can information from multiple documents be truly synthesized. Already, there are hybrid approaches to multi-document summarization that employ natural language generation techniques (McKeown et al., 1999; Elson, 2004), and researchers have experimented with sentential operations to improve the discourse structure of summaries (Otterbacher et al., 2002). The primary purpose of this paper was to identify similarities between multi-document summarization and complex question answering, pointing out potential synergistic opportunities in the area of system evaluation. We hope that this is merely a small part of a sustained dialogue between researchers from these two largely independent communities. Answering complex questions and summarizing multiple documents are essentially opposite sides of the same coin, as they represent different approaches to the common problem of addressing complex user information needs. 6 Acknowledgements We would lik"
W05-0906,P02-1040,0,0.0723663,"bility to conduct evaluations with quick turnaround has lead to rapid progress in the state of the art. Question answering for definition questions appears to be missing this critical ingredient. To address this evaluation gap, we have recently developed P OURPRE, a method for automatically evaluating definition questions based on idfweighted unigram co-occurrences (Lin and DemnerFushman, 2005). This idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the B LEU metric for machine translation (Papineni et al., 2002) and the ROUGE (Lin and Hovy, 2003) metric for summarization. Note that metrics for automatically evaluating definitions should be, like metrics for evaluating summaries, biased towards recall. Fluency (i.e., precision) is not usually of concern because most systems employ extractive techniques to produce answers. Our study reports good correlation between the automatically computed P OURPRE metric and official TREC system ranks. This measure will hopefully spur progress in definition question answering systems. The development of automatic evaluation metrics based on n-gram co-occurrence for"
W05-0906,N04-1007,1,0.624213,". Complementary developments in the summarization community mirror the aforementioned shifts in question answering research. Most notably, the DUC 2005 task requires systems to generate answers to natural language questions based on a collection of known relevant documents: “The system task in 2005 will be to synthesize from a set of 25– 50 documents a brief, well-organized, fluent answer to a need for information that cannot be met by just stating a name, date, quantity, etc.” (DUC 2005 guidelines). These guidelines were modeled after the information synthesis task suggested by Amig´o et al. (2004), which they characterize as “the process of (given a complex information need) extracting, organizing, and inter-relating the pieces of information contained in a set of relevant documents, in order to obtain a comprehensive, non-redundant report that satisfies the information need”. One of the examples they provide, “I’m looking for information concerning the history of text compression both before and with computers”, looks remarkably like a user information need current question answering systems aspire to satisfy. The idea of topic-oriented multi-document summarization isn’t new (Goldstei"
W05-0906,W04-3254,0,0.0255838,"Missing"
W05-0906,H05-1117,1,0.796337,"Missing"
W05-0906,N03-1020,0,0.0734058,"ck turnaround has lead to rapid progress in the state of the art. Question answering for definition questions appears to be missing this critical ingredient. To address this evaluation gap, we have recently developed P OURPRE, a method for automatically evaluating definition questions based on idfweighted unigram co-occurrences (Lin and DemnerFushman, 2005). This idea of employing n-gram co-occurrence statistics to score the output of a computer system against one or more desired reference outputs has its roots in the B LEU metric for machine translation (Papineni et al., 2002) and the ROUGE (Lin and Hovy, 2003) metric for summarization. Note that metrics for automatically evaluating definitions should be, like metrics for evaluating summaries, biased towards recall. Fluency (i.e., precision) is not usually of concern because most systems employ extractive techniques to produce answers. Our study reports good correlation between the automatically computed P OURPRE metric and official TREC system ranks. This measure will hopefully spur progress in definition question answering systems. The development of automatic evaluation metrics based on n-gram co-occurrence for question answering is an example of"
W05-0906,E99-1011,0,\N,Missing
W05-0906,C98-1013,0,\N,Missing
W05-0906,N03-1034,0,\N,Missing
W06-0704,P02-1005,0,0.0235641,"Missing"
W06-0704,C04-1100,0,0.0145455,"within a broader context, i.e., a “scenario”. These complex questions set forth parameters of the desired knowledge, which may include additional facts about the motivation of the information seeker, her assumptions, her current state of knowledge, etc. Presently, most systems that attempt to tackle such complex questions are aimed at serving intelligence analysts, for activities such as counterterrorism and war-fighting. Systems for addressing complex information needs are interesting because they provide an opportunity to explore the role of semantic structures in question answering, e.g., (Narayanan and Harabagiu, 2004). Opportunities include explicit semantic representations for capturing the content of questions and documents, deep inferential mechanisms (Moldovan et al., 2002), and attempts to model task-specific influences in informationseeking environments (Freund et al., 2005). Our own interest in question answering falls in line with these recent developments, but we focus on a different type of user—the primary care physician. The need to answer questions related to patient care at the point of service has been well studied and documented (Gorman et al., 1994; Ely et al., 1999; Ely et al., 2005). How"
W06-0704,W04-0509,0,0.0204964,"based medicine is not new. Many researchers have studied MeSH terms associated with basic clinical tasks (Mendonc¸a and Cimino, 2001; Haynes et al., 1994). Although originally developed as a tool to assist in query formulation, Booth (2000) pointed out that PICO frames can be employed to structure IR results for improving precision; PICObased querying is merely an instance of faceted querying, which has been widely used by librarians since the invention of automated retrieval systems. The feasibility of automatically identifying outcome statements in secondary sources has been demonstrated by Niu and Hirst (2004), but our work differs in its focus on the primary medical literature. Approaching clinical needs from a different perspective, the PERSIVAL system leverages patient records to rerank search results (McKeown et al., 2003). Since the primary focus is on personQuestion answering in the clinical domain is an emerging area of research that has only recently begun to receive serious attention. As a result, there exist relatively few points of comparison to our own work, as the research space is sparsely populated. Discussion and Related Work Recently, researchers have become interested in restricte"
W06-3309,N04-1015,0,0.577447,"erages this knowledge— MetaMap (Aronson, 2001) for concept identification and SemRep (Rindflesch and Fiszman, 2003) for relation extraction—provide a foundation for studying the role of semantics in various tasks. McKnight and Srinivasan (2003) have previously examined the task of categorizing sentences in medical abstracts using supervised discriminative machine learning techniques. Building on the work of Ruch et al. (2003) in the same domain, we present a generative approach that attempts to directly model the discourse structure of MEDLINE abstracts using Hidden Markov Models (HMMs); cf. (Barzilay and Lee, 2004). Although our results were not obtained from the same exact collection as those used by authors of these two previous studies, comparable experiments suggest that our techniques are competitive in terms of performance, and may offer additional advantages as well. Discriminative approaches (especially SVMs) have been shown to be very effective for many supervised classification tasks; see, for example, (Joachims, 1998; Ng and Jordan, 2001). However, their high computational complexity (quadratic in the number of training samples) renders them prohibitive for massive data processing. Under cert"
W06-3309,P02-1047,0,0.0315097,"y a supervised approach. Whereas Barzilay and Lee evaluated their work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985; Marcu and Echihabi, 2002). Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts. 6 Conclusion We believe that there are two contributions as a result of our work. From the perspective of machine learning, the assignment of sequentially-occurring labels represents an underexplored problem with respect to the generative vs. discriminative debate— previous work has mostly focused on stateless classification tasks. This paper demonstrates that Hidden Markov Models are capable of capturing discourse transitions from section to section, and are"
W06-3309,W00-1302,0,0.0187171,"heir work in the context of document summarization, the fourpart structure of medical abstracts allows us to conduct meaningful intrinsic evaluations and focus on the sentence classification task. Nevertheless, their work bolsters our claims regarding the usefulness of generative models in extrinsic tasks, which we do not describe here. Although this study falls under the general topic of discourse modeling, our work differs from previous attempts to characterize text in terms of domainindependent rhetorical elements (McKeown, 1985; Marcu and Echihabi, 2002). Our task is closer to the work of Teufel and Moens (2000), who looked at the problem of intellectual attribution in scientific texts. 6 Conclusion We believe that there are two contributions as a result of our work. From the perspective of machine learning, the assignment of sequentially-occurring labels represents an underexplored problem with respect to the generative vs. discriminative debate— previous work has mostly focused on stateless classification tasks. This paper demonstrates that Hidden Markov Models are capable of capturing discourse transitions from section to section, and are at least competitive with Support Vector Machines from a pu"
W07-0904,1992.tmi-1.9,0,0.0288438,"Missing"
W08-0207,D07-1090,0,0.0789787,"of data. Banko and Brill (2001) were among the first to demonstrate the importance of dataset size as a significant factor governing prediction accuracy in a supervised machine learning task. In fact, they argue that size of training set is perhaps more important than the choice of machine learning algorithm itself. Similarly, experiments in question answering have shown the effectiveness of simple pattern-matching techniques when applied to large quantities of data (Brill et al., 2001). More recently, this line of argumentation has been echoed in experiments with large-scale language models. Brants et al. (2007) show that for statistical machine translation, a simple smoothing method (dubbed Stupid Backoff) approaches the quality of Kneser-Ney Smoothing as the amount of training data increases, and with the simple method one can process significantly more data. Given these observations, it is important to integrate discussions of large-data issues into any course on human language technology. Most existing courses focus on smaller-sized problems and datasets that can be processed on students’ personal computers, making them ill-prepared to cope with the vast quantities of data in operational environm"
W08-0207,W08-0333,1,0.821352,"ctly or indirectly from this project. • Placement of students, e.g., internships and permanent positions, or admission to graduate programs (for undergraduates). • Number of projects with sustained research activities after the conclusion of the course. • Amount of additional research support from other funding agencies (NSF, DARPA, etc.) for which the projects provided preliminary results. Here I provide an interim assessment, as this paper goes to press in mid-April. Preliminary results from the projects have already yielded two separate publications: one on statistical machine translation (Dyer et al., 2008), the other on information retrieval (Elsayed et al., 2008). In terms of student placement, I believe that experience from this course has made several students highly attractive to companies such as Google, Yahoo, and Amazon—both for permanent positions and summer internships. It is far too early to have measurable results with respect to the final two criteria, but otherwise preliminary assessment appears to support the overall success of this course. In addition to the above discussion, it is also worth mentioning that the course is emerging as a nexus of cloud computing on the Maryland cam"
W08-0207,P08-2067,1,0.8269,"Missing"
W08-0207,P01-1005,0,\N,Missing
W08-0333,D07-1090,0,0.0879585,"ty hardware. We describe MapReduce implementations of two algorithms used to estimate the parameters for two word alignment models and one phrase-based translation model, all of which rely on maximum likelihood probability estimates. On a 20-machine cluster, experimental results show that our solutions exhibit good scaling characteristics compared to a hypothetical, optimally-parallelized version of current state-of-the-art single-core tools. 1 Introduction Like many other NLP problems, output quality of statistical machine translation (SMT) systems increases with the amount of training data. Brants et al. (2007) demonstrated that increasing the quantity of training data used for language modeling significantly improves the translation quality of an ArabicEnglish MT system, even with far less sophisticated backoff models. However, the steadily increasing quantities of training data do not come without cost. Figure 1 shows the relationship between the amount of parallel Arabic-English training data used and both the translation quality of a state-ofthe-art phrase-based SMT system and the time required to perform the training with the widely-used Moses toolkit on a commodity server.1 Building a model us"
W08-0333,J93-2003,0,0.0481858,"Missing"
W08-0333,P05-1032,0,0.0267713,"2 and HMM training using a “home-grown” parallelization scheme (Deng and Byrne, 2006). However, the tool relies on a cluster where all nodes have access to the same shared networked file storage, a restriction that MapReduce does not impose. There has been a fair amount of work inspired by the problems of long latencies and excessive space requirements in the construction of phrase-based and hierarchical phrase-based translation models. Several authors have advocated indexing the training data with a suffix array and computing the necessary statistics during or immediately prior to decoding (Callison-Burch et al., 2005; Lopez, 2007). Although this technique works quite well, the standard channel probability P (f |e) cannot be computed, which is not a limitation of MapReduce.10 7 Conclusions We have shown that an important class of modelbuilding algorithms in statistical machine translation can be straightforwardly recast into the MapReduce framework, yielding a distributed solution that is cost-effective, scalable, robust, and exact (i.e., doesn’t resort to approximations). Alternative strategies for parallelizing these algorithms either impose significant demands on the developer, the hardware infrastructu"
W08-0333,N06-4004,0,0.0116113,"pproach suggested by Das et al. (2007), who show that a distributed database running in tandem with MapReduce can be used to provide the parameters for very large mixture models efficiently. Moreover, since the database is distributed across the same nodes as the MapReduce jobs, many of the same data locality benefits that Wolfe et al. (2007) sought to capitalize on will be available without abandoning the guarantees of the MapReduce paradigm. Although it does not use MapReduce, the MTTK tool suite implements distributed Model 1, 2 and HMM training using a “home-grown” parallelization scheme (Deng and Byrne, 2006). However, the tool relies on a cluster where all nodes have access to the same shared networked file storage, a restriction that MapReduce does not impose. There has been a fair amount of work inspired by the problems of long latencies and excessive space requirements in the construction of phrase-based and hierarchical phrase-based translation models. Several authors have advocated indexing the training data with a suffix array and computing the necessary statistics during or immediately prior to decoding (Callison-Burch et al., 2005; Lopez, 2007). Although this technique works quite well, t"
W08-0333,N03-1017,0,0.0092073,"existing parallel programming models. The MapReduce abstraction shields the programmer from having to explicitly worry about system-level issues such as synchronization, data exchange, and fault tolerance (see Section 2 for details). The runtime is able to transparently distribute computations across large clusters of commodity hardware with good scaling characteristics. This frees the programmer to focus on actual MT issues. In this paper we present MapReduce implementations of training algorithms for two kinds of models commonly used in statistical MT today: a phrasebased translation model (Koehn et al., 2003) and word alignment models based on pairwise lexical translation trained using expectation maximization (Dempster et al., 1977). Currently, such models take days to construct using standard tools with publicly available training corpora; our MapReduce implementation cuts this time to hours. As an benefit to the community, it is our intention to release this code under an open source license. It is worthwhile to emphasize that we present 200 these results as a “sweet spot” in the complex design space of engineering decisions. In light of possible tradeoffs, we argue that our solution can be con"
W08-0333,W08-0207,1,0.806872,"ementation), and cheap (in terms of hardware costs). Faster running times could be achieved with more expensive hardware. Similarly, a custom implementation (e.g., in MPI) could extract finergrained parallelism and also yield faster running times. In our opinion, these are not worthwhile tradeoffs. In the first case, financial constraints are obvious. In the second case, the programmer must explicitly manage all the complexities that come with distributed processing (see above). In contrast, our algorithms were developed within a matter of weeks, as part of a “cloud computing” course project (Lin, 2008). Experimental results demonstrate that MapReduce provides nearly optimal scaling characteristics, while retaining a highlevel problem-focused abstraction. The remainder of the paper is structured as follows. In the next section we provide an overview of MapReduce. In Section 3 we describe several general solutions to computing maximum likelihood estimates for finite, discrete probability distributions. Sections 4 and 5 apply these techniques to estimate phrase translation models and perform EM for two word alignment models. Section 6 reviews relevant prior work, and Section 7 concludes. 2 Map"
W08-0333,D07-1104,0,0.0119919,"home-grown” parallelization scheme (Deng and Byrne, 2006). However, the tool relies on a cluster where all nodes have access to the same shared networked file storage, a restriction that MapReduce does not impose. There has been a fair amount of work inspired by the problems of long latencies and excessive space requirements in the construction of phrase-based and hierarchical phrase-based translation models. Several authors have advocated indexing the training data with a suffix array and computing the necessary statistics during or immediately prior to decoding (Callison-Burch et al., 2005; Lopez, 2007). Although this technique works quite well, the standard channel probability P (f |e) cannot be computed, which is not a limitation of MapReduce.10 7 Conclusions We have shown that an important class of modelbuilding algorithms in statistical machine translation can be straightforwardly recast into the MapReduce framework, yielding a distributed solution that is cost-effective, scalable, robust, and exact (i.e., doesn’t resort to approximations). Alternative strategies for parallelizing these algorithms either impose significant demands on the developer, the hardware infrastructure, or both; o"
W08-0333,C00-2163,0,0.0242238,"se of larger translation units, the task of generating a word alignment, the mapping between the words in the source and target sentences that are translationally equivalent, remains crucial to nearly all approaches to statistical machine translation. The IBM models, together with a Hidden Markov Model (HMM), form a class of generative models that are based on a lexical translation model P (fj |ei ) where each word fj in the foreign sentence f1m is generated by precisely one word ei in the sentence el1 , independently of the other translation decisions (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000). Given these assumptions, we let the sentence translation probability be mediated by a latent alignment variable (am 1 in the equations below) that specifies the pairwise mapping between words in the source and target languages. Assuming a given sentence length m for f1m , the translation probability is defined as follows: P (f1m |el1 ) = X l P (f1m , am 1 |e1 ) am 1 = X l m P (am 1 |e1 , f1 ) am 1 m Y P (fj |eaj ) j=1 Once the model parameters have been estimated, the single-best word alignment is computed according to the following decision rule: a ˆm 1 = l m arg max P (am 1 |e1 , f1 ) am 1"
W08-0333,P02-1038,0,0.00803503,"solution to the problem, while providing an implementation that is both scalable and fault tolerant—in fact, transparently so since the runtime hides all these complexities from the researcher. From the graph it is clear that the overhead associated with the framework itself is quite low, especially for large quantities of data. We concede that it may be possible for a custom solution (e.g., with MPI) to achieve even faster running times, but we argue that devoting resources to developing such a solution would not be cost-effective. Next, we explore a class of models where the stan5 Following Och and Ney (2002), it is customary to combine both these probabilities as feature values in a log-linear model. 6 In our cluster, only 19 machines actually compute, and each has two single-core processors. 203 Word Alignment Although word-based translation models have been largely supplanted by models that make use of larger translation units, the task of generating a word alignment, the mapping between the words in the source and target sentences that are translationally equivalent, remains crucial to nearly all approaches to statistical machine translation. The IBM models, together with a Hidden Markov Model"
W08-0333,J03-1002,0,0.00647585,"the total probability of generating a particular observation to be efficiently computed using dynamic programming.8 The HMM alignment model uses the forward-backward algorithm (Baum et al., 1970), which is also an instance of EM. Even with dynamic programming, this requires O(Slm) operations for Model 1, and O(Slm2 ) for the HMM model, where m and l are the average lengths of the foreign and English sentences in the training corpus, and S is the number of sentences. Figure 6 shows measurements of the average iteration run-time for Model 1 and the HMM alignment model as implemented in Giza++ (Och and Ney, 2003), a state-of-the-art C++ implementation of the IBM and HMM alignment models that is widely used. Five iterations are generally necessary to train the models, so the time to carry out full training of the models is approximately five times the per-iteration run-time. 3 hrs 60 min 20 min 3m20s 90 s 30 s 10 s 3s 10000 100000 1e+06 Corpus size (sentences) Figure 6: Per-iteration average run-times for Giza++ implementations of Model 1 and HMM training on corpora of various sizes. be associated with the given training instance. Reducers aggregate these partial counts to compute the total expected jo"
W08-0333,W99-0604,0,0.054748,"e to extract a consistent phrase corresponding to the foreign string la mesa or the English string the small. models in question: representing P (B|A = a) in the phrase model required at most 90k parameters, and in the lexical model, 128k parameters (i.e., the size of the vocabulary for language B). For the remainder of the experiments reported, we confine ourselves to the use of Method 3. 4 Phrase-Based Translation In phrase-based translation, the translation process is modeled by splitting the source sentence into phrases (a contiguous string of words) and translating the phrases as a unit (Och et al., 1999; Koehn et al., 2003). Phrases are extracted from a wordaligned parallel sentence according to the strategy proposed by Och et al. (1999), where every word in a phrase is aligned only to other words in the phrase, and not to any words outside the phrase bounds. Figure 4 shows an example aligned sentence and some of the consistent subphrases that may be extracted. dard tools work primarily in memory, but where the computational complexity of the models is greater. Moses training time MapReduce training (38 M/R) Optimal (Moses/38) 2 days 5 Time (seconds) 12 hrs 3 hrs 60 min 20 min 5 min 1.5 min"
W08-0333,C96-2141,0,0.0805825,"y models that make use of larger translation units, the task of generating a word alignment, the mapping between the words in the source and target sentences that are translationally equivalent, remains crucial to nearly all approaches to statistical machine translation. The IBM models, together with a Hidden Markov Model (HMM), form a class of generative models that are based on a lexical translation model P (fj |ei ) where each word fj in the foreign sentence f1m is generated by precisely one word ei in the sentence el1 , independently of the other translation decisions (Brown et al., 1993; Vogel et al., 1996; Och and Ney, 2000). Given these assumptions, we let the sentence translation probability be mediated by a latent alignment variable (am 1 in the equations below) that specifies the pairwise mapping between words in the source and target languages. Assuming a given sentence length m for f1m , the translation probability is defined as follows: P (f1m |el1 ) = X l P (f1m , am 1 |e1 ) am 1 = X l m P (am 1 |e1 , f1 ) am 1 m Y P (fj |eaj ) j=1 Once the model parameters have been estimated, the single-best word alignment is computed according to the following decision rule: a ˆm 1 = l m arg max P ("
W13-2214,P07-1019,0,0.0376821,"rule extractor (Lopez, 2007). For German, we used the 3,003 sentences in newstest2011 as our Dev set, and report results on the 3,003 sentences of the newstest2012 Test set using B LEU and TER (Snover et al., 2006). For Russian, we took the first 2,000 sentences of newstest2012 for Dev, and report results on the remaining 1,003. For both languages, we selected 1,000 sentences from the bitext to be used as an additional testing set (Test2). Parameter Settings We tune our system toward approximate sentence-level B LEU (Papineni et al., 2002),3 and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 200 candidates at each node. For optimization, we use a learning rate of η=1, regularization strength of C=0.01, and a 500-best list for hope and fear selection (Chiang, 2012) with a single passive-aggressive update for each sentence (Eidelman, 2012). Compound segmentation lattices As German is a morphologically rich language with productive compounding, we use word segmentation lattices as input for the German translation task. These lattices encode alternative segmentations of compound words, allowing the decoder to automatically choose which segmentation is best. We use a m"
W13-2214,2006.amta-papers.25,0,0.0284509,"ng a weighted average to distribute as initial weights for the next iteration. We constructed a 5-gram language model using SRILM (Stolcke, 2002) from the provided English monolingual training data and parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996), which was binarized using KenLM (Heafield, 2011). The sentence-specific translation grammars were extracted using a suffix array rule extractor (Lopez, 2007). For German, we used the 3,003 sentences in newstest2011 as our Dev set, and report results on the 3,003 sentences of the newstest2012 Test set using B LEU and TER (Snover et al., 2006). For Russian, we took the first 2,000 sentences of newstest2012 for Dev, and report results on the remaining 1,003. For both languages, we selected 1,000 sentences from the bitext to be used as an additional testing set (Test2). Parameter Settings We tune our system toward approximate sentence-level B LEU (Papineni et al., 2002),3 and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 200 candidates at each node. For optimization, we use a learning rate of η=1, regularization strength of C=0.01, and a 500-best list for hope and fear selection (Chiang, 2012)"
W13-2214,N03-1017,0,0.00610473,", context-dependent word pairs (for the top 300 word pairs in the training data), and structural distortion (Chiang et al., 2008). For both languages, we used the provided Europarl and News Commentary parallel training data to create the translation grammar necessary for our model. For Russian, we additionally used the Common Crawl and Yandex data. The data were lowercased and tokenized, then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized sing the grow-diag-final-and method (Koehn et al., 2003). 3 We approximate corpus B LEU by scoring sentences using a pseudo-document of previous 1-best translations (Chiang et al., 2009). 129 en de Dev 75k 74k Test 74k 73k Test2 27k 26k 5k 132k 133k 10k 255k 256k 25k 634k 639k 50k 1258k 1272k ru en 16 108k 16 77k Tune ↑B LEU 22.38 23.86 30.18 32.40 Test ↑B LEU 22.69 23.01 29.89 30.81 Test2 24k 25k 15k 350k 371k 24 ↓TER 60.61 59.89 49.05 48.40 λ=0.01 23.8 23.6 λ=0.1 23.4 BLEU de-en +sparse ru-en +sparse # features Test 24k 27k Table 2: Corpus statistics in tokens for Russian. Table 1: Corpus statistics in tokens for German. Set Dev 46k 50k 23.2 23 2"
W13-2214,D07-1104,0,0.0170173,"ights and decoding and updating parameters on a shard of the data. This is followed by a reduce phase, with a single reducer collecting final weights from all mappers and computing a weighted average to distribute as initial weights for the next iteration. We constructed a 5-gram language model using SRILM (Stolcke, 2002) from the provided English monolingual training data and parallel data with modified Kneser-Ney smoothing (Chen and Goodman, 1996), which was binarized using KenLM (Heafield, 2011). The sentence-specific translation grammars were extracted using a suffix array rule extractor (Lopez, 2007). For German, we used the 3,003 sentences in newstest2011 as our Dev set, and report results on the 3,003 sentences of the newstest2012 Test set using B LEU and TER (Snover et al., 2006). For Russian, we took the first 2,000 sentences of newstest2012 for Dev, and report results on the remaining 1,003. For both languages, we selected 1,000 sentences from the bitext to be used as an additional testing set (Test2). Parameter Settings We tune our system toward approximate sentence-level B LEU (Papineni et al., 2002),3 and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with"
W13-2214,J03-1002,0,0.0058822,"and nonterminals), target bigrams, lexical insertions and deletions (for the top 150 unaligned words from the training data), context-dependent word pairs (for the top 300 word pairs in the training data), and structural distortion (Chiang et al., 2008). For both languages, we used the provided Europarl and News Commentary parallel training data to create the translation grammar necessary for our model. For Russian, we additionally used the Common Crawl and Yandex data. The data were lowercased and tokenized, then filtered for length and aligned using the GIZA++ implementation of IBM Model 4 (Och and Ney, 2003) to obtain one-to-many alignments in both directions and symmetrized sing the grow-diag-final-and method (Koehn et al., 2003). 3 We approximate corpus B LEU by scoring sentences using a pseudo-document of previous 1-best translations (Chiang et al., 2009). 129 en de Dev 75k 74k Test 74k 73k Test2 27k 26k 5k 132k 133k 10k 255k 256k 25k 634k 639k 50k 1258k 1272k ru en 16 108k 16 77k Tune ↑B LEU 22.38 23.86 30.18 32.40 Test ↑B LEU 22.69 23.01 29.89 30.81 Test2 24k 25k 15k 350k 371k 24 ↓TER 60.61 59.89 49.05 48.40 λ=0.01 23.8 23.6 λ=0.1 23.4 BLEU de-en +sparse ru-en +sparse # features Test 24k 27k"
W13-2214,P03-1021,0,0.0277435,"e and target words, passing an untranslated source word to the target side, singleton rule and source side, as well as counts for arity-0,1, or 2 SCFG rules, the total number of rules used, and the number of times the glue rule is used. 2.1 3 Evaluation This section describes the experiments we conducted in moving towards a better understanding of the benefits and challenges posed by large-scale high-dimensional discriminative tuning. 3.1 Data preparation Sparse Features The ability to incorporate sparse features is the primary reason for the recent move away from Minimum Error Rate Training (Och, 2003), as well as for performing large-scale discriminative training. We include the following sparse Boolean feature templates in our system in addition to the aforementioned baseline features: rule identity (for every unique rule in the grammar), rule shape (mapping rules to sequences of terminals and nonterminals), target bigrams, lexical insertions and deletions (for the top 150 unaligned words from the training data), context-dependent word pairs (for the top 300 word pairs in the training data), and structural distortion (Chiang et al., 2008). For both languages, we used the provided Europarl"
W13-2214,P02-1040,0,0.0990699,"e sentence-specific translation grammars were extracted using a suffix array rule extractor (Lopez, 2007). For German, we used the 3,003 sentences in newstest2011 as our Dev set, and report results on the 3,003 sentences of the newstest2012 Test set using B LEU and TER (Snover et al., 2006). For Russian, we took the first 2,000 sentences of newstest2012 for Dev, and report results on the remaining 1,003. For both languages, we selected 1,000 sentences from the bitext to be used as an additional testing set (Test2). Parameter Settings We tune our system toward approximate sentence-level B LEU (Papineni et al., 2002),3 and the decoder is configured to use cube pruning (Huang and Chiang, 2007) with a limit of 200 candidates at each node. For optimization, we use a learning rate of η=1, regularization strength of C=0.01, and a 500-best list for hope and fear selection (Chiang, 2012) with a single passive-aggressive update for each sentence (Eidelman, 2012). Compound segmentation lattices As German is a morphologically rich language with productive compounding, we use word segmentation lattices as input for the German translation task. These lattices encode alternative segmentations of compound words, allowi"
W13-2214,P12-1002,0,0.0477022,"to allow arbitrarily scaling the tuning set and utilizing a sparse feature set. We report our findings on German-English and RussianEnglish translation, and discuss benefits, as well as obstacles, to tuning on larger development sets drawn from the parallel training data. 1 Baseline system Introduction The adoption of discriminative learning methods for SMT that scale easily to handle sparse and lexicalized features has been increasing in the last several years (Chiang, 2012; Hopkins and May, 2011). However, relatively few systems take full advantage of the opportunity. With some exceptions (Simianer et al., 2012), most still rely on tuning a handful of common dense features, along with at most a few thousand others, on a relatively small development set (Cherry and Foster, 2012; Chiang et al., 2009). While more features tuned on more data usually results in better performance for other NLP tasks, this has not necessarily been the case for SMT. System design To efficiently encode the information that the learner and decoder require (source sentence, reference translation, grammar rules) in a manner amenable to MapReduce, i.e. avoiding dependencies on “side data” and large transfers across the network,"
W13-2214,N13-1025,0,\N,Missing
W13-2214,D08-1024,1,\N,Missing
W13-2214,W12-3160,1,\N,Missing
W13-2214,P13-1031,0,\N,Missing
W13-2214,W11-2123,0,\N,Missing
W13-2214,N09-1025,0,\N,Missing
W13-2214,P10-4002,1,\N,Missing
W13-2214,N12-1047,0,\N,Missing
W13-2214,N09-1046,0,\N,Missing
W13-2214,P13-4034,1,\N,Missing
W13-2214,D11-1125,0,\N,Missing
