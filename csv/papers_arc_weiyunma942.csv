2021.findings-acl.228,{H}-{FND}: Hierarchical False-Negative Denoising for Distant Supervision Relation Extraction,2021,-1,-1,4,0,8064,jhihwei chen,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2020.sigdial-1.1,Semantic Guidance of Dialogue Generation with Reinforcement Learning,2020,-1,-1,2,0,14914,chenghsun hsueh,Proceedings of the 21th Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"Neural encoder-decoder models have shown promising performance for human-computer dialogue systems over the past few years. However, due to the maximum-likelihood objective for the decoder, the generated responses are often universal and safe to the point that they lack meaningful information and are no longer relevant to the post. To address this, in this paper, we propose semantic guidance using reinforcement learning to ensure that the generated responses indeed include the given or predicted semantics and that these semantics do not appear repeatedly in the response. Synsets, which comprise sets of manually defined synonyms, are used as the form of assigned semantics. For a given/assigned/predicted synset, only one of its synonyms should appear in the generated response; this constitutes a simple but effective semantic-control mechanism. We conduct both quantitative and qualitative evaluations, which show that the generated responses are not only higher-quality but also reflect the assigned semantic controls."
2020.lrec-1.235,Headword-Oriented Entity Linking: A Special Entity Linking Task with Dataset and Baseline,2020,-1,-1,5,0,4840,mu yang,Proceedings of the 12th Language Resources and Evaluation Conference,0,"In this paper, we design headword-oriented entity linking (HEL), a specialized entity linking problem in which only the headwords of the entities are to be linked to knowledge bases; mention scopes of the entities do not need to be identified in the problem setting. This special task is motivated by the fact that in many articles referring to specific products, the complete full product names are rarely written; instead, they are often abbreviated to shorter, irregular versions or even just to their headwords, which are usually their product types, such as {``}stick{''} or {``}mask{''} in a cosmetic context. To fully design the special task, we construct a labeled cosmetic corpus as a public benchmark for this problem, and propose a product embedding model to address the task, where each product corresponds to a dense representation to encode the different information on products and their context jointly. Besides, to increase training data, we propose a special transfer learning framework in which distant supervision with heuristic patterns is first utilized, followed by supervised learning using a small amount of manually labeled data. The experimental results show that our model provides a strong benchmark performance on the special task."
2020.lrec-1.365,{CA}-{EHN}: Commonsense Analogy from {E}-{H}ow{N}et,2020,-1,-1,3,1,17407,penghsuan li,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Embedding commonsense knowledge is crucial for end-to-end models to generalize inference beyond training corpora. However, existing word analogy datasets have tended to be handcrafted, involving permutations of hundreds of words with only dozens of pre-defined relations, mostly morphological relations and named entities. In this work, we model commonsense knowledge down to word-level analogical reasoning by leveraging E-HowNet, an ontology that annotates 88K Chinese words with their structured sense definitions and English translations. We present CA-EHN, the first commonsense word analogy dataset containing 90,505 analogies covering 5,656 words and 763 relations. Experiments show that CA-EHN stands out as a great indicator of how well word representations embed commonsense knowledge. The dataset is publicly available at \url{https://github.com/ckiplab/CA-EHN}."
P19-1136,{G}raph{R}el: Modeling Text as Relational Graphs for Joint Entity and Relation Extraction,2019,0,11,3,1,3990,tsujui fu,Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we present GraphRel, an end-to-end relation extraction model which uses graph convolutional networks (GCNs) to jointly learn named entities and relations. In contrast to previous baselines, we consider the interaction between named entities and relations via a 2nd-phase relation-weighted GCN to better extract relations. Linear and dependency structures are both used to extract both sequential and regional features of the text, and a complete word graph is further utilized to extract implicit features among all word pairs of the text. With the graph-based approach, the prediction for overlapping relations is substantially improved over previous sequential approaches. We evaluate GraphRel on two public datasets: NYT and WebNLG. Results show that GraphRel maintains high precision while increasing recall substantially. Also, GraphRel outperforms previous work by 3.2{\%} and 5.8{\%} (F1 score), achieving a new state-of-the-art for relation extraction."
N19-4015,i{C}omposer: An Automatic Songwriting System for {C}hinese Popular Music,2019,0,1,3,0,25966,hsinpei lee,Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations),0,"In this paper, we introduce iComposer, an interactive web-based songwriting system designed to assist human creators by greatly simplifying music production. iComposer automatically creates melodies to accompany any given text. It also enables users to generate a set of lyrics given arbitrary melodies. iComposer is based on three sequence-to-sequence models, which are used to predict melody, rhythm, and lyrics, respectively. Songs generated by iComposer are compared with human-composed and randomly-generated ones in a subjective test, the experimental results of which demonstrate the capability of the proposed system to write pleasing melodies and meaningful lyrics at a level similar to that of humans."
L18-1132,Word Embedding Evaluation Datasets and {W}ikipedia Title Embedding for {C}hinese,2018,0,4,2,0,17080,chiyen chen,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1724,Extended {H}ow{N}et 2.0 {--} An Entity-Relation Common-Sense Representation Model,2018,0,2,1,1,8066,weiyun ma,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
D18-1474,Speed Reading: Learning to Read {F}or{B}ackward via Shuttle,2018,0,5,2,1,3990,tsujui fu,Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing,0,"We present LSTM-Shuttle, which applies human speed reading techniques to natural language processing tasks for accurate and efficient comprehension. In contrast to previous work, LSTM-Shuttle not only reads shuttling forward but also goes back. Shuttling forward enables high efficiency, and going backward gives the model a chance to recover lost information, ensuring better prediction. We evaluate LSTM-Shuttle on sentiment analysis, news classification, and cloze on IMDB, Rotten Tomatoes, AG, and Children{'}s Book Test datasets. We show that LSTM-Shuttle predicts both better and more quickly. To demonstrate how LSTM-Shuttle actually behaves, we also analyze the shuttling operation and present a case study."
I17-4014,{CKIP} at {IJCNLP}-2017 Task 2: Neural Valence-Arousal Prediction for Phrases,2017,0,1,2,1,17407,penghsuan li,"Proceedings of the {IJCNLP} 2017, Shared Tasks",0,"CKIP takes part in solving the Dimensional Sentiment Analysis for Chinese Phrases (DSAP) share task of IJCNLP 2017. This task calls for systems that can predict the valence and the arousal of Chinese phrases, which are real values between 1 and 9. To achieve this, functions mapping Chinese character sequences to real numbers are built by regression techniques. In addition, the CKIP phrase Valence-Arousal (VA) predictor depends on knowledge of modifier words and head words. This includes the types of known modifier words, VA of head words, and distributional semantics of both these words. The predictor took the second place out of 13 teams on phrase VA prediction, with 0.444 MAE and 0.935 PCC on valence, and 0.395 MAE and 0.904 PCC on arousal."
I17-3015,Guess What: A Question Answering Game via On-demand Knowledge Validation,2017,4,0,4,0,32847,yusheng li,"Proceedings of the {IJCNLP} 2017, System Demonstrations",0,"In this paper, we propose an idea of ondemand knowledge validation and fulfill the idea through an interactive Question-Answering (QA) game system, which is named Guess What. An object (e.g. dog) is first randomly chosen by the system, and then a user can repeatedly ask the system questions in natural language to guess what the object is. The system would respond with yes/no along with a confidence score. Some useful hints can also be given if needed. The proposed framework provides a pioneering example of on-demand knowledge validation in dialog environment to address such needs in AI agents/chatbots. Moreover, the released log data that the system gathered can be used to identify the most critical concepts/attributes of an existing knowledge base, which reflects human{'}s cognition about the world."
E17-2082,Integrating Semantic Knowledge into Lexical Embeddings Based on Information Content Measurement,2017,25,4,2,0,32808,hsinyang wang,"Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",0,"Distributional word representations are widely used in NLP tasks. These representations are based on an assumption that words with a similar context tend to have a similar meaning. To improve the quality of the context-based embeddings, many researches have explored how to make full use of existing lexical resources. In this paper, we argue that while we incorporate the prior knowledge with context-based embeddings, words with different occurrences should be treated differently. Therefore, we propose to rely on the measurement of information content to control the degree of applying prior knowledge into context-based embeddings - different words would have different learning rates when adjusting their embeddings. In the result, we demonstrate that our embeddings get significant improvements on two different tasks: Word Similarity and Analogical Reasoning."
D17-1282,Leveraging Linguistic Structures for Named Entity Recognition with Bidirectional Recursive Neural Networks,2017,13,9,5,1,17407,penghsuan li,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we utilize the linguistic structures of texts to improve named entity recognition by BRNN-CNN, a special bidirectional recursive network attached with a convolutional network. Motivated by the observation that named entities are highly related to linguistic constituents, we propose a constituent-based BRNN-CNN for named entity recognition. In contrast to classical sequential labeling methods, the system first identifies which text chunks are possible named entities by whether they are linguistic constituents. Then it classifies these chunks with a constituency tree structure by recursively propagating syntactic and semantic information to each constituent node. This method surpasses current state-of-the-art on OntoNotes 5.0 with automatically generated parses."
O16-3002,N-best Rescoring for Parsing Based on Dependency-Based Word Embeddings,2016,27,0,2,0,34554,yuming hsieh,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 21, Number 2, {D}ecember 2016",0,None
O16-1010,åºæ¼ç¸ä¾è©åéçåæçµæéä¼°èæåº(N-best Parse Rescoring Based on Dependency-Based Word Embeddings),2016,27,0,2,0,34554,yuming hsieh,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
O16-1031,æ§å»ºä¸åä¸­æåå°æ¸å­¸æå­åé¡èªæåº«(Building a Corpus for Developing the {C}hinese Elementary School Math Word Problem Solver)[In {C}hinese],2016,-1,-1,3,0,22606,shenyun miao,Proceedings of the 28th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2016),0,None
O15-3001,Designing a Tag-Based Statistical Math Word Problem Solver with Reasoning and Explanation,2015,45,4,6,0,10585,yichung lin,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 20, Number 2, {D}ecember 2015 - Special Issue on Selected Papers from {ROCLING} {XXVII}",0,"This paper proposes a tag-based statistical framework to solve math word problems with understanding and reasoning. It analyzes the body and question texts into their associated tag-based logic forms, and then performs inference on them. Comparing to those rule-based approaches, the proposed statistical approach alleviates rules coverage and ambiguity resolution problems, and our tag-based approach also provides the flexibility of handling various kinds of related questions with the same body logic form. On the other hand, comparing to those purely statistical approaches, the proposed approach is more robust to the irrelevant information and could more accurately provide the answer. The major contributions of our work are: (1) proposing a tag-based logic representation such that the system is less sensitive to the irrelevant information and could provide answer more precisely; (2) proposing a unified statistical framework for performing reasoning from the given text."
O15-1006,Designing a Tag-Based Statistical Math Word Problem Solver with Reasoning and Explanation,2015,45,4,6,0,34634,chientsung huang,Proceedings of the 27th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2015),0,"This paper proposes a tag-based statistical framework to solve math word problems with understanding and reasoning. It analyzes the body and question texts into their associated tag-based logic forms, and then performs inference on them. Comparing to those rule-based approaches, the proposed statistical approach alleviates rules coverage and ambiguity resolution problems, and our tag-based approach also provides the flexibility of handling various kinds of related questions with the same body logic form. On the other hand, comparing to those purely statistical approaches, the proposed approach is more robust to the irrelevant information and could more accurately provide the answer. The major contributions of our work are: (1) proposing a tag-based logic representation such that the system is less sensitive to the irrelevant information and could provide answer more precisely; (2) proposing a unified statistical framework for performing reasoning from the given text."
D15-1122,System Combination for Machine Translation through Paraphrasing,2015,23,5,1,1,8066,weiyun ma,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"In this paper, we propose a paraphrasing model to address the task of system combination for machine translation. We dynamically learn hierarchical paraphrases from target hypotheses and form a synchronous context-free grammar to guide a series of transformations of target hypotheses into fused translations. The model is able to exploit phrasal and structural system-weighted consensus and also to utilize existing information about word ordering present in the target hypotheses. In addition, to consider a diverse set of plausible fused translations, we develop a hybrid combination architecture, where we paraphrase every target hypothesis using different fusing techniques to obtain fused translations for each target, and then make the final selection among all fused translations. Our experimental results show that our approach can achieve a significant improvement over combination baselines."
N13-1045,Using a Supertagged Dependency Language Model to Select a Good Translation in System Combination,2013,20,3,1,1,8066,weiyun ma,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a novel, structured language model - Supertagged Dependency Language Model to model the syntactic dependencies between words. The goal is to identify ungrammatical hypotheses from a set of candidate translations in a MT system combination framework and help select the best translation candidates using a variety of sentence-level features. We use a two-step mechanism based on constituent parsing and elementary tree extraction to obtain supertags and their dependency relations. Our experiments show that the structured language model provides significant improvement in the framework of sentence-level system combination."
O12-5001,Detecting and Correcting Syntactic Errors in Machine Translation Using Feature-Based {L}exicalized {T}ree {A}djoining {G}rammars,2012,14,5,1,1,8066,weiyun ma,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 17, Number 4, {D}ecember 2012-Special Issue on Selected Papers from {ROCLING} {XXIV}",0,"Statistical machine translation has made tremendous progress over the past ten years. The output of even the best systems, however, is often ungrammatical because of the lack of sufficient linguistic knowledge. Even when systems incorporate syntax in the translation process, syntactic errors still result. To address this issue, we present a novel approach for detecting and correcting ungrammatical translations. In order to simultaneously detect multiple errors and their corresponding words in a formal framework, we use feature-based lexicalized tree adjoining grammars, where each lexical item is associated with a syntactic elementary tree, in which each node is associated with a set of feature-value pairs to define the lexical itemxe2x80x99s syntactic usage. Our syntactic error detection works by checking the feature values of all lexical items within a sentence using a unification framework. In order to simultaneously detect multiple error types and track their corresponding words, we propose a new unification method which allows the unification procedure to continue when unification fails and also to propagate the failure information to relevant words. Once error types and their corresponding words are detected, one is able to correct errors based on a unified consideration of all related words under the same error types. In this paper, we present some simple mechanism to handle part of the detected situations. We use our approach to detect and correct translations of six single statistical machine translation systems. The results show that most of the corrected translations are improved."
O12-1014,Detecting and Correcting Syntactic Errors in Machine Translation Using Feature-Based {L}exicalized {T}ree {A}djoining {G}rammars,2012,14,5,1,1,8066,weiyun ma,Proceedings of the 24th Conference on Computational Linguistics and Speech Processing ({ROCLING} 2012),0,"Statistical machine translation has made tremendous progress over the past ten years. The output of even the best systems, however, is often ungrammatical because of the lack of sufficient linguistic knowledge. Even when systems incorporate syntax in the translation process, syntactic errors still result. To address this issue, we present a novel approach for detecting and correcting ungrammatical translations. In order to simultaneously detect multiple errors and their corresponding words in a formal framework, we use feature-based lexicalized tree adjoining grammars, where each lexical item is associated with a syntactic elementary tree, in which each node is associated with a set of feature-value pairs to define the lexical itemxe2x80x99s syntactic usage. Our syntactic error detection works by checking the feature values of all lexical items within a sentence using a unification framework. In order to simultaneously detect multiple error types and track their corresponding words, we propose a new unification method which allows the unification procedure to continue when unification fails and also to propagate the failure information to relevant words. Once error types and their corresponding words are detected, one is able to correct errors based on a unified consideration of all related words under the same error types. In this paper, we present some simple mechanism to handle part of the detected situations. We use our approach to detect and correct translations of six single statistical machine translation systems. The results show that most of the corrected translations are improved."
2012.amta-papers.11,Phrase-level System Combination for Machine Translation Based on Target-to-Target Decoding,2012,27,4,1,1,8066,weiyun ma,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"In this paper, we propose a novel lattice-based MT combination methodology that we call Target-to-Target Decoding (TTD). The combination process is carried out as a {``}translation{''} from backbone to the combination result. This perspective suggests the use of existing phrase-based MT techniques in the combination framework. We show how phrase extraction rules and confidence estimations inspired from machine translation improve results. We also propose system-specific LMs for estimating N-gram consensus. Our results show that our approach yields a strong improvement over the best single MT system and competes with other state-of-the-art combination systems."
2011.mtsummit-papers.62,System Combination for Machine Translation Based on Text-to-Text Generation,2011,-1,-1,1,1,8066,weiyun ma,Proceedings of Machine Translation Summit XIII: Papers,0,None
P09-2084,Where{'}s the Verb? Correcting Machine Translation During Question Answering,2009,8,13,1,1,8066,weiyun ma,Proceedings of the {ACL}-{IJCNLP} 2009 Conference Short Papers,0,"When a multi-lingual question-answering (QA) system provides an answer that has been incorrectly translated, it is very likely to be regarded as irrelevant. In this paper, we propose a novel method for correcting a deletion error that affects overall understanding of the sentence. Our post-editing technique uses information available at query time: examples drawn from related documents determined to be relevant to the query. Our results show that 4%-7% of MT sentences are missing the main verb and on average, 79% of the modified sentences are judged to be more comprehensible. The QA performance also benefits from the improved MT: 7% of irrelevant response sentences become relevant."
Y06-1027,Knowledge-Rich Approach to Automatic Grammatical Information Acquisition: Enriching {C}hinese {S}ketch {E}ngine with a Lexical Grammar,2006,7,0,2,0,1504,churen huang,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"This paper discusses the implementation of a knowledge-rich approach to automatic acquisition of grammatical information. Our study is based on Word Sketch Engine (Kilgarriff and Tudgell 2002). The original claims of WSE are two folded: that linguistic generalizations can be automatically extracted from a corpus with simple collocation information provided that the corpus is large enough; and that such a methodology is easily adaptable for a new language. Our work on Chinese Sketch Engine attests to the claim the WSE is adaptable for a new language. More critically, we show that the quality of grammatical information provided has a directly bearing on the result of grammatical information acquisition. We show that when provided with a knowledge rich lexical grammar, both the quantity and quality of the extracted knowledge improves substantially over the results with simple PS rules. 1 Background: Word Sketch Engine and Automatic Acquisition of Grammatical Information The original goal of corpus-based studies was to provide xe2x80x98a body of evidencexe2x80x99 for more theoretical linguistic studies (Francis and Kucera 1965). However, corpus-based studies evolved with the improvements made in electronic data manipulation, making of automatic acquisition of grammatical information a goal of computational linguistics, computational lexicography, as well as theoretical corpus linguistics. Previous works that made significant contribution to the study of automatic extraction of grammatical relation includes Sinclairxe2x80x99s (1987) work on KWIC, Church and Hanksxe2x80x99 (1989) introduction of Mutual Information, and Linxe2x80x99s (1998) introduction of relevance measurement. Kilgarriff and colleaguesxe2x80x99 work on Word Sktech Engine (WSE) makes a bold step forwards in automatic linguistic knowledge acquisition (Kilgarriff and Tudgell 2002, Kilgarriff et al. 2004). The main claim is that a xe2x80x98gargantuanxe2x80x99 corpus 1 contains enough distributional information about most grammatical dependencies in a language such that the set of simple collocational patterns will allow automatic extraction of grammatical relations and other grammatical information. Crucially, the validity of the extracted information does not rely on the preciseness of the rules or the perfect grammaticality of the data. Instead, WSE allows the presence of ungrammatical examples in the corpus and the possibility for collocational patterns to occasionally identify the wrong lexical pairs. WSE assumes that these anomalies will be statistically insignificant, especially when there are enough examples instantiating the intended grammatical information. In addition, WSE relies on Salience measurement to rank the significance of all attested relations. Salience is calculated by MI of a relation multiplied with the frequency of the relation, in order to correct MIxe2x80x99s bias towards low frequency items. WSE follows Linxe2x80x99s (1998) formulation of MI of relations, where ||w1, R, w2|| stands for the frequency of the relation R between w1 and, w2. A wild card * can occurs in place of w1, R, or w2 to represent the all cases. Hence MI between w1, and w2 given a relation R is given below (Kilgarriff and Tudgell 2002): 1 The required corpus size was not specified in WSE literature. However, we estimate from existing work that for WSE to be efficient, corpus scale must be 100 millions words or above."
O06-1002,ä¸­æåè©åç©åå¤æ·ççµ±è¨å¼æ¨¡åè¨­è¨ (A Stochastic Model for Prediction of Deverbal Nouns in {M}andarin {C}hinese) [In {C}hinese],2006,0,0,1,1,8066,weiyun ma,Proceedings of the 18th Conference on Computational Linguistics and Speech Processing,0,None
ma-huang-2006-uniform,Uniform and Effective Tagging of a Heterogeneous Giga-word Corpus,2006,8,19,1,1,8066,weiyun ma,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Tagging as the most crucial annotation of language resources can still be challenging when the corpus size is big and when the corpus data is not homogeneous. The Chinese Gigaword Corpus is confounded by both challenges. The corpus containsroughly 1.12 billion Chinese characters from two heterogeneous sources: respective news in Taiwan and in Mainland China. In other words, in addition to its size, the data also contains two variants of Chinese that are known to exhibit substantial linguistic differences. We utilize Chinese Sketch Engine as the corpus query tool, by which grammar behaviours of the two heterogeneous resources could be captured and displayed in a unified web interface. In this paper, we report our answer to the two challenges to effectively tag this large-scale corpus. The evaluation result shows our mechanism of tagging maintains high annotation quality."
W03-1705,A Bottom-up Merging Algorithm for {C}hinese Unknown Word Extraction,2003,22,62,1,1,8066,weiyun ma,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"Statistical methods for extracting Chinese unknown words usually suffer a problem that superfluous character strings with strong statistical associations are extracted as well. To solve this problem, this paper proposes to use a set of general morphological rules to broaden the coverage and on the other hand, the rules are appended with different linguistic and statistical constraints to increase the precision of the representation. To disambiguate rule applications and reduce the complexity of the rule matching, a bottom-up merging algorithm for extraction is proposed, which merges possible morphemes recursively by consulting above the general rules and dynamically decides which rule should be applied first according to the priorities of the rules. Effects of different priority strategies are compared in our experiment, and experimental results show that the performance of proposed method is very promising."
W03-1726,Introduction to {CKIP} {C}hinese Word Segmentation System for the First International {C}hinese Word Segmentation Bakeoff,2003,6,171,1,1,8066,weiyun ma,Proceedings of the Second {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper presents the results from the ACL-SIGHAN-sponsored First International Chinese Word Segmentation Bakeoff held in 2003 and reported in conjunction with the Second SIGHAN Workshop on Chinese Language Processing, Sapporo, Japan. We give the motivation for having an international segmentation contest (given that there have been two within-China contests to date) and we report on the results of this first international contest, analyze these results, and make some recommendations for the future."
C02-1049,Unknown Word Extraction for {C}hinese Documents,2002,17,102,2,0,39226,kehjiann chen,{COLING} 2002: The 19th International Conference on Computational Linguistics,0,"There is no blank to mark word boundaries in Chinese text. As a result, identifying words is difficult, because of segmentation ambiguities and occurrences of unknown words. Conventionally unknown words were extracted by statistical methods because statistical methods are simple and efficient. However the statistical methods without using linguistic knowledge suffer the drawbacks of low precision and low recall, since character strings with statistical significance might be phrases or partial phrases instead of words and low frequency new words are hardly identifiable by statistical methods. In addition to statistical information, we try to use as much information as possible, such as morphology, syntax, semantics, and world knowledge. The identification system fully utilizes the context and content information of unknown words in the steps of detection process, extraction process, and verification process. A practical unknown word extraction system was implemented which online identifies new words, including low frequency new words, with high precision and high recall rates."
O01-1009,ä¸­æèªæåº«æ§å»ºåç®¡çç³»çµ±è¨­è¨ (Design of Management System for {C}hinese Corpus Construction) [In {C}hinese],2001,0,0,1,1,8066,weiyun ma,Proceedings of Research on Computational Linguistics Conference {XIV},0,None
