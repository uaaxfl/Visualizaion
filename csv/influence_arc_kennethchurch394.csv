1992.tmi-1.9,P91-1034,0,0.344464,"Missing"
1992.tmi-1.9,P91-1022,0,0.282503,"Missing"
1992.tmi-1.9,P91-1017,0,0.117581,"Missing"
1992.tmi-1.9,H90-1070,0,0.0202559,"Missing"
1992.tmi-1.9,C92-2070,1,0.770211,"Missing"
2020.emnlp-main.100,D16-1250,0,0.0159106,"a transformation W that minimizes the discrepancy between Xs and Ys , Introduction Bilingual Lexicon Induction (BLI) studies how to generate word-level translations from non-parallel corpora in two languages. Recently, Irvine and Callison-Burch (2017) observe that rarer words are harder to translate than frequent ones. But their BLI method is based on various “hand-crafted” features. We show that the same phenomenon occurs as well in BLI methods that are based on word embeddings. This type of methods have become especially popular in recent years (Mikolov et al., 2013; Faruqui and Dyer, 2014; Artetxe et al., 2016, 2018) and achieved state-of-art accuracies (Conneau et al., 2018). We briefly review BLI methods that are based on word embeddings. Without loss of generality, in this paper, we focus on “supervised” BLI, which assumes that a seeding dictionary is available. Unsupervised BLI (Artetxe et al., 2018; Conneau et al., 2018) often alternates between inducing a seeding dictionary and using that to refine generated translations. Therefore, to some extent, “supervised” BLI is a key step in its “unsupervised” counterpart, and a more basic prototype to study. Let the source space be X , [x1 , . . . , x"
2020.emnlp-main.100,P18-1073,0,0.0115508,"e than frequent ones. But their BLI method is based on various “hand-crafted” features. We show that the same phenomenon occurs as well in BLI methods that are based on word embeddings. This type of methods have become especially popular in recent years (Mikolov et al., 2013; Faruqui and Dyer, 2014; Artetxe et al., 2016, 2018) and achieved state-of-art accuracies (Conneau et al., 2018). We briefly review BLI methods that are based on word embeddings. Without loss of generality, in this paper, we focus on “supervised” BLI, which assumes that a seeding dictionary is available. Unsupervised BLI (Artetxe et al., 2018; Conneau et al., 2018) often alternates between inducing a seeding dictionary and using that to refine generated translations. Therefore, to some extent, “supervised” BLI is a key step in its “unsupervised” counterpart, and a more basic prototype to study. Let the source space be X , [x1 , . . . , xm ], where xi ∈ Rd is the embedding vector for the W = arg min kWXs − Ys k2F , (1) W∈W where k · kF is Frobenius norm, and W is a constraint set of W. The easiest choice of W may be Rd×d , seen in (Mikolov et al., 2013). On the other hand, Xing et al. (2015) has observed substantial gain by letting"
2020.emnlp-main.100,P19-1399,1,0.825544,"Missing"
2020.emnlp-main.100,J17-2001,0,0.0245295,"rgin between similarities in low frequency regime, and secondly, exacerbated hubness at low frequency. Based on the observation, we further propose two methods to address these two factors, respectively. The larger issue is hubness. Addressing that improves induction accuracy significantly, especially for low-frequency words. 1 The typical supervised BLI works by first learning a transformation W that minimizes the discrepancy between Xs and Ys , Introduction Bilingual Lexicon Induction (BLI) studies how to generate word-level translations from non-parallel corpora in two languages. Recently, Irvine and Callison-Burch (2017) observe that rarer words are harder to translate than frequent ones. But their BLI method is based on various “hand-crafted” features. We show that the same phenomenon occurs as well in BLI methods that are based on word embeddings. This type of methods have become especially popular in recent years (Mikolov et al., 2013; Faruqui and Dyer, 2014; Artetxe et al., 2016, 2018) and achieved state-of-art accuracies (Conneau et al., 2018). We briefly review BLI methods that are based on word embeddings. Without loss of generality, in this paper, we focus on “supervised” BLI, which assumes that a see"
2020.emnlp-main.100,P15-1027,0,0.0310734,"can be considered as the “cost” of translation. In summary, we want to solve the following optimization problem, X min Pi,j cos(Wxi , yj ) Hinge Loss for Learning Transformation We first design a learning objective that enlarges the margin, as follows, min W∈O(d) X X i j:yj 6=trans(xsi ) max {0, P∈[0,1]m×n , γ − cos(Wxsi , yis ) + cos(Wxsi , yj )} (4) where γ &gt; 0 is a threshold. The objective encourages the margin cos(Wxsi , yis ) − cos(Wxsi , yj ) to be bigger than γ. It should be noticed that using hinge loss to learn the transformation is not a new idea. Examples are seen not only for BLI (Lazaridou et al., 2015), but also zero-shot image classification (Frome et al., 2013). Our difference with (Lazaridou et al., 2015) is that W is set to O(d) instead of Rd×d , as empirically we observe some gain. This is consistent with the discovery in (Xing et al., 2015), although they experiment with the Procrustes loss (Eq. (1)) instead. We apply the hinge loss to train the orthogonal transformation, using a seeding dictionary of 10K. Accuracy is reported as the green line in figure 2a. A notable gain is observed over the Procrustes loss (blue line), especially in low frequency regime. Figure 2b validates that th"
2020.emnlp-main.100,N15-1104,0,0.0542431,"Missing"
2020.emnlp-main.100,E14-1049,0,0.0430546,"works by first learning a transformation W that minimizes the discrepancy between Xs and Ys , Introduction Bilingual Lexicon Induction (BLI) studies how to generate word-level translations from non-parallel corpora in two languages. Recently, Irvine and Callison-Burch (2017) observe that rarer words are harder to translate than frequent ones. But their BLI method is based on various “hand-crafted” features. We show that the same phenomenon occurs as well in BLI methods that are based on word embeddings. This type of methods have become especially popular in recent years (Mikolov et al., 2013; Faruqui and Dyer, 2014; Artetxe et al., 2016, 2018) and achieved state-of-art accuracies (Conneau et al., 2018). We briefly review BLI methods that are based on word embeddings. Without loss of generality, in this paper, we focus on “supervised” BLI, which assumes that a seeding dictionary is available. Unsupervised BLI (Artetxe et al., 2018; Conneau et al., 2018) often alternates between inducing a seeding dictionary and using that to refine generated translations. Therefore, to some extent, “supervised” BLI is a key step in its “unsupervised” counterpart, and a more basic prototype to study. Let the source space"
2020.findings-emnlp.346,P12-3018,0,0.312669,"the waveform for the first word, which is also played immediately (see Fig. 2). This results in an O(1) rather than O(n) latency. Experiments on English and Chinese TTS show that our approach achieves similar speech naturalness compared to full sentence methods, but only with a constant (1–2 words) latency.1 This paper makes following contributions: • From the model point of view, with monotonic attention in TTS, we don’t need to retrain the model, and only need to adapt the inference. This is different from all other pre1 There also exist incremental TTS efforts using non-neural techniques (Baumann and Schlangen, 2012c,b; Baumann, 2014b; Pouget et al., 2015; Yanagita et al., 2018) which are fundamentally different from our work. See also Sec. 5. b731afe6-X4evPTjAzgj7EjZJZDKPMUXKnhBxOXbUikI2Rw==-500ddfd78926 vious incremental adaptations in simultaneous translation, ASR and TTS (Ma et al., 2019; Novitasari et al., 2019; Yanagita et al., 2019) which rely on new training algorithms and/or different training data preprocessing. • From a practical point of view, our adaptation reduces the TTS latency from O(n) to O(1), which reduces the TTS response time significantly. We also demonstrate that our neural increm"
2020.findings-emnlp.346,W12-1814,0,0.404688,"the waveform for the first word, which is also played immediately (see Fig. 2). This results in an O(1) rather than O(n) latency. Experiments on English and Chinese TTS show that our approach achieves similar speech naturalness compared to full sentence methods, but only with a constant (1–2 words) latency.1 This paper makes following contributions: • From the model point of view, with monotonic attention in TTS, we don’t need to retrain the model, and only need to adapt the inference. This is different from all other pre1 There also exist incremental TTS efforts using non-neural techniques (Baumann and Schlangen, 2012c,b; Baumann, 2014b; Pouget et al., 2015; Yanagita et al., 2018) which are fundamentally different from our work. See also Sec. 5. b731afe6-X4evPTjAzgj7EjZJZDKPMUXKnhBxOXbUikI2Rw==-500ddfd78926 vious incremental adaptations in simultaneous translation, ASR and TTS (Ma et al., 2019; Novitasari et al., 2019; Yanagita et al., 2019) which rely on new training algorithms and/or different training data preprocessing. • From a practical point of view, our adaptation reduces the TTS latency from O(n) to O(1), which reduces the TTS response time significantly. We also demonstrate that our neural increm"
2020.findings-emnlp.346,W12-1641,0,0.0223187,"econd stage, being much slower, is more commonly parallel (Oord et al., 2018; Prenger et al., 2019). Despite these successes, standard full-sentence neural TTS systems still suffer from two types of latencies: (a) the computational latency (synthesizing time), which still grows linearly with the sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sent"
2020.findings-emnlp.346,W10-4301,0,0.0437413,"Peng et al., 2019), while the second stage, being much slower, is more commonly parallel (Oord et al., 2018; Prenger et al., 2019). Despite these successes, standard full-sentence neural TTS systems still suffer from two types of latencies: (a) the computational latency (synthesizing time), which still grows linearly with the sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computati"
2020.findings-emnlp.346,2020.acl-main.42,1,0.801949,"standard full-sentence neural TTS systems still suffer from two types of latencies: (a) the computational latency (synthesizing time), which still grows linearly with the sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 tex"
2020.findings-emnlp.346,2020.acl-main.254,1,0.830514,"standard full-sentence neural TTS systems still suffer from two types of latencies: (a) the computational latency (synthesizing time), which still grows linearly with the sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 tex"
2020.findings-emnlp.346,D19-1137,1,0.844534,"e sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 text/phonemes k1 = 1 k2 = 0 spectrogram wave audio play time input latency comput. latency time saved Figure 2: Full-sentence TTS vs. our proposed incremental TTS with pref"
2020.findings-emnlp.346,P19-1582,1,0.827152,"e sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 text/phonemes k1 = 1 k2 = 0 spectrogram wave audio play time input latency comput. latency time saved Figure 2: Full-sentence TTS vs. our proposed incremental TTS with pref"
2020.findings-emnlp.346,D19-1144,1,0.839715,"e sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 text/phonemes k1 = 1 k2 = 0 spectrogram wave audio play time input latency comput. latency time saved Figure 2: Full-sentence TTS vs. our proposed incremental TTS with pref"
2020.findings-emnlp.346,2020.findings-emnlp.349,1,0.692511,"standard full-sentence neural TTS systems still suffer from two types of latencies: (a) the computational latency (synthesizing time), which still grows linearly with the sentence length even using parallel inference (esp. in the second stage), and (b) the input latency in scenarios where the input text is incrementally generated or revealed, such as in simultaneous translation (Bangalore et al., 2012; Ma et al., 2019), dialog generation (Skantze and Hjalmarsson, 2010; Buschmeier et al., 2012), and assistive technologies (Elliott, 2003). Especially in simultaneous speechto-speech translation (Zheng et al., 2020b), there are many efforts have been made in the simultaneous text-to-text translation stage to reduce the latency with either fixed (Ma et al., 2019; Zheng et al., 2019c, 2020c) or adaptive on-line decoding policy (Zheng et al., 2019b,a, 2020a,b). But the conventional full-sentence TTS has to wait until 3886 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3886–3896 c November 16 - 20, 2020. 2020 Association for Computational Linguistics full sentence text/phonemes spectrogram wave audio play input latency time computational latency incremental with lookahead-1 tex"
2020.findings-emnlp.349,P19-1126,0,0.139367,"other hand, simultaneous speech-to-speech translation escalates the challenge by considering the smooth cooperation between the modules of speech recognition, translation and speech synthesis. In order to achieve simultaneous speech-tospeech translation (SSST), to the best of our knowledge, most recent approaches (Oda et al., 2014; Xiong et al., 2019) dismantle the entire system into a three-step pipelines, streaming Automatic Speech Recognition (ASR) (Sainath et al., 2020; Inaguma et al., 2020; Li et al., 2020), simultaneous Text-to-Text translation (sT2T) (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng"
2020.findings-emnlp.349,E17-1099,0,0.425458,"s (English, Chinese, etc.); on the other hand, simultaneous speech-to-speech translation escalates the challenge by considering the smooth cooperation between the modules of speech recognition, translation and speech synthesis. In order to achieve simultaneous speech-tospeech translation (SSST), to the best of our knowledge, most recent approaches (Oda et al., 2014; Xiong et al., 2019) dismantle the entire system into a three-step pipelines, streaming Automatic Speech Recognition (ASR) (Sainath et al., 2020; Inaguma et al., 2020; Li et al., 2020), simultaneous Text-to-Text translation (sT2T) (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020"
2020.findings-emnlp.349,P14-2090,0,0.338171,"s://sat-demo.github.io. Equal contribution Work done at Baidu Research. Current address: Kwai Inc., Seattle, WA, USA. ‡ … tgt speech unnatural pauses Sent. #1 Introduction † Sent. #2 src speech SOV languages (German, Japanese, etc.) and SVO languages (English, Chinese, etc.); on the other hand, simultaneous speech-to-speech translation escalates the challenge by considering the smooth cooperation between the modules of speech recognition, translation and speech synthesis. In order to achieve simultaneous speech-tospeech translation (SSST), to the best of our knowledge, most recent approaches (Oda et al., 2014; Xiong et al., 2019) dismantle the entire system into a three-step pipelines, streaming Automatic Speech Recognition (ASR) (Sainath et al., 2020; Inaguma et al., 2020; Li et al., 2020), simultaneous Text-to-Text translation (sT2T) (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve b"
2020.findings-emnlp.349,2020.acl-main.254,1,0.726839,") (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng et al., 2019c, 2020b), or multimodal 3928 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3928–3937 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Avg. latency (seconds) Avg. # of unnatural pauses 8 25 7 6 20 5 4 15 3 10 2 1 5 0 1.0 1.5 2.0 2.5 3.0 Source speech rate (words per second) Figure 2: Average Chinese and English speed rate distribution for different speakers. information (Imankulova et al., 2019). However, is it sufficient to only consider the effectiveness of sT2T and ignore the inter"
2020.findings-emnlp.349,D19-1137,1,0.780204,"2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng et al., 2019c, 2020b), or multimodal 3928 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3928–3937 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Avg. latency (seconds) Avg. # of unnatural pauses 8 25 7 6 20 5 4 15 3 10 2 1 5 0 1.0 1.5 2.0 2.5 3.0 Source speech rate (words per second) Figure 2: Average Chinese and English speed rate distribution for different speakers. information (Imankulova et al., 2019). However, is it sufficient to only consider the effectiveness of sT2T and ignore the interactions between other different components? Furthermore,"
2020.findings-emnlp.349,P19-1582,1,0.742053,"2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng et al., 2019c, 2020b), or multimodal 3928 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3928–3937 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Avg. latency (seconds) Avg. # of unnatural pauses 8 25 7 6 20 5 4 15 3 10 2 1 5 0 1.0 1.5 2.0 2.5 3.0 Source speech rate (words per second) Figure 2: Average Chinese and English speed rate distribution for different speakers. information (Imankulova et al., 2019). However, is it sufficient to only consider the effectiveness of sT2T and ignore the interactions between other different components? Furthermore,"
2020.findings-emnlp.349,D19-1144,1,0.852399,"2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng et al., 2019c, 2020b), or multimodal 3928 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3928–3937 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Avg. latency (seconds) Avg. # of unnatural pauses 8 25 7 6 20 5 4 15 3 10 2 1 5 0 1.0 1.5 2.0 2.5 3.0 Source speech rate (words per second) Figure 2: Average Chinese and English speed rate distribution for different speakers. information (Imankulova et al., 2019). However, is it sufficient to only consider the effectiveness of sT2T and ignore the interactions between other different components? Furthermore,"
2020.findings-emnlp.349,2020.acl-main.42,1,0.718068,") (Gu et al., 2017; Ma et al., 2019; Arivazhagan et al., 2019; Ma et al., 2020b), and Text-to-Speech (TTS) synthesis (Wang et al., 2017; Ping et al., 2017; Oord et al., 2017). Most recent efforts mainly focus on sT2T which is considered the key component to further reduce the translation latency and improve the translation quality for the entire pipeline. To achieve better translation quality and lower latency, there has been extensive research efforts which concentrate on the sT2T by introducing more robust models (Ma et al., 2019; Arivazhagan et al., 2019), better policies (Gu et al., 2017; Zheng et al., 2020a, 2019b,a), new decoding algorithms (Zheng et al., 2019c, 2020b), or multimodal 3928 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3928–3937 c November 16 - 20, 2020. 2020 Association for Computational Linguistics Avg. latency (seconds) Avg. # of unnatural pauses 8 25 7 6 20 5 4 15 3 10 2 1 5 0 1.0 1.5 2.0 2.5 3.0 Source speech rate (words per second) Figure 2: Average Chinese and English speed rate distribution for different speakers. information (Imankulova et al., 2019). However, is it sufficient to only consider the effectiveness of sT2T and ignore the inter"
2021.emnlp-main.501,J90-1003,1,0.275263,"he British National Corpus (BNC) (Aston and Burnard, 1998; Burnard, 2002), 1 500 samples span 15 categories: Press Reportage (44 texts), Press Editorial (27), Press Reviews (17), Religion (17), Skills and Hobbies (36), Popular Lore (48), Miscellaneous US Government & House Organs (30), Learned (80), General Fiction (29), Mystery and Detective Fiction (24), Science Fiction (6), Adventure and Western (29), Romance (29), Humor (9). it was known that 1M was too small for second order statistics (collocations and word associations), but it was hoped that 100M would be sufficient. Around this time, Church and Hanks (1990) used an unbalanced sample of 44M words from the AP (Associated Press) to make the case for PMI (pointwise mutual information). Given the estimates in Table 1, it appears in retrospect that 44M words were just barely enough to make the case for PMI. It was also believed that quality (balance) mattered, but there were few, if any, empirical studies to justify such beliefs. It was extremely controversial when engineers such as Mercer questioned these deeply held beliefs in 19852 with: “there is no data like more data.” Most people working on corpus-based methods in lexicography were deeply commi"
2021.emnlp-main.501,N19-1423,0,0.00478116,"to know what will happen for much (1000x) larger corpora, but one might expect diminishing returns. Of course, extrapolating estimates like these by 10x or more is known to be risky (Efron and Thisted, 1976). Figures 1a-b of (Rapp, 2014b) suggest that while ρ is increasing almost everywhere, there may be some deceleration (negative second derivative), especially for large N . Although Rapp’s estimates predate much of the work on embeddings, we expect these estimates of quantity and quality to hold for static embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual embeddings (Devlin et al., 2019; Sun et al., 2020), assuming the connection between PMI and Word2vec in Levy and Goldberg (2014). In addition to size and balance, there are many other factors to consider. Different languages are different. Languages are constantly evolving. Variations are to be expected over time3,4 (Hamilton et al., 2016; Szymanski, 2017)5 and space, as well as sociolinguistic factors, demographics, gender bias (Pearce, 2008; Drozd et al., 2016; Sheng et al., 2019; Nissim et al., 2020; Kumar et al., 2020), etc. In addition to language change, topics and domains are also constantly evolving. Obviously, 3 ht"
2021.emnlp-main.501,C16-1332,0,0.0200798,"eddings, we expect these estimates of quantity and quality to hold for static embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual embeddings (Devlin et al., 2019; Sun et al., 2020), assuming the connection between PMI and Word2vec in Levy and Goldberg (2014). In addition to size and balance, there are many other factors to consider. Different languages are different. Languages are constantly evolving. Variations are to be expected over time3,4 (Hamilton et al., 2016; Szymanski, 2017)5 and space, as well as sociolinguistic factors, demographics, gender bias (Pearce, 2008; Drozd et al., 2016; Sheng et al., 2019; Nissim et al., 2020; Kumar et al., 2020), etc. In addition to language change, topics and domains are also constantly evolving. Obviously, 3 https://books.google.com/ngrams 4 https://github.com/dimazest/ google-ngram-downloader 5 https://nlp.stanford.edu/projects/ histwords/ news, Wikipedia and web pages are very different from social media (Twitter) and academic writing (ACL Anthology (Radev et al., 2013), ArXiv,6 PubMed7 ). The Brown Corpus predates social media, and most publications in repositories such as: PubMed, ACL Anthology and ArXiv (Church, 2017). The Brown Cor"
2021.emnlp-main.501,2021.eacl-main.113,0,0.0234838,"rrects for the known leakage (Dettmers et al., 2018).13 The correction removes the 7 inverse relations on the right hand side of Table 2, resulting in the test set shown in Table 3. Before the correction, there are 5000 edges over 18 relations in the WN18 test set. After the correction, there are 3134 edges over 11 relations in the WN18RR test set. Unfortunately, there is even more leakage in WN18RR that has not been previously reported. Note that “derivationally related forms” also come in pairs. By construction, derivationally related links are symmetric: xRy ⇒ yRx. That is, if there 13 See Elangovan et al. (2021) for discussion of leakage in other benchmarks. 6212 Forward Links xRy test xRy train xRy valid Totals Inverse Links: test train 24 1011 1011 27,701 39 1003 1074 29,715 yRx valid 39 1003 36 1078 Totals 1074 29,715 1078 31,867 Table 4: Information Leakage in WN18RR: Derivationally related links are symmetric (xRy ⇒ yRx). is an edge in one direction, then there will also be an inverse edge in the reverse direction. This symmetry will leak information between train and test because it is likely that one member of the pair appears in train and the other appears in test. Table 4 shows that many of"
2021.emnlp-main.501,1996.amta-1.36,0,0.742943,"Missing"
2021.emnlp-main.501,P16-1141,0,0.0182376,"ome deceleration (negative second derivative), especially for large N . Although Rapp’s estimates predate much of the work on embeddings, we expect these estimates of quantity and quality to hold for static embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual embeddings (Devlin et al., 2019; Sun et al., 2020), assuming the connection between PMI and Word2vec in Levy and Goldberg (2014). In addition to size and balance, there are many other factors to consider. Different languages are different. Languages are constantly evolving. Variations are to be expected over time3,4 (Hamilton et al., 2016; Szymanski, 2017)5 and space, as well as sociolinguistic factors, demographics, gender bias (Pearce, 2008; Drozd et al., 2016; Sheng et al., 2019; Nissim et al., 2020; Kumar et al., 2020), etc. In addition to language change, topics and domains are also constantly evolving. Obviously, 3 https://books.google.com/ngrams 4 https://github.com/dimazest/ google-ngram-downloader 5 https://nlp.stanford.edu/projects/ histwords/ news, Wikipedia and web pages are very different from social media (Twitter) and academic writing (ACL Anthology (Radev et al., 2013), ArXiv,6 PubMed7 ). The Brown Corpus preda"
2021.emnlp-main.501,J93-2004,0,0.0870697,"Missing"
2021.emnlp-main.501,H93-1061,0,0.0636365,"Missing"
2021.emnlp-main.501,D19-1339,0,0.0267204,"hese estimates of quantity and quality to hold for static embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual embeddings (Devlin et al., 2019; Sun et al., 2020), assuming the connection between PMI and Word2vec in Levy and Goldberg (2014). In addition to size and balance, there are many other factors to consider. Different languages are different. Languages are constantly evolving. Variations are to be expected over time3,4 (Hamilton et al., 2016; Szymanski, 2017)5 and space, as well as sociolinguistic factors, demographics, gender bias (Pearce, 2008; Drozd et al., 2016; Sheng et al., 2019; Nissim et al., 2020; Kumar et al., 2020), etc. In addition to language change, topics and domains are also constantly evolving. Obviously, 3 https://books.google.com/ngrams 4 https://github.com/dimazest/ google-ngram-downloader 5 https://nlp.stanford.edu/projects/ histwords/ news, Wikipedia and web pages are very different from social media (Twitter) and academic writing (ACL Anthology (Radev et al., 2013), ArXiv,6 PubMed7 ). The Brown Corpus predates social media, and most publications in repositories such as: PubMed, ACL Anthology and ArXiv (Church, 2017). The Brown Corpus also predates hu"
2021.emnlp-main.501,P17-2071,0,0.0116314,"ive second derivative), especially for large N . Although Rapp’s estimates predate much of the work on embeddings, we expect these estimates of quantity and quality to hold for static embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual embeddings (Devlin et al., 2019; Sun et al., 2020), assuming the connection between PMI and Word2vec in Levy and Goldberg (2014). In addition to size and balance, there are many other factors to consider. Different languages are different. Languages are constantly evolving. Variations are to be expected over time3,4 (Hamilton et al., 2016; Szymanski, 2017)5 and space, as well as sociolinguistic factors, demographics, gender bias (Pearce, 2008; Drozd et al., 2016; Sheng et al., 2019; Nissim et al., 2020; Kumar et al., 2020), etc. In addition to language change, topics and domains are also constantly evolving. Obviously, 3 https://books.google.com/ngrams 4 https://github.com/dimazest/ google-ngram-downloader 5 https://nlp.stanford.edu/projects/ histwords/ news, Wikipedia and web pages are very different from social media (Twitter) and academic writing (ACL Anthology (Radev et al., 2013), ArXiv,6 PubMed7 ). The Brown Corpus predates social media,"
2021.emnlp-main.501,2020.cl-2.7,0,0.023643,"Missing"
2021.emnlp-main.501,D14-1162,0,0.0855931,"has a better ρ2 than 100M words of BNC. It is hard to know what will happen for much (1000x) larger corpora, but one might expect diminishing returns. Of course, extrapolating estimates like these by 10x or more is known to be risky (Efron and Thisted, 1976). Figures 1a-b of (Rapp, 2014b) suggest that while ρ is increasing almost everywhere, there may be some deceleration (negative second derivative), especially for large N . Although Rapp’s estimates predate much of the work on embeddings, we expect these estimates of quantity and quality to hold for static embeddings (Mikolov et al., 2013; Pennington et al., 2014) and contextual embeddings (Devlin et al., 2019; Sun et al., 2020), assuming the connection between PMI and Word2vec in Levy and Goldberg (2014). In addition to size and balance, there are many other factors to consider. Different languages are different. Languages are constantly evolving. Variations are to be expected over time3,4 (Hamilton et al., 2016; Szymanski, 2017)5 and space, as well as sociolinguistic factors, demographics, gender bias (Pearce, 2008; Drozd et al., 2016; Sheng et al., 2019; Nissim et al., 2020; Kumar et al., 2020), etc. In addition to language change, topics and domain"
2021.emnlp-main.501,C14-1200,0,0.096124,"ointwise mutual information). Given the estimates in Table 1, it appears in retrospect that 44M words were just barely enough to make the case for PMI. It was also believed that quality (balance) mattered, but there were few, if any, empirical studies to justify such beliefs. It was extremely controversial when engineers such as Mercer questioned these deeply held beliefs in 19852 with: “there is no data like more data.” Most people working on corpus-based methods in lexicography were deeply committed to balance as a matter of faith, and were deeply troubled by Mercer’s heresy. More recently, Rapp (2014a,b) provided some empirical evidence that bears on this debate. He used 5 corpora to study quantity (sample size) and quality (balance). In addition to the two balanced corpora mentioned above, Brown and BNC, Rapp looked at 3 unbalanced corpora: 1. 300M words of Wikipedia (Wiki) 2. 2B words of web pages (ukWaC) 3. 4B words of newswire (Gigaword) This study used correlations, ρ, to compare statistical summaries with psycholinguistic norms: familiarity (Coltheart, 1981), association (Kiss et al., 1973) and relatedness (Fernald, 1896). We will refer to unigram statistics and familiarity norms as"
2021.emnlp-main.501,rapp-2014-using-word,0,0.131316,"ointwise mutual information). Given the estimates in Table 1, it appears in retrospect that 44M words were just barely enough to make the case for PMI. It was also believed that quality (balance) mattered, but there were few, if any, empirical studies to justify such beliefs. It was extremely controversial when engineers such as Mercer questioned these deeply held beliefs in 19852 with: “there is no data like more data.” Most people working on corpus-based methods in lexicography were deeply committed to balance as a matter of faith, and were deeply troubled by Mercer’s heresy. More recently, Rapp (2014a,b) provided some empirical evidence that bears on this debate. He used 5 corpora to study quantity (sample size) and quality (balance). In addition to the two balanced corpora mentioned above, Brown and BNC, Rapp looked at 3 unbalanced corpora: 1. 300M words of Wikipedia (Wiki) 2. 2B words of web pages (ukWaC) 3. 4B words of newswire (Gigaword) This study used correlations, ρ, to compare statistical summaries with psycholinguistic norms: familiarity (Coltheart, 1981), association (Kiss et al., 1973) and relatedness (Fernald, 1896). We will refer to unigram statistics and familiarity norms as"
2021.naacl-main.72,W19-4828,0,0.132368,"ffectiveness. The comprehensive analyses on attention redundancy make model understanding and zero-shot model pruning promising. 0 0.6 60 72 84 0.4 96 108 120 0.2 132 144 0 12 24 36 48 60 72 84 96 108 120 132 144 Head index 1 2 3 4 5 6 7 8 9 10 11 12 0.0 Layer index Figure 1: Pair-wise Jensen-Shannon distance of attention heads for the pre-trained BERT-base model (12layer-12-head self-attention). Attention redundancy (cluster with small distances) exists in adjacent attention heads and layers. attention heads. It demonstrates that many attention heads generate very similar attention matrices (Clark et al., 2019; Kovaleva et al., 2019). We take the pre-trained BERT-base model as an example. It learns 12-layer-12-head self-attention matrices describing dependencies between each pair of tokens in a sentence. Then for each token, there are 144 attention vectors. We use Jensen-Shannon distance to measure the relationship between each pair of vectors. Then for one sentence (consisting of a sequence of tokens), the token-averaged distance is utilized to imply the redundancy between each pair of attention matrices. Smaller distance val1 Introduction ues reflect more redundancy. Figure 1 shows the Multi-laye"
2021.naacl-main.72,W18-5446,0,0.142643,"tiondency relations and distant information, respec- ship among the 144 heads (i.e., attention matrices) as redundancy matrix. We consider that the smaller tively. Even with BERT’s success, it still struggles handling some linguistic information and tasks. the distances among some attention heads are, the BERT does not excel at numbers, negation, in- more attention redundancy exists in them. As for the studying objects, we use the wellferences and role-based event prediction (Wallace known natural language understanding benchmark et al., 2019; Ettinger, 2020). It’s questionable to GLUE task1 (Wang et al., 2018) as evaluation obprovide transparency or meaningful explanations jects for the attention redundancy analysis. Among for model predictions on downtream tasks (Jain GLUE, CoLA is for English sentence acceptabiland Wallace, 2019) . Since self-attention is the fundamental mecha- ity judgments. SST-2 and STS-B are sentiment nism in BERT, existing works also investigate ex- analyses tasks. MRPC and QQP are for sentencepair similarity classification. MNLI, QNLI, and tracted attention vectors and/or matrices. These are most relevant to our study. Clark et al. (2019) and 1 We eliminate WNLI due to its"
A88-1019,A88-1030,0,\N,Missing
C00-1027,P99-1022,0,0.181053,"Lore 0 100 200 300 400 500 document number Poisson Doesn’t Fit pendence on frequency as well as dependence on content) are hard to capture in an additive-based cache model. Later in the paper, we will study neighbors, words that do not appear in the history but do appear in documents near the history using an information retrieval notion of near. We find that neighbors adapt more than non-neighbors, but not as much as the history. The shape is in between as well. Neighbors have a modest dependency on frequency, more than the history, but not as much as the prior. Neighbors are an extension of Florian & Yarowsky (1999), who used topic clustering to build a language model for contexts such as: ‘‘It is at least on the Serb side a real setback to the x.’’ Their work was motivated by speech recognition applications where it would be desirable for the language model to favor x = ‘‘peace’’ over x = ‘‘piece.’’ Obviously, acoustic evidence is not very helpful in this case. Trigrams are also not very helpful because the strongest clues (e.g., ‘‘Serb,’’ ‘‘side’’ and ‘‘setback’’) are beyond the window of three words. Florian & Yarowsky cluster documents into about 10 2 topics, and compute a separate trigram language m"
C10-1100,J07-4002,0,0.0647416,"Missing"
C10-1100,P01-1005,0,0.017726,"nearly with the log of the number of types in the auxiliary data set. Google V1 is the one data source for which the relationship between accuracy and number of N-grams is not monotonic. After about 100 million unique N-grams, performance starts decreasing. This drop shows the need for Google V2. Since Google V1 contains duplicated web pages and sentences, mistakes that should be rare can appear to be quite frequent. Google V2, which comes from the same snapshot of the web as Google V1, but has only unique sentences, does not show this drop. We regard the results in Figure 2 as a companion to Banko and Brill (2001)’s work on exponentially increasing the amount of labeled training data. Here we see that varying the amount of 892 data. Performance improves log-linearly with the number of parameters (unique N-grams). One can increase performance with larger models, e.g., increasing the size of the unlabeled corpora, or by decreasing the frequency threshold. Alternatively, one can decrease storage costs with smaller models, e.g., decreasing the size of the unlabeled corpora, or by increasing the frequency threshold. Either way, the log-linear relationship between accuracy and model size makes it easy to est"
C10-1100,D08-1107,0,0.0524726,"Missing"
C10-1100,D07-1086,1,0.37949,"bracketing decision for “and television producers” should be different in each. 887 early mistakes can cascade and lead to a chain of incorrect bracketings. Our approach differs from previous work in NP parsing; rather than greedily inserting brackets as in Barker’s algorithm, we use dynamic programming to find the global maximum-scoring parse. In addition, unlike previous approaches that have used local features to make local decisions, we use the full NP to score each potential bracketing. A related line of research aims to segment longer phrases that are queried on Internet search engines (Bergsma and Wang, 2007; Guo et al., 2008; Tan and Peng, 2008). Bergsma and Wang (2007) focus on NP queries of length four or greater. They use supervised learning to make segmentation decisions, with features derived from the noun compound bracketing literature. Evaluating the benefits of parsing NP queries, rather than simply segmenting them, is a natural application of our system. 3 Annotated Data Our training and testing data are derived from recent annotations by Vadas and Curran (2007a). The original PTB left a flat structure for base noun phrases. For example, “retired science teacher,” would be represented a"
C10-1100,D07-1090,0,0.00709778,"Google V2 NEWS 94 1e4 1e5 1e6 1e7 1e8 1e9 Number of Unique N-grams Acknowledgments Figure 2: There is no data like more data. Accuracy improves with the number of parameters (unique N-grams). This trend holds across three different sources of N-grams. unlabeled data can cause an equally predictable improvement in classification performance, without the cost of labeling data. Suzuki and Isozaki (2008) also found a loglinear relationship between unlabeled data (up to a billion words) and performance on three NLP tasks. We have shown that this trend continues well beyond Gigaword-sized corpora. Brants et al. (2007) also found that more unlabeled data (in the form of input to a language model) leads to improvements in BLEU scores for machine translation. Adding noun phrase parsing to the list of problems for which there is a “bigger is better” relationship between performance and unlabeled data shows the wide applicability of this principle. As both the amount of text on the web and the power of computer architecture continue to grow exponentially, collecting and exploiting web-scale auxiliary data in the form of N-gram corpora should allow us to achieve gains in performance linear in time, without any h"
C10-1100,P05-1022,0,0.0544015,"Missing"
C10-1100,J90-1003,1,0.0530333,"ut weighted differently, in each proposed bracketing’s feature set. 6.1 N-gram Features All of the features described in this section require estimates of the probability of specific words or sequences of words. All probabilities are computed using Google V2 (Section 4). 6.1.1 PMI Recall that the adjacency model for the threeword task uses the associations of the two pairs of adjacent words, while the dependency model uses the associations of the two pairs of attachment sites for the initial noun. We generalize the adjacency and dependency models by including the pointwise mutual information (Church and Hanks, 1990) between all pairs of words in the NP: PMI(x, y) = log p(“x y”) p(“x”)p(“y”) (1) For NPs of length n, for each proposed bracketing, we include separate features for the PMI be tween all n2 pairs of words in the NP. For NPs including conjunctions, we include additional PMI features (Section 6.1.2). Since these features are also tied to the proposed bracketing positions (as explained above), this allows us to learn relationships between various associations within the NP and each potential bracketing. For example, consider a proposed bracketing from word 4 to word 5. We learn that a high associ"
C10-1100,J82-3004,1,0.322975,"Missing"
C10-1100,A88-1019,1,0.64327,"Missing"
C10-1100,J05-1003,0,0.0448013,"Missing"
C10-1100,P08-1109,0,0.0084756,"Missing"
C10-1100,P08-1076,0,0.0795093,"Missing"
C10-1100,W04-3201,0,0.0673423,"Missing"
C10-1100,P07-1031,0,0.599259,"are therefore a major opportunity to improve overall base NP parsing. Since in the general case, NP parsing can no longer be thought of as a single binary classification problem, different strategies are required. Barker (1998) reduces the task of parsing longer NPs to making sequential three-word decisions, moving a sliding window along the NP. The window is first moved from right-to-left, inserting right bracketings, and then again from leftto-right, finalizing left bracketings. While Barker (1998) assumes that these three-word decisions can be made in isolation, this is not always valid.1 Vadas and Curran (2007b) employ Barker’s algorithm, but use a supervised classifier to make the sequential bracketing decisions. Because these approaches rely on a sequence of binary decisions, 1 E.g., although the right-most three words are identical in 1) “soap opera stars and television producers,” and 2) “movie and television producers,” the initial right-bracketing decision for “and television producers” should be different in each. 887 early mistakes can cascade and lead to a chain of incorrect bracketings. Our approach differs from previous work in NP parsing; rather than greedily inserting brackets as in Ba"
C10-1100,N01-1025,0,0.169096,"Missing"
C10-1100,A97-1046,0,0.0947829,"Missing"
C10-1100,P95-1007,0,0.510089,"duce a score for every possible NPinternal bracketing and creates a chart of bracketing scores. This chart can be used as features in a full sentence parser or parsed directly with a chart parser. Our parses are highly accurate, creating a strong new standard for this task. Finally, we present experiments that investigate the effects of N-gram frequency cutoffs and various sources of N-gram data. We show an interesting relationship between accuracy and the number of unique N-gram types in the data. 2 Related Work 2.1 Three-Word Noun Compounds The most commonly used data for NP parsing is from Lauer (1995), who extracted 244 three-word noun compounds from the Grolier encyclopedia. When there are only three words, this task reduces to a binary decision: • Left Branching: * [retired science] teacher • Right Branching: retired [science teacher] In Lauer (1995)’s set of noun compounds, twothirds are left branching. The main approach to these three-word noun compounds has been to compute association statistics between pairs of words and then choose the bracketing that corresponds to the more highly associated pair. The two main models are the adjacency model (Marcus, 1980; Liberman and Sproat, 1992;"
C10-1100,lin-etal-2010-new,1,0.133502,"our N-gram features described in Section 6.1 rely on probabilities derived from unlabeled data. To use the largest amount of data possible, we exploit web-scale N-gram corpora. N-gram counts are an efficient way to compress large amounts of data (such as all the text on the web) into a manageable size. An N-gram corpus records how often each unique sequence of words occurs. Co-occurrence probabilities can be calculated directly from the N-gram counts. To keep the size manageable, N-grams that occur with a frequency below a particular threshold can be filtered. The corpus we use is Google V2 (Lin et al., 2010): a new N-gram corpus with N-grams of length 1-5 that we created from the same 1 trillion word snapshot of the web as Google N-grams Version 1 (Brants and Franz, 2006), but with several enhancements. Duplicate sentences are removed, as well as “sentences” which are probably noise (indicated by having a large proportion of non-alphanumeric characters, being very long, or being very short). Removing duplicate sentences is especially important because automaticallygenerated websites, boilerplate text, and legal disclaimers skew the source web data, with sentences that may have only been authored"
C10-1100,W05-0603,0,0.396873,"Missing"
C10-1100,J93-2005,0,0.031406,"who extracted 244 three-word noun compounds from the Grolier encyclopedia. When there are only three words, this task reduces to a binary decision: • Left Branching: * [retired science] teacher • Right Branching: retired [science teacher] In Lauer (1995)’s set of noun compounds, twothirds are left branching. The main approach to these three-word noun compounds has been to compute association statistics between pairs of words and then choose the bracketing that corresponds to the more highly associated pair. The two main models are the adjacency model (Marcus, 1980; Liberman and Sproat, 1992; Pustejovsky et al., 1993; Resnik, 1993) and the dependency model (Lauer, 1995). Under the adjacency model, the bracketing decision is made by comparing the associations between words one and two versus words two and three (i.e. comparing retired science versus science teacher). In contrast, the dependency model compares the associations between one and two versus one and three (retired science versus retired teacher). Lauer (1995) compares the two models and finds the dependency model to be more accurate. Nakov and Hearst (2005) compute the association scores using frequencies, conditional probabilities, χ2 , and mut"
C10-1100,W95-0107,0,0.0291305,"Missing"
C10-1100,J93-2004,0,\N,Missing
C88-1069,P86-1009,0,\N,Missing
C94-1085,H93-1076,0,0.0257453,"out of one document and paste them into another, or use them as input to an arbitrary program. If we are successful, the user shouldn't have to know about markup languages (e.g., SGML), tables, figures, floating displays, headers, footers, footnotes, columns, fonts, point sizes, character sets (e.g., ascii, unicode), and all sorts of other ""technical details."" As tar as the user is concerned, the system is just faxes (or bitmaps), through and through. 1. Image EMACS: the Ultimate in WYS1WYG Many of the pieces of this proposal are well underway. The Image EMACS editor (Bagley and Kopec, 1992; Bush, 1993), for example, makes it possible to edit bitmaps more or less the same way that one edits a text file. You can scan an image into the computer, change a few words, re-justify a t~aragraph, and then print it out again. hnage EMACS is the nltimatc in WYSIWYG: what you see is what you get, and vice versa. Most WYSIWYG editors do only half the job; they let you print out what you see, but they don't let you scan it back in. The round trip is key. It makes it possible to work with any document in any format. (At worst, the document can be printed out and scanned into hnage EMACS.) Most WYSIWYG edit"
C94-1085,J93-1001,1,0.871841,"Missing"
C94-2178,P93-1002,0,0.319397,"es such as the Canadian Parliamentary Debates (Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. 1. Motivation There have been quite a number of recent papers on parallel text: Brown et al (1990, 1991, 1993), Chen (1993), Church (1993), Church et al (1993), Dagan et al (1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al (1992), WarwickArmstrong and Russell (1990), Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work (Church et al, 1993), w"
C94-2178,P93-1001,1,0.941913,"e Canadian Parliamentary Debates (Hansards). Some of these methods generate a bilingual lexicon as a by-product. We present an alternative alignment strategy which we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. 1. Motivation There have been quite a number of recent papers on parallel text: Brown et al (1990, 1991, 1993), Chen (1993), Church (1993), Church et al (1993), Dagan et al (1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al (1992), WarwickArmstrong and Russell (1990), Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work (Church et al, 1993), we have reported"
C94-2178,J90-2002,0,0.141512,"Missing"
C94-2178,P91-1022,0,0.77201,"Missing"
C94-2178,J93-1004,1,0.896188,"Missing"
C94-2178,P93-1003,0,0.0847022,"we call K-vec, that starts by estimating the lexicon. For example, it discovers that the English word fisheries is similar to the French p~ches by noting that the distribution of fisheries in the English text is similar to the distribution of p~ches in the French. K-vec does not depend on sentence boundaries. 1. Motivation There have been quite a number of recent papers on parallel text: Brown et al (1990, 1991, 1993), Chen (1993), Church (1993), Church et al (1993), Dagan et al (1993), Gale and Church (1991, 1993), Isabelle (1992), Kay and Rgsenschein (1993), Klavans and Tzoukermann (1990), Kupiec (1993), Matsumoto (1991), Ogden and Gonzales (1993), Shemtov (1993), Simard et al (1992), WarwickArmstrong and Russell (1990), Wu (to appear). Most of this work has been focused on European language pairs, especially English-French. It remains an open question how well these methods might generalize to other language pairs, especially pairs such as English-Japanese and EnglishChinese. In previous work (Church et al, 1993), we have reported some preliminary success in aligning the English and Japanese versions of the A W K manual (Aho, Kernighan, Weinberger (1980)), using 1096 charalign (Church, 1993"
C94-2178,P93-1004,0,0.309716,"Missing"
C94-2178,E93-1054,0,\N,Missing
C94-2178,J93-2003,0,\N,Missing
C94-2178,C90-3031,0,\N,Missing
C94-2178,H91-1026,1,\N,Missing
C94-2178,P94-1012,0,\N,Missing
D07-1021,O01-2002,1,0.915604,"Missing"
D07-1021,P96-1010,0,0.0450962,"many people. Walter Mossberg of the Wall Street Journal called out the contextual speller (the blue squiggles) as one of the most notable features in Office 2007: There are other nice additions. In Word, Outlook and PowerPoint, there is now contextual spell checking, which points to a wrong word, even if the spelling is in the dictionary. For example, if you type “their” instead of “they&apos;re,” Office catches the mistake. It really works. 1 The use of contextual language models in spelling correction has been discussed elsewhere: (Church and Gale, 1991), (Mays et al, 1991), (Kukich, 1992) and (Golding and Schabes, 1996). This paper will focus on how to deploy such methods to millions and millions of users. Depending on the particular application and requirements, we need to make different tradeoffs among: 1. Space (for compressed language model), 2. Runtime (for n-gram lookup), and 3. Accuracy (losses for n-gram estimates). HashTBO optimizes space at the expense of the other two. We recommend HashTBO when space concerns dominate the other concerns; otherwise, use ZipTBO. There are many applications where space is extremely tight, especially on cell phones. HashTBO was developed for contextual spelling in Mic"
D07-1021,P02-1023,1,\N,Missing
D11-1103,H89-2027,0,0.461687,"ilized. The details of this method are presented next. 3.2 N best List Rescoring N best list re-scoring is a popular way to capture some long-distance dependencies, though the method can be slow and it can be biased toward the weaker language model that was used in the first pass. Given a word lattice, L, top N paths {π1 , . . . , πN } are extracted such that their joint likelihood under the baseline acoustic and language models are in descending order i.e. that: A[π1 ]γ L[π1 ] ≥ A[π2 ]γ L[π2 ] ≥ . . . ≥ A[πN ]γ L[πN ] Efficient algorithms exist for extracting N best paths from word lattices (Chow and Schwartz, 1989; Mohri and Riley, 2002). If a new language model, Lnew , is provided, which now need not be restricted to finite state machine family, then that can be deployed to get the score of the entire path π. If we denote the new LM scores by Lnew [·], then under N best list paradigm, optimal path π ˜ is found out such that: π ˜ = arg max A[π]η Lnew [π], (2) π∈{π1 ,...,πN } where η acts as the new scaling parameter which may or may not be equal to γ. If N  |L |(where |L |is the total number of complete paths in word lattice, which are exponentially many), then the path obtained using (2) is not guara"
D11-1103,D09-1116,0,0.0142904,"tics language. In addition, if we can move to more general models then we could hope to capture more, as well. However, due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging. Due to the prohibitive increase in the search spac"
D11-1103,D09-1005,0,0.0124286,"from a bi-gram language model (bg) and rescored with relatively better language models (see Table 1 for model definitions). Entropy under the baseline model correlates well with the rank correlation factor, suggesting that exhaustive search need not be necessary for utterances yielding lower entropy. While computation of entropy for N best list is tractable, for a word lattice, the computation of entropy is intractable if one were to enumerate all the hypotheses. Even if we were able to enumerate all hypotheses, this method tends to be slower. Using efficient semiring techniques introduced by Li and Eisner (2009) or using posterior probabilities on the edges leading to end states, we can compute the entropy of a lattice in one single forward pass using dynamic programming. It should, however, be noted that, for dynamic programming technique to work, only n-gram LMs can be used. One has to resort to approximate entropy computation via N best list, if entropy under long span LM is desired. 4.3.1 Speed Up for Iterative Decoding Our speed up technique is simple. Once we have formed self contained sub lattices, we want to prune all but the top few best complete paths (obtained un1123 der baseline / startin"
D11-1103,J01-2004,0,0.0166612,"ssociation for Computational Linguistics language. In addition, if we can move to more general models then we could hope to capture more, as well. However, due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging"
D11-1103,W02-1031,0,0.0191338,"r Computational Linguistics language. In addition, if we can move to more general models then we could hope to capture more, as well. However, due to data sparsity, it is hard to estimate a robust n-gram distribution for large values of n ( say, n &gt; 10) using the conventional Maximum Likelihood techniques, unless a more robust technique is employed for modeling which generalizes well on unseen events. Some of these well known long span / complex language models which have shown to perform very well on many speech tasks include: structured language model (Chelba and Jelinek, 2000; Roark, 2001; Wang and Harper, 2002; Filimonov and Harper, 2009), latent semantic analysis language model (Bellegarda, 2000), topic mixture language models (Iyer and Ostendorf, 1999), whole sentence exponential language models (Rosenfeld, 1997; Rosenfeld et al., 2001), feedforward neural networks (Bengio et al., 2001), recurrent neural network language models (Mikolov et al., 2010), among many others. Although better modeling techniques can now capture longer dependencies in a language, their incorporation in decoders of speech recognition or machine translation systems becomes computationally challenging. Due to the prohibitiv"
H05-1089,J93-1003,0,0.15514,"ncy table given the sample, the margins (document frequencies) and the size of the collection. Not unsurprisingly, computational work and statistical accuracy (variance or errors) depend on sampling rate, as will be shown both theoretically and empirically. Sampling methods become more and more important with larger and larger collections. At Web scale, sampling rates as low as 10−4 may suffice. 1 Introduction Word associations (co-occurrences) have a wide range of applications including: Speech Recognition, Optical Character Recognition and Information Retrieval (IR) (Church and Hanks, 1991; Dunning, 1993; Manning and Schutze, 1999). It is easy to compute association scores for a small corpus, but more challenging to compute lots of scores for lots of data (e.g. the Web), with billions of web pages (D) and millions of word types (V ). For a small corpus, one could compute pair-wise associations by multiplying the (0/1) term-by-document matrix with its transpose (Deerwester et al., 1999). But this is probably infeasible at Web scale. 1 This work was conducted at Microsoft while the first author was an intern. The authors thank Chris Meek, David Heckerman, Robert Moore, Jonathan Goldstein, Trevo"
H05-1089,H05-1089,1,0.0512034,"Missing"
H05-1089,W04-3243,0,0.278801,"Missing"
H05-1089,P05-1077,0,0.13701,"e at Web scale. 1 This work was conducted at Microsoft while the first author was an intern. The authors thank Chris Meek, David Heckerman, Robert Moore, Jonathan Goldstein, Trevor Hastie, David Siegmund, Art Own, Robert Tibshirani and Andrew Ng. Kenneth W. Church Microsoft Research One Microsoft Way Redmond, Washington 98052 church@microsoft.com Approximations are often good enough. We should not have to look at every document to determine that two words are strongly associated. A number of sampling-based randomized algorithms have been implemented at Web scale (Broder, 1997; Charikar, 2002; Ravichandran et al., 2005).2 A conventional random sample is constructed by selecting Ds documents from a corpus of D documents. The (corpus) sampling rate is DDs . Of course, word distributions have long tails. There are a few high frequency words and many low frequency words. It would be convenient if the sampling rate could vary from word to word, unlike conventional sampling where the sampling rate is fixed across the vocabulary. In particular, in our experiments, we will impose a floor to make sure that the sample contains at least 20 documents for each term. (When working at Web scale, one might raise the floor s"
H05-1089,J90-1003,1,\N,Missing
H89-2012,A88-1019,1,0.671564,"t ; o f f ) = 3.7, indicating that the probability of set ... off is 23.7 = 13 times greater than chance. This association is relatively strong; the other particles that Sinclair mentions have scores of: about (-0.9), in (0.6), up (4.6), out (2.2), on (1.0) in the 1987 AP Corpus of 15 million words. 3. Preprocessing the Corpus with a Part of Speech Tagger Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to. We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988) or DeRose (1988), and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition to~in and verbs associated with a following infinitive marker to~to. (Part of speech notation is borrowed from Francis and Kucera (1982); m = preposition; to = infinitive marker; vb = bare verb; vbg = verb + ing; vbd = verb + ed; vbz = verb + s; vbn = verb + en.) The score identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 3.0 or more, there are 768 verbs as"
H89-2012,P89-1010,1,0.628728,"symbol pushing), and need not require explicit semantic interpretation. We have found that it is possible to identify many of these interesting co-occurrence relations by computing simple summary statistics over millions of words of text. This paper will summarize a number of experiments carried out by various subsets of the authors over the last few years. The term collocation will be used quite broadly to include constraints on SVO (subject verb object) triples, phrasal verbs, compound noun phrases, and psycholinguistic notions of word association (e.g., doctor~nurse). 1. Mutual Information Church and Hanks (1989) discussed the use of the mutual information statistic in order to identify a variety of interesting linguistic phenomena, ranging from semantic relations of the doctor/nurse type (content word/content word) to lexico-syntacfic co-occurrence constraints between verbs and prepositions (content word/function word). Mutual information, l(x;y), compares the probability of observing word x and word y together (the joint probability) with the probabilities of observing x and y independently (chance). l(x;y) -= log 2 P(x,y) e(x) e(y) If there is a genuine association between x and y, then the joint p"
H89-2012,J88-1003,0,0.0196352,", indicating that the probability of set ... off is 23.7 = 13 times greater than chance. This association is relatively strong; the other particles that Sinclair mentions have scores of: about (-0.9), in (0.6), up (4.6), out (2.2), on (1.0) in the 1987 AP Corpus of 15 million words. 3. Preprocessing the Corpus with a Part of Speech Tagger Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to. We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988) or DeRose (1988), and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition to~in and verbs associated with a following infinitive marker to~to. (Part of speech notation is borrowed from Francis and Kucera (1982); m = preposition; to = infinitive marker; vb = bare verb; vbg = verb + ing; vbd = verb + ed; vbz = verb + s; vbn = verb + en.) The score identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 3.0 or more, there are 768 verbs associated with the"
H89-2012,P85-1037,0,\N,Missing
H89-2013,W89-0240,1,0.815416,"t makes it possible to summarize the data so concisely that the relevant structure can be observed in a simple plot. Moreover, jii has a natural order and is continuous, so the number of bins can be adjusted for accuracy. In contrast, selecting the first word of the n-gram prescribes the number of bins. The second point, the calculation of variances, is often not discussed in the literature on using the Good-Turing model for language modeling. Variances are necessary to make statements about the statistical significance of differences between observed and predicted frequencies. In other work (Church, Gale, Hanks, and Hindle, 1989), we have used variances to distinguish unusual n-grams from chance. The third point we want to emphasize, the use of relined tests for differences in methods, is discussed in section 4. Four methods, MLE, UE, CC, and GT, were compared to the standard, t-scores were calculated for the differences between the standard and a proposed method and aggregate results across jii. We find that the GT method rapidly approaches ideal performance, though it is outperformed by CC when r is very small, presumably because the binomial assumption is apparently not quite satisfied for small frequencies. There"
H89-2013,P80-1024,0,\N,Missing
H90-1056,1988.tmi-1.19,0,0.052844,"Missing"
H90-1056,C90-2036,1,\N,Missing
H91-1026,J90-2002,0,0.646366,"Missing"
H91-1026,A88-1019,1,0.440746,"Missing"
H91-1026,C88-2142,0,0.0799563,"Missing"
H91-1026,C90-3031,0,\N,Missing
H91-1026,P91-1022,0,\N,Missing
H91-1026,P91-1023,1,\N,Missing
H92-1045,P91-1034,0,0.0356017,"rce handtagged materials for use in testing and training. We have achieved considerable progress recently by using a new source of testing and training materials and the application of Bayesian discrimination methods. Rather than depending on small amounts of hand-tagged text, we have been making use of relatively large amounts of parallel text, text such as the Canadian Hansards, which are available in multiple languages. The translation can 233 often be used in lieu of hand-labeling. For example, consider the polysemous word sentence, which has two major senses: (1) a judicial sentence, and (2), a syntactic sentence. We can collect a number of sense (1) examples by extracting instances that are translated as peine, and we can collect a number of sense (2) examples by extracting instances that are translated as phrase. In this way, we have been able to acquire a considerable amount of testing and training material for developing and testing our disambiguation algorithms. The use of bilingual materials for discrimination decisions in machine tranlation has been discussed by Brown and others (1991), and by Dagan, Itai, and Schwall (1991). The use of bilingual materials for an essential"
H92-1045,P91-1017,0,0.00816638,"Missing"
H92-1045,C92-2070,1,\N,Missing
J01-1001,C94-1101,0,0.142347,". The suffix array data structure makes it convenient to compute the frequency and location of a substring (n-gram) in a long sequence (corpus). The early work was motivated by biological applications such as matching of DNA sequences. Suffix arrays are closely related to PAT arrays (Gonnet, Baeza-Yates, and Snider 1992), which were motivated in part by a project at the University of Waterloo to distribute the Oxford English Dictionary with indexes on CD-ROM. PAT arrays have also been motivated by applications in information retrieval. A similar data structure to suffix arrays was proposed by Nagao and Mori (1994) for processing Japanese text. The alphabet sizes vary considerably in each of these cases. DNA has a relatively small alphabet of just 4 characters, whereas Japanese has a relatively large alphabet of more than 5,000 characters. The methods such as suffix arrays and PAT arrays scale naturally over alphabet size. In the experimental section (Section 3) using the Wall Street Journal corpus, the suffix array is applied to a large corpus of English text, where the alphabet is assumed to be the set of all English words, an unbounded set. It is sometimes assumed that larger alphabets are more chall"
J01-1001,W96-0205,0,0.0265018,"Missing"
J01-1001,H92-1073,0,0.0236272,"the substrings, &quot;to_&quot; and &quot;to_b&quot;. Although there may be N(N + 1)/2 ways to pick i and j, it will turn out that we need only consider 2N - 1 of them when computing tf for all substrings. Nagao and Mori (1994) ran this procedure quite successfully on a large corpus of Japanese text. They report that it takes O(NlogN) time, assuming that the sort step performs O(N log N) comparisons, and that each comparison takes constant time. While these are often reasonable assumptions, we have found that if the corpus contains long repeated substrings (e.g., duplicated articles), as our English corpus does (Paul and Baker 1992), then the sort can consume quadratic time, since each comparison can take order N time. Like Nagao and Mori (1994), we were also able to apply this procedure quite successfully to our Japanese corpus, but for the English corpus, after 50 hours of CPU time, we gave up and turned to Manber and Myers&apos;s (1990) algorithm, which took only two hours. 1 Manber and Myers&apos; algorithm uses some clever, but difficult to describe, techniques to achieve O(N log N) time, even for a corpus with long repeated substrings. For a corpus that would otherwise consume quadratic time, the Manber and Myers algorithm i"
J01-1001,J90-1003,1,\N,Missing
J07-3003,J93-1003,0,0.234117,"n estimate the contingency table for the entire population using straightforward scaling. However, one can do better by taking advantage of the margins (also known as document frequencies). The proposed method cuts the errors roughly in half over Broder’s sketches. 1. Introduction We develop an algorithm for efficiently computing associations, for example, word associations.1 Word associations (co-occurrences, or joint frequencies) have a wide range of applications including: speech recognition, optical character recognition, and information retrieval (IR) (Salton 1989; Church and Hanks 1991; Dunning 1993; Baeza-Yates and Ribeiro-Neto 1999; Manning and Schutze 1999). The Know-It-All project computes such associations at Web scale (Etzioni et al. 2004). It is easy to compute a few association scores for a small corpus, but more challenging to compute lots of scores for lots of data (e.g., the Web), with billions of Web pages (D) and millions of word types. Web search engines produce estimates of page hits, as illustrated in Tables 1– 3.2 Table 1 shows hits for two high frequency words, a and the, suggesting that the total number of English documents is roughly D ≈ 1010 . In addition to the two"
J07-3003,H05-1089,1,0.216088,"Missing"
J07-3003,P05-1077,0,0.305555,"Missing"
J07-3003,J90-1003,1,\N,Missing
J07-3003,W04-3243,0,\N,Missing
J90-1003,W89-0240,1,0.147538,"th the preposition to~in and 551 verbs with the infinitive marker to/to. The ten verbs found to be most associated before to/in are: • to~in: alluding/vbg, adhere/vb, amounted/vbn, relating/ vbg, amounting/vbg, revert/vb, reverted/vbn, resorting/ vbg, relegated/vbn • to~to: obligated/vbn, trying/vbg, compelled/vbn, enables/vbz, supposed/vbn, intends/vbz, vowing/vbg, tried/vbd, enabling/vbg, tends/vbz, tend/vb, intend/vb, tries/vbz Thus, we see there is considerable leverage to be gained by preprocessing the corpus and manipulating the inventory of tokens. 7 PREPROCESSING WITH A PARSER Hindle (Church et al. 1989) has found it helpful to preprocess the input with the Fidditch parser (Hindle 1983a, 1983b) to identify associations between verbs and arguments, and postulate semantic classes for nouns on this basis. Hindle&apos;s method is able to find some very interesting associations, as Tables 5 and 6 demonstrate. After running his parser over the 1988 AP corpus (44 million words), Hindle found N = 4,112,943 subject/verb/ object (SVO) triples. The mutual information between a verb and its object was computed from these 4 million triples by counting how often the verb and its object were found in the same tr"
J90-1003,J90-1003,1,0.148116,"Missing"
J90-1003,H89-2012,1,\N,Missing
J90-1003,A88-1019,1,\N,Missing
J90-1003,P83-1019,0,\N,Missing
J93-1001,J90-3007,0,0.0413581,"to the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makerswwide reading and experience of English, other dictionaries and of course eyes and ears--this dictionary is based on hard, measurable evidence. (Sinclair et al. 1987; p. xv) The experience of writing the COBUILD dictionary is documented in Sinclair (1987), a collection of articles from the COBUILD project; see Boguraev (1990) for a strong positive review of this collection. At the time, the corpus-based approach to lexicography was considered pioneering, even somewhat controversial; today, quite a number of the major lexicography houses are collecting large amounts of corpus data. The traditional alternative to corpora are citation indexes, boxes of interesting citations collected on index cards by large numbers of human readers. Unfortunately, citation indexes tend to be a bit like butterfly collections, full of rare and unusual specimens, but severely lacking in ordinary, garden-variety moths. Murray, the editor"
J93-1001,J90-2002,1,0.364969,"ted over a 27-character alphabet. 3 Lari and Young actually looked at another task involving phonotactic structure where there is also good reason to believe that SCFGs might be able to capture crucial linguistic constraints that might be missed by simpler HMMs. 15 Computational Linguistics Volume 19, Number 1 has tended to favor rationalism, though there are some important exceptions, such as example-based MT (Sato and Nagao 1990). The issue remains as controversial as ever, as evidenced by the lively debate on rationalism versus empiricism at TMI-92, a recent conference on MT.4 The paper by Brown et al. (1990) revives Weaver's information theoretic approach to MT. It requires a bit more squeezing and twisting to fit machine translation into the noisy channel mold: to translate, for example, from French to English, one imagines that the native speaker of French has thought up what he or she wants to say in English and then translates mentally into French before actually saying it. The task of the translation system is to recover the original English, E, from the observed French, F. While this may seem a bit far-fetched, it differs little in principle from using English as an interlingua or as a mean"
J93-1001,J92-1002,1,0.763814,"hes. 5. Machine Translation and Bilingual Lexicography IS machine translation (MT) more suitable for rationalism or empiricism? Both approaches have been investigated. Weaver (1949) was the first to propose an information theoretic approach to MT. The empirical approach was also practiced at G e o r g e t o w n during the 1950s and 1960s (Henisz-Dostert, Ross Macdonald, and Zarechnak 1979) in a system that eventually became k n o w n as SYSTRAN. Recently, most w o r k in MT 2 In fact, the trigram model might be even better than suggested in Table 5, since the estimate for the trigram model in Brown et al. (1992)is computed over a 256-characteralphabet, whereas the estimate for human performance in Shannan (1951) is computed over a 27-character alphabet. 3 Lari and Young actually looked at another task involving phonotactic structure where there is also good reason to believe that SCFGs might be able to capture crucial linguistic constraints that might be missed by simpler HMMs. 15 Computational Linguistics Volume 19, Number 1 has tended to favor rationalism, though there are some important exceptions, such as example-based MT (Sato and Nagao 1990). The issue remains as controversial as ever, as evide"
J93-1001,A88-1019,1,0.634765,"Missing"
J93-1001,P90-1031,0,0.0370696,"Missing"
J93-1001,J88-1003,0,0.0610669,"Missing"
J93-1001,P89-1015,0,0.0329604,"Missing"
J93-1001,C90-3030,0,0.0354528,"Missing"
J93-1001,J93-2005,0,0.0604417,"lingual Lexicography, Machine-Readable Dictionaries (MRDs), and Computational Lexicons There has been a long tradition of empiricist approaches in lexicography, both bilingual and monolingual, dating back to Johnson and Murray. As corpus data and machinereadable dictionaries (MRDs) become more and more available, it is becoming easier to compile lexicons for computers and dictionaries for people. This is a particularly exciting area in computational linguistics as evidenced by the large number of contributions in these special issues: Biber (1993), Brent (1993), Hindle and Rooth (this issue), Pustejovsky et al. (1993), and Smadja (this issue). Starting with the COBUILD dictionary (Sinclair et al. 1987), it is now becoming more and more common to find lexicographers working directly with corpus data. Sinclair makes an excellent case for the use of corpus evidence in the preface to the COBUILD dictionary: For the first time, a dictionary has been compiled by the thorough examination of a representative group of English texts, spoken and written, running to many millions of words. This means that in addition to all the tools of the conventional dictionary makerswwide reading and experience of English, other d"
J93-1001,J82-2005,0,0.111046,"is more directly relevant than in word recognition. In general, phrase structure is probably more important for understanding w h o did what to w h o m , than recognizing what was said. 3 Some tasks are probably more appropriate for Chomsky's rational approach to language and other tasks are probably more appropriate for Shannon's empirical approach to language. Table 6 summarizes some of the differences between the two approaches. 5. Machine Translation and Bilingual Lexicography IS machine translation (MT) more suitable for rationalism or empiricism? Both approaches have been investigated. Weaver (1949) was the first to propose an information theoretic approach to MT. The empirical approach was also practiced at G e o r g e t o w n during the 1950s and 1960s (Henisz-Dostert, Ross Macdonald, and Zarechnak 1979) in a system that eventually became k n o w n as SYSTRAN. Recently, most w o r k in MT 2 In fact, the trigram model might be even better than suggested in Table 5, since the estimate for the trigram model in Brown et al. (1992)is computed over a 256-characteralphabet, whereas the estimate for human performance in Shannan (1951) is computed over a 27-character alphabet. 3 Lari and Young"
J93-1004,1988.tmi-1.19,0,0.0283918,"basis as d o e s that of Brown, Lai, a n d Mercer, while the t w o differ c o n s i d e r a b l y f r o m the lexical a p p r o a c h e s tried b y K a y a n d R6scheisen a n d b y Catizone, Russell, a n d Warwick. The feasibility of o t h e r m e t h o d s has v a r i e d greatly. K a y &apos; s a p p r o a c h is a p p a r e n t l y quite slow. A t least, w i t h the c u r r e n t l y inefficient i m p l e m e n t a t i o n , it m i g h t take h o u r s 2 After we finished most of this work, it came to our attention that the IBM MT group has at least four papers that mention sentence alignment. (Brown et al. 1988a,b) start from a set of aligned sentences, suggesting that they had a solution to the sentence alignment problem back in 1988. Brown et al. (1990) mention that sentence lengths formed the basis of their method. The draft by Brown, Lai, and Mercer (1991) describes their process without giving equations. 77 Computational Linguistics Volume 19, Number 1 Table 4 A bilingual concordance. b a n k / b a n q u e (""money"" sense) it could also be a place where we would have a bank of experts. SENT i k n o w several people w h o a ftre le lieu oti se retrouverait une esp6ce de banque d&apos; experts. SENT je"
J93-1004,C88-1016,0,0.0326748,"basis as d o e s that of Brown, Lai, a n d Mercer, while the t w o differ c o n s i d e r a b l y f r o m the lexical a p p r o a c h e s tried b y K a y a n d R6scheisen a n d b y Catizone, Russell, a n d Warwick. The feasibility of o t h e r m e t h o d s has v a r i e d greatly. K a y &apos; s a p p r o a c h is a p p a r e n t l y quite slow. A t least, w i t h the c u r r e n t l y inefficient i m p l e m e n t a t i o n , it m i g h t take h o u r s 2 After we finished most of this work, it came to our attention that the IBM MT group has at least four papers that mention sentence alignment. (Brown et al. 1988a,b) start from a set of aligned sentences, suggesting that they had a solution to the sentence alignment problem back in 1988. Brown et al. (1990) mention that sentence lengths formed the basis of their method. The draft by Brown, Lai, and Mercer (1991) describes their process without giving equations. 77 Computational Linguistics Volume 19, Number 1 Table 4 A bilingual concordance. b a n k / b a n q u e (""money"" sense) it could also be a place where we would have a bank of experts. SENT i k n o w several people w h o a ftre le lieu oti se retrouverait une esp6ce de banque d&apos; experts. SENT je"
J93-1004,J90-2002,0,0.907539,"hat the method will be useful for many language pairs. To further research on bilingual corpora, a much larger sample of Canadian Hansards (approximately 90 million words, half in English and and half in French) has been aligned with the align program and will be available through the Data Collection Initiative of the Association for Computational Linguistics (ACL/DCI). In addition, in order to facilitate replication of the align program, an appendix is provided with detailed c-code of the more difficult core of the align program. 1. Introduction Researchers in both machine translation (e.g., Brown et al. 1990) and bilingual lexicography (e.g., Klavans and Tzoukermann 1990) have recently become interested in studying bilingual corpora, bodies of text such as the Canadian Hansards (parliamentary debates), which are available in multiple languages (such as French and English). The sentence alignment task is to identify correspondences between sentences in one * AT&T Bell Laboratories600 Mountain Avenue Murray Hill, NJ, 07974 (~) 1993 Associationfor Computational Linguistics Computational Linguistics Volume 19, Number 1 Table 1 Input to alignment program. English According to our survey, 1988 sales of"
J93-1004,P91-1022,0,0.427368,"Missing"
J93-1004,C88-2142,0,0.0492415,"Missing"
J93-1004,C90-3031,0,\N,Missing
J93-1004,A88-1019,1,\N,Missing
lin-etal-2010-new,N04-1043,0,\N,Missing
lin-etal-2010-new,sekine-dalwani-2010-ngram,1,\N,Missing
lin-etal-2010-new,C08-3010,1,\N,Missing
lin-etal-2010-new,1999.tc-1.8,0,\N,Missing
lin-etal-2010-new,J93-2004,0,\N,Missing
lin-etal-2010-new,S07-1044,0,\N,Missing
lin-etal-2010-new,N07-2005,1,\N,Missing
lin-etal-2010-new,J92-4003,0,\N,Missing
lin-etal-2010-new,J03-3005,0,\N,Missing
lin-etal-2010-new,P08-1068,0,\N,Missing
lin-etal-2010-new,P03-1059,0,\N,Missing
lin-etal-2010-new,A00-1031,0,\N,Missing
lin-etal-2010-new,P01-1005,0,\N,Missing
lin-etal-2010-new,W05-0603,0,\N,Missing
lin-etal-2010-new,P09-1116,1,\N,Missing
lin-etal-2010-new,Y09-1024,1,\N,Missing
lin-etal-2010-new,U08-1008,0,\N,Missing
lin-etal-2010-new,I05-2018,0,\N,Missing
lin-etal-2010-new,W04-3205,0,\N,Missing
lin-etal-2010-new,J03-3001,0,\N,Missing
lin-etal-2010-new,U07-1008,0,\N,Missing
N07-2005,P05-3024,1,\N,Missing
P11-1135,P05-1074,0,0.0311999,"nd Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also proposed exploiting monolingual-view and bilingualview predictors. In their work, the bilingual view encodes the per-instance agreem"
P11-1135,P10-1089,1,0.845045,"ual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information as additional features. Adding new fea1354 tures without adding new training data is often problematic, but is promising in our framework, since the bitexts provide so much indirect supervision. 10 Conclusion Resolving coordination ambiguity is hard. Parsers are reporting impressive numbers these days, but coordination remains an area with room for improvement. We focused on a speci"
P11-1135,D08-1092,0,0.0399049,"a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used m"
P11-1135,W10-2906,0,0.0145568,"word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also proposed exploiting monolingual-view and bilingualview predictors. In their work, the bilingual view encodes the per-instance agreement between monolingual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolv"
P11-1135,C90-3063,0,0.177758,"Missing"
P11-1135,P91-1017,0,0.234031,"s were the same or slightly better with 10 or 100 instances. work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both"
P11-1135,2008.amta-srw.2,0,0.171319,"is accuracy carried over to new domains, where bilingual features are not available. We test the robustness of our co-trained monolingual classifier by evaluating it on our labeled WSJ data. The Penn Treebank and the annotations added by Vadas and Curran (2007a) comprise a very special corpus; such data is clearly not available in every domain. We can take advantage of the plentiful labeled examples to also test how our co-trained system compares to supervised systems trained with in1353 Bilingual data has been used to resolve a range of ambiguities, from PP-attachment (Schwartz et al., 2003; Fossum and Knight, 2008), to distinguishing grammatical roles (Schwarck et al., 2010), to full dependency parsing (Huang et al., 2009). Related 4 Nakov and Hearst (2005) use an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5 Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results"
P11-1135,J93-1005,0,0.58057,"Missing"
P11-1135,P07-1086,0,0.245976,"equire a distinct type of reordering when translated into a foreign language. Since coordination is both complex and productive, parsers and machine translation (MT) systems cannot simply memorize the analysis of coordinate phrases from training text. We propose an approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one"
P11-1135,D09-1127,0,0.0370632,"Missing"
P11-1135,2005.mtsummit-papers.11,0,0.116641,"ish text with a corresponding translation in one or more target languages. A variety of mature NLP tools exists in this domain, allowing us to robustly align the parallel text first at the sentence and then at the word level. Given a word-aligned parallel corpus, we can see how the different types of coordinate NPs are translated in the target languages. In Romance languages, examples with ellipsis, such as dairy and meat production (Table 1), tend to correspond to translations with the head in the first position, e.g. “producci´on l´actea y c´arnica” in Spanish (examples taken from Europarl (Koehn, 2005)). When there is no ellipsis, the head-first syntax leads to the “w1 and h w2 ” ordering, e.g. amianto e o cloreto de polivinilo in Portuguese. Another clue for ellipsis is the presence of a dangling hyphen, as in the Finnish maidon- ja lihantuotantoon. We find such hyphens especially common in Germanic languages like Dutch. In addition to language-specific clues, a translation may resolve an ambiguity by paraphrasing the example in the same way it may be paraphrased in English. E.g., we see hard and soft drugs translated into Spanish as drogas blandas y drogas duras with the head, drogas, rep"
P11-1135,P04-1060,0,0.0228937,"and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extracting paraphrases. Our co-training algorithm is well suited to using multiple bitexts because it automatically learns the value of alignment information in each language. In addition, our approach copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also"
P11-1135,P95-1007,0,0.0125732,"Missing"
P11-1135,lin-etal-2010-new,1,0.805628,"rove faster. This is desirable because we don’t have unlimited unlabeled examples to draw from, only those found in our parallel text. 5 Data Web-scale text data is used for monolingual feature counts, parallel text is used for classifier co-training, and labeled data is used for training and evaluation. Web-scale N-gram Data We extract our counts from Google V2: a new N-gram corpus (with N-grams of length one-to-five) created from the same one-trillion-word snapshot of the web as the Google 5-gram Corpus (Brants and Franz, 2006), but with enhanced filtering and processing of the source text (Lin et al., 2010, Section 5). We get counts using the suffix array tools described in (Lin et al., 2010). We add one to all counts for smoothing. Parallel Data We use the Danish, German, Greek, Spanish, Finnish, French, Italian, Dutch, Portuguese, and Swedish portions of Europarl (Koehn, 2005). We also use the Czech, German, Spanish and French news commentary data from WMT 2010.1 Word-aligned English-Foreign bitexts are created using the Berkeley aligner. 2 We run 5 iterations of joint IBM Model 1 training, followed by 3to-5 iterations of joint HMM training, and align with the competitive-thresholding heurist"
P11-1135,J93-2004,0,0.0386474,"Missing"
P11-1135,H05-1105,0,0.392611,"phrases from training text. We propose an approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one classifier by learning from the predictions of the other. Starting from only two 1346 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346–1355, c Portland, Oregon, June 19-24, 2011. 20"
P11-1135,C10-1100,1,0.837257,"approach to recognizing ellipsis that could benefit both MT and other NLP technology that relies on shallow or deep syntactic analysis. While the general case of coordination is quite complicated, we focus on the special case of complex NPs. Errors in NP coordination typically account for the majority of parser coordination errors (Hogan, 2007). The information needed to resolve coordinate NP ambiguity cannot be derived from hand-annotated data, and we follow previous work in looking for new information sources to apply to this problem (Resnik, 1999; Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010). We first resolve coordinate NP ambiguity in a word-aligned parallel corpus. In bitexts, both monolingual and bilingual information can indicate NP structure. We create separate classifiers using monolingual and bilingual feature views. We train the two classifiers using co-training, iteratively improving the accuracy of one classifier by learning from the predictions of the other. Starting from only two 1346 Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics, pages 1346–1355, c Portland, Oregon, June 19-24, 2011. 2011 Association for Computational Linguis"
P11-1135,N10-1113,0,0.224357,"Missing"
P11-1135,2003.mtsummit-papers.44,0,0.386025,"ld be even better if this accuracy carried over to new domains, where bilingual features are not available. We test the robustness of our co-trained monolingual classifier by evaluating it on our labeled WSJ data. The Penn Treebank and the annotations added by Vadas and Curran (2007a) comprise a very special corpus; such data is clearly not available in every domain. We can take advantage of the plentiful labeled examples to also test how our co-trained system compares to supervised systems trained with in1353 Bilingual data has been used to resolve a range of ambiguities, from PP-attachment (Schwartz et al., 2003; Fossum and Knight, 2008), to distinguishing grammatical roles (Schwarck et al., 2010), to full dependency parsing (Huang et al., 2009). Related 4 Nakov and Hearst (2005) use an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5 Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 tr"
P11-1135,W04-3207,0,0.0610601,"tler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Ca"
P11-1135,P09-1009,0,0.0314347,"heir count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge about word senses.” Kuhn (2004) used alignments across several Europarl bitexts to devise rules for identifying parse distituents. Bannard and Callison-Burch (2005) used multiple bitexts as part of a system for extrac"
P11-1135,P07-1031,0,0.375712,"(Resnik, 1999; Nakov and Hearst, 2005). Although pairs of adjectives are usually conjoined (and mixed tags are usually not), this is not always true, as in older/younger above. For comparison, we also state accuracy on the noun-only examples (§ 8). Our task is more narrow than the task tackled by full-sentence parsers, but most parsers do not bracket NP-internal structure at all, since such structure is absent from the primary training corpus for statistical parsers, the Penn Treebank (Marcus et al., 1993). We confirm that standard broad-coverage parsers perform poorly on our task (§ 7). 1347 Vadas and Curran (2007a) manually annotated NP structure in the Penn Treebank, and a few custom NP parsers have recently been developed using this data (Vadas and Curran, 2007b; Pitler et al., 2010). Our task is more narrow than the task handled by these parsers since we do not handle other, less-frequent and sometimes more complex constructions (e.g. robot arms and legs). However, such constructions are clearly amenable to our algorithm. In addition, these parsers have only evaluated coordination resolution within base NPs, simplifying the task and rendering the aforementioned older/younger problem moot. Finally,"
P11-1135,P08-1039,0,0.0454381,"Missing"
P11-1135,J97-3002,0,0.114812,"ch copes with noisy alignments both by aggregating information across languages (and repeated occurrences within a language), and by only selecting the most confident examples at each iteration. Burkett et al. (2010) also proposed exploiting monolingual-view and bilingualview predictors. In their work, the bilingual view encodes the per-instance agreement between monolingual predictors in two languages, while our bilingual view encodes the alignment and target text together, across multiple instances and languages. The other side of the coin is the use of syntax to perform better translation (Wu, 1997). This is a rich field of research with its own annual workshop (Syntax and Structure in Translation). Our monolingual model is most similar to previous work using counts from web-scale text, both for resolving coordination ambiguity (Nakov and Hearst, 2005; Rus et al., 2007; Pitler et al., 2010), and for syntax and semantics in general (Lapata and Keller, 2005; Bergsma et al., 2010). We do not currently use semantic similarity (either taxonomic (Resnik, 1999) or distributional (Hogan, 2007)) which has previously been found useful for coordination. Our model can easily include such information"
P11-1135,N01-1026,1,0.757416,"se an unsupervised algorithm that predicts ellipsis on the basis of a majority vote over a number of pattern counts and established heuristics. 5 Pitler et al. (2010) uses a supervised classifier to predict bracketings; their count and binary features are a strict subset of the features used in our Monolingual classifier. 6 For co-training, we tuned k on the WSJ dev set but left other parameters the same. We start from 2 training instances; results were the same or slightly better with 10 or 100 instances. work has also focused on projecting syntactic annotations from one language to another (Yarowsky and Ngai, 2001; Hwa et al., 2005), and jointly parsing the two sides of a bitext by leveraging the alignments during training and testing (Smith and Smith, 2004; Burkett and Klein, 2008) or just during training (Snyder et al., 2009). None of this work has focused on coordination, nor has it combined bitexts with web-scale monolingual information. Most prior work has focused on leveraging the alignments between a single pair of languages. Dagan et al. (1991) first articulated the need for “a multilingual corpora based system, which exploits the differences between languages to automatically acquire knowledge"
P11-1135,P95-1026,1,0.30405,"ning Create Lm ← L Create Lb ← L Create a pool Um by choosing um examples randomly from U . Create a pool Ub by choosing ub examples randomly from U . for i = 0 to k do Use Lm to train a classifier hm using only x ¯m , the monolingual features of x ¯ Use Lb to train a classifier hb using only x ¯b , the bilingual features of x ¯ Use hm to label Um , move the nm most-confident examples to Lb Use hb to label Ub , move the nb most-confident examples to Lm Replenish Um and Ub randomly from U with nm and nb new examples end for uncertain, and vice versa. This suggests using a co-training approach (Yarowsky, 1995; Blum and Mitchell, 1998). We train separate classifiers on the labeled data. We use the predictions of one classifier to label new examples for training the orthogonal classifier. We iterate this training and labeling. We outline how this procedure can be applied to bitext data in Algorithm 1 (above). We follow prior work in drawing predictions from smaller pools, Um and Ub , rather than from U itself, to ensure the labeled examples “are more representative of the underlying distribution” (Blum and Mitchell, 1998). We use a logistic regression classifier for hm and hb . Like Blum and Mitchel"
P11-1135,P07-2009,0,\N,Missing
P19-1399,P17-1042,0,0.0313182,"ntly, Mikolov et al. (2013) observe that isomorphism exists across word embeddings of different languages. This motivates them to learn a linear mapping to align the spaces, using a seeding dictionary of 5K pairs of translations. After that, more translations can be induced by NN search. Following the seminal work, significant advances have been made. For example, Faruqui and Dyer (2014) use Canonical Component Analysis to align the two embedding spaces. Xing et al. (2015) show a substantial gain by normalizing the embeddings and constraining the mapping to be orthogonal. A series of works by Artetxe et al. (2017, 2018a,b) show that decent accuracies can be achieved even with a tiny or no seeding dictionary. The authors name their method as “self-learning”, which alternates between learning the mapping and inducing more translation pairs. The similar methodology is also seen in (Zhang et al., 2017b), where the induction step reduces a cost called earth mover distance. Conneau et al. (2018) propose to use Generative Adversarial Network (Goodfellow et al., 2014) to learn the mapping when no seeding dictionary is available. Whether using a seeding dictionary or not, the induction always requires to retri"
P19-1399,P18-1073,0,0.0362449,"m NN, ISF and other state-of-the-art like CSLS. In summary, the paper makes the following contributions: 1. We propose an optimization based framework that connects NN, ISF and the proposed HNN. 2. We derive an efficient solver for HNN, which outperforms NN, ISF and other state-of-theart. 3. We show that ISF is a part of HNN’s solver, which explains why ISF works. 2 Bilingual Lexicon Induction with a Seeding Dictionary Since hubness is the major concern of this work, we focus on the case with a seeding dictionary for simplicity. Representative methods (Mikolov et al., 2013; Xing et al., 2015; Artetxe et al., 2018a) often consist of two steps: 1) learning a mapping that aligns the source and target embedding spaces; 2) given a source word, retrieve according to some distance metric, in the target embedding space. We briefly review the two steps in this section. Let the source word embeddings space be X ⊂ Rd , and the target space be Y ⊂ Rd . A typical value of the dimension d is 300. Suppose the vocabulary sizes of source/target languages are m and n respectively. Then X is a set of m embedding vectors, denoted as X = {x1 , . . . , xm } , and Y is a set of n embeddings, Y = {y1 , . . . , yn } . 2.1 Lea"
P19-1399,E14-1049,0,0.0280497,"cross languages. For example, cooccurrence matrices (Rapp, 1995), tf-idf of context words (Fung, 1998), and lexical similarities (Klementiev et al., 2012). A comprehensive study can be found in (Irvine and Callison-Burch, 2017). Recently, Mikolov et al. (2013) observe that isomorphism exists across word embeddings of different languages. This motivates them to learn a linear mapping to align the spaces, using a seeding dictionary of 5K pairs of translations. After that, more translations can be induced by NN search. Following the seminal work, significant advances have been made. For example, Faruqui and Dyer (2014) use Canonical Component Analysis to align the two embedding spaces. Xing et al. (2015) show a substantial gain by normalizing the embeddings and constraining the mapping to be orthogonal. A series of works by Artetxe et al. (2017, 2018a,b) show that decent accuracies can be achieved even with a tiny or no seeding dictionary. The authors name their method as “self-learning”, which alternates between learning the mapping and inducing more translation pairs. The similar methodology is also seen in (Zhang et al., 2017b), where the induction step reduces a cost called earth mover distance. Conneau"
P19-1399,W11-0318,0,0.0738709,"Missing"
P19-1399,P95-1050,0,0.568758,"lexicon of translation equivalents such as, bank:banc or bank:banque automatically from non-parallel corpora. The proposed method not only improves upon but also unifies several recent works that retrieve translations by Nearest Neighbor (NN) (Mikolov et al., 2013) and more advanced techniques like Inverted SoFtmax (ISF) (Smith et al., 2017). There is a long history of BLI using nonparallel corpora. Methods often rely on some designed features, which reveal some shared 1 https://github.com/baidu-research/HNN † qiang.qiu@duke.edu structures across languages. For example, cooccurrence matrices (Rapp, 1995), tf-idf of context words (Fung, 1998), and lexical similarities (Klementiev et al., 2012). A comprehensive study can be found in (Irvine and Callison-Burch, 2017). Recently, Mikolov et al. (2013) observe that isomorphism exists across word embeddings of different languages. This motivates them to learn a linear mapping to align the spaces, using a seeding dictionary of 5K pairs of translations. After that, more translations can be induced by NN search. Following the seminal work, significant advances have been made. For example, Faruqui and Dyer (2014) use Canonical Component Analysis to alig"
P19-1399,D13-1058,0,0.0270405,"ranslation under some distance measure. NN may be the most straightforward approach. However, it is often challenged by a phenomenon called hubness (Radovanovic et al., 2010). Hubness is a tendency that a few words (hubs) are too near to too many other words, especially in high dimensional spaces. It degrades the accuracy of NN in various tasks (Aucouturier and Pachet, 2008; Ozaki et al., 4072 Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, pages 4072–4080 c Florence, Italy, July 28 - August 2, 2019. 2019 Association for Computational Linguistics 2011; Suzuki et al., 2013; Zhang et al., 2017a), including BLI (Dinu et al., 2014). Recently, remarkable improvements have been made in BLI by mitigating hubness. For example, Smith et al. (2017) propose Inverted SoFtmax (ISF) that scales the similarities by a (global) measure of hubness. Conneau et al. (2018) develop a method called Cross domain Similarity Local Scaling (CSLS) that relies on a local measure of hubness instead. This work studies how to overcome hubness in BLI. The new method, HNN, is proposed by introducing an equal preference assumption. As we shall see, the assumption leads to an optimization proble"
P19-1399,N15-1104,0,0.109107,"Missing"
P19-1399,D17-1207,0,0.0910532,"lowing the seminal work, significant advances have been made. For example, Faruqui and Dyer (2014) use Canonical Component Analysis to align the two embedding spaces. Xing et al. (2015) show a substantial gain by normalizing the embeddings and constraining the mapping to be orthogonal. A series of works by Artetxe et al. (2017, 2018a,b) show that decent accuracies can be achieved even with a tiny or no seeding dictionary. The authors name their method as “self-learning”, which alternates between learning the mapping and inducing more translation pairs. The similar methodology is also seen in (Zhang et al., 2017b), where the induction step reduces a cost called earth mover distance. Conneau et al. (2018) propose to use Generative Adversarial Network (Goodfellow et al., 2014) to learn the mapping when no seeding dictionary is available. Whether using a seeding dictionary or not, the induction always requires to retrieve the translation under some distance measure. NN may be the most straightforward approach. However, it is often challenged by a phenomenon called hubness (Radovanovic et al., 2010). Hubness is a tendency that a few words (hubs) are too near to too many other words, especially in high di"
P19-1399,J17-2001,0,0.146607,"roves upon but also unifies several recent works that retrieve translations by Nearest Neighbor (NN) (Mikolov et al., 2013) and more advanced techniques like Inverted SoFtmax (ISF) (Smith et al., 2017). There is a long history of BLI using nonparallel corpora. Methods often rely on some designed features, which reveal some shared 1 https://github.com/baidu-research/HNN † qiang.qiu@duke.edu structures across languages. For example, cooccurrence matrices (Rapp, 1995), tf-idf of context words (Fung, 1998), and lexical similarities (Klementiev et al., 2012). A comprehensive study can be found in (Irvine and Callison-Burch, 2017). Recently, Mikolov et al. (2013) observe that isomorphism exists across word embeddings of different languages. This motivates them to learn a linear mapping to align the spaces, using a seeding dictionary of 5K pairs of translations. After that, more translations can be induced by NN search. Following the seminal work, significant advances have been made. For example, Faruqui and Dyer (2014) use Canonical Component Analysis to align the two embedding spaces. Xing et al. (2015) show a substantial gain by normalizing the embeddings and constraining the mapping to be orthogonal. A series of wor"
P19-1399,E12-1014,0,0.0279376,"cally from non-parallel corpora. The proposed method not only improves upon but also unifies several recent works that retrieve translations by Nearest Neighbor (NN) (Mikolov et al., 2013) and more advanced techniques like Inverted SoFtmax (ISF) (Smith et al., 2017). There is a long history of BLI using nonparallel corpora. Methods often rely on some designed features, which reveal some shared 1 https://github.com/baidu-research/HNN † qiang.qiu@duke.edu structures across languages. For example, cooccurrence matrices (Rapp, 1995), tf-idf of context words (Fung, 1998), and lexical similarities (Klementiev et al., 2012). A comprehensive study can be found in (Irvine and Callison-Burch, 2017). Recently, Mikolov et al. (2013) observe that isomorphism exists across word embeddings of different languages. This motivates them to learn a linear mapping to align the spaces, using a seeding dictionary of 5K pairs of translations. After that, more translations can be induced by NN search. Following the seminal work, significant advances have been made. For example, Faruqui and Dyer (2014) use Canonical Component Analysis to align the two embedding spaces. Xing et al. (2015) show a substantial gain by normalizing the"
P83-1014,P81-1022,0,0.0392959,"Missing"
P83-1014,P80-1012,0,0.0613093,"ng and place assimilation, aspiration, flapping. etc. In short, these mamces are sparse because allophonic and phonotactic constraints are useful be necessary to expand out each of the three cases: (13a) homorganic-nasal-cluster ~ labial-nasal labial-obstruent (13b) homorganie-nasal-cluster ~ coronal-nasal coronal-obstruent (13c) homorganic-nasal-cluster---* velar-nasal velar-obstruent In an effort to alleviate this expansion problem, many researchers have proposed augmentations of various sorts (e.g., ATN registers [26], LFG constraint equations [16], GPSG recta-rules till, local constraints [18], bit vectors [6, 22]). My own solution will be suggested after I have had (15) a chance to describe the parser in further detail. (setq homorganic-nasal-lattice (M + (M* (phoneme-lattice #/m)labial-lattice) (M* (phoneme-lattice # / n ) coronal-lattice) (M* (phoneme-lattice # / G ) velar-lattice))) 2..2 A Parser Based on Matrix Operations illustrating tile use of M + (matrix additit)n) ttt express the uniun of several alternatives and M* (matrix multiplication) to express the This scction will show how the grammar can be implemented in terms of operations on binary matrices. concatenation of s"
P83-1014,J82-1001,0,\N,Missing
P86-1023,P85-1030,1,0.878252,"Missing"
P89-1010,A88-1019,1,0.127859,"ng that the probability of set ... o f f is almost 64 times greater than chance. This association is relatively strong; the other particles that Sincliir mentions have association ratios of: about (1.4), in (2.9), up (6.9), out (4.5), on (3.3) in the 1987 AP Corpus. As Sinclair suggests, the approach is well suited for identifying phrasal verbs. However, phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to. We have found that if we first tag every word in the corpus with a part of speech using a method such as [Church (1988)], and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition to~in and verbs associated with a following infinitive marker to~to. (Part of speech notation is borrowed from [Francis and Kucera (1982)]; in = preposition; to = infinitive marker; vb = bare verb; vbg = verb + ins; vbd = verb + ed; vbz = verb + s; vbn = verb + en.) The association ratio identifies quite a number of verbs associated in an interesting way with to; restricting our attention to pairs with a score of 3.0 or more, there are 768 verbs ass"
P89-1010,P83-1019,0,0.0305988,"Missing"
P89-1010,W89-0240,1,\N,Missing
P89-1010,H89-2012,1,\N,Missing
P89-1010,P80-1024,0,\N,Missing
P91-1023,A88-1019,1,0.728896,"Missing"
P91-1023,P91-1022,0,0.568809,"Missing"
P91-1023,1988.tmi-1.19,0,\N,Missing
P91-1023,C90-3031,0,\N,Missing
P91-1023,C88-1016,0,\N,Missing
P91-1023,J90-2002,0,\N,Missing
P92-1032,P90-1032,0,0.01869,"Missing"
P92-1032,C90-2067,0,0.022795,"Missing"
P92-1032,C92-2070,1,0.732359,"uation, but fails to offer any very satisfying solutions that we might adopt to quantify the performance of our two disambiguation algorithms. 3 At first, we thought that the method was completely dependent on the availability of parallel corpora for training. This has been a problem since parallel text remains somewhat difficult to obtain in large quantity, and what little is available is often fairly unbalanced and unrepresentative of general language. Moreover, the assumption that differences in translation correspond to differences in word-sense has always been somewhat suspect. Recently, Yarowsky (1992) has found a way to extend our use of the Bayesian techniques by training on the Roget's Thesaurus (Chapman, 1977) 2 and G-rolier's Encyclopedia (1991) instead of the Canadian Hansards, thus circumventing many of the objections to our use of the Hansards. Yarowsky (1992) inputs a 100-word context surrounding a polysemous word and scores each of the 1042 Roget Categories by: 1-[ Pr(wlRoget Perhaps the most common evaluation technique is to select a small sample of words and compare the results of the machine with those of a human judge. This method has been used very effectively by Kelly and St"
P92-1032,P91-1034,0,0.135016,"Missing"
P92-1032,A88-1019,1,0.397426,"Missing"
P92-1032,P91-1017,0,0.0511046,"Missing"
P92-1032,H92-1045,1,\N,Missing
P93-1001,P91-1022,0,0.889749,"Missing"
P93-1001,1992.tmi-1.7,0,0.827658,"Missing"
P93-1001,J93-1004,1,\N,Missing
P93-1001,C90-3031,0,\N,Missing
P93-1001,J90-2002,0,\N,Missing
P93-1001,J93-1006,0,\N,Missing
W00-1315,C00-1027,1,0.749123,"ts from the multiple regression: A = a2 + b2 • idf + c2 • log(1 + t f ) where a2 ---- - 4 . 1 , b2 = 0.66 and c2 = 3.9. The differences in the two fits are particularly large when t f = 0; note t h a t b(0) is negligible (0.05) and b2 is quite large (0.66). Reducing the number of parameters from 10 to 3 in this way increases the sum of square errors, which may or may not result in a large degradation in precision and recall. Why take the chance? 3 Burstiness Table 6 is like tables 4 but the binning rule not only uses idf, but also burstiness (B). Burstiness (Church and Gale, 1995)(Katz, 1996)(Church, 2000) is intended to account for the fact that some very good keywords such as ""Kennedy"" tend to be mentioned quite a few times in a document or not at all, whereas less good keywords such as ""except"" tend to be mentioned about the same number of times no m a t t e r what the document tf 0 1 2 3 4+ B=0 a b -0.05 -0.00 -1.23 0.63 -0.76 0.71 0.00 0.69 0.68 0.71 B=i a b -0.61 0.02 -0.80 0.79 -0.05 0.79 0.23 0.82 0.75 0.83 Table 6: Regression coefficients for method fit-B. Note t h a t the slopes and intercepts are larger when B = 1 than when B = 0 (except when t f = 0). Even though A usually lies betw"
W02-1023,J96-1001,1,\N,Missing
W02-1023,W01-0508,1,\N,Missing
W02-1023,A88-1019,1,\N,Missing
W11-0823,P01-1005,0,0.0292774,"anguagelog/archives/005514.ht ml upper and lower case? MWEs? The different curves correspond to different choices. No matter how we define a word, we find that vocabulary grows (rapidly) with corpus size for as far as we can see. This observation appears to hold across a broad set of conditions (languages, definitions of word/ngram, etc.) Vocabulary becomes larger and larger with experience. Similar comments apply to ngrams and MWEs. There is wide agreement that there’s no data like more data (Mercer, 1985).9 Google quoted Mercer in their announcement of ngram counts (Franz and Brants, 2006). Banko and Brill (2001) observed that performance goes up and up and up with experience (data). In the plot below, they note that the differences between lines (learners) are small compared to the gains to be had by simply collecting more data. Based on this observation, Brill has suggested (probably in jest) that we should fire everyone and spend the money on collecting data. Another interpretation is that experience improves performance on a host of tasks. This pattern might help account for the large correlation (0.91) in Terman (1918). Terman suggests that vocabulary size should not be viewed as a measure of int"
W11-0823,P11-1135,1,0.778671,"oes not refer to a house that happens to be white, which is what would be expected under compositional semantics. It is accented on the left (the WHITE house) in contrast with the general pattern where adjective-noun complex nominals are typically accented on the right (a nice HOUSE), though there are many exceptions to this rule (Sproat 1994).14 Linguists would also feel comfortable with diagnostic tests based on paraphrases and transformations. Fixed expressions are fixed. One can’t paraphrase a “red herring” as “*herring that is red.” They resist regular inflection: “*two red herrings.” In Bergsma et al (2011), we use a paraphrase diagnostic to distinguish [N & N] N from N & [ N N]: • [dairy and meat] production o meat and dairy production o production of meat and dairy o production de produits [laitiers et de viand] (French) 14 Sproat has posted a list of 7831 English binary noun compounds with hand assigned accent labels at: http://www.cslu.ogi.edu/~sproatr/newindex/ap90nominals.txt 143 • asbestos and [polyvinyl chloride] o polyvinyl chloride and asbestos o asbestos and chloride o l’asbesto e il [polivinilcloruro] (Italian) The first three paraphrases make it clear that “dairy and meat” is a cons"
W11-0823,H89-2013,1,0.651921,"Missing"
W11-0823,J90-1003,1,0.189334,". Despite their best efforts, there have been a few highly publicized mistakes 13 and there will probably be more unless we find better ways to prevent bloopers. 8.2 Complex Nominals and What is a Word? Complex nominals are probably more common than phrasal verbs. Is “White House” one word or two? Is a word defined in terms of spelling? White space? These days, among computational linguists, there would be considerable sympathy for using distributional statistics such as word frequency and mutual information to find MWEs. Following Firth (1957), we know a word by the company that it keeps. In Church and Hanks (1990), we suggested using pointwise mutual information as a heuristic to look for pairs of words that have non-compositional distributional statistics. That is, if the joint probability, P(x,y), of seeing two words together in a context (e.g., window of 5 words) is much higher than chance, P(x)P(y), then there is probably a hidden variable such as meaning that is causing the deviation from chance. In this way, we are able to discover lots of word associations (e.g., doctor…nurse), collocates, fixed expressions, etc. If the list of MWEs becomes too large and too unmanageable, one could turn to a met"
W11-0823,A94-1006,0,0.393009,"Missing"
W11-0823,H92-1045,1,0.606891,"ions? 139 5.1 User Intent & Spelling Correction Spelling correction is an extreme case where it is often relatively easy for the system to determine user intent. On the web, spelling correction has become synonymous with did-you-mean. The synonymy makes it clear that the point of spelling correction is to get at what users mean as opposed to what they say. Then you should say what you mean,&apos; the March Hare went on. `I do,&apos; Alice hastily replied; `at least--at least I mean what I say-that&apos;s the same thing, you know.&apos; `Not the same thing a bit!&apos; said the Hatter. (Lewis Carroll, 1865) See Kukich (1992) for a comprehensive survey on spelling correction. Boswell (2004) is a nice research exam; it is short and crisp and recent. I’ve worked on Microsoft’s spelling correction products in two different divisions: Office and Web Search. One might think that correcting documents in Microsoft Word would be similar to correcting web queries, but in fact, the two applications have remarkably little in common. A dictionary of general vocabulary is essential for correcting documents and nearly useless for correcting web queries. General vocabulary is more important in documents than web queries. The sur"
W11-0823,J96-3004,0,0.0580114,"ive factors. 9 141 Jelinek (2004) attributes this position to Mercer (1985) http://www.lrec-conf.org/lrec2004/doc/jelinek.pdf. 8 What is a Word? MWE? We tend to think that white space makes it pretty easy to tokenize English text into words. Obviously, white space makes the task much easier than it would be otherwise. There is a considerable literature on word breaking in Chinese and Japanese which is considerably more challenging than English largely because there is no white space in Chinese and Japanese. There are a number of popular dictionary-based solutions such as ChaSen10 and Juman.11 Sproat et al (1996) proposed an alternative solution based on distributional statistics such as mutual information. The situation may not be all that different in English. English is full of multiword expressions. An obvious example involves words that look like prepositions: up, in, on, with. A great example is often attributed to Winston Churchill: This is the sort of arrant nonsense up with which I will not put.12 One could argue that “put up with” is a phrasal verb and therefore it should be treated more like a fixed expression (or a word) than a stranded preposition. 8.1 Preventing Bloopers Almost any high"
W11-0823,H93-1052,0,0.392493,"intent (CQI). Consider five types of clicks. Some types of clicks are evidence that the user knows where she wants to go, and some are evidence that the user is open to suggestions. 1. 2. 3. 4. 5. Algo: clicks on the so-called 10 blue links Paid: clicks on commercial ads Wikipedia: clicks on Wikipedia entries Spelling Corrections: did you mean …? Other suggestions from search engine Many queries are strongly associated with one type of click (more than others). • • Commercial queries à clicks on ads Non-commercial queries à Wikipedia. There is a one-sense-per-X constraint (Gale et al, 1992; Yarowsky, 1993). It is unlikely that the same query will be ambiguous with both commercial and non-commercial senses. Indeed, the click logs show that both ads and Wikipedia are effective, but they have complementary distributions. There are few queries with both clicks on ads and clicks on Wikipedia entries. For a commercial query like, “JC Penney,” it is ok for Google to return an ad and a store locator map, but Google shouldn’t return a Wikipedia discussion of the history of the company. Although the click logs are very large, they are never large enough. How do we resolve the user intention when the clic"
W11-0823,D09-1148,0,\N,Missing
W11-0823,W04-3238,0,\N,Missing
W14-3002,P98-1013,0,0.348324,"Missing"
W14-3002,J90-1003,1,0.239552,"Missing"
W14-3002,J93-2004,0,0.0613946,"Missing"
W89-0240,A88-1019,1,0.682256,"e to ff ) = 3.7, indicating that the probability of set ... off is 23-7 = 13 times greater than chance. This association is relatively strong; the other particles that Sinclair mentions have scores of: about (-0.9), in (0.6), up (4.6), out (2.2), on (1.0) in the 1987 AP Corpus of 15 million words. j. Preprocessing the Corpus with a Part of Speech Tagger Phrasal verbs involving the preposition to raise an interesting problem because of the possible confusion with the infinitive marker to. We have found that if we first tag every word in the corpus with a part of speech using a method such as Church (1988) or DeRose (1988), and then measure associations between tagged words, we can identify interesting contrasts between verbs associated with a following preposition to/in and verbs associated with a following infinitive marker to/to. (Part of speech notation is borrowed from Francis and Kucera (1982); in = preposition; to = infinitive marker, vb = bare verb; vbg = verb + ing; vbd = verb + ed; vbz = verb + s; vbn = verb + en.) The score identifies quite a number of verbs associated in an interesting way with to restricting our attention to pairs with a score of 3.0 or more, there are 768 verbs a"
W89-0240,J88-1003,0,\N,Missing
W89-0240,P85-1037,0,\N,Missing
W89-0240,P89-1010,1,\N,Missing
W93-0301,P91-1023,1,0.893183,"d by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of parti"
W93-0301,1992.tmi-1.9,1,0.883073,"inology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language they chose and retrieve documents that may or may not have been written in the same language as the query. Landauer and Littman used SVD analysis (or Latent Semantic Indexing) on the Canadian Hansards, parliamentary debates that are published in both English and French, in order to estimate a kind of soft thesaurus. They then showed t"
W93-0301,P93-1003,0,0.157087,"square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express querie"
W93-0301,P93-1004,0,0.0961023,"by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to express queries in whatever language th"
W93-0301,1992.tmi-1.7,0,0.804952,"W i l l i a m A. Gale AT&T Bell Laboratories 600 Mountain Avenue Murray Hill, NJ 07974 Abstract We have developed a new program called word_align for aligning parallel text, text such as the Canadian Hansards that are available in two or more languages. The program takes the output of char_align (Church, 1993), a robust alternative to sentence-based alignment programs, and applies word-level constraints using a version of Brown el al.'s Model 2 (Brown et al., 1993), modified and extended to deal with robustness issues. Word_align was tested on a subset of Canadian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and"
W93-0301,P91-1022,0,0.751742,"dian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval app"
W93-0301,P91-1034,0,0.121122,"dian Hansards supplied by Simard (Simard et al., 1992). The combination of word_align plus char_align reduces the variance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval app"
W93-0301,P93-1001,1,0.845328,"iance (average square error) by a factor of 5 over char_align alone. More importantly, because word_align and char_align were designed to work robustly on texts that are smaller and more noisy than the Hansards, it has been possible to successfully deploy the programs at AT&T Language Line Services, a commercial translation service, to help them with difficult terminology. 1 Introduction Aligning parallel texts has recently received considerable attention (Warwick et al., 1990; Brown et al., 1991a; Gale and Church, 1991b; Gale and Church, 1991a; Kay and Rosenschein, 1993; Simard et al., 1992; Church, 1993; Kupiec, 1993; Matsumoto et al., 1993). These methods have been used in machine translation (Brown et al., 1990; Sadler, 1989), terminology research and translation aids (Isabelle, 1992; Ogden and Gonzales, 1993), bilingual lexicography (Klavans and Tzoukermann, 1990), collocation studies (Smadja, 1992), word-sense disambiguation (Brown et al., 1991b; Gale et al., 1992) and information retrieval in a multilingual environment (Landauer and Littman, 1990). The information retrieval application may be of particular relevance to this audience. It would be highly desirable for users to be able to"
