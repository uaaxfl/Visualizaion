D16-1257,Parsing as Language Modeling,2016,16,45,2,1,35620,do choe,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,None
P15-1100,"Sparse, Contextually Informed Models for Irony Detection: Exploiting User Communities, Entities and Sentiment",2015,24,43,3,1,3458,byron wallace,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers),1,"Automatically detecting verbal irony (roughly, sarcasm) in online content is important for many practical applications (e.g., sentiment detection), but it is difficult. Previous approaches have relied predominantly on signal gleaned from word counts and grammatical cues. But such approaches fail to exploit the context in which comments are embedded. We thus propose a novel strategy for verbal irony classification that exploits contextual features, specifically by combining noun phrases and sentiment extracted from comments with the forum type (e.g., conservative or liberal) to which they were posted. We show that this approach improves verbal irony classification performance. Furthermore, because this method generates a very large feature space (and we expect predictive contextual features to be strong but few), we propose a mixed regularization strategy that places a sparsity-inducing `1 penalty on the contextual feature weights on top of the `2 penalty applied to all model coefficients. This increases model sparsity and reduces the variance of model performance."
N15-1008,A Hybrid Generative/Discriminative Approach To Citation Prediction,2015,16,2,2,0,37622,chris tanner,Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"Text documents of varying nature (e.g., summary documents written by analysts or published, scientific papers) often cite others as a means of providing evidence to support a claim, attributing credit, or referring the reader to related work. We address the problem of predicting a documentxe2x80x99s cited sources by introducing a novel, discriminative approach which combines a content-based generative model (LDA) with author-based features. Further, our classifier is able to learn the importance and quality of each topic within our corpus xe2x80x93 which can be useful beyond this task xe2x80x93 and preliminary results suggest its metric is competitive with other standard metrics (Topic Coherence). Our flagship system, Logit-Expanded, provides state-of-the-art performance on the largest corpus ever used for this task."
D15-1160,Syntactic Parse Fusion,2015,28,8,3,1,35620,do choe,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Model combination techniques have consistently shown state-of-the-art performance across multiple tasks, including syntactic parsing. However, they dramatically increase runtime and can be difficult to employ in practice. We demonstrate that applying constituency model combination techniques to n-best lists instead of n different parsers results in significant parsing accuracy improvements. Parses are weighted by their probabilities and combined using an adapted version of Sagae and Lavie (2006). These accuracy gains come with marginal computational costs and are obtained on top of existing parsing techniques such as discriminative reranking and self-training, resulting in state-of-the-art accuracy: 92.6% on WSJ section 23. On out-of-domain corpora, accuracy is improved by 0.4% on average. We empirically confirm that six well-known n-best parsers benefit from the proposed methods across six domains."
W14-1815,Natural Language Generation with Vocabulary Constraints,2014,18,2,3,1,11107,ben swanson,Proceedings of the Ninth Workshop on Innovative Use of {NLP} for Building Educational Applications,0,"We investigate data driven natural language generation under the constraints that all words must come from a fixed vocabulary and a specified word must appear in the generated sentence, motivated by the possibility for automatic generation of language education exercises. We present fast and accurate approximations to the ideal rejection samplers for these constraints and compare various sentence level generative language models. Our best systems produce output that is with high frequency both novel and error free, which we validate with human and automatic evaluations."
W14-1602,Domain-Specific Image Captioning,2014,29,5,2,1,38752,rebecca mason,Proceedings of the Eighteenth Conference on Computational Natural Language Learning,0,"We present a data-driven framework for image caption generation which incorporates visual and textual features with varying degrees of spatial structure. We propose the task of domain-specific image captioning, where many relevant visual details cannot be captured by off-the-shelf general-domain entity detectors. We extract previously-written descriptions from a database and adapt them to new query images, using a joint visual and textual bag-of-words model to determine the correctness of individual words. We implement our model using a large, unlabeled dataset of womenxe2x80x99s shoes images and natural language descriptions (Berg et al., 2010). Using both automatic and human evaluations, we show that our captioning method effectively deletes inaccurate words from extracted captions while maintaining a high level of detail in the generated output."
P14-2084,"Humans Require Context to Infer Ironic Intent (so Computers Probably do, too)",2014,16,46,4,1,3458,byron wallace,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Automatically detecting verbal irony (roughly, sarcasm) is a challenging task because ironists say something other than xe2x80x90 and often opposite to xe2x80x90 what they actually mean. Discerning ironic intent exclusively from the words and syntax comprising texts (e.g., tweets, forum posts) is therefore not always possible: additional contextual information about the speaker and/or the topic at hand is often necessary. We introduce a new corpus that provides empirical evidence for this claim. We show that annotators frequently require context to make judgements concerning ironic intent, and that machine learning approaches tend to misclassify those same comments for which annotators required additional context."
P14-2097,Nonparametric Method for Data-driven Image Captioning,2014,28,42,2,1,38752,rebecca mason,Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We present a nonparametric density estimation technique for image caption generation. Data-driven matching methods have shown to be effective for a variety of complex problems in Computer Vision. These methods reduce an inference problem for an unknown image to finding an existing labeled image which is semantically similar. However, related approaches for image caption generation (Ordonez et al., 2011; Kuznetsova et al., 2012) are hampered by noisy estimations of visual content and poor alignment between images and human-written captions. Our work addresses this challenge by estimating a word frequency representation of the visual content of a query image. This allows us to cast caption generation as an extractive summarization problem. Our model strongly outperforms two state-ofthe-art caption extraction systems according to human judgments of caption relevance."
E14-4033,Data Driven Language Transfer Hypotheses,2014,11,18,2,1,11107,ben swanson,"Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics, volume 2: Short Papers",0,"Language transfer, the preferential second language behavior caused by similarities to the speakerxe2x80x99s native language, requires considerable expertise to be detected by humans alone. Our goal in this work is to replace expert intervention by data-driven methods wherever possible. We define a computational methodology that produces a concise list of lexicalized syntactic patterns that are controlled for redundancy and ranked by relevancy to language transfer. We demonstrate the ability of our methodology to detect hundreds of such candidate patterns from currently available data sources, and validate the quality of the proposed patterns through classification experiments."
W13-1301,Annotation of Online Shopping Images without Labeled Training Examples,2013,17,8,2,1,38752,rebecca mason,Proceedings of the Workshop on Vision and Natural Language Processing,0,"We are interested in the task of image annotation using noisy natural text as training data. An image and its caption convey different information, but are generated by the same underlying concepts. In this paper, we learn latent mixtures of topics that generate image and product descriptions on shopping websites by adapting a topic model for multilingual data (Mimno et al., 2009). We use the trained model to annotate test images without corresponding text. We capture visual properties such as color, texture, shape, and orientation by computing low-level image features, and measure the contribution of each type of visual feature towards the accuracy of the model. Our model significantly outperforms both a competitive baseline and a previous topic model-based system."
P13-1030,A Context Free {TAG} Variant,2013,18,1,3,1,11107,ben swanson,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"We propose a new variant of TreeAdjoining Grammar that allows adjunction of full wrapping trees but still bears only context-free expressivity. We provide a transformation to context-free form, and a further reduction in probabilistic model size through factorization and pooling of parameters. This collapsed context-free form is used to implement efficient grammar estimation and parsing algorithms. We perform parsing experiments the Penn Treebank and draw comparisons to TreeSubstitution Grammars and between different variations in probabilistic model design. Examination of the most probable derivations reveals examples of the linguistically relevant structure that our variant makes possible."
N13-1009,Extracting the Native Language Signal for Second Language Acquisition,2013,25,16,2,1,11107,ben swanson,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We develop a method for effective extraction of linguistic patterns that are differentially expressed based on the native language of the author. This method uses multiple corpora to allow for the removal of data set specific patterns, and addresses both feature relevancy and redundancy. We evaluate different relevancy ranking metrics and show that common measures of relevancy can be inappropriate for data with many rare features. Our feature set is a broad class of syntactic patterns, and to better capture the signal we extend the Bayesian Tree Substitution Grammar induction algorithm to a supervised mixture of latent grammars. We show that this extension can be used to extract a larger set of relevant features."
D13-1148,Naive {B}ayes Word Sense Induction,2013,12,7,2,1,35620,do choe,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,We introduce an extended naive Bayes model for word sense induction (WSI) and apply it to a WSI task. The extended model incorporates the idea the words closer to the target word are more relevant in predicting its sense. The proposed model is very simple yet effective when evaluated on SemEval-2010 WSI data.
D13-1182,"A Generative Joint, Additive, Sequential Model of Topics and Speech Acts in Patient-Doctor Communication",2013,43,8,5,1,3458,byron wallace,Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing,0,We develop a novel generative model of conversation that jointly captures both the topical content and the speech act type associated with each utterance. Our model expresses both token emission and state transition probabilities as log-linear functions of separate components corresponding to topics and speech acts (and their interactions). We apply this model to a dataset comprising annotated patient-physician visits and show that the proposed joint approach outperforms a baseline univariate model.
P12-2038,Native Language Detection with Tree Substitution Grammars,2012,19,35,2,0,42661,benjamin swanson,Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"We investigate the potential of Tree Substitution Grammars as a source of features for native language detection, the task of inferring an author's native language from text in a different language. We compare two state of the art methods for Tree Substitution Grammar induction and show that features from both methods outperform previous state of the art results at native language detection. Furthermore, we contrast these two induction algorithms and show that the Bayesian approach produces superior classification results with a smaller feature set."
N12-1018,Apples to Oranges: Evaluating Image Annotations from Natural Language Processing Systems,2012,31,3,2,1,38752,rebecca mason,Proceedings of the 2012 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We examine evaluation methods for systems that automatically annotate images using co-occurring text. We compare previous datasets for this task using a series of baseline measures inspired by those used in information retrieval, computer vision, and extractive summarization. Some of our baselines match or exceed the best published scores for those datasets. These results illuminate incorrect assumptions and improper practices regarding preprocessing, evaluation metrics, and the collection of gold image annotations. We conclude with a list of recommended practices for future research combining language and vision processing techniques."
W11-0507,Extractive Multi-Document Summaries Should Explicitly Not Contain Document Specific Content,2011,12,14,2,1,38752,rebecca mason,"Proceedings of the Workshop on Automatic Summarization for Different Genres, Media, and Languages",0,"Unsupervised approaches to multi-document summarization consist of two steps: finding a content model of the documents to be summarized, and then generating a summary that best represents the most salient information of the documents. In this paper, we present a sentence selection objective for extractive summarization in which sentences are penalized for containing content that is specific to the documents they were extracted from. We modify an existing system, Hier-Sum (Haghighi & Vanderwende, 2009), to use our objective, which significantly outperforms the original HierSum in pairwise user evaluation. Additionally, our ROUGE scores advance the current state-of-the-art for both supervised and unsupervised systems with statistical significance."
P11-2022,Extending the Entity Grid with Entity-Specific Features,2011,19,44,2,1,1344,micha elsner,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"We extend the popular entity grid representation for local coherence modeling. The grid abstracts away information about the entities it models; we add discourse prominence, named entity type and coreference features to distinguish between important and unimportant entities. We improve the best result for WSJ document discrimination by 6%."
P11-1118,Disentangling Chat with Local Coherence Models,2011,35,31,2,1,1344,micha elsner,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We evaluate several popular models of local discourse coherence for domain and task generality by applying them to chat disentanglement. Using experiments on synthetic multiparty conversations, we show that most models transfer well from text to dialogue. Coherence models improve results overall when good parses and topic models are available, and on a constrained task for real chat data."
J11-4001,{ACL} Lifetime Achievement Award: The Brain as a Statistical Inference Engine{---}and You Can Too,2011,9,5,1,1,35621,eugene charniak,Computational Linguistics,0,"There are several possible templates for award talks.1 The most common is an intellectual historyxe2x80x94how I came to make all these wonderful discoveries. However, I am never completely happy with my work, as it seems a pale shadow of what I think it should have been. Thus I am picking a different modelxe2x80x94things we all know but do not say out loud because we have no evidence to support them; and besides, making such bold claims sounds pretentious. Thus I do not expect to say anything too novel here. I hope all my readers already know that the brain exploits statistics, and most of them suspect that we in statistical computational linguistics have something to say about how this works out in the case of language. My goal is therefore not to say anything you do not believe, but to cause you to believe it more passionately."
I11-1034,$S^3$ - Statistical Sandhi Splitting,2011,9,4,2,0,44759,abhiram natarajan,Proceedings of 5th International Joint Conference on Natural Language Processing,0,None
P10-2007,The Same-Head Heuristic for Coreference,2010,18,8,2,1,1344,micha elsner,Proceedings of the {ACL} 2010 Conference Short Papers,0,"We investigate coreference relationships between NPs with the same head noun. It is relatively common in unsupervised work to assume that such pairs are coreferent-- but this is not always true, especially if realistic mention detection is used. We describe the distribution of non-coreferent same-head pairs in news text, and present an unsupervised generative model which learns not to link some same-head NPs using syntactic features, improving precision."
N10-1004,Automatic Domain Adaptation for Parsing,2010,24,118,2,1,34707,david mcclosky,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"Current statistical parsers tend to perform well only on their training domain and nearby genres. While strong performance on a few related domains is sufficient for many situations, it is advantageous for parsers to be able to generalize to a wide variety of domains. When parsing document collections involving heterogeneous domains (e.g. the web), the optimal parsing model for each document is typically not obvious. We study this problem as a new task --- multiple source parser adaptation. Our system trains on corpora from many different domains. It learns not only statistics of those domains but quantitative measures of domain differences and how those differences affect parsing accuracy. Given a specific target text, the resulting system proposes linear combinations of parsing models trained on the source corpora. Tested across six domains, our system outperforms all non-oracle baselines including the best domain-independent parsing model. Thus, we are able to demonstrate the value of customizing parsing models to specific domains."
J10-3004,Disentangling Chat,2010,29,51,2,1,1344,micha elsner,Computational Linguistics,0,"When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately. We refer to this task as disentanglement. We present a corpus of Internet Relay Chat dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability. We propose a graph-based clustering model for disentanglement, using lexical, timing, and discourse-based features. The model's predicted disentanglements are highly correlated with manual annotations. We conclude by discussing two extensions to the model, specificity tuning and conversation start detection, both of which are promising but do not currently yield practical improvements."
D10-1066,Top-Down Nearly-Context-Sensitive Parsing,2010,20,5,1,1,35621,eugene charniak,Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing,0,"We present a new syntactic parser that works left-to-right and top down, thus maintaining a fully-connected parse tree for a few alternative parse hypotheses. All of the commonly used statistical parsers use context-free dynamic programming algorithms and as such work bottom up on the entire sentence. Thus they only find a complete fully connected parse at the very end. In contrast, both subjective and experimental evidence show that people understand a sentence word-to-word as they go along, or close to it. The constraint that the parser keeps one or more fully connected syntactic trees is intended to operationalize this cognitive fact. Our parser achieves a new best result for top-down parsers of 89.4%, a 20% error reduction over the previous single-parser best result for parsers of this type of 86.8% (Roark, 2001). The improved performance is due to embracing the very large feature set available in exchange for giving up dynamic programming."
N09-1019,Structured Generative Models for Unsupervised Named-Entity Clustering,2009,24,40,2,1,1344,micha elsner,Proceedings of Human Language Technologies: The 2009 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We describe a generative model for clustering named entities which also models named entity internal structure, clustering related words by role. The model is entirely unsupervised; it uses features from the named entity itself and its syntactic context, and coreference information from an unsupervised pronoun resolver. The model scores 86% on the MUC-7 named-entity dataset. To our knowledge, this is the best reported score for a fully unsupervised model, and the best score for a generative model."
E09-1018,{EM} Works for Pronoun Anaphora Resolution,2009,23,75,1,1,35621,eugene charniak,Proceedings of the 12th Conference of the {E}uropean Chapter of the {ACL} ({EACL} 2009),0,"We present an algorithm for pronoun-anaphora (in English) that uses Expectation Maximization (EM) to learn virtually all of its parameters in an unsupervised fashion. While EM frequently fails to find good models for the tasks to which it is set, in this case it works quite well. We have compared it to several systems available on the web (all we have found so far). Our program significantly outperforms all of them. The algorithm is fast and robust, and has been made publically available for downloading."
P08-2011,Coreference-inspired Coherence Modeling,2008,22,43,2,1,1344,micha elsner,"Proceedings of ACL-08: HLT, Short Papers",0,"Research on coreference resolution and summarization has modeled the way entities are realized as concrete phrases in discourse. In particular there exist models of the noun phrase syntax used for discourse-new versus discourse-old referents, and models describing the likely distance between a pronoun and its antecedent. However, models of discourse coherence, as applied to information ordering tasks, have ignored these kinds of information. We apply a discourse-new classifier and pronoun coreference algorithm to the information ordering task, and show significant improvements in performance over the entity grid, a popular model of local coherence."
P08-2026,Self-Training for Biomedical Parsing,2008,12,93,2,1,34707,david mcclosky,"Proceedings of ACL-08: HLT, Short Papers",0,"Parser self-training is the technique of taking an existing parser, parsing extra data and then creating a second parser by treating the extra data as further training data. Here we apply this technique to parser adaptation. In particular, we self-train the standard Charniak/Johnson Penn-Treebank parser using unlabeled biomedical abstracts. This achieves an f-score of 84.3% on a standard test set of biomedical abstracts from the Genia corpus. This is a 20% error reduction over the best previous result on biomedical data (80.2% on the same test set)."
P08-1095,You Talking to Me? A Corpus and Algorithm for Conversation Disentanglement,2008,13,65,2,1,1344,micha elsner,Proceedings of ACL-08: HLT,1,"When multiple conversations occur simultaneously, a listener must decide which conversation each utterance is part of in order to interpret and respond to it appropriately. We refer to this task as disentanglement. We present a corpus of Internet Relay Chat (IRC) dialogue in which the various conversations have been manually disentangled, and evaluate annotator reliability. This is, to our knowledge, the first such corpus for internet chat. We propose a graph-theoretic model for disentanglement, using discourse-based features which have not been previously applied to this task. The modelxe2x80x99s predicted disentanglements are highly correlated with manual annotations."
C08-1042,Evaluating Unsupervised Part-of-Speech Tagging for Grammar Induction,2008,22,26,3,1,47332,william iii,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"This paper explores the relationship between various measures of unsupervised part-of-speech tag induction and the performance of both supervised and unsupervised parsing models trained on induced tags. We find that no standard tagging metrics correlate well with unsupervised parsing performance, and several metrics grounded in information theory have no strong relationship with even supervised parsing performance."
C08-1071,When is Self-Training Effective for Parsing?,2008,14,36,2,1,34707,david mcclosky,Proceedings of the 22nd International Conference on Computational Linguistics (Coling 2008),0,"Self-training has been shown capable of improving on state-of-the-art parser performance (McClosky et al., 2006) despite the conventional wisdom on the matter and several studies to the contrary (Charniak, 1997; Steedman et al., 2003). However, it has remained unclear when and why self-training is helpful. In this paper, we test four hypotheses (namely, presence of a phase transition, impact of search errors, value of non-generative reranker features, and effects of unknown words). From these experiments, we gain a better understanding of why self-training works for parsing. Since improvements from self-training are correlated with unknown bigrams and biheads but not unknown words, the benefit of self-training appears most influenced by seeing known words in new combinations."
N07-2045,Language Modeling for Determiner Selection,2007,14,25,2,0,49319,jenine turner,"Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Companion Volume, Short Papers",0,"We present a method for automatic determiner selection, based on an existing language model. We train on the Penn Tree-bank and also use additional data from the North American News Text Corpus. Our results are a significant improvement over previous best."
N07-1055,A Unified Local and Global Model for Discourse Coherence,2007,16,50,3,1,1344,micha elsner,Human Language Technologies 2007: The Conference of the North {A}merican Chapter of the Association for Computational Linguistics; Proceedings of the Main Conference,0,"We present a model for discourse coherence which combines the local entitybased approach of (Barzilay and Lapata, 2005) and the HMM-based content model of (Barzilay and Lee, 2004). Unlike the mixture model of (Soricut and Marcu, 2006), we learn local and global features jointly, providing a better theoretical explanation of how they are useful. As the local component of our model we adapt (Barzilay and Lapata, 2005) by relaxing independence assumptions so that it is effective when estimated generatively. Our model performs the ordering task competitively with (Soricut and Marcu, 2006), and significantly better than either of the models it is based on."
W06-1636,Learning Phrasal Categories,2006,17,2,2,1,47332,william iii,Proceedings of the 2006 Conference on Empirical Methods in Natural Language Processing,0,"In this work we learn clusters of contextual annotations for non-terminals in the Penn Treebank. Perhaps the best way to think about this problem is to contrast our work with that of Klein and Manning (2003). That research used tree-transformations to create various grammars with different contextual annotations on the non-terminals. These grammars were then used in conjunction with a CKY parser. The authors explored the space of different annotation combinations by hand. Here we try to automate the process -- to learn the right combination automatically. Our results are not quite as good as those carefully created by hand, but they are close (84.8 vs 85.7)."
P06-1043,Reranking and Self-Training for Parser Adaptation,2006,14,202,2,1,34707,david mcclosky,Proceedings of the 21st International Conference on Computational Linguistics and 44th Annual Meeting of the Association for Computational Linguistics,1,"Statistical parsers trained and tested on the Penn Wall Street Journal (WSJ) treebank have shown vast improvements over the last 10 years. Much of this improvement, however, is based upon an ever-increasing number of features to be trained on (typically) the WSJ treebank data. This has led to concern that such parsers may be too finely tuned to this corpus at the expense of portability to other genres. Such worries have merit. The standard Charniak parser checks in at a labeled precision-recall f-measure of 89.7% on the Penn WSJ test set, but only 82.9% on the test set from the Brown treebank corpus.This paper should allay these fears. In particular, we show that the reranking parser described in Charniak and Johnson (2005) improves performance of the parser on Brown to 85.2%. Furthermore, use of the self-training techniques described in (McClosky et al., 2006) raise this to 87.8% (an error reduction of 28%) again without any use of labeled Brown data. This is remarkable since training the parser and reranker on labeled Brown data achieves only 88.4%."
N06-1020,Effective Self-Training for Parsing,2006,20,389,2,1,34707,david mcclosky,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"We present a simple, but surprisingly effective, method of self-training a two-phase parser-reranker system using readily available unlabeled data. We show that this type of bootstrapping is possible for parsing when the bootstrapped parses are processed by a discriminative reranker. Our improved model achieves an f-score of 92.1%, an absolute 1.1% improvement (12% error reduction) over the previous best result for Wall Street Journal parsing. Finally, we provide some analysis to better understand the phenomenon."
N06-1022,Multilevel Coarse-to-Fine {PCFG} Parsing,2006,18,46,1,1,35621,eugene charniak,"Proceedings of the Human Language Technology Conference of the {NAACL}, Main Conference",0,"We present a PCFG parsing algorithm that uses a multilevel coarse-to-fine (mlctf) scheme to improve the efficiency of search for the best parse. Our approach requires the user to specify a sequence of nested partitions or equivalence classes of the PCFG nonterminals. We define a sequence of PCFGs corresponding to each partition, where the nonterminals of each PCFG are clusters of nonterminals of the original source PCFG. We use the results of parsing at a coarser level (i.e., grammar defined in terms of a coarser partition) to prune the next finer level. We present experiments showing that with our algorithm the work load (as measured by the total number of constituents processed) is decreased by a factor of ten with no decrease in parsing accuracy compared to standard CKY parsing with the original PCFG. We suggest that the search space over mlctf algorithms is almost totally unexplored so that future work should be able to improve significantly on these results."
roark-etal-2006-sparseval,{SP}arseval: Evaluation Metrics for Parsing Speech,2006,12,36,3,0.443038,4293,brian roark,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"While both spoken and written language processing stand to benefit from parsing, the standard Parseval metrics (Black et al., 1991) and their canonical implementation (Sekine and Collins, 1997) are only useful for text. The Parseval metrics are undefined when the words input to the parser do not match the words in the gold standard parse tree exactly, and word errors are unavoidable with automatic speech recognition (ASR) systems. To fill this gap, we have developed a publicly available tool for scoring parses that implements a variety of metrics which can handle mismatches in words and segmentations, including: alignment-based bracket evaluation, alignment-based dependency evaluation, and a dependency evaluation that does not require alignment. We describe the different metrics, how to use the tool, and the outcome of an extensive set of experiments on the sensitivity."
P05-1022,Coarse-to-Fine n-Best Parsing and {M}ax{E}nt Discriminative Reranking,2005,18,826,1,1,35621,eugene charniak,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"Discriminative reranking is one method for constructing high-performance statistical parsers (Collins, 2000). A discriminative reranker requires a source of candidate parses for each sentence. This paper describes a simple yet novel method for constructing sets of 50-best parses based on a coarse-to-fine generative parser (Charniak, 2000). This method generates 50-best lists that are of substantially higher quality than previously obtainable. We used these parses as the input to a MaxEnt reranker (Johnson et al., 1999; Riezler et al., 2002) that selects the best parse from the set of parses for each sentence, obtaining an f-score of 91.0% on sentences of length 100 or less."
P05-1036,Supervised and Unsupervised Learning for Sentence Compression,2005,11,131,2,0,49319,jenine turner,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"In Statistics-Based Summarization - Step One: Sentence Compression, Knight and Marcu (Knight and Marcu, 2000) (KM Knight and Marcu use a corpus of 1035 training sentences. More data is not easily available, so in addition to improving the original K&M noisy-channel model, we create unsupervised and semi-supervised models of the task. Finally, we point out problems with modeling the task in this way. They suggest areas for future research."
I05-1006,Parsing Biomedical Literature,2005,25,118,2,0,32561,matthew lease,Second International Joint Conference on Natural Language Processing: Full Papers,0,"We present a preliminary study of several parser adaptation techniques evaluated on the GENIA corpus of MEDLINE abstracts [1,2]. We begin by observing that the Penn Treebank (PTB) is lexically impoverished when measured on various genres of scientific and technical writing, and that this significantly impacts parse accuracy. To resolve this without requiring in-domain treebank data, we show how existing domain-specific lexical resources may be leveraged to augment PTB-training: part-of-speech tags, dictionary collocations, and named-entities. Using a state-of-the-art statistical parser [3] as our baseline, our lexically-adapted parser achieves a 14.2% reduction in error. With oracle-knowledge of named-entities, this error reduction improves to 21.2%."
H05-1030,Effective Use of Prosody in Parsing Conversational Speech,2005,20,36,3,0,50144,jeremy kahn,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We identify a set of prosodic cues for parsing conversational speech and show how such features can be effectively incorporated into a statistical parsing model. On the Switchboard corpus of conversational speech, the system achieves improved parse accuracy over a state-of-the-art system which uses only lexical and syntactic features. Since removal of edit regions is known to improve downstream parse accuracy, we explore alternatives for edit detection and show that PCFGs are not competitive with more specialized techniques."
P04-1005,A {TAG}-based noisy-channel model of speech repairs,2004,7,89,2,0,4047,mark johnson,Proceedings of the 42nd Annual Meeting of the Association for Computational Linguistics ({ACL}-04),1,"This paper describes a noisy channel model of speech repairs, which can identify and correct repairs in speech transcripts. A syntactic parser is used as the source model, and a novel type of TAG-based transducer is the channel model. The use of TAG is motivated by the intuition that the reparandum is a rough copy of the repair. The model is trained and tested on the Switchboard disfluency-annotated corpus."
N04-1011,Sentence-Internal Prosody Does not Help Parsing the Way Punctuation Does,2004,10,21,3,0,23968,michelle gregory,Proceedings of the Human Language Technology Conference of the North {A}merican Chapter of the Association for Computational Linguistics: {HLT}-{NAACL} 2004,0,None
ringger-etal-2004-using,Using the {P}enn {T}reebank to Evaluate Non-Treebank Parsers,2004,12,13,3,0,30821,eric ringger,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes a method for conducting evaluations of Treebank and non-Treebank parsers alike against the English language U. Penn Treebank (Marcus et al., 1993) using a metric that focuses on the accuracy of relatively non-controversial aspects of parse structure. Our conjecture is that if we focus on maximal projections of heads (MPH), we are likely to find much broader agreement than if we try to evaluate based on order of attachment. We hope that this method may find wider acceptance and be useful in establishing a generally applicable framework for evaluation in natural language parsing. We employ this method in an evaluation of NLPWin (Heidorn, 2000), a parser developed at Microsoft Research without reference to the Penn Treebank, and, for comparison, the well-known statistical Treebank parser of Charniak (2000)."
W03-1009,Variation of Entropy and Parse Trees of Sentences as a Function of the Sentence Number,2003,9,39,2,1,13188,dmitriy genzel,Proceedings of the 2003 Conference on Empirical Methods in Natural Language Processing,0,"In this paper we explore the variation of sentences as a function of the sentence number. We demonstrate that while the entropy of the sentence increases with the sentence number, it decreases at the paragraph boundaries in accordance with the Entropy Rate Constancy principle (introduced in related work). We also demonstrate that the principle holds for different genres and languages and explore the role of genre informativeness. We investigate potential causes of entropy variation by looking at the tree depth, the branching factor, the size of constituents, and the occurrence of gapping."
2003.mtsummit-papers.6,Syntax-based language models for statistical machine translation,2003,-1,-1,1,1,35621,eugene charniak,Proceedings of Machine Translation Summit IX: Papers,0,"We present a syntax-based language model for use in noisy-channel machine translation. In particular, a language model based upon that described in (Cha01) is combined with the syntax based translation-model described in (YK01). The resulting system was used to translate 347 sentences from Chinese to English and compared with the results of an IBM-model-4-based system, as well as that of (YK02), all trained on the same data. The translations were sorted into four groups: good/bad syntax crossed with good/bad meaning. While the total number of translations that preserved meaning were the same for (YK02) and the syntax-based system (and both higher than the IBM-model-4-based system), the syntax based system had 45{\%} more translations that also had good syntax than did (YK02) (and approximately 70{\%} more than IBM Model 4). The number of translations that did not preserve meaning, but at least had good grammar, also increased, though to less avail."
W02-1007,Parsing and Disfluency Placement,2002,5,9,2,0,53211,donald engel,Proceedings of the 2002 Conference on Empirical Methods in Natural Language Processing ({EMNLP} 2002),0,"It has been suggested that some forms of speech disfluencies, most notable interjections and parentheticals, tend to occur disproportionally at major clause boundaries [6] and thus might serve to aid parsers in establishing these boundaries. We have tested a current statistical parser [1] on Switchboard text with and without interjections and parentheticals and found that the parser performed better when not faced with these extra phenomena. This suggest that for current parsers, at least, interjection and parenthetical placement does not help in the parsing process."
P02-1026,Entropy Rate Constancy in Text,2002,8,102,2,1,13188,dmitriy genzel,Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics,1,We present a constancy rate principle governing language generation. We show that this principle implies that local measures of entropy (ignoring context) should increase with the sentence number. We demonstrate that this is indeed the case by measuring entropy in three different ways. We also show that this effect has both lexical (which words are used) and non-lexical (how the words are used) causes.
P01-1017,Immediate-Head Parsing for Language Models,2001,18,296,1,1,35621,eugene charniak,Proceedings of the 39th Annual Meeting of the Association for Computational Linguistics,1,"We present two language models based upon an immediate-head parser --- our name for a parser that conditions all events below a constituent c upon the head of c. While all of the most accurate statistical parsers are of the immediate-head variety, no previous grammatical language model uses this technology. The perplexity for both of these models significantly improve upon the trigram model base-line as well as the best previous grammar-based language model. For the better of our two models these improvements are 24% and 14% respectively. We also suggest that improvement of the underlying parser should significantly improve the model's perplexity and that even in the near term there is a lot of potential for improvement in immediate-head language models."
N01-1007,Unsupervised Learning of Name Structure From Coreference Data,2001,11,18,1,1,35621,eugene charniak,Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We present two methods for learning the structure of personal names from unlabeled data. The first simply uses a few implicit constraints governing this structure to gain a toehold on the problem --- e.g., descriptors come before first names, which come before middle names, etc. The second model also uses possible coreference information. We found that coreference constraints on names improve the performance of the model from 92.6% to 97.0%. We are interested in this problem in its own right, but also as a possible way to improve named entity recognition (by recognizing the structure of different kinds of names) and as a way to improve noun-phrase coreference determination."
N01-1016,Edit Detection and Parsing for Transcribed Speech,2001,16,119,1,1,35621,eugene charniak,Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We present a simple architecture for parsing transcribed speech in which an edited-word detector first removes such words from the sentence string, and then a standard statistical parser trained on transcribed speech parses the remaining words. The edit detector achieves a misclassification rate on edited words of 2.2%. (The NULL-model, which marks everything as not edited, has an error rate of 5.9%.) To evaluate our parsing results we introduce a new evaluation metric, the purpose of which is to make evaluation of a parse tree relatively indifferent to the exact tree position of EDITED nodes. By this metric the parser achieves 85.3% precision and 86.5% recall."
W00-1604,"Measuring Efficiency in High-accuracy, Broad-coverage Statistical Parsing",2000,15,11,2,1,4293,brian roark,Proceedings of the {COLING}-2000 Workshop on Efficiency In Large-Scale Parsing Systems,0,"Very little attention has been paid to the comparison of efficiency between high accuracy statistical parsers. This paper proposes one machine-independent metric that is general enough to allow comparisons across very different parsing architectures. This metric, which we call events considered, measures the number of events, however they are defined for a particular parser, for which a probability must be calculated, in order to find the parse. It is applicable to single-pass or multi-stage parsers. We discuss the advantages of the metric, and demonstrate its usefulness by using it to compare two parsers which differ in several fundamental ways."
W00-0601,Reading Comprehension Programs in a Statistical-Language-Processing Class,2000,4,29,1,1,35621,eugene charniak,{ANLP}-{NAACL} 2000 Workshop: Reading Comprehension Tests as Evaluation for Computer-Based Language Understanding Systems,0,"We present some new results for the reading comprehension task described in [3] that improve on the best published results - from 36% in [3] to 41% (the best of the systems described herein). We discuss a variety of techniques that tend to give small improvements, ranging from the fairly simple (give verbs more weight in answer selection) to the fairly complex (use specific techniques for answering specific kinds of questions)."
A00-2018,A Maximum-Entropy-Inspired Parser,2000,17,1485,1,1,35621,eugene charniak,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"We present a new parser for parsing down to Penn tree-bank style parse trees that achieves 90.1% average precision/recall for sentences of length 40 and less, and 89.5% for sentences of length 100 and less when trained and tested on the previously established [5, 9, 10, 15, 17] standard sections of the Wall Street Journal treebank. This represents a 13% decrease in error rate over the best single-parser results on this corpus [9]. The major technical innovation is the use of a maximum-entropy-inspired model for conditioning and smoothing that let us successfully to test and combine many different conditioning events. We also present some partial results showing the effects of different conditioning information, including a surprising 2% improvement due to guessing the lexical head's pre-terminal before guessing the lexical head."
A00-2031,Assigning Function Tags to Parsed Text,2000,7,121,2,1,53214,don blaheta,1st Meeting of the North {A}merican Chapter of the Association for Computational Linguistics,0,"It is generally recognized that the common nonterminal labels for syntactic constituents (NP, VP, etc.) do not exhaust the syntactic and semantic information one would like about parts of a syntactic tree. For example, the Penn Tree-bank gives each constituent zero or more 'function tags' indicating semantic roles and other related information not easily encapsulated in the simple constituent labels. We present a statistical algorithm for assigning these function tags that, on text already parsed to a simple-label level, achieves an F-measure of 87%, which rises to 99% when considering 'no tag' as a valid choice."
W99-0609,Determining the specificity of nouns from text,1999,7,53,2,1,54850,sharon caraballo,1999 Joint {SIGDAT} Conference on Empirical Methods in Natural Language Processing and Very Large Corpora,0,"In this work, we use a large text corpus to order nouns by their level of specificity. This semantic information can for most nouns be determined with over 80% accuracy using simple statistics from a text corpus without using any additional sources of semantic knowledge. This kind of semantic information can be used to help in automatically constructing or augmenting a lexical database such as WordNet."
P99-1008,Finding Parts in Very Large Corpora,1999,9,428,2,0,54910,matthew berland,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"We present a method for extracting parts of objects from wholes (e.g. speedometer from car). Given a very large corpus our method finds part words with 55% accuracy for the top 50 words as ranked by the system. The part list could be scanned by an end-user and added to an existing ontology (such as WordNet), or used as a part of a rough semantic lexicon."
P99-1066,Automatic Compensation for Parser Figure-of-Merit Flaws,1999,8,13,2,1,53214,don blaheta,Proceedings of the 37th Annual Meeting of the Association for Computational Linguistics,1,"Best-first chart parsing utilises a figure of merit (FOM) to efficiently guide a parse by first attending to those edges judged better. In the past it has usually been static; this paper will show that with some extra information, a parser can compensate for FOM flaws which otherwise slow it down. Our results are faster than the prior best by a factor of 2.5; and the speedup is won with no significant decrease in parser accuracy."
W98-1115,Edge-Based Best-First Chart Parsing,1998,12,4,1,1,35621,eugene charniak,Sixth Workshop on Very Large Corpora,0,"Natural language grammars are often very large and full of ambiguities, making standard computer parsers too slow to be practical for many tasks. Best-first parsing attempts to address this problem by preferentially working to expand subparses that are judged ``good'''' by some probabilistic figure of merit. We explain the standard non-probabilistic and best-first chart parsing paradigms, then describe a new method of best-first parsing which improves upon previous work by ranking subparses at a more fine-grained level, speeding up parsing by approximately a factor of 20 over the best previous results. Moreover, these results are achieved with a higher level of accuracy than is obtained by parsing to exhaustion."
W98-1119,A Statistical Approach to Anaphora Resolution,1998,8,197,3,0,19679,niyu ge,Sixth Workshop on Very Large Corpora,0,None
P98-2182,Noun-Phrase Co-occurrence Statistics for Semi-Automatic Semantic Lexicon Construction,1998,8,132,2,1,4293,brian roark,"36th Annual Meeting of the Association for Computational Linguistics and 17th International Conference on Computational Linguistics, Volume 2",0,"Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an enhancer of existing broad-coverage resources."
J98-2004,New Figures of Merit for Best-First Probabilistic Chart Parsing,1998,14,95,2,1,54850,sharon caraballo,Computational Linguistics,0,"Best-first parsing methods for natural language try to parse efficiently by considering the most likely constituents first. Some figure of merit is needed by which to compare the likelihood of constituents, and the choice of this figure has a substantial impact on the efficiency of the parser. While several parsers described in the literature have used such techniques, there is little published data on their efficacy, much less attempts to judge their relative merits. We propose and evaluate several figures of merit for best-first parsing, and we identify an easily computable figure of merit that provides excellent performance on various measures and two different grammars."
C98-2177,Noun-phrase co-occurrence statistics for semi-automatic semantic lexicon construction,1998,8,132,2,1,4293,brian roark,{COLING} 1998 Volume 2: The 17th International Conference on Computational Linguistics,0,"Generating semantic lexicons semi-automatically could be a great time saver, relative to creating them by hand. In this paper, we present an algorithm for extracting potential entries for a category from an on-line corpus, based upon a small set of exemplars. Our algorithm finds more correct terms and fewer incorrect ones than previous work in this area. Additionally, the entries that are generated potentially provide broader coverage of the category than would occur to an individual coding them by hand. Our algorithm finds many terms not included within Wordnet (many more than previous algorithms), and could be viewed as an enhancer of existing broad-coverage resources."
W96-0212,Figures of Merit for Best-First Probabilistic Chart Parsing,1996,0,8,2,1,54850,sharon caraballo,Conference on Empirical Methods in Natural Language Processing,0,None
P88-1011,A Logic for Semantic Interpretation,1988,10,70,1,1,35621,eugene charniak,26th Annual Meeting of the Association for Computational Linguistics,1,"We propose that logic (enhanced to encode probability information) is a good way of characterizing semantic interpretation. In support of this we give a fragment of an axiomatization for word-sense disambiguation, nounphrase (and verb) reference, and case disambiguation. We describe an inference engine (Frail3) which actually takes this axiomatization and uses it to drive the semantic interpretation process. We claim three benefits from this scheme. First, the interface between semantic interpretation and pragmatics has always been problematic, since all of the above tasks in general require pragmatic inference. Now the interface is trival, since both semantic interpretation and pragmatics use the same vocabulary and inference engine. The second benefit, related to the first, is that semantic guidance of syntax is a side effect of the interpretation. The third benefit is the elegance of the semantic interpretation theory. A few simple rules capture a remarkable diversity of semantic phenomena."
T87-1015,Connectionism and Explanation,1987,0,12,1,1,35621,eugene charniak,Theoretical Issues in Natural Language Processing 3,0,"More generally, the problem of representing new proposit.ions in connectionist networks is a real mess. The connectionists know about this however, and t, here has been some work on the topic. The basic idea is tha t one uses the s ta te of the entire network to represent a proposition ra ther than concentra t ing it a t a node. So, one might have one set o1 nodes which represent the first a rgument of a proposition, one for the second, and one for the predicate, and each set of nodes could indicate different individuals, depending on the pat tern . The networks could be tra.ined so that , say, if  fa ther -of was the propositi.ou, it, would tend to like  j ack as argument one, and  a n n  as a rgument two, assuming one wanted to store the fact that, Jack is the father of Ann. Using the connectionist abili ty to complete pat terns, one can a.lso see how such a network might fill in  j a c k  if both  fa ther -of and  a n n  where put into the appropr ia te places. David McClelland does something like this in his work on case assignment using connectionist networks. There are other ways to repreresent propositions as well (perhaps best being the work of David Touretzky) , but they all loose what is so nice about the unsophisticated version of the networks. Before, the network as a whole represented the situation as a whole, and filling in"
P86-1003,Time and Tense in {E}nglish,1986,7,11,2,0.244577,40215,mary harper,24th Annual Meeting of the Association for Computational Linguistics,1,"Tense, temporal adverbs, and temporal connectives provide information about when events described in English sentences occur. To extract this temporal information from a sentence, it must be parsed into a semantic representation which captures the meaning of tense, temporal adverbs, and temporal connectives. Representations were developed for the basic tenses, some temporal adverbs, as well as some of the temporal connectives. Five criteria were suggested for judging these representations, and based on these criteria the representations were judged."
T78-1027,With a Spoon in Hand This Must Be the Eating Frame,1978,-1,-1,1,1,35621,eugene charniak,Theoretical Issues in Natural Language Processing-2,0,None
J78-3035,With a Spoon in Hand This Must Be the Eating Frame,1978,-1,-1,1,1,35621,eugene charniak,American Journal of Computational Linguistics,0,None
T75-2010,Organization and Inference in a Frame-Like System of Common Sense Knowledge,1975,4,43,1,1,35621,eugene charniak,Theoretical Issues in Natural Language Processing,0,"My goals have not changed since (Charniak 72). I am still interested in the construction of a computer program which will answer questions about simple narration (e.g. children's stories). More exactly, if one makes the somewhat unrealistic division of the problem into (a) going from natural language to a convenient internal representation, and (b) being able to reason about the information in the story in order to answer questions, my interests are clearly in the latter section. I will take it as given that such reasoning requires large amounts of common sense knowledge about the topics mentioned in the text, so I will not demonstrate this point. (However it should come out incidentally from the examples used to demonstrate other points.) To reason with this knowledge requires that it be organized, by which I simply mean it must be structured so that the system can get at necessary knowledge when it is needed, but that unnecessary knowledge will not clog the system with the all too familiar combinatorial explosion. I will start with my current thoughts on organization."
