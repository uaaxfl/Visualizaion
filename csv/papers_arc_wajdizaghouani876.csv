2021.wanlp-1.36,Overview of the {WANLP} 2021 Shared Task on Sarcasm and Sentiment Detection in {A}rabic,2021,-1,-1,2,0,497,ibrahim farha,Proceedings of the Sixth Arabic Natural Language Processing Workshop,0,"This paper provides an overview of the WANLP 2021 shared task on sarcasm and sentiment detection in Arabic. The shared task has two subtasks: sarcasm detection (subtask 1) and sentiment analysis (subtask 2). This shared task aims to promote and bring attention to Arabic sarcasm detection, which is crucial to improve the performance in other tasks such as sentiment analysis. The dataset used in this shared task, namely ArSarcasm-v2, consists of 15,548 tweets labelled for sarcasm, sentiment and dialect. We received 27 and 22 submissions for subtasks 1 and 2 respectively. Most of the approaches relied on using and fine-tuning pre-trained language models such as AraBERT and MARBERT. The top achieved results for the sarcasm detection and sentiment analysis tasks were 0.6225 F1-score and 0.748 F1-PN respectively."
2021.nlp4if-1.12,Findings of the {NLP}4{IF}-2021 Shared Tasks on Fighting the {COVID}-19 Infodemic and Censorship Detection,2021,-1,-1,5,0,1632,shaden shaar,"Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda",0,"We present the results and the main findings of the NLP4IF-2021 shared tasks. Task 1 focused on fighting the COVID-19 infodemic in social media, and it was offered in Arabic, Bulgarian, and English. Given a tweet, it asked to predict whether that tweet contains a verifiable claim, and if so, whether it is likely to be false, is of general interest, is likely to be harmful, and is worthy of manual fact-checking; also, whether it is harmful to society, and whether it requires the attention of policy makers. Task 2 focused on censorship detection, and was offered in Chinese. A total of ten teams submitted systems for task 1, and one team participated in task 2; nine teams also submitted a system description paper. Here, we present the tasks, analyze the results, and discuss the system submissions and the methods they used. Most submissions achieved sizable improvements over several baselines, and the best systems used pre-trained Transformers and ensembles. The data, the scorers and the leaderboards for the tasks are available at http://gitlab.com/NLP4IF/nlp4if-2021."
2021.findings-emnlp.56,"Fighting the {COVID}-19 Infodemic: Modeling the Perspective of Journalists, Fact-Checkers, Social Media Platforms, Policy Makers, and the Society",2021,-1,-1,12,0,1633,firoj alam,Findings of the Association for Computational Linguistics: EMNLP 2021,0,"With the emergence of the COVID-19 pandemic, the political and the medical aspects of disinformation merged as the problem got elevated to a whole new level to become the first global infodemic. Fighting this infodemic has been declared one of the most important focus areas of the World Health Organization, with dangers ranging from promoting fake cures, rumors, and conspiracy theories to spreading xenophobia and panic. Addressing the issue requires solving a number of challenging problems such as identifying messages containing claims, determining their check-worthiness and factuality, and their potential to do harm as well as the nature of that harm, to mention just a few. To address this gap, we release a large dataset of 16K manually annotated tweets for fine-grained disinformation analysis that (i) focuses on COVID-19, (ii) combines the perspectives and the interests of journalists, fact-checkers, social media platforms, policy makers, and society, and (iii) covers Arabic, Bulgarian, Dutch, and English. Finally, we show strong evaluation results using pretrained Transformers, thus confirming the practical utility of the dataset in monolingual vs. multilingual, and single task vs. multitask settings."
2020.osact-1.4,Building a Corpus of Qatari {A}rabic Expressions,2020,-1,-1,2,0,15937,sara almulla,"Proceedings of the 4th Workshop on Open-Source Arabic Corpora and Processing Tools, with a Shared Task on Offensive Language Detection",0,"The current Arabic natural language processing resources are mainly build to address the Modern Standard Arabic (MSA), while we witnessed some scattered efforts to build resources for various Arabic dialects such as the Levantine and the Egyptian dialects. We observed a lack of resources for Gulf Arabic and especially the Qatari variety. In this paper, we present the first Qatari idioms and expression corpus of 1000 entries. The corpus was created from on-line and printed sources in addition to transcribed recorded interviews. The corpus covers various Qatari traditional expressions and idioms. To this end, audio recordings were collected from interviews and an online survey questionnaire was conducted to validate our data. This corpus aims to help advance the dialectal Arabic Speech and Natural Language Processing tools and applications for the Qatari dialect."
2020.lrec-1.768,{DAICT}: A Dialectal {A}rabic Irony Corpus Extracted from {T}witter,2020,-1,-1,2,0,18132,ines abbes,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Identifying irony in user-generated social media content has a wide range of applications; however to date Arabic content has received limited attention. To bridge this gap, this study builds a new open domain Arabic corpus annotated for irony detection. We query Twitter using irony-related hashtags to collect ironic messages, which are then manually annotated by two linguists according to our working definition of irony. Challenges which we have encountered during the annotation process reflect the inherent limitations of Twitter messages interpretation, as well as the complexity of Arabic and its dialects. Once published, our corpus will be a valuable free resource for developing open domain systems for automatic irony recognition in Arabic language and its dialects in social media text."
R19-1023,A Fine-Grained Annotated Multi-Dialectal {A}rabic Corpus,2019,0,0,2,0,25283,anis charfi,Proceedings of the International Conference on Recent Advances in Natural Language Processing (RANLP 2019),0,"We present ARAP-Tweet 2.0, a corpus of 5 million dialectal Arabic tweets and 50 million words of about 3000 Twitter users from 17 Arab countries. Compared to the first version, the new corpus has significant improvements in terms of the data volume and the annotation quality. It is fully balanced with respect to dialect, gender, and three age groups: under 25 years, between 25 and 34, and 35 years and above. This paper describes the process of creating the corpus starting from gathering the dialectal phrases to find the users, to annotating their accounts and retrieving their tweets. We also report on the evaluation of the annotation quality using the inter-annotator agreement measures which were applied to the whole corpus and not just a subset. The obtained results were substantial with average Cohen{'}s Kappa values of 0.99, 0.92, and 0.88 for the annotation of gender, dialect, and age respectively. We also discuss some challenges encountered when developing this corpus.s."
L18-1111,"Arap-Tweet: A Large Multi-Dialect {T}witter Corpus for Gender, Age and Language Variety Identification",2018,0,4,1,1,579,wajdi zaghouani,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1415,{MADAR}i: A Web Interface for Joint {A}rabic Morphological Annotation and Spelling Correction,2018,0,0,5,1,18329,ossama obeid,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1535,The {MADAR} {A}rabic Dialect Corpus and Lexicon,2018,0,9,4,0.199436,516,houda bouamor,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
L18-1574,Unified Guidelines and Resources for {A}rabic Dialect Orthography,2018,-1,-1,8,0,517,nizar habash,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
W16-4115,Using Ambiguity Detection to Streamline Linguistic Annotation,2016,0,1,1,1,579,wajdi zaghouani,Proceedings of the Workshop on Computational Linguistics for Linguistic Complexity ({CL}4{LC}),0,"Arabic writing is typically underspecified for short vowels and other markups, referred to as diacritics. In addition to the lexical ambiguity exhibited in most languages, the lack of diacritics in written Arabic adds another layer of ambiguity which is an artifact of the orthography. In this paper, we present the details of three annotation experimental conditions designed to study the impact of automatic ambiguity detection, on annotation speed and quality in a large scale annotation project."
L16-1295,Building an {A}rabic Machine Translation Post-Edited Corpus: Guidelines and Annotation,2016,0,5,1,1,579,wajdi zaghouani,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"We present our guidelines and annotation procedure to create a human corrected machine translated post-edited corpus for the Modern Standard Arabic. Our overarching goal is to use the annotated corpus to develop automatic machine translation post-editing systems for Arabic that can be used to help accelerate the human revision process of translated texts. The creation of any manually annotated corpus usually presents many challenges. In order to address these challenges, we created comprehensive and simplified annotation guidelines which were used by a team of five annotators and one lead annotator. In order to ensure a high annotation agreement between the annotators, multiple training sessions were held and regular inter-annotator agreement measures were performed to check the annotation quality. The created corpus of manual post-edited translations of English to Arabic articles is the largest to date for this language pair."
L16-1577,Guidelines and Framework for a Large Scale {A}rabic Diacritized Corpus,2016,19,6,1,1,579,wajdi zaghouani,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"This paper presents the annotation guidelines developed as part of an effort to create a large scale manually diacritized corpus for various Arabic text genres. The target size of the annotated corpus is 2 million words. We summarize the guidelines and describe issues encountered during the training of the annotators. We also discuss the challenges posed by the complexity of the Arabic language and how they are addressed. Finally, we present the diacritization annotation procedure and detail the quality of the resulting annotations."
L16-1578,Applying the Cognitive Machine Translation Evaluation Approach to {A}rabic,2016,27,1,2,0,23303,irina temnikova,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"The goal of the cognitive machine translation (MT) evaluation approach is to build classifiers which assign post-editing effort scores to new texts. The approach helps estimate fair compensation for post-editors in the translation industry by evaluating the cognitive difficulty of post-editing MT output. The approach counts the number of errors classified in different categories on the basis of how much cognitive effort they require in order to be corrected. In this paper, we present the results of applying an existing cognitive evaluation approach to Modern Standard Arabic (MSA). We provide a comparison of the number of errors and categories of errors in three MSA texts of different MT quality (without any language-specific adaptation), as well as a comparison between MSA texts and texts from three Indo-European languages (Russian, Spanish, and Bulgarian), taken from a previous experiment. The results show how the error distributions change passing from the MSA texts of worse MT quality to MSA texts of better MT quality, as well as a similarity in distinguishing the texts of better MT quality for all four languages."
W15-5116,Generating acceptable {A}rabic Core Vocabularies and Symbols for {AAC} users,2015,20,0,5,0,36520,ea draffan,Proceedings of {SLPAT} 2015: 6th Workshop on Speech and Language Processing for Assistive Technologies,0,"This paper discusses the development of an Arabic Symbol Dictionary for Augmentative and Alternative Communication (AAC) users, their families, carers, therapists and teachers as well as those who may benefit from the use of symbols to enhance literacy skills. With a requirement for a bi-lingual dictionary, a vocabulary list analyzer has been developed to evaluate similarities and differences in word frequencies from a range of word lists in order to collect suitable AAC lexical entries. An online bespoke symbol management has been created to hold the lexical entries alongside specifically designed symbols which are then accepted via a voting system using a series of criteria. Results to date have highlighted how successful these systems can be when encouraging participation along with the need for further research into the development of personalised context sensitive core vocabularies."
W15-3204,The Second {QALB} Shared Task on Automatic Text Correction for {A}rabic,2015,-1,-1,4,0.19826,8356,alla rozovskaya,Proceedings of the Second Workshop on {A}rabic Natural Language Processing,0,None
W15-3209,A Pilot Study on {A}rabic Multi-Genre Corpus Diacritization,2015,13,6,2,0.213874,516,houda bouamor,Proceedings of the Second Workshop on {A}rabic Natural Language Processing,0,"Arabic script writing is typically underspecified for short vowels and other mark up, referred to as diacritics. Apart from the lexical ambiguity found in words, similar to that exhibited in other languages, the lack of diacritics in written Arabic script adds another layer of ambiguity which is an artifact of the orthography. Diacritization of written text has a significant impact on Arabic NLP applications. In this paper, we present a pilot study on building a diacritized multi-genre corpus in Arabic. We annotate a sample of nondiacritized words extracted from five text genres. We explore different annotation strategies: Basic where we present only the bare undiacritized forms to the annotators, Intermediate (Basic formstheir POS tags), and Advanced (automatically diacritized words). We present the impact of the annotation strategy on annotation quality. Moreover, we study different diacritization schemes in the process."
W15-3219,{SAHSOH}@{QALB}-2015 Shared Task: A Rule-Based Correction Method of Common {A}rabic Native and Non-Native Speakers{'} Errors,2015,17,0,1,1,579,wajdi zaghouani,Proceedings of the Second Workshop on {A}rabic Natural Language Processing,0,This paper describes our participation in the QALB-2015 Automatic Correction of Arabic Text shared task. We employed various tools and external resources to build a rule based correction method. Hand written linguistic rules were added by using existing lexicons and regular expressions. We handled specific errors with dedicated rules reserved for nonnative speakers. The system is simple as it does not employ any sophisticated machine learning methods and it does not correct punctuation errors. The system achieved results comparable to other approaches when the punctuation errors are ignored with an F1 of 66.9% for native speakersxe2x80x99 data and an F1 of 31.72% for the non-native speakersxe2x80x99 data.
W15-1614,Correction Annotation for Non-Native {A}rabic Texts: Guidelines and Corpus,2015,30,15,1,1,579,wajdi zaghouani,Proceedings of The 9th Linguistic Annotation Workshop,0,"We present our correction annotation guidelines to create a manually corrected nonnative (L2) Arabic corpus. We develop our approach by extending an L1 large-scale Arabic corpus and its manual corrections, to include manually corrected non-native Arabic learner essays. Our overarching goal is to use the annotated corpus to develop components for automatic detection and correction of language errors that can be used to help Standard Arabic learners (native and non-native) improve the quality of the Arabic text they produce. The created corpus of L2 text manual corrections is the largest to date. We evaluate our guidelines using inter-annotator agreement and show a high degree of consistency."
W14-3605,The First {QALB} Shared Task on Automatic Text Correction for {A}rabic,2014,33,38,4,0,35024,behrang mohit,Proceedings of the {EMNLP} 2014 Workshop on {A}rabic Natural Language Processing ({ANLP}),0,"We present a summary of the first shared task on automatic text correction for Arabic text. The shared task received 18 systems submissions from nine teams in six countries and represented a diversity of approaches. Our report includes an overview of the QALB corpus which was the source of the datasets used for training and evaluation, an overview of participating systems, results of the competition and an analysis of the results and systems."
W14-3618,{CMUQ}@{QALB}-2014: An {SMT}-based System for Automatic {A}rabic Error Correction,2014,22,5,3,0,23961,serena jeblee,Proceedings of the {EMNLP} 2014 Workshop on {A}rabic Natural Language Processing ({ANLP}),0,"In this paper, we describe the CMUQ system we submitted to The ANLP-QALB 2014 Shared Task on Automatic Text Correction for Arabic. Our system combines rule-based linguistic techniques with statistical language modeling techniques and machine translationbased methods. Our system outperforms the baseline and reaches an F-score of 65.42% on the test set of QALB corpus. This ranks us 3rd in the competition."
zaghouani-dukes-2014-crowdsourcing,Can Crowdsourcing be used for Effective Annotation of {A}rabic?,2014,20,2,1,1,579,wajdi zaghouani,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"Crowdsourcing has been used recently as an alternative to traditional costly annotation by many natural language processing groups. In this paper, we explore the use of Amazon Mechanical Turk (AMT) in order to assess the feasibility of using AMT workers (also known as Turkers) to perform linguistic annotation of Arabic. We used a gold standard data set taken from the Quran corpus project annotated with part-of-speech and morphological information. An Arabic language qualification test was used to filter out potential non-qualified participants. Two experiments were performed, a part-of-speech tagging task in where the annotators were asked to choose a correct word-category from a multiple choice list and case ending identification task. The results obtained so far showed that annotating Arabic grammatical case is harder than POS tagging, and crowdsourcing for Arabic linguistic annotation requiring expert annotators could be not as effective as other crowdsourcing experiments requiring less expertise and qualifications."
zaghouani-etal-2014-large,Large Scale {A}rabic Error Annotation: Guidelines and Framework,2014,22,46,1,1,579,wajdi zaghouani,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"We present annotation guidelines and a web-based annotation framework developed as part of an effort to create a manually annotated Arabic corpus of errors and corrections for various text types. Such a corpus will be invaluable for developing Arabic error correction tools, both for training models and as a gold standard for evaluating error correction algorithms. We summarize the guidelines we created. We also describe issues encountered during the training of the annotators, as well as problems that are specific to the Arabic language that arose during the annotation process. Finally, we present the annotation tool that was developed as part of this project, the annotation pipeline, and the quality of the resulting annotations."
I13-2001,A Web-based Annotation Framework For Large-Scale Text Correction,2013,6,8,2,1,18329,ossama obeid,The Companion Volume of the Proceedings of {IJCNLP} 2013: System Demonstrations,0,"We demonstrate a web-based, languageindependent annotation framework used for manual correction of a large Arabic corpus. Our framework provides intuitive interfaces for annotating text and managing the annotation process. We describe the details of both the annotation and the administration interfaces as well as the back-end engine. We also show how this framework is able to speed up the annotation process by employing automated annotators to fix basic Arabic spelling errors."
W12-2511,A Pilot {P}rop{B}ank Annotation for Quranic {A}rabic,2012,12,8,1,1,579,wajdi zaghouani,Proceedings of the {NAACL}-{HLT} 2012 Workshop on Computational Linguistics for Literature,0,"The Quran is a significant religious text written in a unique literary style, close to very poetic language in nature. Accordingly it is significantly richer and more complex than the newswire style used in the previously released Arabic PropBank (Zaghouani et al., 2010; Diab et al., 2008). We present preliminary work on the creation of a unique Arabic proposition repository for Quranic Arabic. We annotate the semantic roles for the 50 most frequent verbs in the Quranic Arabic Dependency Treebank (QATB) (Dukes and Buckwalter 2010). The Quranic Arabic PropBank (QAPB) will be a unique new resource of its kind for the Arabic NLP research community as it will allow for interesting insights into the semantic use of classical Arabic, poetic literary Arabic, as well as significant religious texts. Moreover, on a pragmatic level QAPB will add approximately 810 new verbs to the existing Arabic PropBank (APB). In this pilot experiment, we leverage our knowledge and experience from our involvement in the APB project. All the QAPB annotations will be made freely available for research purposes."
W12-2015,Developing {ARET}: An {NLP}-based Educational Tool Set for {A}rabic Reading Enhancement,2012,10,5,2,0,42383,mohammed maamouri,Proceedings of the Seventh Workshop on Building Educational Applications Using {NLP},0,"This paper describes a novel Arabic Reading Enhancement Tool (ARET) for classroom use, which has been built using corpus-based Natural Language Processing in combination with expert linguistic annotation. The NLP techniques include a widely used morphological analyzer for Modern Standard Arabic to provide word-level grammatical details, and a relational database index of corpus texts to provide word concordances. ARET also makes use of a commercial Arabic text-to-speech (TTS) system to add a speech layer (with male and female voices) to the Al-Kitaab language textbook resources. The system generates test questions and distractors, offering teachers and students an interesting computer-aided language learning tool. We describe the background and the motivation behind the building of ARET, presenting the various components and the method used to build the tools."
W10-1836,The Revised {A}rabic {P}rop{B}ank,2010,26,23,1,1,579,wajdi zaghouani,Proceedings of the Fourth Linguistic Annotation Workshop,0,The revised Arabic PropBank (APB) reflects a number of changes to the data and the process of PropBanking. Several changes stem from Treebank revisions. An automatic process was put in place to map existing annotation to the new trees. We have revised the original 493 Frame Files from the Pilot APB and added 1462 new files for a total of 1955 Frame Files with 2446 framesets. In addition to a heightened attention to sense distinctions this cycle includes a greater attempt to address complicated predicates such as light verb constructions and multi-word expressions. New tools facilitate the data tagging and also simplify frame creation.
maamouri-etal-2010-speech,From Speech to Trees: Applying Treebank Annotation to {A}rabic Broadcast News,2010,7,5,4,0,34888,mohamed maamouri,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"The Arabic Treebank (ATB) Project at the Linguistic Data Consortium (LDC) has embarked on a large corpus of Broadcast News (BN) transcriptions, and this has led to a number of new challenges for the data processing and annotation procedures that were originally developed for Arabic newswire text (ATB1, ATB2 and ATB3). The corpus requirements currently posed by the DARPA GALE Program, including English translation of Arabic BN transcripts, word-level alignment of Arabic and English data, and creation of a corresponding English Treebank, place significant new constraints on ATB corpus creation, and require careful coordination among a wide assortment of concurrent activities and participants. Nonetheless, in spite of the new challenges posed by BN data, the ATBÂs newly improved pipeline and revised annotation guidelines for newswire have proven to be robust enough that very few changes were necessary to account for the new genre of data. This paper presents the points where some adaptation has been necessary, and the overall pipeline as used in the production of BN ATB data."
zaghouani-etal-2010-adapting,Adapting a resource-light highly multilingual Named Entity Recognition system to {A}rabic,2010,16,21,1,1,579,wajdi zaghouani,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"We present a fully functional Arabic information extraction (IE) system that is used to analyze large volumes of news texts every day to extract the named entity (NE) types person, organization, location, date and number, as well as quotations (direct reported speech) by and about people. The Named Entity Recognition (NER) system was not developed for Arabic, but - instead - a highly multilingual, almost language-independent NER system was adapted to also cover Arabic. The Semitic language Arabic substantially differs from the Indo-European and Finno-Ugric languages currently covered. This paper thus describes what Arabic language-specific resources had to be developed and what changes needed to be made to the otherwise language-independent rule set in order to be applicable to the Arabic language. The achieved evaluation results are generally satisfactory, but could be improved for certain entity types. The results of the IE tools can be seen on the Arabic pages of the freely accessible Europe Media Monitor (EMM) application NewsExplorer, which can be found at http://press.jrc.it/overview.html."
2010.jeptalnrecital-demonstration.4,L{'}int{\\'e}gration d{'}un outil de rep{\\'e}rage d{'}entit{\\'e}s nomm{\\'e}es pour la langue arabe dans un syst{\\`e}me de veille,2010,-1,-1,1,1,579,wajdi zaghouani,Actes de la 17e conf{\\'e}rence sur le Traitement Automatique des Langues Naturelles. D{\\'e}monstrations,0,"Dans cette d{\'e}monstration, nous pr{\'e}sentons l{'}impl{\'e}mentation d{'}un outil de rep{\'e}rage d{'}entit{\'e}s nomm{\'e}es {\`a} base de r{\`e}gle pour la langue arabe dans le syst{\`e}me de veille m{\'e}diatique EMM (Europe Media Monitor)."
palmer-etal-2008-pilot,A Pilot {A}rabic {P}ropbank,2008,13,37,7,0,4859,martha palmer,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"In this paper, we present the details of creating a pilot Arabic proposition bank (Propbank). Propbanks exist for both English and Chinese. However the morphological and syntactic expression of linguistic phenomena in Arabic yields a very different type of process in creating an Arabic propbank. Hence, we highlight those characteristics of Arabic that make creating a propbank for the language a different challenge compared to the creation of an English Propbank.We believe that many of the lessons learned in dealing with Arabic could generalise to other languages that exhibit equally rich morphology and relatively free word order."
chiao-etal-2006-evaluation,Evaluation of multilingual text alignment systems: the {ARCADE} {II} project,2006,4,21,8,0,50126,yunchuang chiao,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"This paper describes the ARCADE II project, concerned with the evaluation of parallel text alignment systems. The ARCADE II project aims at exploring the techniques of multilingual text alignment through a fine evaluation of the existing techniques and the development of new alignment methods. The evaluation campaign consists of two tracks devoted to the evaluation of alignment at sentence and word level respectively. It differs from ARCADE I in the multilingual aspect and the investigation of lexical alignment."
pouliquen-etal-2006-geocoding,"Geocoding Multilingual Texts: Recognition, Disambiguation and Visualisation",2006,6,54,8,0,33358,bruno pouliquen,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"We are presenting a method to recognise geographical references in free text. Our tool must work on various languages with a minimum of language-dependent resources, except a gazetteer. The main difficulty is to disambiguate these place names by distinguishing places from persons and by selecting the most likely place out of a list of homographic place names world-wide. The system uses a number of language-independent clues and heuristics to disambiguate place name homographs. The final aim is to index texts with the countries and cities they mention and to automatically visualise this information on geographical maps using various tools."
