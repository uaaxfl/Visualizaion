2016.lilt-14.3,baker-etal-2010-modality,0,0.0173017,"s between 68.7 and 93.5, depending on the verb. Closer investigation of their data set, however, reveals a strong bias in sense distributions. As a consequence, the majority sense baselines are hard to beat: none of their classification models is able to beat the 3 Krippendorff’s α, see Krippendorff (2004) Modal Sense Classification At Large / 7 majority baseline with uniform settings across all modal verb types. Prabhakaran et al. (2012) present annotation and classification experiments on manually annotated data from different genres. Their annotation scheme is based on five categories from Baker et al. (2010): Ability, Effort, Intention, Success, and Want. They performed preselection based on an existing modality tagger and performed annotation using Amazon Mechanical Turk. Classification was performed with an SVM multiclass classification model based on lexical and shallow (no syntactic) features. They report 41.9 F1 score on heterogeneous gold data consisting of newswire, letters, blog, and email genres. By contrast, 71.9 F1 score was achieved for 4-fold cross-validation (CV) on a subset of homogeneous genre data consisting of email only. Using confidence values from annotator agreement in train"
2016.lilt-14.3,W12-3802,0,0.0295668,"rocessing. Recent work in Marasović and Frank (2016) show that convolutional neural networks are able to improve on manually crafted feature-based approaches and are easily portable to novel languages, while preserving semantic factors that were found to be relevant in the present work. Currently, we build lexically specific local classifier models. A single classifier model for all modal verbs could also be explored. Further extensions will include source and target role assignment. Possible application areas for our work are argumentation mining (Becker et al., 2016) and sentiment analysis (Benamara et al., 2012) or detection of speculative language. Lexical modal sense classification also has immediate applications in Machine Translation (Baker et al., 2012). Acknowledgments This work was partially funded by the Leibniz Science Campus Empirical Linguistics and Computational Language Modeling, funded by the Leibniz Association under grant no. SAS-2015-IDS-LWC and by the Ministry of Science, Research, and Art (MWK) of the state of BadenWürttemberg, as well as the German Research Foundation as part of the Research Training Group Adaptive Preparation of Information from Heterogeneous Sources (AIPHES) und"
2016.lilt-14.3,N09-2035,0,0.126008,"neous data (cf. Section 2).41 A natural next step from our current findings is to take further advantage of the manually labeled portions of the multi-genre corpus MASC to perform systematic domain or genre adaptation. This could be done by using sampling 41 Though we cannot compare directly to their work given that their annotation scheme and experimental data differs from ours. Modal Sense Classification At Large / 47 techniques that re-proportion training data to approximate the distribution of out-of-domain target genres. Another option is to instead adapt model parameters, as proposed in Bloodgood and Vijay-Shanker (2009), who adapt the cost factor of SVM classifier models to the estimated distribution of out-of-domain evaluation data. 9 Conclusions Modality is an important aspect of discourse analysis relating to argumentation, planning, and reasoning in intensional contexts. In this work, we focused on the classification of modal verb senses that carry different types of intensional meaning: epistemic, deontic and dynamic modality. Prior work in Ruppenhofer and Rehbein (2012) established a wellfounded annotation scheme for modal senses, provided manually annotated resources and induced lexical modal sense cl"
2016.lilt-14.3,W13-0304,0,0.0481662,"Missing"
2016.lilt-14.3,W08-1301,0,0.112187,"Missing"
2016.lilt-14.3,J12-2003,0,0.0627065,"Missing"
2016.lilt-14.3,P02-1033,0,0.251902,"See Section 5.3 for a more detailed discussion of MASC. 8 / LiLT volume 14, issue 3 August 2016 semantic role labeling (Padó and Lapata, 2009). In contrast to this line of work, we do not exploit existing and possibly noisy automatic classification models to project annotations to a novel target language. Our aim is to induce high-quality modal sense annotations from scratch, by exploiting unambiguous paraphrases of specific modal senses in a source language and projecting these senses to the target language. A similar technique has been applied for the induction of word sense annotations by Diab and Resnik (2002). 3 Research Questions and Overview Our leading hypothesis is that modal sense classification can be improved by semantically enriched feature sets. To explore this hypothesis, we aim to find automated methods for extending the size of the currently available data sets, to reduce sparsity and distributional bias. An open question is also whether modal sense classification models are able to generalize across different genres. We explore these questions in the following order: Q1: How can we resolve the manual annotation bottleneck for modal sense classification, in view of acquiring large-size"
2016.lilt-14.3,P14-2085,1,0.783597,"rporate semantic factors at various levels. In the following we motivate and describe our semantic feature set, which we organize in seven groups. An overview of the proposed features is given in Table 7.19 VB: Lexical features of the embedded verb. The embedded verb in the scope of the modal plays an important role in determining modal sense. For instance, with the embedded verb fly in (10.a), we prefer a dynamic reading of can, whereas with play in (10.b) we find a deontic reading. 19 In Zhou et al. (2015) our feature set included an additional feature group “Lexical aspect” (LA), following Friedrich and Palmer (2014). Significance tests showed that this group was not effective and thus can be omitted for overall simplicity. 18 / LiLT volume 14, issue 3 August 2016 (10) a. The children can fly (if they just believe, says Peter Pan)! b. The children can play outside. Building on the dependency parser20 output, we access the lemma of the embedded verb and its part-of-speech tag in the sentence. We also extract whether the verb has a particle (e.g. the plane could take off ), and if yes, which. In most cases, the relation between a modal verb and the embedded verb is given through the auxiliary (aux) dependen"
2016.lilt-14.3,N13-1092,0,0.0240711,"(2012). Ruppenhofer and Rehbein (2012) (henceforth, R&R) manually annotated modal verbs in the MPQA corpus of Wiebe et al. (2005). They build on the well-established inventory of modal sense categories of Kratzer (1991) in formal semantics: epistemic, deontic/bouletic and circumstantial/dynamic modality, illustrated in (3) above. R&R add three further categories: concessive, conditional and optative. Their annotation scheme proves reliable, with an inter-annotator agreement that ranges from K=0.6 to 0.84 for the different modal verbs. 6 / LiLT volume 14, issue 3 August 2016 Rubinstein et al. (2013)’s modality annotation scheme is equally grounded in the categories of Kratzer (1981, 1991), but assumes various subdivisions, resulting in a fine-grained scheme of 8 categories. They also investigate various groupings of these 8 classes, up to a binary distinction of epistemic vs. non-epistemic modality. They measured very poor levels of inter-annotator reliability for the fine-grained classes, and substantial agreement for the binary distinction, at α=0.89.3 Similar experience is reported by Cui and Chi (2013) for modality annotation on the Chinese Treebank. Nissim et al. (2013) propose a fi"
2016.lilt-14.3,hendrickx-etal-2012-modality,0,0.176785,"Missing"
2016.lilt-14.3,ide-etal-2008-masc,0,0.0168562,"Missing"
2016.lilt-14.3,2005.mtsummit-papers.11,0,0.00613649,"the classifiers are able to generalize to novel textual genres or domains in case their sense distributions differ from those seen in training. 5.2 EPOS: the Europarl and OpenSubtitles heuristically labeled data sets To our knowledge, R&R’s modal sense annotation on MPQA is the largest existing data set so labeled that covers the full range of modal verbs in English.13 We aim to build a larger modal sense-annotated corpus without extensive manual annotation, using cross-lingual sense projection (cf. Section 4). For this approach we selected the parallel German-English datasets from Europarl (Koehn, 2005) and from the OPUS OpenSubtitles corpus (Tiedemann, 2012). The German-English Europarl corpus contains roughly 1.5 million aligned sentences, and the German-English section of OpenSubtitles contains more than 5 million aligned sentences. By applying our projection method, we obtained 7,693 instances of heuristically sense-annotated modal verbs (Table 3). We refer to this corpus henceforth as EPOS. Note that the extracted data set, because it is derived by alignment with a selected set of paraphrases, cannot be considered to represent a natural distribution. In fact, while this data set is simi"
2016.lilt-14.3,D15-1189,0,0.0236564,"adjectives, as well as various types of attitude verbs. With FactBank, Saurí and Pustejovsky (2009) proposed an annotation scheme and an annotated resource that distinguishes 8 degrees of (non)factuality of events. Saurí (2008) and Saurí and Pustejovsky (2012) developed a rule-based system for factuality recognition trained on FactBank, including recognition of sources. de Marneffe et al. (2012) refined the annotations on FactBank and developed a machine learning classifier for event factuality using lexical and structural features, as well as approximations of world knowledge. Recent work in Lee et al. (2015) builds on and further improves upon this work. FactBank’s focus is on notions of (degrees of) factuality or veridicality of events, and considers primarily epistemic uses of modal verbs. Related work in the biomedical domain (i.a. Light et al., 2004, Thompson et al., 2008, Morante and Daelemans, 2011, Szarvas et al., 2012) similarly focuses on the detection of factuality, hedging, and expressions marking evidence. In contrast, our work is concerned with the task of sense disambiguation of modal verbs, which imply non-factuality of their embedded verbs. Annotating and classifying modal senses."
2016.lilt-14.3,W04-3103,0,0.0163406,"vsky (2012) developed a rule-based system for factuality recognition trained on FactBank, including recognition of sources. de Marneffe et al. (2012) refined the annotations on FactBank and developed a machine learning classifier for event factuality using lexical and structural features, as well as approximations of world knowledge. Recent work in Lee et al. (2015) builds on and further improves upon this work. FactBank’s focus is on notions of (degrees of) factuality or veridicality of events, and considers primarily epistemic uses of modal verbs. Related work in the biomedical domain (i.a. Light et al., 2004, Thompson et al., 2008, Morante and Daelemans, 2011, Szarvas et al., 2012) similarly focuses on the detection of factuality, hedging, and expressions marking evidence. In contrast, our work is concerned with the task of sense disambiguation of modal verbs, which imply non-factuality of their embedded verbs. Annotating and classifying modal senses. Most relevant to our work is the current state of the art in automatic modal sense classification in Ruppenhofer and Rehbein (2012). Ruppenhofer and Rehbein (2012) (henceforth, R&R) manually annotated modal verbs in the MPQA corpus of Wiebe et al. ("
2016.lilt-14.3,P14-5010,0,0.00295742,"type (common, proper, pronoun). person is identified via personal pronoun features; the remaining features are extracted from POS tags. The countability of the subject head is obtained from the Celex database (Baayen et al., 1996). We make use of the following columns from the database: lemma (word), countability (C_N) and uncountability (Unc_N) of a noun.21 Since nouns can appear more than once with the same or different column values, we convert the output, so that every noun appears only once with a single countability value from 20 Feature extraction is performed using Stanford’s CoreNLP (Manning et al., 2014) and the Stanford dependencies (De Marneffe and Manning, 2008). 21 Available at http://celex.mpi.nl/scripts/colstart.pl. Modal Sense Classification At Large / 19 Embedded verb VB lemma part-of-speech particle lemma of head POS of head up, off, on,... TVA passive progressive perfect voice true / false true / false true / false active / passive NEG negation true / false WNV WN sense [0 − 2] WN senseTop WN senses (head+hypernyms) top sense in hypernym hierarchy Subject noun phrase SBJ number person countability noun type WN sense [0 − 2] WN senseTop WN lexname active WN lexname passive sg, pl 1,"
2016.lilt-14.3,W16-1613,1,0.835133,"tion experiment. In response to these questions, we have made various contributions and obtained a number of insights. We applied a paraphrase-driven projection method for the automatic induction of sense-labeled training data using parallel corpora. The senses of modal verbs bear an ideal level of granularity for sense projection using a small set of paraphrases. Using carefully selected paraphrase seeds, the induced annotations yield high accuracy of 0.92. This method can be applied on a broad scale and for many languages, given that bitexts are available in large quantities. Recent work in Marasović and Frank (2016) applied the same method to automatically acquire a large dataset for modal sense classification for German. We designed a linguistically motivated semantic feature space for modal sense classification that is effective in reducing misclassifications. 48 / LiLT volume 14, issue 3 August 2016 We obtain high performance gains especially for modal verbs with difficult sense distinctions and variable sense distributions (most prominently can and could). The high generalization capacity of these models is confirmed in balanced and unbalanced training settings. Even in isolation, the novel semantic"
2016.lilt-14.3,J12-2001,0,0.0792226,"d models have high generalization capacity, especially in unstable distributional settings. 1 Introduction Factuality recognition (Saurí, 2008, de Marneffe et al., 2011, 2012, Saurí and Pustejovsky, 2012) is an important subtask in information extraction. Beyond the bare filtering aspects of veridicality recognition, classification of modal senses plays an important role in text understanding, plan recognition, and the emerging field of argumentation mining. Communication often revolves about hypothetical, planned, apprehended or desired states of affairs. Such “extra-propositional meanings” (Morante and Sporleder, 2012), or intensional contexts are often linguistically marked using modal verbs, adverbs, or attitude verbs,2 as in (1) for hypothetical situations, or (2) for expression of desires or requests. (1) a. He must’ve hurt himself. b. He has certainly found the place by now. c. We anticipate that no one will leave. (2) a. We must solve this problem. b. It is mandatory to resolve this situation. c. I want you to solve this problem. In the present work, we focus on modal verbs and their epistemic and non-epistemic meaning distinctions. Following Kratzer (1991)’s seminal work in formal semantics, recent c"
2016.lilt-14.3,W13-0501,0,0.434247,"Missing"
2016.lilt-14.3,C14-1054,0,0.168897,"ire, letters, blog, and email genres. By contrast, 71.9 F1 score was achieved for 4-fold cross-validation (CV) on a subset of homogeneous genre data consisting of email only. Using confidence values from annotator agreement in training, numbers rose to 44.0 F1 score on gold data, and to 91.1 F1 on within-domain training on email. We thus observe clear overfitting (R&R) and cross-genre effects (Prabhakaran et al.) in state-of-the-art modal sense classification that call for careful analysis of data composition as well as the generalization capacities of specific classification models. Finally, Passonneau et al. (2014) use a variety of stylistic features to model genre variation in the multi-genre MASC corpus.4 Using principal component analysis with a set of 37 lexical, word class, and grammatical features (e.g. past tense), four components are identified as relevant for genre classification. These primarily involve typing over nouns and noun phrases (including named entities) but also reflect various attributes of the verb or the clause (e.g. adverbs or past participles). These results support our choice of MASC for studying cross-genre variation for modal senses. Cross-lingual annotation projection. Cros"
2016.lilt-14.3,W12-3807,0,0.0732284,"ese features are able to capture very diverse contextual factors, but it is difficult to interpret their impact on distinguishing modal senses. The lexical classifiers yield accuracies between 68.7 and 93.5, depending on the verb. Closer investigation of their data set, however, reveals a strong bias in sense distributions. As a consequence, the majority sense baselines are hard to beat: none of their classification models is able to beat the 3 Krippendorff’s α, see Krippendorff (2004) Modal Sense Classification At Large / 7 majority baseline with uniform settings across all modal verb types. Prabhakaran et al. (2012) present annotation and classification experiments on manually annotated data from different genres. Their annotation scheme is based on five categories from Baker et al. (2010): Ability, Effort, Intention, Success, and Want. They performed preselection based on an existing modality tagger and performed annotation using Amazon Mechanical Turk. Classification was performed with an SVM multiclass classification model based on lexical and shallow (no syntactic) features. They report 41.9 F1 score on heterogeneous gold data consisting of newswire, letters, blog, and email genres. By contrast, 71.9"
2016.lilt-14.3,P10-1005,1,0.720779,"Individual features and feature groups. 20 / LiLT volume 14, issue 3 August 2016 {countable, uncountable, ambiguous, none}. If the surface form of the subject head can be found in the above described countability database, countability of the surface form is used. Otherwise, the subject head is lemmatized and the countability of the lemma is assigned to it. Since entries in the Celex database are case-sensitive, the subject is lower cased if it is the first word in the sentence. Lexical semantic features for the subject head are extracted from WordNet (Fellbaum, 1999) using NLTK.22 Following Reiter and Frank (2010), we take the most frequent sense of the common or proper noun in WordNet (subject_sense0), add the direct hypernym of this sense, the direct hypernym of that hypernym, etc., resulting in features subject_sense[1, 2, 3]. We also extract the topmost hypernym of subject_sense0 in WordNet as subject_sense_top (e.g. entity). Finally, the name of the lexicographer file in WordNet, containing subject_sense0, is retrieved using two features, lexname_active and lexname_passive. If the subject appears in an active construction, the WordNet lexical filename is assigned to lexname_active and lexname_pass"
2016.lilt-14.3,W13-0306,0,0.170583,"nhofer and Rehbein (2012). Ruppenhofer and Rehbein (2012) (henceforth, R&R) manually annotated modal verbs in the MPQA corpus of Wiebe et al. (2005). They build on the well-established inventory of modal sense categories of Kratzer (1991) in formal semantics: epistemic, deontic/bouletic and circumstantial/dynamic modality, illustrated in (3) above. R&R add three further categories: concessive, conditional and optative. Their annotation scheme proves reliable, with an inter-annotator agreement that ranges from K=0.6 to 0.84 for the different modal verbs. 6 / LiLT volume 14, issue 3 August 2016 Rubinstein et al. (2013)’s modality annotation scheme is equally grounded in the categories of Kratzer (1981, 1991), but assumes various subdivisions, resulting in a fine-grained scheme of 8 categories. They also investigate various groupings of these 8 classes, up to a binary distinction of epistemic vs. non-epistemic modality. They measured very poor levels of inter-annotator reliability for the fine-grained classes, and substantial agreement for the binary distinction, at α=0.89.3 Similar experience is reported by Cui and Chi (2013) for modality annotation on the Chinese Treebank. Nissim et al. (2013) propose a fi"
2016.lilt-14.3,ruppenhofer-rehbein-2012-yes,0,0.483321,"ten linguistically marked using modal verbs, adverbs, or attitude verbs,2 as in (1) for hypothetical situations, or (2) for expression of desires or requests. (1) a. He must’ve hurt himself. b. He has certainly found the place by now. c. We anticipate that no one will leave. (2) a. We must solve this problem. b. It is mandatory to resolve this situation. c. I want you to solve this problem. In the present work, we focus on modal verbs and their epistemic and non-epistemic meaning distinctions. Following Kratzer (1991)’s seminal work in formal semantics, recent computational approaches such as Ruppenhofer and Rehbein (2012) distinguish different modal ‘senses’, 2 These are common strategies for encoding extra-propositional meaning in English and many other European languages; other grammatical mechanisms beyond these constructions are employed in the world’s languages. Modal Sense Classification At Large / 3 most prominently, epistemic (3.a), deontic/bouletic (3.b) and circumstantial/dynamic (3.c) modality. (3) a. Geez, Buddha must be so annoyed! (epistemic – possibility) b. We must have clear European standards. (deontic – permission/request) c. She can’t even read them. (dynamic – ability) Modal sense tagging"
2016.lilt-14.3,J12-2002,0,0.0757924,"affect classification performance. Our investigations uncover the difficulty of specific sense distinctions and how they are affected by training set size and distributional bias. Our large-scale experiments confirm that semantically enriched models outperform models built on shallow feature sets. Cross-genre experiments shed light on differences in sense distributions across genres and confirm that semantically enriched models have high generalization capacity, especially in unstable distributional settings. 1 Introduction Factuality recognition (Saurí, 2008, de Marneffe et al., 2011, 2012, Saurí and Pustejovsky, 2012) is an important subtask in information extraction. Beyond the bare filtering aspects of veridicality recognition, classification of modal senses plays an important role in text understanding, plan recognition, and the emerging field of argumentation mining. Communication often revolves about hypothetical, planned, apprehended or desired states of affairs. Such “extra-propositional meanings” (Morante and Sporleder, 2012), or intensional contexts are often linguistically marked using modal verbs, adverbs, or attitude verbs,2 as in (1) for hypothetical situations, or (2) for expression of desire"
2016.lilt-14.3,I08-1064,1,0.804533,"ed training instances for an under-resourced language (the target language) from automatically labeled instances provided by an existing labeling system in another language (the source language) (Resnik, 2004). The annotations on the source language are projected via word alignment from the source to the target language sentences, building on the hypothesis that crucial, especially semantic, properties are shared between translated sentences in a parallel corpus. This technique has been successfully applied for part-of-speech and named entity tagging (Yarowsky et al., 2001), temporal tagging (Spreyer and Frank, 2008), dependency parsing (Hwa et al., 2005), and 4 See Section 5.3 for a more detailed discussion of MASC. 8 / LiLT volume 14, issue 3 August 2016 semantic role labeling (Padó and Lapata, 2009). In contrast to this line of work, we do not exploit existing and possibly noisy automatic classification models to project annotations to a novel target language. Our aim is to induce high-quality modal sense annotations from scratch, by exploiting unambiguous paraphrases of specific modal senses in a source language and projecting these senses to the target language. A similar technique has been applied f"
2016.lilt-14.3,J12-2004,0,0.0157701,"ined on FactBank, including recognition of sources. de Marneffe et al. (2012) refined the annotations on FactBank and developed a machine learning classifier for event factuality using lexical and structural features, as well as approximations of world knowledge. Recent work in Lee et al. (2015) builds on and further improves upon this work. FactBank’s focus is on notions of (degrees of) factuality or veridicality of events, and considers primarily epistemic uses of modal verbs. Related work in the biomedical domain (i.a. Light et al., 2004, Thompson et al., 2008, Morante and Daelemans, 2011, Szarvas et al., 2012) similarly focuses on the detection of factuality, hedging, and expressions marking evidence. In contrast, our work is concerned with the task of sense disambiguation of modal verbs, which imply non-factuality of their embedded verbs. Annotating and classifying modal senses. Most relevant to our work is the current state of the art in automatic modal sense classification in Ruppenhofer and Rehbein (2012). Ruppenhofer and Rehbein (2012) (henceforth, R&R) manually annotated modal verbs in the MPQA corpus of Wiebe et al. (2005). They build on the well-established inventory of modal sense categori"
2016.lilt-14.3,tiedemann-2012-parallel,0,0.14773,"fentlich (hopefully – deontic), adjectives like erforderlich (necessary – deontic), verbs like gelingen (succeed – dynamic), erlauben (admit – deontic) or affixes such as -bar (-able) as in (lesbar (readable) – dynamic). The seed paraphrases are given in Appendix A.1, including the number of extracted instances per paraphrase and their estimated reliability. Reliability of seeds was evaluated in terms of accuracy, based on manual evaluation of 20 randomly-extracted instances for each seed. For projection we employed the word-aligned Europarl and OpenSubtitles parallel corpus provided by OPUS (Tiedemann, 2012). We used PostCAT (Graca et al., 2007) for word alignment.7 6 Both examples are drawn from the MASC data set. made use of PostCAT because its model optimizes the agreement between source-target and target-source alignments and hence size and quality of the intersective alignment, which is particularly important for our task. Graca et al. found that AER on the Hansard corpus benefits quite significantly from this method. 7 We 12 / LiLT volume 14, issue 3 August 2016 EPOS data (subset) avg. MPQA data (subset) κ_class κ_majclass_experts κ_experts 0.62 0.83 0.87 κ(exp1, gold) κ(exp2, gold) κ(exp1,"
2016.lilt-14.3,P13-4001,0,0.0395971,"Missing"
2016.lilt-14.3,W15-2705,1,0.74635,"Missing"
2020.acl-main.743,W12-3808,0,0.069544,"Missing"
2020.acl-main.743,L16-1597,0,0.0374931,"Missing"
2020.acl-main.743,D16-1025,0,0.0172497,"nalysis benefits from processing negation (Wiegand et al., 2010). For example, like generally carries positive sentiment, but not when modified by a negation cue (e.g., don’t like). Wilson et al. (2005) introduce the idea of contextual polarity, and note that negation may intensify rather than change polarity (e.g., not good vs. not only good but amazing). Jia et al. (2009) present a set of heuristic rules to determine sentiment when negation is present, and Councill et al. (2010) show that information about the scope of negation is beneficial to predict sentiment. Outside sentiment analysis, Bentivogli et al. (2016) point out that neural machine translation struggles translating negation, and point to focus detection as a possible solution. Neural networks are hard to interpret, but there is evidence that they learn to process negation—to a certain degree—when trained to predict sentiment analysis. Li et al. (2016) visually show that neural networks are capable of meaning composition in the presence of, among others, negation and intensification. Wang et al. (2015) show that an LSTM architecture is capable of determining sentiment of sequences containing negation such as not good and not bad. These previ"
2020.acl-main.743,P11-1059,1,0.876938,"(e.g., There is no friend like [. . . ]), pronouns (e.g., [. . . ] has yielded nothing to a careful search), affixes (e.g., The inexplicable tangle seemed [. . . ]), and others. Other corpora annotating scope in English include efforts with biomedical texts (Vincze et al., 2008) and working with reviews (Councill et al., 2010; Konstantinova et al., 2012). Corpora Annotating Focus. Although focus of negation is defined as a subset of the scope, there is no corpus annotating both of them in the same texts. We work with PB-FOC, the largest publicly available corpus annotating focus of negation (Blanco and Moldovan, 2011). PB-FOC annotates the focus of the negations marked with M - NEG role in PropBank (Palmer et al., 2005), which in turn annotates semantic roles on top of the Penn TreeBank (Taylor et al., 2003). As a result, PB-FOC annotates the focus of 3,544 verbal negations (i.e., when a negation cue such as never or not syntactically modifies a verb). As per the authors, the annotation process consisted of selecting the semantic role most likely to be the focus. Therefore, focus annotations in PB-FOC are always all the tokens corresponding to a semantic role of the (negated) verb. Finally, M - NEG role is"
2020.acl-main.743,W05-0620,0,0.384248,"Missing"
2020.acl-main.743,W10-3110,0,0.598777,"per role, percentages of negated verbs having each role, and percentage of each role being the focus. of which contain a negation. CD-SCO annotates all negations, including verbs (e.g., I fail to see how you could have done more), adverbs (e.g., It was never proved that [. . . ]), determiners (e.g., There is no friend like [. . . ]), pronouns (e.g., [. . . ] has yielded nothing to a careful search), affixes (e.g., The inexplicable tangle seemed [. . . ]), and others. Other corpora annotating scope in English include efforts with biomedical texts (Vincze et al., 2008) and working with reviews (Councill et al., 2010; Konstantinova et al., 2012). Corpora Annotating Focus. Although focus of negation is defined as a subset of the scope, there is no corpus annotating both of them in the same texts. We work with PB-FOC, the largest publicly available corpus annotating focus of negation (Blanco and Moldovan, 2011). PB-FOC annotates the focus of the negations marked with M - NEG role in PropBank (Palmer et al., 2005), which in turn annotates semantic roles on top of the Penn TreeBank (Taylor et al., 2003). As a result, PB-FOC annotates the focus of 3,544 verbal negations (i.e., when a negation cue such as never"
2020.acl-main.743,P19-1508,0,0.0208876,"ng that is in some way opposed to e—a semantic and highly ambiguous undertaking that comes naturally to humans in everyday communication. Negation is generally understood to carry positive meaning, or in other words, to suggest an affirmative alternative. For example, John didn’t leave the house implicates that John stayed inside the house. Hasson and Glucksberg (2006) show that comprehending negation involves considering the representation of affirmative alternatives. While not fully understood, there is evidence that negation involves reduced access to the affirmative mental representation (Djokic et al., 2019). Orenes et al. (2014) provide evidence that humans switch to the affirmative alternative in binary scenarios (e.g., from not red to green when processing The figure could be red or green. The figure is not red). In such multary scenarios, however, humans keep the negated representation unless the affirmative interpretation is obvious from context (e.g., humans keep not red when processing The figure is red, green, yellow or blue. The figure is not red.). From a linguistic perspective, negation is understood in terms of scope and focus (Section 2). The scope is the part of the meaning that is"
2020.acl-main.743,P16-1047,0,0.118667,"get focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a scope detector trained with CD-SCO to predict the focus of negation. While we only incorporate small modifications to previously proposed architectures, our scope detector outperforms previous work (Section 4). Focus Identification. Although focus is part of t"
2020.acl-main.743,E17-2010,0,0.012047,"antially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a scope detector trained with CD-SCO to predict the focus of negation. While we only incorporate small modifications to previously proposed architectures, our scope detector outperforms previous work (Section 4). Focus Identification. Although focus is part of the scope, state-of-the-art approaches to identify the focus of negation ignore information about scope. Possible reasons are that (a) existing corpora annotating scope and focus contain substantially different te"
2020.acl-main.743,2020.cl-1.5,0,0.0193241,"Missing"
2020.acl-main.743,konstantinova-etal-2012-review,0,0.0744004,"Missing"
2020.acl-main.743,N16-1082,0,0.0276705,".g., not good vs. not only good but amazing). Jia et al. (2009) present a set of heuristic rules to determine sentiment when negation is present, and Councill et al. (2010) show that information about the scope of negation is beneficial to predict sentiment. Outside sentiment analysis, Bentivogli et al. (2016) point out that neural machine translation struggles translating negation, and point to focus detection as a possible solution. Neural networks are hard to interpret, but there is evidence that they learn to process negation—to a certain degree—when trained to predict sentiment analysis. Li et al. (2016) visually show that neural networks are capable of meaning composition in the presence of, among others, negation and intensification. Wang et al. (2015) show that an LSTM architecture is capable of determining sentiment of sequences containing negation such as not good and not bad. These previous works train a model for a particular task (i.e., sentiment analysis) and then investigate whether the model learnt anything related to negation that is useful for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent ne"
2020.acl-main.743,S12-1035,1,0.856583,". These previous works train a model for a particular task (i.e., sentiment analysis) and then investigate whether the model learnt anything related to negation that is useful for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a scope detector trained with CD-SCO to predict the foc"
2020.acl-main.743,W09-1105,0,0.0501164,"15) show that an LSTM architecture is capable of determining sentiment of sequences containing negation such as not good and not bad. These previous works train a model for a particular task (i.e., sentiment analysis) and then investigate whether the model learnt anything related to negation that is useful for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is muc"
2020.acl-main.743,morante-daelemans-2012-conandoyle,0,0.250939,"Missing"
2020.acl-main.743,P14-1007,0,0.0135182,"rnatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a scope detector trained with CD-SCO to predict the focus of negation. While we only incorporate small modifications to previously proposed architectures, our scope detector outperforms previous work (Section 4). Focus Identification. Although focus is part of the scope, state-of-the-art approaches to identify the focus of ne"
2020.acl-main.743,J05-1004,0,0.133998,"affixes (e.g., The inexplicable tangle seemed [. . . ]), and others. Other corpora annotating scope in English include efforts with biomedical texts (Vincze et al., 2008) and working with reviews (Councill et al., 2010; Konstantinova et al., 2012). Corpora Annotating Focus. Although focus of negation is defined as a subset of the scope, there is no corpus annotating both of them in the same texts. We work with PB-FOC, the largest publicly available corpus annotating focus of negation (Blanco and Moldovan, 2011). PB-FOC annotates the focus of the negations marked with M - NEG role in PropBank (Palmer et al., 2005), which in turn annotates semantic roles on top of the Penn TreeBank (Taylor et al., 2003). As a result, PB-FOC annotates the focus of 3,544 verbal negations (i.e., when a negation cue such as never or not syntactically modifies a verb). As per the authors, the annotation process consisted of selecting the semantic role most likely to be the focus. Therefore, focus annotations in PB-FOC are always all the tokens corresponding to a semantic role of the (negated) verb. Finally, M - NEG role is chosen when the focus is the verb. The annotations in PB-FOC were carried out taking into account the p"
2020.acl-main.743,N18-1202,0,0.0469745,"ture The network architecture (Fig. 1) consists of a base NN (all components except those inside dotted shapes) plus additional components to include information about the scope and context of negation. Base NN. The base network is inspired by Huang et al. (2015) and Reimers and Gurevych (2017). It is a 3-layer Bidirectional Long Short-Term Memory (BiLSTM) network with a Conditional Random Field (CRF) layer. The network takes as input the sentence containing the negation whose focus is to be predicted, where each word is represented with the concatenation of (a) its pre-trained ELMo embedding Peters et al. (2018), (b) a specialized embedding indicating whether a token is the negated verb (not the negation cue), and (c) a specialized embedding indicating semantic roles (one per role label). The specialized embeddings are trained from scratch as part of the tuning of the network. Scope Information. We add an extra input at the token level indicating whether a token belongs to the scope of the negation whose focus is to be predicted. This new input is then mapped to a third specialized embedding (two values: inside or outside the scope), and concatenated to the word representation prior to feeding it to"
2020.acl-main.743,S12-1041,0,0.02697,"for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a scope detector trained with CD-SCO to predict the focus of negation. While we only incorporate small modifications to previously proposed architectures, our scope detector outperforms previous work (Section 4). Focus Iden"
2020.acl-main.743,S12-1039,0,0.0140588,"(Section 4). Focus Identification. Although focus is part of the scope, state-of-the-art approaches to identify the focus of negation ignore information about scope. Possible reasons are that (a) existing corpora annotating scope and focus contain substantially different texts (Section 2), and (b) incorporating scope information is not straightforward with traditional machine learning and manually defined features. The initial proposals obtain modest results and only consider the sentence containing the negation (Blanco and Moldovan, 2011), including scope information in a rule-based system (Rosenberg and Bergler, 2012). Zou et al. (2014, 2015) propose 8391 N_F N_F N_F F F CRF Layer context, previous context, next FC ap FC ap an FC an ap FC an ap FC an an ap 3 layer BiLSTM 2 layer BiLSTM 2 layer BiLSTM scope information Scope emb. StatesWest Nevada The SRL emb. (previous sentence) guarantees (next sentence) Neg. verb emb. Word emb. (ELMo) (current sentence) (neg. verb, roles, scope) The (N, A0, I_S) turned (Y, V, I_S) not (N, AM-NEG, O_S) a (N, A1, I_S) profit (N, A1, I_S) Figure 1: Neural network to predict the focus of negation. The core of the architecture (NN, all components except those inside dotted sh"
2020.acl-main.743,D16-1119,1,0.864561,"Missing"
2020.acl-main.743,E17-1081,1,0.858173,"Missing"
2020.acl-main.743,D19-1230,0,0.0473769,"A1, I_S) profit (N, A1, I_S) Figure 1: Neural network to predict the focus of negation. The core of the architecture (NN, all components except those inside dotted shapes) takes as input the sentence containing the negation, and each word is represented with its word embedding and specialized embeddings for the negated verb and semantic roles. The additional components inside dotted shapes incorporate information about (a) the scope and (b) context (previous and next sentences). graph-based models that incorporate discourse information and obtain improvements over previous works. In addition, Shen et al. (2019) present a neural model that leverages word-level and topic-level attention mechanisms to utilize contextual information. We compare our results and theirs in Section 4.2. In this paper, we show that (a) neural networks considering the scope of negation obtain the best results to date and (b) context is not beneficial if scope is available (Section 4). 4 Predicting the Focus of Negation We approach the task of predicting focus of negation as a sequence labeling task with a neural network. We first describe the network architecture, and then present quantitative results. Section 5 presents a de"
2020.acl-main.743,W08-0606,0,0.0744822,"s containing negation such as not good and not bad. These previous works train a model for a particular task (i.e., sentiment analysis) and then investigate whether the model learnt anything related to negation that is useful for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Morante and Daelemans, 2009) were trained in the biomedical domain with BioScope (Szarvas et al., 2008). The *SEM-2012 Shared Task (Morante and Blanco, 2012) included scope identification with CD-SCO (Section 2), and the winner proposed an SVM-based ranking of syntactic constituents to identify the scope (Read et al., 2012). More recently, Fancellu et al. (2016) present neural networks for this task, and Packard et al. (2014) present a complementary approach that operates over semantic representations obtained with an offthe-shelf parser. Finally, Fancellu et al. (2017) present an error analysis showing that scope is much easier to identify when delimited by punctuation. In this paper, we use a"
2020.acl-main.743,P15-1130,0,0.0319357,"ouncill et al. (2010) show that information about the scope of negation is beneficial to predict sentiment. Outside sentiment analysis, Bentivogli et al. (2016) point out that neural machine translation struggles translating negation, and point to focus detection as a possible solution. Neural networks are hard to interpret, but there is evidence that they learn to process negation—to a certain degree—when trained to predict sentiment analysis. Li et al. (2016) visually show that neural networks are capable of meaning composition in the presence of, among others, negation and intensification. Wang et al. (2015) show that an LSTM architecture is capable of determining sentiment of sequences containing negation such as not good and not bad. These previous works train a model for a particular task (i.e., sentiment analysis) and then investigate whether the model learnt anything related to negation that is useful for that task. Unlike them, we target focus of negation detection—and the resulting affirmative alternatives—and work with task-independent negations. Scope Identification. Compared to focus identification, scope identification has received substantially more attention. The first proposals (Mor"
2020.acl-main.743,W10-3111,0,0.10326,"Missing"
2020.acl-main.743,H05-1044,0,0.0788373,"), use dependency trees instead of roles (Sarabi and Blanco, 2016), target non-verbal negations (Sarabi and Blanco, 2017), and work with tutorial dialogues (Banjade and Rus, 2016). 3 Previous Work In addition to identifying negation cues and resolving the scope and focus of negation, there is work showing that processing negation is important for natural language understanding in general. In particular, sentiment analysis benefits from processing negation (Wiegand et al., 2010). For example, like generally carries positive sentiment, but not when modified by a negation cue (e.g., don’t like). Wilson et al. (2005) introduce the idea of contextual polarity, and note that negation may intensify rather than change polarity (e.g., not good vs. not only good but amazing). Jia et al. (2009) present a set of heuristic rules to determine sentiment when negation is present, and Councill et al. (2010) show that information about the scope of negation is beneficial to predict sentiment. Outside sentiment analysis, Bentivogli et al. (2016) point out that neural machine translation struggles translating negation, and point to focus detection as a possible solution. Neural networks are hard to interpret, but there i"
2020.acl-main.743,N16-1174,0,0.0589814,"g the scope of negation: 79.41 F1 (vs. 77.77 F1). We do not elaborate on the scope detector as we only leverage it to predict focus. Context. We also experiment with an additional component to add contextual information (previous and next sentences), as previous work has shown empirically that doing so is beneficial (Zou et al., 2014). While we tried many strategies (e.g., concatenating sentence embeddings to the representations from the 3-layer BiLSTM), we present only the one yielding the best results. Specifically, we use 2-layer Bi-LSTMs with an attention mechanism (Bahdanau et al., 2014; Yang et al., 2016). The attention weights (ap and an for the previous 8392 Zou et al. (2014) Zou et al. (2015) Shen et al. (2019) NN (baseline) NN + S NN + Cntxt NN + S + Cntxt P 71.67 n/a n/a 72.14 75.92 73.69 74.15 R 67.43 n/a n/a 71.63 75.7 73.17 73.74 F1 69.49 n/a n/a 71.88 75.81 73.43 73.94 Acc 67.1 69.4 70.5 71.6 75.7 73.2 73.7 ARG 0 ARG 1 ARG 2 ARG 3 ARG 4 M - NEG M - TMP M - MNR M - ADV Table 2: Focus prediction results of the best performing previous works and our neural network (baseline network and adding components). S and Cntxt refer to Scope and Context, respectively. Note that Zou et al. (2014) d"
2020.acl-main.743,P14-1049,0,0.100283,"tion. Although focus is part of the scope, state-of-the-art approaches to identify the focus of negation ignore information about scope. Possible reasons are that (a) existing corpora annotating scope and focus contain substantially different texts (Section 2), and (b) incorporating scope information is not straightforward with traditional machine learning and manually defined features. The initial proposals obtain modest results and only consider the sentence containing the negation (Blanco and Moldovan, 2011), including scope information in a rule-based system (Rosenberg and Bergler, 2012). Zou et al. (2014, 2015) propose 8391 N_F N_F N_F F F CRF Layer context, previous context, next FC ap FC ap an FC an ap FC an ap FC an an ap 3 layer BiLSTM 2 layer BiLSTM 2 layer BiLSTM scope information Scope emb. StatesWest Nevada The SRL emb. (previous sentence) guarantees (next sentence) Neg. verb emb. Word emb. (ELMo) (current sentence) (neg. verb, roles, scope) The (N, A0, I_S) turned (Y, V, I_S) not (N, AM-NEG, O_S) a (N, A1, I_S) profit (N, A1, I_S) Figure 1: Neural network to predict the focus of negation. The core of the architecture (NN, all components except those inside dotted shapes) takes as inp"
2020.acl-main.743,D15-1187,0,0.0265037,"Missing"
2020.findings-emnlp.345,L16-1100,0,0.028889,"point of claiming human parity in some high-resource language pairs and in-domain settings (Hassan et al., 2018),1 fine-grained semantic differences become 1 We direct the reader to (Läubli et al., 2018) and (Toral antonis@gmu.edu increasingly important. Negation in particular, with its property of logical reversal, has the potential to cause loss of (or mis-)information if mistranslated. Other linguistic phenomena and analysis axes have gathered significant attention in NMT evaluation studies, including anaphora resolution (Hardmeier et al., 2014; Voita et al., 2018) and pronoun translation (Guillou and Hardmeier, 2016), modality (Baker et al., 2012), ellipsis and deixis (Voita et al., 2019), word sense disambiguation (Tang et al., 2018), and morphological competence (Burlot and Yvon, 2017). Nevertheless, the last comprehensive study of the effect of negation in MT pertains to older, phrase-based models (Fancellu and Webber, 2015). In this work, we set out to study the effect of negation in modern NMT systems. Specifically, we explore: 1. Whether negation affects the quality of the produced translations (it does); 2. Whether the typically-used evaluation datasets include a significant amount of negated examp"
2020.findings-emnlp.345,E17-2002,0,0.0128272,"ine-Y afrl-sys online-A afrl-ewc TartuNLP-u online-X.0 0.161 0.143 0.134 0.133 0.126 0.125 0.108 0.091 0.024 0.022 -0.030 -0.039 -0.096 Q4: Is Translating Negation between Similar Languages Easier? Intuition may lead us to believe that it is easier to translate negation between similar languages. We show the correlation between language similarity and relative differences in Z-scores with and without negation in Figure 2. To calculate similarity between two languages, we follow Zhang and Toral (2019) and Berzak et al. (2017). Briefly, we obtain feature vectors for each language from lang2vec (Littell et al., 2017), and define the similarity between two languages as the cosine similarity between their feature vectors. More specifically, we concatenate 103 morphosyntactic features and 87 language family features (only those relevant to the languages we work with) from the URIEL typological database. We conclude that similarity between languages is only a weak indicator of how difficult it is to translate negation. We revisit this question in Section 5 with an in-depth linguistic discussion. (Z(w/ neg.) - Z(w/o neg.)) / Z(all sentences) Table 3: Rankings of all submissions translating from Russian to Engl"
2020.findings-emnlp.345,W19-6623,0,0.0130524,"ed evaluation datasets include a significant amount of negated examples (they don’t); 3. Whether different systems quantifiably handle negation differently across different settings (they do); and 4. Whether there is a linguistics-grounded explanation of our findings (there is). Our conclusion is that indeed negation still poses an issue for NMT systems in several language pairs, an issue which should be tackled in future NMT systems development. Negation should be taken into consideration especially when deploying realworld systems which might produce incredibly fluent but inadequate output (Martindale et al., 2019). 2 Negation 101 Negation at its most straightforward—simple negation of declarative sentences—involves reversing et al., 2018) for further examination of such claims. 3869 Findings of the Association for Computational Linguistics: EMNLP 2020, pages 3869–3885 c November 16 - 20, 2020. 2020 Association for Computational Linguistics the truth value of a sentence. Skies are blue, for example, becomes Skies are not blue. Clausal negation of an existing utterance, defined roughly as negation of the entire clause rather than a subpart, produces a second utterance whose meaning contradicts that of th"
2020.findings-emnlp.345,W17-4770,0,0.0479203,"Missing"
2020.findings-emnlp.345,W18-6319,0,0.0173336,"igating the role of negation WMT competitions, and are preferred to au- in machine translation by only looking at English tomated metrics to assess the quality of MT. negations likely misses valid insights. For examNevertheless, most of the MT community still ple, a Spanish sentence containing negation (e.g., relies on automated metrics for development “El ladrón no estaba preocupado hasta que vino la and system comparisons. Thus, we also work policía”) can be translated into English either with with three automated metrics, in particular negation (“The thief was not worried until the poBLEU (Post, 2018), chrF++ (Popovi´c, 2017), and lice arrived”), or without negation (“The thief only METEOR (Denkowski and Lavie, 2011). worried when the police arrived”). We reserve for future work a more thorough analysis of corresponIn the remainder of the paper we present two dences between negation in source sentences and complementary analyses. First, we investigate the role of negation in machine translation with an em- negation in English reference translations. phasis on numeric evaluation (Section 4). Second, 4 Quantitative Analysis we investigate from a linguistic perspective what makes translating"
2020.lrec-1.140,L16-1592,0,0.0190746,"oth Nakov and Hearst (2013) and Tratz and Hovy (2010) consider possession expressed by noun compounds, such as “family estate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, or even inter-sentential. The non-restrictive approach presented here is similar to that of Banea et al. (2016), who annotate possessions of particular bloggers at the time of utterance. In our previous work, we (Chinnappa and Blanco, 2018a) first extract possessions from a sentence using a deterministic procedure and then identify the types and temporal anchors of possession. In a latter work, we (Chinnappa and Blanco, 2018b) work with the same Wikipedia articles about artworks as the current work, and extract their possessors. We match these possessors to their temporal information with respect to the years explicitly mentioned (before, during or after). Unlike the current work, we limit the temporal"
2020.lrec-1.140,L18-1242,0,0.0168755,"capable of changing hands. We are interested in tracking change of possession and thus restrict our work to alienable possessions. Previous work on automatic extraction of possession has mostly focused on particular syntactic constructions. Tratz and Hovy (2013) investigate various semantic relations realized by English possessive constructions, and both Nakov and Hearst (2013) and Tratz and Hovy (2010) consider possession expressed by noun compounds, such as “family estate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, or even inter-sentential. The non-restrictive approach presented here is similar to that of Banea et al. (2016), who annotate possessions of particular bloggers at the time of utterance. In our previous work, we (Chinnappa and Blanco, 2018a) first extract possessions from a sentence using a deterministic procedure and then identify the types and temporal anchors of"
2020.lrec-1.140,D15-1075,0,0.0420045,"cores to each possessor and each temporal relation; and e) assemble individual possession events into a global possession timeline. In addition to the corpus, we release evaluation scripts and a baseline model for the task. Keywords: possession, timeline, wikipossesions 1. Temporally-oriented possession Research in machine reading, text comprehension, and natural language understanding continues to proceed at a rapid pace. Each time a new approach claims to surpass human performance on benchmark evaluation data sets, a new data set comes along, offering a new twist on the previous challenges (Bowman et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019, for example). At the same time, the community has questioned whether these data sets in fact require comprehension of reading texts by the machine (Kaushik and Lipton, 2018). With this paper, we offer a new benchmark evaluation data set which combines several related tasks requiring different degrees of inferential complexity. Rather than focusing on question-answer pairs linked to particular text passages, the unit of analysis for this data set is a complete Wikipedia article. As a comparison, the average text length in our evaluation data set is an"
2020.lrec-1.140,N18-1046,1,0.835807,"tate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, or even inter-sentential. The non-restrictive approach presented here is similar to that of Banea et al. (2016), who annotate possessions of particular bloggers at the time of utterance. In our previous work, we (Chinnappa and Blanco, 2018a) first extract possessions from a sentence using a deterministic procedure and then identify the types and temporal anchors of possession. In a latter work, we (Chinnappa and Blanco, 2018b) work with the same Wikipedia articles about artworks as the current work, and extract their possessors. We match these possessors to their temporal information with respect to the years explicitly mentioned (before, during or after). Unlike the current work, we limit the temporal information to be a year within a Wikipedia section. We do not capture any other possession attributes such as certainty or ord"
2020.lrec-1.140,D18-1251,1,0.831571,"tate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, or even inter-sentential. The non-restrictive approach presented here is similar to that of Banea et al. (2016), who annotate possessions of particular bloggers at the time of utterance. In our previous work, we (Chinnappa and Blanco, 2018a) first extract possessions from a sentence using a deterministic procedure and then identify the types and temporal anchors of possession. In a latter work, we (Chinnappa and Blanco, 2018b) work with the same Wikipedia articles about artworks as the current work, and extract their possessors. We match these possessors to their temporal information with respect to the years explicitly mentioned (before, during or after). Unlike the current work, we limit the temporal information to be a year within a Wikipedia section. We do not capture any other possession attributes such as certainty or ord"
2020.lrec-1.140,N19-1246,0,0.0216997,"ation; and e) assemble individual possession events into a global possession timeline. In addition to the corpus, we release evaluation scripts and a baseline model for the task. Keywords: possession, timeline, wikipossesions 1. Temporally-oriented possession Research in machine reading, text comprehension, and natural language understanding continues to proceed at a rapid pace. Each time a new approach claims to surpass human performance on benchmark evaluation data sets, a new data set comes along, offering a new twist on the previous challenges (Bowman et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019, for example). At the same time, the community has questioned whether these data sets in fact require comprehension of reading texts by the machine (Kaushik and Lipton, 2018). With this paper, we offer a new benchmark evaluation data set which combines several related tasks requiring different degrees of inferential complexity. Rather than focusing on question-answer pairs linked to particular text passages, the unit of analysis for this data set is a complete Wikipedia article. As a comparison, the average text length in our evaluation data set is an order of magnitude longer than the averag"
2020.lrec-1.140,D18-1546,0,0.0230909,"or the task. Keywords: possession, timeline, wikipossesions 1. Temporally-oriented possession Research in machine reading, text comprehension, and natural language understanding continues to proceed at a rapid pace. Each time a new approach claims to surpass human performance on benchmark evaluation data sets, a new data set comes along, offering a new twist on the previous challenges (Bowman et al., 2015; Rajpurkar et al., 2016; Dua et al., 2019, for example). At the same time, the community has questioned whether these data sets in fact require comprehension of reading texts by the machine (Kaushik and Lipton, 2018). With this paper, we offer a new benchmark evaluation data set which combines several related tasks requiring different degrees of inferential complexity. Rather than focusing on question-answer pairs linked to particular text passages, the unit of analysis for this data set is a complete Wikipedia article. As a comparison, the average text length in our evaluation data set is an order of magnitude longer than the average passage in the DROP corpus (Dua et al., 2019), which was developed specifically to demand reasoning across longer text spans than previous reading comprehension benchmarks."
2020.lrec-1.140,D16-1264,0,0.0623639,"Missing"
2020.lrec-1.140,P10-1070,0,0.0298767,"possession, in which such separation is not possible (Aikhenvald and Dixon, 2012; Heine, 1997, among others). Unlike inalienable possessions, which are permanent, alienable possessions are temporary and, therefore, capable of changing hands. We are interested in tracking change of possession and thus restrict our work to alienable possessions. Previous work on automatic extraction of possession has mostly focused on particular syntactic constructions. Tratz and Hovy (2013) investigate various semantic relations realized by English possessive constructions, and both Nakov and Hearst (2013) and Tratz and Hovy (2010) consider possession expressed by noun compounds, such as “family estate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, or even inter-sentential. The non-restrictive approach presented here is similar to that of Banea et al. (2016), who annotate possessions of part"
2020.lrec-1.140,P13-1037,0,0.0252925,"rature makes a conceptual distinction between alienable possession, in which possessees can be separated from their possessors, and inalienable possession, in which such separation is not possible (Aikhenvald and Dixon, 2012; Heine, 1997, among others). Unlike inalienable possessions, which are permanent, alienable possessions are temporary and, therefore, capable of changing hands. We are interested in tracking change of possession and thus restrict our work to alienable possessions. Previous work on automatic extraction of possession has mostly focused on particular syntactic constructions. Tratz and Hovy (2013) investigate various semantic relations realized by English possessive constructions, and both Nakov and Hearst (2013) and Tratz and Hovy (2010) consider possession expressed by noun compounds, such as “family estate.” Badulescu and Moldovan (2009) extract possession as one of the many semantic relations expressed by English genetives. Blodgett and Schneider (2018) present a corpus of web reviews annotating genitives with adpositional supersenses, finding that this inventory works well for canonical possessives. We consider all expressions of possession, whether phrasal, clausal, sentential, o"
2020.lrec-1.140,S13-2001,0,0.037591,"2: Corpus statistics for WikiPossessions. Table 1: Statistics for all marked possessors (2) It was loaned to the Ashmolean Museum in early 1900s, its whereabouts after this are unknown; it was rediscovered in a Battersea home in the early 1960s, boxed in over a chimney. [Flaming June] 4. Stage Two: Temporal Anchoring, Temporal Relations, and Certainty. The second step takes as input a text passage and an extracted (artifact, possessor) relation; output is a temporal anchor for the possession relation. This task corresponds to the TLINK task in the TempEval shared tasks (Verhagen et al., 2010; UzZaman et al., 2013), limiting the range to only possession-type events. In our case, we allow three types of anchors: a year, a range of years, or a major historical event (as in ex. 3). (3) After the victory of Francisco Franco in Spain, the painting was sent to the United States to raise funds and support for Spanish refugees. [Guernica] Once the temporal anchor has been identified, the next task is temporal relation identification (Verhagen et al., 2010; UzZaman et al., 2013). Specifically, the system should indicate whether the possession event held BEFORE the temporal anchor, DURING the temporal anchor, or"
2020.semeval-1.243,D19-1565,0,0.0620809,"Missing"
2020.semeval-1.243,2020.semeval-1.186,0,0.0950146,"Missing"
2020.semeval-1.243,D14-1162,0,0.0886755,"e input texts are usually multi-sentence documents. For labeling, we collapse the distinction to simply propaganda (P) or not-propaganda (N), such that the example above would be labeled as the sequence: [P×6, N×10]. Final features for Subtask 1 appear in the left-hand side of Table 1. Basic features include the word itself, its immediate textual context and fine-grained part of speech labels for both target and context words. We also include syntactic dependency information, a bag-of-words feature consisting of the token’s close context, and the token’s word embedding, using GloVe embeddings(Pennington et al., 2014). Finally, we include word embeddings for the tokens in the BoW features. For the chunk approach, we tried the first and last words of each chunk, plus a bag-of-words with all words in the chunk, plus the words in the two adjacent fragments, word embeddings for the chunk and its neighbors, and sentiment scores for the chunk and its neighbors. Subtask 2: Technique Classification. For Subtask 2, we use a Logistic Regression classifier to label individual propagandistic fragments with one of fourteen technique labels. See Table 2 for a list of labels. We first build a simple baseline classifier f"
2020.semeval-1.294,W19-0413,0,0.0246861,"studies of the additional linguistic features establish which linguistic features are most effective in each task. The goals of these studies are: To further target which features are most informative for each of the sub-tasks; to also narrow in on features that are ineffective or may be detrimental to classification; and to identify which types of offensive constructions are still missed by the model and why. In other tasks, such as multi-lingual sentiment analysis, success has been found in Deep Learning architectures that have utilized both word embeddings and feature embeddings as input (Akhtar et al., 2019). For some tasks, the most appropriate types of feature embeddings may be directly apparent. However, in a complex semantic problem such as recognizing offensive and abusive language, the ideal feature support to the word embedding input is not immediately clear. Feature engineering and error analysis can offer This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/. 1 Code and hand-annotated data sets available via GitHub: https://github.com/jmfromkn/Offenseval_Code 2209 Proceedings of the 14th Interna"
2020.semeval-1.294,D15-1162,0,0.0115687,"ate a held-out hand-annotated development set from the training data for each task and use it as a test set for picking a good threshold.1 Using Scikit-learn’s Linear SVM implementation (Pedregosa et al., 2011), we build a classifier with features based on two sets of pre-trained word embeddings: GloVe’s 200-dimension Twitter embeddings (Pennington et al., 2014) and 200-dimensional word2vec Twitter embeddings (Deriu et al., 2017). Additionally, we explore four categories of linguistic features beyond the embeddings: lexical, part of speech, dependency, and named entity features. We use spaCy (Honnibal and Johnson, 2015) for preprocessing and extraction of linguistic features. Ablation studies of the additional linguistic features establish which linguistic features are most effective in each task. The goals of these studies are: To further target which features are most informative for each of the sub-tasks; to also narrow in on features that are ineffective or may be detrimental to classification; and to identify which types of offensive constructions are still missed by the model and why. In other tasks, such as multi-lingual sentiment analysis, success has been found in Deep Learning architectures that ha"
2020.semeval-1.294,S19-2109,0,0.0147681,"removed all of the difficult and/or vague cases. We use the simple method for our submitted system. 3 Model and Features Although the most successful systems from Offenseval 2019 were primarily neural systems (Zampieri et al., 2019b), we choose to use a non-neural SVM classifier because of the ease of feature inspection, feature specification, and error analysis. Specifically, we use scikit-learn’s linear SVM implementation (Pedregosa et al., 2011) with default parameter values. Previous SVM-based approaches to offensive language detection investigate the effectiveness of sentence embeddings (Indurthi et al., 2019, for example) and lexical features such as sentiment analysis and offensive/profane word lexicons (Plaza-del Arco et al., 2019, among others). We hypothesize that part of speech, dependency, and named entity features capture additional context useful for classifying tweets with more elusive forms of offensive language. These context-level features could also be useful in Task B, to capture differences in the constructions seen in targeted vs. untargeted tweets. Our base model uses the dimensions of a document embedding (described below) as features; our additional linguistic features are list"
2020.semeval-1.294,N13-1039,0,0.0470852,"Missing"
2020.semeval-1.294,D14-1162,0,0.0906615,"onfidence value between 0 and 1; 1 is aligned to the positive class for the task and 0 to the negative class. Thus an additional challenge in this task is determining an appropriate threshold value for mapping confidence measures to labels. To this end, we create a held-out hand-annotated development set from the training data for each task and use it as a test set for picking a good threshold.1 Using Scikit-learn’s Linear SVM implementation (Pedregosa et al., 2011), we build a classifier with features based on two sets of pre-trained word embeddings: GloVe’s 200-dimension Twitter embeddings (Pennington et al., 2014) and 200-dimensional word2vec Twitter embeddings (Deriu et al., 2017). Additionally, we explore four categories of linguistic features beyond the embeddings: lexical, part of speech, dependency, and named entity features. We use spaCy (Honnibal and Johnson, 2015) for preprocessing and extraction of linguistic features. Ablation studies of the additional linguistic features establish which linguistic features are most effective in each task. The goals of these studies are: To further target which features are most informative for each of the sub-tasks; to also narrow in on features that are ine"
2020.semeval-1.294,N18-1202,0,0.0218329,"scores of 0.882 for Task A (Rank 71) and 0.617 for Task B (Rank 16). The model with these scores used thresholds of 0.38 and 0.33 for Tasks A and B. Additional threshold testing on the development set after the initial submission period suggests that further fine-tuning of the threshold may increase F1. Use of the filtering strategy and additional filter parameter testing could also improve performance; we leave investigation of these directions for future work. To further explore this task, we would like to consider other embedding approaches, considering contextual embeddings such as ELMo (Peters et al., 2018) or large-scale fastText (Bojanowski et al., 2016) embeddings for improving coverage on unknown words and lexical variations common to Twitter. 2213 Another interesting direction is the use of methods to transform emojis into word representation (Singh et al., 2019); we suspect that emojis may play a strong role in conveying offensive language. Since linguistic features seem to be useful for this task, we plan to look into twitter-specific toolkits for extraction of linguistic features; two possibilities are NLTK’s tweet tokenizer (Bird et al., 2009) and TweetNLP’s part-of-speech taggers and d"
2020.semeval-1.294,S19-2129,0,0.0236411,"Missing"
2020.semeval-1.294,N19-1214,0,0.0175185,"ning of the threshold may increase F1. Use of the filtering strategy and additional filter parameter testing could also improve performance; we leave investigation of these directions for future work. To further explore this task, we would like to consider other embedding approaches, considering contextual embeddings such as ELMo (Peters et al., 2018) or large-scale fastText (Bojanowski et al., 2016) embeddings for improving coverage on unknown words and lexical variations common to Twitter. 2213 Another interesting direction is the use of methods to transform emojis into word representation (Singh et al., 2019); we suspect that emojis may play a strong role in conveying offensive language. Since linguistic features seem to be useful for this task, we plan to look into twitter-specific toolkits for extraction of linguistic features; two possibilities are NLTK’s tweet tokenizer (Bird et al., 2009) and TweetNLP’s part-of-speech taggers and dependency parsers (Owoputi et al., 2013). Finally, larger development sets with more annotators should lead to better threshold testing, especially for Task B. 6 Conclusions While document-level embeddings are a useful foundation for both Tasks A and B, the usefulne"
2020.semeval-1.294,N19-1144,0,0.169879,"Task A, and NE features for Task B. 1 Introduction and System Overview SemEval 2020 Task 12: Offenseval 2 (Zampieri et al., 2020) is an offensive language identification task revolving around classifying social media comments. For the English track, there are three sub-tasks: Offensive Language Identification (Task A), Offense Type Categorization (Task B), and Offense Target Identification (Task C). We focus on Tasks A & B. In contrast to 2019’s task, this year’s training data is labeled using distant supervision: various supervised models trained on the manually-annotated OLID2019 data set (Zampieri et al., 2019a) output labels and confidence values for a much larger data set, resulting in the SOLID dataset (Rosenthal et al., 2020). The semi-supervised labels are converted to an average confidence value between 0 and 1; 1 is aligned to the positive class for the task and 0 to the negative class. Thus an additional challenge in this task is determining an appropriate threshold value for mapping confidence measures to labels. To this end, we create a held-out hand-annotated development set from the training data for each task and use it as a test set for picking a good threshold.1 Using Scikit-learn’s"
2020.semeval-1.294,S19-2010,0,0.0250892,"Missing"
2020.sltu-1.48,apresjan-etal-2006-syntactically,0,0.0146223,"Missing"
2020.sltu-1.48,Q17-1010,0,0.0921251,"s appropriate when users are looking for orthographically similar, but not necessarily exactly matched strings. Word embeddings have also been successful in approximate search, finding semantic similarities between words even across languages. A word embedding is typically a vector representation, trained on large amounts (at least 1M tokens) of monolingual text, whose values reflect syntactic and semantic characteristics of the word, based on the contexts in which the word appears. These embeddings can be obtained using different algorithms, such as GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017), or BERT (Devlin et al., 2019), among many others. 4.2. Graphical User Interface (GUI) A graphical user interface is essential to the system because it makes it easy for our target users (teachers) to query the corpus. Our GUI’s design is simple and languageindependent. From the user’s perspective, the GUI workflow consists of the following steps: 1. An initial query sentence is entered into the interface 347 From the system’s perspective, this interaction requires the following steps: 1. We compute an embedding for each sentence in the entire corpus and store it along with the sentence’s tex"
2020.sltu-1.48,N19-1423,0,0.0206181,"g for orthographically similar, but not necessarily exactly matched strings. Word embeddings have also been successful in approximate search, finding semantic similarities between words even across languages. A word embedding is typically a vector representation, trained on large amounts (at least 1M tokens) of monolingual text, whose values reflect syntactic and semantic characteristics of the word, based on the contexts in which the word appears. These embeddings can be obtained using different algorithms, such as GloVe (Pennington et al., 2014), fastText (Bojanowski et al., 2017), or BERT (Devlin et al., 2019), among many others. 4.2. Graphical User Interface (GUI) A graphical user interface is essential to the system because it makes it easy for our target users (teachers) to query the corpus. Our GUI’s design is simple and languageindependent. From the user’s perspective, the GUI workflow consists of the following steps: 1. An initial query sentence is entered into the interface 347 From the system’s perspective, this interaction requires the following steps: 1. We compute an embedding for each sentence in the entire corpus and store it along with the sentence’s text. 2. On receiving the user’s f"
2020.sltu-1.48,W17-0102,0,0.0708481,"Missing"
2020.sltu-1.48,P14-2050,0,0.00985133,"-ui.com). Embedding algorithms typically assume that a lot of training data is available, and getting good embeddings with a small corpus has been a challenge. Another challenge is that embeddings tend to capture primarily semantic information with some syntactic information, while we want the reverse. Our team has continued to investigate variations on algorithms that might produce the best results. Approaches under consideration include transfer learning with BERT (Devlin et al., 2019), as well as training using skip-grams over part of speech tags or dependency parses (Mikolov et al., 2013; Levy and Goldberg, 2014). We plan to continue developing embedding strategies that are performant and syntactically rich even when trained with little data, to incorporate fuzzy string matching (possibly augmented with regular expression capabilities) into our system, and to conduct human evaluations that will assess the system’s success as a search interface. 5. Social Media Recently, it has become prevalent for speakers or learners of endangered languages to interact with language on social media (Huaman and Stokes, 2011; Jones and UribeJongbloed, 2012). Previous works on developing extensions for social media incl"
2020.sltu-1.48,D15-1166,0,0.0123109,"f this workshop, including issues discussed, and various conceived and implemented technologies for nine languages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw’ida, Kwak’wala, Ojibwe, San Juan Quiahije Chatino, and Seneca. Keywords: Low-resource languages, language documentation, language revitalization 1. Introduction Recently there have been large advances in natural language processing and language technology, leading to usable systems for speech recognition (Hinton et al., 2012; Graves et al., 2013; Hannun et al., 2014; Amodei et al., 2016), machine translation (Bahdanau et al., 2015; Luong et al., 2015; Wu et al., 2016), text-to-speech (van den Oord et al., 2016), and question answering (Seo et al., 2017) for a few of the world’s most-spoken languages, such as English, German, and Chinese. However, there is an urgent need for similar technology for the rest of the world’s languages, particularly those that are threatened or endangered. The rapid documentation and revitalization of these languages is of paramount importance, but all too often language technology plays little role in this process. In August 2019, the first edition of a ‘Workshop on Language Technology for Language Documentati"
2020.sltu-1.48,C16-1328,1,0.858998,"er. The universal speech recognition method has already been deployed as an app that can be used for documentation at https://www.dictate.app, but quantitative testing of its utility in an actual documentation scenario (as in Michaud et al. (2018)) is yet to be performed. Currently, Allosaurus can currently only output a phone that it has been trained to recognize, but as confused sounds frequently occur at the same place of articulation (Ng, 1998), it should be possible to find links between Allosaurus’s inventory and the provided inventory. Future work and possible integration with PanPhon (Mortensen et al., 2016) could allow the tool to adapt the output to the 343 nearest available sound (considering phonological distance) in the language’s inventory. 2.2. Phone to Orthography Decoder Ideally, one would like to convert the phones recognized by Allosaurus to native orthography. If successful, this would provide a speech recognition system that can directly recognize to the native orthography for low-resource languages, with minimal expertise and effort. Many low-resource languages have fairly new orthographies that are adapted from standard scripts. Because the orthographies are new, the phonetic value"
2020.sltu-1.48,D14-1162,0,0.0843609,"Missing"
2020.sltu-1.48,P16-1162,0,0.0098494,"our website. While some dictionaries have morphological information available that will allow for matching queries to stems or suffixes, not all do. One approach to producing automatic alternatives is to use morphological segmentation tools, including unsupervised tools that try to infer morphological boundaries from data. For Kwak’wala, we experimented with two approaches: Morfessor, a probabilistic model for doing unsupervised or semi-supervised morphological segmentation (Virpioja et al., 2013) and byte pair encoding, a compression model that can be applied to automatically segment words (Sennrich et al., 2016). We use these models to split words up into approximate roots and affixes, which are then used for search or to cluster similar words to show to users. Note that there is scope for improvement on this front, including having language speakers determine which automatic systems produce the best segmentations, exploring different ways of visualizing related words, and using these to improve dictionary search. 3.2. Inuktitut Inuktitut is a polysynthetic language spoken in Northern Canada and is an official language of the Territory of Nunavut. Inuktitut words contain many morphemes, which 345 are"
2021.americasnlp-1.23,2021.americasnlp-1.30,0,0.035558,"ill now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 2017). We employed the hyperparameters proposed by Guzmán et al. (2019) for a low-resource scenario. We implemented the model using Fairseq (Ott et al., 2019). The implementation of the baseline can be found in the official shared task repository.4 4.2 University of British Columbia The team of the University of British Columbia (UBC-NLP; Billah-Nagoudi et al., 2021) participated for all ten language pairs and in both tracks. They used an encoder-decoder transformer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages have more available data than others, this dataset is unbalanced in favor of languages like Nahuatl, Guarani, and Quechua. The team also proposed a two-stage fine-tuning method: first fine-tuning on the entire da"
2021.americasnlp-1.23,2021.americasnlp-1.28,0,0.0389782,"Missing"
2021.americasnlp-1.23,2020.lrec-1.320,1,0.692133,"test datasets were translated to modern Nahuatl. In particular, the translations belong to Nahuatl Central/Nahuatl de la Huasteca (Hidalgo y San Luis Potosí) dialects. In order to be closer to the training corpus, an orthographic normalization was applied. A simple rule based approach was used, which was based on the most predictable orthographic changes between modern varieties and Classical Nahuatl. Spanish—Guarani Guarani is mostly spoken in Paraguay, Bolivia, Argentina and Brazil. It belongs to the Tupian language family (ISO gnw, gun, gug, gui, grn, nhd). The training corpus for Guarani (Chiruzzo et al., 2020) was collected from web sources (blogs and news articles) that contained a mix of dialects, from pure Guarani to more mixed Jopara which combines Guarani with Spanish neologisms. The development and test corpora, on the other hand, are in standard Paraguayan Guarani. Spanish—Bribri Bribri is a Chibchan language spoken in southern Costa Rica (ISO code bzd). The training set for Bribri was extracted from six sources (Feldman and Coto-Solano, 2020; Margery, 2005; Jara Murillo, 2018a; Constenla et al., 2004; Jara Murillo and García Segura, 2013; Jara Murillo, 2018b; Flores Solórzano, 2017), includ"
2021.americasnlp-1.23,D18-1269,0,0.0884352,"uage families: Aymaran, Arawak, Chibchan, Tupi-Guarani, UtoAztecan, Oto-Manguean, Quechuan, and Panoan. The ten language pairs included in the shared task are: Quechua–Spanish, Wixarika–Spanish, Shipibo-Konibo–Spanish, Asháninka–Spanish, Raramuri–Spanish, Nahuatl–Spanish, Otomí– Spanish, Aymara–Spanish, Guarani–Spanish, and Bribri–Spanish. For development and testing, we used parallel sentences belonging to a new natural language inference dataset for the 10 indigenous languages featured in our shared task, which is a manual translation of the Spanish version of the multilingual XNLI dataset (Conneau et al., 2018). For a complete description of this dataset we refer the reader to Ebrahimi et al. (2021). Together with the data, we also provided: a simple baseline based on the small transformer architecture (Vaswani et al., 2017) proposed together with the FLORES dataset (Guzmán et al., 2019); and a description of challenges and particular characteristics for all provided resources1 . We established two tracks: one where training models on the development set after hyperparameter tuning is 1 https://github.com/AmericasNLP/americasnlp2021/ blob/main/data/information_datasets.pdf 202 Proceedings of the Fir"
2021.americasnlp-1.23,P18-1128,0,0.020486,"8.4 8.3 ChrF 39.4 38.3 35.8 33.2 32.8 31.8 26.9 10.3 9.8 9.0 6.6 ChrF 39.9 38.0 29.7 28.6 16.3 15.5 12.4 ChrF 25.8 24.8 24.7 23.9 21.6 16.5 15.9 12.2 10.5 10.5 8.4 Table 3: Results of Track 1 (development set used for training) for all systems and language pairs. The results are ranked by the official metric of the shared task: ChrF. One team decided to send a anonymous submission (Anonym). Best results are shown in bold, and they are significantly better than the second place team (in each language-pair) according to the Wilcoxon signed-ranked test and Pitman’s permutation test with p&lt;0.05 (Dror et al., 2018). 208 ious high-resource languages, and then finetuned for each target language using the official provided data. 4.7 NRC-CNRC The team of the National Research Council Canada (NRC-CNRC; Knowles et al., 2021) submitted systems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used"
2021.americasnlp-1.23,2020.coling-main.351,1,0.754898,"oken in Paraguay, Bolivia, Argentina and Brazil. It belongs to the Tupian language family (ISO gnw, gun, gug, gui, grn, nhd). The training corpus for Guarani (Chiruzzo et al., 2020) was collected from web sources (blogs and news articles) that contained a mix of dialects, from pure Guarani to more mixed Jopara which combines Guarani with Spanish neologisms. The development and test corpora, on the other hand, are in standard Paraguayan Guarani. Spanish—Bribri Bribri is a Chibchan language spoken in southern Costa Rica (ISO code bzd). The training set for Bribri was extracted from six sources (Feldman and Coto-Solano, 2020; Margery, 2005; Jara Murillo, 2018a; Constenla et al., 2004; Jara Murillo and García Segura, 2013; Jara Murillo, 2018b; Flores Solórzano, 2017), including a dictionary, a grammar, two language learning textbooks, one storybook and the transcribed sentences from Spanish–Wixarika Wixarika (also known as 2 Huichol) with ISO code hch is spoken in Mexico ISO 639-3 for the Nahutal languages: and belongs to the Yuto-Aztecan linguistic family. nch, ncx, naz, nln, nhe, ngu, nhk, nhx, nhp, ncl, nhm, nhy, The training, development and test sets all belong nlv, ppl, nhz, npl, nhc, nhv, to the same dialec"
2021.americasnlp-1.23,galarreta-etal-2017-corpus,1,0.842998,"training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a bilingual teacher, and follows the most recent guidelines of the Minister of Education in Peru, however, the third source is an extraction of parallel sentences from an old dictionary. The development and test sets were created following the official convention as in the translated training sets. Spanish—Quechua Quechua is a family of languages spoken in Argentina, Bolivia, Colombia, Spanish—Asháninka Asháninka is an Ecuador, Peru, and Chile with many ISO codes for Arawakan language (ISO: cni) spoken"
2021.americasnlp-1.23,W19-6804,1,0.768784,"az jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a bilingual teacher, and follows the most recent guidelines of the Minister of Education in Peru, however, the third source is an extraction of parallel sentences from an old dictionary. The development and test sets were created following the official convention as in the translated training sets. Spanish—Quechua Quechua is a family of languages spoken in Argentina, Bolivia, Colombia, Spanish—Asháninka Asháninka is an Ecu"
2021.americasnlp-1.23,N15-2021,1,0.735842,"eceive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available parallel corpora (§3). Additionally, all participants were allowed to use other existing datasets or create their own resources for training in order to improve their systems. E"
2021.americasnlp-1.23,L16-1666,1,0.700987,"ng similarities and differences between training sets on the one hand and development and test sets on the other. The training data (Mager et al., 2018a) is a translation of the fairy tales of Hans Christian Andersen and contains word acquisitions and code-switching. Spanish–Nahuatl Nahuatl is a Yuto-Aztecan language spoken in Mexico and El Salvador, with a wide dialectal variation (around 30 variants). For each main dialect a specific ISO 639-3 code is available.2 There is a lack of consensus regarding the orthographic standard. This is very noticeable in the training data: the train corpus (Gutierrez-Vasques et al., 2016) has dialectal, domain, orthographic and diachronic variation (Nahuatl side). However, the majority of entries are closer to a Classical Nahuatl orthographic “standard”. The development and test datasets were translated to modern Nahuatl. In particular, the translations belong to Nahuatl Central/Nahuatl de la Huasteca (Hidalgo y San Luis Potosí) dialects. In order to be closer to the training corpus, an orthographic normalization was applied. A simple rule based approach was used, which was based on the most predictable orthographic changes between modern varieties and Classical Nahuatl. Spani"
2021.americasnlp-1.23,D19-1632,1,0.923833,"nish, Aymara–Spanish, Guarani–Spanish, and Bribri–Spanish. For development and testing, we used parallel sentences belonging to a new natural language inference dataset for the 10 indigenous languages featured in our shared task, which is a manual translation of the Spanish version of the multilingual XNLI dataset (Conneau et al., 2018). For a complete description of this dataset we refer the reader to Ebrahimi et al. (2021). Together with the data, we also provided: a simple baseline based on the small transformer architecture (Vaswani et al., 2017) proposed together with the FLORES dataset (Guzmán et al., 2019); and a description of challenges and particular characteristics for all provided resources1 . We established two tracks: one where training models on the development set after hyperparameter tuning is 1 https://github.com/AmericasNLP/americasnlp2021/ blob/main/data/information_datasets.pdf 202 Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas, pages 202–217 June 11, 2021. ©2021 Association for Computational Linguistics allowed (Track 1), and one where models cannot be trained directly on the development set (Track 2). Machine translation"
2021.americasnlp-1.23,2021.americasnlp-1.25,0,0.125338,"dded to the existing dataset. Then, a normalization process was done using existing tools, and the aligned data was further filtered. The quality of the data was also considered, and each dataset was assigned a weight depending on a noisiness estimation. The team used a transformer sequenceto-sequence model trained via two steps. For their main submission they first trained on data which 4 Otomí online corpus: https://tsunkua.elotl.mx/about/ 206 https://github.com/AmericasNLP/americasnlp2021 Team Langs. Sub. CoAStaL (Bollmann et al., 2021) 10 20 Helsinki (Vázquez et al., 2021) 10 50 NRC-CNRC (Knowles et al., 2021) 4 17 REPUcs (Moreno, 2021) Tamalli (Parida et al., 2021) 1 2 10 UBC-NLP (BillahNagoudi et al., 2021) UTokyo (Zheng et al., 2021) Anonymous Data Models Multilingual Pretrained PB-SMT, Constrained Random Strings Transformer NMT No No 42 Bible, JW300, OPUS, Wikipedia, New collected data Bible, OPUS, Constitutions, Normalization, Filtering, BackTranslation No external data, preoricessing, BPE Dropout. JW300, New dataset, Europarl - 8 29 Bible, Wikipedia 10 40 8 14 Monolingual from other languages. Data - Yes, all ST No languages + Spanish-English Transofrmer NMT Yes, languages 4- No Transformer N"
2021.americasnlp-1.23,P07-2045,0,0.0229964,"lcoxon signed-ranked test and Pitman’s permutation test with p&lt;0.05 (Dror et al., 2018). 208 ious high-resource languages, and then finetuned for each target language using the official provided data. 4.7 NRC-CNRC The team of the National Research Council Canada (NRC-CNRC; Knowles et al., 2021) submitted systems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used BPE for tokenization. The team experimented with multilingual models pretrained on either 3 or 4 languages, finding that the 4 language model achieved higher performance. Additionally the team trained a Translation Memory (Simard and Fujita, 2012) using half of the examples of the development set. Surprisingly, even given its small amount of training data, this system outperformed the team’s Track 2 submission for Rarámuri. 4.8 Tamalli The team Tamalli7 (Parida et al., 2021) participated in Track 1 for all 10 language pairs. T"
2021.americasnlp-1.23,D18-2012,0,0.0268314,"encoders using UD annotations could not get any meaningful results. 4.5 REPUcs the the Spanish–Quechua language pair in both tracks. The team collected external data from 3 different sources and analyzed the domain disparity between this training data and the development set. To solve the problem of domain mismatch, they decided to collect additional data that could be a better match for the target domain. The used data from a handbook (Iter and Ortiz-Cárdenas, 2019), a lexicon,5 and poems on the web (Duran, 2010).6 Their model is a transformer encoder-decoder architecture with SentencePiece (Kudo and Richardson, 2018) tokenization. Together with the existing parallel corpora, the new paired data was used for finetuning on top of a pretrained Spanish–English translation model. The team submitted two versions of their system: the first was only finetuned on JW300+ data, while the second one additionally leveraged the newly collected dataset. 4.6 UTokyo The team of the University of Tokyo (UTokyo; Zheng et al., 2021) submitted systems for all languages and both tracks. A multilingual pretrained encoder-decoder model (mBART; Liu et al., 2020) was used, implemented with the Fairseq toolkit (Ott et al., 2019). T"
2021.americasnlp-1.23,C18-1006,1,0.909443,"ms achieved 12.97 ChrF higher than baseline, when averaged across languages. 1 Introduction Many of the world’s languages, including languages native to the Americas, receive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available para"
2021.americasnlp-1.23,mayer-cysouw-2014-creating,0,0.0200209,"to the Valle del Mezquital dialect (ote). This was specially challenging for the translation task, since the development and test sets are from the Ñûhmû de Ixtenco, Tlaxcala, variant (otz), which also has its own orthographic system. This variant is especially endangered as less than 100 elders still speak it. 3.3 External Data Used by Participants In addition to the provided datasets, participants also used additional publicly available parallel data, monolingual corpora or newly collected data sets. The most common datasets were JW300 (Agi´c and Vuli´c, 2019) and the Bible’s New Testament (Mayer and Cysouw, 2014; Christodouloupoulos and Steedman, 2015; McCarthy et al., 2020). Besides those, GlobalVoices (Prokopidis et al., 2016) and datasets available at OPUS (Tiedemann, 2012) were added. New datasets were extracted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a"
2021.americasnlp-1.23,2020.lrec-1.352,0,0.0111319,"llenging for the translation task, since the development and test sets are from the Ñûhmû de Ixtenco, Tlaxcala, variant (otz), which also has its own orthographic system. This variant is especially endangered as less than 100 elders still speak it. 3.3 External Data Used by Participants In addition to the provided datasets, participants also used additional publicly available parallel data, monolingual corpora or newly collected data sets. The most common datasets were JW300 (Agi´c and Vuli´c, 2019) and the Bible’s New Testament (Mayer and Cysouw, 2014; Christodouloupoulos and Steedman, 2015; McCarthy et al., 2020). Besides those, GlobalVoices (Prokopidis et al., 2016) and datasets available at OPUS (Tiedemann, 2012) were added. New datasets were extracted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 20"
2021.americasnlp-1.23,nordhoff-hammarstrom-2012-glottolog,0,0.0139496,"manually translated sentences for the development and test sets. An official baseline trained on this data was also provided. Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline. The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages. 1 Introduction Many of the world’s languages, including languages native to the Americas, receive worryingly little attention from NLP researchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; B"
2021.americasnlp-1.23,2020.loresmt-1.1,1,0.864016,"earchers. According to Glottolog (Nordhoff and Hammarström, 2012), 86 language families and 95 language isolates can be found in the Americas, and many of them are labeled as endangered. From an NLP perspective, the development of language technologies has the potential to help language communities and activists in the documentation, promotion and revitalization of their languages (Mager et al., 2018b; Galla, 2016). There have been recent initiatives to promote research on languages of the Americas (Fernández et al., 2013; Coler and Homola, 2014; Gutierrez-Vasques, 2015; Mager and Meza, 2018; Ortega et al., 2020; Zhang et al., 2020; Schwartz et al., 2020; Barrault et al., 2020). ∗ *The first three authors contributed equally. The AmericasNLP 2021 Shared Task on Open Machine Translation (OMT) aimed at moving research on indigenous and endangered languages more into the focus of the NLP community. As the official shared task training sets, we provided a collection of publicly available parallel corpora (§3). Additionally, all participants were allowed to use other existing datasets or create their own resources for training in order to improve their systems. Each language pair used in the shared task c"
2021.americasnlp-1.23,N19-4009,1,0.919181,"acted from constitutions, dictionaries, and educational books. For monolingual text, Wikipedia was most commonly used, assuming one was available in a language. 3 4 Baseline and Submitted Systems We will now describe our baseline as well as all submitted systems. An overview of all teams and the main ideas going into their submissions is shown in Table 2. 4.1 Baseline Our baseline system was a transformer-based sequence to sequence model (Vaswani et al., 2017). We employed the hyperparameters proposed by Guzmán et al. (2019) for a low-resource scenario. We implemented the model using Fairseq (Ott et al., 2019). The implementation of the baseline can be found in the official shared task repository.4 4.2 University of British Columbia The team of the University of British Columbia (UBC-NLP; Billah-Nagoudi et al., 2021) participated for all ten language pairs and in both tracks. They used an encoder-decoder transformer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages h"
2021.americasnlp-1.23,P02-1040,0,0.110098,"ee to use any resources they could find. Possible resources could, for instance, include existing or newly created parallel data, dictionaries, tools, or pretrained models. We invited submissions to two different tracks: Systems in Track 1 were allowed to use the development set as part of the training data, since this is a common practice in the machine translation community. Systems in Track 2 were not allowed to be trained directly on the development set, mimicking a more realistic low-resource setting. 2.2 resulting in a small number of words per sentence. We further reported BLEU scores (Papineni et al., 2002) for all systems and languages. Adequacy The output sentence expresses the meaning of the reference. 1. Extremely bad: The original meaning is not contained at all. 2. Bad: Some words or phrases allow to guess the content. 3. Neutral. 4. Sufficiently good: The original meaning is understandable, but some parts are unclear or incorrect. 5. Excellent: The meaning of the output is the same as that of the reference. Fluency The output sentence is easily readable and looks like a human-produced text. Primary Evaluation In order to be able to evaluate a large number of systems on all 10 languages, w"
2021.americasnlp-1.23,2021.americasnlp-1.24,0,0.0516404,"Missing"
2021.americasnlp-1.23,W15-3049,0,0.0738763,"Missing"
2021.americasnlp-1.23,L16-1144,0,0.161871,"2019), which consists of Jehovah’s Witness texts, sentences extracted from the official dictionary of the Minister of Education (MINEDU), and miscellaneous dictionary entries and samples which have been collected and reviewed by Huarcaya Taquiri (2020). Spanish–Aymara Aymara is a Aymaran language spoken in Bolivia, Peru, and Chile (ISO codes aym, ayr, ayc). The development and test sets are translated into the Central Aymara variant (ayr), specifically Aymara La Paz jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al.,"
2021.americasnlp-1.23,2012.amta-papers.26,0,0.0148085,"ems for the Spanish to Wixárika, Nahuatl, Rarámuri and Guarani language pairs for both tracks. Due to ethical considerations, the team decided not to use external data, and restricted themselves to the data provided for the shared task. All data was preprocessed with standard Moses tools (Koehn et al., 2007). The submitted systems were based on a Transformer model, and used BPE for tokenization. The team experimented with multilingual models pretrained on either 3 or 4 languages, finding that the 4 language model achieved higher performance. Additionally the team trained a Translation Memory (Simard and Fujita, 2012) using half of the examples of the development set. Surprisingly, even given its small amount of training data, this system outperformed the team’s Track 2 submission for Rarámuri. 4.8 Tamalli The team Tamalli7 (Parida et al., 2021) participated in Track 1 for all 10 language pairs. The team used an IBM Model 2 for SMT, and a transformer model for NMT. The team’s NMT models were trained in two settings: one-to-one, with one model being trained per target language, and one-to-many, where decoder weights were shared across languages and a language embedding layer was added to the decoder. They s"
2021.americasnlp-1.23,tiedemann-2012-parallel,0,0.274715,"sentences extracted from the official dictionary of the Minister of Education (MINEDU), and miscellaneous dictionary entries and samples which have been collected and reviewed by Huarcaya Taquiri (2020). Spanish–Aymara Aymara is a Aymaran language spoken in Bolivia, Peru, and Chile (ISO codes aym, ayr, ayc). The development and test sets are translated into the Central Aymara variant (ayr), specifically Aymara La Paz jilata, the largest variant. This is similar to the variant of the available training set, which is obtained from Global Voices (Prokopidis et al., 2016) (and published in OPUS (Tiedemann, 2012)), a news portal translated by volunteers. However, the text may have potentially different writing styles that are not necessarily edited. Spanish-–Shipibo-Konibo Shipibo-Konibo is a Panoan language spoken in Perú (ISO shp and kaq). The training sets for Shipibo-Konibo have been obtained from different sources and translators: Sources include translations of a sample from the Tatoeba dataset (Gómez Montoya et al., 2019), translated sentences from books for bilingual education (Galarreta et al., 2017), and dictionary entries and examples (Loriot et al., 1993). Translated text was created by a"
2021.americasnlp-1.23,2021.americasnlp-1.29,0,0.0275308,"rmer model based on T5 (Raffel et al., 2020). This model was pretrained on a dataset consisting of 10 indigenous languages and Spanish, that was collected by the team from different sources such as the Bible and Wikipedia, totaling 1.17 GB of text. However, given that some of the languages have more available data than others, this dataset is unbalanced in favor of languages like Nahuatl, Guarani, and Quechua. The team also proposed a two-stage fine-tuning method: first fine-tuning on the entire dataset, and then only on the target languages. 4.3 Helsinki The University of Helsinki (Helsinki; Vázquez et al., 2021) participated for all ten language pairs in both tracks. This team did an extensive exploration of the existing datasets, and collected additional resources both from commonly used sources such as the Bible and Wikipedia, as well as other minor sources such as constitutions. Monolingual data was used to generate paired sentences through back-translation, and these parallel examples were added to the existing dataset. Then, a normalization process was done using existing tools, and the aligned data was further filtered. The quality of the data was also considered, and each dataset was assigned"
2021.americasnlp-1.23,2020.emnlp-main.43,0,0.0550595,"Missing"
2021.americasnlp-1.23,2021.americasnlp-1.26,0,0.0686019,"d. The quality of the data was also considered, and each dataset was assigned a weight depending on a noisiness estimation. The team used a transformer sequenceto-sequence model trained via two steps. For their main submission they first trained on data which 4 Otomí online corpus: https://tsunkua.elotl.mx/about/ 206 https://github.com/AmericasNLP/americasnlp2021 Team Langs. Sub. CoAStaL (Bollmann et al., 2021) 10 20 Helsinki (Vázquez et al., 2021) 10 50 NRC-CNRC (Knowles et al., 2021) 4 17 REPUcs (Moreno, 2021) Tamalli (Parida et al., 2021) 1 2 10 UBC-NLP (BillahNagoudi et al., 2021) UTokyo (Zheng et al., 2021) Anonymous Data Models Multilingual Pretrained PB-SMT, Constrained Random Strings Transformer NMT No No 42 Bible, JW300, OPUS, Wikipedia, New collected data Bible, OPUS, Constitutions, Normalization, Filtering, BackTranslation No external data, preoricessing, BPE Dropout. JW300, New dataset, Europarl - 8 29 Bible, Wikipedia 10 40 8 14 Monolingual from other languages. Data - Yes, all ST No languages + Spanish-English Transofrmer NMT Yes, languages 4- No Transformer NMT. WB-SMT. Transformer NMT, Transformer T5 Yes, with Spanish-English 10-languages Spanish-English pretraining No 10-Languages Ne"
2021.sigmorphon-1.10,2020.acl-main.695,0,0.0948004,"ical similarity and the other focusing on semantic similarity. Intuitively, we would expect the cluster of forms for walk to be recognizable largely based on orthographic similarity. The partially irregular cluster for bring shows greater orthographical variability in the past-tense form brought and so might be expected to require information beyond orthographic similarity. tional morphology and unsupervised learning that could be combined to approach this problem. Previous work has identified the benefit of combining rules based on linguistic characteristics with machine learning techniques. Erdmann et al. (2020) established a baseline for the Paradigm Discovery Problem that clusters the unannotated sentences first by a combination of string similarity and lexical semantics and then uses this clustering as input for a neural transducer. Erdmann and Habash (2018) investigated the benefits of different similarity models as they apply to Arabic dialects. Their findings demonstrated that Word2Vec embeddings significantly underperformed in comparison to the Levenshtein distance baseline. The highest performing representation was a combination of FastText and a de-lexicalized morphological analyzer. The Fas"
2021.sigmorphon-1.10,W18-5806,0,0.0244382,"cal variability in the past-tense form brought and so might be expected to require information beyond orthographic similarity. tional morphology and unsupervised learning that could be combined to approach this problem. Previous work has identified the benefit of combining rules based on linguistic characteristics with machine learning techniques. Erdmann et al. (2020) established a baseline for the Paradigm Discovery Problem that clusters the unannotated sentences first by a combination of string similarity and lexical semantics and then uses this clustering as input for a neural transducer. Erdmann and Habash (2018) investigated the benefits of different similarity models as they apply to Arabic dialects. Their findings demonstrated that Word2Vec embeddings significantly underperformed in comparison to the Levenshtein distance baseline. The highest performing representation was a combination of FastText and a de-lexicalized morphological analyzer. The FastText embeddings (Bojanowski et al., 2016) have the benefit of including sub-word information by representing words as character n-grams. The de-lexicalized analyzer relies on linguistic expert knowledge of Arabic to identify the morphological closeness"
2021.sigmorphon-1.10,2020.scil-1.19,0,0.0197155,"f representations. We would also like to explore the use of predefined centers for clustering. These pre-defined centers could be provided using either a longest common subsequence method or a graph-based algorithm such as that described in section 4.3. The final output of the system is a set of clusters, each one representing a morphological paradigm. 2 Previous Work The SIGMORPHON 2020 shared task set included an open problem calling for unsupervised systems to complete morphological paradigms. For the 2020 task, participants were provided with the set of lemmas available for each language (Kann, 2020). In contrast, the 2021 SIGMORPHON task 2 outlines that submissions are unsupervised systems that cluster input tokens into the appropriate morphological paradigm (Nicolai et al., 2020). Given the novelty of the task, there is a lack of previous work done to cluster morphological paradigms in an unsupervised manner. However, we have identified key methods from previous work in computa3 Task overview The 2021 SIGMORPHON Shared Task 2 created a call for unsupervised systems that would create morphological paradigm clusters. This was intended to build upon the shared task from 2020 that focused o"
2021.sigmorphon-1.10,2020.lrec-1.352,0,0.334982,"Missing"
2021.sigmorphon-1.10,N15-1186,0,0.0732154,"f FastText and a de-lexicalized morphological analyzer. The FastText embeddings (Bojanowski et al., 2016) have the benefit of including sub-word information by representing words as character n-grams. The de-lexicalized analyzer relies on linguistic expert knowledge of Arabic to identify the morphological closeness of two words. In the context of the paper, it is used to prune out word relations that do not conform to Arabic morphological rules. The approach mentioned greatly benefits from the use of a morphological analyzer, something that is not readily available for low-resource languages. Soricut and Och (2015) focused on the use of morphological transformations as the basis for word representations. Their representation can be quite accurate for affix-based morphology. Our representations are based entirely off of unlabelled data and do not require linguistic experts to provide morphological transformation rules for the language. Additionally, we hoped to create a system that would be robust for languages that include non-affix based morphology. In this work we compare Word2Vec representations to characterbased representations to represent orthography. We have not yet evaluated additional represent"
C10-1107,D09-1031,1,0.849439,"cisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation scheme, where InterAnnotator Agreement (IAA) is low and thus the consistency of th"
C10-1107,burchardt-etal-2006-salsa,0,0.0832951,"Missing"
C10-1107,P07-1007,0,0.0721121,"instances, based on its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be ap"
C10-1107,N06-1016,0,0.323805,"onment, using fine-grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for learning to assign sem"
C10-1107,N04-1012,0,0.202379,"ier is re-trained on the new data set. The newly trained classifier now picks the next instances, based on its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the"
C10-1107,rehbein-ruppenhofer-2010-theres,1,0.836151,"f sentences selected by the classifier. For our human annotators, however, due to different annotation decisions the resulting set of sentences is expected to differ. Sampling method Uncertainty sampling is a standard sampling method for AL where new instances are selected based on the confidence of the classifier for predicting the appropriate label. During early stages of the learning process when the classifier is trained on a very small seed data set, it is not beneficial to add the instances with the lowest classifier confidence. Instead, we use a dynamic version of uncertainty sampling (Rehbein and Ruppenhofer, 2010), based on the confidence of a maximum entropy classifier2 , taking into account how much the classifier has learned so far. In each iteration one new instance is selected from the pool and presented to the oracle. After annotation the classifier is retrained on the new data set. The modified uncertainty sampling results in a more robust classifier performance during early stages of the learning process. 952 2 http://maxent.sourceforge.net A1 A2 A3 A4 A5 A6 ø sl Anlass R U 8.6 9.6 4.4 5.7 9.9 9.2 5.8 4.9 3.0 3.5 5.4 6.3 6.2 6.5 25.8 27.8 Motiv R U 5.9 6.6 4.8 5.9 6.8 6.7 3.6 3.6 3.0 2.6 5.3 4."
C10-1107,P08-1098,0,0.0208374,"ions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for learning to assign semantic frames to predicates, following Erk (200"
C10-1107,W07-1516,0,0.17367,"signs the correct label. The newly-annotated instances are added to the seed data and the classifier is re-trained on the new data set. The newly trained classifier now picks the next instances, based on its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL see"
C10-1107,D07-1051,0,0.0387629,"grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for learning to assign semantic frames to predic"
C10-1107,D07-1082,0,0.198912,"its updated knowledge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grai"
C10-1107,C08-1143,0,0.124091,"ge, and the process repeats. If the learning process can provide precisely that information which the classifier still needs to learn, a smaller number of instances should suffice to achieve the same accuracy as on a larger training set of randomly selected training examples. Active learning has been applied to a number of natural language processing tasks like POS tagging (Ringger et al., 2007), NER (Laws and Sch¨utze, 2008; Tomanek and Hahn, 2009), syntactic parsing (Osborne and Baldridge, 2004; Hwa, 2004), Word Sense Disambiguation (Chen et al., 2006; Chan and Ng, 2007; Zhu and Hovy, 2007; Zhu et al., 2008) and morpheme glossing for language documentation (Baldridge and Palmer, 2009). While most of these studies successfully show that the same classification accuracy can be achieved with a substantially smaller data set, these findings are mostly based on simulations using gold standard data. For our task of Word Sense Disambiguation (WSD), mixed results have been achieved. AL seems to improve results in a WSD task with coarse-grained sense distinctions (Chan and Ng, 2007), but the results of (Dang, 2004) raise doubts as to whether AL can successfully be applied to a fine-grained annotation sche"
C10-1107,J04-3001,0,0.391882,"istic environment, using fine-grained sense distinctions, and investigate whether AL can reduce annotation cost and boost classifier performance when applied to a real-world task. 1 Introduction Active learning has recently attracted attention as having the potential to overcome the knowledge acquisition bottleneck by limiting the amount of human annotation needed to create training data for statistical classifiers. Active learning has been shown, for a number of different NLP tasks, to reduce the number of manually annotated instances needed for obtaining a consistent classifier performance (Hwa, 2004; Chen et al., 2006; Tomanek et al., 2007; Reichart et al., 2008). The majority of such results have been achieved by simulating the annotation scenario using prelabelled gold standard annotations as a stand-in for real-time human annotation. Simulating annotation allows one to test different parameter settings without incurring the cost of human annotation. There is, however, a major drawback: we In this paper we bring active learning to life in the context of frame semantic annotation of German texts within the SALSA project (Burchardt et al., 2006). Specifically, we apply AL methods for lea"
C10-1107,C08-1059,0,0.314897,"Missing"
C10-1107,P08-1000,0,\N,Missing
C10-2107,S07-1018,0,0.0848176,"Missing"
C10-2107,W08-2208,0,0.0332928,"t al., 2009; Shen and Lapata, 2007). Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text. Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling (F¨urstenau and Lapata, 2009b; F¨urstenau and Lapata, 2009a; Deschacht and Moens, 2009; Gordon and Swanson, 2007; Pad´o et al., 2008) to systems which can infer missing word senses (Pennacchiotti et al., 2008b; Pennacchiotti et al., 2008a; Cao et al., 2008; Burchardt et al., 2005). However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology. Section 4 describes our approach to coverage evaluation, Section 3 discusses the texts analyzed, and the analysis itself appears in Section 5. Section 6 then looks at one possibility for addre"
C10-2107,D09-1003,0,0.0267383,"ion answering. However, studies also found that state-of-the-art FrameNet-style SRL systems perform too poorly to provide any substantial benefit to real applications (Burchardt et al., 2009; Shen and Lapata, 2007). Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text. Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling (F¨urstenau and Lapata, 2009b; F¨urstenau and Lapata, 2009a; Deschacht and Moens, 2009; Gordon and Swanson, 2007; Pad´o et al., 2008) to systems which can infer missing word senses (Pennacchiotti et al., 2008b; Pennacchiotti et al., 2008a; Cao et al., 2008; Burchardt et al., 2005). However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology. Section 4 describes ou"
C10-2107,erk-pado-2006-shalmaneser,0,0.371376,"Missing"
C10-2107,D09-1002,0,0.202434,"Missing"
C10-2107,E09-1026,0,0.0593346,"Missing"
C10-2107,J02-3001,0,0.432239,"itself appears in Section 5. Section 6 then looks at one possibility for addressing the coverage problem. The final section presents some discussion and conclusions. 929 (a) (b) Figure 1: Terminology: (a) Frame with core frame elements (FEs) and frame-evoking elements (FEEs) (b) Target with possible frame assignments and resultant lexical units (LUs) 2 FrameNet Manual annotation of corpora with semantic argument structure information has enabled the development of statistical and supervised machine learning techniques for semantic role labeling (Toutanova et al., 2008; Moschitti et al., 2008; Gildea and Jurafsky, 2002). The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). PropBank aims to provide a semantic role annotation for every verb in the Penn TreeBank (Marcus et al., 1994) and assigns roles on a verbby-verb basis, without making higher-level generalizations. Whether two distinct usages of a given verb are viewed as different senses or not is thus driven by both syntax (namely, differences in syntactic argument structure) and semantics (via basic, easily-discernable differences in meaning). FrameNet1 is a lexicographic project whose aim it is to create a l"
C10-2107,P07-1025,0,0.0206462,"dies also found that state-of-the-art FrameNet-style SRL systems perform too poorly to provide any substantial benefit to real applications (Burchardt et al., 2009; Shen and Lapata, 2007). Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text. Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling (F¨urstenau and Lapata, 2009b; F¨urstenau and Lapata, 2009a; Deschacht and Moens, 2009; Gordon and Swanson, 2007; Pad´o et al., 2008) to systems which can infer missing word senses (Pennacchiotti et al., 2008b; Pennacchiotti et al., 2008a; Cao et al., 2008; Burchardt et al., 2005). However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology. Section 4 describes our approach to coverage eva"
C10-2107,J08-2003,0,0.0166903,"lyzed, and the analysis itself appears in Section 5. Section 6 then looks at one possibility for addressing the coverage problem. The final section presents some discussion and conclusions. 929 (a) (b) Figure 1: Terminology: (a) Frame with core frame elements (FEs) and frame-evoking elements (FEEs) (b) Target with possible frame assignments and resultant lexical units (LUs) 2 FrameNet Manual annotation of corpora with semantic argument structure information has enabled the development of statistical and supervised machine learning techniques for semantic role labeling (Toutanova et al., 2008; Moschitti et al., 2008; Gildea and Jurafsky, 2002). The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). PropBank aims to provide a semantic role annotation for every verb in the Penn TreeBank (Marcus et al., 1994) and assigns roles on a verbby-verb basis, without making higher-level generalizations. Whether two distinct usages of a given verb are viewed as different senses or not is thus driven by both syntax (namely, differences in syntactic argument structure) and semantics (via basic, easily-discernable differences in meaning). FrameNet1 is a lexicographic project w"
C10-2107,C08-1084,1,0.903036,"Missing"
C10-2107,J05-1004,0,0.109455,"ity for addressing the coverage problem. The final section presents some discussion and conclusions. 929 (a) (b) Figure 1: Terminology: (a) Frame with core frame elements (FEs) and frame-evoking elements (FEEs) (b) Target with possible frame assignments and resultant lexical units (LUs) 2 FrameNet Manual annotation of corpora with semantic argument structure information has enabled the development of statistical and supervised machine learning techniques for semantic role labeling (Toutanova et al., 2008; Moschitti et al., 2008; Gildea and Jurafsky, 2002). The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). PropBank aims to provide a semantic role annotation for every verb in the Penn TreeBank (Marcus et al., 1994) and assigns roles on a verbby-verb basis, without making higher-level generalizations. Whether two distinct usages of a given verb are viewed as different senses or not is thus driven by both syntax (namely, differences in syntactic argument structure) and semantics (via basic, easily-discernable differences in meaning). FrameNet1 is a lexicographic project whose aim it is to create a lexical resource documenting valence structures for differen"
C10-2107,D08-1048,0,0.112106,"any substantial benefit to real applications (Burchardt et al., 2009; Shen and Lapata, 2007). Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text. Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling (F¨urstenau and Lapata, 2009b; F¨urstenau and Lapata, 2009a; Deschacht and Moens, 2009; Gordon and Swanson, 2007; Pad´o et al., 2008) to systems which can infer missing word senses (Pennacchiotti et al., 2008b; Pennacchiotti et al., 2008a; Cao et al., 2008; Burchardt et al., 2005). However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology. Section 4 describes our approach to coverage evaluation, Section 3 discusses the texts analyzed, and the analysis itself appears in Section 5. S"
C10-2107,pennacchiotti-etal-2008-towards,0,0.0127403,"any substantial benefit to real applications (Burchardt et al., 2009; Shen and Lapata, 2007). Extending the value of automated semantic parsing for a variety of applications requires improving the ability of systems to process unrestricted text. Several methods have been proposed to address different aspects of the coverage problem, ranging from automatic data expansion and semi-supervised semantic role labelling (F¨urstenau and Lapata, 2009b; F¨urstenau and Lapata, 2009a; Deschacht and Moens, 2009; Gordon and Swanson, 2007; Pad´o et al., 2008) to systems which can infer missing word senses (Pennacchiotti et al., 2008b; Pennacchiotti et al., 2008a; Cao et al., 2008; Burchardt et al., 2005). However, so far there has not been a detailed analysis of the problem. In this paper we provide that detailed analysis, by defining different types of coverage problems and performing analysis of both coverage and performance of an automated SRL system on three different data sets. Section 2 of the paper provides an introduction to FrameNet and introduces the basic terminology. Section 4 describes our approach to coverage evaluation, Section 3 discusses the texts analyzed, and the analysis itself appears in Section 5. S"
C10-2107,D07-1002,0,0.414803,"g data for each sense. This approach to evaluation arises from the Automated frame-semantic analysis aims to extract from text the key event-denoting predicates and the semantic argument structure for those predicates. The semantic argument structure of a predicate describing an event encodes relationships between the participants involved in the event, e.g. who did what to whom. Knowledge of semantic argument structure is essential for language understanding and thus important for applications such as information extraction (Moschitti et al., 2003; Surdeanu et al., 2003), question answering (Shen and Lapata, 2007), or recognizing textual entailment (Burchardt et al., 2009). Evaluating an existing system for its ability to aid such tasks is unrealistic if the evaluation is lemmabased rather than text-based. Consequently, there continues to be significant interest in developing semantic role labeling (SRL) systems able to automatically compute the semantic argument structures in an input text. Performance on the full text task, though, is typically much lower than for the more restricted evaluations. The SemEval 2007 Task on “Frame Semantic Structure Extraction,” for example, required systems to identify"
C10-2107,P03-1002,0,0.0134661,"d (ii) there is a suitable amount of training data for each sense. This approach to evaluation arises from the Automated frame-semantic analysis aims to extract from text the key event-denoting predicates and the semantic argument structure for those predicates. The semantic argument structure of a predicate describing an event encodes relationships between the participants involved in the event, e.g. who did what to whom. Knowledge of semantic argument structure is essential for language understanding and thus important for applications such as information extraction (Moschitti et al., 2003; Surdeanu et al., 2003), question answering (Shen and Lapata, 2007), or recognizing textual entailment (Burchardt et al., 2009). Evaluating an existing system for its ability to aid such tasks is unrealistic if the evaluation is lemmabased rather than text-based. Consequently, there continues to be significant interest in developing semantic role labeling (SRL) systems able to automatically compute the semantic argument structures in an input text. Performance on the full text task, though, is typically much lower than for the more restricted evaluations. The SemEval 2007 Task on “Frame Semantic Structure Extraction"
C10-2107,J08-2002,0,0.0130768,"discusses the texts analyzed, and the analysis itself appears in Section 5. Section 6 then looks at one possibility for addressing the coverage problem. The final section presents some discussion and conclusions. 929 (a) (b) Figure 1: Terminology: (a) Frame with core frame elements (FEs) and frame-evoking elements (FEEs) (b) Target with possible frame assignments and resultant lexical units (LUs) 2 FrameNet Manual annotation of corpora with semantic argument structure information has enabled the development of statistical and supervised machine learning techniques for semantic role labeling (Toutanova et al., 2008; Moschitti et al., 2008; Gildea and Jurafsky, 2002). The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). PropBank aims to provide a semantic role annotation for every verb in the Penn TreeBank (Marcus et al., 1994) and assigns roles on a verbby-verb basis, without making higher-level generalizations. Whether two distinct usages of a given verb are viewed as different senses or not is thus driven by both syntax (namely, differences in syntactic argument structure) and semantics (via basic, easily-discernable differences in meaning). FrameNet1 is a"
C10-2107,H94-1020,0,0.0333817,"core frame elements (FEs) and frame-evoking elements (FEEs) (b) Target with possible frame assignments and resultant lexical units (LUs) 2 FrameNet Manual annotation of corpora with semantic argument structure information has enabled the development of statistical and supervised machine learning techniques for semantic role labeling (Toutanova et al., 2008; Moschitti et al., 2008; Gildea and Jurafsky, 2002). The two main resources are PropBank (Palmer et al., 2005) and FrameNet (Ruppenhofer et al., 2006). PropBank aims to provide a semantic role annotation for every verb in the Penn TreeBank (Marcus et al., 1994) and assigns roles on a verbby-verb basis, without making higher-level generalizations. Whether two distinct usages of a given verb are viewed as different senses or not is thus driven by both syntax (namely, differences in syntactic argument structure) and semantics (via basic, easily-discernable differences in meaning). FrameNet1 is a lexicographic project whose aim it is to create a lexical resource documenting valence structures for different word senses and their possible mappings to underlying semantic argument structure (Ruppenhofer et al., 2006). In contrast to PropBank, FrameNet is pr"
C10-2107,S10-1008,1,\N,Missing
C10-2107,ruppenhofer-etal-2010-generating,0,\N,Missing
D09-1031,W09-1905,1,0.934703,"CL and AFNLP Language Danish Dutch English Swedish Uspanteko #words-tr 62825 129586 167593 127684 43473 #words-dev 31561 65483 131768 63783 19906 #tags 10 13 45 41 69 #sents-tr 3570 9365 6945 7326 7423 #sents-dev 1618 3982 5527 3714 3288 Avg.sent 18.18 14.61 24.00 17.34 5.92 Avg.tr.sent 17.60 13.84 24.13 17.43 5.86 Avg.dev.sent 19.50 16.44 23.84 17.17 6.05 Table 1: Corpora: number of words and sentences, number of possible tags, and average sentence length. Data We use a collection of 32 interlinear glossed texts (IGT) in the Mayan language Uspanteko. This corpus was cleaned up and adapted by Palmer et al. (2009) from an original collection of 67 texts that were collected, transcribed, translated and annotated by the OKMA language documentation project (Pixabaj et al., 2007). Two core tasks in creating IGT are morphological analysis and tagging morphemes with their glosses (labels indicating part-of-speech and/or grammatical function). We deal with the latter task and assume texts are morphologically segmented. Standard four-line IGT has morphemes on one line and their glosses on the next. The gloss line includes labels for grammatical morphemes (e.g. PL or COM) and translations of stems (e.g. hablar"
D09-1031,W09-1903,0,0.0461665,"Missing"
D09-1031,W06-2920,0,0.0525978,"Missing"
D09-1031,D08-1027,0,0.142097,"Missing"
D09-1031,E03-1071,0,0.0102236,"cost-sensitive selection, but our results—from live (non-simulated) active learning experiments of real-world scale— empirically support the need to consider costsensitive selection if better cost reductions are to be achieved. Classification model. We use a standard maximum entropy classifier for tagging Danish, Dutch, English, and Swedish words with POS-tags and tagging Uspanteko morphemes with Gloss/POS tags. The label for a word/morpheme is predicted based on the word/morpheme itself plus a window of two units before and after. Standard part-of-speech tagging features (Ratnaparkhi, 1998; Curran and Clark, 2003) are extracted from the morpheme to help with predicting labels for previously unseen morphemes. This is a strong but standard model; better, more complex models could be used, but the gains are likely to be small. Thus, we opted for simplicity in our model so as to focus more on the interaction between the annotator and different levels of machine involvement. The accuracy of the tagger on the datasets when trained on all available training material is given in the following table, along with accuracy of a unigram model (learned from the training set and constrained by a tag dictionary for kn"
D09-1031,W09-1906,0,0.0336635,"Missing"
D09-1031,W05-0619,0,0.0173153,"orpheme; clauses with the highest average entropy are selected for labeling. A recent development in active learning is cost298 Measuring annotation cost. Active learning studies usually simulate annotation and use a unit cost assumption that each word, sentence, constituent, document, etc. takes the same time to annotate. This is often the only option since corpora typically do not retain annotation time, but it is likely to exaggerate the annotation cost reductions achieved. This is exacerbated with active learning: the informative examples it seeks to find are typically harder to annotate (Hachey et al., 2005). Baldridge and Osborne (2008) correlate a unit cost in terms of discriminants (decisions made by annotators about valid parses) to annotation time. This is a better approximation than unit costs where such a relationship cannot be established. However, it is based on a static measurement of annotation time, and clearly the time taken to annotate an example is not a function of the example alone. Annotation time is actually dynamic in that it is dependent on how many and what kinds of examples have already been annotated. An “informative” example is likely to take longer to annotate if selecte"
D09-1031,J93-2004,0,0.0315661,"j inyolj iin M ORPH: kita’ t-in-ch’abe-j laj in-yol-j iin G LOSS: NEG INC-E1S-hablar-SC PREP A1S-idioma-SC yo POS: PART TAM-PERS-VT-SUF PREP PERS-S-SUF PRON T RANS: ‘No le hablo en mi idioma.’ We use a single layer that is a combination of the G LOSS and POS layers (Palmer et al., 2009). For (1), the morphemes and labels for our task are: (2) kita’ t- in- ch’abe -j laj in- yol -j iin NEG INC E1S VT SC PREP A1S S SC PRON We also consider POS-tagging for Danish, Dutch, English, and Swedish; the English is from sections 00-05 (as training set) and 19-21 (as development set) of the Penn Treebank (Marcus et al., 1993), and the other languages are from the CoNLL-X dependency parsing shared task (Buchholz and Marsi, 2006).1 We split the original training data into training and development sets. Table 1 shows the number of words and sentences in each split of each dataset, as well as the number of possible labels and the average sentence length. The Uspanteko data is counted in morphemes rather than words; also, the Uspanteko texts are divided at the clause rather than sentence level. This gives the corpus a much lower average clause length than the other languages (Table 1). 1 The subset of the Penn Treebank"
D09-1031,P00-1016,0,0.0183417,"alone. Annotation time is actually dynamic in that it is dependent on how many and what kinds of examples have already been annotated. An “informative” example is likely to take longer to annotate if selected early than it would after the annotator has seen many other examples. Thus, it is important to measure annotation time embedded in the context of a particular annotation experiment with the sample selection/labeling strategies of interest. In our annotation experiments, we measure the exact time taken to annotate each example by each annotator and use this as the cost metric, inspired by Ngai and Yarowsky (2000). In the simulation studies, as we are unable to measure time, we measure cost by sentence/clause and word/morpheme. fitted nonlinear regression models rather than averaging over a subset of data points. Specifically, we fit a modified Michaelis-Menton model: f (cost, (K, Vm , A)) = Vm (A + cost) K + cost The (original) parameters Vm and K respectively correspond to the horizontal asymptote and the cost where accuracy is halfway between 0 and Vm . The additional parameter A allows for a better fit to our data by allowing for less sharp elbows and letting cost be zero. Model parameters were det"
friedrich-etal-2014-lqvsumm,D08-1020,0,\N,Missing
friedrich-etal-2014-lqvsumm,P10-1056,0,\N,Missing
friedrich-etal-2014-lqvsumm,J08-1001,0,\N,Missing
friedrich-etal-2014-lqvsumm,P13-1010,0,\N,Missing
friedrich-etal-2014-lqvsumm,P12-1106,0,\N,Missing
friedrich-etal-2014-lqvsumm,P13-1026,0,\N,Missing
friedrich-etal-2014-lqvsumm,prasad-etal-2008-penn,0,\N,Missing
friedrich-etal-2014-lqvsumm,N13-1136,0,\N,Missing
friedrich-etal-2014-lqvsumm,W01-0100,0,\N,Missing
friedrich-etal-2014-lqvsumm,W11-1415,0,\N,Missing
friedrich-etal-2014-lqvsumm,W11-0416,0,\N,Missing
horbach-etal-2014-finding,P11-1076,0,\N,Missing
horbach-etal-2014-finding,W12-2022,0,\N,Missing
horbach-etal-2014-finding,W11-2401,0,\N,Missing
horbach-etal-2014-finding,Q13-1032,0,\N,Missing
horbach-etal-2014-finding,S13-1041,1,\N,Missing
horbach-etal-2014-finding,W05-0202,0,\N,Missing
I11-1021,S07-1018,0,0.028347,"ces required to reach a given performance level using supervised machine learning techniques. This is accomplished by allowing the learner to guide the selection of examples to be annotated and added to the training set; at each iteration the learner queries for the example (or set of examples) that will be most informative to its present state. AL is an attractive idea for natural language processing (NLP) because of its potential to dramatically reduce the 1 For recent work on SRL, see, among others: (Das et al., 2010; Hajiˇc et al., 2009; Surdeanu et al., 2008; Carreras and M`arquez, 2005; Baker et al., 2007). 183 Proceedings of the 5th International Joint Conference on Natural Language Processing, pages 183–191, c Chiang Mai, Thailand, November 8 – 13, 2011. 2011 AFNLP (1) compressed dependency trees encode the target predicate and the key dependents of the verb complex in a sentence. As illustrated in Section 3, the structural relationships defined by the compressed dependency trees well encapsulate key features used in automatic SRL. For a more complete picture of the potential for AL with respect to SRL, we investigate a set of strategies designed to select the most informative training exampl"
I11-1021,N04-1012,0,0.0730589,"Missing"
I11-1021,J05-1004,0,0.156725,"sentence involves identification and disambiguation of target predicates as well as identification and labeling of their arguments. Because our focus is on the active learning more so than on the semantic role labeling itself, we address only the argument labeling stage of the process, assuming that predicates and argument spans alike have already been identified and correctly labeled. Broadly speaking, there are two different styles of semantic parsing and semantic role labeling (SRL): those based on FrameNet-style analysis (Ruppenhofer et al., 2006) and those using PropBank-style analysis (Palmer et al., 2005). This work takes the PropBank approach, which considers only verbal predicates and is strongly tied to syntactic structure. In (1), for example, the two arguments of the predicate idolize are labeled as Arg0 and Arg1. 2 Note that logistic regression is used together with a regularized term to avoid the overfitting problem by penalizing the complexity of the trained model. Generally, the regularized term is defined as a function of the learned parameters over the weights. The L1 regularization, also called lasso penalty, is used to penalize both large and small weights. 3 In ongoing work, we r"
I11-1021,P05-1072,0,0.0237605,"y’, ’as a result of’) Active or passive LOC, TMP, etc. 1) Sbj*, obj* are defined as: Sbj* ← Obj Passive Sbj* ← LGS passive Sbj* ← Active vt sbj date not set . 3 5 7 8 Alternation if applicable Compressed Dependency Tree OBJ* P ADV date not set . 3 5 7 8 Figure 1: Producing compressed dependency tree Obj* ← Sbj Passive Obj* ← Sbj VI (intransitive verb) Obj* ← Obj Active VT = 1; transitive VI = 2; intransitive TO IM=3; begins with ’to’ V Adj = 4; verb followed by adjective words (e.g. ’sounds good’, ’looks pretty’) PV = 5; phrasal verb (e.g. ’pick up’) e.g. ”has not been set” in figure 1 2008b; Pradhan et al., 2005). At the same time, much of the structural and relational information represented in a dependency tree is not relevant for the SRL task. We use a compressed dependency tree (CDT) to encode just the relationships between a target predicate and the key dependents of the verb complex. The new tree is always rooted in the target predicate, which often means resetting the root from an auxiliary or other finite main verb. We generate the CDT from the output of an existing dependency parser through the process described in a simplified form below, using the example sentence in Fig. 1. adjectival comp"
I11-1021,J08-2006,0,0.0326744,"may be due to a conflict between the two selection criteria. In any event, there is clearly a trade-off between informativity and representativeness, and results are influenced by the details of the manner of combining the two. The results of other INF / REP combinations are presented in Table 2, in terms of their reduction in error compared to random selection. 6.3 7 Related Work Much research efforts have been devoted to statistical machine learning methodologies for SRL (Bjkelund et al., 2009; Gildea and Jurafsky, 2002; Shi et al., 2009; Johansson and Nugues, 2008a; Lang and Lapata, 2010; Pradhan et al., 2008; F¨urstenau and Lapata, 2009; Titov and Klementiev, 2011, among others). For example, Johansson et al. (Johansson and Nugues, 2008a) applied logistic regression with L2 norm to dependency-based SRL. Similarly, we also use logistic regression to train the classifier with a probabilistic explanation. However, we use L1 normed Weighting the two criteria Finally, we set α with different values (i.e., 0.3, 0.5 and 0.7) to investigate how the trade-off between informativity and representativeness may affect the SRL performance. We also compare our solution to the information density solution propos"
I11-1021,W07-1516,0,0.063528,"Missing"
I11-1021,D08-1112,0,0.558796,"pool P of unlabeled sentences, for every unlabeled sentence s ∈ P , the representativeness of the sentence, denoted as rep(s), is defined as nsimilar , representing the number of edges in the pool that are similar to the edges of the CDT for s. Intuitively, the larger the number of similar CDT edges in the unlabeled pool, the more representative the sentence is overall of the input data. Representativeness A disadvantage of selecting examples based only on informativity is the tendency of the learner to query outliers (Settles, 2010). It has therefore been proposed (Dredze and Crammer, 2008; Settles and Craven, 2008) to temper such selection strategies with a notion of relevance or representativeness. Ours is the first work to use such a combined strategy for SRL. We measure the representativeness of unlabeled sentences based on sentence similarity, taking two different approaches: cosine similarity, and a measure based on CDTs. COS: Cosine Similarity. Given two sentences s and s0 , let i1 , i2 , . . . , im , and i01 , i02 ,. . . ,i0n be their instances, respectively. The similarity of the two sentences, denoted as similarity(s, s0 ), is defined Pm P as j=1 nk=1 sim(ij , i0k ), where sim(ij , i0k ) is the"
I11-1021,D07-1002,0,0.0574311,"and experiment on SRL task. They defined structured output by constraining the relations among class labels, e.g., one predicate only has one of the labels. The classification problem is defined via constraints among output labels. The most uncertain instances are selected to satisfy predefined constraints. Rather than a structured relation between output labels, our work exploits the structure of the sentences themselves via compressed dependency trees. In the area of sentence similarity measurement, most current work focuses on semantic similarity (Haghighi et al., 2005; Tang et al., 2002; Shen and Lapata, 2007). We define similarity between sentences in terms of the nodes and edges in the dependency tree instead of semantic/lexical similarity of the sentences. We are interested in the structure of a sentence and how it is constructed due to the need of SRL tasks. Wang and Neumann (2007) use a similar sort of compressed dependency tree comprised of keywords and collapsed dependency relations to calculate the semantic similarity of sentences for the textual entailment task. Under their approach, dependency relations themselves are collapsed; we keep the specific dependency relations and collapse the t"
I11-1021,P04-1075,0,0.0687189,"Missing"
I11-1021,P02-1016,0,0.0398086,"r structured output and experiment on SRL task. They defined structured output by constraining the relations among class labels, e.g., one predicate only has one of the labels. The classification problem is defined via constraints among output labels. The most uncertain instances are selected to satisfy predefined constraints. Rather than a structured relation between output labels, our work exploits the structure of the sentences themselves via compressed dependency trees. In the area of sentence similarity measurement, most current work focuses on semantic similarity (Haghighi et al., 2005; Tang et al., 2002; Shen and Lapata, 2007). We define similarity between sentences in terms of the nodes and edges in the dependency tree instead of semantic/lexical similarity of the sentences. We are interested in the structure of a sentence and how it is constructed due to the need of SRL tasks. Wang and Neumann (2007) use a similar sort of compressed dependency tree comprised of keywords and collapsed dependency relations to calculate the semantic similarity of sentences for the textual entailment task. Under their approach, dependency relations themselves are collapsed; we keep the specific dependency rela"
I11-1021,P11-1145,0,0.0598375,"cally by training classifiers using large annotated corpora.1 Although such approaches can achieve reasonably good performance, annotating a large corpus is still expensive and time consuming. Moreover, the performance of trained classifiers may degrade remarkably when they are applied to out-of-domain data (Johansson and Nugues, 2008a). There is very little work on AL for SRL (e.g. Roth and Small (2006)), although much interesting work has been done with semi-supervised and unsupervised approaches to the problem (Grenager and Manning, 2006; F¨urstenau and Lapata, 2009; Lang and Lapata, 2010; Titov and Klementiev, 2011, among others). In this paper we explore the use of compressed dependency trees (CDTs) as features for supervised semantic role labeling and, most importantly, as a way of measuring how representative an individual instance is of the input data. We then incorporate representativeness as part of the metric used for sample selection in active learning. The This paper explores new approaches to active learning (AL) for semantic role labeling (SRL), focusing in particular on combining typical informativity-based sampling strategies with a novel measure of representativeness based on compressed de"
I11-1021,W09-1906,0,0.0134804,"dependency relations (e.g., subject, modifier). Thus, a dependency tree of a sentence encodes the dependency relation between the head words and their dependents. It has been reported that SRL can benefit from phrase-structure and dependency-based syntactic parsing (Hacioglu, 2004; Johansson and Nugues, 185 representativeness, or how well a training example represents the overall input patterns of the unlabeled data. While some results from AL are robust across different datasets and even different tasks, it is clear that there is no single approach to AL that is suitable for all situations (Tomanek and Olsson, 2009). Because there is very little previous work on AL for the task of semantic role labeling, we do not assume previous solutions but rather investigate a number of different strategies. 5. Heuristically determine voice of clause and alter some CDT dependency labels(e.g. S BJ PASSIVE becomes O BJ *); these are the asterisk-marked relations in Table. 1. For example, in (2): (2) At the same time, the government did not want to appear to favor GM by allowing a minority stake that might preclude a full bid by Ford. 4.2 the verb complex is {did, n’t, want, appear, favor}. The subject phrase the govern"
I11-1021,W07-1406,0,0.0316028,"tisfy predefined constraints. Rather than a structured relation between output labels, our work exploits the structure of the sentences themselves via compressed dependency trees. In the area of sentence similarity measurement, most current work focuses on semantic similarity (Haghighi et al., 2005; Tang et al., 2002; Shen and Lapata, 2007). We define similarity between sentences in terms of the nodes and edges in the dependency tree instead of semantic/lexical similarity of the sentences. We are interested in the structure of a sentence and how it is constructed due to the need of SRL tasks. Wang and Neumann (2007) use a similar sort of compressed dependency tree comprised of keywords and collapsed dependency relations to calculate the semantic similarity of sentences for the textual entailment task. Under their approach, dependency relations themselves are collapsed; we keep the specific dependency relations and collapse the trees, aiming for structural rather than semantic similarity. In addition, Filippova et al. (Filippova and Strube, 2008) proposed to compress a sentence using dependency trees and take the importance of words as weight. They found compressed dependency tree can better ensure the gr"
I11-1021,D09-1031,1,0.902092,"Missing"
I11-1021,W05-0620,0,0.0851586,"Missing"
I11-1021,N10-1138,0,0.0227454,"Missing"
I11-1021,de-marneffe-etal-2006-generating,0,0.0106804,"Missing"
I11-1021,P08-2059,0,0.0219528,"art-of-speech tag. Given a pool P of unlabeled sentences, for every unlabeled sentence s ∈ P , the representativeness of the sentence, denoted as rep(s), is defined as nsimilar , representing the number of edges in the pool that are similar to the edges of the CDT for s. Intuitively, the larger the number of similar CDT edges in the unlabeled pool, the more representative the sentence is overall of the input data. Representativeness A disadvantage of selecting examples based only on informativity is the tendency of the learner to query outliers (Settles, 2010). It has therefore been proposed (Dredze and Crammer, 2008; Settles and Craven, 2008) to temper such selection strategies with a notion of relevance or representativeness. Ours is the first work to use such a combined strategy for SRL. We measure the representativeness of unlabeled sentences based on sentence similarity, taking two different approaches: cosine similarity, and a measure based on CDTs. COS: Cosine Similarity. Given two sentences s and s0 , let i1 , i2 , . . . , im , and i01 , i02 ,. . . ,i0n be their instances, respectively. The similarity of the two sentences, denoted as similarity(s, s0 ), is defined Pm P as j=1 nk=1 sim(ij , i0k ),"
I11-1021,W08-1105,0,0.0289968,"ree instead of semantic/lexical similarity of the sentences. We are interested in the structure of a sentence and how it is constructed due to the need of SRL tasks. Wang and Neumann (2007) use a similar sort of compressed dependency tree comprised of keywords and collapsed dependency relations to calculate the semantic similarity of sentences for the textual entailment task. Under their approach, dependency relations themselves are collapsed; we keep the specific dependency relations and collapse the trees, aiming for structural rather than semantic similarity. In addition, Filippova et al. (Filippova and Strube, 2008) proposed to compress a sentence using dependency trees and take the importance of words as weight. They found compressed dependency tree can better ensure the grammaticality of the sentences to preserve the same lexical meaning as much as possible. In our work, we are more interested in the explicit dependency relation of predicate-argument pairs. Our goal is to apply compressed dependency tree to extract 8 Conclusions This paper investigates the use of active learning for semantic role labeling. To improve the learning accuracy and reduce the size of training set, compressed dependency trees"
I11-1021,E09-1026,0,0.0383099,"Missing"
I11-1021,J02-3001,0,0.0435082,"350, the solution of using INF 2 only achieves a higher accuracy than the combined solution. This may be due to a conflict between the two selection criteria. In any event, there is clearly a trade-off between informativity and representativeness, and results are influenced by the details of the manner of combining the two. The results of other INF / REP combinations are presented in Table 2, in terms of their reduction in error compared to random selection. 6.3 7 Related Work Much research efforts have been devoted to statistical machine learning methodologies for SRL (Bjkelund et al., 2009; Gildea and Jurafsky, 2002; Shi et al., 2009; Johansson and Nugues, 2008a; Lang and Lapata, 2010; Pradhan et al., 2008; F¨urstenau and Lapata, 2009; Titov and Klementiev, 2011, among others). For example, Johansson et al. (Johansson and Nugues, 2008a) applied logistic regression with L2 norm to dependency-based SRL. Similarly, we also use logistic regression to train the classifier with a probabilistic explanation. However, we use L1 normed Weighting the two criteria Finally, we set α with different values (i.e., 0.3, 0.5 and 0.7) to investigate how the trade-off between informativity and representativeness may affect"
I11-1021,W06-1601,0,0.204667,"Missing"
I11-1021,C04-1186,0,0.0326564,"sion process, we use output from the Stanford parser to complement the dependency relations found in the gold-standard data. Dependency Tree Compression Given a sentence, the task of dependency parsing is to identify the head word and its corresponding dependents and to classify their functional relationships according to a set of dependency relations (e.g., subject, modifier). Thus, a dependency tree of a sentence encodes the dependency relation between the head words and their dependents. It has been reported that SRL can benefit from phrase-structure and dependency-based syntactic parsing (Hacioglu, 2004; Johansson and Nugues, 185 representativeness, or how well a training example represents the overall input patterns of the unlabeled data. While some results from AL are robust across different datasets and even different tasks, it is clear that there is no single approach to AL that is suitable for all situations (Tomanek and Olsson, 2009). Because there is very little previous work on AL for the task of semantic role labeling, we do not assume previous solutions but rather investigate a number of different strategies. 5. Heuristically determine voice of clause and alter some CDT dependency"
I11-1021,H05-1049,0,0.0348322,"e learning framework for structured output and experiment on SRL task. They defined structured output by constraining the relations among class labels, e.g., one predicate only has one of the labels. The classification problem is defined via constraints among output labels. The most uncertain instances are selected to satisfy predefined constraints. Rather than a structured relation between output labels, our work exploits the structure of the sentences themselves via compressed dependency trees. In the area of sentence similarity measurement, most current work focuses on semantic similarity (Haghighi et al., 2005; Tang et al., 2002; Shen and Lapata, 2007). We define similarity between sentences in terms of the nodes and edges in the dependency tree instead of semantic/lexical similarity of the sentences. We are interested in the structure of a sentence and how it is constructed due to the need of SRL tasks. Wang and Neumann (2007) use a similar sort of compressed dependency tree comprised of keywords and collapsed dependency relations to calculate the semantic similarity of sentences for the textual entailment task. Under their approach, dependency relations themselves are collapsed; we keep the speci"
I11-1021,W09-1201,0,0.109155,"Missing"
I11-1021,J04-3001,0,0.0623597,"Missing"
I11-1021,D08-1008,0,0.284553,"annotated data and the expense of annotating new data are at least as relevant for semantic role labeling (SRL) as for the above-mentioned areas of NLP. Existing work on automatic SRL usually explores supervised machine learning approaches to mark the semantic roles of predicates automatically by training classifiers using large annotated corpora.1 Although such approaches can achieve reasonably good performance, annotating a large corpus is still expensive and time consuming. Moreover, the performance of trained classifiers may degrade remarkably when they are applied to out-of-domain data (Johansson and Nugues, 2008a). There is very little work on AL for SRL (e.g. Roth and Small (2006)), although much interesting work has been done with semi-supervised and unsupervised approaches to the problem (Grenager and Manning, 2006; F¨urstenau and Lapata, 2009; Lang and Lapata, 2010; Titov and Klementiev, 2011, among others). In this paper we explore the use of compressed dependency trees (CDTs) as features for supervised semantic role labeling and, most importantly, as a way of measuring how representative an individual instance is of the input data. We then incorporate representativeness as part of the metric us"
I11-1021,C08-1050,0,0.0873161,"annotated data and the expense of annotating new data are at least as relevant for semantic role labeling (SRL) as for the above-mentioned areas of NLP. Existing work on automatic SRL usually explores supervised machine learning approaches to mark the semantic roles of predicates automatically by training classifiers using large annotated corpora.1 Although such approaches can achieve reasonably good performance, annotating a large corpus is still expensive and time consuming. Moreover, the performance of trained classifiers may degrade remarkably when they are applied to out-of-domain data (Johansson and Nugues, 2008a). There is very little work on AL for SRL (e.g. Roth and Small (2006)), although much interesting work has been done with semi-supervised and unsupervised approaches to the problem (Grenager and Manning, 2006; F¨urstenau and Lapata, 2009; Lang and Lapata, 2010; Titov and Klementiev, 2011, among others). In this paper we explore the use of compressed dependency trees (CDTs) as features for supervised semantic role labeling and, most importantly, as a way of measuring how representative an individual instance is of the input data. We then incorporate representativeness as part of the metric us"
I11-1021,N10-1137,0,0.198237,"of predicates automatically by training classifiers using large annotated corpora.1 Although such approaches can achieve reasonably good performance, annotating a large corpus is still expensive and time consuming. Moreover, the performance of trained classifiers may degrade remarkably when they are applied to out-of-domain data (Johansson and Nugues, 2008a). There is very little work on AL for SRL (e.g. Roth and Small (2006)), although much interesting work has been done with semi-supervised and unsupervised approaches to the problem (Grenager and Manning, 2006; F¨urstenau and Lapata, 2009; Lang and Lapata, 2010; Titov and Klementiev, 2011, among others). In this paper we explore the use of compressed dependency trees (CDTs) as features for supervised semantic role labeling and, most importantly, as a way of measuring how representative an individual instance is of the input data. We then incorporate representativeness as part of the metric used for sample selection in active learning. The This paper explores new approaches to active learning (AL) for semantic role labeling (SRL), focusing in particular on combining typical informativity-based sampling strategies with a novel measure of representativ"
I11-1021,W08-2121,0,\N,Missing
I11-1021,N07-1070,0,\N,Missing
N18-2026,J11-4005,0,0.294218,"r example, if John Doe started his drive to work at 8:00am, it is reasonable to expect him to start working by 9:00am because commuting took him (most likely) between a few minutes to an hour. Related Work TimeBank (Pustejovsky et al., 2006) is the corpus of reference for temporal information. The annotations follow TimeML (Pustejovsky et al., 2010) and include events, temporal expressions (e.g., last Friday), temporal signals (e.g., when, during), and links encoding relations. TimeBank does not annotate the expected duration of events. Annotating and learning event durations was pioneered by Pan et al. (2011), who annotated the events in TimeBank with their expected durations. Gusev et al. (2011) use query patterns in an unsupervised approach to predict the duration of events. The work presented here builds upon these previous works: we introduce additional features and an LSTM ensemble that obtains the best results to date. The new features are inspired by previous work on assigning situation entity (SE) type labels to clauses (Friedrich et al., 2016). SE types are a linguistic categorization of semantic clause type, whereby each clause is labeled according to the type of situation it introduces"
N18-2026,D14-1162,0,0.0771133,"ct and object in the WordNet taxonomy countability from WebCelex of the subject and object number of modifiers in the sentence adverbial degree of the sentence whether the sentence contains an adverb flags indicating the Brown clusters present in the sentence Table 1: Feature set to predict the expected duration of events with SVM. Features 1–25 were previously proposed for the same task. Features 26–700 are inspired by previous work assigning situation entity types to clauses (2016). 3 Corpus with TensorFlow backend (Abadi et al., 2015). All networks use GloVe embeddings with 300 dimensions (Pennington et al., 2014) and the Adam optimizer (Kingma and Ba, 2014). We use grid search and 5-fold cross-validation to tune hyperparameters (C and γ for SVM, and batch size, dropout rate, etc. for neural networks). We use the corpus by Pan et al. (2011), who annotated the events in TimeBank (Pustejovsky et al., 2003) with their expected durations by specifying upper and lower bounds. The authors clustered these bounds into two labels: less than a day (<day) and a day or longer (≥day), and the corpus contains 2,354 events (<day: 958, ≥day: 1,396). The same event predicate may have different durations depending on co"
N18-2026,P14-2082,0,0.0637745,"extual understanding requires identifying events and temporal relations between them. Beyond event participants, a crucial piece of information regarding events is their duration, an attribute rarely mentioned explicitly. For example, taking a shower lasts a few minutes (not days), and a vacation lasts a few days (not years). Core tasks such as temporal understanding and reasoning, as well as applications such as temporal question answering (Llorens et al., 2015) would benefit from knowing the expected duration of events. Consider a system that extracts temporal relations such as IS INCLUDED (Cassidy et al., 2014, among others). When deciding whether a relation holds between an event and a temporal expression, such a system would benefit from knowing the duration of the event at hand. For example, argument y of IS INCLUDED(built a house, y) must be a temporal span ranging from a few weeks to a year—the expected duration of built a house. Thus relation candidates such as IS INCLUDED(built a house, 4/5/2016 ) could be discarded right away. Similarly, event durations combined with event ordering and temporal anchoring would help to determine the time of subsequent events. For example, if John Doe started"
N18-2026,pustejovsky-etal-2010-iso,0,0.0187439,"expected duration of built a house. Thus relation candidates such as IS INCLUDED(built a house, 4/5/2016 ) could be discarded right away. Similarly, event durations combined with event ordering and temporal anchoring would help to determine the time of subsequent events. For example, if John Doe started his drive to work at 8:00am, it is reasonable to expect him to start working by 9:00am because commuting took him (most likely) between a few minutes to an hour. Related Work TimeBank (Pustejovsky et al., 2006) is the corpus of reference for temporal information. The annotations follow TimeML (Pustejovsky et al., 2010) and include events, temporal expressions (e.g., last Friday), temporal signals (e.g., when, during), and links encoding relations. TimeBank does not annotate the expected duration of events. Annotating and learning event durations was pioneered by Pan et al. (2011), who annotated the events in TimeBank with their expected durations. Gusev et al. (2011) use query patterns in an unsupervised approach to predict the duration of events. The work presented here builds upon these previous works: we introduce additional features and an LSTM ensemble that obtains the best results to date. The new fea"
N18-2026,P16-1166,1,0.913302,"ring), and links encoding relations. TimeBank does not annotate the expected duration of events. Annotating and learning event durations was pioneered by Pan et al. (2011), who annotated the events in TimeBank with their expected durations. Gusev et al. (2011) use query patterns in an unsupervised approach to predict the duration of events. The work presented here builds upon these previous works: we introduce additional features and an LSTM ensemble that obtains the best results to date. The new features are inspired by previous work on assigning situation entity (SE) type labels to clauses (Friedrich et al., 2016). SE types are a linguistic categorization of semantic clause type, whereby each clause is labeled according to the type of situation it introduces to a discourse (STATE, EVENT, GENERIC, and GENERALIZING SENTENCE (also known as habituals)). Other related works include efforts modeling event durations in social media (Williams and Katz, 2012), and temporal anchoring of, among others, durative events (Reimers et al., 2016). 164 Proceedings of NAACL-HLT 2018, pages 164–168 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Pan et al. Gusev et al. Aspectual"
N18-2026,P16-1207,0,0.0173499,"eatures and an LSTM ensemble that obtains the best results to date. The new features are inspired by previous work on assigning situation entity (SE) type labels to clauses (Friedrich et al., 2016). SE types are a linguistic categorization of semantic clause type, whereby each clause is labeled according to the type of situation it introduces to a discourse (STATE, EVENT, GENERIC, and GENERALIZING SENTENCE (also known as habituals)). Other related works include efforts modeling event durations in social media (Williams and Katz, 2012), and temporal anchoring of, among others, durative events (Reimers et al., 2016). 164 Proceedings of NAACL-HLT 2018, pages 164–168 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Pan et al. Gusev et al. Aspectual features inspired by situation entities (Friedrich et al.) # 1-3 4-9 10-18 19-20 21 22-25 26 27 28 29 30 31 32-40 41-43 44-46 47 48 49 50 51-700 Description event token, lemma and POS tag head word, lemma and POS tags of the syntactic subject and object of the event three closest hypernyms of the event, subject and object named entity types of the syntactic subject and object of the event flag indicating if the event is"
N18-2026,W11-0116,0,0.579031,"him to start working by 9:00am because commuting took him (most likely) between a few minutes to an hour. Related Work TimeBank (Pustejovsky et al., 2006) is the corpus of reference for temporal information. The annotations follow TimeML (Pustejovsky et al., 2010) and include events, temporal expressions (e.g., last Friday), temporal signals (e.g., when, during), and links encoding relations. TimeBank does not annotate the expected duration of events. Annotating and learning event durations was pioneered by Pan et al. (2011), who annotated the events in TimeBank with their expected durations. Gusev et al. (2011) use query patterns in an unsupervised approach to predict the duration of events. The work presented here builds upon these previous works: we introduce additional features and an LSTM ensemble that obtains the best results to date. The new features are inspired by previous work on assigning situation entity (SE) type labels to clauses (Friedrich et al., 2016). SE types are a linguistic categorization of semantic clause type, whereby each clause is labeled according to the type of situation it introduces to a discourse (STATE, EVENT, GENERIC, and GENERALIZING SENTENCE (also known as habituals"
N18-2026,P10-1040,0,0.0084426,"the tokens after the event (top right). is episodic, habitual, or static. It is primarily these criteria which features 26-50 aim to capture. For example, bare plural subjects with a simple present tense verb (e.g., Bats eat mosquitos) are a hallmark of GENERIC clauses. Although situation entity types do not directly map onto the duration labels (<day or ≥day), the criteria which contribute to determining them clearly influence aspectual interpretation, thus influencing understanding of the duration of events. Regarding Brown clusters, we use freely available clusters trained on news data by Turian et al. (2010) using the implementation by Liang (2005). We include one feature per cluster and set it to true if any word in the sentence belongs to the cluster. 4.2 Pan et al. Pan et al. + Gusev et al. Pan et al. + Gusev et al. + Situation Entities Feed-forward neural network LSTM ensemble Feed-Forward Neural Network The first neural network we experiment with is a one-hidden-layer feed-forward neural network that takes as input the event embedding. The tuning process revealed that the size of the hidden layer is not important, thus we report results using a hidden layer with 5 neurons. Intuitively, this"
N18-2026,S15-2134,0,0.159294,"and temporal structure of the clause provide useful clues, and that an LSTM ensemble captures relevant context around the event. 1 Introduction 2 Robust textual understanding requires identifying events and temporal relations between them. Beyond event participants, a crucial piece of information regarding events is their duration, an attribute rarely mentioned explicitly. For example, taking a shower lasts a few minutes (not days), and a vacation lasts a few days (not years). Core tasks such as temporal understanding and reasoning, as well as applications such as temporal question answering (Llorens et al., 2015) would benefit from knowing the expected duration of events. Consider a system that extracts temporal relations such as IS INCLUDED (Cassidy et al., 2014, among others). When deciding whether a relation holds between an event and a temporal expression, such a system would benefit from knowing the duration of the event at hand. For example, argument y of IS INCLUDED(built a house, y) must be a temporal span ranging from a few weeks to a year—the expected duration of built a house. Thus relation candidates such as IS INCLUDED(built a house, 4/5/2016 ) could be discarded right away. Similarly, ev"
N18-2026,P12-2044,0,0.249696,"The work presented here builds upon these previous works: we introduce additional features and an LSTM ensemble that obtains the best results to date. The new features are inspired by previous work on assigning situation entity (SE) type labels to clauses (Friedrich et al., 2016). SE types are a linguistic categorization of semantic clause type, whereby each clause is labeled according to the type of situation it introduces to a discourse (STATE, EVENT, GENERIC, and GENERALIZING SENTENCE (also known as habituals)). Other related works include efforts modeling event durations in social media (Williams and Katz, 2012), and temporal anchoring of, among others, durative events (Reimers et al., 2016). 164 Proceedings of NAACL-HLT 2018, pages 164–168 c New Orleans, Louisiana, June 1 - 6, 2018. 2018 Association for Computational Linguistics Pan et al. Gusev et al. Aspectual features inspired by situation entities (Friedrich et al.) # 1-3 4-9 10-18 19-20 21 22-25 26 27 28 29 30 31 32-40 41-43 44-46 47 48 49 50 51-700 Description event token, lemma and POS tag head word, lemma and POS tags of the syntactic subject and object of the event three closest hypernyms of the event, subject and object named entity types"
P07-1113,P04-1014,0,0.0154558,"L WT F ORCE P RED G EN P RED H AS F IN H AS M ODAL F REQ A DV M ODAL A DV VOL A DV F IRST V B WTLG WTL V ERBS V ERBTAGS M AIN V B S UBJ S UPER (see above) all verbs in clause POS tags for all verbs main verb of clause subject of clause (lexical item) CCG supertag FACT P RED 4.2 Table 2: Feature sets for SE classification 3.3 Preprocessing The linguistic tests for SE classification appeal to multiple levels of linguistic information; there are lexical, morphological, syntactic, categorial, and structural tests. In order to access categorial and structural information, we used the C&C2 toolkit (Clark and Curran, 2004). It provides part-of-speech tags and Combinatory Categorial Grammar (CCG) (Steedman, 2000) categories for words and syntactic dependencies across words. 4 Features One of our goals in undertaking this study was to explore the use of linguistically-motivated features and deep syntactic features in probabilistic models for SE classification. The nature of the task requires features characterizing the entire clause. Here, we describe our four feature sets, summarized in table 2. The feature sets are additive, extending very basic feature sets first with linguistically-motivated features and then"
P07-1113,C92-4177,0,0.309751,"Missing"
P07-1113,J01-3003,0,0.0128459,"ristics. Linguistic indicators for aspectual classification are also used by Siegel (1999), who evaluates 14 indicators to test verbs for stativity and telicity. Many of his indicators overlap with our features. Siegel and McKeown (2001) address classification of verbs for stativity (event vs. state) and for completedness (culminated vs. non-culminated events). They compare three supervised and one unsupervised machine learning systems. The systems obtain relatively high accuracy figures, but they are domain-specific, require extensive human supervision, and do not address aspectual coercion. Merlo and Stevenson (2001) use corpus-based thematic role information to identify and classify unergative, unaccusative, and object-drop verbs. Stevenson and Merlo note that statistical analysis cannot and should not be separated from deeper linguistic analysis, and our results support that claim. The advantages of our approach are the broadened conception of the classification task and the use of sequence prediction to capture a wider context. 8 References N. Asher. 1993. Reference to Abstract objects in Discourse. Kluwer Academic Publishers. A. Berger, S. Della Pietra, and V. Della Pietra. 1996. A maximum entropy app"
P07-1113,J88-2003,0,0.539159,"lson and Pelletier, 1995). Consider the text passage below, which introduces an event-type entity in (1), a report-type entity in (2), and a statetype entity in (3). (1) Sony Corp. has heavily promoted the Video Walkman since the product’s introduction last summer , (2) but Bob Gerson , video editor of This Week in Consumer Electronics , says (3) Sony conceives of 8mm as a “family of products , camcorders and VCR decks , ” SE classification is a fundamental component in determining the discourse mode of texts (Smith, 2003) and, along with aspectual classification, for temporal interpretation (Moens and Steedman, 1988). It may be useful for discourse relation projection and discourse parsing. Though situation entities are well-studied in linguistics, they have received very little computational treatment. This paper presents the first data-driven models for SE classification. Our two main strategies are (a) the use of linguistically-motivated features and (b) the implementation of SE classification as a sequencing task. Our results also provide empirical support for the very notion of discourse modes, as we see clear genre effects in SE classification. We begin by discussing SEs in more detail. Section 3 de"
P07-1113,P99-1015,0,0.541871,"he effect of mixed domain training. 7 Related work Though we are aware of no previous work in SE classification, others have focused on automatic detection of aspectual and temporal data. Klavans and Chodorow (1992) laid the foundation for probabilistic verb classification with their interpretation of aspectual properties as gradient and their use of statistics to model the gradience. They implement a single linguistic test for stativity, treating lexical properties of verbs as tendencies rather than absolute characteristics. Linguistic indicators for aspectual classification are also used by Siegel (1999), who evaluates 14 indicators to test verbs for stativity and telicity. Many of his indicators overlap with our features. Siegel and McKeown (2001) address classification of verbs for stativity (event vs. state) and for completedness (culminated vs. non-culminated events). They compare three supervised and one unsupervised machine learning systems. The systems obtain relatively high accuracy figures, but they are domain-specific, require extensive human supervision, and do not address aspectual coercion. Merlo and Stevenson (2001) use corpus-based thematic role information to identify and clas"
P07-1113,J96-1002,0,\N,Missing
P07-1113,J00-4004,0,\N,Missing
P14-2085,J86-2003,0,0.048043,"Missing"
P14-2085,W08-1202,0,0.0177872,"here they disagree between DYNAMIC and STATIVE . Such differences in annotation prefer1 Direct comparison on their data is not possible; feature values for the verbs studied are available, but full texts and the English Slot Grammar parser (McCord, 1990) are not. 2 Corpus freely available from www.coli.uni-saarland.de/˜afried. 518 DYNAMIC DYNAMIC STATIVE BOTH 1444 168 44 STATIVE 201 697 31 F EATURE frequency present past future perfect progressive negated particle no subject BOTH 54 20 8 Table 3: Asp-Ambig: confusion matrix for two annotators. Cohen’s κ is 0.6. ences are not uncommon (Beigman Klebanov et al., 2008). We observe higher agreement in the jokes and news subcorpora than for letters; texts in the letters subcorpora are largely argumentative and thus have a different rhetorical style than the more straightforward narratives and reports found in jokes. Overall, we find substantial agreement. The data for our experiments uses the label DYNAMIC or STATIVE whenever annotators agree, and BOTH whenever they disagree or when at least one annotator marked the clause as BOTH, assuming that both readings are possible in such cases. Because we don’t want to model the authors’ personal view of the theory,"
P14-2085,P10-2013,0,0.0739,"perform experiments on a set of 20 verbs. To the best of our knowledge, there is no previous work comprehensively addressing aspectual classification of verbs in context. 3 Data Verb type seed sets Using the LCS Database (Dorr, 2001), we identify sets of verb types whose senses are only stative (188 verbs, e.g. belong, cost, possess), only dynamic (3760 verbs, e.g. alter, knock, resign), or mixed (215 verbs, e.g. fill, stand, take), following a procedure described by Dorr and Olsen (1997). Asp-MASC The Asp-MASC corpus consists of 7875 clauses from the letters, news and jokes sections of MASC (Ide et al., 2010), each labeled by two annotators for the aspectual class of the main verb.2 Texts were segmented into clauses using SPADE (Soricut and Marcu, 2003) with some heuristic post-processing. We parse the corpus using the Stanford dependency parser (De Marneffe et al., 2006) and extract the main verb of each segment. We use 6161 clauses for the classification task, omitting clauses with have or be as the main verb and those where no main verb could be identified due to parsing errors (none). Table 1 shows inter-annotator agreement; Table 2 shows the confusion matrix for the two annotators. Our two an"
P14-2085,S13-2002,0,0.0291162,"es, achievements, accomplishments). Early studies on the computational modeling of aspectual class (Nakhimovsky, 1988; Passonneau, 1988; Brent, 1991; Klavans and Chodorow, 1992) laid foundations for a cluster of papers published over a decade ago (Siegel and McKeown, 2000; Siegel, 1998b; Siegel, 1998a). Since then, it has mostly been treated as a subtask within temporal reasoning, such as in efforts related to TimeBank (Pustejovsky et al., 2003) and the TempEval challenges (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), where top-performing systems (Jung and Stent, 2013; Bethard, 2013; Chambers, 2013) use corpus-based features, WordNet synsets, parse paths and features from typed dependencies to classify events as a joint task with determining the event’s span. Costa and Branco (2012) explore the usefulness of a wider range of explicitly aspectual features for temporal relation classification. Siegel and McKeown (2000) present the most extensive study of predicting aspectual class, which is the main inspiration for this work. While all of their linguistically motivated features (see section 4.1) are type-based, they train on and evaluate over labeled verbs in context. Thei"
P14-2085,S13-2004,0,0.0190302,"hree classes (activities, achievements, accomplishments). Early studies on the computational modeling of aspectual class (Nakhimovsky, 1988; Passonneau, 1988; Brent, 1991; Klavans and Chodorow, 1992) laid foundations for a cluster of papers published over a decade ago (Siegel and McKeown, 2000; Siegel, 1998b; Siegel, 1998a). Since then, it has mostly been treated as a subtask within temporal reasoning, such as in efforts related to TimeBank (Pustejovsky et al., 2003) and the TempEval challenges (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), where top-performing systems (Jung and Stent, 2013; Bethard, 2013; Chambers, 2013) use corpus-based features, WordNet synsets, parse paths and features from typed dependencies to classify events as a joint task with determining the event’s span. Costa and Branco (2012) explore the usefulness of a wider range of explicitly aspectual features for temporal relation classification. Siegel and McKeown (2000) present the most extensive study of predicting aspectual class, which is the main inspiration for this work. While all of their linguistically motivated features (see section 4.1) are type-based, they train on and evaluate over labeled verbs i"
P14-2085,E91-1039,0,0.402657,"Linguistics 2 Related work genre jokes letters news all Aspectual class is well treated in the linguistic literature (Vendler, 1957; Dowty, 1979; Smith, 1991, for example). Our notion of the stative/dynamic distinction corresponds to Bach’s (1986) distinction between states and non-states; to states versus occurrences (events and processes) according to Mourelatos (1978); and to Vendler’s (1957) distinction between states and the other three classes (activities, achievements, accomplishments). Early studies on the computational modeling of aspectual class (Nakhimovsky, 1988; Passonneau, 1988; Brent, 1991; Klavans and Chodorow, 1992) laid foundations for a cluster of papers published over a decade ago (Siegel and McKeown, 2000; Siegel, 1998b; Siegel, 1998a). Since then, it has mostly been treated as a subtask within temporal reasoning, such as in efforts related to TimeBank (Pustejovsky et al., 2003) and the TempEval challenges (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), where top-performing systems (Jung and Stent, 2013; Bethard, 2013; Chambers, 2013) use corpus-based features, WordNet synsets, parse paths and features from typed dependencies to classify events as a"
P14-2085,C92-4177,0,0.900794,"predominant aspectual class for unseen verb types. Our work differs from prior work in that we treat the problem as a three-way classification task, predicting DYNAMIC, STATIVE or BOTH as the aspectual class of a verb in context. Introduction In this work, we focus on the automatic prediction of whether a verb in context is used in a stative or in a dynamic sense, the most fundamental distinction in all taxonomies of aspectual class. The aspectual class of a discourse’s finite verbs is an important factor in conveying and interpreting temporal structure (Moens and Steedman, 1988; Dorr, 1992; Klavans and Chodorow, 1992); others are tense, grammatical aspect, mood and whether the utterance represents an event as completed. More accurate temporal information processing is expected to be beneficial for a variety of natural language processing tasks (Costa and Branco, 2012; UzZaman et al., 2013). While most verbs have one predominant interpretation, others are more flexible for aspectual class and can occur as either stative (1) or dynamic (2) depending on the context. There are also cases that allow for both readings, such as (3). 517 Proceedings of the 52nd Annual Meeting of the Association for Computational L"
P14-2085,S13-2012,0,0.0153738,"s, accomplishments). Early studies on the computational modeling of aspectual class (Nakhimovsky, 1988; Passonneau, 1988; Brent, 1991; Klavans and Chodorow, 1992) laid foundations for a cluster of papers published over a decade ago (Siegel and McKeown, 2000; Siegel, 1998b; Siegel, 1998a). Since then, it has mostly been treated as a subtask within temporal reasoning, such as in efforts related to TimeBank (Pustejovsky et al., 2003) and the TempEval challenges (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), where top-performing systems (Jung and Stent, 2013; Bethard, 2013; Chambers, 2013) use corpus-based features, WordNet synsets, parse paths and features from typed dependencies to classify events as a joint task with determining the event’s span. Costa and Branco (2012) explore the usefulness of a wider range of explicitly aspectual features for temporal relation classification. Siegel and McKeown (2000) present the most extensive study of predicting aspectual class, which is the main inspiration for this work. While all of their linguistically motivated features (see section 4.1) are type-based, they train on and evaluate over labeled verbs in context. Their data set taken"
P14-2085,loaiciga-etal-2014-english,0,0.121295,"Missing"
P14-2085,E12-1027,0,0.39122,"e focus on the automatic prediction of whether a verb in context is used in a stative or in a dynamic sense, the most fundamental distinction in all taxonomies of aspectual class. The aspectual class of a discourse’s finite verbs is an important factor in conveying and interpreting temporal structure (Moens and Steedman, 1988; Dorr, 1992; Klavans and Chodorow, 1992); others are tense, grammatical aspect, mood and whether the utterance represents an event as completed. More accurate temporal information processing is expected to be beneficial for a variety of natural language processing tasks (Costa and Branco, 2012; UzZaman et al., 2013). While most verbs have one predominant interpretation, others are more flexible for aspectual class and can occur as either stative (1) or dynamic (2) depending on the context. There are also cases that allow for both readings, such as (3). 517 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 517–523, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Related work genre jokes letters news all Aspectual class is well treated in the linguistic literature (Vendler, 1957;"
P14-2085,J88-2003,0,0.92134,"presentative verbs, accurately predict predominant aspectual class for unseen verb types. Our work differs from prior work in that we treat the problem as a three-way classification task, predicting DYNAMIC, STATIVE or BOTH as the aspectual class of a verb in context. Introduction In this work, we focus on the automatic prediction of whether a verb in context is used in a stative or in a dynamic sense, the most fundamental distinction in all taxonomies of aspectual class. The aspectual class of a discourse’s finite verbs is an important factor in conveying and interpreting temporal structure (Moens and Steedman, 1988; Dorr, 1992; Klavans and Chodorow, 1992); others are tense, grammatical aspect, mood and whether the utterance represents an event as completed. More accurate temporal information processing is expected to be beneficial for a variety of natural language processing tasks (Costa and Branco, 2012; UzZaman et al., 2013). While most verbs have one predominant interpretation, others are more flexible for aspectual class and can occur as either stative (1) or dynamic (2) depending on the context. There are also cases that allow for both readings, such as (3). 517 Proceedings of the 52nd Annual Meeti"
P14-2085,de-marneffe-etal-2006-generating,0,0.0435324,"Missing"
P14-2085,P97-1020,0,0.367057,"0.7. spired by this work, our present work goes one step further and uses a larger set of instance-based contextual features to perform experiments on a set of 20 verbs. To the best of our knowledge, there is no previous work comprehensively addressing aspectual classification of verbs in context. 3 Data Verb type seed sets Using the LCS Database (Dorr, 2001), we identify sets of verb types whose senses are only stative (188 verbs, e.g. belong, cost, possess), only dynamic (3760 verbs, e.g. alter, knock, resign), or mixed (215 verbs, e.g. fill, stand, take), following a procedure described by Dorr and Olsen (1997). Asp-MASC The Asp-MASC corpus consists of 7875 clauses from the letters, news and jokes sections of MASC (Ide et al., 2010), each labeled by two annotators for the aspectual class of the main verb.2 Texts were segmented into clauses using SPADE (Soricut and Marcu, 2003) with some heuristic post-processing. We parse the corpus using the Stanford dependency parser (De Marneffe et al., 2006) and extract the main verb of each segment. We use 6161 clauses for the classification task, omitting clauses with have or be as the main verb and those where no main verb could be identified due to parsing e"
P14-2085,J88-2004,0,0.543443,". 2014 Association for Computational Linguistics 2 Related work genre jokes letters news all Aspectual class is well treated in the linguistic literature (Vendler, 1957; Dowty, 1979; Smith, 1991, for example). Our notion of the stative/dynamic distinction corresponds to Bach’s (1986) distinction between states and non-states; to states versus occurrences (events and processes) according to Mourelatos (1978); and to Vendler’s (1957) distinction between states and the other three classes (activities, achievements, accomplishments). Early studies on the computational modeling of aspectual class (Nakhimovsky, 1988; Passonneau, 1988; Brent, 1991; Klavans and Chodorow, 1992) laid foundations for a cluster of papers published over a decade ago (Siegel and McKeown, 2000; Siegel, 1998b; Siegel, 1998a). Since then, it has mostly been treated as a subtask within temporal reasoning, such as in efforts related to TimeBank (Pustejovsky et al., 2003) and the TempEval challenges (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), where top-performing systems (Jung and Stent, 2013; Bethard, 2013; Chambers, 2013) use corpus-based features, WordNet synsets, parse paths and features from typed depend"
P14-2085,J88-2005,0,0.846639,"for Computational Linguistics 2 Related work genre jokes letters news all Aspectual class is well treated in the linguistic literature (Vendler, 1957; Dowty, 1979; Smith, 1991, for example). Our notion of the stative/dynamic distinction corresponds to Bach’s (1986) distinction between states and non-states; to states versus occurrences (events and processes) according to Mourelatos (1978); and to Vendler’s (1957) distinction between states and the other three classes (activities, achievements, accomplishments). Early studies on the computational modeling of aspectual class (Nakhimovsky, 1988; Passonneau, 1988; Brent, 1991; Klavans and Chodorow, 1992) laid foundations for a cluster of papers published over a decade ago (Siegel and McKeown, 2000; Siegel, 1998b; Siegel, 1998a). Since then, it has mostly been treated as a subtask within temporal reasoning, such as in efforts related to TimeBank (Pustejovsky et al., 2003) and the TempEval challenges (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), where top-performing systems (Jung and Stent, 2013; Bethard, 2013; Chambers, 2013) use corpus-based features, WordNet synsets, parse paths and features from typed dependencies to classify"
P14-2085,J00-4004,0,0.655046,"ven for unseen verbs. Many frequent verbs can be either stative or dynamic in different contexts, which has not been modeled by previous work; we use contextual features to resolve this ambiguity. In addition, we introduce two new datasets of clauses marked for aspectual class. 1 (2) The pool slowly filled with water. (dynamic) (3) Your soul was made to be filled with God Himself. (both) (Brown corpus, religion) Cases like (3) do not imply that there is a third class, but rather that two interpretations are available for the sentence, of which usually one will be chosen by a reader. Following Siegel and McKeown (2000), we aim to automatically classify clauses for fundamental aspectual class, a function of the main verb and a select group of complements, which may differ per verb (Siegel and McKeown, 2000; Siegel, 1998b). This corresponds to the aspectual class of the clause’s main verb when ignoring any aspectual markers or transformations. For example, English sentences with perfect tense are usually considered to introduce states to the discourse (Smith, 1991; Katz, 2003), but we are interested in the aspectual class before this transformation takes place. The clause John has kissed Mary introduces a sta"
P14-2085,W98-0702,0,0.86095,"oduce two new datasets of clauses marked for aspectual class. 1 (2) The pool slowly filled with water. (dynamic) (3) Your soul was made to be filled with God Himself. (both) (Brown corpus, religion) Cases like (3) do not imply that there is a third class, but rather that two interpretations are available for the sentence, of which usually one will be chosen by a reader. Following Siegel and McKeown (2000), we aim to automatically classify clauses for fundamental aspectual class, a function of the main verb and a select group of complements, which may differ per verb (Siegel and McKeown, 2000; Siegel, 1998b). This corresponds to the aspectual class of the clause’s main verb when ignoring any aspectual markers or transformations. For example, English sentences with perfect tense are usually considered to introduce states to the discourse (Smith, 1991; Katz, 2003), but we are interested in the aspectual class before this transformation takes place. The clause John has kissed Mary introduces a state, but the fundamental aspectual class of the ‘tenseless’ clause John kiss Mary is dynamic. In contrast to Siegel and McKeown (2000), we do not conduct the task of predicting aspectual class solely at th"
P14-2085,N03-1030,0,0.144269,"ification of verbs in context. 3 Data Verb type seed sets Using the LCS Database (Dorr, 2001), we identify sets of verb types whose senses are only stative (188 verbs, e.g. belong, cost, possess), only dynamic (3760 verbs, e.g. alter, knock, resign), or mixed (215 verbs, e.g. fill, stand, take), following a procedure described by Dorr and Olsen (1997). Asp-MASC The Asp-MASC corpus consists of 7875 clauses from the letters, news and jokes sections of MASC (Ide et al., 2010), each labeled by two annotators for the aspectual class of the main verb.2 Texts were segmented into clauses using SPADE (Soricut and Marcu, 2003) with some heuristic post-processing. We parse the corpus using the Stanford dependency parser (De Marneffe et al., 2006) and extract the main verb of each segment. We use 6161 clauses for the classification task, omitting clauses with have or be as the main verb and those where no main verb could be identified due to parsing errors (none). Table 1 shows inter-annotator agreement; Table 2 shows the confusion matrix for the two annotators. Our two annotators exhibit different preferences on the 598 cases where they disagree between DYNAMIC and STATIVE . Such differences in annotation prefer1 Di"
P14-2085,I11-1127,0,0.0611416,"Missing"
P14-2085,S13-2001,0,0.169176,"prediction of whether a verb in context is used in a stative or in a dynamic sense, the most fundamental distinction in all taxonomies of aspectual class. The aspectual class of a discourse’s finite verbs is an important factor in conveying and interpreting temporal structure (Moens and Steedman, 1988; Dorr, 1992; Klavans and Chodorow, 1992); others are tense, grammatical aspect, mood and whether the utterance represents an event as completed. More accurate temporal information processing is expected to be beneficial for a variety of natural language processing tasks (Costa and Branco, 2012; UzZaman et al., 2013). While most verbs have one predominant interpretation, others are more flexible for aspectual class and can occur as either stative (1) or dynamic (2) depending on the context. There are also cases that allow for both readings, such as (3). 517 Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics (Short Papers), pages 517–523, c Baltimore, Maryland, USA, June 23-25 2014. 2014 Association for Computational Linguistics 2 Related work genre jokes letters news all Aspectual class is well treated in the linguistic literature (Vendler, 1957; Dowty, 1979; Smith, 19"
P14-2085,S07-1014,0,0.0339386,"according to Mourelatos (1978); and to Vendler’s (1957) distinction between states and the other three classes (activities, achievements, accomplishments). Early studies on the computational modeling of aspectual class (Nakhimovsky, 1988; Passonneau, 1988; Brent, 1991; Klavans and Chodorow, 1992) laid foundations for a cluster of papers published over a decade ago (Siegel and McKeown, 2000; Siegel, 1998b; Siegel, 1998a). Since then, it has mostly been treated as a subtask within temporal reasoning, such as in efforts related to TimeBank (Pustejovsky et al., 2003) and the TempEval challenges (Verhagen et al., 2007; Verhagen et al., 2010; UzZaman et al., 2013), where top-performing systems (Jung and Stent, 2013; Bethard, 2013; Chambers, 2013) use corpus-based features, WordNet synsets, parse paths and features from typed dependencies to classify events as a joint task with determining the event’s span. Costa and Branco (2012) explore the usefulness of a wider range of explicitly aspectual features for temporal relation classification. Siegel and McKeown (2000) present the most extensive study of predicting aspectual class, which is the main inspiration for this work. While all of their linguistically mo"
P14-2085,W91-0222,0,\N,Missing
P14-2085,S10-1010,0,\N,Missing
P16-1166,J92-4003,0,0.549144,"including tokenization, POS tagging (Toutanova et al., 2003) and dependency parsing (Klein and Manning, 2002) using the UIMA-based DKPro framework (Ferrucci and Lally, 2004; Eckart de Castilho and Gurevych, 2014). A-pos: part-of-speech tags. These features count how often each POS tag occurs in a clause. A-bc: Brown cluster features. UT07 relies mostly on words and word/POS tag pairs. These simple features work well on the small Brown data set, but the approach quickly becomes impractical with increasing corpus size. We instead turn to distributional information in the form of Brown clusters (Brown et al., 1992), which can be learned from raw text and represent word classes in a hierarchical way. Originally developed in the context of n-gram language modeling, they aim to assign words to classes such that the average mutual information of the words in the clusters is maximized. We use existing, freely-available clusters trained on news data by Turian et al. (2010) using the implementation by Liang (2005).2 Clusterings with 320 and 1000 Brown clusters work best for our task. We use one feature per cluster, counting how often a word in the clause was assigned to this cluster (0 for most clusters). B-mv"
P16-1166,E12-1027,0,0.432546,"Missing"
P16-1166,W14-5201,0,0.0746748,"Missing"
P16-1166,W14-4921,1,0.935726,"s expanded by Palmer et al. (2007) to include three additional types: R EPORT, Q UES TION and I MPERATIVE. The latter two categories were added to accommodate exhaustive annota1 Corpora, annotation manual and code available at www.coli.uni-saarland.de/projects/sitent FACT: Objects of knowledge. I know that Mary refused the offer. P ROPOSITION: Objects of belief. I believe that Mary refused the offer. Figure 2: Abstract Entity SE types. tion of text; R EPORT is a subtype of event for attributions of quoted speech. Two parts of a clause provide important information for determining the SE type (Friedrich and Palmer, 2014b): a clause’s main verb and its main referent. The latter is loosely defined as the main entity that the segment is about; in English this is usually the subject. For example, main referents of G ENERIC S ENTENCEs are kinds or classes as in “Elephants are huge”, while the main referents of Eventualities and G ENERALIZ ING S ENTENCEs are particular individuals (“John is short”). For English, the main verb is the non-auxiliary verb ranked highest in the dependency parse (e.g. “kiss” in “John has kissed Joe”). S TATEs and E VENTs differ in the fundamental lexical aspectual class (Siegel and McKe"
P16-1166,W15-1603,1,0.903298,"Missing"
P16-1166,D15-1294,1,0.85804,"ical features adopted from theoretical work on aspectual classification. The model is described in Section 6.1. Another related body of work has to do with determining event class as a precursor to temporal relation classification. The inventory of event classes, described in detail in the TimeML annotation guidelines (Saur´ı et al., 2006), combines semantic (REPORTING, PERCEPTION), aspectual (ASPECTUAL, STATE, OCCURRENCE), and intensional (I ACTION, I STATE) properties of events. Finally, there are close connections to systems which predict genericity of noun phrases (Reiter and Frank, 2010; Friedrich and Pinkal, 2015a), and habituality of clauses (Mathew and Katz, 2009; Friedrich and Pinkal, 2015b). 4 Data sets The experiments presented in this paper make use of two data sets labeled with SE types. Brown data. This data set consists of 20 texts from the popular lore section of the Brown corpus (Francis and Kuˇcera, 1979), manually segmented into 4391 clauses and marked by two annotators in corpus MASC Wikipedia tokens 357078 148040 SEs 30333 10607 Fleiss’ κ 0.69 0.66 Table 1: SE-labeled corpora: size and agreement. SE type S TATE E VENT R EPORT G ENERIC G ENERALIZING Q UESTION I MPERATIVE undecided MASC 4"
P16-1166,P15-1123,1,0.920896,"ical features adopted from theoretical work on aspectual classification. The model is described in Section 6.1. Another related body of work has to do with determining event class as a precursor to temporal relation classification. The inventory of event classes, described in detail in the TimeML annotation guidelines (Saur´ı et al., 2006), combines semantic (REPORTING, PERCEPTION), aspectual (ASPECTUAL, STATE, OCCURRENCE), and intensional (I ACTION, I STATE) properties of events. Finally, there are close connections to systems which predict genericity of noun phrases (Reiter and Frank, 2010; Friedrich and Pinkal, 2015a), and habituality of clauses (Mathew and Katz, 2009; Friedrich and Pinkal, 2015b). 4 Data sets The experiments presented in this paper make use of two data sets labeled with SE types. Brown data. This data set consists of 20 texts from the popular lore section of the Brown corpus (Francis and Kuˇcera, 1979), manually segmented into 4391 clauses and marked by two annotators in corpus MASC Wikipedia tokens 357078 148040 SEs 30333 10607 Fleiss’ κ 0.69 0.66 Table 1: SE-labeled corpora: size and agreement. SE type S TATE E VENT R EPORT G ENERIC G ENERALIZING Q UESTION I MPERATIVE undecided MASC 4"
P16-1166,ide-etal-2008-masc,0,0.230728,"Missing"
P16-1166,P10-2013,0,0.34781,"Missing"
P16-1166,P14-2085,1,0.775096,"s expanded by Palmer et al. (2007) to include three additional types: R EPORT, Q UES TION and I MPERATIVE. The latter two categories were added to accommodate exhaustive annota1 Corpora, annotation manual and code available at www.coli.uni-saarland.de/projects/sitent FACT: Objects of knowledge. I know that Mary refused the offer. P ROPOSITION: Objects of belief. I believe that Mary refused the offer. Figure 2: Abstract Entity SE types. tion of text; R EPORT is a subtype of event for attributions of quoted speech. Two parts of a clause provide important information for determining the SE type (Friedrich and Palmer, 2014b): a clause’s main verb and its main referent. The latter is loosely defined as the main entity that the segment is about; in English this is usually the subject. For example, main referents of G ENERIC S ENTENCEs are kinds or classes as in “Elephants are huge”, while the main referents of Eventualities and G ENERALIZ ING S ENTENCEs are particular individuals (“John is short”). For English, the main verb is the non-auxiliary verb ranked highest in the dependency parse (e.g. “kiss” in “John has kissed Joe”). S TATEs and E VENTs differ in the fundamental lexical aspectual class (Siegel and McKe"
P16-1166,R09-1035,0,0.0283688,"Missing"
P16-1166,loaiciga-etal-2014-english,0,0.0236599,"2010) using the implementation by Liang (2005).2 Clusterings with 320 and 1000 Brown clusters work best for our task. We use one feature per cluster, counting how often a word in the clause was assigned to this cluster (0 for most clusters). B-mv: main verb. Using dependency parses, we extract the verb ranked highest in the clause’s parse as the main verb, and extract the set of features listed in Table 3 for that token. Features based on WordNet (Fellbaum, 1998) use the most frequent sense of the lemma. Tense and voice information is extracted from sequences of POS tags using a set of rules (Loaiciga et al., 2014). Linguistic indicators (Siegel and McKeown, 2000) are features collected per verb type over a large parsed background corpus, encoding how often a verb type occurred with each linguistic marker, e.g., in past tense or with an in-PP. We use values collected from Gigaword (Graff et al., 2003); these are freely available at our project web site (Friedrich and Palmer, 2014a). B-mr: main referent. We extract the grammatical subject of the main verb (i.e., nsubj or nsubjpass) as the clause’s main referent. While the main verb must occur within the clause, the 2 http://metaoptimize.com/projects/ wor"
P16-1166,P10-1005,0,0.207583,"well as a number of lexical features adopted from theoretical work on aspectual classification. The model is described in Section 6.1. Another related body of work has to do with determining event class as a precursor to temporal relation classification. The inventory of event classes, described in detail in the TimeML annotation guidelines (Saur´ı et al., 2006), combines semantic (REPORTING, PERCEPTION), aspectual (ASPECTUAL, STATE, OCCURRENCE), and intensional (I ACTION, I STATE) properties of events. Finally, there are close connections to systems which predict genericity of noun phrases (Reiter and Frank, 2010; Friedrich and Pinkal, 2015a), and habituality of clauses (Mathew and Katz, 2009; Friedrich and Pinkal, 2015b). 4 Data sets The experiments presented in this paper make use of two data sets labeled with SE types. Brown data. This data set consists of 20 texts from the popular lore section of the Brown corpus (Francis and Kuˇcera, 1979), manually segmented into 4391 clauses and marked by two annotators in corpus MASC Wikipedia tokens 357078 148040 SEs 30333 10607 Fleiss’ κ 0.69 0.66 Table 1: SE-labeled corpora: size and agreement. SE type S TATE E VENT R EPORT G ENERIC G ENERALIZING Q UESTION"
P16-1166,J00-4004,0,0.849584,"nd Palmer, 2014b): a clause’s main verb and its main referent. The latter is loosely defined as the main entity that the segment is about; in English this is usually the subject. For example, main referents of G ENERIC S ENTENCEs are kinds or classes as in “Elephants are huge”, while the main referents of Eventualities and G ENERALIZ ING S ENTENCEs are particular individuals (“John is short”). For English, the main verb is the non-auxiliary verb ranked highest in the dependency parse (e.g. “kiss” in “John has kissed Joe”). S TATEs and E VENTs differ in the fundamental lexical aspectual class (Siegel and McKeown, 2000) of their main verbs (e.g. dynamic in “She filled the glass with water” vs. stative in “Water fills the glass”). While fundamental lexical aspectual class is a word-sense level attribute of the clause’s main verb, habituality is a property of the entire clause which is helpful to determine the clause’s SE type. For example, E VENT and G ENERALIZ ING S ENTENCE differ in habituality (e.g. episodic in “John cycled to work yesterday” vs. habitual in “John cycles to work”). Like habituality, SE types are a categorization at the clause level. Properties of the clause such as modals, negation, or the"
P16-1166,N13-1091,0,0.0709463,"Missing"
P16-1166,N03-1030,0,0.0706951,"Missing"
P16-1166,N03-1033,0,0.0222387,"thods used in our approach, which models SE type labeling as a supervised sequence labeling task. 5.1 Feature sets Our feature sets are designed to work well on large data sets, across genres and domains. Features are grouped into two sets: A consists of standard NLP features including POS tags and Brown clusters. Set B targets SE labeling, focusing on syntacticsemantic properties of the main verb and main referent, as well as properties of the clause which indicate its aspectual nature. Texts are pre-processed with Stanford CoreNLP (Manning et al., 2014), including tokenization, POS tagging (Toutanova et al., 2003) and dependency parsing (Klein and Manning, 2002) using the UIMA-based DKPro framework (Ferrucci and Lally, 2004; Eckart de Castilho and Gurevych, 2014). A-pos: part-of-speech tags. These features count how often each POS tag occurs in a clause. A-bc: Brown cluster features. UT07 relies mostly on words and word/POS tag pairs. These simple features work well on the small Brown data set, but the approach quickly becomes impractical with increasing corpus size. We instead turn to distributional information in the form of Brown clusters (Brown et al., 1992), which can be learned from raw text and"
P16-1166,P07-1113,1,0.717502,"subtypes. Eventualities comprise E VENT and S TATE, categories for clauses representing actual happenings, states of the world, or attributes of entities or situations. General Statives include G ENERIC S ENTENCE and G ENERALIZING S EN TENCE and reflect regularities in the world or general information predicated over classes or kinds. Finally, Abstract Entities (Figure 2) have the subtypes FACT and P ROPOSITION. Although Abstract Entities are part of the label inventory for UT07, we treat them in a separate identification step, for reasons discussed in Section 7. The inventory was expanded by Palmer et al. (2007) to include three additional types: R EPORT, Q UES TION and I MPERATIVE. The latter two categories were added to accommodate exhaustive annota1 Corpora, annotation manual and code available at www.coli.uni-saarland.de/projects/sitent FACT: Objects of knowledge. I know that Mary refused the offer. P ROPOSITION: Objects of belief. I believe that Mary refused the offer. Figure 2: Abstract Entity SE types. tion of text; R EPORT is a subtype of event for attributions of quoted speech. Two parts of a clause provide important information for determining the SE type (Friedrich and Palmer, 2014b): a cl"
P16-1166,P10-1040,0,0.0274098,"words and word/POS tag pairs. These simple features work well on the small Brown data set, but the approach quickly becomes impractical with increasing corpus size. We instead turn to distributional information in the form of Brown clusters (Brown et al., 1992), which can be learned from raw text and represent word classes in a hierarchical way. Originally developed in the context of n-gram language modeling, they aim to assign words to classes such that the average mutual information of the words in the clusters is maximized. We use existing, freely-available clusters trained on news data by Turian et al. (2010) using the implementation by Liang (2005).2 Clusterings with 320 and 1000 Brown clusters work best for our task. We use one feature per cluster, counting how often a word in the clause was assigned to this cluster (0 for most clusters). B-mv: main verb. Using dependency parses, we extract the verb ranked highest in the clause’s parse as the main verb, and extract the set of features listed in Table 3 for that token. Features based on WordNet (Fellbaum, 1998) use the most frequent sense of the lemma. Tense and voice information is extracted from sequences of POS tags using a set of rules (Loaic"
P16-1166,P15-2092,0,0.0665933,"Missing"
P16-1166,zarcone-lenci-2008-computational,0,0.337346,"cribe four categories as lexical properties of verbs, distinguishing states from three types of events (accomplishment, achievement, and activity), differing according to temporal and aspectual properties (e.g. telicity and punctuality). The work of Siegel and McKeown (2000) is a major inspiration in computational work on modeling these linguistic phenomena, introducing the use of linguistic indicators (see Section 5.1). Hermes et al. (2015) model Vendler classes computationally on a verbtype level for 95 different German verbs, combining distributional vectors with supervised classification. Zarcone and Lenci (2008) investigate both supervised and unsupervised classification frameworks for occurrences of 28 Italian verbs, and Friedrich and Palmer (2014a) predict lexical aspectual class for English verbs in context. The only previous approach to automatic classification of SE types comes from Palmer et al. (2007). This system (UT07) uses word and POS tag features as well as a number of lexical features adopted from theoretical work on aspectual classification. The model is described in Section 6.1. Another related body of work has to do with determining event class as a precursor to temporal relation clas"
palmer-etal-2004-utilization,J97-1005,0,\N,Missing
palmer-etal-2004-utilization,C92-2117,0,\N,Missing
palmer-etal-2004-utilization,P97-1020,0,\N,Missing
palmer-etal-2004-utilization,P02-1035,0,\N,Missing
R11-1090,W04-0817,0,0.0614556,"Missing"
R11-1090,W08-2208,0,0.0207302,"the arguments. ies have addressed the coverage issue. For example, Das et al. (2010) introduce a latent variable ranging over seen targets, allowing them to infer likely frames for unseen words, and the SRL system of Johansson and Nugues (2007) uses WordNet to generalise to unseen lemmas. In a similar vein, Burchardt et al. (2005) propose a system that generalizes over WordNet synsets to guess frames for unknown words. Pennacchiotti et al. (2008) compare WordNet-based and distributional approaches to inferring frames and conclude that a combination of the two leads to the best results, while (Cao et al., 2008) discuss how different distributional models can be utilised. Several approaches have also addressed other coverage problems, e.g., how to automatically expand the number of example sentences for a given lexical unit (Pad´o et al., 2008; F¨urstenau and Lapata, 2009). Another related approach is that of generalizing over semantic roles. Baldewein et al. (2004) use the FrameNet hierarchy to model the similarity of roles, boosting seldom-seen instances by reusing training data for similar roles, though without significant gains in performance. The most extensive study on role generalization to da"
R11-1090,N10-1138,0,0.0122428,"t-in difference between predicting a frame label or semantic role for seen versus unseen instances. Naturally, the outcome of prediction will be more accurate if the model has seen several instances similar to a test instance (i.e., from the same lexical unit or lemma). But even for unseen instances, the model is still capable of generalizing the properties of the training instances given that there are similarities between their available features, such as the syntactic pattern and the semantic properties of the predicate and the arguments. ies have addressed the coverage issue. For example, Das et al. (2010) introduce a latent variable ranging over seen targets, allowing them to infer likely frames for unseen words, and the SRL system of Johansson and Nugues (2007) uses WordNet to generalise to unseen lemmas. In a similar vein, Burchardt et al. (2005) propose a system that generalizes over WordNet synsets to guess frames for unknown words. Pennacchiotti et al. (2008) compare WordNet-based and distributional approaches to inferring frames and conclude that a combination of the two leads to the best results, while (Cao et al., 2008) discuss how different distributional models can be utilised. Sever"
R11-1090,erk-pado-2006-shalmaneser,0,0.0488517,"Missing"
R11-1090,E09-1026,0,0.0377882,"Missing"
R11-1090,J02-3001,0,0.0666911,"se (frame) to a noun and a verb as in (1), where both competition and play are assigned the C OMPETITION frame. Also, FrameNet assigns semantic roles not only to syntactic arguments of the target but also to constituents which are not directly syntactically dependent on the target but can be semantically understood as filling a role, e.g., Wivenhoe Town in (1a). (1) a. b. 2 [Wivenhoe Town]Participant1 have never won the competitionCompetition . [Olympiakos]Participant1 playsCompetition [against Aris Salonica]Participant1 [in Piraeus]Place . Related Work While early FrameNet-style SRL systems (Gildea and Jurafsky, 2002; Erk and Pad´o, 2006, among others) are unable to make predictions for LUs not seen in the training data, several more recent studA major challenge for FrameNet-style SRL is posed by the limited coverage of available annotated data. The FrameNet lexicographic corpus 1 Under the SemEval-07 partial matching scheme, a majority of the other frame predictions receive partial credit. 628 Proceedings of Recent Advances in Natural Language Processing, pages 628–633, Hissar, Bulgaria, 12-14 September 2011. are predicted based on the acquired constructions (or clusters), and the extracted features from"
R11-1090,P09-1003,0,0.0587843,"cuss how different distributional models can be utilised. Several approaches have also addressed other coverage problems, e.g., how to automatically expand the number of example sentences for a given lexical unit (Pad´o et al., 2008; F¨urstenau and Lapata, 2009). Another related approach is that of generalizing over semantic roles. Baldewein et al. (2004) use the FrameNet hierarchy to model the similarity of roles, boosting seldom-seen instances by reusing training data for similar roles, though without significant gains in performance. The most extensive study on role generalization to date (Matsubayashi et al., 2009) compares different ways of grouping roles—exploiting hierarchical relations in FrameNet, generalizing via role names, utilising role types, and using thematic roles from VerbNet—with the best results from using all groups together. 3 Model 3.1 We formalize frame and role assignment using an extended version of the construction learning model of Alishahi and Stevenson (2010). The model uses Bayesian clustering for learning argument structure constructions: each construction is a grouping of individual predicate usages which probabilistically share form-meaning associations. These groupings typ"
R11-1090,C08-1084,1,0.908726,"Missing"
R11-1090,C10-2107,1,0.810969,"Missing"
R11-1090,J05-1004,0,0.0515042,"ed coverage of available annotated data. Our SRL model is based on Bayesian clustering and has the advantage of being very robust in the face of unseen and incomplete data. Frame labeling and role labeling are modeled in like fashions, allowing cascading classification scenarios. The model is shown to perform especially well on unseen data. In addition, we show that for seen data, predicting semantic types for roles improves role labeling performance. 1 Introduction The majority of recent work in semantic role labeling (SRL) has been carried out on PropBankstyle semantic argument annotations (Palmer et al., 2005), rather than on FrameNet-style annotations (Ruppenhofer et al., 2006). FrameNet differs from PropBank in that FrameNet annotations are more strongly semantically driven. FrameNet generalizes over different parts of speech and can assign the same sense (frame) to a noun and a verb as in (1), where both competition and play are assigned the C OMPETITION frame. Also, FrameNet assigns semantic roles not only to syntactic arguments of the target but also to constituents which are not directly syntactically dependent on the target but can be semantically understood as filling a role, e.g., Wivenhoe"
R11-1090,D08-1048,0,0.0169724,"ies of the training instances given that there are similarities between their available features, such as the syntactic pattern and the semantic properties of the predicate and the arguments. ies have addressed the coverage issue. For example, Das et al. (2010) introduce a latent variable ranging over seen targets, allowing them to infer likely frames for unseen words, and the SRL system of Johansson and Nugues (2007) uses WordNet to generalise to unseen lemmas. In a similar vein, Burchardt et al. (2005) propose a system that generalizes over WordNet synsets to guess frames for unknown words. Pennacchiotti et al. (2008) compare WordNet-based and distributional approaches to inferring frames and conclude that a combination of the two leads to the best results, while (Cao et al., 2008) discuss how different distributional models can be utilised. Several approaches have also addressed other coverage problems, e.g., how to automatically expand the number of example sentences for a given lexical unit (Pad´o et al., 2008; F¨urstenau and Lapata, 2009). Another related approach is that of generalizing over semantic roles. Baldewein et al. (2004) use the FrameNet hierarchy to model the similarity of roles, boosting s"
S13-1041,W08-0913,0,0.329927,"f foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target"
S13-1041,W12-2002,0,0.0562051,"Missing"
S13-1041,W12-2039,0,0.805582,"Missing"
S13-1041,W97-0802,0,0.250549,"for short answer scoring in the context of foreign language learning. 3 Answer-based models sions in our implementation of that model.1 Preprocessing We preprocess all material (learner answers, target answers, questions and reading texts) using standard NLP tools for sentence splitting and tokenization (both OpenNLP2 ), POS tagging and stemming (both Treetagger (Schmid, 1994)), NP chunking (OpenNLP), and dependency parsing (Zurich Parser (Sennrich et al., 2009)). We use an NE Tagger (Faruqui and Pad´o, 2010) to annotate named entities. Synonyms and semantic types are extracted from GermaNet (Hamp and Feldweg, 1997). For keywords, which serve to give more emphasis to content words in the target answer, we extract all nouns from the target answer. Given that we are dealing with learner language, but do not want to penalize answers for typical learner errors, spellchecking (and subsequent correction of spelling errors) is especially important for this task. Our approach is as follows: we first identify all words from the learner answers that are not accepted by a German spellchecker (aspell3 ). We then check for each word whether the word nevertheless occurs in the target answer, question or reading text."
S13-1041,W11-2401,0,0.706813,"g. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target science concepts, are"
S13-1041,P10-1123,0,0.0696294,"Missing"
S13-1041,E09-1065,0,0.0253111,"be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). S"
S13-1041,P11-1076,0,0.33602,". Those labels can either be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al"
S13-1041,W05-0202,0,0.337334,"proaches to short answer scoring In short answer scoring (SAS) the task is to automatically assign labels to individual learner answers. Those labels can either be binary, a value on some scale of points or grades, or a more fine-grained diagnosis. For example, one fine-grained set of labels (Bailey, 2008) classifies answers as (among others) correct, as missing a necessary concept or concepts, containing extra content, or as failing to answer the question. Our present study is restricted to binary classification. Previous work on SAS, including early systems like (Leacock and Chodorow, 2003; Pulman and Sukkarieh, 2005; Sukkarieh and Pulman, 2005) is of course not only in the domain of foreign language learning. For example, Mohler et al. (2011) and Mohler and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is s"
S13-1041,W12-2022,0,0.376269,"r and Mihalcea (2009) use semantic graph alignments and semantic similarity measures to assess student answers to computer science questions, comparing them to sample solutions provided by a teacher. Accordingly, not all SAS settings include reading or other reference texts; many involve only questions, target answers, and learner answers. Our approach is relevant for scenarios in which some sort of reference text is available. The work we present here is strongly based on approaches towards SAS by Meurers and colleagues (Bailey and Meurers, 2008; Meurers et al., 2011a; Meurers et al., 2011b; Ziai et al., 2012). Specifically, the sentence alignment model described in Section 3 (and again discussed in Section 4) is modeled after the one used by Meurers et al. to align target answers and student answers. Rather than using answers provided by instructors, Nielsen et al. (2008) represent target answers to science questions as a set of hand-annotated facets, i.e. important aspects of the answer, typically represented by a pair of words and the relation that connects them. Student answers, and consequently students’ understanding of target science concepts, are then assessed by determining whether the rel"
S17-1027,P15-1123,0,0.0608045,"re easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another. 1 Introduction Semantic clause types, called Situation Entity (SE) types (Smith, 2003; Palmer et al., 2007) are linguistic characterizations of aspectual properties shown to be useful for argumentation structure analysis (Becker et al., 2016b), genre characterization (Palmer and Friedrich, 2014), and detection of generic and generalizing sentences (Friedrich and Pinkal, 2015). Recent work on automatic identification of SE types relies on feature-based classifiers for English that have been successfully applied to various textual genres (Friedrich et al., 2016), and also show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 230–240, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We present a novel take on modeling and exploiting genre information and test it on"
S17-1027,W16-2803,1,0.91818,"the advantage of our neural model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another. 1 Introduction Semantic clause types, called Situation Entity (SE) types (Smith, 2003; Palmer et al., 2007) are linguistic characterizations of aspectual properties shown to be useful for argumentation structure analysis (Becker et al., 2016b), genre characterization (Palmer and Friedrich, 2014), and detection of generic and generalizing sentences (Friedrich and Pinkal, 2015). Recent work on automatic identification of SE types relies on feature-based classifiers for English that have been successfully applied to various textual genres (Friedrich et al., 2016), and also show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 230–240, c Vancouver, Canada, August 3-4,"
S17-1027,P10-2013,0,0.0559985,"Missing"
S17-1027,P14-1062,0,0.0646515,"Missing"
S17-1027,D14-1181,0,0.0271982,"deling” ♠ Heidelberg University, Department of Computational Linguistics ♣ University of North Texas, Department of Linguistics {mbecker,staniek,nastase,frank}@cl.uni-heidelberg.de alexis.palmer@unt.edu Abstract Deep learning provides a powerful framework in which linguistic and semantic regularities can be implicitly captured through word embeddings (Mikolov et al., 2013b). Patterns in larger text fragments can be encoded and exploited by recurrent (RNNs) or convolutional neural networks (CNNs) which have been successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the indivi"
S17-1027,J92-4003,0,0.43155,"Missing"
S17-1027,N16-1030,0,0.0618447,"state (memory) at time t, and h˜t is the candidate activation at time t. W∗ and U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tanh(W xt + U (rt ht−1 )) zt = σ(Wz xt + Uz ht−1 ) 2014). RNN variations – with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete input information in its hidd"
S17-1027,W14-4012,0,0.0847139,"Missing"
S17-1027,P16-1004,0,0.18652,"successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the individual classification task and (ii) in a setting approximating sequential labeling in which the attention vector provides features that describe the clauses preceding the current instance. Compared to the strong baseline provided by the feature based system of Friedrich et al. (2016), we achieve competitive performance and find that attention as well as context representation using predicted or goldstandard labels of the previous N clauses, and text genre information improve our model. A strong motivation for developing NN-b"
S17-1027,loaiciga-etal-2014-english,0,0.0424883,"Missing"
S17-1027,W14-4921,1,0.86387,"Missing"
S17-1027,J93-2004,0,0.060196,"Missing"
S17-1027,P16-1166,1,0.352268,"GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the individual classification task and (ii) in a setting approximating sequential labeling in which the attention vector provides features that describe the clauses preceding the current instance. Compared to the strong baseline provided by the feature based system of Friedrich et al. (2016), we achieve competitive performance and find that attention as well as context representation using predicted or goldstandard labels of the previous N clauses, and text genre information improve our model. A strong motivation for developing NN-based systems is that they can be transferred with low cost to other languages without major feature engineering or use of hand-crafted linguistic knowledge resources. Given the highly-engineered feature sets used for SE classification so far (Friedrich et al., 2016), porting such classifiers to other languages is a non-trivial issue. We test the portab"
S17-1027,W15-2702,1,0.892449,"show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Proceedings of the 6th Joint Conference on Lexical and Computational Semantics (*SEM 2017), pages 230–240, c Vancouver, Canada, August 3-4, 2017. 2017 Association for Computational Linguistics We present a novel take on modeling and exploiting genre information and test it on the English multi-genre corpus of Friedrich et al. (2016). distribution of SE types in text passages and discourse modes, e.g., narrative, informative, or argumentative (Palmer and Friedrich, 2014; Mavridou et al., 2015; Becker et al., 2016a). Our aims and contributions are: (i) We study the performance of GRU-based models enhanced with attention for modeling local and non-local characteristics of semantic clause types. (ii) We compare the effectiveness of the learned attention weights as features for a sequence labeling system to the explicitly defined syntactic-semantic features in (Friedrich et al., 2016). (iii) We define extensions of our models that integrate external knowledge about genre and show that this can be used to improve classification performance across genres. (iv) We test the portability of"
S17-1027,W15-1603,1,0.839673,"ing both attention and genre information leads to a 2.13 pp increase over the model that uses only attention. Adding context information beyond the local clause – a window of up to three previous clauses – improves the wordbased attention models slightly, but a wider window (four or more clauses) causes a major drop Baseline systems. The feature-based system of Palmer07 (Palmer et al., 2007) (Palmer07 in Table 2) simulates context through predicted labels from previous clauses. Friedrich et al. (2016) (Fried16 in Table 2) report results for their CRF-based SE 7 The Wiki texts were selected by Friedrich et al. (2015) precisely in order to target G ENERIC S ENTENCE clauses. 8 The cross validation splits of the data used by Friedrich et al. (2016) are not available. 235 GRU + att + gLab (1) GRU + att + gLab (2) GRU + att + gLab (3) GRU + att + gLab (4) GRU + att + gLab (5) GRU + att + gLab + genre (1) GRU + att + gLab + genre (2) GRU + att + gLab + genre (3) GRU + att + gLab + genre (4) GRU + att + gLab + genre (5) Acc 72.71 72.68 72.66 72.61 73.40 73.44 73.45 72.84 73.12 73.34 F1 65.37 66.51 65.03 64.33 66.39 66.76 66.51 66.29 66.21 66.13 Table 4: SE-type classification on English test set, sequence oracle"
S17-1027,N03-1030,0,0.361054,"Missing"
S17-1027,N13-1090,0,0.12279,"ling Context and Genre Characteristics with Recurrent Neural Networks and Attention Maria Becker♦♠ , Michael Staniek♦♠ , Vivi Nastase♦♠ , Alexis Palmer♣ , Anette Frank♦♠ ♦ Leibniz ScienceCampus “Empirical Linguistics and Computational Language Modeling” ♠ Heidelberg University, Department of Computational Linguistics ♣ University of North Texas, Department of Linguistics {mbecker,staniek,nastase,frank}@cl.uni-heidelberg.de alexis.palmer@unt.edu Abstract Deep learning provides a powerful framework in which linguistic and semantic regularities can be implicitly captured through word embeddings (Mikolov et al., 2013b). Patterns in larger text fragments can be encoded and exploited by recurrent (RNNs) or convolutional neural networks (CNNs) which have been successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initia"
S17-1027,P15-1150,0,0.094046,"Missing"
S17-1027,P16-1105,0,0.0374456,"with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete input information in its hidden state. Attention has been used successfully e.g. in aspect-based sentiment classification (Wang et al., 2016), for modeling relations between words or phrases in encoder-decoder models for translation (Bahdanau et al., 2015), or bi-clausal classification tasks such as textu"
S17-1027,N16-1065,0,0.0546958,"Missing"
S17-1027,P07-1113,1,0.66066,"ant context not only for the current instance, but also for the larger context. Apart from implicitly capturing task relevant features, the advantage of our neural model is that it avoids the need to reproduce linguistic features for other languages and is thus more easily transferable. We present experiments for English and German that achieve competitive performance. We present a novel take on modeling and exploiting genre information and showcase the adaptation of our system from one language to another. 1 Introduction Semantic clause types, called Situation Entity (SE) types (Smith, 2003; Palmer et al., 2007) are linguistic characterizations of aspectual properties shown to be useful for argumentation structure analysis (Becker et al., 2016b), genre characterization (Palmer and Friedrich, 2014), and detection of generic and generalizing sentences (Friedrich and Pinkal, 2015). Recent work on automatic identification of SE types relies on feature-based classifiers for English that have been successfully applied to various textual genres (Friedrich et al., 2016), and also show that a sequence labeling approach that models contextual clause labels yields improved classification performance. 230 Procee"
S17-1027,D16-1058,0,0.172418,"nal neural networks (CNNs) which have been successfully used for various sentence-based classification tasks, e.g. sentiment (Kim, 2014) or relation classification (Vu et al., 2016; Tai et al., 2015). We frame the task of classifying clauses with respect to their aspectual properties – i.e., situation entity types – in a recurrent neural network architecture. We adopt a Gated Recurrent Unit (GRU)based RNN architecture that is well suited to modeling long sequences (Yin et al., 2017). This initial model is enhanced with an attention mechanism shown to be beneficial for sentence classification (Wang et al., 2016) and sequence modeling (Dong and Lapata, 2016). We explore the usefulness of attention in two settings: (i) the individual classification task and (ii) in a setting approximating sequential labeling in which the attention vector provides features that describe the clauses preceding the current instance. Compared to the strong baseline provided by the feature based system of Friedrich et al. (2016), we achieve competitive performance and find that attention as well as context representation using predicted or goldstandard labels of the previous N clauses, and text genre information improve our"
S17-1027,P16-2067,0,0.0229775,"to keep. ht is the hidden state (memory) at time t, and h˜t is the candidate activation at time t. W∗ and U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tanh(W xt + U (rt ht−1 )) zt = σ(Wz xt + Uz ht−1 ) 2014). RNN variations – with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete inp"
S17-1027,Q16-1027,0,0.0166683,"hbrenner et al., son/thing/situation the clause is about, often realized as its grammatical subject. 2 Code and data: https://github.com/annefried/sitent The main referent of a clause is roughly the per231 rent connections, which allow them to find patterns in – and thus model – sequences. Simple RNNs cannot capture long-term dependencies (Bengio et al., 1994) because the gradients tend to vanish or grow out of control with long sequences. Gated Recurrent Unit (GRU) RNNs, proposed by Cho et al. (2014), address this shortcoming. GRUs have fewer parameters and thus need less data to generalize (Zhou et al., 2016) than LSTM RNNs, and also outperform the LSTM in many cases (Yin et al., 2017), which makes them a good choice for our relatively small dataset.3 The relevant equations for a GRU are below. xt is the input at time t, rt is a reset gate which determines how to combine the new input with the previous memory, and the update gate zt defines how much of the previous memory to keep. ht is the hidden state (memory) at time t, and h˜t is the candidate activation at time t. W∗ and U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tan"
S17-1027,P15-1109,0,0.0139583,"nd U∗ are weights that are learned. denotes the element-wise multiplication of two vectors. rt = σ(Wr xt + Ur ht−1 ) h˜t = tanh(W xt + U (rt ht−1 )) zt = σ(Wz xt + Uz ht−1 ) 2014). RNN variations – with Gated Recurrent Units (GRU) (Cho et al., 2014) or Long Short-Term Memory units (LSTM) (Hochreiter and Schmidhuber, 1997) – have since achieved state of the art performance in both sequence modeling and classification tasks. Recent work applies bi-LSTM models in sequence modeling (PoS tagging, Plank et al. (2016), NER Lample et al. (2016)) and structure prediction tasks (Semantic Role Labeling, Zhou and Xu (2015) or semantic parsing into logical forms Dong and Lapata (2016)). Tree-based LSTM models have been shown to often perform better than purely sequential bi-LSTMs (Tai et al., 2015; Miwa and Bansal, 2016), but depend on parsed input. Attention. Attention has been established as an effective mechanism that allows models to focus on specific words in the larger context. A model with attention learns what input tokens or token sequences to attend to and thus does not need to capture the complete input information in its hidden state. Attention has been used successfully e.g. in aspect-based sentimen"
S19-1017,S13-1004,0,0.0132074,", 1997). In this paper, we focus on verbal negations, i.e., when the negation mark—usually an adverb such as never and not—is grammatically associated with a verb. Positive Interpretations. In philosophy and linguistics, it is accepted that negation conveys positive meaning (Horn, 1989). This positive meaning ranges from implicatures, i.e., what is suggested in an utterance even though neither expressed nor strictly implied (Blackburn, 2008), to entailments. Other terms used in the literature include implied meanings (Mitkov, 2005), implied alternatives (Rooth, 1985) and semantically similar (Agirre et al., 2013). We do not strictly fit into any of this terminology, we reveal positive interpretations as intuitively done by humans when reading text. Note that a positive interpretation is a statement that does not contain negation, not a statement that conveys positive sentiment. For example, The seller didn’t ship the right parts implicitly conveys The seller shipped the wrong parts, which has negative sentiment. Potential Positive Interpretations. Given a sentence containing negation, we use the term potential positive interpretation to refer to positive interpretations that are automatically generate"
S19-1017,W12-3808,0,0.075973,"Missing"
S19-1017,D15-1162,0,0.0338223,"Missing"
S19-1017,D16-1025,0,0.0226121,"Missing"
S19-1017,C10-1076,0,0.0728592,"Missing"
S19-1017,D15-1075,0,0.02611,"ential positive interpretations become actual positive interpretations. Negation and natural language understanding. Generating positive interpretations from negation has several potential applications. First, while neural machine translation is in general superior to phrase-based methods, that is not the case when translating negation (Bentivogli et al., 2016). Since our positive interpretations effectively rewrite negation-containing sentences to remove the negation, we argue that they have the potential to help machine translation. Second, current benchmarks for natural language inference (Bowman et al., 2015), do not include challenging examples with negation. As a result, state-of-the-art approaches (Chen et al., 3 Previous Work From a theoretical perspective, it is accepted that negation has scope and focus, and that the focus yields positive interpretations (Horn, 1989; Rooth, 1992). Scope is “the part of the meaning that is negated” and focus “the part of the scope that is most prominently or explicitly negated” (Huddleston and Pullum, 2002). Scope of negation detection has received a lot ¨ ur and Radev, 2009; Packard of attention (Ozg¨ et al., 2014), mostly using two corpora: BioScope (Szarva"
S19-1017,D15-1166,0,0.0215435,"etation, however, reveals that He has the intention of getting more money. Context, which is not shown in Table 8, support the correctness and validation scores (e.g., He is wealthy). 6 Experiments The task of generating positive interpretations from a sentence containing negation can be approached with sequence-to-sequence (seq2seq) models (input: sentence containing negation, output: positive interpretation). In this section, we present baseline results with existing seq2seq models. Specifically, we experiment with a basic seq2seq model (Cho et al., 2014), two seq2seq models with attention (Luong et al., 2015; Bahdanau et al., 2014), and Google’s neural machine translation (NMT) system (Wu et al., 2016), which is also seq2seq model with attention and arguably the most complex. We acknowledge that these systems are usually trained with orders of magnitude more examples, and comparing them when trained with our fairly small corpus may be unfair because they were designed for other tasks. Our goal is not to obtain the best results possible, but rather provide baseline results for our task and corpus. Figure 3: Distribution of correctness (top) and novelty (bottom) scores in our corpus. Interpretation"
S19-1017,W17-5307,0,0.0471949,"Missing"
S19-1017,D14-1179,0,0.0436641,"Missing"
S19-1017,S12-1035,1,0.882943,"Missing"
S19-1017,P16-1047,0,0.037469,"Missing"
S19-1017,P14-1007,0,0.0530314,"Missing"
S19-1017,J05-1004,0,0.248144,"Missing"
S19-1017,D16-1119,1,0.869512,"Missing"
S19-1017,W08-0606,0,0.0553874,"Missing"
S19-1017,J12-2005,0,0.0651736,"Missing"
W07-1528,U03-1008,0,0.0231727,"and (4) a free translation of each sentence. Another characteristic of language documentation projects is the tentative nature of many analyses, given that linguistic analysis is often occurring in tandem with the annotation process, sometimes for the first time in the recorded history of the language. Furthermore, language documentation projects require long-term accessibility of the collected language data as well as easy accessibility to community members as well as to linguists. In this paper we propose a new XML format for representing IGT, which we call IGT-XML. We build on the model of Hughes et al (2003) (the BHB model from now on), who first proposed using the IGT structure directly as a basis for an XML format. While their format shows closely integrated annotation layers using XML embedding, our model has a more loosely coupled and flexible representation of different annotation layers, to accommodate (a) selective manual reannotation of individual layers, and (b) the (semi-)automatic extension of annotation, without the format posing an a priori restriction on the annotation levels that can be added. The IGTXML representation is thus a first step toward partial automation of the productio"
W07-1528,kuhn-mateo-toledo-2004-applying,0,0.0211633,"3, c Prague, June 2007. 2007 Association for Computational Linguistics with language data. Plan of the paper. After discussing interlinearized glosses in Section 2, we show the BHB model and corresponding XML format in Section 3. Section 4 presents the IGT-XML format that we propose. Section 5 demonstrates the applicability of IGT-XML to data from different languages and different documentation projects, and Section 6 concludes. 2 Interlinearized glossed text IGT is a way of encoding linguistic data commonly used to present linguistic examples. The example below is a segment of IGT taken from Kuhn and Mateo-Toledo (2004). The language is Q’anjob’al, a Mayan language of Guatemala. (1) Maxab’ ek’elteq ix unin yet sq’inib’alil tu. (2) max-ab’ ek’-el-teq ix unin y-et COM-EV pass-DIR-DIR CL child E3S-when s-q’inib’-al-il tu E3S-early-ABS-ABS DEM ’The child came out early that morning (they say)’ 2 The format of the IGT in this example is typical of the presentation of individual examples in the linguistics literature. The raw, unannotated text (1) is associated with three layers of annotation, shown in (2). The first annotation layer shows the same text with each word segmented into its constituent morphemes. The"
W07-1528,mengel-lezius-2000-xml,0,0.0305113,"Missing"
W07-1528,W01-1506,0,\N,Missing
W09-1905,W04-3202,1,0.896776,"Missing"
W09-1905,W05-0619,0,0.0505642,"ix experimental 41 40 20 10 Seconds per Morpheme 30 Non−expert Expert 0 money. Since annotator pay may be variable but will (under standard assumptions) be constant for a given annotator, the best approximation of likely cost savings is to measure the time taken to annotate under different levels of automated support. This is especially important in sample selection and its interaction with automated suggestions: active learning seeks to find more informative examples, and these will most likely involve more difficult decisions, decreasing annotation quality and/or increasing annotation time (Hachey et al., 2005). Thus, we measure cost in terms of the time taken by each annotator on each example. This allows us to measure the actual time taken to produce a given labeled data set, and thus compare the effectiveness of different levels of automated support plus their interaction with annotators of different levels of expertise. Recent work shows that paying attention to predicted annotation cost in sample selection itself can increase the effectiveness of active learning (Settles et al., 2008; Haertel et al., 2008b). Though we have not explored cost-sensitive selection here, the scenario described here"
W09-1905,P08-2017,0,0.0342038,"more difficult decisions, decreasing annotation quality and/or increasing annotation time (Hachey et al., 2005). Thus, we measure cost in terms of the time taken by each annotator on each example. This allows us to measure the actual time taken to produce a given labeled data set, and thus compare the effectiveness of different levels of automated support plus their interaction with annotators of different levels of expertise. Recent work shows that paying attention to predicted annotation cost in sample selection itself can increase the effectiveness of active learning (Settles et al., 2008; Haertel et al., 2008b). Though we have not explored cost-sensitive selection here, the scenario described here is an appropriate test ground for it: in fact, the results of our experiments, reported in the next section, provide strong evidence for a real natural language annotation task that active learning selection with cost-sensitivity is indeed sub-optimal. 0 10 20 30 40 50 Number of Annotation Rounds Figure 1: Average annotation time (in seconds per morpheme) over annotation rounds, averaged over all six conditions for each annotator. cases for each annotator. The newly-labeled clauses are then added to the"
W09-1905,W00-1306,0,0.0351614,"st setups. For uncertainty sampling, we measure uncertainty of a clause as the average entropy per morpheme (i.e., per labeling decision). 3.5 Measuring annotation cost Not all examples take the same amount of effort to annotate. Even so, the bulk of the literature on active learning assumes some sort of unit cost to determine the effectiveness of different sample selection strategies. Examples of unit cost measurements include the number of documents in text classification, the number of sentences in part-of-speech tagging (Settles and Craven, 2008), or the number of constituents in parsing (Hwa, 2000). These measures are convenient for performing active learning simulations, but awareness has grown that they are not truly representative measures of the actual cost of annotation (Haertel et al., 2008a; Settles et al., 2008), with Ngai and Yarowsky (2000) being an early exception to the unit-cost approach. Also, Baldridge and Osborne (2004) use discriminants in parse selection, which are annotation decisions that they later showed correlate with timing information (Baldridge and Osborne, 2008). The cost of annotation ultimately comes down to 4 Discussion This section presents and discusses p"
W09-1905,I08-2093,0,0.0339371,"phemes with tags, some corresponding to parts-of-speech and others to semantic distinctions. There is no single standard format for IGT. The IGT systems developed by documentation projects tend to be idiosyncratic: they may be linguistically well-motivated and intuitive, but they are unlikely to be compatible or interchangeable with systems developed by other projects. They may lack internal consistency as well. Nonetheless, IGT in a readily accessible format is an important resource that can be used fruitfully by linguists to examine hypotheses on novel data (e.g. Xia and Lewis (2007; 2008), Lewis and Xia (2008)). Furthermore, it can be used by educators and language activists to create curriculum material for mother language education and promote the survival of the language. Despite the urgent need for such resources, IGT annotations are time consuming to create entirely by hand, and both human and financial resources are extremely limited in this domain. Thus, language 1 KEY: COM=completive DIR=directional aspect, DEM=demonstrative, Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 36–44, c Boulder, Colorado, June 2009. 2009 Association for Computation"
W09-1905,P00-1016,0,0.224916,"Missing"
W09-1905,W07-1528,1,0.75142,"lycomplete texts were filled in by the expert annotator. A challenge for representing IGT in a machinereadable format is maintaining the links between S=sustantivo (noun), SC=category suffix, TAM=tense/aspect/mood, VT=transitive verb 5 SUF=suffix, http://www.sil.org/computing/shoebox/ 38 the source text morphemes in the second tier and the morpheme-by-morpheme glosses in the third tier. The standard Shoebox output format, for example, enforces these links through management of the number of spaces between items in the output. To address this, we converted the cleaned annotations into IGT-XML (Palmer and Erk, 2007) with help from the Shoebox/Toolbox interfaces provided in the Natural Language Toolkit (Robinson et al., 2007). Automating the transformation from Shoebox format to IGT-XML’s hierarchical format required cleaning up tier-to-tier alignment and checking segmentation in some cases where morphemes and glosses were misaligned, as in (5) below.6 (4) Non li in yolow rk’il (5) Non li in yolow r-k’il DEM DEM yo platicar AP E3s.-SR DEM DEM PRON VI SUF PERS SREL ’Solo asi yo aprendi con e´ l.’ Here, the number of elements in the morpheme tier (first line of (5)) does not match the number of elements in"
W09-1905,D08-1112,0,0.190892,"ree components: 1) presenting examples to the annotator and storing the annotations, 2) training and evaluation of tagging models using data labeled by the annotator, and 3) selecting new examples for annotation. The processes are managed and coordinated using the OpenNLP IGT Editor.7 The annotation component of the tool, and in particular the user interface, is built on the Interlinear Text Editor (Lowe et al., 2004). For tagging we use a strong but simple standard classifier. There certainly are many other modeling strategies that could be used, for example a conditional random field (as in Settles and Craven (2008)), or a model that deals differently with POS labels and morpheme gloss labels. Nonetheless, a documentary linguistics project would be most likely to use a straightforward, off-the-shelf labeler, and our focus is on exploring different annotation approaches in a realistic documentation setting rather than building an optimal classifier. To that end, we use a standard maximum entropy classifier which predicts the label for a morpheme based on the morpheme itself plus a window of two morphemes before and after. Standard features used in part-of-speech taggers are extracted from the morpheme to"
W09-1905,N07-1057,0,0.0293942,"nd labeling of stems and morphemes with tags, some corresponding to parts-of-speech and others to semantic distinctions. There is no single standard format for IGT. The IGT systems developed by documentation projects tend to be idiosyncratic: they may be linguistically well-motivated and intuitive, but they are unlikely to be compatible or interchangeable with systems developed by other projects. They may lack internal consistency as well. Nonetheless, IGT in a readily accessible format is an important resource that can be used fruitfully by linguists to examine hypotheses on novel data (e.g. Xia and Lewis (2007; 2008), Lewis and Xia (2008)). Furthermore, it can be used by educators and language activists to create curriculum material for mother language education and promote the survival of the language. Despite the urgent need for such resources, IGT annotations are time consuming to create entirely by hand, and both human and financial resources are extremely limited in this domain. Thus, language 1 KEY: COM=completive DIR=directional aspect, DEM=demonstrative, Proceedings of the NAACL HLT Workshop on Active Learning for Natural Language Processing, pages 36–44, c Boulder, Colorado, June 2009. 200"
W09-1905,I08-1069,0,0.106181,"Missing"
W12-0205,P07-1009,0,0.0754418,"Missing"
W12-0205,N09-1067,0,0.0319192,"Missing"
W12-0205,C10-1044,0,0.0381525,"Missing"
W12-0205,I08-2093,0,0.0712563,"Missing"
W12-0205,W10-2110,0,0.0640354,"Missing"
W14-2211,W06-0605,0,0.347299,"Missing"
W14-2211,P10-1010,0,0.0671941,"cross-linguistic resources, which we survey in section 2. To the best of our knowledge, though, no existing effort meets these two desiderata to the extent of our corpus, which we name SeedLing: a seed corpus for the Human Language Project. To produce SeedLing, we have drawn on four web sources, described in section 3.2. To bring the four resources into a common format and data structure (section 3.1), each required different degrees and types of cleaning and standardisation. We describe the steps required in section 4, A broad-coverage corpus such as the Human Language Project envisioned by Abney and Bird (2010) would be a powerful resource for the study of endangered languages. Existing corpora are limited in the range of languages covered, in standardisation, or in machine-readability. In this paper we present SeedLing, a seed corpus for the Human Language Project. We first survey existing efforts to compile cross-linguistic resources, then describe our own approach. To build the foundation text for a Universal Corpus, we crawl and clean texts from several web sources that contain data from a large number of languages, and convert them into a standardised form consistent with the guidelines of Abne"
W14-2211,W11-1216,0,0.0929071,"Project, and the Rosetta Project. These projects are to be praised for their commitment to universality, but in their current forms it is difficult to use their data to perform large-scale NLP. Parallel corpora. The Machine Translation community has assembled a number of parallel corpora, which are crucial for statistical machine translation. The OPUS corpus (Tiedemann, 2012) subsumes a number of other well-known parallel corpora, such as Europarl, and covers documents from 350 languages, with various language pairs. 3 3.1 Universal Corpus and Data Structure Building on their previous paper, Abney and Bird (2011) describe the data structure they envisage for a Universal Corpus in more detail, and we aim to adopt this structure where possible. Two types of text are distinguished: Aligned texts consist of parallel documents, aligned at the document, sentence, or word level. Note that monolingual documents are viewed as aligned texts only tied to a single language. Analysed texts, in addition to the raw text, contain more detailed annotations including parts of speech, morphological information, and syntactic relations. This is stored as a table, where rows represent words, and columns represent: documen"
W14-2211,tiedemann-2012-parallel,0,0.033823,"Cross-linguistic projects. Responding to the call to document and preserve the world’s languages, highly cross-linguistic projects have sprung up, striving towards the aim of universality. Of particular note are the Endangered Languages Project, and the Rosetta Project. These projects are to be praised for their commitment to universality, but in their current forms it is difficult to use their data to perform large-scale NLP. Parallel corpora. The Machine Translation community has assembled a number of parallel corpora, which are crucial for statistical machine translation. The OPUS corpus (Tiedemann, 2012) subsumes a number of other well-known parallel corpora, such as Europarl, and covers documents from 350 languages, with various language pairs. 3 3.1 Universal Corpus and Data Structure Building on their previous paper, Abney and Bird (2011) describe the data structure they envisage for a Universal Corpus in more detail, and we aim to adopt this structure where possible. Two types of text are distinguished: Aligned texts consist of parallel documents, aligned at the document, sentence, or word level. Note that monolingual documents are viewed as aligned texts only tied to a single language. A"
W14-2211,N09-1067,0,0.074742,"Missing"
W14-2211,N07-1057,0,0.178492,"> <line>3sgA-1sgO-say-rise-PERF </line> <line>‘She woke me up’ (by verbal action)</line> </example> </igit> To exemplify the use of SeedLing for computational research on low-resource languages, we experiment with automatic detection of similar languages. When working on endangered languages, documentary and computational linguists alike face a lack of resources. It is often helpful to exploit lexical, syntactic or morphological knowledge of related languages. For example, similar high-resource languages can be used in bootstrapping approaches, such as described by Yarowsky and Ngai (2001) or Xia and Lewis (2007). Language classification can be carried out in various ways. Two common approaches are genealogical classification, mapping languages onto family trees according to their historical relatedness (Swadesh, 1952; Starostin, 2010); and typological classification, grouping languages according to linguistic features (Georgi et al., 2010; Daum´e III, 2009). Both of these approaches require linguistic analysis. By contrast, we use surface features (character n-gram and word unigram frequencies) extracted from SeedLing, and apply an off-the-shelf hierarchical clustering algorithm.12 Specifically, each"
W14-2211,xia-etal-2010-problems,0,0.137347,"’Tower of Babel’). Even after automatic extraction, some noise in the data remained, such as explanatory notes given in parentheses, which are written in English and not the target language. Even though the total amount of data here is small compared to our other sources, the amount of effort required to process it was not, because of these idiosyncracies. We expect that researchers seeking to convert data from human-readable to machinereadable formats will encounter similar problems, but unfortunately there is unlikely to be a one-sizefits-all solution to this problem. 4.2 Language Codes As Xia et al. (2010) explain, language names do not always suffice to identify languages, since many names are ambiguous. For this reason, sets of language codes exist to more accurately identify languages. We use ISO 639-310 as our standard set of codes, since it aims for universal coverage, and has widespread acceptance in the community. The data from ODIN and the UDHR already used this standard. To facilitate the standardization of language codes, we have written a python API that can be used to query information about a language or a code, fetching up-to-date information from SIL International (which maintain"
W14-2211,N01-1026,0,0.0849426,"a-t Yimas (Foley 1991)</line> <line>3sgA-1sgO-say-rise-PERF </line> <line>‘She woke me up’ (by verbal action)</line> </example> </igit> To exemplify the use of SeedLing for computational research on low-resource languages, we experiment with automatic detection of similar languages. When working on endangered languages, documentary and computational linguists alike face a lack of resources. It is often helpful to exploit lexical, syntactic or morphological knowledge of related languages. For example, similar high-resource languages can be used in bootstrapping approaches, such as described by Yarowsky and Ngai (2001) or Xia and Lewis (2007). Language classification can be carried out in various ways. Two common approaches are genealogical classification, mapping languages onto family trees according to their historical relatedness (Swadesh, 1952; Starostin, 2010); and typological classification, grouping languages according to linguistic features (Georgi et al., 2010; Daum´e III, 2009). Both of these approaches require linguistic analysis. By contrast, we use surface features (character n-gram and word unigram frequencies) extracted from SeedLing, and apply an off-the-shelf hierarchical clustering algorit"
W14-2211,C10-1044,0,0.0630962,"processing. To test this claim, we use SeedLing in a sample application (section 5): the task of language clustering. With no additional pre-processing, we extract surface-level features (frequencies of character n-grams and words) to estimate the similarity of two languages. Unlike most previous approaches to the task, we make no use of resources curated for linguistic typology (e.g. values of typological features as in WALS (Dryer and Haspelmath, 2013), Swadesh word lists). Despite our approach being highly dependent on orthography, our clustering performance matches the results obtained by Georgi et al. (2010) using typolological features, which demonstrates SeedLing’s utility in cross-linguistic research. 2 is not always machine readable. Even where the data is available digitally, these often take the form of scanned images or audio files. While both can provide invaluable information, they are extremely difficult to process with a computer, requiring an impractical level of image or video pre-processing before linguistic analysis can begin. Even textual data, which avoids these issues, may not be available in a machine-readable form, being stored as pdfs or other opaque formats. Secondly, when d"
W14-2212,P12-3005,0,0.0227568,"facilitating use for both research and development purposes. The main challenges for this project involved managing the interaction between the algorithm 2 3 3.2 Language ID for many languages This project4 addresses the task of language identification. Given a string of text in an arbitrary language, can we train a system to recognize what language the text is written in? Excellent classification rates have been achieved in previous work, but for a relatively small number of languages, and the task becomes noticeably more difficult as the number of languages increases (Baldwin and Lui, 2010; Lui and Baldwin, 2012, for example). With few exceptions (Brown, 2013; Xia et al., 2010; Xia et al., 2009), existing systems have only attempted to distinguish between fewer than 200 of the thousands of written languages currently in use. This team of three students aimed to expand coverage of language identification systems as much as possible given existing sources of data. To do this, they first needed to gather and standardize data from various sources. They targeted three sources of data: the Universal Declaration of Human Rights, Wikipedia,5 ODIN (Lewis and Xia, 2010), and some portions of the data available"
W14-2212,D10-1120,0,0.0164214,"suitability. Many existing models in computational linguistics implicitly encode or expect characteristics of high-resource languages (Bender, 2011); for example, much work on computational syntax uses models that exploit linear ordering of elements in utterances. Such models are not straightforwardly applicable for languages with free or flexible word order, nor for highly agglutinative languages where, for example, complete utterances are encoded as single words. Approaches to this issues include adaptation of models using linguistic knowledge and/or universals (Boonkwan and Steedman, 2011; Naseem et al., 2010). The third issue to note is the difficulty of evaluation. The output of systems or tools performing automated analysis are predictions of analyses for new data; these predictions must be evaluated against a ground truth or humansupplied analysis of the same data. Evaluation is difficult in the low-resource setting, both because of limited availability of expert-labeled data and because, in some cases, the ground truth isn’t known, or analyses are shifting as knowledge about the language develops. 3 Four projects in four months The course described here (“NLP tools for LowResource Languages”)"
W14-2212,P10-1010,0,0.0929376,"(more than 1000 languages) language identification system, to language-specific systems: a lemmatizer for the Mayan language Uspanteko and named entity recognition systems for both Slovak and Persian. Teaching efforts such as these are an excellent way to develop not only tools for low-resource languages, but also computational linguists well-equipped to work on endangered and low-resource languages. 1 Introduction There is a strong argument to be made for bringing together computational and documentary linguistics in order to support the documentation and description of endangered languages (Abney and Bird, 2010; Bird, 2009). Documentation, description, and revitalization work for endangered languages, as well as efforts to produce digital and machine-readable resources for languages currently lacking such data, benefit from technological support in many different ways. Here we focus on support via (a) tools facilitating more efficient development of resources, with easy learning curves, and (b) linguistic analysis tools. Various meetings and workshops in recent years have helped to bring the two fields closer together, but a sizeable gap remains. We’ve come 2 Teaching NLP for LRLs Working on LRLs fr"
W14-2212,N10-1027,0,0.0188175,"valuation module, thus facilitating use for both research and development purposes. The main challenges for this project involved managing the interaction between the algorithm 2 3 3.2 Language ID for many languages This project4 addresses the task of language identification. Given a string of text in an arbitrary language, can we train a system to recognize what language the text is written in? Excellent classification rates have been achieved in previous work, but for a relatively small number of languages, and the task becomes noticeably more difficult as the number of languages increases (Baldwin and Lui, 2010; Lui and Baldwin, 2012, for example). With few exceptions (Brown, 2013; Xia et al., 2010; Xia et al., 2009), existing systems have only attempted to distinguish between fewer than 200 of the thousands of written languages currently in use. This team of three students aimed to expand coverage of language identification systems as much as possible given existing sources of data. To do this, they first needed to gather and standardize data from various sources. They targeted three sources of data: the Universal Declaration of Human Rights, Wikipedia,5 ODIN (Lewis and Xia, 2010), and some portion"
W14-2212,J09-3007,0,0.031748,"ges) language identification system, to language-specific systems: a lemmatizer for the Mayan language Uspanteko and named entity recognition systems for both Slovak and Persian. Teaching efforts such as these are an excellent way to develop not only tools for low-resource languages, but also computational linguists well-equipped to work on endangered and low-resource languages. 1 Introduction There is a strong argument to be made for bringing together computational and documentary linguistics in order to support the documentation and description of endangered languages (Abney and Bird, 2010; Bird, 2009). Documentation, description, and revitalization work for endangered languages, as well as efforts to produce digital and machine-readable resources for languages currently lacking such data, benefit from technological support in many different ways. Here we focus on support via (a) tools facilitating more efficient development of resources, with easy learning curves, and (b) linguistic analysis tools. Various meetings and workshops in recent years have helped to bring the two fields closer together, but a sizeable gap remains. We’ve come 2 Teaching NLP for LRLs Working on LRLs from a computat"
W14-2212,I11-1049,0,0.0145113,"). A second concern is model suitability. Many existing models in computational linguistics implicitly encode or expect characteristics of high-resource languages (Bender, 2011); for example, much work on computational syntax uses models that exploit linear ordering of elements in utterances. Such models are not straightforwardly applicable for languages with free or flexible word order, nor for highly agglutinative languages where, for example, complete utterances are encoded as single words. Approaches to this issues include adaptation of models using linguistic knowledge and/or universals (Boonkwan and Steedman, 2011; Naseem et al., 2010). The third issue to note is the difficulty of evaluation. The output of systems or tools performing automated analysis are predictions of analyses for new data; these predictions must be evaluated against a ground truth or humansupplied analysis of the same data. Evaluation is difficult in the low-resource setting, both because of limited availability of expert-labeled data and because, in some cases, the ground truth isn’t known, or analyses are shifting as knowledge about the language develops. 3 Four projects in four months The course described here (“NLP tools for Lo"
W14-2212,P14-5019,1,0.827229,"as getting familiar with development in Windows. The practical utility of this project is immediately evident: any user with a Windows machine can install the necessary components and have a working small-vocabulary recognizer within several hours. Of course, more time and data may be required to improve performance of the recognizer, which currently reaches in the mid-70s with five audio samples per word. These results, as well as further details about the system (including where to download the code, and discussion of substituting other high-resource language recognizers), are described in Vakil et al. (2014). Small-vocabulary ASR for any language This project2 builds on existing research for smallvocabulary (up to roughly 100 distinct words) speech recognition. Such technology is desirable for, among other things, developing speech interfaces to mobile applications (e.g. to deliver medical information or weather reports; see Sherwani (2009)), but dedicated speech recognition engines are available only for a relatively small number of languages. For small-vocabulary applications, though, an existing recognizer for a high-resource language can be used to do recognition in the target language, given"
W14-2212,N07-1057,0,0.0139011,"for addressing these same challenges. This section briefly surveys some of the relevant issues, with pointers to representative studies. The first and most obvious concern is data sparsity. Many of the most successful and widelytaught methods and models in computational linguistics rely on either large amounts of labeled data or massive amounts of unlabeled data. Methods and models explicitly addressing LRLs need to maximize the utility of available data. Approaches for addressing data sparsity range from data collection proposals (Abney and Bird, 2010) to leveraging high-resource languages (Xia and Lewis, 2007) to maximizing annotation effort (Garrette and Baldridge, 2013). A second concern is model suitability. Many existing models in computational linguistics implicitly encode or expect characteristics of high-resource languages (Bender, 2011); for example, much work on computational syntax uses models that exploit linear ordering of elements in utterances. Such models are not straightforwardly applicable for languages with free or flexible word order, nor for highly agglutinative languages where, for example, complete utterances are encoded as single words. Approaches to this issues include adapt"
W14-2212,E09-1099,0,0.013617,"project involved managing the interaction between the algorithm 2 3 3.2 Language ID for many languages This project4 addresses the task of language identification. Given a string of text in an arbitrary language, can we train a system to recognize what language the text is written in? Excellent classification rates have been achieved in previous work, but for a relatively small number of languages, and the task becomes noticeably more difficult as the number of languages increases (Baldwin and Lui, 2010; Lui and Baldwin, 2012, for example). With few exceptions (Brown, 2013; Xia et al., 2010; Xia et al., 2009), existing systems have only attempted to distinguish between fewer than 200 of the thousands of written languages currently in use. This team of three students aimed to expand coverage of language identification systems as much as possible given existing sources of data. To do this, they first needed to gather and standardize data from various sources. They targeted three sources of data: the Universal Declaration of Human Rights, Wikipedia,5 ODIN (Lewis and Xia, 2010), and some portions of the data available from Omniglot.5 The challenges faced by this group lay primarily in two areas: issue"
W14-2212,xia-etal-2010-problems,0,0.0179863,"hallenges for this project involved managing the interaction between the algorithm 2 3 3.2 Language ID for many languages This project4 addresses the task of language identification. Given a string of text in an arbitrary language, can we train a system to recognize what language the text is written in? Excellent classification rates have been achieved in previous work, but for a relatively small number of languages, and the task becomes noticeably more difficult as the number of languages increases (Baldwin and Lui, 2010; Lui and Baldwin, 2012, for example). With few exceptions (Brown, 2013; Xia et al., 2010; Xia et al., 2009), existing systems have only attempted to distinguish between fewer than 200 of the thousands of written languages currently in use. This team of three students aimed to expand coverage of language identification systems as much as possible given existing sources of data. To do this, they first needed to gather and standardize data from various sources. They targeted three sources of data: the Universal Declaration of Human Rights, Wikipedia,5 ODIN (Lewis and Xia, 2010), and some portions of the data available from Omniglot.5 The challenges faced by this group lay primarily"
W14-2212,W14-2211,1,0.860249,"e second set of challenges have to do with the high degree of skew in the data collected. Though their system covers over 1000 languages, the amount of data per language ranges from a single sentence to hundreds of thousands of words. Along the way, the students realized that this collection of data in a stan4 https://github.com/lex4all/lex4all 5 http://msdn.microsoft.com/en-us/library/hh361572 88 https://github.com/alvations/SeedLing http://www.wikipedia.com,http://www.omniglot.com dard, machine-readable form is useful for many other purposes. The corpus and how to access it are described in Emerson et al. (2014). A second paper presenting the language identification results (including those for low-resource languages) is planned for later this year. 3.3 gazetteers and a few language-specific heuristic components to perform NER in a new language. In this project, resource acquisition and evaluation were the main challenges. The students used some existing resources for both languages, but also devoted quite some time to producing new gazetteers. For Slovak, additional challenges were presented by the language’s large number of inflectional cases and resulting variability in form. For example, some inf"
W14-2212,N13-1014,0,\N,Missing
W14-3505,P05-1074,0,0.147038,"Missing"
W14-3505,W03-1004,0,0.120097,"Missing"
W14-3505,P01-1008,0,0.216901,"Missing"
W14-3505,D08-1021,0,0.0607471,"Missing"
W14-3505,S13-2100,0,0.0650705,"Missing"
W14-3505,W12-2039,0,0.14856,"Missing"
W14-3505,W97-0802,0,0.290394,"nce, correct LA – text sentence, incorrect LA – text sentence. We increase the training material available by boosting the corpus in several ways. First, to emphasize the importance of lexical identity for learning word alignments, we add trivially63 identical pairs: each reading text sentence paired with itself, and each word in the CREG corpus vocabulary, also paired with itself. Additionally, we repeat non-identical sentence pairs, with the number of repetitions linked to the nature of the sub-corpus in which the pair appears. We have also begun experiments adding word pairs from GermaNet (Hamp and Feldweg, 1997), in order to learn lexical paraphrases, but the results reported here do not include GermaNet-based boosting. For intrinsic evaluation of the detected paraphrase fragments (Section 3.3), we aim to reduce noise in the data and emphasize reliable sentence pairs. Accordingly, each pair involving correct LAs, as well as those with TAs and text sentences, is copied 10 times. Pairs involving incorrect LAs appear just one time. The trivially-identical pairs are entered 10 times for sentences and 20 times for word pairs. Preprocessing. To prepare the data for word alignment, we apply a standard lingu"
W14-3505,S13-1041,1,0.891129,"ur use of paraphrases for short answer scoring, after which we conclude. 2 Related Work Approaches to automatic short answer scoring usually target the grading task by comparing the learner answer to a target answer specified by a teacher. While early systems used handcrafted patterns (Pulman and Sukkarieh, 2005), most systems rely on alignments between learner and target answer, mostly using lexical and syntactic information (Leacock and Chodorow, 2003; Mohler et al., 2011; Meurers et al., 2011a,b), and sometimes explicitly using lexical paraphrase resources such as WordNet (Fellbaum, 1998). Horbach et al. (2013) include the text as an additional source of information in grading learner answers, by comparing whether learner answer and target answer can be linked to the same text sentence. The restriction to sentence-sized units is one limitation addressed by our approach. We compare our work to our reimplementation of the alignment-based approach by Meurers et al. (2011b). This model uses alignments on different linguistic levels (like words, lemmas, chunks and dependency triples) to align elements in the learner answer to elements in the 61 target answer. Features (e.g. percentage of aligned tokens/c"
W14-3505,W11-2401,0,0.366851,"describe and evaluate paraphrase fragment detection on the CREG corpus in section 3. Section 4 describes and evaluates our use of paraphrases for short answer scoring, after which we conclude. 2 Related Work Approaches to automatic short answer scoring usually target the grading task by comparing the learner answer to a target answer specified by a teacher. While early systems used handcrafted patterns (Pulman and Sukkarieh, 2005), most systems rely on alignments between learner and target answer, mostly using lexical and syntactic information (Leacock and Chodorow, 2003; Mohler et al., 2011; Meurers et al., 2011a,b), and sometimes explicitly using lexical paraphrase resources such as WordNet (Fellbaum, 1998). Horbach et al. (2013) include the text as an additional source of information in grading learner answers, by comparing whether learner answer and target answer can be linked to the same text sentence. The restriction to sentence-sized units is one limitation addressed by our approach. We compare our work to our reimplementation of the alignment-based approach by Meurers et al. (2011b). This model uses alignments on different linguistic levels (like words, lemmas, chunks and dependency triples) t"
W14-3505,P11-1076,0,0.0279202,"es in section 2, and describe and evaluate paraphrase fragment detection on the CREG corpus in section 3. Section 4 describes and evaluates our use of paraphrases for short answer scoring, after which we conclude. 2 Related Work Approaches to automatic short answer scoring usually target the grading task by comparing the learner answer to a target answer specified by a teacher. While early systems used handcrafted patterns (Pulman and Sukkarieh, 2005), most systems rely on alignments between learner and target answer, mostly using lexical and syntactic information (Leacock and Chodorow, 2003; Mohler et al., 2011; Meurers et al., 2011a,b), and sometimes explicitly using lexical paraphrase resources such as WordNet (Fellbaum, 1998). Horbach et al. (2013) include the text as an additional source of information in grading learner answers, by comparing whether learner answer and target answer can be linked to the same text sentence. The restriction to sentence-sized units is one limitation addressed by our approach. We compare our work to our reimplementation of the alignment-based approach by Meurers et al. (2011b). This model uses alignments on different linguistic levels (like words, lemmas, chunks and"
W14-3505,P06-1011,0,0.0229672,"s. Following previous work (Wang and Callison-Burch, 2011; Regneri and Wang, 2012), we pass our input corpus to GIZA++ (Och and Ney, 2003) in order to: (a) estimate word alignments for input sentence pairs, and (b) obtain a lexical correspondence table with scores for individual word pairs. Links between aligned words in the sentence pairs are then classified as positive or negative based on their scores, a technique which has previously been applied to extract paraphrase fragments from non-parallel bilingual corpora and has been shown to improve a state of the art machine translation system (Munteanu and Marcu, 2006). Word pairs containing punctuation or stop words are excluded from the alignment prior to scoring.5 Afterwards, the alignment is refined by removing all negatively-scored word pairs, such that only very strong alignments survive. We then smooth the alignment by recomputing scores for each word, averaging over a window of five words. In this way we often capture context words 1 http://opennlp.apache.org/ http://nlp.stanford.edu/software/tokenizer.shtml 3 http://nlp.stanford.edu/software/CRF-NER.shtml 4 http://aspell.net/ 5 http://www.ranks.nl/stopwords/german.html 2 64 that are left out of the"
W14-3505,J03-1002,0,0.0107569,"emantic information which is easily obtained and robust to learner language and other requirements of the SAS setting. Central to our approach is a method that provides information about paraphrase relations between (parts of) student answer and target answer. We adopt the approach of Wang and Callison-Burch (2011) and Regneri and Wang (2012), who extract sub-sentential paraphrase candidates (“paraphrase fragments”) from monolingual parallel corpora, making essential use of GIZA++, a word alignment algorithm originally developed for aligning bilingual 60 parallel texts in Machine Translation (Och and Ney, 2003). The alignment algorithm learns semantic information from the corpus in an unsupervised way, without any labeled training material. Once this semantic information is given, paraphrase fragments are predicted in a robust manner, using no or (in the chunk-based version of the algorithm) only very shallow additional linguistic information. An example for the fragments that are extracted from a learner answer and the corresponding target answer are the bold-print parts of the example in figure 1. We create a parallel corpus using the Corpus of Reading Comprehension Exercises in German (CREG) (Ott"
W14-3505,W05-0202,0,0.0416844,"hich surpasses the state of the art and seems to be appropriate for practical application. The remainder of this paper is structured as follows: we discuss related approaches in section 2, and describe and evaluate paraphrase fragment detection on the CREG corpus in section 3. Section 4 describes and evaluates our use of paraphrases for short answer scoring, after which we conclude. 2 Related Work Approaches to automatic short answer scoring usually target the grading task by comparing the learner answer to a target answer specified by a teacher. While early systems used handcrafted patterns (Pulman and Sukkarieh, 2005), most systems rely on alignments between learner and target answer, mostly using lexical and syntactic information (Leacock and Chodorow, 2003; Mohler et al., 2011; Meurers et al., 2011a,b), and sometimes explicitly using lexical paraphrase resources such as WordNet (Fellbaum, 1998). Horbach et al. (2013) include the text as an additional source of information in grading learner answers, by comparing whether learner answer and target answer can be linked to the same text sentence. The restriction to sentence-sized units is one limitation addressed by our approach. We compare our work to our r"
W14-3505,W04-3219,0,0.0813885,"Missing"
W14-3505,D12-1084,0,0.0915974,"xical and shallow syntactic information, plus possibly lexical-semantic resources such as WordNet (Fellbaum, 1998), in part with impressively good results (for an overview, see Ziai et al. (2012)). In the present paper, we describe an approach to short answer scoring that uses semantic information which is easily obtained and robust to learner language and other requirements of the SAS setting. Central to our approach is a method that provides information about paraphrase relations between (parts of) student answer and target answer. We adopt the approach of Wang and Callison-Burch (2011) and Regneri and Wang (2012), who extract sub-sentential paraphrase candidates (“paraphrase fragments”) from monolingual parallel corpora, making essential use of GIZA++, a word alignment algorithm originally developed for aligning bilingual 60 parallel texts in Machine Translation (Och and Ney, 2003). The alignment algorithm learns semantic information from the corpus in an unsupervised way, without any labeled training material. Once this semantic information is given, paraphrase fragments are predicted in a robust manner, using no or (in the chunk-based version of the algorithm) only very shallow additional linguistic"
W14-3505,W11-1208,0,0.0935423,"and target answer, mostly using lexical and shallow syntactic information, plus possibly lexical-semantic resources such as WordNet (Fellbaum, 1998), in part with impressively good results (for an overview, see Ziai et al. (2012)). In the present paper, we describe an approach to short answer scoring that uses semantic information which is easily obtained and robust to learner language and other requirements of the SAS setting. Central to our approach is a method that provides information about paraphrase relations between (parts of) student answer and target answer. We adopt the approach of Wang and Callison-Burch (2011) and Regneri and Wang (2012), who extract sub-sentential paraphrase candidates (“paraphrase fragments”) from monolingual parallel corpora, making essential use of GIZA++, a word alignment algorithm originally developed for aligning bilingual 60 parallel texts in Machine Translation (Och and Ney, 2003). The alignment algorithm learns semantic information from the corpus in an unsupervised way, without any labeled training material. Once this semantic information is given, paraphrase fragments are predicted in a robust manner, using no or (in the chunk-based version of the algorithm) only very s"
W14-3505,P08-1089,0,0.0503632,"Missing"
W14-3505,W12-2022,0,0.40292,"ven completely impossible, as can be seen in the learner answer of figure 1. Also, both the target answers and, in particular, the student answers, have a tendency to keep close to the text surface. Therefore, shallow approaches considering only surface information form a strong baseline. Existing approaches to automatic short answer scoring typically rely on alignments between learner and target answer, mostly using lexical and shallow syntactic information, plus possibly lexical-semantic resources such as WordNet (Fellbaum, 1998), in part with impressively good results (for an overview, see Ziai et al. (2012)). In the present paper, we describe an approach to short answer scoring that uses semantic information which is easily obtained and robust to learner language and other requirements of the SAS setting. Central to our approach is a method that provides information about paraphrase relations between (parts of) student answer and target answer. We adopt the approach of Wang and Callison-Burch (2011) and Regneri and Wang (2012), who extract sub-sentential paraphrase candidates (“paraphrase fragments”) from monolingual parallel corpora, making essential use of GIZA++, a word alignment algorithm or"
W14-4921,J86-2003,0,0.0761837,"n referent, S TATE) he would win. (specific, S TATE) 3.2.2 Fundamental aspectual class: stative or dynamic Following Siegel and McKeown (2000), we determine the fundamental aspectual class of a clause. This notion is the extension of lexical aspect or aktionsart, which describe the “real life shape” of situations denoted by verbs, to the level of clauses. More specifically, aspectual class is a feature of the main verb and a select group of modifiers, which may differ per verb. The stative/dynamic distinction is the most fundamental distinction in taxonomies of aspectual class (Vendler, 1967; Bach, 1986; Mourelatos, 1978). We allow three labels for this feature: dynamic for cases where the verb and its arguments describe some event (something happens), stative for cases where they introduce some properties of the main referent to the discourse, or both for cases where annotators see both interpretations. It is important to note that the fundamental aspectual class of a verb can be different from the type of situation entity introduced by the clause as a whole. The basic situation type of building a house is dynamic, and in the examples below we see this fundamental aspectual class appearing"
W14-4921,S13-2002,0,0.0387763,"Missing"
W14-4921,E12-3007,0,0.0363319,"Missing"
W14-4921,E12-1027,0,0.180209,"Missing"
W14-4921,P14-2085,1,0.917989,"rge corpus of annotated text for the following purposes: (1) To assess the applicability of SE type classification as described by Smith (2003): to what extent can situations be classified easily, which borderline cases occur, and how do humans perform on this task? (see Sec. 4) (2) Training, development and evaluation of automatic systems classifying situation entities, as well as sub-tasks which have (partially) been studied by the NLP community, but for which no large annotated corpora are available (for example, automatically predicting the fundamental aspectual class of verbs in context (Friedrich and Palmer, 2014) or the genericity of clauses and noun phrases). (3) To provide a foundation for analysis of the theory of Discourse Modes (Smith, 2003), which we explain next (Sec. 2). 2 Background and related work Within a text, one recognizes stretches that are intuitively of different types and can be clustered by their characteristic linguistic features and interpretations. Smith (2003) posits five discourse modes: Narrative, Report, Description, Informative and Argument/Commentary. Texts of almost all genre categories have passages of different modes. The discourse modes are characterized by (a) the typ"
W14-4921,P10-2013,0,0.534075,"notator disagreements, as the features indicate precisely how annotators differ in their interpretation of the situation. Analysis of intra-annotator consistency shows that personal preferences of annotators play a role, and we conclude that disagreements often highlight cases where multiple interpretations are possible. We further argue that such cases should be handled carefully in supervised learning approaches targeting methods to automatically classify situation entity types. As the first phase of the SE annotation project, we are in the process of annotating the written portion of MASC (Ide et al., 2010), the manually-annotated subcorpus of the Open American National Corpus. MASC provides texts from 20 different genres and has already been annotated with various linguistic and semantic phenomena.1 MASC offers several benefits: it includes text from a wide variety of genres, it facilitates study of interactions between various levels of analysis, and the data is freely available with straightforward mechanisms for distribution. In this paper we report results for three of the MASC This work is licenced under a Creative Commons Attribution 4.0 International License. Page numbers and proceedings"
W14-4921,C92-4177,0,0.255725,"Missing"
W14-4921,I11-1068,0,0.237368,"Missing"
W14-4921,palmer-etal-2004-utilization,1,0.827988,"Missing"
W14-4921,P07-1113,1,0.781412,"Missing"
W14-4921,E14-1078,0,0.0221813,"nt types or for identifying abstract entities. In both cases, data sparseness is a problem; there are only very few generic main referents and abstract entities in our current corpus. We plan to conduct case studies on data that is specifically selected for these phenomena. However, in many of the hard cases, several readings are possible. Rather than using an adjudicated data set for training and evaluation of supervised classifiers for labeling clauses with situation entities, we plan to leverage such disagreements for training, following proposals by Beigman Klebanov and Beigman (2009) and Plank et al. (2014). The annotation reported here is ongoing; our next goal is to extend annotation to additional genres within MASC, starting with essays, journal, fiction, and travel guides. Following SE annotation, we will extend the project to annotation of discourse modes. Finally, we are very interested in exploring and annotating SEs in other languages, as we expect a similar inventory but different linguistic realizations. Acknowledgments We thank the anonymous reviewers, Bonnie Webber and Andreas Peldszus for helpful comments, and our annotators Ambika Kirkland, Ruth K¨uhn and Fernando Ardente. This res"
W14-4921,P10-1005,0,0.299636,"Missing"
W14-4921,J00-4004,0,0.499591,"Gerunds may occur as the subject in English sentences. When they describe a specific process as in (16a), we mark them as specific. If they instead describe a kind of process as in (16b), we mark them as generic. (16) (a) Knitting this scarf took me 3 days. (specific, E VENT) (b) Knitting a scarf is generally fun. (generic, G ENERIC S ENTENCE) We also give annotators the option to explicitly mark the main referent as expletive, as in (17). (17) It seemed like (expletive = no main referent, S TATE) he would win. (specific, S TATE) 3.2.2 Fundamental aspectual class: stative or dynamic Following Siegel and McKeown (2000), we determine the fundamental aspectual class of a clause. This notion is the extension of lexical aspect or aktionsart, which describe the “real life shape” of situations denoted by verbs, to the level of clauses. More specifically, aspectual class is a feature of the main verb and a select group of modifiers, which may differ per verb. The stative/dynamic distinction is the most fundamental distinction in taxonomies of aspectual class (Vendler, 1967; Bach, 1986; Mourelatos, 1978). We allow three labels for this feature: dynamic for cases where the verb and its arguments describe some event"
W14-4921,W98-0702,0,0.131623,"Missing"
W14-4921,P99-1015,0,0.123218,"Missing"
W14-4921,N03-1030,0,0.873658,"Missing"
W14-4921,S13-2001,0,0.0382696,"Missing"
W14-4921,J09-4005,0,\N,Missing
W15-0108,D10-1115,0,0.48573,"the result of a compositional process that combines the meanings of the base term read and the affix +er. This puts derivation into the purview of compositional distributional semantic models (CDSMs). CDSMs are normally used to compute the meaning of phrases and sentences by combining distributional representations of the individual words. A first generation of CDSMs represented all words as vectors and modeled composition as vector combination (Mitchell and Lapata, 2010). A second generation represents the meaning of predicates as higher-order algebraic objects such as matrices and tensors (Baroni and Zamparelli, 2010; Coecke et al., 2010), which are combined using various composition operations. Lazaridou et al. predict vectors for derived terms and evaluate their approach on a set of English derivation patterns. Building on and extending their analysis, we turn to German derivation patterns and offer both qualitative and quantitative analyses of two composition models on a state-of-the-art vector space, with the aim of better understanding where these models work well and where they fail. Our contributions are as follows. First, we perform all analyses in parallel for six derivation patterns (two each fo"
W15-0108,C10-1011,0,0.0148778,"cal function model (L EX F UN) represents the pattern as a matrix P that encodes the linear transformation that maps base onto derived terms: P b ≈ d. The best matrix Pˆ is typically computed via least-squares regression between the predicted vectors dˆi and the actual vectors di . 3 Experimental Setup Distributional model. We build a vector space from the SdeWaC corpus (Faaß and Eckart, 2013), part-of-speech tagged and lemmatized using TreeTagger (Schmid, 1994). To alleviate sparsity arising from TreeTagger’s lexicon-driven lemmatization, we back off for unrecognized words to the MATE Tools (Bohnet, 2010), which have higher recall but lower precision than TreeTagger. We also reconstruct lemmas for separated prefix verbs based on the MATE dependency analysis. Finally, we get a word list with 289,946 types (content words only). From the corpus, we extract lemmatized sentences and train a state-of-the art predictive model, namely CBOW (Mikolov et al., 2013). This model builds distributed word vectors by learning to predict the current word based on a context. We use lemma-POS pairs as both target and context elements, 300 dimensions, negative sampling set to 15, and no hierarchical softmax. Selec"
W15-0108,P13-1149,0,0.341749,"ng base-derived term pairs of the same pattern. A regression analysis shows that semantic coherence of the base and derived terms within a pattern, as well as coherence of the semantic shifts from base to derived terms, all significantly impact prediction quality. 1 Introduction Derivation is a major morphological process of word formation (e.g., read → read+er), which is typically associated with a fairly specific semantic shift (+er: agentivization). It may therefore be surprising that the semantics of derivation is a relatively understudied phenomenon in distributional semantics. Recently, Lazaridou et al. (2013) proposed to consider the semantics of a derived term like read+er as the result of a compositional process that combines the meanings of the base term read and the affix +er. This puts derivation into the purview of compositional distributional semantic models (CDSMs). CDSMs are normally used to compute the meaning of phrases and sentences by combining distributional representations of the individual words. A first generation of CDSMs represented all words as vectors and modeled composition as vector combination (Mitchell and Lapata, 2010). A second generation represents the meaning of predic"
W15-0108,W13-3512,0,0.125158,"a large number of distinct patterns. Some cross part-of-speech boundaries (nominalization, verbalization, adjectivization), but many do not (gender indicators like actor / actress or (de-)intensifiers like red / reddish). In many languages, such as German ˇ or the Slavic languages, derivational morphology is extremely productive (Stekauer and Lieber, 2005). Particularly relevant from a semantic perspective is that the meanings of the base and derived terms are often, but not always, closely related to each other. Consequently, derivational knowledge can be used to improve semantic processing (Luong et al., 2013; Pad´o et al., 2013). However, relatively few databases of derivational relations exist. CELEX (Baayen et al., 1996) contains derivational information for several languages, but was largely hand-written. A recent large-coverage resource for German, DErivBase (Zeller et al., 2013), covers 280k lemmas and was created from a rule-based framework that is fairly portable across languages. It is unique in that each base-derived lemma pair is labeled with a sequence of derivation patterns from a set of 267 patterns, enabling easy access to instances of specific patterns (cf. Section 3). Compositiona"
W15-0108,N13-1090,0,0.267478,"work that is fairly portable across languages. It is unique in that each base-derived lemma pair is labeled with a sequence of derivation patterns from a set of 267 patterns, enabling easy access to instances of specific patterns (cf. Section 3). Compositional models for derivation. Base and derived terms are closely related in meaning. In addition, this relation is coherent to a substantial extent, due to the phenomenon of productivity. In English, for example, the suffix -er generally indicates an agentive nominalization (sleep / sleeper) and un- is a negation prefix (well / unwell). Though Mikolov et al. (2013) address some inflectional patterns, Lazaridou et al. (2013) were the first to use this observation to motivate modeling derivation with CDSMs. Conceptually, the meaning of the base term (represented as a distributional vector) is combined with some distributional representation of the affix to obtain a vector representing the meaning of the derived term. In their experiments, they found that the two best-motivated and best-performing composition models were the full additive model (Zanzotto et al., 2010) and the lexical function model (Baroni and Zamparelli, 2010). Botha and Blunsom (2014) us"
W15-0108,P13-2128,1,0.92115,"Missing"
W15-0108,C10-1142,0,0.0530275,"tive nominalization (sleep / sleeper) and un- is a negation prefix (well / unwell). Though Mikolov et al. (2013) address some inflectional patterns, Lazaridou et al. (2013) were the first to use this observation to motivate modeling derivation with CDSMs. Conceptually, the meaning of the base term (represented as a distributional vector) is combined with some distributional representation of the affix to obtain a vector representing the meaning of the derived term. In their experiments, they found that the two best-motivated and best-performing composition models were the full additive model (Zanzotto et al., 2010) and the lexical function model (Baroni and Zamparelli, 2010). Botha and Blunsom (2014) use a related approach to model morphology for language modeling. The additive model (A DD) (Mitchell and Lapata, 2010) generally represents a derivation pattern p as a vector computed as the shift from base term vector b to the derived term vector d, i.e., b + p ≈ d. Given a set of P base-derived term pairs (b, d) for p, the best pˆ is computed as the average of the vector difference, 1 pˆ = N i (di − bi ).1 The lexical function model (L EX F UN) represents the pattern as a matrix P that encodes the linear"
W15-0108,P13-1118,1,0.897303,"Missing"
W15-0108,P13-4006,0,\N,Missing
W15-1603,P10-1143,0,0.0161259,"to locate’ (Officials reported ...). In our opinion, the latter interferes with the definition of SPC as marking cases where the entity referred to is a particular object in the real world, even if the author does not know its identity (At least four people were injured). The breadth of the USP category causes problems with consistency of application (see Section 3). The ACE annotation scheme has also been applied in the Newsreader project.3 The ECB+ corpus (Cybulska and Vossen, 2014) is an extension of EventCorefBank (ECB), a corpus of news articles marked with event coreference information (Bejan and Harabagiu, 2010). ECB+ annotates entity mentions according to ACE-2005, but collapses the three non-GEN labels into a single category. Roughly 12500 event participant mentions are annotated, some doubly and some singly. Agreement statistics for genericity are not reported. 3 www.newsreader-project.eu 23 2.1.2 Other corpora annotated at the NP-level The resources surveyed here apply carefullydefined notions of genericity but are too small to be feasible machine learning training data. The question of whether an NP is generic or not arises in the research context of coreference resolution. Some approaches mark"
W15-1603,bhatia-etal-2014-unified,0,0.0634569,"Missing"
W15-1603,bjorkelund-etal-2014-extended,0,0.0649647,"Missing"
W15-1603,cybulska-vossen-2014-using,0,0.0226472,"is providing new opportunities for women in New Delhi), and cases where the author mentions an entity whose identity would be ‘difficult to locate’ (Officials reported ...). In our opinion, the latter interferes with the definition of SPC as marking cases where the entity referred to is a particular object in the real world, even if the author does not know its identity (At least four people were injured). The breadth of the USP category causes problems with consistency of application (see Section 3). The ACE annotation scheme has also been applied in the Newsreader project.3 The ECB+ corpus (Cybulska and Vossen, 2014) is an extension of EventCorefBank (ECB), a corpus of news articles marked with event coreference information (Bejan and Harabagiu, 2010). ECB+ annotates entity mentions according to ACE-2005, but collapses the three non-GEN labels into a single category. Roughly 12500 event participant mentions are annotated, some doubly and some singly. Agreement statistics for genericity are not reported. 3 www.newsreader-project.eu 23 2.1.2 Other corpora annotated at the NP-level The resources surveyed here apply carefullydefined notions of genericity but are too small to be feasible machine learning train"
W15-1603,doddington-etal-2004-automatic,0,0.0944194,"Missing"
W15-1603,W14-4921,1,0.641791,"vial cases, one of which is the case of nominal modifiers described above. In summary, the ACE scheme problematically fails to treat subject NPs differently from NPs in other syntactic positions, and ‘fuzzy’ points in the guidelines, particularly concerning the USP label, contribute to disagreements between annotators. 4 Annotating genericity as reference to kinds on NP- and clause-level We next present an annotation scheme for marking both clauses and their subject NPs with regard to whether they are generic. Our scheme is primarily motivated by the contributions of clauses to the discourse (Friedrich and Palmer, 2014): do they report on a particular event or state, or do they report on some regularity? These different types of clauses have different entailment properties, and differ in how they contribute to the temporal structure of the discourse. In this work, we focus on separating generic clauses from other types of clauses. We approach the problem from a linguistic perspective rather than focusing on any particular content extraction task, arguing that any generally applicable annotation scheme must be based on solid theoretical foundations. We believe our annotation scheme is a step toward solving th"
W15-1603,W10-1809,0,0.058422,"Missing"
W15-1603,W11-0118,0,0.0551305,"Missing"
W15-1603,N06-2015,0,0.170767,"Missing"
W15-1603,P10-2013,0,0.545796,"ric expressions, in turn facilitating knowledge extraction from natural language text. In this paper we provide the next steps for such an annotation endeavor. Our contributions are: (1) we survey the most important previous projects annotating genericity, focusing on resources for English; (2) with a new agreement study we identify problems in the annotation scheme of the largest currentlyavailable resource (ACE-2005); and (3) we introduce a linguistically-motivated annotation scheme for marking both clauses and their subjects with regard to their genericity. (4) We present a corpus of MASC (Ide et al., 2010) and Wikipedia texts annotated according to our scheme, achieving substantial agreement. 1 Manfred Pinkal1 Making this distinction is important for NLP tasks that aim to disentangle information about particular events or entities from general information about classes, kinds, or particular individuals, such as question answering or knowledge base population. Our present work targets the current lack of a large and satisfactorily-annotated data set for genericity, which is a prerequisite for research aiming to automatically identify these linguistic phenomena. Krifka et al. (1995) report the ce"
W15-1603,I11-1068,0,0.0632525,"phenomena related to genericity on clauses of text. Annotating habituality. Mathew and Katz (2009) conduct a study on automatically distinguishing habitual from episodic sentences. Habitual sentences are taken to be sentences whose main verb is lexically dynamic, but which do not refer to particular events (see for example (3)), and may have generic or non-generic subjects. Their singlyannotated data set, from which they excluded verb types with skewed class distributions, comprises 1052 examples covering 57 verb stems. Their data set is not publicly available. General vs. specific sentences. Louis and Nenkova (2011) describe a method for automatic classification of sentences as general or specific. General sentences are loosely defined as those which make “broad statements about a topic,” while specific sentences convey more detailed information. This distinction is not immediately related to the phenomena treated as generics in the literature. Kind-referring subjects can occur in both general (4a) and specific (4b) sentences; general sentences can also have non-kind-referring subjects (4c). (4) (a) Climatologists and policy makers, ..., need to ponder such complexities... (general) (b) Solid silicon com"
W15-1603,W13-2313,0,0.132507,"Missing"
W15-1603,W04-0210,0,0.626043,"Missing"
W15-1603,P10-1005,0,0.115996,"1 describes corpora from the Automatic Content Extraction (ACE) program (Doddington et 22 al., 2004); other NP-level approaches are described in Section 2.1.2. 2.1.1 ACE entity class annotations The research objective of the ACE program (1999-2008) was the detection and characterization of entities, relations and events in natural text (Linguistic Data Consortium, 2000). All entity mentions receive an entity class label indicating their genericity status. Of the corpora described here, the ACE corpora have been the most widely used for recent research on automatically identifying generic NPs (Reiter and Frank, 2010). The annotation guidelines developed over time; we describe both the initial guidelines of ACE-2 and those from ACE-2005. The ACE-2 corpus (Mitchell et al., 2003) includes 40106 annotated entity mentions in 520 newswire and broadcast documents. The annotation guidelines give no formal definition of genericity; annotators are asked to determine whether each entity refers to “any member of the set in question” (generic) or rather “some particular, identifiable member of that set” (specific/non-generic).1 This leads to a mix of constructions being marked as generic: types of entities (Good stude"
W15-1603,N03-1030,0,0.447667,"Missing"
W15-1603,C14-1100,0,\N,Missing
W15-1903,P07-1011,0,0.0269285,"t are particularly problematic for a certain geographic region, i.e. speakers of a certain L1. They compare how often a certain construction that can be indicative of an error is used in comparison to its correct counterpart in that region and compare this ratio to the one in a native population. In this way, they reliably detect constructions corresponding to common errors for learners of that L1. The approach to model learner language for multiple individual L1s is not commonly integrated into Automatic Error Detection, but used also in some other works such as (Hermet and D´esilets, 2009). Sun et al. (2007) use so-called labeled sequential patterns that overcome the locality of trigrams and consist of (not necessarily consecutive) sequences of words that might be indicative of errors. They mine such patterns and use them to classify correct and erroneous sentences. While these approaches mostly focus on lexical items and errors connected to them, we stay with our analyses on the side of POS and mixed model trigrams. In terms of error detection, we thus lack the granularity needed for this task and rather observe over- and underusages that might be indicative of errors but do not directly allow e"
W15-1903,A00-2019,0,0.111177,"essing This section describes the four corpora used in our experiments and preprocessing steps. The primary corpus is the ETS Corpus of Non-Native Written English, which contains essays in English from learners from eleven different L1s. The secondary resources used are three corpora of texts written by native speakers: LOCNESS for English, the FalkoEssayL1 corpus for German, and the Penn Chinese Treebank for Chinese. 2 Related Work Aspects of our approach are similar to some work in grammatical error detection that also makes use of trigrams or similar measures. For example, the ALEC system (Chodorow and Leacock, 2000) compares the local context of a specific word in an essay to the context in a native corpus to identify erroneous usages in learner texts. 3.1 Corpora The ETS Corpus of Non-Native Written English. The ETS corpus (Blanchard et al., 2014) contains a total of 12,100 essays (more than 4 million tokens) from EFL learners of eleven different L1 origins, namely Arabic, Chinese, French, GerProceedings of the 4th workshop on NLP for Computer Assisted Language Learning at NODALIDA 2015 22 Arabic (ARA) German (DEU) French (FRA) Hindi (HIN) Italian (ITA) Japanese (JPN) Korean (KOR) Spanish (SPA) Telugu ("
W15-1903,W13-1706,0,0.0161995,"his simple distinction is correct in most cases, there is room to further refine this heuristic. For instance, the word only is both an adverb and ends in -ly, however it is not a content word. Also the categories RBR and RBS (comparative and superlative, respectively) are not completely clear-cut. RBR can be the part of speech for function words like more, less, but likewise for content words like better, faster, stronger, and similarly for the superlative RBS tag. with different L1s. One prominent example is native language identification where many systems use some sort of n-gram features (Tetreault et al., 2013). In our case, we use trigram models to capture syntactic properties of various subgroups of language learners, grouping by both L1 and proficiency level. We concentrate on trigrams as they are long enough to capture some context of a word, but do not cause sparse data problems. We build a model of each particular learner group – for example, medium-proficiency learners whose native language is Hindi – by collecting frequency counts for a selected set of trigrams (here, the most frequent trigrams in a native English corpus). Trigram counts are extracted from the set of English essays written b"
W15-1903,D12-1064,0,0.171905,"e speakers of various languages. We investigate, via several different exploratory studies, the role of L1 influences on the shallow syntactic structures produced by learners of English as a Foreign Language (EFL). Our shallow syntactic analysis consists of partof-speech (POS) tags and certain lexical items, primarily closed-class function words. In this way we abstract away (to a large extent) from lexical biases due to topic, and instead focus on syntactic aspects of the learner language. This approach has also been used in work on Native Language Identification (Nagata and Whittaker, 2013; Wong et al., 2012). We build a vector space of trigram frequencies for different groups of learners of English, as well as for native speakers of several languages, and we use these vectors to compare language variants, using one standard similarity metric and one novel similarity metric. The models are described in more detail in Sec. 4. The first aim of the study is to confirm the validity of this modeling approach in the language learning context (Sec. 5.1). Our model shows (not surprisingly) that native English and L2 English indeed differ in the distribution of our vector components: learners of English us"
W15-1903,W09-2110,0,0.0793248,"Missing"
W15-1903,J93-2004,0,0.0490671,"e Penn Chinese Treebank (Xue et al., 2002) is a corpus of Chinese news texts that comes already with among other annotation layers - manual annotations for word segmentation and POS tags. 3.2 Preprocessing The ETS corpus is already tokenized, and we use this tokenization. Falko and Penn Chinese Treebank come with token and POS annotations. LOCNESS requires sentence-splitting and tokenization, for which we use the OpenNLP toolkit.2 The final step needed to have suitable input for our models is POS tagging. We use Treetagger (Schmid, 1994), which uses a refined form of the Penn Treebank tagset (Marcus et al., 1993), to tag all English texts. For a description of these tags, refer to Table 2. The other two corpora are pretagged, and in both cases we use the existing POS tags. Falko corpus texts (as well as the normalized form we use) have been tagged with the Treetagger and the STTS tagset (Schiller et al., 1999), and the Penn Chinese Treebank comes with manual POS annotations. The LOCNESS corpus. The LOCNESS corpus1 contains 410 essays from British and American high school students, amounting to 320,000 tokens of text. We use it as a comparison corpus for comparing the different variants of L2 writings"
W15-1903,C02-1145,0,0.0110834,"high). Table 1 shows the distribution over languages and proficiency levels. We can see that the levels are not balanced and we have substantially more essays from a medium proficiency range than for low or high proficiency. Proficiency levels are derived from 5-point essay scores assigned by human raters, who adressed various aspects of an essay in their grade, such as lexical choice, grammar, coherence and argumentative structure. the raw essay texts in order to minimize POS tagging problems due to misspelled and therefore unknown words. The Penn Chinese Treebank. The Penn Chinese Treebank (Xue et al., 2002) is a corpus of Chinese news texts that comes already with among other annotation layers - manual annotations for word segmentation and POS tags. 3.2 Preprocessing The ETS corpus is already tokenized, and we use this tokenization. Falko and Penn Chinese Treebank come with token and POS annotations. LOCNESS requires sentence-splitting and tokenization, for which we use the OpenNLP toolkit.2 The final step needed to have suitable input for our models is POS tagging. We use Treetagger (Schmid, 1994), which uses a refined form of the Penn Treebank tagset (Marcus et al., 1993), to tag all English t"
W15-1903,P13-1112,0,0.10657,"y classification. POS models. In the POS models, vectors are constructed by extracting trigram counts from POS-tagged texts. This means that each word is tagged, and the original lexical material of the text is discarded. The aim of using POS tag sequences is to abstract away from concrete topics in the data and rely as much as possible on the grammatical structures present in the text. Mixed models. In the mixed models, vectors are constructed by extracting trigram counts from texts that have been transformed into a mix of POS tags and lexical items (as done similarly by Nagata and Whittaker (2013) and Wong et al. (2012)). The motivation for our mixed models is that many learner deviations manifest on the lexicosyntactic level rather than purely on the POS level. In other words, it often matters not just whether a preposition is used, but which one, or not only whether an article is used at all, but whether it is 3 We 5.1 Study 1: Measuring the distance between native and non-native English In this study we investigate how far away from native English different learner groups are (i.e. individual combinations of L1 and proficiency level), 4 More specifically, lexical items are replaced"
W15-1903,P11-1019,0,0.0350777,"we have seen that the prompt can be visible even on the abstraction level of POS models. For example, students that write essays in response to the prompt (21) frequently reused the prepositional phrase In twenty years, which resulted in higher frequency counts for the POS trigram PP CD NNS. ment, we begin the investigation into whether vectors from our mixed models are beneficial for the task of automatic proficiency classification into the three proficiency levels low, medium and high. While both lexical and POS trigrams have been used in related work on automatic grading of learner texts (Yannakoudakis et al., 2011), we are specifically interested in investigating the effectiveness of L1-specific classifiers. We operationalize this question using two different feature sets. We use a baseline that consists of just 5 features: number of tokens, number of sentences, average number of tokens per sentence, number of individual types and type-token-ratio. Additionally, we use the frequencies of the most frequent 500 native English trigrams as features. For classification, we train an out-of-the-box logistic regression model using the WEKA toolkit (Hall et al., 2009). We train and evaluate classifiers per L1, u"
W15-1903,petrov-etal-2012-universal,0,0.152216,"ent stylistic differences rather than actual errors, and only in certain selected contexts can the usage factor help to automatically identify problematic constructions in learner text. Next, we consider how the influence of students’ L1 changes as learners become more proficient in the relevant L2, in this case English (Sec. 5.3). We investigate this by measuring the similarity between various English learner groups and texts written by native speakers of English, German, and Chinese. This investigation requires mapping the POS tags for English, German, and Chinese into the Universal Tagset (Petrov et al., 2012), a coarsegrained tagset designed to be suitable for all languages (as the name suggests). We use existing mapping scripts to convert tagsets for the three languages into the Universal Tagset, and we build a new vector space based on the coarse-grained POS tags. In every case, even low-proficiency L2English is closer to native English than to either native German or native Chinese. Some effects seem to be due at least in part to typological differences between L1s. Finally, building on the observation that trigram distributions change as learner proficiency increases, we use trigram vectors as"
W15-2702,W14-4921,1,0.740293,"SE types differentiate between clauses describing events, those describing states, and those conveying generic information (for more detail, see Section 4). While these semantic types are language-independent, they differ in their linguistic realizations. Here we perform the first detailed cross-linguistic study of SE types, aiming to understand both the differences in their linguistic characteristics across languages (Section 4.1) and how closely SE types correspond to each other cross-linguistically (Section 5.2). This requires adaptation of an existing annotation scheme for SEs in English (Friedrich and Palmer, 2014) to German. We discuss this adaptation (Section 4.1), including an experiment on the interpretation of The main contribution of this paper is a cross-linguistic empirical analysis of two interacting levels of linguistic analysis of written text: situation entity (SE) types, the semantic types of situations evoked by clauses of text, and discourse modes (DMs), a characterization of passages at the sub-document level. We adapt an existing annotation scheme for SEs in English to be used for German data, with a detailed discussion of the most important differences. We create the first parallel cor"
W15-2702,N03-1030,0,0.0952293,"mized for translation studies (Islam and Mehler, 2012), two documents from the news commentary corpus (WMT 2013 shared task training data2 ), sections from the novels Alice in Wonderland and Anna Karenina from the OPUS collection (Tiedemann, 2012),3 and two texts from a multilingual news website.4 These texts were segmented into clauses manually by one of the authors. English and German segments were also aligned manually. In addition, we use two documents (Sophie’s world and economy) from the Smultron corpus (Volk et al., 2010). We split the English part of Smultron into clauses using SPADE (Soricut and Marcu, 2003), and the German part using a syntax-based discourse segmenter for German.5 The Smultron corpus provides alignments on a token-/phrase-level, but these phrases do not necessarily match the clause segmentation. To align clauses, we first identify the main verb of each English segment using dependency parses (Klein and Manning, 2002). We then align each segment to the German segment containing the verb to which the identified (English) main verb is aligned. For all texts, paragraph segmentation follows the paragraph breaks in the original source texts. Related Work. Unlike genre, a notion of tex"
W15-2702,W15-1603,1,0.878854,"Missing"
W15-2702,islam-mehler-2012-customization,0,0.0165149,"Data This study requires aligned parallel data with different text types. We collect 11 parallel EnglishGerman texts from a variety of sources and produce clause- and paragraph-level alignments for the texts. Table 1 gives statistics on the number of segments, tokens, and paragraphs in each document, as well as aggregate statistics for the corpus. The translation direction differs across documents, and part of the data consists of translations from a third language into both English and German.1 The corpus includes three documents from a version of Europarl customized for translation studies (Islam and Mehler, 2012), two documents from the news commentary corpus (WMT 2013 shared task training data2 ), sections from the novels Alice in Wonderland and Anna Karenina from the OPUS collection (Tiedemann, 2012),3 and two texts from a multilingual news website.4 These texts were segmented into clauses manually by one of the authors. English and German segments were also aligned manually. In addition, we use two documents (Sophie’s world and economy) from the Smultron corpus (Volk et al., 2010). We split the English part of Smultron into clauses using SPADE (Soricut and Marcu, 2003), and the German part using a"
W15-2702,tiedemann-2012-parallel,0,0.0365078,"e texts. Table 1 gives statistics on the number of segments, tokens, and paragraphs in each document, as well as aggregate statistics for the corpus. The translation direction differs across documents, and part of the data consists of translations from a third language into both English and German.1 The corpus includes three documents from a version of Europarl customized for translation studies (Islam and Mehler, 2012), two documents from the news commentary corpus (WMT 2013 shared task training data2 ), sections from the novels Alice in Wonderland and Anna Karenina from the OPUS collection (Tiedemann, 2012),3 and two texts from a multilingual news website.4 These texts were segmented into clauses manually by one of the authors. English and German segments were also aligned manually. In addition, we use two documents (Sophie’s world and economy) from the Smultron corpus (Volk et al., 2010). We split the English part of Smultron into clauses using SPADE (Soricut and Marcu, 2003), and the German part using a syntax-based discourse segmenter for German.5 The Smultron corpus provides alignments on a token-/phrase-level, but these phrases do not necessarily match the clause segmentation. To align clau"
W15-2702,P09-1076,0,0.0132507,"tific background or Contrastive/comparative statements. The key difference is that AZ is a genre-specific approach, and DMs are relevant for most written text genres. Liakata et al. (2013) use AZ to improve summarization of scientific articles, showing that subdocument structure can indeed be useful in downstream applications. Santini (2006) also employ types over passages of text (called simply “text types”), with labels that are partially similar to Smith’s DMs. These text types are then used as building blocks for automatic web genre classification. Palmer and Friedrich (2014), inspired by Webber (2009), investigate the distribution of SE types for various genres of text. In contrast, here we study the distribution of SE types per DM. Re3 Annotating discourse modes This exploratory study takes the first steps toward computational treatment of DMs, resulting in the first corpus of texts labeled with DMs. 1 This metadata is available for each document pair. http://statmt.org/wmt13 3 http://opus.lingfil.uu.se 4 http://globalvoicesonline.org 5 Publication in preparation. 2 13 source text/excerpt OPUS: novels OPUS: novels Europarl Europarl Europarl GlobalVoices GlobalVoices NewsCommentary NewsCom"
W15-2702,D13-1070,0,0.0196569,"follows the paragraph breaks in the original source texts. Related Work. Unlike genre, a notion of text type for entire documents, DMs are an aspect of sub-document structure, and thus are similar to approaches such as Argumentative Zoning (AZ) (Teufel, 2010). AZ analyzes scientific research articles according to the rhetorical functions of their text passages, identifying and labeling passages with categories like General scientific background or Contrastive/comparative statements. The key difference is that AZ is a genre-specific approach, and DMs are relevant for most written text genres. Liakata et al. (2013) use AZ to improve summarization of scientific articles, showing that subdocument structure can indeed be useful in downstream applications. Santini (2006) also employ types over passages of text (called simply “text types”), with labels that are partially similar to Smith’s DMs. These text types are then used as building blocks for automatic web genre classification. Palmer and Friedrich (2014), inspired by Webber (2009), investigate the distribution of SE types for various genres of text. In contrast, here we study the distribution of SE types per DM. Re3 Annotating discourse modes This expl"
W15-2702,C14-1054,0,0.02692,"from a large-scale experimental study; (b) SEs mainly correspond to each other in parallel text, and a large part of the mismatches are systematic; (c) the DM annotation task can be performed intuitively with reasonable agreement; and (d) the annotated DMs show the predicted differences in the distributions of SE types. 1 Introduction There are complex and interwoven relationships between the nature of a text – whether construed as genre, register, text type, discourse mode, or something else – and the linguistic characteristics of the text (Werlich, 1975; Smith, 2003; Biber and Conrad, 2009; Passonneau et al., 2014, among others). Furthermore, these relationships involve phenomena at different levels, from lexical to structural, and from semantic to functional/pragmatic. In this paper we investigate correspondences across two levels of linguistic analysis, for phenomena spanning semantics and discourse, for two languages (English and German). 12 Proceedings of the EMNLP 2015 Workshop on Linking Models of Lexical, Sentential and Discourse-level Semantics, pages 12–21, c Lisboa, Portugal, 18 September 2015. 2015 Association for Computational Linguistics. lated work for the other subparts of the study is d"
W15-2705,W13-0501,0,0.259632,"Missing"
W15-2705,baker-etal-2010-modality,0,0.228442,"Missing"
W15-2705,P10-1005,1,0.829769,"res using sequences of POS tags of the verbal complex, following Loaiciga et al. (2014). The boolean features perfect and progressive indicate the respective grammatical aspect; voice indicates active or passive voice. We extract person and number of the subject and the noun type (common, proper, pronoun). Person is identified via personal pronoun features, and the other features are extracted from POS tags. The countability of the noun is obtained from the Celex database (Baayen et al., 1996). Lexical semantic features for the subject NP are extracted from WordNet (Fellbaum, 1999). Following Reiter and Frank (2010), we take the most frequent sense of the noun in WN (subject sense0), add the direct hypernym of this sense, the direct hypernym of that hypernym, etc., resulting in features subject sense[1, 2, 3]. We also extract the top sense in the WN hierarchy subject sense top (e.g. entity) and the WN lexical filename (e.g. person). LA: Lexical aspectual class. Verbs can be used in a dynamic or stative sense, e.g. I ate an apple vs. I like apples (Vendler, 1957). The lexical aspect of a verb in context influences modal sense in some cases. In contrast to (4.a), for example, where the eventive verb return"
W15-2705,ruppenhofer-rehbein-2012-yes,0,0.169135,"to lexical, proposition-level, and discourse-level semantic factors. Besides improved classification performance, especially for difficult sense distinctions, closer examination of interpretable feature sets allows us to obtain a better understanding of relevant semantic and contextual factors in modal sense classification. 1 (2) a. Geez, Buddha must be so annoyed! (epistemic – possibility) b. We must have clear European standards. (deontic – permission/request) c. She can’t even read them. (dynamic – ability) Modal sense tagging is typically framed as a supervised classification task, as in Ruppenhofer and Rehbein (2012), who manually annotated the modal verbs must, may, can, could, shall and should in the MPQA corpus of Wiebe et al. (2005). The obtained data set comprises 1340 instances. Maximum entropy classifiers trained on this data yield accuracies from 68.7 to 93.5 for the different lexical classifier models. While these accuracies seem high, we note a strong distributional bias in their data set. Due to the small data set size (200-600 instances per modal verb) and its distributional bias, classifiers trained on this corpus are prone to overfitting and hardly beat the majority baseline. Indeed, none of"
W15-2705,J00-4004,0,0.118118,"Missing"
W15-2705,P02-1033,0,0.0343069,". Our annotation reliability is largely comparable. (5) a. He may be home by now. (possibility) b. You may enter this building. (permission) c. May you live 100 years. (wish) (6) a. Vielleicht ist er schon zu Hause. M AYBE IS HE ALREADY AT HOME . b. Es ist gestattet, das Geb¨aude zu betreten. I T IS PERMITTED THE BUILDING TO ENTER c. Hoffentlich werden Sie 100 Jahre. H OPEFULLY BECOME YOU 100 YEARS Capitalizing on the paraphrasing capacity of such expressions, we apply a semi-supervised cross-lingual projection approach, similar to prior work in annotation projection (Yarowsky and Ngai, 2001; Diab and Resnik, 2002): (i) we select a seed set of cross-lingual sense indicating paraphrases, (ii) we extract modal verbs in context that are in direct alignment with one of the seed expressions in word-aligned parallel corpora, and (iii) we project the label of the sense-indicating paraphrase to the aligned modal verb. Experimental setup and annotation scheme. German is our source language, and we project into English. We adopt R&R’s annotation scheme, which is grounded in Kratzer’s modal senses epistemic, deontic and dynamic. While R&R add the novel categories conditional, concessive and optative,1 we subsume t"
W15-2705,tiedemann-2012-parallel,0,0.0413797,"ntic Features for Modal Sense Classification In our work we expand the feature inventory used for modal sense classification to incorporate semantic factors at various levels. An overview of our semantic features is given in Table 1. We define specific feature groups for focused experimental investigation in Section 5. Feature extraction is performed using Stanford’s CoreNLP (Manning et al., 2014) and Stanford parser (Klein and Manning, 2002) to obtain syntactic dependencies. Seed selection. The seeds were manually selected from PPDB (Ganitkevitch et al., 2013) and parallel corpora from OPUS (Tiedemann, 2012). The major criterion, besides frequency of occurrence, was non-ambiguity regarding the modal 2 We split permission and request to make the task more accessible and merged them to deontic later. 3 Cohen’s Kappa, Cohen (1960) 1 Examples: “Should anyone call, please take a message” (conditional), “But, fool though he may be, he is powerful” (concessive), and “Long may she live!” (optative). (R&R) 46 VB: Lexical features of the embedded verb. The embedded verb in the scope of the modal plays an important role in determining modal sense. For instance, with the embedded verb fly in (7.a), we prefer"
W15-2705,P14-2085,1,0.915633,"Missing"
W15-2705,N13-1092,0,0.0399079,"Missing"
W15-2705,J86-2003,0,0.331126,"lex database (Baayen et al., 1996). Lexical semantic features for the subject NP are extracted from WordNet (Fellbaum, 1999). Following Reiter and Frank (2010), we take the most frequent sense of the noun in WN (subject sense0), add the direct hypernym of this sense, the direct hypernym of that hypernym, etc., resulting in features subject sense[1, 2, 3]. We also extract the top sense in the WN hierarchy subject sense top (e.g. entity) and the WN lexical filename (e.g. person). LA: Lexical aspectual class. Verbs can be used in a dynamic or stative sense, e.g. I ate an apple vs. I like apples (Vendler, 1957). The lexical aspect of a verb in context influences modal sense in some cases. In contrast to (4.a), for example, where the eventive verb return triggers the deontic sense, perfect aspect in (10) coerces the clause to stative, triggering the epistemic sense of must. (10) The prisoners must have returned their weapons. TVA: Tense/voice/grammatical aspect features. These features capture tense and grammatical aspect of the embedded verb complex. LA below notes how grammatical aspect influences modal sense. At the same time, tense is an important factor for modal sense disambiguation. (10) clear"
W15-2705,2005.mtsummit-papers.11,0,0.00980443,"Missing"
W15-2705,N01-1026,0,0.0188503,"rent modal verbs was 0.67. Our annotation reliability is largely comparable. (5) a. He may be home by now. (possibility) b. You may enter this building. (permission) c. May you live 100 years. (wish) (6) a. Vielleicht ist er schon zu Hause. M AYBE IS HE ALREADY AT HOME . b. Es ist gestattet, das Geb¨aude zu betreten. I T IS PERMITTED THE BUILDING TO ENTER c. Hoffentlich werden Sie 100 Jahre. H OPEFULLY BECOME YOU 100 YEARS Capitalizing on the paraphrasing capacity of such expressions, we apply a semi-supervised cross-lingual projection approach, similar to prior work in annotation projection (Yarowsky and Ngai, 2001; Diab and Resnik, 2002): (i) we select a seed set of cross-lingual sense indicating paraphrases, (ii) we extract modal verbs in context that are in direct alignment with one of the seed expressions in word-aligned parallel corpora, and (iii) we project the label of the sense-indicating paraphrase to the aligned modal verb. Experimental setup and annotation scheme. German is our source language, and we project into English. We adopt R&R’s annotation scheme, which is grounded in Kratzer’s modal senses epistemic, deontic and dynamic. While R&R add the novel categories conditional, concessive and"
W15-2705,P14-5010,0,0.00398233,"Kratzer’s modal senses epistemic, deontic and dynamic. While R&R add the novel categories conditional, concessive and optative,1 we subsume the former two as cases of epistemic and optative as a subtype of deontic. 4 Semantic Features for Modal Sense Classification In our work we expand the feature inventory used for modal sense classification to incorporate semantic factors at various levels. An overview of our semantic features is given in Table 1. We define specific feature groups for focused experimental investigation in Section 5. Feature extraction is performed using Stanford’s CoreNLP (Manning et al., 2014) and Stanford parser (Klein and Manning, 2002) to obtain syntactic dependencies. Seed selection. The seeds were manually selected from PPDB (Ganitkevitch et al., 2013) and parallel corpora from OPUS (Tiedemann, 2012). The major criterion, besides frequency of occurrence, was non-ambiguity regarding the modal 2 We split permission and request to make the task more accessible and merged them to deontic later. 3 Cohen’s Kappa, Cohen (1960) 1 Examples: “Should anyone call, please take a message” (conditional), “But, fool though he may be, he is powerful” (concessive), and “Long may she live!” (opt"
W15-2705,loaiciga-etal-2014-english,0,\N,Missing
W16-0535,Q13-1032,0,0.296091,"trained on randomly-sampled data. An interesting outcome of the experiments by Zesch et al. (2015) is the highly-variable performance of classifiers trained on a fixed number of randomly-sampled instances; out of 1000 random trials, the difference between the best and worst runs is considerable. The highly-variable performance of systems trained on randomly-selected data underscores the need for more informed ways of selecting training data. A related approach to human effort reduction is the use of clustering in a computer-assisted scoring setting (Brooks et al., 2014; Horbach et al., 2014; Basu et al., 2013). In these studies, answers are clustered through automatic means, and teachers then label clusters of similar answers instead of individual student responses. The approaches vary in whether human grading is actual or simulated, and also with respect to how many items in each cluster graders inspect. The value of clustering in these works has no connection with supervised classification, but rather lies in the ability it gives teachers both to reduce their grading effort and to discover subgroups of responses that may correspond to new correct solutions or to common student misconceptions. In"
W16-0535,P11-2002,0,0.0311756,"labeled items, and in steps of 100 until all data has been labeled. Note that this approach does not directly fit into the general AL framework. In AL, the set of labeled data is increased incrementally, while with this approach a larger training set is not necessarily a proper superset of a smaller training set but may contain different items. 4.3 Seed selection The seed set in AL is the initial set of labeled data used to train the first classifier and thus to initialize the item selection process. The quality of the seeds has been shown to play an important role for the performance of AL (Dligach and Palmer, 2011). Here we consider two ways of selecting seed set items. First is the baseline of (a) random seed selection. Random selection can be suboptimal when it produces unbalanced seed sets, especially if one or more classes are not contained in the seed data at all or – in the worst case – the seed set contains only items of one class. Some of the ASAP data sets are very skewed (e.g. questions 5 and 6, see Table 1) and carry a high risk of producing such suboptimal seeds via random selection. The second condition is (b) equal seed selection, in which seed items are selected such that all classes are"
W16-0535,W15-0610,0,0.0549432,"ers. This task of automatically assessing such student responses (as opposed to, e.g., gap-filling questions) is widely referred to as short answer scoring (SAS), and automatic methods have been developed for tasks ranging from science assessments to reading comprehension, and for such varied domains as foreign language learning, citizenship exams, and more traditional classrooms. Most existing automatic SAS systems rely on supervised machine learning techniques that require large amounts of manually labeled training data to achieve reasonable performance, and recent work (Zesch et al., 2015; Heilman and Madnani, 2015; One solution to this problem is to develop generic scoring models which do not require re-training in order to do assessment for a new data set (i.e. a new question/prompt plus responses). Meurers et al. (2011) apply such a model for scoring short reading comprehension responses written by learners of German. This system crucially relies on features which directly compare learner responses to target answers provided as part of the data set, and the responses are mostly one sentence or phrase. In this work we are concerned with longer responses generated from a wide range of prompt types, fro"
W16-0535,horbach-etal-2014-finding,1,0.822295,"ment over a classifier trained on randomly-sampled data. An interesting outcome of the experiments by Zesch et al. (2015) is the highly-variable performance of classifiers trained on a fixed number of randomly-sampled instances; out of 1000 random trials, the difference between the best and worst runs is considerable. The highly-variable performance of systems trained on randomly-selected data underscores the need for more informed ways of selecting training data. A related approach to human effort reduction is the use of clustering in a computer-assisted scoring setting (Brooks et al., 2014; Horbach et al., 2014; Basu et al., 2013). In these studies, answers are clustered through automatic means, and teachers then label clusters of similar answers instead of individual student responses. The approaches vary in whether human grading is actual or simulated, and also with respect to how many items in each cluster graders inspect. The value of clustering in these works has no connection with supervised classification, but rather lies in the ability it gives teachers both to reduce their grading effort and to discover subgroups of responses that may correspond to new correct solutions or to common student"
W16-0535,P14-5010,0,0.00622721,"Missing"
W16-0535,W11-2401,0,0.243515,"anging from science assessments to reading comprehension, and for such varied domains as foreign language learning, citizenship exams, and more traditional classrooms. Most existing automatic SAS systems rely on supervised machine learning techniques that require large amounts of manually labeled training data to achieve reasonable performance, and recent work (Zesch et al., 2015; Heilman and Madnani, 2015; One solution to this problem is to develop generic scoring models which do not require re-training in order to do assessment for a new data set (i.e. a new question/prompt plus responses). Meurers et al. (2011) apply such a model for scoring short reading comprehension responses written by learners of German. This system crucially relies on features which directly compare learner responses to target answers provided as part of the data set, and the responses are mostly one sentence or phrase. In this work we are concerned with longer responses generated from a wide range of prompt types, from questions asking for list-like responses to those seeking coherent multi-sentence texts (details in Section 3). For such questions, there is generally no single best response, and thus the system cannot rely on"
W16-0535,W15-0623,0,0.0202006,"t responses. The approaches vary in whether human grading is actual or simulated, and also with respect to how many items in each cluster graders inspect. The value of clustering in these works has no connection with supervised classification, but rather lies in the ability it gives teachers both to reduce their grading effort and to discover subgroups of responses that may correspond to new correct solutions or to common student misconceptions. In the domain of educational applications, AL has recently been used in two different settings where reduction of human annotation cost is desirable. Niraula and Rus (2015) use AL to judge the quality of automatically generated gap-filling questions, and Dronen et al. (2014) explore AL for essay scoring using sampling methods for linear regression. To the best of our knowledge, AL has not previously been applied to automatic SAS. Our task is most closely related to studies such as Figueroa et al. (2012), where summaries of clinical texts are classified using AL, or Tong and Koller (2002) and McCallum and Nigam (1998), both of which label newspaper texts with topics. Unlike most other previous AL studies, text classification tasks need AL methods that are suitabl"
W16-0535,W15-0615,0,0.592623,"he workload of teachers. This task of automatically assessing such student responses (as opposed to, e.g., gap-filling questions) is widely referred to as short answer scoring (SAS), and automatic methods have been developed for tasks ranging from science assessments to reading comprehension, and for such varied domains as foreign language learning, citizenship exams, and more traditional classrooms. Most existing automatic SAS systems rely on supervised machine learning techniques that require large amounts of manually labeled training data to achieve reasonable performance, and recent work (Zesch et al., 2015; Heilman and Madnani, 2015; One solution to this problem is to develop generic scoring models which do not require re-training in order to do assessment for a new data set (i.e. a new question/prompt plus responses). Meurers et al. (2011) apply such a model for scoring short reading comprehension responses written by learners of German. This system crucially relies on features which directly compare learner responses to target answers provided as part of the data set, and the responses are mostly one sentence or phrase. In this work we are concerned with longer responses generated from a wide"
W16-2015,E14-4008,0,0.0371815,"t random. For example, tunnel (n.) occurs 38,967 times in the corpus and tunnel (v.) 2,949 times. Through downsampling, the vectors both for tunnel (n.) and for tunnel (v.) are constructed from 2,949 instances. 3.4 Frequency. We assess the frequency hypothesis by directly comparing the number of nominal and verbal corpus occurrences of a target lemma. Semantic Specificity. We operationalize the semantic specificity hypothesis by applying measures of information content to distributional representations. This follows the example of two recent studies. In the context of hyponymy identification, Santus et al. (2014) proposed entropy as a measure of the semantic specificity S(w) of a word w, via its distributional, L1-normalized vector w. ~ Entropy is supposed to be inversely correlated with semantic specificity, since higher specificity corresponds to more restrictions on context, which means lower entropy, defined as Vector Representations To measure semantic specificity, we perform a distributional analysis which represents each conversion case with two 10,000-dimensional bag-ofwords vectors: one for the verb and one for the noun, relying on automatic PoS tags (cf. Section 3.2). The dimensions correspo"
W16-2015,P13-2078,0,0.125647,"ith the number of conversion instances for each class. The two predictors agree more often than they disagree, but among the disagreements, the strong asymmetry between N-to-V (top) and V-to-N (below) is readily visible. Table 1: Accuracies for predicting the direction of derivation, presented by gold standard direction (all results on downsampled space) Predictor Intercept ∆ entropy ∆ KL divergence ∆ log frequency Estimate Std. Err. Sig. 0.15 -2.08 -2.22 1.74 0.06 0.18 0.18 0.09 ** *** *** *** Table 2: Logistic regression model (∆ always denotes noun value minus verb value) The second study (Herbelot and Ganesalingam, 2013) was directly interested in measuring specificity and proposed to equate it with the KullbackLeibler (KL) divergence D between a word vector w ~ and the “neutral” vector ~n: X w ~i S(w) = D(w||~ ~ n) = w ~ i · log (2) ~ni i where ~n is the prior distribution over all words. We compute ~n as the centroid of approximately 28,000 word vectors in our vector space; the vectors are computed according to the procedure in Section 3.3. In this approach, higher KL divergence corresponds to higher semantic specificity. We note that the entropy and KL divergence values are closely related mathematically a"
W16-2803,E12-1027,0,0.0146156,", c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with respect to clause type? 2. Do particular clause types correlate with particular elements in the argumenta"
W16-2803,W13-2707,0,0.0262467,"text type of the passage. For more detail see Section 2. Furthermore, SE types are recognizable (and annotatable) through a combination of linguistic features of the clause and its main verb, and models have recently been released for their automatic classification (Friedrich et al., 2016). Our approach is the first we know of to link clause type to argumentative structure, although features of the verb have been widely used in previous work for classifying argumentative vs. nonargumentative sentences. For example, Moens et al. (2007) include verb lemmas and modal auxiliaries as features, and Florou et al. (2013) find that, for Greek web texts related to public policy issues, tense and mood features of verbal constructions are helpful for determining the role of the sentences within argumentative structures. Our analysis is performed on German texts. Taking the argumentative microtext corpus (Peldszus and Stede, 2015a) as a set of prototypical argumentative text passages, we annotated each clause with its SE type (Section 3).1 In this way we are able to investigate which SE types are most prevalent in argumentative texts and, further, to link the clause Argumentative texts have been thoroughly analyze"
W16-2803,W14-4921,1,0.94697,"texts and clause types Maria Becker, Alexis Palmer, and Anette Frank Leibniz ScienceCampus Empirical Linguistics and Computational Language Modeling Department of Computational Linguistics, Heidelberg University Institute of German Language, Mannheim {mbecker,palmer,frank}@cl.uni-heidelberg.de Abstract for classifying the argumentative functions served by premises. Specifically, this is an empirical investigation of the semantic types of clauses found in argumentative text passages, using the inventory of clause types developed by Smith (2003) and extended in later work (Palmer et al., 2007; Friedrich and Palmer, 2014). Situation entity (SE) types describe how clauses behave in discourse, and as such they are aspectual rather than ontological categories. Individual clauses of text evoke different types of situations (for example, states, events, generics, or habituals), and the situations evoked in a text passage are linked to the text type of the passage. For more detail see Section 2. Furthermore, SE types are recognizable (and annotatable) through a combination of linguistic features of the clause and its main verb, and models have recently been released for their automatic classification (Friedrich et a"
W16-2803,D15-1110,0,0.402966,"s the first we know of to link clause type to argumentative structure, although features of the verb have been widely used in previous work for classifying argumentative vs. nonargumentative sentences. For example, Moens et al. (2007) include verb lemmas and modal auxiliaries as features, and Florou et al. (2013) find that, for Greek web texts related to public policy issues, tense and mood features of verbal constructions are helpful for determining the role of the sentences within argumentative structures. Our analysis is performed on German texts. Taking the argumentative microtext corpus (Peldszus and Stede, 2015a) as a set of prototypical argumentative text passages, we annotated each clause with its SE type (Section 3).1 In this way we are able to investigate which SE types are most prevalent in argumentative texts and, further, to link the clause Argumentative texts have been thoroughly analyzed for their argumentative structure, and recent efforts aim at their automatic classification. This work investigates linguistic properties of argumentative texts and text passages in terms of their semantic clause types. We annotate argumentative texts with Situation Entity (SE) classes, which combine notion"
W16-2803,P15-1123,0,0.171345,"16 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with respect to clause type? 2. Do particular clause types correlate with particular elements in the argumentation graphs? 3. Do particular clause types cor"
W16-2803,P10-1005,1,0.677539,"ment Mining, pages 21–30, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with respect to clause type? 2. Do particular clause types correlate with particular e"
W16-2803,P16-1166,1,0.854985,"Palmer, 2014). Situation entity (SE) types describe how clauses behave in discourse, and as such they are aspectual rather than ontological categories. Individual clauses of text evoke different types of situations (for example, states, events, generics, or habituals), and the situations evoked in a text passage are linked to the text type of the passage. For more detail see Section 2. Furthermore, SE types are recognizable (and annotatable) through a combination of linguistic features of the clause and its main verb, and models have recently been released for their automatic classification (Friedrich et al., 2016). Our approach is the first we know of to link clause type to argumentative structure, although features of the verb have been widely used in previous work for classifying argumentative vs. nonargumentative sentences. For example, Moens et al. (2007) include verb lemmas and modal auxiliaries as features, and Florou et al. (2013) find that, for Greek web texts related to public policy issues, tense and mood features of verbal constructions are helpful for determining the role of the sentences within argumentative structures. Our analysis is performed on German texts. Taking the argumentative mi"
W16-2803,J00-4004,0,0.0488191,"o clauses is discussed in Section 3.2. 21 Proceedings of the 3rd Workshop on Argument Mining, pages 21–30, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with re"
W16-2803,W15-2702,1,0.842794,"nden nicht den Arzte-Status” tragen. (GEN) It doesn’t matter after all \ that those who administer the treatment don’t have ’doctor status’. Table 3: Sample microtext (micro b010), both German and English versions, with SE labels. generic, and (c) whether the clause is habitual (a pattern of occurrences), episodic (a fixed number of occurrences), or static (an attribute, characteristic, or quality). Using these feature values in a decision tree has been shown to improve human agreement on the SE type annotation task (Friedrich and Palmer, 2014). annotators and one expert annotator. Following Mavridou et al. (2015), with a modified and translated version of an existing SE annotation manual,5 student annotators were trained on a set of longer texts from different genres, automatically segmented as described above: fiction (47 segments), reports (42 segments), TED talks (50 segAnnotators and annotator training. Each text was annotated by two trained (but novice) student 5 www.coli.uni-saarland.de/projects/ sitent/page.php?id=resources 25 ments), and commentary (127 segments). Inter-annotator agreement. We compute agreement separately for SE type and for the three features introduced above, as shown in Tab"
W16-2803,W13-2313,0,0.0265232,"ust 7-12, 2016. 2016 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with respect to clause type? 2. Do particular clause types correlate with particular elements in the argumentation graphs? 3. Do"
W16-2803,L16-1167,0,0.0836072,"characterized by an above-average number of G ENERIC S ENTENCES 28 linguistically. Due to the small dataset, our results can be interpreted solely as tendencies which have to be confirmed by more extensive studies in the future. Nonetheless there is some evidence that the observed tendencies can be deployed for automatic recognition and fine-grained classification of argumentative text passages. In addition to the ongoing annotation work which will give us more data to analyze, we intend to cross-match SE types with the newly-available discourse structure annotations for the microtext corpus (Stede et al., 2016). We would additionally explore the role of modal verbs within this intersection of SE type and argument structure status. The end goal of this investigation, of course, is to deploy automatically-labeled SE types as features for argument mining. Mit der BA-Arbeit kann man jedoch die Interessen und die Fachkenntnisse besonders gut zeigen. Schlielich ist man nicht in jedem Fach sehr gut. (G ENERIC S ENTENCE) (Translation: With a BA dissertation one can, however, demonstrate interests and subject matter expertise particularly well. After all one doesn’t excel in every subject.) Es bleibt jedoch"
W16-2803,C14-1002,0,0.0210569,"Missing"
W16-2803,J86-2003,0,0.0978753,"curring events, such as habits of individuals. 4. G ENERIC S ENTENCE (GEN): Birds can fly. / Scientific papers make arguments. 5. G ENERALIZING S ENTENCE (GS): Fei travels to India every year. Theoretical background The phrase situation entity refers to the fact that clauses of text evoke situations within a discourse. For example, the previous sentence describes two situations: (i) the meaning of situation entity, and (ii) what clauses of text do, in general. The second situation is embedded as part of the first. Notions related to SE type have been widely studied in theoretical linguistics (Vendler, 1957; Verkuyl, 1972; Dowty, 1979; Smith, 1991; Asher, 1993; Carlson and Pelletier, 1995, among others) and have The next category of SE types is broadly referred to as Abstract Entities. This type of clause presents semantic content in a manner that draws attention to its epistemic status. We focus primarily on a small subset of constructions - factive and propositional predicates with clausal complements. Of course a wide range of linguistic constructions can be used to convey such information, and to address them all would require a comprehensive treament of subjective language. In the examples"
W16-2803,P07-1113,1,0.890231,"Missing"
W16-2803,zarcone-lenci-2008-computational,0,0.0175346,"Section 3.2. 21 Proceedings of the 3rd Workshop on Argument Mining, pages 21–30, c Berlin, Germany, August 7-12, 2016. 2016 Association for Computational Linguistics type (via SE label) to the argumentation graphs provided with the microtext corpus (Section 4). We additionally provide SE annotations for a number of non-argumentative (or at least only partially argumentative) German texts, in order to contrast SE type distributions across these text types. This annotation case study addresses the following questions: seen growing interest in computational linguistics (Siegel and McKeown, 2000; Zarcone and Lenci, 2008; Herbelot and Copestake, 2009; Reiter and Frank, 2010; Costa and Branco, 2012; Nedoluzhko, 2013; Friedrich and Pinkal, 2015, for example). 2.1 We directly follow Friedrich and Palmer (2014) for the inventory of SE types, described below. The inventory of SE types starts with states and events, including a subtype of events for attributional statements. R EPORT-type clauses such as (3) do not necessarily refer to an actual event of speaking but rather indicate a source of information. 1. Do argumentative text passages differ from non-argumentative text passages with respect to clause type? 2."
W16-2803,W13-2324,0,\N,Missing
W16-2803,J17-3005,0,\N,Missing
W17-3014,N03-1033,0,0.00665333,"hough, comes with a giant caveat: the corpus was collected precisely to investigate pejorative nominalizations. To test this hypothesis in a less-biased setting, we build a second corpus of instances extracted automatically from Twitter using twarc.5 To move closer to automatic detection of abusive language, LF is assigned by an automatic part-of-speech tagger, and annotation is done via crowd-sourcing. Tagger selection. Before selecting a tagger, we investigated several different options, running all taggers with default settings: the standard English POS tagging model from Stanford CoreNLP (Toutanova et al., 2003); the GATE Twitter POS tagger (Derczynski et al., 2013);8 and TweetNLP (Owoputi et al., 2013).9 For a small test suite (57 instances), TweetNLP with its native tag set gave the best results for the four target words, looking at both adjectival and nominal uses. The TweetNLP tag set is a coarse-grained tag set extended with Twitter-specific tags for elements like hashtags and URLs. Of interest for our task are the tags N for nouns and A for adjectives. 4.1 The corpus This corpus has two subsets: T W TARGETS and T W O PEN. Both subcorpora were de-duplicated using twarc’s built-in utilities. NomC"
W17-3014,Y13-1035,0,0.119749,"Missing"
W17-3014,R13-1026,0,0.0193066,"cted precisely to investigate pejorative nominalizations. To test this hypothesis in a less-biased setting, we build a second corpus of instances extracted automatically from Twitter using twarc.5 To move closer to automatic detection of abusive language, LF is assigned by an automatic part-of-speech tagger, and annotation is done via crowd-sourcing. Tagger selection. Before selecting a tagger, we investigated several different options, running all taggers with default settings: the standard English POS tagging model from Stanford CoreNLP (Toutanova et al., 2003); the GATE Twitter POS tagger (Derczynski et al., 2013);8 and TweetNLP (Owoputi et al., 2013).9 For a small test suite (57 instances), TweetNLP with its native tag set gave the best results for the four target words, looking at both adjectival and nominal uses. The TweetNLP tag set is a coarse-grained tag set extended with Twitter-specific tags for elements like hashtags and URLs. Of interest for our task are the tags N for nouns and A for adjectives. 4.1 The corpus This corpus has two subsets: T W TARGETS and T W O PEN. Both subcorpora were de-duplicated using twarc’s built-in utilities. NomCatcher: tag correction for nominalizations. A number of"
W17-3014,W12-2103,0,0.699871,"nt a tool called NomCatcher to correct these mistaggings, and find that the same tool is effective for identifying new adjectives subject to transformation via nominalization into abusive language. 1 1. I think the Mexicans are going to end up loving Donald Trump. [cited in Liberman (2016b)] Introduction 2. I think the Mexican people are going to end up loving Donald Trump. [constructed] Detection of abusive language tends to focus on identification of key words and character sequences that indicate expression of strongly negative attitudes toward individuals or groups of people (for example, Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Nobata et al., 2016). Some key words, such as racial or ethnic slurs, are highly effective predictors, while other key words may signal contentious topics rather than actual abusive language. This second type of key word is semantically flexible. Depending on the context of individual occurrences, these words may be In (2), Mexican is an adjective modifying the noun people; in (1), Mexican has been nominalized.2 This paper presents work in progress exploring the utility of linguistic form (i.e. particular syntactic constructions, discussed in Section 2) for automatical"
W17-3014,N16-2013,0,0.0669678,"o correct these mistaggings, and find that the same tool is effective for identifying new adjectives subject to transformation via nominalization into abusive language. 1 1. I think the Mexicans are going to end up loving Donald Trump. [cited in Liberman (2016b)] Introduction 2. I think the Mexican people are going to end up loving Donald Trump. [constructed] Detection of abusive language tends to focus on identification of key words and character sequences that indicate expression of strongly negative attitudes toward individuals or groups of people (for example, Warner and Hirschberg, 2012; Waseem and Hovy, 2016; Nobata et al., 2016). Some key words, such as racial or ethnic slurs, are highly effective predictors, while other key words may signal contentious topics rather than actual abusive language. This second type of key word is semantically flexible. Depending on the context of individual occurrences, these words may be In (2), Mexican is an adjective modifying the noun people; in (1), Mexican has been nominalized.2 This paper presents work in progress exploring the utility of linguistic form (i.e. particular syntactic constructions, discussed in Section 2) for automatically identifying this mor"
W17-3014,N13-1039,0,0.0258841,"Missing"
W17-4910,L16-1397,0,0.0129253,"10) per document. LIWC - word norms The English Linguistic Inquiry and Word Count (Tausczik and Pennebaker, 2010; Pennebaker et al., 2015) contains 6400 words and stems (and select emoticons). The German version (Wolf et al., 2008) includes 7510 entries. It provides a hierarchical annotation of 68 linguistic and psychological categories, e.g. the word cried is part of five categories: sadness, negative emotion, overall affect, verbs and past focus. Hence, all five will be counted for the document. Preprocessing for feature extraction. We use the Julie Lab Segmenter (Tokenization, Sentences) (Hahn et al., 2016) and the RF-Tagger (Lemmatization, STTS pos-tags, SMOR morphological tags) (Schmid and Laws, 2008). Part-of-Speech Tags We use the StuttgartTübingen Tagset (STTS)12 with 47 tags. Connectives The HDK list of 312 discourse connectives is described in (Versley, 2010). We match connectives by iterating over word n-grams. For connectives with a gap (""entweder ... oder""), we look ahead 20 words. If the right side element returns a match, we include the whole (gapped) connective, otherwise we only count the left side. Verb Classes German verb classes are retrieved from GermaNet (Hamp et al., 1997; He"
W17-4910,W97-0802,0,0.0697257,"Missing"
W17-4910,henrich-hinrichs-2010-gernedit,0,0.01136,"6) and the RF-Tagger (Lemmatization, STTS pos-tags, SMOR morphological tags) (Schmid and Laws, 2008). Part-of-Speech Tags We use the StuttgartTübingen Tagset (STTS)12 with 47 tags. Connectives The HDK list of 312 discourse connectives is described in (Versley, 2010). We match connectives by iterating over word n-grams. For connectives with a gap (""entweder ... oder""), we look ahead 20 words. If the right side element returns a match, we include the whole (gapped) connective, otherwise we only count the left side. Verb Classes German verb classes are retrieved from GermaNet (Hamp et al., 1997; Henrich and Hinrichs, 2010). The GermaNet scheme contains 9,382 unique verbs (including particles and affixes) across 15 groups, where a verb can be a member of several groups, totaling 15,327 tokens. For each verb token that we detect, we count every relevant class with equal weight. Stopwords Our German stopword list is by solariz,13 containing 996 inflected wordforms (of which 4 do not occur in the corpus). 12 http://www.ims.uni-stuttgart.de/ forschung/ressourcen/lexika/TagSets/ stts-table.html 13 https://solariz.de/de/deutsche_ stopwords.htm 78 Feat.set Features POS BASIC SELECT FULL POS3 LDA200 Part-of-speech tags"
W17-4910,ide-etal-2008-masc,0,0.0285543,"Missing"
W17-4910,C94-2174,0,0.343388,"y of discourse. We operationalize the core of Steen’s theory for corpus design, modeling register variation top-down with prototype semantics to develop a comparative genre taxonomy (Section 3.1). The taxonomy is then implemented in a general genre/register corpus of contemporary German. (Section 3.2). We employ a wide range of stylistic features for the classification of text, (Section 3.3), going beyond previous computational stylometric genre analysis, that has often relied on shallow lexicosyntactic patterns such as function words, surface forms, character / part-of-speech n-grams, etc., (Karlgren and Cutting, 1994; Stamatatos et al., 2000a,b; Koppel et al., 2003; Gries and Shaoul, 2011; Sharoff, 2007; Kanaris and Stamatatos, 2007), extending beyond linguistically motivated features (Biber and Conrad, 2009; Santini, 2005) with a fine-grained morphology, psycholinguistic word norms, and topic models. With these feature sets and corpus, we perform supervised genre classification (Section 4), showing that results remain high and stable across shifting sets of categories. A major problem with relying on surface level features - particularly lexical features - is that they tend to capture topical information"
W17-4910,kupietz-etal-2010-german,0,0.0405857,"Missing"
W17-4910,C08-1098,0,0.0333692,"we focus on the engineering of fine grained morpho-syntactic features, linguistic lexicons, word norms and surface forms. To test the topic sensitivity of genre, we also generate topic distributions for documents with Latent Dirichlet Allocation (LDA). Our feature-groups are organized as a nested hierarchy, shown in Table 5. Individual features are described below. We implemented our feature extraction pipeline in python. Each feature is normalized relative to its own individual group (e.g. pos with pos) per text. Before classification, we use the sklearn StandardScaler. Morphology RF-Tagger (Schmid and Laws, 2008) annotates very fine-grained (767) morphological tags according to SMOR (Schmid et al., 2004). One such feature would be “VFIN.Full.2.Pl.Pres.Ind” for a full finite verb in second person plural present indicative. WWN word norms Lahl et al. (2009) crowdsourced ratings for concreteness, valency and arousal for 2,654 German nouns. We draw the mean for each dimension (0 - 10) per document. LIWC - word norms The English Linguistic Inquiry and Word Count (Tausczik and Pennebaker, 2010; Pennebaker et al., 2015) contains 6400 words and stems (and select emoticons). The German version (Wolf et al., 20"
W17-4910,sharoff-etal-2010-web,0,0.0248296,"1964/79) and the LancasterOslo/Bergen (LOB) corpus (Johansson et al., 1978). Both were sampled according to library classification systems and contain relatively small numbers of samples distributed over various genre classes of different granularity. MASC1 (Ide, 2008) also balances genre classes over number of tokens. To analyze the variety across texts, one needs to arbitrarily split its documents (to 2000 tokens, as done by Passonneau (2014)). There is an extensive collection of web-genre corpora (Santini, 2007; Meyer zu Eißen and Stein, 2004; Rehm et al., 2008; Santini et al., 2010). See Sharoff and Markert (2010) for an overview and the success of Char-4-bin features (later found to be unstable by Petrenz and Webber (2011)). GECCo is a bilingual (English-German) corpus for investigating cohesion across register (LapshinovaKoltunski et al., 2012). It is not freely available. The DWDS ’Kernkorpus’ for super-genre of 20th century texts is also not available.2 The Hierarchical Genre Corpus (HGC) (Stubbe and Ringlstetter, 2007) and the British National Corpus (BNC) 3 are designed to offer representative samples across different genres in a hierarchical fashion. However, the categories of HGC are not clear-"
W17-4910,C14-1054,0,0.0131323,"tising legal texts ... inform persuade instruct ... beyond web-genre, or are freely available. Early examples for English include the Brown corpus (Francis and Kuˇcera, 1964/79) and the LancasterOslo/Bergen (LOB) corpus (Johansson et al., 1978). Both were sampled according to library classification systems and contain relatively small numbers of samples distributed over various genre classes of different granularity. MASC1 (Ide, 2008) also balances genre classes over number of tokens. To analyze the variety across texts, one needs to arbitrarily split its documents (to 2000 tokens, as done by Passonneau (2014)). There is an extensive collection of web-genre corpora (Santini, 2007; Meyer zu Eißen and Stein, 2004; Rehm et al., 2008; Santini et al., 2010). See Sharoff and Markert (2010) for an overview and the success of Char-4-bin features (later found to be unstable by Petrenz and Webber (2011)). GECCo is a bilingual (English-German) corpus for investigating cohesion across register (LapshinovaKoltunski et al., 2012). It is not freely available. The DWDS ’Kernkorpus’ for super-genre of 20th century texts is also not available.2 The Hierarchical Genre Corpus (HGC) (Stubbe and Ringlstetter, 2007) and"
W17-4910,J00-4001,0,0.248916,"alize the core of Steen’s theory for corpus design, modeling register variation top-down with prototype semantics to develop a comparative genre taxonomy (Section 3.1). The taxonomy is then implemented in a general genre/register corpus of contemporary German. (Section 3.2). We employ a wide range of stylistic features for the classification of text, (Section 3.3), going beyond previous computational stylometric genre analysis, that has often relied on shallow lexicosyntactic patterns such as function words, surface forms, character / part-of-speech n-grams, etc., (Karlgren and Cutting, 1994; Stamatatos et al., 2000a,b; Koppel et al., 2003; Gries and Shaoul, 2011; Sharoff, 2007; Kanaris and Stamatatos, 2007), extending beyond linguistically motivated features (Biber and Conrad, 2009; Santini, 2005) with a fine-grained morphology, psycholinguistic word norms, and topic models. With these feature sets and corpus, we perform supervised genre classification (Section 4), showing that results remain high and stable across shifting sets of categories. A major problem with relying on surface level features - particularly lexical features - is that they tend to capture topical information. Petrenz and Webber (201"
W17-4910,C00-2117,0,0.173788,"alize the core of Steen’s theory for corpus design, modeling register variation top-down with prototype semantics to develop a comparative genre taxonomy (Section 3.1). The taxonomy is then implemented in a general genre/register corpus of contemporary German. (Section 3.2). We employ a wide range of stylistic features for the classification of text, (Section 3.3), going beyond previous computational stylometric genre analysis, that has often relied on shallow lexicosyntactic patterns such as function words, surface forms, character / part-of-speech n-grams, etc., (Karlgren and Cutting, 1994; Stamatatos et al., 2000a,b; Koppel et al., 2003; Gries and Shaoul, 2011; Sharoff, 2007; Kanaris and Stamatatos, 2007), extending beyond linguistically motivated features (Biber and Conrad, 2009; Santini, 2005) with a fine-grained morphology, psycholinguistic word norms, and topic models. With these feature sets and corpus, we perform supervised genre classification (Section 4), showing that results remain high and stable across shifting sets of categories. A major problem with relying on surface level features - particularly lexical features - is that they tend to capture topical information. Petrenz and Webber (201"
W17-4910,J11-2004,0,0.28741,"ary classification systems and contain relatively small numbers of samples distributed over various genre classes of different granularity. MASC1 (Ide, 2008) also balances genre classes over number of tokens. To analyze the variety across texts, one needs to arbitrarily split its documents (to 2000 tokens, as done by Passonneau (2014)). There is an extensive collection of web-genre corpora (Santini, 2007; Meyer zu Eißen and Stein, 2004; Rehm et al., 2008; Santini et al., 2010). See Sharoff and Markert (2010) for an overview and the success of Char-4-bin features (later found to be unstable by Petrenz and Webber (2011)). GECCo is a bilingual (English-German) corpus for investigating cohesion across register (LapshinovaKoltunski et al., 2012). It is not freely available. The DWDS ’Kernkorpus’ for super-genre of 20th century texts is also not available.2 The Hierarchical Genre Corpus (HGC) (Stubbe and Ringlstetter, 2007) and the British National Corpus (BNC) 3 are designed to offer representative samples across different genres in a hierarchical fashion. However, the categories of HGC are not clear-cut and focus on web-genre. The BNC is highly imbalanced. Some additional related work uses features from system"
W19-4211,P15-2044,0,0.0249278,"data sets. For some languages we have access to the entire Bible, and for others only the New Testament (NT). This introduces discrepancies in the amount of data used to train embeddings from language to language, as the Old Testament is much longer than the New Testament. The Bible is a natural source of parallel data, as it is available (either in whole or in parts) in Related Work The core idea of using the Bible as parallel data in low-resource settings is largely inspired by previous work. The Bible has been used as a means of alignment for cross-lingual projection, both for POS tagging (Agic et al., 2015) and for dependency parsing (Agic et al., 2016), as well as for base noun-phrase bracketing, named-entity tagging, and morphological analysis (Yarowsky et al., 2001) with promising results. Peters et al. (2018) introduce ELMo embeddings, contextual word embeddings which incorporate character-level information using a CNN. 88 Shared task language UD Belarusian-HSE UD Breton-KEB UD Galician-CTG UD Galician-TreeGal UD Gothic-PROIEL UD Norwegian-Nynorsk UD Norwegian-NynorskLIA UD Upper Sorbian-UFAL ISO code bel bre glg glg got nno nno hsb UD Armenian-ArmTDP UD Irish-IDT UD Persian-Seraji hy ga fa"
W19-4211,Q18-1034,0,0.0133102,"t, as follows: The model description consists of two parts: the core model, for morphological analysis, and two non-core components, for part-of-speech tagging and lemmatization. 5.1 Core model: morphological analysis Our core system addresses the task of morphological analysis with minimal supervision from labeled training data. The approach exploits parallel data in the form of a multilingual Bible corpus. 5.1.1 Contextual embeddings for every Bible Prior research has shown embedded word vector representations are capable of capturing contextual nuances in meaning beyond one sense per word (Arora et al., 2018, for example). Because context variance is an important factor affecting morphological analysis, we use ELMo embeddings (Peters et al., 2018) as our base representation. As a first step, we train separate ELMo models on each of the Bible translations in our corpus. For each language, we hold out four books (Mark, Ephesians, 2 Timothy, and Hebrews) for model evaluation and train on all remaining books. Models are trained at the sentence level, using default parameter settings and following recommendations from the AllenNLP bilm-tf repository.3 5.1.2 min W k W xi − zi k2 i=1 5.1.3 Inducing vect"
W19-4211,L18-1293,0,0.0318498,"publican, computer, or NASA). The limited domain of the text offers both advantages and disadvantages. On the one hand, much of the vocabulary found in the shared task evaluation data does not occur in the Bible. Using embeddings trained on the Bible, then, results in an extremely large number of out-of-vocabulary tokens at test time. On the other, the semantic territory covered by the embedding spaces varies remarkably little from language to language, increasing the feasibility of aligning embedding spaces across multiple languages. 4.2 4.3 Sigmorphon data We use the provided training data (McCarthy et al., 2018) primarily to train a part-of-speech tagger and lemmatizer for each shared task data set, and the provided test data is used to evaluate the system. We use portions of the training data for three other purposes: a) to build contrasting sets of words for each UniMorph tag (Section 5.1.3); b) to build lists of UniMorph tags relevant for each language; and c) to create a simple baseline for the two languages for which we have no Bible, proxy language or otherwise. Proxy languages In order to do morphological analysis for a given language, our method requires access to a digitally-available versio"
W19-4211,E17-2002,0,0.031925,"ists of UniMorph tags relevant for each language; and c) to create a simple baseline for the two languages for which we have no Bible, proxy language or otherwise. Proxy languages In order to do morphological analysis for a given language, our method requires access to a digitally-available version of at least portions of the Bible for that language. At the time the model was developed, we did not have access to Bibles for all shared task languages. For each missing 2 In an early experiment, we investigated the effectiveness of similarity measures over language vectors (Malaviya et al., 2017; Littell et al., 2017) for selecting proxy languages. The results were mixed, so we opted for expert selection of proxy languages instead. Lin et al. (2019) discusses some of the issues involved. 89 5 Models losing the encoded information about contextual polysemy, for which ELMo is particularly useful. Schuster et al. (2019) propose using contextfree anchors to align contextually-dependent embedding spaces (such as ELMo). We propose instead to calculate translation matrices at the verse level, computing the representation for each verse as the unweighted average of its constituent contextual word embeddings. First"
W19-4211,D17-1268,0,0.0226196,"n 5.1.3); b) to build lists of UniMorph tags relevant for each language; and c) to create a simple baseline for the two languages for which we have no Bible, proxy language or otherwise. Proxy languages In order to do morphological analysis for a given language, our method requires access to a digitally-available version of at least portions of the Bible for that language. At the time the model was developed, we did not have access to Bibles for all shared task languages. For each missing 2 In an early experiment, we investigated the effectiveness of similarity measures over language vectors (Malaviya et al., 2017; Littell et al., 2017) for selecting proxy languages. The results were mixed, so we opted for expert selection of proxy languages instead. Lin et al. (2019) discusses some of the issues involved. 89 5 Models losing the encoded information about contextual polysemy, for which ELMo is particularly useful. Schuster et al. (2019) propose using contextfree anchors to align contextually-dependent embedding spaces (such as ELMo). We propose instead to calculate translation matrices at the verse level, computing the representation for each verse as the unweighted average of its constituent contextual"
W19-4211,N19-1155,0,0.0176752,"raining and development datasets. Given our interest in methods which reduce the need for large labeled corpora and supervised learning, we additionally implemented some simple heuristics based on previously-generated morpheme tags. For example, a word is given a higher probability of being tagged as a verb if it has a modal, tense, or other conjugative tag already assigned to it (e.g., V.PTCP or PRS). These heuristics were designed to be entirely language-neutral, generalizing to the full set of test languages. As a final task, we perform lemma generation using a joint neural model following Malaviya et al. (2019)’s proposed method. The joint model consists of a simple LSTM-based tagger to recover the morphology of a sentence and a sequence-tosequence model with hard attention mechanism as a lemmatizer. The lemmatization model trains over words and their morphological information Intuitively, this method is plausible because words, their inflected forms, synonyms, and closely related terms tend to occur in tight clusters in embedding spaces. Therefore, subtracting the embedding for the PL tag from the embedding for the should not produce a close match in English, since the plural tag is never associate"
W19-4211,W18-6011,0,0.0251408,"publican, computer, or NASA). The limited domain of the text offers both advantages and disadvantages. On the one hand, much of the vocabulary found in the shared task evaluation data does not occur in the Bible. Using embeddings trained on the Bible, then, results in an extremely large number of out-of-vocabulary tokens at test time. On the other, the semantic territory covered by the embedding spaces varies remarkably little from language to language, increasing the feasibility of aligning embedding spaces across multiple languages. 4.2 4.3 Sigmorphon data We use the provided training data (McCarthy et al., 2018) primarily to train a part-of-speech tagger and lemmatizer for each shared task data set, and the provided test data is used to evaluate the system. We use portions of the training data for three other purposes: a) to build contrasting sets of words for each UniMorph tag (Section 5.1.3); b) to build lists of UniMorph tags relevant for each language; and c) to create a simple baseline for the two languages for which we have no Bible, proxy language or otherwise. Proxy languages In order to do morphological analysis for a given language, our method requires access to a digitally-available versio"
W19-4211,N13-1090,0,0.642624,"le 1 http://www.wycliffe.net/statistics 87 Proceedings of the 16th Workshop on Computational Research in Phonetics, Phonology, and Morphology, pages 87–94 c Florence, Italy. August 2, 2019 2019 Association for Computational Linguistics Both of these properties - sensitivity to context and the ability to capture sub-word information make contextual embeddings suitable for the task at hand. In order to make embeddings useful across languages, we need a method for aligning embedding spaces across languages. Ruder et al. (2017) provide an excellent survey of methods for aligning embedding spaces. Mikolov et al. (2013a) introduce a translation matrix for aligning embeddings spaces in different languages and show how this is useful for machine translation purposes. We adopt this approach to do alignment at the verse level. Alignment with contextual embeddings is more complicated, since the embeddings are dynamic by their very nature (different across different contexts). In order to align these dynamic embeddings, Schuster et al. (2019) introduce a number of methods, however they all require either a supervised dictionary for each language, or access to the MUSE framework for alignment, neither of which we"
W19-4211,N18-1202,0,0.53073,"kark@ifi.uio.no ** Department of Computer Science and Engineering, University of North Texas {bradfordaiken,suleymanolcaypolat}@my.unt.edu, rodney.nielsen@unt.edu Abstract without annotated training data. We propose a model to perform morphosyntactic annotation for any language with a translation of the Bible. According to Wycliffe1 , there are currently 683 languages in the world which contain a translation of the entire Bible, and an additional 1534 languages for which the entire New Testament, and sometimes other sections, are available. We train contextual word representations using ELMo (Peters et al., 2018) and align embedding spaces for language pairs using Bible verse numbers as an alignment signal. We then compute vector representations for UniMorph tags in English and project those representations into the target language. The projected morpheme tag embeddings are used to identify morphological features and label tokens in context with UniMorph tags. We give a system overview in Section 2, with more detailed model descriptions in Section 5. The system’s performance is currently poor; we outline known limitations and make some suggestions for improvement. This paper presents the UNT HiLT+Ling"
W19-4211,N19-1162,0,0.145404,"ul across languages, we need a method for aligning embedding spaces across languages. Ruder et al. (2017) provide an excellent survey of methods for aligning embedding spaces. Mikolov et al. (2013a) introduce a translation matrix for aligning embeddings spaces in different languages and show how this is useful for machine translation purposes. We adopt this approach to do alignment at the verse level. Alignment with contextual embeddings is more complicated, since the embeddings are dynamic by their very nature (different across different contexts). In order to align these dynamic embeddings, Schuster et al. (2019) introduce a number of methods, however they all require either a supervised dictionary for each language, or access to the MUSE framework for alignment, neither of which we assume in our work. The UniMorph 2.0 data-set (Kirov et al., 2018) provides resources for morphosyntactic analysis across 111 different languages. The work described here uses the tag set from UniMorph. data, verse-aligned; and b) roughly twenty words per from the training data per UniMorph tag. Once this model has been developed, it can be applied for a new language with no annotated training data for the task; the only d"
W19-4211,H01-1035,0,0.139291,"used to train embeddings from language to language, as the Old Testament is much longer than the New Testament. The Bible is a natural source of parallel data, as it is available (either in whole or in parts) in Related Work The core idea of using the Bible as parallel data in low-resource settings is largely inspired by previous work. The Bible has been used as a means of alignment for cross-lingual projection, both for POS tagging (Agic et al., 2015) and for dependency parsing (Agic et al., 2016), as well as for base noun-phrase bracketing, named-entity tagging, and morphological analysis (Yarowsky et al., 2001) with promising results. Peters et al. (2018) introduce ELMo embeddings, contextual word embeddings which incorporate character-level information using a CNN. 88 Shared task language UD Belarusian-HSE UD Breton-KEB UD Galician-CTG UD Galician-TreeGal UD Gothic-PROIEL UD Norwegian-Nynorsk UD Norwegian-NynorskLIA UD Upper Sorbian-UFAL ISO code bel bre glg glg got nno nno hsb UD Armenian-ArmTDP UD Irish-IDT UD Persian-Seraji hy ga fa UD Akkadian-PISANDUB UD Sanskrit-UFAL akk san Proxy language Russian Irish Gaelic Portuguese Portuguese Icelandic Icelandic Icelandic Czech Dialect Eastern Armenian"
