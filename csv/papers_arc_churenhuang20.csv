2021.sigdial-1.26,{S}cikit-talk: A toolkit for processing real-world conversational speech data,2021,-1,-1,3,1,1502,andreas liesenfeld,Proceedings of the 22nd Annual Meeting of the Special Interest Group on Discourse and Dialogue,0,"We present Scikit-talk, an open-source toolkit for processing collections of real-world conversational speech in Python. First of its kind, the toolkit equips those interested in studying or modeling conversations with an easy-to-use interface to build and explore large collections of transcriptions and annotations of talk-in-interaction. Designed for applications in speech processing and Conversational AI, Scikit-talk provides tools to custom-build datasets for tasks such as intent prototyping, dialog flow testing, and conversation design. Its \textit{preprocessor} module comes with several pre-built interfaces for common transcription formats, which aim to make working across multiple data sources more accessible. The \textit{explorer} module provides a collection of tools to explore and analyse this data type via string matching and unsupervised machine learning techniques. Scikit-talk serves as a platform to collect and connect different transcription formats and representations of talk, enabling the user to quickly build multilingual datasets of varying detail and granularity. Thus, the toolkit aims to make working with authentic conversational speech data in Python more accessible and to provide the user with comprehensive options to work with representations of talk in appropriate detail for any downstream task. For the latest updates and information on currently supported languages and language resources, please refer to: https://pypi.org/project/scikit-talk/"
2021.semeval-1.70,{P}oly{U} {CBS}-Comp at {S}em{E}val-2021 Task 1: Lexical Complexity Prediction ({LCP}),2021,-1,-1,6,1,1824,rong xiang,Proceedings of the 15th International Workshop on Semantic Evaluation (SemEval-2021),0,"In this contribution, we describe the system presented by the PolyU CBS-Comp Team at the Task 1 of SemEval 2021, where the goal was the estimation of the complexity of words in a given sentence context. Our top system, based on a combination of lexical, syntactic, word embeddings and Transformers-derived features and on a Gradient Boosting Regressor, achieves a top correlation score of 0.754 on the subtask 1 for single words and 0.659 on the subtask 2 for multiword expressions."
2021.rocling-1.51,{ROCLING}-2021 Shared Task: Dimensional Sentiment Analysis for Educational Texts,2021,-1,-1,4,0,2441,liangchih yu,Proceedings of the 33rd Conference on Computational Linguistics and Speech Processing (ROCLING 2021),0,"This paper presents the ROCLING 2021 shared task on dimensional sentiment analysis for educational texts which seeks to identify a real-value sentiment score of self-evaluation comments written by Chinese students in the both valence and arousal dimensions. Valence represents the degree of pleasant and unpleasant (or positive and negative) feelings, and arousal represents the degree of excitement and calm. Of the 7 teams registered for this shared task for two-dimensional sentiment analysis, 6 submitted results. We expected that this evaluation campaign could produce more advanced dimensional sentiment analysis techniques for the educational domain. All data sets with gold standards and scoring script are made publicly available to researchers."
2021.findings-acl.258,Modeling the Influence of Verb Aspect on the Activation of Typical Event Locations with {BERT},2021,-1,-1,4,0,120,won cho,Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021,0,None
2021.econlp-1.5,Is Domain Adaptation Worth Your Investment? Comparing {BERT} and {F}in{BERT} on Financial Tasks,2021,-1,-1,4,0,2442,bo peng,Proceedings of the Third Workshop on Economics and Natural Language Processing,0,"With the recent rise in popularity of Transformer models in Natural Language Processing, research efforts have been dedicated to the development of domain-adapted versions of BERT-like architectures. In this study, we focus on FinBERT, a Transformer model trained on text from the financial domain. By comparing its performances with the original BERT on a wide variety of financial text processing tasks, we found continual pretraining from the original model to be the more beneficial option. Domain-specific pretraining from scratch, conversely, seems to be less effective."
2020.starsem-1.4,Automatic Learning of Modality Exclusivity Norms with Crosslingual Word Embeddings,2020,-1,-1,4,1,180,emmanuele chersoni,Proceedings of the Ninth Joint Conference on Lexical and Computational Semantics,0,"Collecting modality exclusivity norms for lexical items has recently become a common practice in psycholinguistics and cognitive research. However, these norms are available only for a relatively small number of languages and often involve a costly and time-consuming collection of ratings. In this work, we aim at learning a mapping between word embeddings and modality norms. Our experiments focused on crosslingual word embeddings, in order to predict modality association scores by training on a high-resource language and testing on a low-resource one. We ran two experiments, one in a monolingual and the other one in a crosslingual setting. Results show that modality prediction using off-the-shelf crosslingual embeddings indeed has moderate-to-high correlations with human ratings even when regression algorithms are trained on an English resource and tested on a completely unseen language."
2020.paclic-1.4,Sketching the {E}nglish Translations of Kum{\\=a}raj{\\=\\i}va{'}s The Diamond Sutra: A Comparison of Individual Translators and Translation Teams,2020,-1,-1,3,0,762,xi chen,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.12,Language change in Report on the Work of the Government by Premiers of the People{'}s {R}epublic of {C}hina,2020,-1,-1,2,0,15810,renkui hou,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.19,"Predicting gender and age categories in {E}nglish conversations using lexical, non-lexical, and turn-taking features",2020,-1,-1,4,1,1502,andreas liesenfeld,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.36,Sensorimotor Enhanced Neural Network for Metaphor Detection,2020,-1,-1,5,1,15875,mingyu wan,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.37,A Parallel Corpus-driven Approach to Bilingual Oenology Term Banks: How Culture Differences Influence Wine Tasting Terms,2020,-1,-1,4,0,15798,vincent wang,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.38,Corpus-based Comparison of Verbs of Separation {``}Qie{''} and {``}{G}e{''},2020,-1,-1,2,0,15877,ngain wu,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.52,Marking Trustworthiness with Near Synonyms: A Corpus-based Study of {``}Renwei{''} and {``}Yiwei{''} in {C}hinese,2020,-1,-1,2,0,12925,bei li,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.paclic-1.65,{A}bstract {M}eaning {R}epresentation for {MWE}: A study of the mapping of aspectuality based on {M}andarin light verb jiayi,2020,-1,-1,3,0,15927,lu lu,"Proceedings of the 34th Pacific Asia Conference on Language, Information and Computation",0,None
2020.lrec-1.14,Affection Driven Neural Networks for Sentiment Analysis,2020,-1,-1,6,1,1824,rong xiang,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Deep neural network models have played a critical role in sentiment analysis with promising results in the recent decade. One of the essential challenges, however, is how external sentiment knowledge can be effectively utilized. In this work, we propose a novel affection-driven approach to incorporating affective knowledge into neural network models. The affective knowledge is obtained in the form of a lexicon under the Affect Control Theory (ACT), which is represented by vectors of three-dimensional attributes in Evaluation, Potency, and Activity (EPA). The EPA vectors are mapped to an affective influence value and then integrated into Long Short-term Memory (LSTM) models to highlight affective terms. Experimental results show a consistent improvement of our approach over conventional LSTM models by 1.0{\%} to 1.5{\%} in accuracy on three large benchmark datasets. Evaluations across a variety of algorithms have also proven the effectiveness of leveraging affective terms for deep model enhancement."
2020.lrec-1.700,Are Word Embeddings Really a Bad Fit for the Estimation of Thematic Fit?,2020,-1,-1,5,1,180,emmanuele chersoni,Proceedings of the 12th Language Resources and Evaluation Conference,0,"While neural embeddings represent a popular choice for word representation in a wide variety of NLP tasks, their usage for thematic fit modeling has been limited, as they have been reported to lag behind syntax-based count models. In this paper, we propose a complete evaluation of count models and word embeddings on thematic fit estimation, by taking into account a larger number of parameters and verb roles and introducing also dependency-based embeddings in the comparison. Our results show a complex scenario, where a determinant factor for the performance seems to be the availability to the model of reliable syntactic information for building the distributional representations of the roles."
2020.lrec-1.701,{C}iron: a New Benchmark Dataset for {C}hinese Irony Detection,2020,-1,-1,7,1,1824,rong xiang,Proceedings of the 12th Language Resources and Evaluation Conference,0,"Automatic Chinese irony detection is a challenging task, and it has a strong impact on linguistic research. However, Chinese irony detection often lacks labeled benchmark datasets. In this paper, we introduce Ciron, the first Chinese benchmark dataset available for irony detection for machine learning models. Ciron includes more than 8.7K posts, collected from Weibo, a micro blogging platform. Most importantly, Ciron is collected with no pre-conditions to ensure a much wider coverage. Evaluation on seven different machine learning classifiers proves the usefulness of Ciron as an important resource for Chinese irony detection."
2020.figlang-1.16,Using Conceptual Norms for Metaphor Detection,2020,-1,-1,7,1,15875,mingyu wan,Proceedings of the Second Workshop on Figurative Language Processing,0,"This paper reports a linguistically-enriched method of detecting token-level metaphors for the second shared task on Metaphor Detection. We participate in all four phases of competition with both datasets, i.e. Verbs and AllPOS on the VUA and the TOFEL datasets. We use the modality exclusivity and embodiment norms for constructing a conceptual representation of the nodes and the context. Our system obtains an F-score of 0.652 for the VUA Verbs track, which is 5{\%} higher than the strong baselines. The experimental results across models and datasets indicate the salient contribution of using modality exclusivity and modality shift information for predicting metaphoricity."
2020.aacl-main.26,"Comparing Probabilistic, Distributional and Transformer-Based Models on Logical Metonymy Interpretation",2020,-1,-1,5,1,927,giulia rambelli,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"In linguistics and cognitive science, Logical metonymies are defined as type clashes between an event-selecting verb and an entity-denoting noun (e.g. The editor finished the article), which are typically interpreted by inferring a hidden event (e.g. reading) on the basis of contextual cues. This paper tackles the problem of logical metonymy interpretation, that is, the retrieval of the covert event via computational methods. We compare different types of models, including the probabilistic and the distributional ones previously introduced in the literature on the topic. For the first time, we also tested on this task some of the recent Transformer-based models, such as BERT, RoBERTa, XLNet, and GPT-2. Our results show a complex scenario, in which the best Transformer-based models and some traditional distributional models perform very similarly. However, the low performance on some of the testing datasets suggests that logical metonymy is still a challenging phenomenon for computational modeling."
2020.aacl-main.84,Sina {M}andarin Alphabetical Words:A Web-driven Code-mixing Lexical Resource,2020,-1,-1,4,1,1824,rong xiang,Proceedings of the 1st Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics and the 10th International Joint Conference on Natural Language Processing,0,"Mandarin Alphabetical Word (MAW) is one indispensable component of Modern Chinese that demonstrates unique code-mixing idiosyncrasies influenced by language exchanges. Yet, this interesting phenomenon has not been properly addressed and is mostly excluded from the Chinese language system. This paper addresses the core problem of MAW identification and proposes to construct a large collection of MAWs from Sina Weibo (SMAW) using an automatic web-based technique which includes rule-based identification, informatics-based extraction, as well as Baidu search engine validation. A collection of 16,207 qualified SMAWs are obtained using this technique along with an annotated corpus of more than 200,000 sentences for linguistic research and applicable inquiries."
W19-5521,{P}oly{U}{\\_}{CBS}-{CFA} at the {F}in{SBD} Task: Sentence Boundary Detection of Financial Data with Domain Knowledge Enhancement and Bilingual Training,2019,-1,-1,10,1,15875,mingyu wan,Proceedings of the First Workshop on Financial Technology and Natural Language Processing,0,None
W19-3312,Distributional Semantics Meets Construction Grammar. towards a Unified Usage-Based Model of Grammar and Meaning,2019,0,0,4,1,927,giulia rambelli,Proceedings of the First International Workshop on Designing Meaning Representations,0,"In this paper, we propose a new type of semantic representation of Construction Grammar that combines constructions with the vector representations used in Distributional Semantics. We introduce a new framework, Distributional Construction Grammar, where grammar and meaning are systematically modeled from language use, and finally, we discuss the kind of contributions that distributional models can provide to CxG representation from a linguistic and cognitive perspective."
W19-1401,A Report on the Third {V}ar{D}ial Evaluation Campaign,2019,-1,-1,9,0,622,marcos zampieri,"Proceedings of the Sixth Workshop on {NLP} for Similar Languages, Varieties and Dialects",0,"In this paper, we present the findings of the Third VarDial Evaluation Campaign organized as part of the sixth edition of the workshop on Natural Language Processing (NLP) for Similar Languages, Varieties and Dialects (VarDial), co-located with NAACL 2019. This year, the campaign included five shared tasks, including one task re-run {--} German Dialect Identification (GDI) {--} and four new tasks {--} Cross-lingual Morphological Analysis (CMA), Discriminating between Mainland and Taiwan variation of Mandarin Chinese (DMT), Moldavian vs. Romanian Cross-dialect Topic identification (MRC), and Cuneiform Language Identification (CLI). A total of 22 teams submitted runs across the five shared tasks. After the end of the competition, we received 14 system description papers, which are published in the VarDial workshop proceedings and referred to in this report."
Y18-2001,How do non-tastes taste? A corpus-based study on {C}hinese people{'}s perception of spicy and numbing food,2018,-1,-1,3,0,27458,sicong dong,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 25th Joint Workshop on Linguistics and Language Processing",0,None
Y18-2004,Food-Related Sentiment Analysis for {C}antonese,2018,-1,-1,3,0,23830,natalia klyueva,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 25th Joint Workshop on Linguistics and Language Processing",0,None
Y18-2006,Pleasing to the Mouth of Pleasant Personality: A corpus-based study of conceptualization of desserts in online {C}hinese food reviews,2018,-1,-1,2,0,27459,yin zhong,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation: 25th Joint Workshop on Linguistics and Language Processing",0,None
Y18-1084,Facilitating and Blocking Conditions of Haplology: A comparative study of {H}ong {K}ong {C}antonese and {T}aiwan {M}andarin,2018,-1,-1,3,0,27576,sam wong,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
Y18-1091,Semantic Transparency of Radicals in {C}hinese Characters: An Ontological Perspective,2018,0,2,2,0,15881,yike yang,"Proceedings of the 32nd Pacific Asia Conference on Language, Information and Computation",0,None
W18-6220,Dual Memory Network Model for Biased Product Review Classification,2018,0,4,5,1,16437,yunfei long,"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis",0,"In sentiment analysis (SA) of product reviews, both user and product information are proven to be useful. Current tasks handle user profile and product information in a unified model which may not be able to learn salient features of users and products effectively. In this work, we propose a dual user and product memory network (DUPMN) model to learn user profiles and product reviews using separate memory networks. Then, the two representations are used jointly for sentiment prediction. The use of separate models aims to capture user profiles and product information more effectively. Compared to state-of-the-art unified prediction models, the evaluations on three benchmark datasets, IMDB, Yelp13, and Yelp14, show that our dual learning model gives performance gain of 0.6{\%}, 1.2{\%}, and 0.9{\%}, respectively. The improvements are also deemed very significant measured by \textit{p-values}."
L18-1394,Annotating {C}hinese Light Verb Constructions according to {PARSEME} guidelines,2018,0,0,4,1,16507,menghan jiang,Proceedings of the Eleventh International Conference on Language Resources and Evaluation ({LREC} 2018),0,None
Y17-1011,Stylometric Studies based on Tone and Word Length Motifs,2017,-1,-1,2,0,15810,renkui hou,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",0,None
Y17-1042,Multi-dimensional Meanings of Subjective Adverbs - Case Study of {M}andarin {C}hinese Adverb Pianpian,2017,0,0,3,0,31111,mi zhou,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",0,None
Y17-1047,"Lexicalization, Separation and transitivity: A comparative study of {M}andarin {VO} compound Variations",2017,-1,-1,2,1,16507,menghan jiang,"Proceedings of the 31st Pacific Asia Conference on Language, Information and Computation",0,None
K17-1006,Leveraging Eventive Information for Better Metaphor Detection and Classification,2017,25,0,4,1,27577,ihsuan chen,Proceedings of the 21st Conference on Computational Natural Language Learning ({C}o{NLL} 2017),0,"Metaphor detection has been both challenging and rewarding in natural language processing applications. This study offers a new approach based on eventive information in detecting metaphors by leveraging the Chinese writing system, which is a culturally bound ontological system organized according to the basic concepts represented by radicals. As such, the information represented is available in all Chinese text without pre-processing. Since metaphor detection is another culturally based conceptual representation, we hypothesize that sub-textual information can facilitate the identification and classification of the types of metaphoric events denoted in Chinese text. We propose a set of syntactic conditions crucial to event structures to improve the model based on the classification of radical groups. With the proposed syntactic conditions, the model achieves a performance of 0.8859 in terms of F-scores, making 1.7{\%} of improvement than the same classifier with only Bag-of-word features. Results show that eventive information can improve the effectiveness of metaphor detection. Event information is rooted in every language, and thus this approach has a high potential to be applied to metaphor detection in other languages."
I17-2043,Fake News Detection Through Multi-Perspective Speaker Profiles,2017,0,37,5,1,16437,yunfei long,Proceedings of the Eighth International Joint Conference on Natural Language Processing (Volume 2: Short Papers),0,"Automatic fake news detection is an important, yet very challenging topic. Traditional methods using lexical features have only very limited success. This paper proposes a novel method to incorporate speaker profiles into an attention based LSTM model for fake news detection. Speaker profiles contribute to the model in two ways. One is to include them in the attention model. The other includes them as additional input data. By adding speaker profiles such as party affiliation, speaker title, location and credit history, our model outperforms the state-of-the-art method by 14.5{\%} in accuracy using a benchmark fake news detection dataset. This proves that speaker profiles provide valuable information to validate the credibility of news articles."
D17-1048,A Cognition Based Attention Model for Sentiment Analysis,2017,20,15,5,1,16437,yunfei long,Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing,0,"Attention models are proposed in sentiment analysis because some words are more important than others. However,most existing methods either use local context based text information or user preference information. In this work, we propose a novel attention model trained by cognition grounded eye-tracking data. A reading prediction model is first built using eye-tracking data as dependent data and other features in the context as independent data. The predicted reading time is then used to build a cognition based attention (CBA) layer for neural sentiment analysis. As a comprehensive model, We can capture attentions of words in sentences as well as sentences in documents. Different attention mechanisms can also be incorporated to capture other aspects of attentions. Evaluations show the CBA based method outperforms the state-of-the-art local context based attention methods significantly. This brings insight to how cognition grounded data can be brought into NLP tasks."
Y16-3018,Transitivity in Light Verb Variations in {M}andarin {C}hinese {--} A Comparable Corpus-based Statistical Approach,2016,1,2,3,1,16507,menghan jiang,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Posters",0,None
Y16-3021,The Synaesthetic and Metaphorical Uses of å³ wei {`}taste{'} in {C}hinese Buddhist Suttas,2016,-1,-1,2,1,33277,jiajuan xiong,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Posters",0,None
Y16-3024,The use of body part terms in {T}aiwan and {C}hina: Analyzing è¡ xue {`}blood{'} and éª¨ gu {`}bone{'} in {C}hinese {G}igaword v. 2.0,2016,-1,-1,2,1,15619,renfeng duann,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Posters",0,None
Y16-2021,Testing {APS}yn against Vector Cosine on Similarity Estimation,2016,29,11,4,1,181,enrico santus,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Oral Papers",0,"In Distributional Semantic Models (DSMs), Vector Cosine is widely used to estimate similarity between word vectors, although this measure was noticed to suffer from several shortcomings. The recent literature has proposed other methods which attempt to mitigate such biases. In this paper, we intend to investigate APSyn, a measure that computes the extent of the intersection between the most associated contexts of two target words, weighting it by context relevance. We evaluated this metric in a similarity estimation task on several popular test sets, and our results show that APSyn is in fact highly competitive, even with respect to the results reported in the literature for word embeddings. On top of it, APSyn addresses some of the weaknesses of Vector Cosine, performing well also on genuine similarity estimation."
Y16-1006,Endurant vs Perdurant: Ontological Motivation for Language Variations,2016,-1,-1,1,1,1504,churen huang,"Proceedings of the 30th Pacific Asia Conference on Language, Information and Computation: Keynote Speeches and Invited Talks",0,None
W16-5411,Selective Annotation of Sentence Parts: Identification of Relevant Sub-sentential Units,2016,0,0,3,1,33484,ge xu,Proceedings of the 12th Workshop on {A}sian Language Resources ({ALR}12),0,"Many NLP tasks involve sentence-level annotation yet the relevant information is not encoded at sentence level but at some relevant parts of the sentence. Such tasks include but are not limited to: sentiment expression annotation, product feature annotation, and template annotation for Q{\&}A systems. However, annotation of the full corpus sentence by sentence is resource intensive. In this paper, we propose an approach that iteratively extracts frequent parts of sentences for annotating, and compresses the set of sentences after each round of annotation. Our approach can also be used in preparing training sentences for binary classification (domain-related vs. noise, subjectivity vs. objectivity, etc.), assuming that sentence-type annotation can be predicted by annotation of the most relevant sub-sentences. Two experiments are performed to test our proposal and evaluated in terms of time saved and agreement of annotation."
L16-1360,A lexicon of perception for the identification of synaesthetic metaphors in corpora,2016,0,2,2,0,35097,francesca lievers,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Synaesthesia is a type of metaphor associating linguistic expressions that refer to two different sensory modalities. Previous studies, based on the analysis of poetic texts, have shown that synaesthetic transfers tend to go from the lower toward the higher senses (e.g., sweet music vs. musical sweetness). In non-literary language synaesthesia is rare, and finding a sufficient number of examples manually would be too time-consuming. In order to verify whether the directionality also holds for conventional synaesthesia found in non-literary texts, an automatic procedure for the identification of instances of synaesthesia is therefore highly desirable. In this paper, we first focus on the preliminary step of this procedure, that is, the creation of a controlled lexicon of perception. Next, we present the results of a small pilot study that applies the extraction procedure to English and Italian corpus data."
L16-1636,Database of {M}andarin Neighborhood Statistics,2016,0,1,3,1,35348,karl neergaard,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In the design of controlled experiments with language stimuli, researchers from psycholinguistic, neurolinguistic, and related fields, require language resources that isolate variables known to affect language processing. This article describes a freely available database that provides word level statistics for words and nonwords of Mandarin, Chinese. The featured lexical statistics include subtitle corpus frequency, phonological neighborhood density, neighborhood frequency, and homophone density. The accompanying word descriptors include pinyin, ascii phonetic transcription (sampa), lexical tone, syllable structure, dominant PoS, and syllable, segment and pinyin lengths for each phonological word. It is designed for researchers particularly concerned with language processing of isolated words and made to accommodate multiple existing hypotheses concerning the structure of the Mandarin syllable. The database is divided into multiple files according to the desired search criteria: 1) the syllable segmentation schema used to calculate density measures, and 2) whether the search is for words or nonwords. The database is open to the research community at https://github.com/karlneergaard/Mandarin-Neighborhood-Statistics."
L16-1722,Nine Features in a Random Forest to Learn Taxonomical Semantic Relations,2016,3,12,5,1,181,enrico santus,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"ROOT9 is a supervised system for the classification of hypernyms, co-hyponyms and random words that is derived from the already introduced ROOT13 (Santus et al., 2016). It relies on a Random Forest algorithm and nine unsupervised corpus-based features. We evaluate it with a 10-fold cross validation on 9,600 pairs, equally distributed among the three classes and involving several Parts-Of-Speech (i.e. adjectives, nouns and verbs). When all the classes are present, ROOT9 achieves an F1 score of 90.7{\%}, against a baseline of 57.2{\%} (vector cosine). When the classification is binary, ROOT9 achieves the following results against the baseline. hypernyms-co-hyponyms 95.7{\%} vs. 69.8{\%}, hypernyms-random 91.8{\%} vs. 64.1{\%} and co-hyponyms-random 97.8{\%} vs. 79.4{\%}. In order to compare the performance with the state-of-the-art, we have also evaluated ROOT9 in subsets of the Weeds et al. (2014) datasets, proving that it is in fact competitive. Finally, we investigated whether the system learns the semantic relation or it simply learns the prototypical hypernyms, as claimed by Levy et al. (2015). The second possibility seems to be the most likely, even though ROOT9 can be trained on negative examples (i.e., switched hypernyms) to drastically reduce this bias."
L16-1723,What a Nerd! Beating Students and Vector Cosine in the {ESL} and {TOEFL} Datasets,2016,4,6,5,1,181,enrico santus,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"In this paper, we claim that Vector Cosine â which is generally considered one of the most efficient unsupervised measures for identifying word similarity in Vector Space Models â can be outperformed by a completely unsupervised measure that evaluates the extent of the intersection among the most associated contexts of two target words, weighting such intersection according to the rank of the shared contexts in the dependency ranked lists. This claim comes from the hypothesis that similar words do not simply occur in similar contexts, but they share a larger portion of their most relevant contexts compared to other related words. To prove it, we describe and evaluate APSyn, a variant of Average Precision that â independently of the adopted parameters â outperforms the Vector Cosine and the co-occurrence on the ESL and TOEFL test sets. In the best setting, APSyn reaches 0.73 accuracy on the ESL dataset and 0.70 accuracy in the TOEFL dataset, beating therefore the non-English US college applicants (whose average, as reported in the literature, is 64.50{\%}) and several state-of-the-art approaches."
L16-1726,{EVAL}ution-{MAN}: A {C}hinese Dataset for the Training and Evaluation of {DSM}s,2016,0,1,4,0,35433,liu hongchao,Proceedings of the Tenth International Conference on Language Resources and Evaluation ({LREC}'16),0,"Distributional semantic models (DSMs) are currently being used in the measurement of word relatedness and word similarity. One shortcoming of DSMs is that they do not provide a principled way to discriminate different semantic relations. Several approaches have been adopted that rely on annotated data either in the training of the model or later in its evaluation. In this paper, we introduce a dataset for training and evaluating DSMs on semantic relations discrimination between words, in Mandarin, Chinese. The construction of the dataset followed EVALution 1.0, which is an English dataset for the training and evaluating of DSMs. The dataset contains 360 relation pairs, distributed in five different semantic relations, including antonymy, synonymy, hypernymy, meronymy and nearsynonymy. All relation pairs were checked manually to estimate their quality. In the 360 word relation pairs, there are 373 relata. They were all extracted and subsequently manually tagged according to their semantic type. The relatas{'} frequency was calculated in a combined corpus of Sinica and Chinese Gigaword. To the best of our knowledge, EVALution-MAN is the first of its kind for Mandarin, Chinese."
D16-1205,Representing Verbs with Rich Contexts: an Evaluation on Verb Similarity,2016,22,1,5,0.7277,180,emmanuele chersoni,Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing,0,"Several studies on sentence processing suggest that the mental lexicon keeps track of the mutual expectations between words. Current DSMs, however, represent context words as separate features, thereby loosing important information for word expectations, such as word interrelations. In this paper, we present a DSM that addresses this issue by defining verb contexts as joint syntactic dependencies. We test our representation in a verb similarity task on two datasets, showing that joint contexts achieve performances comparable to single dependencies or even better. Moreover, they are able to overcome the data sparsity problem of joint feature spaces, in spite of the limited size of our training corpus."
Y15-2035,Graph Theoretic Features of the Adult Mental lexicon Predict Language Production in {M}andarin: Clustering Coefficient,2015,18,0,2,1,35348,karl neergaard,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation: Posters",0,"Graph theory has recently been used to explore the mathematical structure of the mental lexicon. In this study we tested the influence of graph measures on Mandarin speech production. Thirty-six native Mandarin-speaking adults took part in a shadowing task containing 194 monosyllabic words, 94 of which consisted of 3 phonemes and were the items under analysis. Linear mixed effect modeling revealed that clustering coefficient (C) predicted spoken production of Mandarin monosyllabic words, while network degree, in this case its phonological neighborhood density (PND) failed to account for lexical processing. High C resulted in shorter reaction times, contrary to evidence in English. While these findings suggest that lexical processing is affected by the network structure of the mental lexicon, they also suggest that language specific traits lead to differing behavioral outcomes. While PND can be understood as the underlying lattice for which a similarity network is created, lexical selection is not affected by only a target wordxe2x80x99s neighbors but instead the level of interconnectivity of words (C) within the network."
Y15-1007,{M}echanical {T}urk-based Experiment vs Laboratory-based Experiment: A Case Study on the Comparison of Semantic Transparency Rating Data,2015,24,10,2,1,36312,shichang wang,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"In this paper, we conducted semantic transparency rating experiments using both the traditional laboratory-based method and the crowdsourcing-based method. Then we compared the rating data obtained from these two experiments. We observed very strong correlation coefficients for both overall semantic transparency rating data and constituent semantic transparency data (rho > 0:9) which means the two experiments may yield comparable data and crowdsourcing-based experiment is a feasible alternative to the laboratorybased experiment in linguistic studies. We also observed a scale shrinkage phenomenon in both experiments: the actual scale of the rating results cannot cover the ideal scale [0;1], both ends of the actual scale shrink towards the center. However, the scale shrinkage of the crowdsourcing-based experiment is stronger than that of the laboratory-based experiment, this makes the rating results obtained in these two experiments not directly comparable. In order to make the results directly comparable, we explored two data transformation algorithms, z-score transformation and adjusted normalization to unify the scales. We also investigated the uncertainty of semantic transparency judgment among raters, we found that it had a regular relation with semantic transparency magnitude and this may further reveal a general cognitive mechanism of human judgment."
Y15-1021,Sentiment Analyzer with Rich Features for Ironic and Sarcastic Tweets,2015,27,3,4,0,36323,piyoros tungthamthiti,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"Sentiment Analysis of tweets is a complex task, because these short messages employ unconventional language to increase the expressiveness. This task becomes even more difficult when people use figurative language (e.g. irony, sarcasm and metaphors) because it causes a mismatch between the literal meaning and the actual expressed sentiment. In this paper, we describe a sentiment analysis system designed for handling ironic and sarcastic tweets. Features grounded on several linguistic levels are proposed and used to classify the tweets in a 11-scale range, using a decision tree. The system is evaluated on the dataset released by the organizers of the SemEval 2015, task 11. The results show that our method largely outperforms the systems proposed by the participants of the task on ironic and sarcastic tweets."
Y15-1024,The Invertible Construction in {C}hinese,2015,10,0,2,0,11367,yan cong,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,None
Y15-1036,Auditory Synaesthesia and Near Synonyms: A Corpus-Based Analysis of sheng1 and yin1 in {M}andarin {C}hinese,2015,-1,-1,2,0,36330,qingqing zhao,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,None
Y15-1045,When Embodiment Meets {G}enerative {L}exicon: The Human Body Part Metaphors in Sinica Corpus,2015,18,2,2,1,15619,renfeng duann,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,"This research aims to integrate embodiment with generative lexicon. By analyzing the metaphorically used human body part terms in Sinica Corpus, the first balanced modern Chinese corpus, we reveal how these two theories complement each other. Embodiment strengthens generative lexicon by spelling out the cognitive reasons which underlies the production of meaning, and generative lexicon, specifically the qualia structure, complements embodiment by accounting for the reason underlying the selection of a particular body part for metaphorization. Discussing how the four body part termsxe2x80x94xe8xa1x80 xie xe2x80x9cbloodxe2x80x9d, xe8x82x89 rou xe2x80x9cfleshxe2x80x9d, xe9xaaxa8 gu xe2x80x9cbonexe2x80x9d, xe8x84x88 mai xe2x80x9cmeridianxe2x80x9dxe2x80x94 behave metaphorically, this research argues that the visibility and the telic role of the qualia structure are the major reasons motivating the choice of a body part to represent a comparatively abstract notion. The finding accounts for what constrains the selection of body parts for metaphorical uses. It also facilitates the prediction of the behavior of the four body part terms in these uses, which can function as the starting point to examine whether the two factorsxe2x80x94visibility and telicityxe2x80x94also motivate the metaphorization of the rest human body parts."
Y15-1049,De-verbalization and Nominal Categories in {M}andarin {C}hinese: A corpus-driven study in both Mainland {M}andarin and {T}aiwan {M}andarin,2015,-1,-1,2,1,33277,jiajuan xiong,"Proceedings of the 29th Pacific Asia Conference on Language, Information and Computation",0,None
W15-4208,{EVAL}ution 1.0: an Evolving Semantic Dataset for Training and Evaluation of Distributional Semantic Models,2015,28,37,4,1,181,enrico santus,Proceedings of the 4th Workshop on Linked Data in Linguistics: Resources and Applications,0,"In this paper, we introduce EVALution 1.0, a dataset designed for the training and the evaluation of Distributional Semantic Models (DSMs). This version consists of almost 7.5K tuples, instantiating several semantic relations between word pairs (including hypernymy, synonymy, antonymy, meronymy). The dataset is enriched with a large amount of additional information (i.e. relation domain, word frequency, word POS, word semantic field, etc.) that can be used for either filtering the pairs or performing an in-depth analysis of the results. The tuples were extracted from a combination of ConceptNet 5.0 and WordNet 4.0, and subsequently filtered through automatic methods and crowdsourcing in order to ensure their quality. The dataset is freely downloadable1. An extension in RDF format, including also scripts for data processing, is under development."
W15-3102,Create a Manual {C}hinese Word Segmentation Dataset Using Crowdsourcing Method,2015,21,0,2,1,36312,shichang wang,Proceedings of the Eighth {SIGHAN} Workshop on {C}hinese Language Processing,0,"The manual Chinese word segmentation dataset WordSegCHC 1.0 which was built by eight crowdsourcing tasks conducted on the Crowdflower platform contains the manual word segmentation data of 152 Chinese sentences whose length ranges from 20 to 46 characters without punctuations. All the sentences received 200 segmentation responses in their corresponding crowdsourcing tasks and the numbers of valid response of them range from 123 to 143 (each sentence was segmented by more than 120 subjects). We also proposed an evaluation method called manual segmentation error rate (MSER) to evaluate the dataset; the MSER of the dataset is proved to be very low which indicates reliable data quality. In this work, we applied the crowdsourcing method to Chinese word segmentation task and the results confirmed again that the crowdsourcing method is a promising tool for linguistic data collection; the framework of crowdsourcing linguistic data collection used in this work can be reused in similar tasks; the resultant dataset filled a gap in Chinese language resources to the best of our knowledge, and it has potential applications in the research of word intuition of Chinese speakers and Chinese language processing."
S15-2113,{LLT}-{P}oly{U}: Identifying Sentiment Intensity in Ironic Tweets,2015,32,11,4,1,16510,hongzhi xu,Proceedings of the 9th International Workshop on Semantic Evaluation ({S}em{E}val 2015),0,"In this paper, we describe the system we built for Task 11 of SemEval2015, which aims at identifying the sentiment intensity of figurative language in tweets. We use various features, including those specially concerned with the identification of irony and sarcasm. The features are evaluated through a decision tree regression model and a support vector regression model. The experiment result of the fivecross validation on the training data shows that the tree regression model outperforms the support vector regression model. The former is therefore used for the final evaluation of the task. The results show that our model performs especially well in predicting the sentiment intensity of tweets involving irony and sarcasm."
P15-5008,What You Need to Know about {C}hinese for {C}hinese Language Processing,2015,0,0,1,1,1504,churen huang,Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing: Tutorial Abstracts,0,"The synergy between language sciences and language technology has been an elusive one for the computational linguistics community, especially when dealing with a language other than English. The reasons are two-fold: the lack of an accessible comprehensive and robust account of a specific language so as to allow strategic linking between a processing task to linguistic devices, and the lack of successful computational studies taking advantage of such links. With a fast growing number of available online resources, as well as a rapidly increasing number of members of the CL community who are interested in and/or working on Chinese language processing, the time is ripe to take a serious look at how knowledge of Chinese can help Chinese language processing."
Y14-1018,Taking Antonymy Mask off in Vector Space,2014,36,13,4,1,181,enrico santus,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"Automatic detection of antonymy is an important task in Natural Language Processing (NLP) for Information Retrieval (IR), Ontology Learning (OL) and many other semantic applications. However, current unsupervised approaches to antonymy detection are still not fully effective because they cannot discriminate antonyms from synonyms. In this paper, we introduce APAnt, a new AveragePrecision-based measure for the unsupervised discrimination of antonymy from synonymy using Distributional Semantic Models (DSMs). APAnt makes use of Average Precision to estimate the extent and salience of the intersection among the most descriptive contexts of two target words. Evaluation shows that the proposed method is able to distinguish antonyms and synonyms with high accuracy across different parts of speech, including nouns, adjectives and verbs. APAnt outperforms the vector cosine and a baseline model implementing the cooccurrence hypothesis."
Y14-1057,An Analysis of Radicals-based Features in Subjectivity Classification on Simplified {C}hinese Sentences,2014,5,0,2,1,33484,ge xu,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,"Chinese radicals are linguistic elements smaller than Chinese characters1. Normally, a radical is a semantic category and almost all characters contain radicals or are radicals themselves. In subjectivity classification on sentences, we can use radicals to represent characters, which reduce the scale of word space while keep the subjectivity information. In this paper, we manually labeled a character set to build a high-quality radical-character mapping, and then the mapping is used to generalize character-based features with radicals. In experiments, we at first evaluated the performance when directly generalizing characters with radicals, and then offer a hypothesis that can reduce noises. Experiments show that this approach based on our hypothesis can reduce feature space while keep or improve the performance, which is especially useful when the training samples are scarce. keyword: sentiment analysis, subjectivity classification, radical, Chinese character"
Y14-1070,On the Argument Structures of the Transitive Verb {`}annoy; be annoyed; bother to do{'}: A Study Based on Two Comparable Corpora,2014,-1,-1,2,1,33277,jiajuan xiong,"Proceedings of the 28th Pacific Asia Conference on Language, Information and Computing",0,None
W14-5810,Annotation and Classification of Light Verbs and Light Verb Variations in {M}andarin {C}hinese,2014,6,5,4,1,27504,jingxia lin,Proceedings of Workshop on Lexical and Grammatical Resources for Language Processing,0,"Light verbs pose an a challenge in linguistics because of its syntactic and semantic versatility and its unique distribution different from regular verbs with higher semantic content and selectional resrictions. Due to its light grammatical content, earlier natural language processing studies typically put light verbs in a stop word list and ignore them. Recently, however, classification and identification of light verbs and light verb construction have become a focus of study in computational linguistics, especially in the context of multi-word expression, information retrieval, disambiguation, and parsing. Past linguistic and computational studies on light verbs had very different foci. Linguistic studies tend to focus on the status of light verbs and its various selectional constraints. While NLP studies have focused on light verbs in the context of either a multi-word expression (MWE) or a construction to be identified, classified, or translated, trying to overcome the apparent poverty of semantic content of light verbs. There has been nearly no work attempting to bridge these two lines of research. This paper takes this challenge by proposing a corpus-bases study which classifies and captures syntactic-semantic difference among all light verbs. In this study, we first incorporate results from past linguistic studies to create annotated light verb corpora with syntactic-semantics features. We next adopt a statistic method for automatic identification of light verbs based on this annotated corpora. Our results show that a language resource based methodology optimally incorporating linguistic information can resolve challenges posed by light verbs in NLP."
W14-5818,Building a Semantic Transparency Dataset of {C}hinese Nominal Compounds: A Practice of Crowdsourcing Methodology,2014,13,3,2,1,36312,shichang wang,Proceedings of Workshop on Lexical and Grammatical Resources for Language Processing,0,"This paper describes the work which aimed to create a semantic transparency dataset of Chinese nominal compounds (SemTransCNC 1.0) by crowdsourcing methodology. We firstly selected about 1,200 Chinese nominal compounds from a lexicon of modern Chinese and the Sinica Corpus. Then through a series of crowdsourcing experiments conducted on the Crowdflower platform, we successfully collected both overall semantic transparency and constituent semantic transparency data for each of them. According to our evaluation, the data quality is good. This work filled a gap in Chinese language resources and also practiced and explored the crowdsourcing methodology for linguistic experiment and language resource construction."
W14-5819,"Annotate and Identify Modalities, Speech Acts and Finer-Grained Event Types in {C}hinese Text",2014,21,2,2,1,16510,hongzhi xu,Proceedings of Workshop on Lexical and Grammatical Resources for Language Processing,0,"Discriminating sentences that denote modalities and speech acts from the ones that describe or report events is a fundamental task for accurate event processing. However, little attention has been paid on this issue. No Chinese corpus is available by now with all different types of sentences annotated with their main functionalities in terms of modality, speech act or event. This paper describes a Chinese corpus with all the information annotated. Based on the five event types that are usually adopted in previous studies of event classification, namely state, activity, achievement, accomplishment and semelfactive, we further provide finer-grained categories, considering that each of the finer-grained event types has different semantic entailments. To differentiate them is useful for deep semantic processing and will thus benefit NLP applications such as question answering and machine translation, etc. We also provide experiments to show that the different types of sentences are differentiable with a promising performance."
W14-5301,Corpus-based Study and Identification of {M}andarin {C}hinese Light Verb Variations,2014,12,11,1,1,1504,churen huang,"Proceedings of the First Workshop on Applying {NLP} Tools to Similar Languages, Varieties and Dialects",0,"When PRC was founded on mainland China and the KMT retreated to Taiwan in 1949, the relation between mainland China and Taiwan became a classical Cold War instance. Neither travel, visit, nor correspondences were allowed between the people until 1987, when government on both sides started to allow small number of Taiwan people with relatives in China to return to visit through a third location. Although the thawing eventually lead to frequent exchanges, direct travel links, and close commercial ties between Taiwan and mainland China today, 38 years of total isolation from each other did allow the language use to develop into different varieties, which have become a popular topic for mainly lexical studies (e.g., Xu, 1995; Zeng, 1995; Wang & Li, 1996). Grammatical difference of these two variants, however, was not well studied beyond anecdotal observation, partly because the near identity of their grammatical systems. This paper focuses on light verb variations in Mainland and Taiwan variants and finds that the light verbs of these two variants indeed show distributional tendencies. Light verbs are chosen for two reasons: first, they are semantically bleached hence more susceptible to changes and variations. Second, the classification of light verbs is a challenging topic in NLP. We hope our study will contribute to the study of light verbs in Chinese in general. The data adopted for this study was a comparable corpus extracted from Chinese Gigaword Corpus and manually annotated with contextual features that may contribute to light verb variations. A multivariate analysis was conducted to show that for each light verb there is at least one context where the two variants show differences in tendencies (usually the presence/absence of a tendency rather than contrasting tendencies) and can be differentiated. In addition, we carried out a K-Means clustering analysis for the variations and the results are consistent with the multivariate analysis, i.e. the light verbs in Mainland and Taiwan indeed have variations and the variations can be successfully differentiated."
W14-4715,Exploring Mental Lexicon in an Efficient and Economic Way: Crowdsourcing Method for Linguistic Experiments,2014,16,5,2,1,36312,shichang wang,Proceedings of the 4th Workshop on Cognitive Aspects of the Lexicon ({C}og{AL}ex),0,"Mental lexicon plays a central role in human language competence and inspires the creation of new lexical resources. The traditional linguistic experiment method which is used to explore mental lexicon has some disadvantages. Crowdsourcing has become a promising method to conduct linguistic experiments which enables us to explore mental lexicon in an efficient and economic way. We focus on the feasibility and quality control issues of conducting Chinese linguistic experiments to collect Chinese word segmentation and semantic transparency data on the international crowdsourcing platforms Amazon Mechanical Turk and Crowdflower. Through this work, a framework for crowdsourcing linguistic experiments is proposed."
lee-etal-2014-annotating,Annotating Events in an Emotion Corpus,2014,20,9,3,1,17031,sophia lee,Proceedings of the Ninth International Conference on Language Resources and Evaluation ({LREC}'14),0,"This paper presents the development of a Chinese event-based emotion corpus. It specifically describes the corpus design, collection and annotation. The proposed annotation scheme provides a consistent way of identifying some emotion-associated events (namely pre-events and post-events). Corpus data show that there are significant interactions between emotions and pre-events as well as that of between emotion and post-events. We believe that emotion as a pivot event underlies an innovative approach towards a linguistic model of emotion as well as automatic emotion detection and classification."
W13-5402,Metaphor and Qualia: Embodiment or Eventuality,2013,-1,-1,1,1,1504,churen huang,Proceedings of the 6th International Conference on Generative Approaches to the Lexicon ({GL}2013),0,None
W13-5408,Primitives of Events and the Semantic Representation,2013,-1,-1,2,1,16510,hongzhi xu,Proceedings of the 6th International Conference on Generative Approaches to the Lexicon ({GL}2013),0,None
P13-2091,Joint Modeling of News Reader{'}s and Comment Writer{'}s Emotions,2013,25,18,4,0,41437,huanhuan liu,Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers),0,"Emotion classification can be generally done from both the writerxe2x80x99s and readerxe2x80x99s perspectives. In this study, we find that two foundational tasks in emotion classification, i.e., readerxe2x80x99s emotion classification on the news and writerxe2x80x99s emotion classification on the comments, are strongly related to each other in terms of coarse-grained emotion categories, i.e., negative and positive. On the basis, we propose a respective way to jointly model these two tasks. In particular, a cotraining algorithm is proposed to improve semi-supervised learning of the two tasks. Experimental evaluation shows the effectiveness of our joint modeling approach."
O13-3002,"ä»¥ä¸­æååè©èªæåº«çºåºç¤ä¹å\
©å²¸è©å½å°æ¯ç ç©¶ (Cross-Strait Lexical Differences: A Comparative Study based on {C}hinese {G}igaword Corpus) [In {C}hinese]",2013,-1,-1,2,1,37549,jiafei hong,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 18, Number 2, June 2013-Special Issue on {C}hinese Lexical Resources: Theories and Applications",0,None
I13-1096,A Rule System for {C}hinese Time Entity Recognition by Comprehensive Linguistic Study,2013,6,0,2,1,16510,hongzhi xu,Proceedings of the Sixth International Joint Conference on Natural Language Processing,0,"Chinese time entity is quite complex. In this paper, we give a comprehensive linguistic study on it. Based on the analysis, we present a rule system which only considers the inner structure of Chinese time entities for the recognition. Experiments on Sinica and TempEval-2 corpus show that the rule system performs much better than the CRFs model. When using the rules as features within a CRFs model, the performance could be further improved."
Y12-1007,Compositionality of {NN} Compounds: A Case Study on [{N}1+{A}rtifactual-Type Event Nouns],2012,16,2,2,1,11705,shan wang,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"26th Pacific Asia Conference on Language, Information and Computation, PACLIC 2012, Bali, 7 November 2012"
Y12-1046,The Headedness of {M}andarin {C}hinese Serial Verb Constructions: A Corpus-Based Study,2012,20,2,2,1,27504,jingxia lin,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"26th Pacific Asia Conference on Language, Information and Computation, PACLIC 2012, Bali, 7 November 2012"
Y12-1063,Type Construction of Event Nouns in {M}andarin {C}hinese,2012,6,4,2,1,11705,shan wang,"Proceedings of the 26th Pacific Asia Conference on Language, Information, and Computation",0,"Natural and non-natural kinds have significant differences. This paper explores the subclasses of each kind and establishes the type system for event nouns. These nouns are divided into natural types, artifactual types, complex types (including natural complex types and artifactual complex types). This new classification not only enriches the Generative Lexicon theory, but also helps us to capture the properties of different types of event nouns."
xu-etal-2012-grammar,A Grammar-informed Corpus-based Sentence Database for Linguistic and Computational Studies,2012,3,0,3,1,16510,hongzhi xu,Proceedings of the Eighth International Conference on Language Resources and Evaluation ({LREC}'12),0,"We adopt the corpus-informed approach to example sentence selections for the construction of a reference grammar. In the process, a database containing sentences that are carefully selected by linguistic experts including the full range of linguistic facts covered in an authoritative Chinese Reference Grammar is constructed and structured according to the reference grammar. A search engine system is developed to facilitate the process of finding the most typical examples the users need to study a linguistic problem or prove their hypotheses. The database can also be used as a training corpus by computational linguists to train models for Chinese word segmentation, POS tagging and sentence parsing."
C12-3066,{SMR}-{C}mp: Square-Mean-Root Approach to Comparison of Monolingual Contrastive Corpora,2012,7,1,2,1,41975,huarui zhang,Proceedings of {COLING} 2012: Demonstration Papers,0,"The basic statistic tools used in computational and corpus linguistics to capture distributional information have not changed much in the past 20 years even though many standard tools have been proved to be inadequate. In this demo (SMR-Cmp), we adopt the new tool of Square-MeanRoot (SMR) similarity, which measures the evenness of distribution between contrastive corpora, to extract lexical variations. The result based on one case study shows that the novel approach outperforms traditional statistical measures, including chi-square (xcfx87 2 ) and log-likelihood ratio"
C12-2067,Active Learning for {C}hinese Word Segmentation,2012,33,8,3,1,9448,shoushan li,Proceedings of {COLING} 2012: Posters,0,"Currently, the best performing models for Chinese word segmentation (CWS) are extremely resource intensive in terms of annotation data quanti ty. One promising solution to minimize the cost of data acquisition is active learning, which aims to actively select the most useful instances to annotate for learning. Active learning on CWS, h owever, remains challenging due to its inherent nature. In this paper, we propose a Word Bounda ry Annotation (WBA) model to make effective active learning on CWS possible. This is achie ved by annotating only those uncertain boundaries. In this way, the manual annotation cost is l argely reduced, compared to annotating the whole character sequence. To further minimize the a nnotation effort, a diversity measurement among the instances is considered to avoid duplicat e annotation. Experimental results show that employing the WBA model and the diversity measurement into active learning on CWS can save much annotation cost with little loss in the perfor mance."
C12-2121,Sourcing the Crowd for a Few Good Ones: Event Type Detection,2012,28,2,2,0,6,tommaso caselli,Proceedings of {COLING} 2012: Posters,0,"This paper reports a crowdsourcing experiment on the identification and classification of event types in Italian. The data collected show that the task is not trivial (360 trusted judgments collected vs. 475 untrsuted ones) but it has been shown to be linguistically felicitous. The overall accuracy of the annotation is 61.6%. A reliability threshold assigned to the workers allows us to indentify the sub-population who has the awareness to perform this complex task and the accuracy of this sub-population is raised to 93%. Our hypothesis is that although the initial crowdsourced data is necessarily noisy, it can yield high quality results if the sub-population of xe2x80x98goodxe2x80x99 workers can be identified. In other words, crowdsourcing offers a solution to difficult annotation tasks as long as there is an effective way to identify the reliable workers. TITLE AND ABSTRACT IN ANOTHER LANGUAGE, L2 (OPTIONAL, AND ON SAME PAGE) Identificare Annotatori Affidabili: Riconoscimento di Tipi di Evento Questo articolo descrive un esperimento di crowdsourcing per il riconoscimento e la classificazione dei tipi di evento in Italiano. I dati raccolti mostrano che il compito non e banale (360 giudizi affidabili vs. 475 giudizi non affidabili), ma dimostra di essere linguisticamente xe2x80x9cfelicexe2x80x9d. Lxe2x80x99accuratezza globale della annotazione e del 61,6%. Una soglia di affidabilita assegnata ai lavoratori ci permette di identificare la sotto-popolazione che ha la consapevolezza di svolgere questo compito complesso la cui accuratezza arriva fino al 93%. La nostra ipotesi e che, sebbene i dati iniziali ottenuti tramite tecniche di crowdsourcing siano necessariamente rumorosi, dei risultati di buona qualita possono essere ottenuti se la sotto-popolazione di buoni lavoratori e identificabile. In altre parole, il crowdsourcing offre una soluzione per compiti di annotazione difficili finche vi e un modo efficace per identificare i lavoratori affidabili."
Y11-1054,Compound Event Nouns of the {`}Modifier-head{'} Type in {M}andarin {C}hinese,2011,4,4,2,1,11705,shan wang,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"Event nouns can lexically encode eventive information. Recently these nouns have generated considerable scholarly interest. However, little research has been conducted in their morphological and syntactic structure, qualia modification, event representing feature, and information inheritance characteristics. This study has these main findings. 1) Morphologically, the modifier and the head is either free or bound morpheme. Syntactically the modifier is a nominal, adjectival, verbal or numeral morpheme, while the head is a nominal morpheme. 2) The modifier acts as a qualia role of the head. 3) All heads represent events, while the modifier is or is not an event. 4) The semantic information of a compound event noun can be inherited from the modifier or the head."
Y11-1055,The Co-occurrence of Two Delimiters: An Investigation of {M}andarin {C}hinese Resultatives,2011,10,0,2,1,27504,jingxia lin,"Proceedings of the 25th Pacific Asia Conference on Language, Information and Computation",0,"Tenny (1987, 1994: 79) proposes the Single Delimiting Constraint that the event described by a verb can only be delimited once (cf. Goldberg 1991, 1995). The constraint applies the effects of delimitedness as an aspectual property to the mapping of semantics and syntax, and explains why sentences with two delimiters (e.g., *Martha wiped the table dry1 clean2 Tenny 1994: 80) are unacceptable. The constraint is challenged with English counterexamples and modified by Matsumoto (2006) and Zhou (2008). However, this study proposes that the constraint holds for Mandarin Chinese resultatives, whereas the revised constraints do not. Furthermore, we point out that while two independent delimiters usually do not co-occur in Chinese, a second delimiter that further specifies or reinforces the endpoint/ endstate denoted by the first delimiter is allowed (cf. Tenny 1994, Goldberg 1991). The results of this study may shed light on event structure of Chinese."
Y10-1045,Using Corpus-based Linguistic Approaches in Sense Prediction Study,2010,17,0,3,1,37549,jiafei hong,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"In this study, we propose to use two corpus-based linguistic approaches for a sense prediction study. We will concentrate on the character similarity clustering approach and concept similarity clustering approach to predict the senses of non-assigned words by using corpora and tools, such as Chinese Gigaword Corpus, and HowNet. In this study, we would then like to evaluate their predictions via the sense divisions of Chinese Wordnet and Xiandai Hanyu Cidian. Using these corpora, we will determine the clusters of our four target words ---- chi1 eat, wan2 play, huan4 change and shao1 burn in order to predict their all possible senses and evaluate them. This requirement will demonstrate the visibility of the corpus-based approaches."
Y10-1062,Incorporate Credibility into Context for the Best Social Media Answers,2010,16,6,3,0.987654,3748,qi su,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"In this paper, we focus on the task of identifying the best answer for a usergenerated question in Collaborative Question Answering (CQA) services. Given that most existing research on CQA has focused on non-textual features such as click-through counts which are relatively difficult to access, we examine the effectiveness of diverse content-based features for the task. Specially, we propose to explore how the information of evidentiality can contribute to the task. By the comparison of diverse textual features and their combinations, the current study provides useful insight into the issues of detecting the best answer to a given question in CQA without user features or system specific link structures."
Y10-1081,Adjectival Modification to Nouns in {M}andarin {C}hinese: Case Studies on {``}ch{\\'a}ng+noun{''} and {``}adjective+t{\\'u} sh{\\=u} gu n{''},2010,5,1,2,1,11705,shan wang,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"This paper studies the adjectival modification to nouns in Mandarin Chinese based on selective binding. The main findings include: xe2x91xb4An adjective can select different types of head nouns as arguments and an adjective may modify an individual or an event. xe2x91xb5The qualia structure of a noun helps us better understand an adjective's selectional preference. Meanwhile, an adjective can modify multi-facet or one facet of the qualia role of a noun. xe2x91xb6 The adjacent adjective of a noun is not necessarily modifying the noun."
Y10-1082,Compositional Operations of {M}andarin {C}hinese Perception Verb {``}k{\\`a}n{''}: A {G}enerative {L}exicon Approach,2010,3,2,2,1,11705,shan wang,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,"The compositional operation of a predicate can be very complex. This paper captures a full picture of such an operation through investigating genuine corpus data of Chinese perception verb xe2x80x9cxe7x9cx8bxe2x80x9d (kan, look at). The study reveals that context and qualia structure can affect the meaning of verb-argument composition of xe2x80x9ckanxe2x80x9d. It also shows that the compositional ability of xe2x80x9ckanxe2x80x9d varies under different senses, and each sense has its own type selection preference."
Y10-1097,Cross-sortal Predication and Polysemy,2010,32,1,2,0.652174,45061,petr vsimon,"Proceedings of the 24th Pacific Asia Conference on Language, Information and Computation",0,This paper develops new treatment of the problem of cross-sortal predication and co- predication in particular. We argue that the solution to these predicate-argument sort mismatches can be solved by a more flexible treatment of polysemy based on the notion of dependent type and dynamic construction of meaning.
W10-4102,Textual Emotion Processing From Event Analysis,2010,20,2,1,1,1504,churen huang,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,None
W10-4152,The {C}hinese Persons Name Diambiguation Evaluation: Exploration of Personal Name Disambiguation in {C}hinese News,2010,5,7,4,1,12273,ying chen,{CIPS}-{SIGHAN} Joint Conference on {C}hinese Language Processing,0,"Personal name disambiguation becomes hot as it provides a way to incorporate semantic understanding into information retrieval. In this campaign, we explore Chinese personal name disambiguation in news. In order to examine how well disambiguation technologies work, we concentrate on news articles, which is well-formatted and whose genre is well-studied. We then design a diagnosis test to explore the impact of Chinese word segmentation to personal name disambiguation."
W10-2102,Evidentiality for Text Trustworthiness Detection,2010,20,17,2,0.987654,3748,qi su,Proceedings of the 2010 Workshop on {NLP} and Linguistics: Finding the Common Ground,0,"Evidentiality is the linguistic representation of the nature of evidence for a statement. In other words, it is the linguistically encoded evidence for the trustworthiness of a statement. In this paper, we aim to explore how linguistically encoded information of evidentiality can contribute to the prediction of trustworthiness in natural language processing (NLP). We propose to incorporate evidentiality into a framework of machine learning based text classification. We first construct a taxonomy of evidentials. Then experiments involving collaborative question answering (CQA) are designed and implemented using this taxonomy. The experimental results confirm that evidentiality is an important clue for text trustworthiness detection. With the binarized vector setting, evidential based text representation model has considerably performaned better than both the bag-of-word model and the content word based model. Most crucially, we show that the best trustworthiness detection result is achieved when evidentiality is incorporated in a linguistically sophisticated model where their meanings are interpreted in both semantic and pragmatic terms."
W10-0206,A Text-driven Rule-based System for Emotion Cause Detection,2010,24,40,3,1,17031,sophia lee,Proceedings of the {NAACL} {HLT} 2010 Workshop on Computational Approaches to Analysis and Generation of Emotion in Text,0,"Emotion cause detection is a new research area in emotion processing even though most theories of emotion treat recognition of a triggering cause event as an integral part of emotion. As a first step towards fully automatic inference of cause-emotion correlation, we propose a text-driven, rule-based approach to emotion cause detection in this paper. First of all, a Chinese emotion cause annotated corpus is constructed based on our proposed annotation scheme. By analyzing the corpus data, we identify seven groups of linguistic cues and generalize two sets of linguistic rules for detection of emotion causes. With the linguistic rules, we then develop a rule-based system for emotion cause detection. In addition, we propose an evaluation scheme with two phases for performance assessment. Experiments show that our system achieves a promising performance for cause occurrence detection as well as cause event detection. The current study should lay the ground for future research on the inferences of implicit information and the discovery of new information based on cause-event relation."
P10-1043,Employing Personal/Impersonal Views in Supervised and Semi-Supervised Sentiment Classification,2010,26,71,2,1,9448,shoushan li,Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics,1,"In this paper, we adopt two views, personal and impersonal views, and systematically employ them in both supervised and semi-supervised sentiment classification. Here, personal views consist of those sentences which directly express speaker's feeling and preference towards a target object while impersonal views focus on statements towards a target object for evaluation. To obtain them, an unsupervised mining approach is proposed. On this basis, an ensemble method and a co-training algorithm are explored to employ the two views in supervised and semi-supervised sentiment classification respectively. Experimental results across eight domains demonstrate the effectiveness of our proposed approach."
lee-etal-2010-emotion,Emotion Cause Events: Corpus Construction and Analysis,2010,20,17,4,1,17031,sophia lee,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Emotion processing has always been a great challenge. Given the fact that an emotion is triggered by cause events and that cause events are an integral part of emotion, this paper constructs a Chinese emotion cause corpus as a first step towards automatic inference of cause-emotion correlation. The corpus focuses on five primary emotions, namely happiness, sadness, fear, anger, and surprise. It is annotated with emotion cause events based on our proposed annotation scheme. Corpus data shows that most emotions are expressed with causes, and that causes mostly occur before the corresponding emotion verbs. We also examine the correlations between emotions and cause events in terms of linguistic cues: causative verbs, perception verbs, epistemic markers, conjunctions, prepositions, and others. Results show that each group of linguistic cues serves as an indicator marking the cause events in different structures of emotional constructions. We believe that the emotion cause corpus will be the useful resource for automatic emotion cause detection as well as emotion detection and classification."
wang-etal-2010-automatic,Automatic Acquisition of {C}hinese Novel Noun Compounds,2010,18,0,2,0,44691,meng wang,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Automatic acquisition of novel compounds is notoriously difficult because most novel compounds have relatively low frequency in a corpus. The current study proposes a new method to deal with the novel compound acquisition challenge. We model this task as a two-class classification problem in which a candidate compound is either classified as a compound or a non-compound. A machine learning method using SVM, incorporating two types of linguistically motivated features: semantic features and character features, is applied to identify rare but valid noun compounds. We explore two kinds of training data: one is virtual training data which is obtained by three statistical scores, i.e. co-occurrence frequency, mutual information and dependent ratio, from the frequent compounds; the other is real training data which is randomly selected from the infrequent compounds. We conduct comparative experiments, and the experimental results show that even with limited direct evidence in the corpus for the novel compounds, we can make full use of the typical frequent compounds to help in the discovery of the novel compounds."
C10-1021,Emotion Cause Detection with Linguistic Constructions,2010,15,53,4,1,12273,ying chen,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"This paper proposes a multi-label approach to detect emotion causes. The multi-label model not only detects multi-clause causes, but also captures the long-distance information to facilitate emotion cause detection. In addition, based on the linguistic analysis, we create two sets of linguistic patterns during feature extraction. Both manually generalized patterns and automatically generalized patterns are designed to extract general cause expressions or specific constructions for emotion causes. Experiments show that our system achieves a performance much higher than a baseline model."
C10-1072,Sentiment Classification and Polarity Shifting,2010,26,84,4,1,9448,shoushan li,Proceedings of the 23rd International Conference on Computational Linguistics (Coling 2010),0,"Polarity shifting marked by various linguistic structures has been a challenge to automatic sentiment classification. In this paper, we propose a machine learning approach to incorporate polarity shifting information into a document-level sentiment classification system. First, a feature selection method is adopted to automatically generate the training data for a binary classifier on polarity shifting detection of sentences. Then, by using the obtained binary classifier, each document in the original polarity classification training data is split into two partitions, polarity-shifted and polarity-unshifted, which are used to train two base classifiers respectively for further classifier combination. The experimental results across four different domains demonstrate the effectiveness of our approach."
Y09-2029,Bridging the Gap between Graph Modeling and Developmental Psycholinguistics: An Experiment on Measuring Lexical Proximity in {C}hinese Semantic Space,2009,6,2,5,1,2402,shukai hsieh,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Modeling of semantic space is a new and challenging research topic both in cog- nitive science and linguistics. Existing approaches can be classified into two different types according to how the calculation are done: either a word-by-word co-occurrence matrix or a word-by-context matrix (Riordan 2007). In this paper, we argue that the existing popular distributional semantic model (vector space model), does not adequately explain the age-of- acquisition data in Chinese. An alternatively measure of semantic proximity called PROX (Gaume et al, 2006) is applied instead. The application or PROX has interesting psycholin- guistic implications. Unlike previous semantic space models, PROX can be trained with children's data as well as adult data. This allows us to test the hypothesis that children's se- mantic space approximates the target of acquisition: adult's semantic space. It also allows us to compare our Chinese experiment results with French results to see to attest the universality of the approximation model."
Y09-2034,Word Boundary Decision with {CRF} for {C}hinese Word Segmentation,2009,10,6,2,1,9448,shoushan li,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 2",0,"Chinese word segmentation systems necessarily perform both accurately and quickly for real applications. In this paper, we study on word boundary decision (WBD) approach for Chinese word segmentation and implement it as a 2-tag character tagging with conditional random filed (CRF). With a help of tag transition features, WBD with CRF segmentation approach can achieve comparative performances compared to 4-tag character tagging approach (represents the state-of-the-art segmentation approach). But it requires only about half training time and memory space as much as 4-tag character tagging approach. These results encourage that WBD segmentation approach is a good choice for real Chinese word segmentation systems."
Y09-1010,An Integrated Approach to Heterogeneous Data for Information Extraction,2009,16,0,3,1,12273,ying chen,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"The paper proposes an integrated framework for web personal information extraction, such as biographical information and occupation, and those kinds of information are necessary to further construct a social network (a kind of semantic web) for a person. As web data is heterogeneous in nature, most of IE systems, regardless of named entity recognition (NER) or relation detection and recognition (RDR) systems, fail to get reliably robust results. We propose a flexible framework, which can effectively complement stateof-the-art statistical IE systems with rule-based IE systems for web data, and achieves substantial improvement over other existing systems. In particular, in our current experiment, both the rule-based IE system, which is designed according to some web specific expression patterns, and the statistical IE systems, which are developed for some homogeneous corpora, are sensitive only to specific information types. Hence we argue that our system performance can be incrementally improved when new and effective IE systems are added into our framework."
Y09-1011,Are Emotions Enumerable or Decomposable? And its Implications for Emotion Processing,2009,-1,-1,3,1,12273,ying chen,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,None
Y09-1031,{C}hinese {W}ord{N}et Domains: Bootstrapping {C}hinese {W}ord{N}et with Semantic Domain Labels,2009,18,2,3,0,1205,lunghao lee,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"We bootstrapped Chinese WordNet with semantic domain labels of WordNet Domains for constructing a language resource called Chinese WordNet Domains. The bootstrapping methods work from three aspects: 1) Princeton WordNet alignment, 2) lexical semantic relations and 3) domain taxonomy mapping. Experimental results of our proposed bootstrapping based domain predication achieve satisfying effects. We believe the resulting Chinese WordNet Domains will be the first oriental language resource, which can be used to interoperate with the existing WordNet Domains of several languages and benefits for cross-language and domain-specific researches and applications. In addition, we also plan to release resulting Chinese WordNet Domains to the community for research purposes."
Y09-1032,Cause Event Representations for Happiness and Surprise,2009,-1,-1,3,1,17031,sophia lee,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,None
Y09-1033,Sentiment Classification Considering Negation and Contrast Transition,2009,12,11,2,1,9448,shoushan li,"Proceedings of the 23rd Pacific Asia Conference on Language, Information and Computation, Volume 1",0,"Negation and contrast transition are two kinds of linguistic phenomena which are popularly used to reverse the sentiment polarity of some words and sentences. In this paper, we propose an approach to incorporate their classification information into our sentiment classification system: First, we classify sentences into sentiment reversed and non-reversed parts. Then, represent them as two different bags-of-words. Third, present three general strategies to do classification with two-bag-of-words modeling. We collect a large-scale product reviews involving five domains and conduct our experiments on them. The experimental results show that incorporating both negation and contrast transition information is effective and performs robustly better than traditional machine learning approach (based on one-bag-of-words modeling) across five different domains."
W09-3418,{CWN}-{LMF}: {C}hinese {W}ord{N}et in the {L}exical {M}arkup {F}ramework,2009,17,8,3,0,1205,lunghao lee,Proceedings of the 7th Workshop on {A}sian Language Resources ({ALR}7),0,"Lexical Markup Framework (LMF, ISO-24613) is the ISO standard which provides a common standardized framework for the construction of natural language processing lexicons. LMF facilitates data exchange among computational linguistic resources, and also promises a convenient uniformity for future application. This study describes the design and implementation of the WordNet-LMF used to represent lexical semantics in Chinese WordNet. The compiled CWN-LMF will be released to the community for linguistic researches."
W09-3421,Query Expansion using {LMF}-Compliant Lexical Resources,2009,8,4,9,0,301,takenobu tokunaga,Proceedings of the 7th Workshop on {A}sian Language Resources ({ALR}7),0,"This paper reports prototype multilingual query expansion system relying on LMF compliant lexical resources. The system is one of the deliverables of a three-year project aiming at establishing an international standard for language resources which is applicable to Asian languages. Our important contributions to ISO 24613, standard Lexical Markup Framework (LMF) include its robustness to deal with Asian languages, and its applicability to cross-lingual query tasks, as illustrated by the prototype introduced in this paper."
W09-3303,{W}iktionary for Natural Language Processing: Methodology and Limitations,2009,0,2,8,0,38143,emmanuel navarro,Proceedings of the 2009 Workshop on The People{'}s Web Meets {NLP}: Collaboratively Constructed Semantic Resources (People{'}s Web),0,None
W09-3001,A Cognitive-based Annotation System for Emotion Computing,2009,14,16,3,1,12273,ying chen,Proceedings of the Third Linguistic Annotation Workshop ({LAW} {III}),0,"Emotion computing is very important for expressive information extraction. In this paper, we provide a robust and versatile emotion annotation scheme based on cognitive emotion theories, which not only can annotate both explicit and implicit emotion expressions, but also can encode different levels of emotion information for the given emotion content. In addition, motivated by a cognitive framework, an automatic emotion annotation system is developed, and large and comparatively high-quality emotion corpora are created for emotion computing, one in Chinese and the other in English. Such an annotation system can be easily adapted for different kinds of emotion applications and be extended to other languages."
P09-5001,Fundamentals of {C}hinese Language Processing,2009,3,0,1,1,1504,churen huang,Tutorial Abstracts of {ACL}-{IJCNLP} 2009,0,"This tutorial gives an introduction to the fundamentals of Chinese language processing for text processing. Today, more and more Chinese information are available in electronic form and over the internet. Computer processing of Chinese text requires the understanding of both the language itself and the technology to handle them. This tutorial is targeted for both Chinese linguists who are interested in computational linguistics and computer scientists who are interested in research on processing Chinese."
P09-1078,A Framework of Feature Selection Methods for Text Categorization,2009,19,87,4,1,9448,shoushan li,Proceedings of the Joint Conference of the 47th Annual Meeting of the {ACL} and the 4th International Joint Conference on Natural Language Processing of the {AFNLP},1,"In text categorization, feature selection (FS) is a strategy that aims at making text classifiers more efficient and accurate. However, when dealing with a new task, it is still difficult to quickly select a suitable one from various FS methods provided by many previous studies. In this paper, we propose a theoretic framework of FS methods based on two basic measurements: frequency measurement and ratio measurement. Then six popular FS methods are in detail discussed under this framework. Moreover, with the guidance of our theoretical analysis, we propose a novel method called weighed frequency and odds (WFO) that combines the two measurements with trained weights. The experimental results on data sets from both topic-based and sentiment classification tasks show that this new method is robust across different tasks and numbers of selected features."
Y08-1018,An Ontology of {C}hinese Radicals: Concept Derivation and Knowledge Representation based on the Semantic Symbols of the Four Hoofed-Mammals,2008,4,5,1,1,1504,churen huang,"Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation",0,"Semantic symbols are essential components of Chinese characters. ShuoWenJieZi (Xyu Shen 121), the oldest dictionary of Chinese, is organized according to the radical forms as semantic symbols. Characters are classified according radicals, and their meanings cluster around the basic concept of the semantic symbol. We believe that ShuoWenJieZi radicals systemreflect conventional conceptualization when Chinese character orthography was invented. In this research, we use the semantic symbols representing four hoofed-mammals in ShuoWenJieZi ,xe2x80x9cbovid,xe2x80x9d xe2x80x9cdeer,xe2x80x9d xe2x80x9ccattle,xe2x80x9d and xe2x80x9chorse,xe2x80x9d as our research objects. In principle we assume that semantic symbols represent basic concepts, and further more we distinguish the relations between derived characters and each basic concept to construct a conventionalized ontology headed by basic concepts expressed by the semantic symbols. Our analysis and comparative studies of the semantic symbol ontologies for the four hoofed-mammals show that they share similar conceptual structures strongly motivated by their functions in human society. In particular, we show that the conceptual dependencies between the basic concept of a radical and the meanings of the derived characters can be explained by an enriched version of the Generative Lexicon."
Y08-1042,Contrastive Approach towards Text Source Classification based on Top-Bag-of-Word Similarity,2008,19,20,1,1,1504,churen huang,"Proceedings of the 22nd Pacific Asia Conference on Language, Information and Computation",0,"This paper proposes a method to automatically classify texts from different varieties of the same language. We show that similarity measure is a robust tool for studying comparable corpora of language variations. We take LDCxe2x80x99s Chinese Gigaword Corpus composed of three varieties of Chinese from Mainland China, Singapore, and Taiwan, as the comparable corpora. Top-bag-of-word similarity measures reflect distances among the three varieties of the same language. A Top-bag-of-word similarity based contrastive approach was taken to solve the text source classification problem. Our results show that a contrastive approach using similarity to rule out identity of source and to arrive actual source by inference is more robust that directly confirmation of source by similarity. We show that this approach is robust when applied to other texts."
W08-1907,Multilingual Conceptual Access to Lexicon based on Shared Orthography: An ontology-driven study of {C}hinese and {J}apanese,2008,15,3,1,1,1504,churen huang,Coling 2008: Proceedings of the Workshop on Cognitive Aspects of the Lexicon ({COGALEX} 2008),0,"In this paper we propose a model for conceptual access to multilingual lexicon based on shared orthography. Our proposal relies crucially on two facts: That both Chinese and Japanese conventionally use Chinese orthography in their respective writing systems, and that the Chinese orthography is anchored on a system of radical parts which encodes basic concepts. Each orthographic unit, called hanzi and kanji respectively, contains a radical which indicates the broad semantic class of the meaning of that unit. Our study utilizes the homomorphism between the Chinese hanzi and Japanese kanji systems to identify bilingual word correspondences. We use bilingual dictionaries, including WordNet, to verify semantic relation between the crosslingual pairs. These bilingual pairs are then mapped to an ontology constructed based on relations to the relation between the meaning of each character and the basic concept of their radical parts. The conceptual structure of the radical ontology is proposed as a model for simultaneous conceptual access to both languages. A study based on words containing characters composed of the (mouth) radical is given to illustrate the proposal and the actual model. The fact that this model works for two typologically very different languages and that the model contains generative lexicon like coersive links suggests that this model has the conceptual robustness to be applied to other languages."
O08-2008,å¤é åæä»¶éä¹è©å½æ¦å¿µæ´å±èç¥è­æ¶æ§ä¹å»ºç« (Conceptual Expansion and Ontological Mapping of Multi-domain Documents) [In {C}hinese],2008,0,0,4,0,47959,yongxiang chen,{ROCLING} 2008 Poster Papers,0,None
O08-1009,A Realistic and Robust Model for {C}hinese Word Segmentation,2008,13,2,1,1,1504,churen huang,Proceedings of the 20th Conference on Computational Linguistics and Speech Processing,0,"A realistic Chinese word segmentation tool must adapt to textual variations with minimal training input and yet robust enough to yield reliable segmentation result for all variants. Various lexicon-driven approaches to Chinese segmentation, e.g. [1,16], achieve high f-scores yet require massive training for any variation. Text-driven approach, e.g. [12], can be easily adapted for domain and genre changes yet has difficulty matching the high f-scores of the lexicon-driven approaches. In this paper, we refine and implement an innovative text-driven word boundary decision (WBD) segmentation model proposed in [15]. The WBD model treats word segmentation simply and efficiently as a binary decision on whether to realize the natural textual break between two adjacent characters as a word boundary. The WBD model allows simple and quick training data preparation converting characters as contextual vectors for learning the word boundary decision. Machine learning experiments with four different classifiers show that training with 1,000 vectors and 1 million vectors achieve comparable and reliable results. In addition, when applied to SigHAN Bakeoff 3 competition data, the WBD model produces OOV recall rates that are higher than all published results. Unlike all previous work, our OOV recall rate is comparable to our own F-score. Both experiments support the claim that the WBD model is a realistic model for Chinese word segmentation as it can be easily adapted for new variants with robust result. In conclusion, we will discuss linguistic ramifications as well as future implications for the WBD approach."
huang-etal-2008-quality,Quality Assurance of Automatic Annotation of Very Large Corpora: a Study based on heterogeneous Tagging System,2008,9,7,1,1,1504,churen huang,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,We propose a set of heuristics for improving annotation quality of very large corpora efficiently. The Xinhua News portion of the Chinese Gigaword Corpus was tagged independently with both the Peking University ICL tagset and the Academia Sinica CKIP tagset. The corpus-based POS tags mapping will serve as the basis of the possible contrast in grammatical systems between PRC and Taiwan. And it can serve as the basic model for mapping between the CKIP and ICL tagging systems for any data.
vossen-etal-2008-kyoto,"{KYOTO}: a System for Mining, Structuring and Distributing Knowledge across Languages and Cultures",2008,32,44,6,0,5469,piek vossen,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"We outline work performed within the framework of a current EC project. The goal is to construct a language-independent information system for a specific domain (environment/ecology/biodiversity) anchored in a language-independent ontology that is linked to wordnets in seven languages. For each language, information extraction and identification of lexicalized concepts with ontological entries is carried out by text miners (ÂKybotsÂ). The mapping of language-specific lexemes to the ontology allows for crosslinguistic identification and translation of equivalent terms. The infrastructure developed within this project enables long-range knowledge sharing and transfer across many languages and cultures, addressing the need for global and uniform transition of knowledge beyond the specific domains addressed here."
chung-etal-2008-extracting,Extracting Concrete Senses of Lexicon through Measurement of Conceptual Similarity in Ontologies,2008,5,0,6,1,15600,siawfong chung,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"The measurement of conceptual similarity in a hierarchical structure has been proposed by studies such as Wu and Palmer (1994) which have been summarized and evaluated in Budanisky and Hirst (2006). The present study applies the measurement of conceptual similarity to conceptual metaphor research by comparing concreteness of ontological resource nodes to several prototypical concrete nodes selected by human subjects. Here, the purpose of comparing conceptual similarity between nodes is to select a concrete sense for a word which is used metaphorically. Through using WordNet-SUMO interface such as SinicaBow (Huang, Chang and Lee, 2004), concrete senses of a lexicon will be selected once its SUMO nodes have been compared in terms of conceptual similarity with the prototypical concrete nodes. This study has strong implications for the interaction of psycholinguistic and computational linguistic fields in conceptual metaphor research."
tokunaga-etal-2008-adapting,Adapting International Standard for {A}sian Language Technologies,2008,8,6,3,0,301,takenobu tokunaga,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Corpus-based approaches and statistical approaches have been the main stream of natural language processing research for the past two decades. Language resources play a key role in such approaches, but there is an insufficient amount of language resources in many Asian languages. In this situation, standardisation of language resources would be of great help in developing resources in new languages. This paper presents the latest development efforts of our project which aims at creating a common standard for Asian language resources that is compatible with an international standard. In particular, the paper focuses on i) lexical specification and data categories relevant for building multilingual lexical resources for Asian languages; ii) a core upper-layer ontology needed for ensuring multilingual interoperability and iii) the evaluation platform used to test the entire architectural framework."
chou-etal-2008-extended,The Extended Architecture of Hantology for {J}apan Kanji,2008,0,2,2,1,47717,yamin chou,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Chinese writing system is not only used by Chinese but also used by Japanese. The motivation of this paper is to extend the architecture of Hantology which describes the features of Chinese writing system to integrate Japan Kanji and Chinese characters into the same ontology. The problem is Chinese characters adopted by Japan have been changed, thus, the modification of the original architecture of Hantology is needed. A extended architecture consists orthographic, pronunciation, sense and derived lexicon dimensions. is proposed in this paper. The contribution of this study is that the extension architecture of Hantology provides a platform to analyze the variation of Chinese characters used in Japan. The analytic results of variation for a specific Kanji can be integrated into Hantology, so it is easier to study the variation of Chinese characters systematically"
I08-1052,Constructing Taxonomy of Numerative Classifiers for {A}sian Languages,2008,5,9,3,0,29633,kiyoaki shirai,Proceedings of the Third International Joint Conference on Natural Language Processing: Volume-{I},0,"Numerative classifiers are ubiquitous in many Asian languages. This paper proposes a method to construct a taxonomy of numerative classifiers based on a nounclassifier agreement database. The taxonomy defines superordinate-subordinate relation among numerative classifiers and represents the relations in tree structures. The experiments to construct taxonomies were conducted for evaluation by using data from three different languages: Chinese, Japanese and Thai. We found that our method was promising for Chinese and Japanese, but inappropriate for Thai. It confirms that there really is no hierarchy among Thai classifiers."
Y07-1012,Computing Thresholds of Linguistic Saliency,2007,0,0,4,1,15600,siawfong chung,"Proceedings of the 21st Pacific Asia Conference on Language, Information and Computation",0,None
Y07-1015,The Polysemy of Da3: An ontology-based lexical semantic study,2007,9,1,2,1,37549,jiafei hong,"Proceedings of the 21st Pacific Asia Conference on Language, Information and Computation",0,"In this study, we explore the polysemy of da3 through the ontological conceptual structure found in SUMO. First, we divide several different senses for da3, clustering physical event senses and metaphorical event senses. In here, we only focus on physical event senses of da3. From the physical event senses of da3, we divide them into two main categories: 1) hit and 2) pump. We then use SUMO ontological concepts to identify these physical senses. Finally, we can observe the common patterns of the xe2x80x9chitxe2x80x9d sense group and the xe2x80x9cpumpxe2x80x9d sense group for da3."
P07-2018,"Rethinking {C}hinese Word Segmentation: Tokenization, Character Classification, or Wordbreak Identification",2007,6,27,1,1,1504,churen huang,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"This paper addresses two remaining challenges in Chinese word segmentation. The challenge in HLT is to find a robust segmentation method that requires no prior lexical knowledge and no extensive training to adapt to new types of data. The challenge in modelling human cognition and acquisition it to segment words efficiently without using knowledge of wordhood. We propose a radical method of word segmentation to meet both challenges. The most critical concept that we introduce is that Chinese word segmentation is the classification of a string of character-boundaries (CB's) into either word-boundaries (WB's) and non-word-boundaries. In Chinese, CB's are delimited and distributed in between two characters. Hence we can use the distributional properties of CB among the background character strings to predict which CB's are WB's."
P07-2039,Automatic Discovery of Named Entity Variants: Grammar-driven Approaches to Non-Alphabetical Transliterations,2007,16,2,1,1,1504,churen huang,Proceedings of the 45th Annual Meeting of the Association for Computational Linguistics Companion Volume Proceedings of the Demo and Poster Sessions,0,"Identification of transliterated names is a particularly difficult task of Named Entity Recognition (NER), especially in the Chinese context. Of all possible variations of transliterated named entities, the difference between PRC and Taiwan is the most prevalent and most challenging. In this paper, we introduce a novel approach to the automatic extraction of diverging transliterations of foreign named entities by bootstrapping co-occurrence statistics from tagged and segmented Chinese corpus. Preliminary experiment yields promising results and shows its potential in NLP applications."
O07-2003,"ä»¥ä¸­æååè©èªæåº«çºåºç¤ä¹å\
©å²¸è©å½å°æ¯ç ç©¶ (A Study of Lexical Differences between {C}hina and {T}aiwan based on the {C}hinese {G}igaword Corpus) [In {C}hinese]",2007,-1,-1,2,0,49249,jiafei hung,{ROCLING} 2007 Poster Papers,0,None
O07-2006,"ä¸­æè©ç¾©å\
¨ææ¨è¨èªæåº«ä¹è¨­è¨èéå½¢è£½ä½ (Design and Prototype of a Fully Sense-tagged Corpus) [In {C}hinese]",2007,0,0,2,0,49250,sujin ker,{ROCLING} 2007 Poster Papers,0,None
Y06-1024,Using {C}hinese {G}igaword Corpus and {C}hinese Word Sketch in linguistic Research,2006,3,13,2,1,37549,jiafei hong,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"We explore the possibility of deeper linguistic research based on corpus and computational linguistic tools in this paper. In particular, we adopt Chinese Word Sketch, the application of Word Sketch Engine to Chinese GigaWord Corpus, for linguistic research. We apply Chinese Sketch Engine results to deeper linguistic account such as selectional restriction and event type selection. The study is based on the comparison of two basic verbs of ingestion: chi1 xe2x80x98to eatxe2x80x99 and he1 xe2x80x98to drinkxe2x80x99."
Y06-1027,Knowledge-Rich Approach to Automatic Grammatical Information Acquisition: Enriching {C}hinese {S}ketch {E}ngine with a Lexical Grammar,2006,7,0,1,1,1504,churen huang,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"This paper discusses the implementation of a knowledge-rich approach to automatic acquisition of grammatical information. Our study is based on Word Sketch Engine (Kilgarriff and Tudgell 2002). The original claims of WSE are two folded: that linguistic generalizations can be automatically extracted from a corpus with simple collocation information provided that the corpus is large enough; and that such a methodology is easily adaptable for a new language. Our work on Chinese Sketch Engine attests to the claim the WSE is adaptable for a new language. More critically, we show that the quality of grammatical information provided has a directly bearing on the result of grammatical information acquisition. We show that when provided with a knowledge rich lexical grammar, both the quantity and quality of the extracted knowledge improves substantially over the results with simple PS rules. 1 Background: Word Sketch Engine and Automatic Acquisition of Grammatical Information The original goal of corpus-based studies was to provide xe2x80x98a body of evidencexe2x80x99 for more theoretical linguistic studies (Francis and Kucera 1965). However, corpus-based studies evolved with the improvements made in electronic data manipulation, making of automatic acquisition of grammatical information a goal of computational linguistics, computational lexicography, as well as theoretical corpus linguistics. Previous works that made significant contribution to the study of automatic extraction of grammatical relation includes Sinclairxe2x80x99s (1987) work on KWIC, Church and Hanksxe2x80x99 (1989) introduction of Mutual Information, and Linxe2x80x99s (1998) introduction of relevance measurement. Kilgarriff and colleaguesxe2x80x99 work on Word Sktech Engine (WSE) makes a bold step forwards in automatic linguistic knowledge acquisition (Kilgarriff and Tudgell 2002, Kilgarriff et al. 2004). The main claim is that a xe2x80x98gargantuanxe2x80x99 corpus 1 contains enough distributional information about most grammatical dependencies in a language such that the set of simple collocational patterns will allow automatic extraction of grammatical relations and other grammatical information. Crucially, the validity of the extracted information does not rely on the preciseness of the rules or the perfect grammaticality of the data. Instead, WSE allows the presence of ungrammatical examples in the corpus and the possibility for collocational patterns to occasionally identify the wrong lexical pairs. WSE assumes that these anomalies will be statistically insignificant, especially when there are enough examples instantiating the intended grammatical information. In addition, WSE relies on Salience measurement to rank the significance of all attested relations. Salience is calculated by MI of a relation multiplied with the frequency of the relation, in order to correct MIxe2x80x99s bias towards low frequency items. WSE follows Linxe2x80x99s (1998) formulation of MI of relations, where ||w1, R, w2|| stands for the frequency of the relation R between w1 and, w2. A wild card * can occurs in place of w1, R, or w2 to represent the all cases. Hence MI between w1, and w2 given a relation R is given below (Kilgarriff and Tudgell 2002): 1 The required corpus size was not specified in WSE literature. However, we estimate from existing work that for WSE to be efficient, corpus scale must be 100 millions words or above."
Y06-1043,Using the {S}wadesh list for creating a simple common taxonomy,2006,18,1,2,1,11527,laurent prevot,"Proceedings of the 20th Pacific Asia Conference on Language, Information and Computation",0,"One of the main goal of xe2x80x99Developing International Standards of Language Resources for Semantic Web Applicationsxe2x80x99 [Takenobu et al., 2006], an international project sponsored by Japanxe2x80x99s NEDO foundation, is to implement standards of language resources that can be very robust when applied to different languages of the world. In addition, the project concentrates on Asian language resources. Hence the project plans to construct lexica of Japanese, Mandarin, Thai and Italian; and integrate them as parts of a multilingual resource linked to the original Princeton WordNet (WN) [Fellbaum, 1998]. It is therefore comparable with projects such as EuroWordNet [Vossen, 1998], in which the coherence and homogeneity across different languages remained the main issue that hampered the true interoperability of the final resource. Since the project is exposed to these risks by its design, a priority for the project members is therefore to tackle this issue from the very beginning. For this purpose, one of the measures taken is to build jointly an upper-level ontology1 that will play the role of a structured interlingual index. The NEDO participants are currently exploring several ways for selecting a basic vocabulary that will serve as a starting point for designing this language-independent core of the resource. This paper describes some of the preliminary experiments we are currently conducing. More precisely we are (i) using the Swadesh list [Swadesh, 1952] as a basic core vocabulary and (ii) exploring the possibilities offered by this list for creating a simple common ontology. These practical objectives confront us however with a complex discussion of contemporary linguistics, namely the relativist/universalist debate [Gumperz and Levinson, 1996], and more precisely its consequences for the lexical organization. In the context of this project the existence and the nature of a common universal structure is a background question that we would like to contribute to answer."
W06-1003,Towards Agent-based Cross-Lingual Interoperability of Distributed Lexical Resources,2006,9,6,6,0,30233,claudia soria,Proceedings of the Workshop on Multilingual Language Resources and Interoperability,0,"In this paper we present an application fostering the integration and interoperability of computational lexicons, focusing on the particular case of mutual linking and cross-lingual enrichment of two wordnets, the ItalWordNet and Sinica BOW lexicons. This is intended as a case-study investigating the needs and requirements of semi-automatic integration and interoperability of lexical resources."
P06-2050,When Conset Meets Synset: A Preliminary Survey of an Ontological Lexical Resource Based on {C}hinese Characters,2006,9,3,2,1,2402,shukai hsieh,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"This paper describes an on-going project concerning with an ontological lexical resource based on the abundant conceptual information grounded on Chinese characters. The ultimate goal of this project is set to construct a cognitively sound and computationally effective character-grounded machine-understandable resource.n n Philosophically, Chinese ideogram has its ontological status, but its applicability to the NLP task has not been expressed explicitly in terms of language resource. We thus propose the first attempt to locate Chinese characters within the context of ontology. Having the primary success in applying it to some NLP tasks, we believe that the construction of this knowledge resource will shed new light on theoretical setting as well as the construction of Chinese lexical semantic resources."
P06-2106,Infrastructure for Standardization of {A}sian Language Resources,2006,10,18,7,0,301,takenobu tokunaga,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"As an area of great linguistic and cultural diversity, Asian language resources have received much less attention than their western counterparts. Creating a common standard for Asian language resources that is compatible with an international standard has at least three strong advantages: to increase the competitive edge of Asian countries, to bring Asian countries to closer to their western counterparts, and to bring more cohesion among Asian countries. To achieve this goal, we have launched a two year project to create a common standard for Asian language resources. The project is comprised of four research items, (1) building a description framework of lexical entries, (2) building sample lexicons, (3) building an upper-layer ontology and (4) evaluating the proposed framework through an application. This paper outlines the project in terms of its aim and approach."
O06-1002,ä¸­æåè©åç©åå¤æ·ççµ±è¨å¼æ¨¡åè¨­è¨ (A Stochastic Model for Prediction of Deverbal Nouns in {M}andarin {C}hinese) [In {C}hinese],2006,0,0,2,0,8066,weiyun ma,Proceedings of the 18th Conference on Computational Linguistics and Speech Processing,0,None
O06-1003,å¤§è¦æ¨¡è©å½èªæéä¿èªåæ¨ç¤ºä¹åæ­¥ç ç©¶: ä»¥ä¸­æè©ç¶²({C}hinese {W}ordnet)çºä¾ (A Preliminary Study on Large-scale Automatic Labeling of Lexical Semantic Relations: A Case study of {C}hinese {W}ordnet) [In {C}hinese],2006,3,0,3,1,2402,shukai hsieh,Proceedings of the 18th Conference on Computational Linguistics and Speech Processing,0,None
ma-huang-2006-uniform,Uniform and Effective Tagging of a Heterogeneous Giga-word Corpus,2006,8,19,2,0,8066,weiyun ma,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Tagging as the most crucial annotation of language resources can still be challenging when the corpus size is big and when the corpus data is not homogeneous. The Chinese Gigaword Corpus is confounded by both challenges. The corpus containsroughly 1.12 billion Chinese characters from two heterogeneous sources: respective news in Taiwan and in Mainland China. In other words, in addition to its size, the data also contains two variants of Chinese that are known to exhibit substantial linguistic differences. We utilize Chinese Sketch Engine as the corpus query tool, by which grammar behaviours of the two heterogeneous resources could be captured and displayed in a unified web interface. In this paper, we report our answer to the two challenges to effectively tag this large-scale corpus. The evaluation result shows our mechanism of tagging maintains high annotation quality."
chou-huang-2006-hantology,Hantology-A Linguistic Resource for {C}hinese Language Processing and Studying,2006,6,6,2,1,47717,yamin chou,Proceedings of the Fifth International Conference on Language Resources and Evaluation ({LREC}{'}06),0,"Hantology, a character-based Chinese language resource is created to provide an infrastructure for language processing and research on the writing system. Unlike alphabetic or syllabic writing systems, the ideographic writing system of Chinese poses both a challenge and an opportunity. The challenge is that a totally different resources structure must be created to represent and process speakerÂs conventionalization of the language. The rare opportunity is that the structure itself is enriched with conceptual classification and can be utilized for ontology building. We describe the contents and possible applications of Hantology in this paper. The applications of Hantology include: (1) an account for the diachronic development of Chinese lexica (2) character-based language processing, (3) a study of conceptual structure differences in Chinese and English, and (4) comparisons of different ideographic writing systems."
Y05-1020,In and Out: Senses and Meaning Extension of {M}andarin Spatial Terms nei and wai,2005,-1,-1,3,0,49511,yiching wu,"Proceedings of the 19th Pacific Asia Conference on Language, Information and Computation",0,None
O05-5001,The Sinica Sense Management System: Design and Implementation,2005,48,11,1,1,1504,churen huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 4, {D}ecember 2005: Special Issue on Selected Papers from {CLSW}-5",0,"A sense-based lexical knowledgebase is a core foundation for language engineering. Two important criteria must be satisfied when constructing a knowledgebase: linguistic felicity and data cohesion. In this paper, we discuss how data cohesion of the sense information collected using the Sinica Sense Management System (SSMS) can be achieved. SSMS manages both lexical entries and word senses, and has been designed and implemented by the Chinese Wordnet Team at Academia Sinica. SSMS contains all the basic information that can be merged with the future Chinese Wordnet. In addition to senses and meaning facets, SSMS also includes the following information: POS, example sentences, corresponding English synset(s) from Princeton WordNet, and lexical semantic relations, such as synonym/antonym and hypernym/hyponym. Moreover, the overarching structure of the system is managed by using a sense serial number, and an inter-entry structure is established by means of cross-references among synsets and homographs. SSMS is not only a versatile development tool and management system for a sense-based lexical knowledgebase. It can also serve as the database backend for both Chinese Wordnet and any sense-based applications for Chinese language processing."
O05-5012,Source Domains as Concept Domains in Metaphorical Expressions,2005,22,11,3,1,15600,siawfong chung,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 10, Number 4, {D}ecember 2005: Special Issue on Selected Papers from {CLSW}-5",0,"The use of lexical resources in linguistic analysis has expanded rapidly in recent years. However, most lexical resources, such as WordNet or online dictionaries, at this point do not usually indicate figurative meanings, such as conceptual metaphors, as part of a lexical entry. Studies that attempt to establish the relationships between literal and figurative language by detecting the connectivity between WordNet relations usually do not deal with linguistic data directly. However, the present study demonstrates that SUMO definitions can be used to identify the source domains used in conceptual metaphors. This is achieved by identifying the relationships between metaphorical expressions and their corresponding ontological nodes. Such links are important because they show which lexical items are mapped under which concepts. This, in turn, helps specify which lexical items in electronic resources involve conceptual mappings. Looking specifically at the concept of PERSON, this work also establishes connectivity between lexical items which are related to xe2x80x9cOrganism.xe2x80x9d Therefore, the methodology reported herein not only aids the categorizing of lexical items according to their conceptual domains but also can establish links between these items. Such bottom-up and top-down analyses of lexical items may provide a means of representing metaphorical entries in lexical resources."
O05-1018,ç°é«å­èªå¢éä¿çåæèå»ºç« (A Framework for the Contextual Analysis of {C}hinese Characters Variants) [In {C}hinese],2005,-1,-1,2,1,47717,yamin chou,Proceedings of the 17th Conference on Computational Linguistics and Speech Processing,0,None
I05-7002,{H}antology: An Ontology based on Conventionalized Conceptualization,2005,0,9,2,1,47717,yamin chou,Proceedings of {O}nto{L}ex 2005 - Ontologies and Lexical Resources,0,None
I05-7003,From General Ontology to Specialized Ontology: A study based on a single author historical corpus,2005,5,0,2,1,2297,ruyng chang,Proceedings of {O}nto{L}ex 2005 - Ontologies and Lexical Resources,0,"This paper proposes to deal with the construction of a specialized ontology as the discovery of a new knowledge structure, based on the premise that ontology is a structured knowledge. Our study integrates the following approaches: the mental lexicon approach, the Shakespeareangarden approach, and the ontologymerging as ontology-discovery approach. In particular, WordNet is used both a source of lexical knowledge and a (linguistic) ontology. SUMO (Suggested Upper Merged Ontology), on the other hand, is used as an upper ontology that provides fragments of well-structures knowledge. These two resources are compared and merged through Sinica BOW (Academia Sinica Bilingual Ontological Wordnet). The domain knowledge that we model our specialized ontology on is the collection of Su-Shixe2x80x99s poems from Song dynasty. This work is chosen not only because the knowledge system is sufficiently different from the current one, but also because it is well-suited for the text-based and lexicon-driven strategy to discover knowledge structure. This study supports our text-based and lexicondriven approach as an efficient way to build a specialized ontology as well as to infer domain knowledge. Based on this result, we further outlined an architecture for a workbench for semi-automatic construction of specialized ontologies. 1 Motivation: General and Specialized Ontologies Gruber et al. (1994) described ontology as xe2x80x9can explicit specification of conceptualization.xe2x80x9d Ontologies can be used to explicitly represent structured information and support knowledge sharing and reuse. As a model for knowledge formation, the architecture of ontology critically depends on the type of knowledge to be represented. In terms of coverage, there are two contrasting types of ontology, general and specialized ontology. General Ontology is the upper ontology shared by all domains such as SUMO (Suggested Upper Merged Ontology). Specialized ontologies represent exhaustive information for certain domains, more specialized schemata must be created to make the data useful in making real world decisions. A specialized ontology may signify an ontology specific to a domain, historical period, an author etc. According to real conditions in a specific domain, reconstruction and verification of conceptual structure should be done. Another problem that arises in different space, time, and domain is knowledge processing with mismatched knowledge structure. One of the greatest challenges to the research on ontology is how to ensure both the felicity and compatibility of a new specialized ontology. Felicity here refers to the faithful and comprehensive representation of domain knowledge. Compatibility refers to the interchangeable and interpretability of the domain knowledge with regard to a shared upper ontology. In this paper, we propose that this challenge can be met when we take the creation of a specialized ontology as the merging of the segments of domain lexical knowledge to SUMO and WordNet. The mapping to SUMO ensures compatibility to an upper ontology, while the mapping to WordNet allows comprehensive representation of domain knowledge. Merging of the partially mapped ontology segments reduces the portion of knowledge representation which is missing from either prototypical ontology. 2 Research Methodology Three important attributes characterize our methodology: lexicon-driven, text-based, and ontologymerging. First, our methodology is (mental) lexicon-driven. Mental lexicon is defined as a language userxe2x80x99s knowledge of words. (Aitchison, 2003) The idea underlying our lexicon-driven approach is that concepts are stored in the mental lexicon and accessible through lexical access. In other words, we treat lexicon as a structured inventory of conceptual atoms. Second, we call our text-based approach a Shakespearean garden approach (Huang et al. 2004). A Shakespearean garden collects all the plants referred to in Shakespearean texts by identifying plants mentioned in the plays and sonnets. A Shakespearean garden is used to illustrate the flora of the Shakespearean England and gives scholars a context in which to interpret his work. For instance, a Shakespearean garden helps to illustrate how plants played important roles in medicine, religious, and history (Keyser, 2004). In our textbased approach, we do not actually grow the plants in a garden. In stead, we treat a collection of texts as an opus with an underlying knowledge structure. Since texts are composed of lexemes, we collect lexemes of a specific domain from the text just like plants are xe2x80x98collectedxe2x80x99 from Shakespearean texts. Third, we take the ontology-merging as ontology-discovery approach. This approach deals with the dilemma for the construction of a new ontology. On one hand, if no existing ontology was referred to, a new ontology could only be an reinvented wheel. On the other hand, when an existing ontology was referred to, errors could be introduced through pre-conceived conceptual structure and important generalizations could be missed. To resolve the dilemma, we propose to map conceptual atoms to two (or more) reference ontologies. The merging of two ontologies leads to three possible scenarios: matched mapping, mismatched mapping, and complimentary mapping. Matched mapping simply confirms the knowledge structure. Mismatch mapping suggests that only one or neither is correct, and possibly lead to discovery of new knowledge structure. Lastly, when concepts are not attested in either ontology, we will have complimentary mappings. In this scenario, the coverage of either ontology can be increased coverage. A previous study that is perhaps most relevant to this current work was Huang et al.xe2x80x99s (2004) study of the ontology of Tang poetry. They segmented and classified a lexicon of 300 Tang Poems (618907 A.D.), to build a small ontology of the Tang civilization. Three domain ontologies, animals, plants, and artifact, were manually constructed by mapping the extracted words to SUMO. The study was able to draw some tentative generalizations, including that the Tang civilization was primarily land-locked and that it was fascinated with flying. The first generalization is supported by the fact that the node of marine mammals as well as the dominance of hoofed animals. The second generalization is supported by the dominant frequency of birds among vertebrates, as well as the fact that insects are the only attested invertebrates (since they are the only winged invertebrates). In short, the knowledge structure of a specialized ontology helps to form new knowledge."
I05-4007,Cross-lingual Conversion of Lexical Semantic Relations: Building Parallel Wordnets,2005,3,1,1,1,1504,churen huang,Proceedings of the Fifth Workshop on {A}sian Language Resources ({ALR}-05) and First Symposium on {A}sian Language Resources Network ({ALRN}),0,None
I05-3007,{C}hinese {S}ketch {E}ngine and the Extraction of Grammatical Collocations,2005,9,21,1,1,1504,churen huang,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper introduces a new technology for collocation extraction in Chinese. Sketch Engine (Kilgarriff et al., 2004) has proven to be a very effective tool for automatic description of lexical information, including collocation extraction, based on large-scale corpus. The original work of Sketch Engine was based on BNC. We extend Sketch Engine to Chinese based on Gigaword corpus from LDC. We discuss the available functions of the prototype Chinese Sketch Engine (CSE) as well as the robustness of language-independent adaptation of Sketch Engine. We conclude by discussing how Chinese-specific linguistic information can be incorporated to improve the"
I05-3014,The Robustness of Domain Lexico-Taxonomy: Expanding Domain Lexicon with {C}i{L}in,2005,9,2,1,1,1504,churen huang,Proceedings of the Fourth {SIGHAN} Workshop on {C}hinese Language Processing,0,"This paper deals with the robust expansion of Domain LexicoTaxonomy (DLT). DLT is a domain taxonomy enriched with domain lexica. DLT was proposed as an infrastructure for crossing domain barriers (Huang et al. 2004). The DLT proposal is based on the observation that domain lexica contain entries that are also part of a general lexicon. Hence, when entries of a general lexicon are marked with their associated domain attributes, this information can have two important applications. First, the DLT will serve as seeds for domain lexica. Second, the DLT offers the most reliable evidence for deciding the domain of a new text since these lexical clues belong to the general lexicon and do occur reliably in all texts. Hence general lexicon lemmas are extracted to populate domain lexica, which are situated in domain taxonomy. Based on this previous work, we show in this paper that the original DLT can be further expanded when a new language resource is introduced. We applied CiLin, a Chinese thesaurus, and added more than 1000 new entries for DLT and show with evaluation that the DLT approach is robust since the size and number of domain lexica increased effectively."
Y04-1003,Text-based Construction and Comparison of Domain Ontology : A Study Based on Classical Poetry,2004,2,5,1,1,1504,churen huang,"Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation",0,"1. Knowledge Structure: shared upper ontology and domain ontology The fact that people from different backgrounds may have knowledge structures unlike ours is a crucial issue to be addressed in knowledge engineering. In order to become sharable and reusable knowledge, all extracted information must first be correctly situated in a knowledge structure. In addition, the situated information must be allowed to transfer from knowledge structure to knowledge structure without losing its meaningful content. This is the vision behind the Suggested Upper Merged Ontology (SUMO, http://www.ontologyportal.org) proposed by an IEEE working group. A shared upper ontology will both anchor the structured transfer of knowledge as well as set a standard for the construction of a middle and lower level ontology for each domain. This vision also has promising applications in the Semantic Web. The most salient factors dictating variations in knowledge structures are time, space, and domain. These factors are compounded with language, which is both the product and conduit of the conceptual structure of its speakers. In order to demonstrate the felicity of the shared upper ontology approach, we need to show that it can successfully applied to comparative studies of different knowledge structures regardless of their ontological variations. We apply the shared ontology proposal to the interpretation historical texts by adopting the Shakespearean-garden approach towards construction of historical ontology."
Y04-1015,Ontology-based Prediction of Compound Relations : A Study Based on {SUMO},2004,6,5,3,1,37549,jiafei hong,"Proceedings of the 18th Pacific Asia Conference on Language, Information and Computation",0,"This paper explores the interaction between conceptual structure and morpho-syntax. In particular, we show that ontology-based conceptual classification can be used to predict internal relations in compounds. We propose an ontology-based approach to predict the semantic relation between the two component words in Mandarin VV compounds. A Mandarin VV compound is classified according to the eventive relation between the two simplex verbs. These relations specify how the eventive meanings of the two simplex verbs combine to form the meaning of the compound. The three types of eventive relations that we deal with in this paper are: coordinate, modificational, and resultative. Since the way in which two events combine with each other depends upon their event types, we hypothesize that the eventive relations can be predicted by the conceptual classified event types of the two simplex verbs. An approach of ontology-based prediction is proposed based on this hypothesis. The assignment of ontology classification for each simplex verb is based on SUMO and Sinica BOW. The correlation between the ontology class of each verb position and each eventive type is trained and scored based on a manually tagged lexical database. We encode the ontology information of each VV compound in a 3-tuple based on these correlation scores. This 3-tuple is represented as a three-dimensional vector and used to predict the eventive type of new VV compounds. Our classification experiment on unknown VV compounds yields good recall and precision."
O04-1030,ä¸­å¤®ç ç©¶é¢ä¸­è±éèªç¥è­æ¬é«è©ç¶²(Sinica {BOW})ï¼çµåè©ç¶²ï¼ç¥è­æ¬é«ï¼èé åæ¨è¨çè©å½ç¥è­åº« (The Academia Sinica Bilingual Ontological {W}ordnet) [In {C}hinese],2004,-1,-1,2,1,2297,ruyng chang,Proceedings of the 16th Conference on Computational Linguistics and Speech Processing,0,None
huang-etal-2004-sinica,Sinica {BOW} (Bilingual Ontological {W}ordnet): Integration of Bilingual {W}ord{N}et and {SUMO},2004,3,61,1,1,1504,churen huang,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"The Academia Sinica Bilingual Ontological Wordnet (Sinica BOW) integrates three resources: WordNet, English-Chinese Translation Equivalents Database (ECTED), and SUMO (Suggested Upper Merged Ontology). The three resources were originally linked in two pairs: WordNet 1.6 was manually mapped to SUMO (Niles & Pease 2003) and also to ECTED (the English lemmas in WordNet were mapped to their Chinese lexical equivalents). ECTED encodes both equivalent pairs and their semantic relations (Huang et al. 2003). With the integration of these three key resources, Sinica BOW functions both as an English-Chinese bilingual wordnet and a bilingual lexical access to SUMO. Sinica BOW allows versatile access and facilitates a combination of lexical, semantic, and ontological information. Versatility is built in with its bilinguality, and the lemma-based merging of multiple resources. First, either English or Chinese can be used for the query, as well as for presenting the content of the resources. Second, the user can easily access the logical structure of both the WordNet and SUMO ontology using either words or conceptual nodes. Third, multiple linguistic indexing is built in to allow additional versatility. Fourth, domain information allows another dimension of knowledge manipulation. 1 Background and Motivation Conceptual structure and lexical access are two essential elements of human knowledge. Bilingual representation of both conceptual structure and lexical information will enable language independent knowledge processing. In this paper, we introduce a new type of integrated language resources: Bilingual Ontological Wordnet. The Academia Sinica Bilingual Ontological Wordnet (Sinica BOW) was constructed in 2003. We argue that such combination of ontology and wordnet will 1) give each linguistic form a rigorous conceptual location, 2) clarify the relation between the conceptual classification and its linguistic instantiation, and 3) facilitate genuine cross-lingual access of knowledge. 2 Resources and Structure The Academia Sinica Bilingual Ontological Wordnet (Sinica BOW) integrates three resources: WordNet, English-Chinese Translation Equivalents Database (ECTED), and SUMO (Suggested Upper Merged Ontology). WordNet is a lexical knowledgebase for English language that was created at Cognitive Science Laboratory of Princeton University in 1990 (Fellbaum 1998). Its content is divided into four categories based on psycholinguistic principles: nouns, verbs, adjectives and adverbs. WordNet organizes the lexical information according to word meaning and each synset groups together a set of lemmas sharing the same sense. In addition, WordNet is a semantic network linking synsets withvlexical semantic relations. WordNet is widely used in Natural Language Processing applications and linguistic research. The most updated version of WordNet is WordNet 2.0. We adopted WordNet 1.6., the version which is used by most applications so far. ECTED was constructed at Academia Sinica as a crucial step towards bootstrapping a Chinese wordnet with English WordNet (Huang et al. 2002, Huang et al. 2003). The translation equivalence database was hand-crafted by the WordNet team at CKIP, Academia Sinica. First, all possible Chinese translations of an English synset word (from WN 1.6.) are extracted from several available online bilingual (EC or CE) resources. These translation candidates were then checked by a team of translators with near-native bilingual ability. For each of the 99,642 English synsets, the translator selected the three most appropriate translation equivalents whenever possible. The translation equivalences were defaulted to lexicalized words, rather than descriptive phrases, whenever possible. The translation equivalences were then manually verified. Note that after the first round of translation, there were about 5% of the lemmas whose Chinese translation can neither be found in our bilingual resources nor be filled by the translators. We spent another 2 person-year consulting various special dictionaries to fill in the gaps. SUMO is a upper ontology constructed by the IEEE Standard Upper Ontology Working Group and maintained at Teknowledge Corporation. SUMO contains roughly 1,000 conceptual nodes for knowledge representation. It can be applied to automated reasoning, information retrieval and inter-operability in E-commerce, education and NLP tasks. Niles & Pease (2003) mapped synsets of WordNet and concept of SUMO in three relations: synonymy, hypernymy and instantiation. For instance, the synset animal (a living organism characterized by voluntary movement) in WordNet is synonymous with the SUMO concept of Animal. In bank (a financial institution that accepts deposits and channels the money into lending activities) this case, bank is a corporation that is a hypernym of the associated synset. President of the United States (the office of the US head of state) is an instantiation of position concept. Through the linking and the interface available at the SUMO website (http://ontology.teknowledge.com), each English lemma can be mapped to a SUMO ontology node. Figure 1: The resource and structure of Sinica BOW The three above resources were originally linked in two pairs: WordNet 1.6 was mapped to SUMO by Niles and Pease. ECTED maps English synsets in WordNet to Chinese lexical equivalents, which encodes both equivalent pairs and their semantic relations (Huang et al. 2003). WordNet synsets thus became the natural mediation for our integration work. Thus, with the integration of these three key resources, Sinica BOW can function both as an English-Chinese bilingual wordnet and a bilingual lexical access to SUMO. In other words, Sinica BOW allows a 2x2x2 query design, where a query could be in either Chinese or English, either in lexical lemmas of SUMO terms, and the query target can either be the wordnet content or the SUMO ontology. The design of Sinica BOW has an additional domain information layer, as shown in Figure 1. The domain information will be represented by a set of Domain Lexico-Taxonomy (DLT, Huang, Li, & Hong 2004). In this design, our main concern is domain inter-operability. It can be safely assumed that domain exclusive words (i.e. lemma-sense pairs) are recorded only in domain lexicon, hence there will be no ambiguity and no inter-operability issues. We concentrate instead on the lexical items that intersect with the general lexicon. On one hand, since these are the lemmas that may occur in more than one domain with one or more different meanings, domain specification would help resolving the ambiguity. On the other hand, these general lemmas with domain applicability can be effective signatures for the applicable domains. The real challenge to domain inter-operability involves the 'unknown' domains where no comprehensive domain lexica/corpora are available. We argue that this problem can be greatly ameliorated by tagging the general lexicon with possible domain tags. When domain tags are assigned to lemmas whenever possible, the general lexicon will contain substantial partial domain lexica. Although we cannot expect to construct full-scale domain lexica within the general lexicon, these domain-tagged lexical items 3 Presentational Versatility Sinica BOW allows versatile access and facilitates a combination of lexical semantic and ontological information. The versatility is built in with bilinguality, and lemma-based merging of multiple language sources. The versatility and combinatory presentation is crucial to the presentation of a knowledge system.. 3.1 Lexicon-driven Access Since the main goal of Sinica BOW concerns knowledge representation, the lemma based or conceptual node based query results are directed linked to the full knowledgebase and expandable. The Sinica BOW access is lexicon-driven. Each query returns a structured lexical entry, presented as a tree-structured menu. A keyword query returns with a menu arranged according to word senses, as shown in Figure 2. The top level information returned including POS, usage ranking, and cross-reference links. In addition to wordnet information, cross-references to up to five resources are pre-compiled for either language. For an English word, the main resource is of course the bilingual wordnet information that our team constructed. Major outside references are listed for quick hyperlink. These include corpora and both EC and CE dictionaries. For Chinese, the main resource is again our bilingual wordnet. In addition, links are established to Sinica Corpus, to Wen-Land (a learner's Lexical KnowledgeNet), and to online monolingual and bilingual dictionaries. In addition to online access of multiple sources information, each lemma's distribution in these resources is also a good indicator of its usage level. The access to the ontology and the domain taxonomy are also lexicon-driven. That is, in addition to using the pre-defined ontology or domain terms (in either English or Chinese), a query based on a lexical term is also possible. For SUMO, it will return a node where the word appears in. It can also be achieved by looking up the ontological or domain node the word belongs to. One last but critical feature of the lexicon-driven access is the possibility to re-start a query with any lexical node. When expansion reaches at the leave node and results in a new word, clicking on the word is equivalent to start a new keyword search. WorNet Offset SUMO Domain Domain Lexicons ECTED WordNet Sinica BOW"
zhang-etal-2004-distributional,Distributional Consistency: As a General Method for Defining a Core Lexicon,2004,8,10,2,1,41975,huarui zhang,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We propose Distributional Consistency (DC) as a general method for defining a Core Lexicon. The property of DC is investigated theoretically and empirically, showing that it is clearly distinguishable from word frequency and range of distribution. DC is also shown to reflect intuitive interpretations, especially when its value is close to 1. Its immediate application in NLP would include defining a core lexicon in a language and identifying topical words in a document. We also categorize the existent measures of dispersion into 3 groups via ratio of norm or entropy, proposed a simplified measure and a combined kind of measure. These new measures can be used as virtual prototype or medium type for the study and comparison of existent measures in the future."
Y03-1013,The Semantics of Shapes : A Study based on {M}andarin Quan1zi5(åå­),2003,8,0,2,0,50745,cuixia weng,"Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation",0,"Mandarin shape nouns, such as a fang1xing2 xe2x80x98squarexe2x80x99 and san1jiao3xing2 xe2x80x98trianglexe2x80x99, share a set of very interesting lexical semantic features. These nouns can refer to either the contour (i.e. the outside edge) or the enclosed area of a shape. In this paper, we will try to explain this lexical semantic fact in terms of the cognitive theories of grounding and visualization. Our study will focus on quan1zi5 xe2x80x98circlexe2x80x99, which has the typical semantic behaviors of the shape nouns, but also allows two additional interesting meaning extensions."
Y03-1020,The Semantics of Onomatopoeic Speech Act Verbs,2003,12,0,2,0,52532,ini tsai,"Proceedings of the 17th Pacific Asia Conference on Language, Information and Computation",0,"This paper attempts to explore the semantics of onomatopoeic speech act verbs. Language abounds with small bits of utterances to show speaker's emotions, to maintain the flow of speech and to do some daily exchange routines. These tiny vocalizations have been regarded as vocal gestures and largely studied under the framework of 'interjection'. In this paper, the emphasis is placed on the perlocutionary force the vocal tokens contain. We describe their conventionalized lexical meaning and term them as onomatopoeic speech act verb. An onomatopoeic speech act verb refers to a syntactically independent monomorphemic utterance which performs illocutionary or perlocutionary forces. It is normally directed at the listener, which making the recipient to do something or to solicit recipient's response or reaction. They are onomatopoeic because most of them are imitation of the sounds produced by doing some actions."
W03-1405,Conceptual Metaphors: Ontology-based Representation and Corpora Driven Mapping Principles,2003,9,39,3,0.929145,15811,kathleen ahrens,Proceedings of the {ACL} 2003 Workshop on the Lexicon and Figurative Language,0,"The goal of this paper is to integrate the Conceptual Mapping Model with an ontology-based knowledge representation (i.e. Suggested Upper Merged Ontology (SUMO)) in order to demonstrate that conceptual metaphor analysis can be restricted and eventually, automated. In particular, we will propose a corpora-based operational definition for Mapping Principles, which are explanations of why a conventional conceptual metaphor has a particular source-target domain pairing. This paper will examine 2000 random examples of 'economy' (jingji) in Mandarin Chinese and postulate Mapping Principles based frequency and delimited with SUMO."
O03-1006,{ECONOMY} {IS} {A} {PERSON}: A {C}hinese-{E}nglish Corpora and Ontological-based Comparison Using the {C}onceptual {M}apping {M}odel,2003,7,16,3,1,15600,siawfong chung,Proceedings of Research on Computational Linguistics Conference {XV},0,"This paper proposes a corpora-based approach in comparing the Mapping Principles for economy metaphors in English and Chinese. The Mapping Principles are validated using an upper ontology (SUMO). This research extends on the work of Ahrens, Chung and Huang (2003) by examining the xe2x80x98economyxe2x80x99 metaphors in Chinese and English. In Ahrens, Chung and Huang (2003), they proposed to delimit the Mapping Principle via two steps: First, they used a corpora-based analysis on the word jingji xe2x80x98economyxe2x80x99 to find out the most prototypical mappings in a metaphor Second, they used an upper ontology (SUMO) to examine whether the mapping principle is a representation of conceptual knowledge in the ontology. This paper goes a step further by examining the similarities and differences of source domains in English and Chinese. Using the Conceptual Mapping Model, this paper looks particularly into the example of ECONOMY IS A PERSON. This paper observes the representation of shared knowledge in the source domain in different languages and explains the similarities and differences by looking into the definition of inference rules in the upper ontology of SUMO."
Y02-1031,The Structure of Polysemy : A Study of Multi-sense Words Based on {W}ord{N}et,2001,-1,-1,4,0,53090,jenyi lin,"Proceedings of the 16th Pacific Asia Conference on Language, Information and Computation",0,None
W02-1805,Categorical Ambiguity and Information Content: A Corpus-based Study of {C}hinese,2002,5,2,1,1,1504,churen huang,{COLING}-02: The First {SIGHAN} Workshop on {C}hinese Language Processing,0,"Assignment of grammatical categories is the fundamental step in natural language processing. And ambiguity resolution is one of the most challenging NLP tasks that is currently still beyond the power of machines. When two questions are combined together, the problem of resolution of categorical ambiguity is what a computational linguistic system can do reasonably good, but yet still unable to mimic the excellence of human beings. This task is even more challenging in Chinese language processing because of the poverty of morphological information to mark categories and the lack of convention to mark word boundaries. In this paper, we try to investigate the nature of categorical ambiguity in Chinese based on Sinica Corpus. The study differs crucially from previous studies in that it directly measure information content as the degree of ambiguity. This method not only offers an alternative interpretation of ambiguity, it also allows a different measure of success of categorical disambiguation. Instead of precision or recall, we can also measure by how much the information load has been reduced. This approach also allows us to identify which are the most ambiguous words in terms of information content. The somewhat surprising result actually reinforces the Saussurian view that underlying the systemic linguistic structure, assignment of linguistic content for each linguistic symbol is arbitrary."
W02-1205,{OLACMS}: Comparisons and Applications in {C}hinese and Formosan Languages,2002,5,1,2,1,2297,ruyng chang,{COLING}-02: The 3rd Workshop on {A}sian Language Resources and International Standardization,0,OLACMS (stands for Open Language Archives Community Metadata Set) is a standard for describe language resources. This paper provides suggestion to OLACMS 0.4 version by comparing it with other standards and applying it to Chinese and Formosan languages.
W02-1102,Induction of Classification from Lexicon Expansion: Assigning Domain Tags to {W}ord{N}et Entries,2002,4,8,2,0,53200,echa chang,{COLING}-02: {SEMANET}: Building and Using Semantic Networks,0,"We present in this paper a series of induced methods to assign domain tags to WordNet entries. Our prime objective is to enrich the contextual information in WordNet specific to each synset entry. By using the available lexical sources such as Far East Dictionary and the contextual information in WordNet itself, we can find a foundation upon which we can base our categorization. Next we further examine the similarity between common lexical taxonomy and the semantic hierarchy of WordNet. Based on this observation and the knowledge of other semantic relations we enlarge the coverage of our findings in a systematic way. Evaluation of the results shows that we achieved reasonable and satisfactory accuracy. We propose this as the first step of wordnet expansion into a bona fide semantic network linked to real-world knowledge."
W02-1106,Translating Lexical Semantic Relations: The First Step towards Multilingual Wordnets,2002,2,17,1,1,1504,churen huang,{COLING}-02: {SEMANET}: Building and Using Semantic Networks,0,"Establishing correspondences between wordnets of different languages is essential to both multilingual knowledge processing and for bootstrapping wordnets of low-density languages. We claim that such correspondences must be based on lexical semantic relations, rather than top ontology or word translations. In particular, we define a translation equivalence relation as a bilingual lexical semantic relation. Such relations can then be part of a logical entailment predicting whether source language semantic relations will hold in a target language or not. Our claim is tested with a study of 210 Chinese lexical lemmas and their possible semantic relations links bootstrapped from the Princeton WordNet. The results show that lexical semantic relation translations are indeed highly precise when they are logically inferable."
Y01-1002,A Comparative Study of {E}nglish and {C}hinese Synonym Pairs : An Approach based on The Module-Attribute Representation of Verbal Semantics,2001,2,0,2,0.972744,15811,kathleen ahrens,"Proceedings of the 15th Pacific Asia Conference on Language, Information and Computation",0,"The Module-Attribute Representation of Verbal Semantics (MARVS) is a theory of the representation of verbal semantics that is developed based on Mandarin Chinese data (Huang et al., 2000). This theory proposes two types of modules: event structure modules and role modules, as well as two sets of attributes: event-internal attributes and role-internal attributes which are linked to the event structure module and role module respectively. These module-attribute semantic representations have associated grammatical consequences. Studies in MARVS (e.g. Biq 2000) found that the composition of an event modules and its attested lexical semantic attribute(s) could be generalized to a natural semantic class. For example, the contrast between bai3 ( `set') and fang4 ( `pue) in Mandarin Chinese to do with the fact that bai3 has a roleinternal feature of [design] attached to the location role, while fang4 does not. Moreover, this contrast can be generalized across the semantic class of verbs that involve design on the focussed location role (i.e. hua4 'to paint') and those that do not (i.e. tu2 'to cover with paint, to doodle'). What we would like to determine in this paper is if similar contrasts be found in near synonyms of other languages. In particular, are event modules in other languages (such as English) organized along similar conceptual lines? In this paper, we examine this theory in light of the English data. In particular, we will look at the near synonym contrast of the verbs 'put' and 'set' based on data from the two million word sampler of the British National Corpus (BNC). To preview our results, we find that the event structure in English is slightly different from that of Chinese. In Chinese, bai3 has a role-internal feature of [design] attached to the location role, while fang4 does not. However, in English, 'put' has the roles of Agent, Theme and Location in its event structure, while 'set' has the three roles of Agent, Theme, and Proposition. Thus, conceptualizations of 'set' and 'put' in English and Mandarin have different semantic and syntactic entailments."
Y00-1012,The Module-Attribute Representation of Verbal Semantics,2000,16,10,1,1,1504,churen huang,"Proceedings of the 14th Pacific Asia Conference on Language, Information and Computation",0,"In this paper, we set forth a theory of lexical knowledge. we propose two types of modules: event structure modules and role modules, as well as two attributes: event-internal attributes and role-internal attributes which are linked to the event structure module and role module respectively. These module-attribute semantic representations have associated grammatical consequences. Our data is drawn from a comprehensive corpus-based study of Mandarin Chinese verbal semantics."
W00-1205,"{S}inica {T}reebank: Design Criteria, Annotation Guidelines, and On-line Interface",2000,10,41,1,1,1504,churen huang,Second {C}hinese Language Processing Workshop,0,"This paper describes the design criteria and annotation guidelines of Sinica Treebank. The three design criteria are: Maximal Resource Sharing, Minimal Structural Complexity, and Optimal Semantic Information. One of the important design decisions following these criteria is the encoding of thematic role information. An on-line interface facilitating empirical studies of Chinese phrase structure is also described."
O00-2001,æ¼¢èªåè©è¾­å½èªæåæï¼è¡¨éæ¨¡å¼èç ç©¶æ¹æ³ (A Lexical-Semantic Analysis of {M}andarin {C}hinese Verbs: Representation and Methodology) [In {C}hinese],2000,0,0,3,1,54364,lili chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 5, Number 1, {F}ebruary 2000: Special Issue on {C}hinese Verbal Semantics",0,None
O00-2002,The Module-Attribute Representation of Verbal Semantics: From Semantic to Argument Structure,2000,0,35,1,1,1504,churen huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 5, Number 1, {F}ebruary 2000: Special Issue on {C}hinese Verbal Semantics",0,"In this paper, we set forth a theory of lexical knowledge. We propose two types of modules: event structure modules and role modules, as well as two sets of attributes: event-internal attributes and role-internal attributes, which are linked to the event structure module and role module, respectively. These module-attribute semantic representations have associated grammatical consequences. Our data is drawn from a comprehensive corpus-based study of Mandarin Chinese verbal semantics, and four particular case studies are presented."
O00-2003,What Can Near Synonyms Tell Us,2000,-1,-1,2,0,54365,liancheng chief,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 5, Number 1, {F}ebruary 2000: Special Issue on {C}hinese Verbal Semantics",0,None
O00-2004,Alternation Across Semantic Fields: A Study on {M}andarin Verbs of Emotion,2000,4,11,3,1,54364,lili chang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 5, Number 1, {F}ebruary 2000: Special Issue on {C}hinese Verbal Semantics",0,"This paper explores possible co-relations between lexical semantics and morpho-syntactic structures. We first examine a consistent dichotomy among verbs of emotion, which was first observed for verbs of happiness by Tsai et al. [1998]. It is shown that the dichotomy can be determined based on the criterion of whether a verb is a VV compound or not.2 The linguistic contrasts observed include: the grammatical functions of a verb as well as their distribution, the selectional restrictions the verbs impose as an adjunct, a verb's occurrences in imperative and evaluative constructions, its aktionsart, and its transitivity. We will show that the overt morpho-syntactic contrasts are due to lexical event structure properties. The description of a state (of emotion) can focus on how the state comes to be (i.e., the inchoative state) or on the being of the state (i.e., the homogeneous state). Since VV compounding has the semantic function of referring to the generic properties of the set of event tokens, it is natural for VV compounding to be chosen as the morpho-syntactic representation of homogeneity."
O00-2005,When Endpoint Meets Endpoint: A Corpus-based Lexical Semantic Study of {M}andarin Verbs of Throwing,2000,-1,-1,2,1,19301,meichun liu,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 5, Number 1, {F}ebruary 2000: Special Issue on {C}hinese Verbal Semantics",0,None
Y99-1004,Lexical Information and Beyond : Constructional Inferences in Semantic Representation,1999,14,2,2,1,19301,meichun liu,"Proceedings of the 13th Pacific Asia Conference on Language, Information and Computation",0,"This paper aims to show that semantic representation of verbs may require the inclusion of constructional inferences in addition to lexical specifications. By examining the transitive pattern of the Mandarin verb GAN (,y) 'rush' , we found that verbal semantics can only be adequately represented if constructionally coerced information is taken into consideration. The construction [GAN  NP] renders specific interpretations that are not directly derived from the lexical meaning of either the verb or the object NP. The construction itself carries salient information for the appropriate interpretation. Besides outlining a compositional approach with Qualia Structure (Pustejovsky 1995) , this paper attempts to account for the constructiontriggered meanings from the perspective of Construction Grammar and to explain their interrelationship with cognitive mechanisms for sense extension."
Y99-1005,Alternation Across Semantic Fields : A Study of {M}andarin Verbs of Emotion,1999,-1,-1,3,1,54364,lili chang,"Proceedings of the 13th Pacific Asia Conference on Language, Information and Computation",0,None
O99-1005,Semantic Representation of Verbal Information {--} A Case from {M}andarin Verbs of Judging,1999,0,3,2,1,19301,meichun liu,Proceedings of Research on Computational Linguistics Conference {XII},0,None
O98-3003,Meaning Representation and Meaning Instantiation for {C}hinese Nominals,1998,0,14,4,1,15811,kathleen ahrens,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 3, Number 1, {F}ebruary 1998: Special Issue on the 10th Research on Computational Linguistics International Conference",0,None
O98-3004,Towards a Representation of Verbal Semantics {--} An Approach Based on Near-Synonyms,1998,7,22,2,1,50914,meichih tsai,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 3, Number 1, {F}ebruary 1998: Special Issue on the 10th Research on Computational Linguistics International Conference",0,"In this paper we propose using the distributional differences in the syntactic patterns of near-synonyms to deduce the relevant components of verb meaning. Our method involves determining the distributional differences in syntactic patterns, deducing the semantic features from the syntactic phenomena, and testing the semantic features in new syntactic frames. We determine the distributional differences in syntactic patterns through the following five steps: First, we search for all instances of the verb in the corpus. Second, we classify each of these instances into its type of syntactic function. Third, we classify each of these instances into its argument structure type. Fourth, we determine the aspectual type that is associated with each verb. Lastly, we determine each verb's sentential type. Once the distributional differences have been determined, then the relevant semantic features are postulated. Our goal is to tease out the lexical semantic features as the explanation, and as the motivation of the syntactic contrasts."
O98-1004,Quantitative Criteria for Computational {C}hinese Lexicography,1998,-1,-1,1,1,1504,churen huang,Proceedings of Research on Computational Linguistics Conference {XI},0,None
O97-4003,Segmentation Standard for {C}hinese Natural Language Processing,1997,0,36,1,1,1504,churen huang,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 2, Number 2, August 1997",0,None
O97-3006,"å»ºæ§ä¸åä»¥å\
±æèæ­·æèªè¨ç ç©¶çºå°åçæ­·å²èªæåº« (Historical Corpora for Synchronic and Diachronic Linguistics Studies) [In {C}hinese]",1997,-1,-1,4,0,55646,peichuan wei,"International Journal of Computational Linguistics {\\&} {C}hinese Language Processing, Volume 2, Number 1, {F}ebruary 1997: Special Issue on Computational Resources for Research in {C}hinese Linguistics",0,None
O97-1001,Meaning Representation and Meaning Instantiation for {C}hinese Nominals,1997,-1,-1,4,1,15811,kathleen ahrens,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
O97-1003,Towards a Representation of Verbal Semantics {--} An Approach Based on Near Synonyms,1997,-1,-1,2,1,50914,meichih tsai,Proceedings of the 10th Research on Computational Linguistics International Conference,0,None
Y96-1001,Classifiers and Semantic Type Coercion : Motivating a New Classification of Classifiers,1996,4,4,2,1,15811,kathleen ahrens,"Proceedings of the 11th Pacific Asia Conference on Language, Information and Computation",0,"This paper argues that the traditional view that nouns refer only to classic individuals is inadequate. Instead, we argue that nouns are coerced by different types of classifiers to refer to kinds and events as well as to individuals. This finding is important because 1) the semantics of nouns involves more than just individuals, and 2) it is the first time that the previously abstract semantic distinctions between kinds, individuals and events is found to be instantiated in a particular system of a natural language grammar, namely, the classifier system."
Y96-1018,{S}INICA {C}ORPUS : Design Methodology for Balanced Corpora,1996,9,131,2,0.587314,39226,kehjiann chen,"Proceedings of the 11th Pacific Asia Conference on Language, Information and Computation",0,"The Academia Sinica Balanced Corpus (Sinica Corpus) is the first balanced Chinese corpus with part-of-speech tagging. The corpus (Sinica 2.0) is open to the research community through the WWW (http://www.sinica.edu.twiftmsbinikiwi.sh). Current size of the corpus is 3.5 million words, and the immediate expansion target is five million words. Each text in the corpus is classified and marked according to five criteria: genre, style, mode, topic, and source. The feature values of these classifications are assigned in a hierarchy. Subcorpora can be defined with a specific set of attributes to serve different research purposes. Texts in the corpus are segmented according to the word segmentation standard proposed by the ROC Computational Linguistic Society. Each segmented word is tagged with its part-of-speech. Linguistic patterns and language structures can be extracted from the tagged corpus via a corpus inspection program which has the functions of KWIC searching, filtering, statistics, printing, and collocation."
O96-1009,"èªæåº«å¨è¾­å\
¸ç·¨è¼¯ä¸çéç¨ (The Application of Language Corpus on Dictionary Editing) [In {C}hinese]",1996,0,1,3,1,54364,lili chang,Proceedings of Rocling {IX} Computational Linguistics Conference {IX},0,None
O96-1010,èªæåº«çºæ¬çèªç¾©è¨æ¯æ½åèè¾¨æä»¥è¿ç¾©è©ç ç©¶çºä¾ (Synonym Discrimination Based on Corpus) [In {C}hinese],1996,-1,-1,2,1,50914,meichih tsai,Proceedings of Rocling {IX} Computational Linguistics Conference {IX},0,None
C96-2184,Segmentation Standard for {C}hinese Natural Language Processing,1996,13,20,1,1,1504,churen huang,{COLING} 1996 Volume 2: The 16th International Conference on Computational Linguistics,0,"This paper proposes a segmentation standard for Chinese natural language processing. The standard is proposed to achieve linguistic felicity, computational feasibility, and data uniformity. Linguistic felicity is maintained by defining a segmentation unit to be equivalent to the theoretical definition of word, and by providing a set of segmentation principles that are equivalent to a functional definition of a word. Computational feasibility is ensured by the fact that the above functional definitions are procedural in nature and can be converted to segmentation algorithms, as well as by the implementable heuristic guidelines which deal with specific linguistic categories. Data uniformity is achieved by stratification of the standard itself and by defining a standard lexicon as part of the segmentation standard."
Y95-1011,Construction as a Theoretical Entity : An Argument Based on {M}andarin Existential Sentences,1995,2,3,2,0,56095,chaoran chen,"Proceedings of the 10th Pacific Asia Conference on Language, Information and Computation",0,"The role that constructions play in a linguistic theory has changed throughout the evolution of generative theories. Construction specific rules are common when transformations are envisioned as tree to tree operations in classical TG. On the other extreme, constructions, as well as all structural properties, are regarded as derived linguistic properties predictable from various principles in recent GB theories. Since whether a construction is an autonomous linguistic entity or not has great implications for either a formal or a computational linguistic theory, we will examine the status of Mandarin existential construction based on the theory of Construction Grammar [1,2]. We will show that the Mandarin existential construction represents an unique structure-meaning pair that cannot be captured in a grammar unless the pairing is regarded as a theoretical entity in linguistics ([3]). Since constructions are shown to exist in Mandarin Chinese, we support the theoretical claims of Construction Grammar as well as the position that constructions must be taken into account in NLP. I. Existential constructions and its two sub-constructions The existential sentences in Mandarin Chinese, such as (1) and (2), have been traditionally considered to involve movement or lexical rule in a verb-centered paradigm. (1)Zhuo-shang fang LE yi ben shu. table on put ASP a CL book 'There is a book on the table' (2) Chuang-shang tang ZHE yi ge ren. bed on lie ASP a CL person 'There is a person lying on the bed' The surface structure of these existential sentences can be schematized as follows: (3) [ Locative V Theme ] The apparent structural uniformity represented by (3) gives the verb-centered accounts their strongest motivation. This is because a verb-centered account can easily predict the structure by the cross-the-board nature of the movement rule or the lexical projection rule. However, there are two facts involving the existential sentences that would prove difficult for verbcentered accounts but could be easily predicted by a constructional account. First, the existential meaning is manifested only when the structure schema of (3) is employed. In other words, if the existential sense is attributed to the verbs, the grammar will be overloaded with unnecessary ambiguities in structures other than (3). The verb-centered"
C94-1088,Character-based Collocation for {M}andarin {C}hinese,1994,9,6,1,1,1504,churen huang,{COLING} 1994 Volume 1: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics,0,None
O92-1005,"æ¼¢èªçåè©åç©ååæ¢{--}æ¼¢èªä¸­å¸¶è«å\
çåç©åæ´¾çåè© (A preliminary study on Nominalization in {M}andarin {C}hinese {--} argument-taking deverbal nouns) [In {C}hinese]",1992,-1,-1,3,0,56943,marie yeh,Proceedings of Rocling V Computational Linguistics Conference V,0,None
O92-1007,"Reduplication In {M}andarin {C}hinese: Their Formation Rules, Syntactic Behavior And {ICG} Representation",1992,0,2,3,0,54218,fengyi chen,Proceedings of Rocling V Computational Linguistics Conference V,0,None
C92-4194,A {C}hinese Corpus for Linguistic Research,1992,4,16,1,1,1504,churen huang,{COLING} 1992 Volume 4: The 14th {I}nternational {C}onference on {C}omputational {L}inguistics,0,This is a project note on the first stage of the construction of a comprehensive corpus of both Modern and Classical Chinese. The corpus is built with the dual aim of serving as the central database for Chinese language processing and for supporting in-depth linguistic research in Mandarin Chinese.
O91-1003,Determinative-Measure Compounds in {M}andarin {C}hinese Formation Rules and Parser Implementation,1991,0,10,4,0,56945,ruoping mo,Proceedings of Rocling {IV} Computational Linguistics Conference {IV},0,None
C90-2010,Information-based Case Grammar,1990,16,15,2,1,39226,kehjiann chen,{COLING} 1990 Volume 2: Papers presented to the 13th International Conference on Computational Linguistics,0,"In this paper we propose a framework of Information-based Case Grammar (ICG). This grammatical formalism entails that the lexical entry for each word contain both semantic and syntactic feature structures. In the feature structure of a phrasal head, we encode syntactic and semantic constraints on grammatical phrasal patterns in terms of thematic structures, and encode the precedence relations in terms of adjunct structures. Such feature structures denote partial information which defines the set of legal phrases. They also provide sufficient information to identify thematic roles. With this formalism, parsing and thematic analysis can be achieved simultaneously. Due to the simplicity and flexibility of Information-based Case Grammar, context dependent and discontinuous relations such as agreements, coordinations, long-distance dependencies, and control and binding, can be easily expressed. ICG is a kind of unification-based formalism. Therefore it inherits the advantages of unification-based formalisms and more."
O89-1001,The Identification Of Thematic Roles In Parsing {M}andarin {C}hinese,1989,0,0,2,0,39226,kehjiann chen,Proceedings of Rocling {II} Computational Linguistics Conference {II},0,None
O89-1009,A Unification-based Approach to {M}andarin Questions,1989,0,1,2,0,57738,yuling shiu,Proceedings of Rocling {II} Computational Linguistics Conference {II},0,None
