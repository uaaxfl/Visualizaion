2021.acl-demo.9,{MT}-{T}elescope: {A}n interactive platform for contrastive evaluation of {MT} systems,2021,-1,-1,5,1,7230,ricardo rei,Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing: System Demonstrations,0,"We present MT-Telescope, a visualization platform designed to facilitate comparative analysis of the output quality of two Machine Translation (MT) systems. While automated MT evaluation metrics are commonly used to evaluate MT systems at a corpus-level, our platform supports fine-grained segment-level analysis and interactive visualisations that expose the fundamental differences in the performance of the compared systems. MT-Telescope also supports dynamic corpus filtering to enable focused analysis on specific phenomena such as; translation of named entities, handling of terminology, and the impact of input segment length on translation quality. Furthermore, the platform provides a bootstrapped t-test for statistical significance as a means of evaluating the rigor of the resulting system ranking. MT-Telescope is open source, written in Python, and is built around a user friendly and dynamic web interface. Complementing other existing tools, our platform is designed to facilitate and promote the broader adoption of more rigorous analysis practices in the evaluation of MT quality."
2020.wmt-1.101,Unbabel{'}s Participation in the {WMT}20 Metrics Shared Task,2020,-1,-1,4,1,7230,ricardo rei,Proceedings of the Fifth Conference on Machine Translation,0,"We present the contribution of the Unbabel team to the WMT 2020 Shared Task on Metrics. We intend to participate on the segmentlevel, document-level and system-level tracks on all language pairs, as well as the {``}QE as a Metric{''} track. Accordingly, we illustrate results of our models in these tracks with reference to test sets from the previous year. Our submissions build upon the recently proposed COMET framework: we train several estimator models to regress on different humangenerated quality scores and a novel ranking model trained on relative ranks obtained from Direct Assessments. We also propose a simple technique for converting segment-level predictions into a document-level score. Overall, our systems achieve strong results for all language pairs on previous test sets and in many cases set a new state-of-the-art."
2020.emnlp-main.213,{COMET}: A Neural Framework for {MT} Evaluation,2020,-1,-1,4,1,7230,ricardo rei,Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP),0,"We present COMET, a neural framework for training multilingual machine translation evaluation models which obtains new state-of-the-art levels of correlation with human judgements. Our framework leverages recent breakthroughs in cross-lingual pretrained language modeling resulting in highly multilingual and adaptable MT evaluation models that exploit information from both the source input and a target-language reference translation in order to more accurately predict MT quality. To showcase our framework, we train three models with different types of human judgements: Direct Assessments, Human-mediated Translation Edit Rate and Multidimensional Quality Metric. Our models achieve new state-of-the-art performance on the WMT 2019 Metrics shared task and demonstrate robustness to high-performing systems."
2020.amta-user.4,{COMET} - Deploying a New State-of-the-art {MT} Evaluation Metric in Production,2020,-1,-1,4,1,13538,craig stewart,Proceedings of the 14th Conference of the Association for Machine Translation in the Americas (Volume 2: User Track),0,None
P16-1103,Synthesizing Compound Words for Machine Translation,2016,25,4,3,1,26314,austin matthews,Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers),1,"Most machine translation systems construct translations from a closed vocabulary of target word forms, posing problems for translating into languages that have productive compounding processes. We present a simple and effective approach that deals with this problem in two phases. First, we build a classifier that identifies spans of the input text that can be translated into a single compound word in the target language. Then, for each identified span, we generate a pool of possible compounds which are added to the translation model as xe2x80x9csyntheticxe2x80x9d phrase translations. Experiments reveal that (i) we can effectively predict what spans can be compounded; (ii) our compound generation model produces good compounds; and (iii) modest improvements are possible in end-to-end Englishxe2x80x90German and Englishxe2x80x90Finnish translation tasks. We additionally introduce KomposEval, a new multi-reference dataset of English phrases and their translations into German compounds."
D15-1284,Humor Recognition and Humor Anchor Extraction,2015,25,41,2,0,894,diyi yang,Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing,0,"Humor is an essential component in personal communication. How to create computational models to discover the structures behind humor, recognize humor and even extract humor anchors remains a challenge. In this work, we first identify several semantic structures behind humor and design sets of features for each structure, and next employ a computational approach to recognize humor. Furthermore, we develop a simple and effective method to extract anchors that enable humor in a sentence. Experiments conducted on two datasets demonstrate that our humor recognizer is effective in automatically distinguishing between humorous and non-humorous texts and our extracted humor anchors correlate quite well with human annotations."
W14-3627,Domain and Dialect Adaptation for Machine Translation into {E}gyptian {A}rabic,2014,31,8,4,0,23961,serena jeblee,Proceedings of the {EMNLP} 2014 Workshop on {A}rabic Natural Language Processing ({ANLP}),0,"In this paper, we present a statistical machine translation system for English to Dialectal Arabic (DA), using Modern Standard Arabic (MSA) as a pivot. We create a core system to translate from English to MSA using a large bilingual parallel corpus. Then, we design two separate pathways for translation from MSA into DA: a two-step domain and dialect adaptation system and a one-step simultaneous domain and dialect adaptation system. Both variants of the adaptation systems are trained on a 100k sentence tri-parallel corpus of English, MSA, and Egyptian Arabic generated by a rule-based transformation. We test our systems on a held-out Egyptian Arabic test set from the 100k sentence corpus and we achieve our best performance using the two-step domain and dialect adaptation system with a BLEU score of 42.9."
W14-3315,The {CMU} Machine Translation Systems at {WMT} 2014,2014,24,7,9,1,26314,austin matthews,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"We describe the CMU systems submitted to the 2014 WMT shared translation task. We participated in two language pairs, Germanxe2x80x93English and Hindixe2x80x93English. Our innovations include: a label coarsening scheme for syntactic tree-to-tree translation, a host of new discriminative features, several modules to create xe2x80x9csynthetic translation optionsxe2x80x9d that can generalize beyond what is directly observed in the training data, and a method of combining the output of multiple word aligners to uncover extra phrase pairs and grammar rules."
W14-3348,Meteor Universal: Language Specific Translation Evaluation for Any Target Language,2014,11,699,2,1,20879,michael denkowski,Proceedings of the Ninth Workshop on Statistical Machine Translation,0,"This paper describes Meteor Universal, released for the 2014 ACL Workshop on Statistical Machine Translation. Meteor Universal brings language specific evaluation to previously unsupported target languages by (1) automatically extracting linguistic resources (paraphrase tables and function word lists) from the bitext used to train MT systems and (2) using a universal parameter set learned from pooling human judgments of translation quality from several language directions. Meteor Universal is shown to significantly outperform baseline BLEU on two new languages, Russian (WMT13) and Hindi (WMT14)."
W14-0311,Real Time Adaptive Machine Translation for Post-Editing with cdec and {T}rans{C}enter,2014,17,16,2,1,20879,michael denkowski,Proceedings of the {EACL} 2014 Workshop on Humans and Computer-assisted Translation,0,"Using machine translation output as a starting point for human translation has recently gained traction in the translation community. This paper describes cdec Realtime, a framework for building adaptive MT systems that learn from post-editor feedback, and TransCenter, a web-based translation interface that connects users to Realtime systems and logs post-editing activity. This combination allows the straightforward deployment of MT systems specifically for post-editing and analysis of human translator productivity when working with these systems. All tools, as well as actual post-editing data collected as part of a validation experiment, are freely available under an open source license."
Q14-1031,Locally Non-Linear Learning for Statistical Machine Translation via Discretization and Structured Regularization,2014,48,3,3,1,3375,jonathan clark,Transactions of the Association for Computational Linguistics,0,"Linear models, which support efficient learning and inference, are the workhorses of statistical machine translation; however, linear decision rules are less attractive from a modeling perspective. In this work, we introduce a technique for learning arbitrary, rule-local, non-linear feature transforms that improve model expressivity, but do not sacrifice the efficient inference and learning associated with linear models. To demonstrate the value of our technique, we discard the customary log transform of lexical probabilities and drop the phrasal translation probability in favor of raw counts. We observe that our algorithm learns a variation of a log transform that leads to better translation quality compared to the explicit log transform. We conclude that non-linear responses play an important role in SMT, an observation that we hope will inform the efforts of feature engineers."
E14-1042,Learning from Post-Editing: Online Model Adaptation for Statistical Machine Translation,2014,30,39,3,1,20879,michael denkowski,Proceedings of the 14th Conference of the {E}uropean Chapter of the Association for Computational Linguistics,0,"Using machine translation output as a starting point for human translation has become an increasingly common application of MT. We propose and evaluate three computationally efficient online methods for updating statistical MT systems in a scenario where post-edited MT output is constantly being returned to the system: (1) adding new rules to the translation model from the post-edited content, (2) updating a Bayesian language model of the target language that is used by the MT system, and (3) updating the MT systemxe2x80x99s discriminative parameters with a MIRA step. Individually, these techniques can substantially improve MT quality, even over strong baselines. Moreover, we see super-additive improvements when all three techniques are used in tandem."
2014.amta-wptp.6,Cognitive demand and cognitive effort in post-editing,2014,-1,-1,3,0,30157,isabel lacruz,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas,0,"The pause to word ratio, the number of pauses per word in a post-edited MT segment, is an indicator of cognitive effort in post-editing (Lacruz and Shreve, 2014). We investigate how low the pause threshold can reasonably be taken, and we propose that 300 ms is a good choice, as pioneered by Schilperoord (1996). We then seek to identify a good measure of the cognitive demand imposed by MT output on the post-editor, as opposed to the cognitive effort actually exerted by the post-editor during post-editing. Measuring cognitive demand is closely related to measuring MT utility, the MT quality as perceived by the post-editor. HTER, an extrinsic edit to word ratio that does not necessarily correspond to actual edits per word performed by the post-editor, is a well-established measure of MT quality, but it does not comprehensively capture cognitive demand (Koponen, 2012). We investigate intrinsic measures of MT quality, and so of cognitive demand, through edited-error to word metrics. We find that the transfer-error to word ratio predicts cognitive effort better than mechanical-error to word ratio (Koby and Champe, 2013). We identify specific categories of cognitively challenging MT errors whose error to word ratios correlate well with cognitive effort."
2014.amta-wptp.14,Real time adaptive machine translation: cdec and {T}rans{C}enter,2014,-1,-1,2,1,20879,michael denkowski,Proceedings of the 11th Conference of the Association for Machine Translation in the Americas,0,cdec Realtime and TransCenter provide an end-to-end experimental setup for machine translation post-editing research. Realtime provides a framework for building adaptive MT systems that learn from post-editor feedback while TransCenter incorporates a web-based translation interface that connects users to these systems and logs post-editing activity. This combination allows the straightforward deployment of MT systems specifically for post-editing and analysis of translator productivity when working with adaptive systems. Both toolkits are freely available under open source licenses.
W13-2205,"The {CMU} Machine Translation Systems at {WMT} 2013: Syntax, Synthetic Translation Options, and Pseudo-References",2013,26,13,9,0.833333,23186,waleed ammar,Proceedings of the Eighth Workshop on Statistical Machine Translation,0,"We describe the CMU systems submitted to the 2013 WMT shared task in machine translation. We participated in three language pairs, Frenchxe2x80x90English, Russianxe2x80x90 English, and Englishxe2x80x90Russian. Our particular innovations include: a labelcoarsening scheme for syntactic tree-totree translation and the use of specialized modules to create xe2x80x9csynthetic translation optionsxe2x80x9d that can both generalize beyond what is directly observed in the parallel training data and use rich source language context to decide how a phrase should translate in context."
N13-1029,Improving Syntax-Augmented Machine Translation by Coarsening the Label Set,2013,20,15,2,1,13984,greg hanneman,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We present a new variant of the SyntaxAugmented Machine Translation (SAMT) formalism with a category-coarsening algorithm originally developed for tree-to-tree grammars. We induce bilingual labels into the SAMT grammar, use them for category coarsening, then project back to monolingual labeling as in standard SAMT. The result is a xe2x80x9ccollapsedxe2x80x9d grammar with the same expressive power and format as the original, but many fewer nonterminal labels. We show that the smaller label set provides improved translation scores by 1.14 BLEU on two Chinesexe2x80x90 English test sets while reducing the occurrence of sparsity and ambiguity problems common to large label sets."
N13-1116,Grouping Language Model Boundary Words to Speed K{--}Best Extraction from Hypergraphs,2013,36,20,3,1,10335,kenneth heafield,Proceedings of the 2013 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies,0,"We propose a new algorithm to approximately extract top-scoring hypotheses from a hypergraph when the score includes an N xe2x80x90gram language model. In the popular cube pruning algorithm, every hypothesis is annotated with boundary words and permitted to recombine only if all boundary words are equal. However, many hypotheses share some, but not all, boundary words. We use these common boundary words to group hypotheses and do so recursively, resulting in a tree of hypotheses. This tree forms the basis for our new search algorithm that iteratively refines groups of boundary words on demand. Machine translation experiments show our algorithm makes translation 1.50 to 3.51 times as fast as with cube pruning in common cases."
2013.mtsummit-user.1,Analyzing and Predicting {MT} Utility and Post-Editing Productivity in Enterprise-scale Translation Projects,2013,-1,-1,1,1,13539,alon lavie,Proceedings of Machine Translation Summit XIV: User track,0,None
W12-3131,The {CMU}-Avenue {F}rench-{E}nglish Translation System,2012,17,17,3,1,20879,michael denkowski,Proceedings of the Seventh Workshop on Statistical Machine Translation,0,"This paper describes the French-English translation system developed by the Avenue research group at Carnegie Mellon University for the Seventh Workshop on Statistical Machine Translation (NAACL WMT12). We present a method for training data selection, a description of our hierarchical phrase-based translation system, and a discussion of the impact of data size on best practice for system building."
D12-1107,Language Model Rest Costs and Space-Efficient Storage,2012,27,6,3,1,10335,kenneth heafield,Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning,0,"Approximate search algorithms, such as cube pruning in syntactic machine translation, rely on the language model to estimate probabilities of sentence fragments. We contribute two changes that trade between accuracy of these estimates and memory, holding sentence-level scores constant. Common practice uses lower-order entries in an N-gram model to score the first few words of a fragment; this violates assumptions made by common smoothing strategies, including Kneser-Ney. Instead, we use a unigram model to score the first word, a bigram for the second, etc. This improves search at the expense of memory. Conversely, we show how to save memory by collapsing probability and backoff into a single value without changing sentence-level scores, at the expense of less accurate estimates for sentence fragments. These changes can be stacked, achieving better estimates with unchanged memory usage. In order to interpret changes in search accuracy, we adjust the pop limit so that accuracy is unchanged and report the change in CPU time. In a German-English Moses system with target-side syntax, improved estimates yielded a 63% reduction in CPU time; for a Hiero-style version, the reduction is 21%. The compressed language model uses 26% less RAM while equivalent search quality takes 27% more CPU. Source code is released as part of KenLM."
2012.amta-papers.4,"One System, Many Domains: Open-Domain Statistical Machine Translation via Feature Augmentation",2012,29,14,2,1,3375,jonathan clark,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"In this paper, we introduce a simple technique for incorporating domain information into a statistical machine translation system that significantly improves translation quality when test data comes from multiple domains. Our approach augments (conjoins) standard translation model and language model features with domain indicator features and requires only minimal modifications to the optimization and decoding procedures. We evaluate our method on two language pairs with varying numbers of domains, and observe significant improvements of up to 1.0 BLEU."
2012.amta-papers.6,Challenges in Predicting Machine Translation Utility for Human Post-Editors,2012,16,10,2,1,20879,michael denkowski,Proceedings of the 10th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"As machine translation quality continues to improve, the idea of using MT to assist human translators becomes increasingly attractive. In this work, we discuss and provide empirical evidence of the challenges faced when adapting traditional MT systems to provide automatic translations for human post-editors to correct. We discuss the differences between this task and traditional adequacy-based tasks and the challenges that arise when using automatic metrics to predict the amount of effort required to post-edit translations. A series of experiments simulating a real-world localization scenario shows that current metrics under-perform on this task, even when tuned to maximize correlation with expert translator judgments, illustrating the need to rethink traditional MT pipelines when addressing the challenges of this translation task."
W11-2107,Meteor 1.3: Automatic Metric for Reliable Optimization and Evaluation of Machine Translation Systems,2011,15,243,2,1,20879,michael denkowski,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes Meteor 1.3, our submission to the 2011 EMNLP Workshop on Statistical Machine Translation automatic evaluation metric tasks. New metric features include improved text normalization, higher-precision paraphrase matching, and discrimination between content and function words. We include Ranking and Adequacy versions of the metric shown to have high correlation with human judgments of translation quality as well as a more balanced Tuning version shown to outperform BLEU in minimum error rate training for a phrase-based Urdu-English system."
W11-2117,{CMU} System Combination in {WMT} 2011,2011,14,7,2,1,10335,kenneth heafield,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"This paper describes our submissions, cmu-heafield-combo, to the ten tracks of the 2011 Workshop on Machine Translation's system combination task. We show how the combination scheme operates by flexibly aligning system outputs then searching a space constructed from the alignments. Humans judged our combination the best on eight of ten tracks."
W11-2143,{CMU} Syntax-Based Machine Translation at {WMT} 2011,2011,20,1,2,1,13984,greg hanneman,Proceedings of the Sixth Workshop on Statistical Machine Translation,0,"We present the Carnegie Mellon University Stat-XFER group submission to the WMT 2011 shared translation task. We built a hybrid syntactic MT system for French--English using the Joshua decoder and an automatically acquired SCFG. New work for this year includes training data selection and grammar filtering. Expanded training data selection significantly increased translation scores and lowered OOV rates, while results on grammar filtering were mixed."
W11-1011,Automatic Category Label Coarsening for Syntax-Based Machine Translation,2011,15,12,2,1,13984,greg hanneman,"Proceedings of Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"We consider SCFG-based MT systems that get syntactic category labels from parsing both the source and target sides of parallel training data. The resulting joint nonterminals often lead to needlessly large label sets that are not optimized for an MT scenario. This paper presents a method of iteratively coarsening a label set for a particular language pair and training corpus. We apply this label collapsing on Chinese--English and French--English grammars, obtaining test-set improvements of up to 2.8 BLEU, 5.2 TER, and 0.9 METEOR on Chinese--English translation. An analysis of label collapsing's effect on the grammar and the decoding process is also given."
W11-1015,A General-Purpose Rule Extractor for {SCFG}-Based Machine Translation,2011,18,9,3,1,13984,greg hanneman,"Proceedings of Fifth Workshop on Syntax, Semantics and Structure in Statistical Translation",0,"We present a rule extractor for SCFG-based MT that generalizes many of the contraints present in existing SCFG extraction algorithms. Our method's increased rule coverage comes from allowing multiple alignments, virtual nodes, and multiple tree decompositions in the extraction process. At decoding time, we improve automatic metric scores by significantly increasing the number of phrase pairs that match a given test set, while our experiments with hierarchical grammar filtering indicate that more intelligent filtering schemes will also provide a key to future gains."
P11-2031,Better Hypothesis Testing for Statistical Machine Translation: Controlling for Optimizer Instability,2011,28,335,3,1,3375,jonathan clark,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,0,"In statistical machine translation, a researcher seeks to determine whether some innovation (e.g., a new feature, model, or inference algorithm) improves translation quality in comparison to a baseline system. To answer this question, he runs an experiment to evaluate the behavior of the two systems on held-out data. In this paper, we consider how to make such experiments more statistically reliable. We provide a systematic analysis of the effects of optimizer instability---an extraneous variable that is seldom controlled for---on experimental outcomes, and make recommendations for reporting results more accurately."
P11-1042,Unsupervised Word Alignment with Arbitrary Features,2011,42,39,3,0.152581,3925,chris dyer,Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies,1,"We introduce a discriminatively trained, globally normalized, log-linear variant of the lexical translation models proposed by Brown et al. (1993). In our model, arbitrary, non-independent features may be freely incorporated, thereby overcoming the inherent limitation of generative models, which require that features be sensitive to the conditional independencies of the generative process. However, unlike previous work on discriminative modeling of word alignment (which also permits the use of arbitrary features), the parameters in our models are learned from unannotated parallel sentences, rather than from supervised word alignments. Using a variety of intrinsic and extrinsic measures, including translation performance, we show our model yields better alignments than generative baselines in a number of language pairs."
2011.mtsummit-tutorials.3,Evaluating the Output of Machine Translation Systems,2011,-1,-1,1,1,13539,alon lavie,Proceedings of Machine Translation Summit XIII: Tutorial Abstracts,0,"This half-day tutorial provides a broad overview of how to evaluate translations that are produced by machine translation systems. The range of issues covered includes a broad survey of both human evaluation measures and commonly-used automated metrics, and a review of how these are used for various types of evaluation tasks, such as assessing the translation quality of MT-translated sentences, comparing the performance of alternative MT systems, or measuring the productivity gains of incorporating MT into translation workflows."
W10-1709,Improved Features and Grammar Selection for Syntax-Based {MT},2010,17,5,3,1,13984,greg hanneman,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"We present the Carnegie Mellon University Stat-XFER group submission to the WMT 2010 shared translation task. Updates to our syntax-based SMT system mainly fell in the areas of new feature formulations in the translation model and improved filtering of SCFG rules. Compared to our WMT 2009 submission, we report a gain of 1.73 BLEU by using the new features and decoding environment, and a gain of up to 0.52 BLEU from improved grammar selection."
W10-1744,{CMU} Multi-Engine Machine Translation for {WMT} 2010,2010,16,6,2,1,10335,kenneth heafield,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,"This paper describes our submission, cmu--heafield--combo, to the WMT 2010 machine translation system combination task. Using constrained resources, we participated in all nine language pairs, namely translating English to and from Czech, French, German, and Spanish as well as combining English translations from multiple languages. Combination proceeds by aligning all pairs of system outputs then navigating the aligned outputs from left to right where each path is a candidate combination. Candidate combinations are scored by their length, agreement with the underlying systems, and a language model. On tuning data, improvement in BLEU over the best system depends on the language pair and ranges from 0.89% to 5.57% with mean 2.37%."
W10-1751,{METEOR}-{NEXT} and the {METEOR} Paraphrase Tables: Improved Evaluation Support for Five Target Languages,2010,7,61,2,1,20879,michael denkowski,Proceedings of the Joint Fifth Workshop on Statistical Machine Translation and {M}etrics{MATR},0,This paper describes our submission to the WMT10 Shared Evaluation Task and MetricsMATR10. We present a version of the Meteor-next metric with paraphrase tables for five target languages. We describe the creation of these paraphrase tables and conduct a tuning experiment that demonstrates consistent improvement across all languages over baseline versions of the metric without paraphrase resources.
W10-0709,Exploring Normalization Techniques for Human Judgments of Machine Translation Adequacy Collected Using {A}mazon {M}echanical {T}urk,2010,2,11,2,1,20879,michael denkowski,Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk,0,This paper discusses a machine translation evaluation task conducted using Amazon Mechanical Turk. We present a translation adequacy assessment task for untrained Arabic-speaking annotators and discuss several techniques for normalizing the resulting data. We present a novel 2-stage normalization technique shown to have the best performance on this task and further discuss the results of all techniques and the usability of the resulting adequacy scores.
W10-0711,Turker-Assisted Paraphrasing for {E}nglish-{A}rabic Machine Translation,2010,3,28,3,1,20879,michael denkowski,Proceedings of the {NAACL} {HLT} 2010 Workshop on Creating Speech and Language Data with {A}mazon{'}s Mechanical Turk,0,"This paper describes a semi-automatic paraphrasing task for English-Arabic machine translation conducted using Amazon Mechanical Turk. The method for automatically extracting paraphrases is described, as are several human judgment tasks completed by Turkers. An ideal task type, revised specifically to address feedback from Turkers, is shown to be sophisticated enough to identify and filter problem Turkers while remaining simple enough for non-experts to complete. The results of this task are discussed along with the viability of using this data to combat data sparsity in MT."
N10-1031,Extending the {METEOR} Machine Translation Evaluation Metric to the Phrase Level,2010,9,46,2,1,20879,michael denkowski,Human Language Technologies: The 2010 Annual Conference of the North {A}merican Chapter of the Association for Computational Linguistics,0,"This paper presents Meteor-next, an extended version of the Meteor metric designed to have high correlation with post-editing measures of machine translation quality. We describe changes made to the metric's sentence aligner and scoring scheme as well as a method for tuning the metric's parameters to optimize correlation with human-targeted Translation Edit Rate (HTER). We then show that Meteor-next improves correlation with HTER over baseline metrics, including earlier versions of Meteor, and approaches the correlation level of a state-of-the-art metric, TER-plus (TERp)."
clark-lavie-2010-loonybin,{L}oony{B}in: Keeping Language Technologists Sane through Automated Management of Experimental (Hyper)Workflows,2010,10,5,2,1,3375,jonathan clark,Proceedings of the Seventh International Conference on Language Resources and Evaluation ({LREC}'10),0,"Many contemporary language technology systems are characterized by long pipelines of tools with complex dependencies. Too often, these workflows are implemented by ad hoc scripts; or, worse, tools are run manually, making experiments difficult to reproduce. These practices are difficult to maintain in the face of rapidly evolving workflows while they also fail to expose and record important details about intermediate data. Further complicating these systems are hyperparameters, which often cannot be directly optimized by conventional methods, requiring users to determine which combination of values is best via trial and error. We describe LoonyBin, an open-source tool that addresses these issues by providing: 1) a visual interface for the user to create and modify workflows; 2) a well-defined mechanism for tracking metadata and provenance; 3) a script generator that compiles visual workflows into shell scripts; and 4) a new workflow representation we call a HyperWorkflow, which intuitively and succinctly encodes small experimental variations within a larger workflow."
2010.amta-tutorials.4,Evaluating the Output of Machine Translation Systems,2010,10,18,1,1,13539,alon lavie,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Tutorials,0,None
2010.amta-srw.4,"Machine Translation between {H}ebrew and {A}rabic: Needs, Challenges and Preliminary Solutions",2010,14,9,3,0,24423,reshef shilon,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Student Research Workshop,0,"Hebrew and Arabic are related but mutually incomprehensible languages with complex morphology and scarce parallel corpora. Machine translation between the two languages is therefore interesting and challenging. We discuss similarities and differences between Hebrew and Arabic, the benefits and challenges that they induce, respectively, and their implications for machine translation. We highlight the shortcomings of using English as a pivot language and advocate a direct, transfer-based and linguistically-informed (but still statistical, and hence scalable) approach. We report preliminary results of such a system that we are currently developing."
2010.amta-papers.4,The Impact of {A}rabic Morphological Segmentation on Broad-coverage {E}nglish-to-{A}rabic Statistical Machine Translation,2010,-1,-1,2,0,45511,hassan alhaj,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,Morphologically rich languages pose a challenge for statistical machine translation (SMT). This challenge is magnified when translating into a morphologically rich language. In this work we address this challenge in the framework of a broad-coverage English-to-Arabic phrase based statistical machine translation (PBSMT). We explore the full spectrum of Arabic segmentation schemes ranging from full word form to fully segmented forms and examine the effects on system performance. Our results show a difference of 2.61 BLEU points between the best and worst segmentation schemes indicating that the choice of the segmentation scheme has a significant effect on the performance of a PBSMT system in a large data scenario. We also show that a simple segmentation scheme can perform as good as the best and more complicated segmentation scheme. We also report results on a wide set of techniques for recombining the segmented Arabic output.
2010.amta-papers.17,Using Variable Decoding Weight for Language Model in Statistical Machine Translation,2010,17,4,3,0,35024,behrang mohit,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"This paper investigates varying the decoder weight of the language model (LM) when translating different parts of a sentence. We determine the condition under which the LM weight should be adapted. We find that a better translation can be achieved by varying the LM weight when decoding the most problematic spot in a sentence, which we refer to as a difficult segment. Two adaptation strategies are proposed and compared through experiments. We find that adapting a different LM weight for every difficult segment resulted in the largest improvement in translation quality."
2010.amta-papers.20,Choosing the Right Evaluation for Machine Translation: an Examination of Annotator and Automatic Metric Performance on Human Judgment Tasks,2010,12,22,2,1,20879,michael denkowski,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"This paper examines the motivation, design, and practical results of several types of human evaluation tasks for machine translation. In addition to considering annotator performance and task informativeness over multiple evaluations, we explore the practicality of tuning automatic evaluation metrics to each judgment type in a comprehensive experiment using the METEOR-NEXT metric. We present results showing clear advantages of tuning to certain types of judgments and discuss causes of inconsistency when tuning to various judgment data, as well as sources of difficulty in the human evaluation tasks themselves."
2010.amta-papers.34,Voting on N-grams for Machine Translation System Combination,2010,23,5,2,1,10335,kenneth heafield,Proceedings of the 9th Conference of the Association for Machine Translation in the Americas: Research Papers,0,"System combination exploits differences between machine translation systems to form a combined translation from several system outputs. Core to this process are features that reward n-gram matches between a candidate combination and each system output. Systems differ in performance at the n-gram level despite similar overall scores. We therefore advocate a new feature formulation: for each system and each small n, a feature counts n-gram matches between the system and candidate. We show post-evaluation improvement of 6.67 BLEU over the best system on NIST MT09 Arabic-English test data. Compared to a baseline system combination scheme from WMT 2009, we show improvement in the range of 1 BLEU point."
W09-2301,Decoding with Syntactic and Non-Syntactic Phrases in a Syntax-Based Machine Translation System,2009,18,12,2,1,13984,greg hanneman,Proceedings of the Third Workshop on Syntax and Structure in Statistical Translation ({SSST}-3) at {NAACL} {HLT} 2009,0,"A key concern in building syntax-based machine translation systems is how to improve coverage by incorporating more traditional phrase-based SMT phrase pairs that do not correspond to syntactic constituents. At the same time, it is desirable to include as much syntactic information in the system as possible in order to carry out linguistically motivated reordering, for example. We apply an extended and modified version of the approach of Tinsley et al. (2007), extracting syntax-based phrase pairs from a large parallel parsed corpus, combining them with PBSMT phrases, and performing joint decoding in a syntax-based MT framework without loss of translation quality. This effectively addresses the low coverage of purely syntactic MT without discarding syntactic information. Further, we show the potential for improved translation results with the inclusion of a syntactic grammar. We also introduce a new syntax-prioritized technique for combining syntactic and non-syntactic phrases that reduces overall phrase table size and decoding time by 61%, with only a minimal drop in automatic translation metric scores."
W09-0408,Machine Translation System Combination with Flexible Word Ordering,2009,8,23,3,1,10335,kenneth heafield,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"We describe a synthetic method for combining machine translations produced by different systems given the same input. One-best outputs are explicitly aligned to remove duplicate words. Hypotheses follow system outputs in sentence order, switching between systems mid-sentence to produce a combined output. Experiments with the WMT 2009 tuning data showed improvement of 2 BLEU and 1 METEOR point over the best Hungarian-English system. Constrained to data provided by the contest, our system was submitted to the WMT 2009 shared system combination task."
W09-0425,An Improved Statistical Transfer System for {F}rench-{E}nglish Machine Translation,2009,11,10,5,1,13984,greg hanneman,Proceedings of the Fourth Workshop on Statistical Machine Translation,0,"This paper presents the Carnegie Mellon University statistical transfer MT system submitted to the 2009 WMT shared task in French-to-English translation. We describe a syntax-based approach that incorporates both syntactic and non-syntactic phrase pairs in addition to a syntactic grammar. After reporting development test results, we conduct a preliminary analysis of the coverage and effectiveness of the system's components."
2009.mtsummit-posters.2,Extraction of Syntactic Translation Models from Parallel Data using Syntax from Source and Target Languages,2009,15,13,2,1,44215,vamshi ambati,Proceedings of Machine Translation Summit XII: Posters,0,"We propose a generic rule induction framework that is informed by syntax from both sides of a parsed parallel corpus, as sets of structural, boundary and labeling related constraints. Factoring syntax in this manner empowers our framework to work with independent annotations coming from multiple resources and not necessarily a single syntactic structure. We then explore the issue of lexical coverage of translation models learned in different scenarios using syntax from one side vs. both sides. We specifically look at how the non-isomorphic nature of parse trees for the two languages affects coverage. We propose a novel technique for restructuring targetside parse trees, that generates alternate isomorphic target trees that preserve the syntactic boundaries of constituents that were aligned in the original parse trees. We also show that combining rules extracted by restructuring syntactic trees on both sides produces significantly better translation models. The improved precision and coverage of our syntax tables particularly fill in for the lack of lexical coverage in Syntax based Machine Translation approaches."
W08-0708,Evaluating an Agglutinative Segmentation Model for {P}ara{M}or,2008,20,9,2,1,44751,christian monson,Proceedings of the Tenth Meeting of {ACL} Special Interest Group on Computational Morphology and Phonology,0,"This paper describes and evaluates a modification to the segmentation model used in the unsupervised morphology induction system, ParaMor. Our improved segmentation model permits multiple morpheme boundaries in a single word. To prepare ParaMor to effectively apply the new agglutinative segmentation model, two heuristics improve ParaMor's precision. These precision-enhancing heuristics are adaptations of those used in other unsupervised morphology induction systems, including work by Hafer and Weiss (1974) and Goldsmith (2006). By reformulating the segmentation model used in ParaMor, we significantly improve ParaMor's performance in all language tracks and in both the linguistic evaluation as well as in the task based information retrieval (IR) evaluation of the peer operated competition Morpho Challenge 2007. ParaMor's improved morpheme recall in the linguistic evaluations of German, Finnish, and Turkish is higher than that of any system which competed in the Challenge. In the three languages of the IR evaluation, our enhanced ParaMor significantly outperforms, at average precision over newswire queries, a morphologically naive baseline; scoring just behind the leading system from Morpho Challenge 2007 in English and ahead of the first place system in German."
W08-0411,Syntax-Driven Learning of Sub-Sentential Translation Equivalents and Translation Rules from Parsed Parallel Corpora,2008,16,46,1,1,13539,alon lavie,Proceedings of the {ACL}-08: {HLT} Second Workshop on Syntax and Structure in Statistical Translation ({SSST}-2),0,"We describe a multi-step process for automatically learning reliable sub-sentential syntactic phrases that are translation equivalents of each other and syntactic translation rules between two languages. The input to the process is a corpus of parallel sentences, word-aligned and annotated with phrase-structure parse trees. We first apply a newly developed algorithm for aligning parse-tree nodes between the two parallel trees. Next, we extract all aligned sub-sentential syntactic constituents from the parallel sentences, and create a syntax-based phrase-table. Finally, we treat the node alignments as tree decomposition points and extract from the corpus all possible synchronous parallel tree fragments. These are then converted into synchronous context-free rules. We describe the approach and analyze its application to Chinese-English parallel data."
W08-0312,"Meteor, {M}-{BLEU} and {M}-{TER}: Evaluation Metrics for High-Correlation with Human Rankings of Machine Translation Output",2008,13,65,2,0,47821,abhaya agarwal,Proceedings of the Third Workshop on Statistical Machine Translation,0,"This paper describes our submissions to the machine translation evaluation shared task in ACL WMT-08. Our primary submission is the Meteor metric tuned for optimizing correlation with human rankings of translation hypotheses. We show significant improvement in correlation as compared to the earlier version of metric which was tuned to optimized correlation with traditional adequacy and fluency judgments. We also describe m-bleu and m-ter, enhanced versions of two other widely used metrics bleu and ter respectively, which extend the exact word matching used in these metrics with the flexible matching based on stemming and Wordnet in Meteor."
W08-0324,Statistical Transfer Systems for {F}rench-{E}nglish and {G}erman-{E}nglish Machine Translation,2008,10,8,7,1,13984,greg hanneman,Proceedings of the Third Workshop on Statistical Machine Translation,0,We apply the Stat-XFER statistical transfer machine translation framework to the task of translating from French and German into English. We introduce statistical methods within our framework that allow for the principled extraction of syntax-based transfer rules from parallel corpora given word alignments and constituency parses. Performance is evaluated on test sets from the 2007 WMT shared task.
monson-etal-2008-linguistic,Linguistic Structure and Bilingual Informants Help Induce Machine Translation of Lesser-Resourced Languages,2008,14,10,5,1,44751,christian monson,Proceedings of the Sixth International Conference on Language Resources and Evaluation ({LREC}'08),0,"Producing machine translation (MT) for the many minority languages in the world is a serious challenge. Minority languages typically have few resources for building MT systems. For many minor languages there is little machine readable text, few knowledgeable linguists, and little money available for MT development. For these reasons, our research programs on minority language MT have focused on leveraging to the maximum extent two resources that are available for minority languages: linguistic structure and bilingual informants. All natural languages contain linguistic structure. And although the details of that linguistic structure vary from language to language, language universals such as context-free syntactic structure and the paradigmatic structure of inflectional morphology, allow us to learn the specific details of a minority language. Similarly, most minority languages possess speakers who are bilingual with the major language of the area. This paper discusses our efforts to utilize linguistic structure and the translation information that bilingual informants can provide in three sub-areas of our rapid development MT program: morphology induction, syntactic transfer rule learning, and refinement of imperfect learned rules."
2008.amta-srw.1,Improving Syntax-Driven Translation Models by Re-structuring Divergent and Nonisomorphic Parse Tree Structures,2008,-1,-1,2,1,44215,vamshi ambati,Proceedings of the 8th Conference of the Association for Machine Translation in the Americas: Student Research Workshop,0,"Syntax-based approaches to statistical MT require syntax-aware methods for acquiring their underlying translation models from parallel data. This acquisition process can be driven by syntactic trees for either the source or target language, or by trees on both sides. Work to date has demonstrated that using trees for both sides suffers from severe coverage problems. This is primarily due to the highly restrictive space of constituent segmentations that the trees on two sides introduce, which adversely affects the recall of the resulting translation models. Approaches that project from trees on one side, on the other hand, have higher levels of recall, but suffer from lower precision, due to the lack of syntactically-aware word alignments. In this paper we explore the issue of lexical coverage of the translation models learned in both of these scenarios. We specifically look at how the non-isomorphic nature of the parse trees for the two languages affects recall and coverage. We then propose a novel technique for restructuring target parse trees, that generates highly isomorphic target trees that preserve the syntactic boundaries of constituents that were aligned in the original parse trees. We evaluate the translation models learned from these restructured trees and show that they are significantly better than those learned using trees on both sides and trees on one side."
W07-1315,{P}ara{M}or: Minimally Supervised Induction of Paradigm Structure and Morphological Analysis,2007,9,19,3,1,44751,christian monson,Proceedings of Ninth Meeting of the {ACL} Special Interest Group in Computational Morphology and Phonology,0,"Paradigms provide an inherent organizational structure to natural language morphology. ParaMor, our minimally supervised morphology induction algorithm, retrusses the word forms of raw text corpora back onto their paradigmatic skeletons; performing on par with state-of-the-art minimally supervised morphology induction algorithms at morphological analysis of English and German. ParaMor consists of two phases. Our algorithm first constructs sets of affixes closely mimicking the paradigms of a language. And with these structures in hand, ParaMor then annotates word forms with morpheme boundaries. To set ParaMor's few free parameters we analyze a training corpus of Spanish. Without adjusting parameters, we induce the morphological structure of English and German. Adopting the evaluation methodology of Morpho Challenge 2007 (Kurimo et al., 2007), we compare ParaMor's morphological analyses with Morfessor (Creutz, 2006), a modern minimally supervised morphology induction system. ParaMor consistently achieves competitive F1 measures."
W07-0909,Cross Lingual and Semantic Retrieval for Cultural Heritage Appreciation,2007,23,5,3,0,9890,idan szpektor,Proceedings of the Workshop on Language Technology for Cultural Heritage Data ({L}a{T}e{CH} 2007).,0,"We describe a system which enhances the experience of museum visits by providing users with language-technology-based information retrieval capabilities. The system consists of a cross-lingual search engine, augmented by state of the art semantic expansion technology, specifically designed for the domain of the museum (history and archaeology of Israel). We discuss the technology incorporated in the system, its adaptation to the specific domain and its contribution to cultural heritage appreciation."
W07-0734,{METEOR}: An Automatic Metric for {MT} Evaluation with High Levels of Correlation with Human Judgments,2007,8,417,1,1,13539,alon lavie,Proceedings of the Second Workshop on Statistical Machine Translation,0,"Meteor is an automatic metric for Machine Translation evaluation which has been demonstrated to have high levels of correlation with human judgments of translation quality, significantly outperforming the more commonly used Bleu metric. It is one of several automatic metrics used in this year's shared task within the ACL WMT-07 workshop. This paper recaps the technical details underlying the metric and describes recent improvements in the metric. The latest release includes improved metric parameters and extends the metric to support evaluation of MT output in Spanish, French and German, in addition to English."
W07-0604,High-accuracy Annotation and Parsing of {CHILDES} Transcripts,2007,14,45,3,1,6910,kenji sagae,Proceedings of the Workshop on Cognitive Aspects of Computational Language Acquisition,0,"Corpora of child language are essential for psycholinguistic research. Linguistic annotation of the corpora provides researchers with better means for exploring the development of grammatical constructions and their usage. We describe an ongoing project that aims to annotate the English section of the CHILDES database with grammatical relations in the form of labeled dependency structures. To date, we have produced a corpus of over 65,000 words with manually curated gold-standard grammatical relation annotations. Using this corpus, we have developed a highly accurate data-driven parser for English CHILDES data. The parser and the manually annotated data are freely available for research purposes."
2007.mtsummit-papers.25,Improving transfer-based {MT} systems with automatic refinements,2007,-1,-1,3,0.931234,48477,ariadna llitjos,Proceedings of Machine Translation Summit XI: Papers,0,None
2007.mtsummit-papers.33,Experiments with a noun-phrase driven statistical machine translation system,2007,-1,-1,2,0,37937,sanjika hewavitharana,Proceedings of Machine Translation Summit XI: Papers,0,None
P06-2089,A Best-First Probabilistic Shift-Reduce Parser,2006,21,58,2,1,6910,kenji sagae,Proceedings of the {COLING}/{ACL} 2006 Main Conference Poster Sessions,0,"Recently proposed deterministic classifier-based parsers (Nivre and Scholz, 2004; Sagae and Lavie, 2005; Yamada and Mat-sumoto, 2003) offer attractive alternatives to generative statistical parsers. Deterministic parsers are fast, efficient, and simple to implement, but generally less accurate than optimal (or nearly optimal) statistical parsers. We present a statistical shift-reduce parser that bridges the gap between deterministic and probabilistic parsers. The parsing model is essentially the same as one previously used for deterministic parsing, but the parser performs a best-first search instead of a greedy search. Using the standard sections of the WSJ corpus of the Penn Treebank for training and testing, our parser has 88.1% precision and 87.8% recall (using automatically assigned part-of-speech tags). Perhaps more interestingly, the parsing model is significantly different from the generative models used by other well-known accurate parsers, allowing for a simple combination that produces precision and recall of 90.9% and 90.7%, respectively."
N06-2033,Parser Combination by Reparsing,2006,11,154,2,1,6910,kenji sagae,"Proceedings of the Human Language Technology Conference of the {NAACL}, Companion Volume: Short Papers",0,"We present a novel parser combination scheme that works by reparsing input sentences once they have already been parsed by several different parsers. We apply this idea to dependency and constituent parsing, generating results that surpass state-of-the-art accuracy levels for individual parsers."
W05-1513,A Classifier-Based Parser with Linear Run-Time Complexity,2005,16,92,2,1,6910,kenji sagae,Proceedings of the Ninth International Workshop on Parsing Technology,0,"We present a classifier-based parser that produces constituent trees in linear time. The parser uses a basic bottom-up shift-reduce algorithm, but employs a classifier to determine parser actions instead of a grammar. This can be seen as an extension of the deterministic dependency parser of Nivre and Scholz (2004) to full constituent parsing. We show that, with an appropriate feature set used in classification, a very simple one-path greedy parser can perform at the same level of accuracy as more complex parsers. We evaluate our parser on section 23 of the WSJ section of the Penn Treebank, and obtain precision and recall of 87.54% and 87.61%, respectively."
W05-0909,{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments,2005,5,1394,2,0,44378,satanjeev banerjee,Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization,0,"We describe METEOR, an automatic metric for machine translation evaluation that is based on a generalized concept of unigram matching between the machineproduced translation and human-produced reference translations. Unigrams can be matched based on their surface forms, stemmed forms, and meanings; furthermore, METEOR can be easily extended to include more advanced matching strategies. Once all generalized unigram matches between the two strings have been found, METEOR computes a score for this matching using a combination of unigram-precision, unigram-recall, and a measure of fragmentation that is designed to directly capture how well-ordered the matched words in the machine translation are in relation to the reference. We evaluate METEOR by measuring the correlation between the metric scores and human judgments of translation quality. We compute the Pearson R correlation value between its scores and human quality assessments of the LDC TIDES 2003 Arabic-to-English and Chinese-to-English datasets. We perform segment-bysegment correlation, and show that METEOR gets an R correlation value of 0.347 on the Arabic data and 0.331 on the Chinese data. This is shown to be an improvement on using simply unigramprecision, unigram-recall and their harmonic F1 combination. We also perform experiments to show the relative contributions of the various mapping modules."
P05-3026,Multi-Engine Machine Translation Guided by Explicit Word Matching,2005,8,51,2,0,50891,shyamsundar jayaraman,Proceedings of the {ACL} Interactive Poster and Demonstration Sessions,0,"We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input. The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality. Our approach uses the individual MT engines as black boxes and does not require any explicit cooperation from the original MT systems. A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines. The highest scoring sentence hypothesis is selected as the final output of our system. Experiments, using several Arabic-to-English systems of similar quality, show a substantial improvement in the quality of the translation output."
P05-1025,Automatic Measurement of Syntactic Development in Child Language,2005,16,58,2,1,6910,kenji sagae,Proceedings of the 43rd Annual Meeting of the Association for Computational Linguistics ({ACL}{'}05),1,"To facilitate the use of syntactic information in the study of child language acquisition, a coding scheme for Grammatical Relations (GRs) in transcripts of parent-child dialogs has been proposed by Sagae, MacWhinney and Lavie (2004). We discuss the use of current NLP techniques to produce the GRs in this annotation scheme. By using a statistical parser (Charniak, 2000) and memory-based learning tools for classification (Daelemans et al., 2004), we obtain high precision and recall of several GRs. We demonstrate the usefulness of this approach by performing automatic measurements of syntactic development with the Index of Productive Syntax (Scarborough, 1990) at similar levels to what child language researchers compute manually."
H05-1093,{BLANC}: Learning Evaluation Metrics for {MT},2005,11,28,3,0,48643,lucian lita,Proceedings of Human Language Technology Conference and Conference on Empirical Methods in Natural Language Processing,0,"We introduce BLANC, a family of dynamic, trainable evaluation metrics for machine translation. Flexible, parametrized models can be learned from past data and automatically optimized to correlate well with human judgments for different criteria (e.g. adequacy, fluency) using different correlation measures. Towards this end, we discuss ACS (all common skip-ngrams), a practical algorithm with trainable parameters that estimates reference-candidate translation overlap by computing a weighted sum of all common skip-ngrams in polynomial time. We show that the BLEU and ROUGE metric families are special cases of BLANC, and we compare correlations with human judgments across these three metric families. We analyze the algorithmic complexity of ACS and argue that it is more powerful in modeling both local meaning and sentence-level structure, while offering the same practicality as the established algorithms it generalizes."
2005.eamt-1.13,A framework for interactive and automatic refinement of transfer-based machine translation,2005,6,29,3,0.887722,48477,ariadna llitjos,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"Most current Machine Translation (MT) systems do not improve with feedback from post-editors beyond the addition of corrected translations to parallel training data (for statistical and example-base MT) or to a memory database. Rule based systems to date improve only via manual debugging. In contrast, we propose a largely automated method for capturing more information from human post-editors, so that corrections may be performed automatically to translation grammar rules and lexical entries. This paper introduces a general framework for incorporating a refinement module into rule-based transfer MT systems. This framework allows for generalizing post-editing efforts in an effective way, by identifying and correcting rules semi-automatically on order to improve coverage and overall translation quality."
2005.eamt-1.20,Multi-engine machine translation guided by explicit word matching,2005,8,51,2,0,50891,shyamsundar jayaraman,Proceedings of the 10th EAMT Conference: Practical applications of machine translation,0,"We describe a new approach for synthetically combining the output of several different Machine Translation (MT) engines operating on the same input. The goal is to produce a synthetic combination that surpasses all of the original systems in translation quality. Our approach uses the individual MT engines as black boxes and does not require any explicit cooperation from the original MT systems. A decoding algorithm uses explicit word matches, in conjunction with confidence estimates for the various engines and a trigram language model in order to score and rank a collection of sentence hypotheses that are synthetic combinations of words from the various original engines. The highest scoring sentence hypothesis is selected as the final output of our system. Experiments, using several Arabic-to-English systems of similar quality, show a substantial improvement in the quality of the translation output."
W04-0107,Unsupervised Induction of Natural Language Morphology Inflection Classes,2004,13,19,2,1,44751,christian monson,Proceedings of the 7th Meeting of the {ACL} Special Interest Group in Computational Phonology: Current Themes in Computational Phonology and Morphology,0,"We propose a novel language-independent framework for inducing a collection of morphological inflection classes from a monolingual corpus of full form words. Our approach involves two main stages. In the first stage, we generate a large data structure of candidate inflection classes and their interrelationships. In the second stage, search and filtering techniques are applied to this data structure, to identify a select collection of true inflection classes of the language. We describe the basic methodology involved in both stages of our approach and present an evaluation of our baseline techniques applied to induction of major inflection classes of Spanish. The preliminary results on an initial training corpus already surpass an F1 of 0.5 against ideal Spanish inflectional morphology classes."
monson-etal-2004-data,Data Collection and Analysis of {M}apudungun Morphology for Spelling Correction,2004,4,2,6,1,44751,christian monson,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"This paper describes part of a three year collaboration between Carnegie Mellon University's Language Technologies Institute, the Programa de Educacion Intercultural Bilingue of the Chilean Ministry of Education, and Universidad de La Frontera (Temuco, Chile). We are currently constructing a spelling checker for Mapudungun, a polysynthetic language spoken by the Mapuche people in Chile and Argentina. The spelling checker will be built in MySpell, the spell checking system used by the open source office suite OpenOffice. This paper also describes the spoken language corpus that is used as a source of data for developing the spelling checker."
sagae-etal-2004-adding,Adding Syntactic Annotations to Transcripts of Parent-Child Dialogs,2004,17,17,3,1,6910,kenji sagae,Proceedings of the Fourth International Conference on Language Resources and Evaluation ({LREC}{'}04),0,"We describe an annotation scheme for syntactic information in the CHILDES database (MacWhinney, 2000), which contains several megabytes of transcribed dialogs between parents and children. The annotation scheme is based on grammatical relations (GRs) that are composed of bilexical dependencies (between a head and a dependent) labeled with the name of the relation involving the two words (such as subject, object and adjunct). We also discuss automatic annotation using our syntactic annotation scheme."
2004.tmi-1.1,Rapid prototyping of a transfer-based {H}ebrew-to-{E}nglish machine translation system,2004,8,28,1,1,13539,alon lavie,Proceedings of the 10th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,"We describe the rapid development of a preliminary Hebrew-to-English Machine Translation system under a transfer-based framework specifically designed for rapid MT prototyping for languages with limited linguistic resources. The task is particularly challenging due to two main reasons: the high lexical and morphological ambiguity of Hebrew and the dearth of available resources for the language. Existing, publicly available resources were adapted in novel ways to support the MT task. The methodology behind the system combines two separate modules: a transfer engine which produces a lattice of possible translation segments, and a decoder which searches and selects the most likely translation according to an English language model. We demonstrate that a small manually crafted set of transfer rules suffices to produce legible translations. Performance results are evaluated using state of the art measures and are shown to be encouraging."
2004.eamt-1.14,A trainable transfer-based {MT} approach for languages with limited resources,2004,11,24,1,1,13539,alon lavie,Proceedings of the 9th EAMT Workshop: Broadening horizons of machine translation and its applications,0,"We describe a Machine Translation (MT) approach that is specifically designed to enable rapid development of MT for languages with limited amounts of online resources. Our approach assumes the availability of a small number of bi-lingual speakers of the two languages, but these need not be linguistic experts. The bi-lingual speakers create a comparatively small corpus of word aligned phrases and sentences (on the order of magnitude of a few thousand sentence pairs) using a specially designed elicitation tool. From this data, the learning module of our system automatically infers hierarchical syntactic transfer rules, which encode how syntactic constituent structures in the source language transfer to the target language. The collection of transfer rules is then used in our run-time system to translate previously unseen source language text into the target language. We describe the general principles underlying our approach, and present results from an experiment, where we developed a basic Hindi-to-English MT system over the course of two months, using extremely limited resources."
lavie-etal-2004-significance,The significance of recall in automatic metrics for {MT} evaluation,2004,11,66,1,1,13539,alon lavie,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Recent research has shown that a balanced harmonic mean (F1 measure) of unigram precision and recall outperforms the widely used BLEU and NIST metrics for Machine Translation evaluation in terms of correlation with human judgments of translation quality. We show that significantly better correlations can be achieved by placing more weight on recall than on precision. While this may seem unexpected, since BLEU and NIST focus on n-gram precision and disregard recall, our experiments show that correlation with human judgments is highest when almost all of the weight is assigned to recall. We also show that stemming is significantly beneficial not just to simpler unigram precision and recall based metrics, but also to BLEU and NIST."
probst-lavie-2004-structurally,A structurally diverse minimal corpus for eliciting structural mappings between languages,2004,7,3,2,0.609756,48480,katharina probst,Proceedings of the 6th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,We describe an approach to creating a small but diverse corpus in English that can be used to elicit information about any target language. The focus of the corpus is on structural information. The resulting bilingual corpus can then be used for natural language processing tasks such as inferring transfer mappings for Machine Translation. The corpus is sufficiently small that a bilingual user can translate and word-align it within a matter of hours. We describe how the corpus is created and how its structural diversity is ensured. We then argue that it is not necessary to introduce a large amount of redundancy into the corpus. This is shown by creating an increasingly redundant corpus and observing that the information gained converges as redundancy increases.
W03-3014,Parsing Domain Actions with Phrase-Level Grammars and Memory-Based Learners,2003,12,2,2,1,52566,chad langley,Proceedings of the Eighth International Conference on Parsing Technologies,0,"In this paper, we describe an approach to analysis for spoken language translation that combines phrase-level grammar-based parsing and automatic domain action classification. The job of the analyzer is to transform utterances into a shallow semantic task-oriented interlingua representation. The goal of our hybrid approach is to provide accurate real-time analyses and to improve robustness and portability to new domains and languages."
W03-3019,Combining Rule-based and Data-driven Techniques for Grammatical Relation Extraction in Spoken Language,2003,14,6,2,1,6910,kenji sagae,Proceedings of the Eighth International Conference on Parsing Technologies,0,"We investigate an aspect of the relationship between parsing and corpus-based methods in NLP that has received relatively little attention: coverage augmentation in rule-based parsers. In the specific task of determining grammatical relations (such as subjects and objects) in transcribed spoken language, we show that a combination of rule-based and corpus-based approaches, where a rule-based system is used as the teacher (or an automatic data annotator) to a corpus-based system, outperforms either system in isolation."
W03-2118,Domain Specific Speech Acts for Spoken Language Translation,2003,14,38,3,0,17380,lori levin,Proceedings of the Fourth {SIG}dial Workshop of Discourse and Dialogue,0,"We describe a coding scheme for machine translation of spoken taskoriented dialogue. The coding scheme covers two levels of speaker intention xe2x88x92 domain independent speech acts and domain dependent domain actions. Our database contains over 14,000 tagged sentences in English, Italian, and German. We argue that domain actions, and not speech acts, are the relevant discourse unit for improving translation quality. We also show that, although domain actions are domain specific, the approach scales up to large domains without an explosion of domain actions and can be coded with high inter-coder reliability across research sites. Furthermore, although the number of domain actions is on the order of ten times the number of speech acts, sparseness is not a problem for the training of classifiers for identifying the domain action. We describe our work on developing high accuracy speech act and domain action classifiers, which is the core of the source language analysis module of our NESPOLE machine translation system."
N03-4015,{S}peechalator: Two-Way Speech-to-Speech Translation in Your Hand,2003,2,20,6,0,5073,alex waibel,Companion Volume of the Proceedings of {HLT}-{NAACL} 2003 - Demonstrations,0,"This demonstration involves two-way automatic speech-to-speech translation on a consumer off-the-shelf PDA. This work was done as part of the DARPA-funded Babylon project, investigating better speech-to-speech translation systems for communication in the field. The development of the Speechalator software-based translation system required addressing a number of hard issues, including a new language for the team (Egyptian Arabic), close integration on a small device, computational efficiency on a limited platform, and scalable coverage for the domain."
W02-0703,Spoken Language Parsing Using Phrase-Level Grammars and Trainable Classifiers,2002,11,6,2,1,52566,chad langley,Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems,0,"In this paper, we describe a novel approach to spoken language analysis for translation, which uses a combination of grammar-based phrase-level parsing and automatic classification. The job of the analyzer is to produce a shallow semantic interlingua representation for spoken task-oriented utterances. The goal of our hybrid approach is to provide accurate real-time analyses while improving robustness and portability to new domains and languages."
W02-0708,Balancing Expressiveness and Simplicity in an Interlingua for Task Based Dialogue,2002,8,18,7,0.548735,17380,lori levin,Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems,0,"In this paper we compare two interlingua representations for speech translation. The basis of this paper is a distributional analysis of the C-STAR II and NESPOLE databases tagged with interlingua representations. The C-STAR II database has been partially re-tagged with the NESPOLE interlingua, which enables us to make comparisons on the same data with two types of interlinguas and on two types of data (C-STAR II and NESPOLE) with the same interlingua. The distributional information presented in this paper show that the NESPOLE interlingua maintains the language-independence and simplicity of the C-STAR II speech-act-based approach, while increasing semantic expressiveness and scalability."
W02-0717,A Multi-Perspective Evaluation of the {NESPOLE}! Speech-to-Speech Translation System,2002,9,19,1,1,13539,alon lavie,Proceedings of the {ACL}-02 Workshop on Speech-to-Speech Translation: Algorithms and Systems,0,"Performance and usability of real-world speech-to-speech translation systems, like the one developed within the NESPOLE! project, are affected by several aspects that go beyond the pure translation quality provided by the underlying components of the system. In this paper we describe these aspects as perspectives along which we have evaluated the NESPOLE! system. Four main issues are investigated: (1) assessing system performance under various network traffic conditions; (2) a study on the usage and utility of multi-modality in the context of multi-lingual communication; (3) a comparison of the features of the individual speech recognition engines, and (4) an end-to-end evaluation of the system."
2002.tmi-papers.19,Rapid adaptive development of semantic analysis grammars,2002,-1,-1,2,0,49102,alicia tribble,Proceedings of the 9th Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages: Papers,0,None
lavie-etal-2002-nespole,The {NESPOLE}! speech-to-speech translation system,2002,9,19,1,1,13539,alon lavie,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: System Descriptions,0,"NESPOLE! is a speech-to-speech machine translation research system designed to provide fully functional speech-to-speech capabilities within real-world settings of common users involved in e-commerce applications. The project is funded jointly by the European Commission and the US NSF. The NESPOLE! system uses a client-server architecture to allow a common user, who is browsing web-pages on the internet, to connect seamlessly in real-time to an agent of the service provider, using a video-conferencing channel and with speech-to-speech translation services mediating the conversation. Shared web pages and annotated images supported via a Whiteboard application are available to enhance the communication."
carbonell-etal-2002-automatic,Automatic rule learning for resource-limited {MT},2002,8,21,5,0,10837,jaime carbonell,Proceedings of the 5th Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"Machine Translation of minority languages presents unique challenges, including the paucity of bilingual training data and the unavailability of linguistically-trained speakers. This paper focuses on a machine learning approach to transfer-based MT, where data in the form of translations and lexical alignments are elicited from bilingual speakers, and a seeded version-space learning algorithm formulates and refines transfer rules. A rule-generalization lattice is defined based on LFG-style f-structures, permitting generalization operators in the search for the most general rules consistent with the elicited data. The paper presents these methods and illustrates examples."
W01-1816,Parsing the {CHILDES} Database: Methodology and Lessons Learned,2001,15,6,2,1,6910,kenji sagae,Proceedings of the Seventh International Workshop on Parsing Technologies,0,"This paper discusses the process of parsing adult utterances directed to a child, in an effort to produce a syntactically annotated corpus of the verbal input to a human language learner. In parsing the Eve corpus of the CHILDES database, we encountered several challenges relating to parser coverage and ambiguity, for which we describe solutions that result in a system capable of analyzing almost 80% of the adult utterances in the corpus correctly. We describe characteristics of the language in the corpus that make this task unique, and present specific ways to deal with the analysis of this type of language. We discuss each step of the corpus analysis in detail, focusing on how selected techniques, such as part-of-speech tagging, rule-based robust parsing and statistical disambiguation, affect the trade-off between coverage and accuracy. Finally, we present a detailed evaluation of the performance of our system. A parsed corpus resulting from the research described in this paper is available to the research community."
H01-1007,Architecture and Design Considerations in {NESPOLE}!: a Speech Translation System for {E}-commerce Applications,2001,3,30,1,1,13539,alon lavie,Proceedings of the First International Conference on Human Language Technology Research,0,"NESPOLE! is a speech-to-speech machine translation research project funded jointly by the European Commission and the US NSF. The main goal of the NESPOLE! project is to advance the state-of-the-art of speech-to-speech translation in a real-world setting of common users involved in e-commerce applications. The project is a collaboration between three European research labs (IRST in Trento Italy, ISL at University of Karlsruhe in Germany, CLIPS at UJF in Grenoble France), a US research group (ISL at Carnegie Mellon in Pittsburgh) and two industrial partners (APT - the Trentino provincial tourism bureau, and Aethra - an Italian tele-communications commercial company). The speech-to-speech translation approach taken by the project builds upon previous work that the research partners conducted within the context of the C-STAR consortium (see http://www.c-star.org). The prototype system developed in NESPOLE! is intended to provide effective multi-lingual speech-to-speech communication between all pairs of four languages (Italian, German, French and English) within broad, but yet restricted domains. The first showcase currently under development is in the domain of tourism and travel information."
H01-1018,Domain Portability in Speech-to-Speech Translation,2001,12,10,1,1,13539,alon lavie,Proceedings of the First International Conference on Human Language Technology Research,0,"Speech-to-speech translation has made significant advances over the past decade, with several high-visibility projects (C-STAR, Verb-mobil, the Spoken Language Translator, and others) significantly advancing the state-of-the-art. While speech recognition can currently effectively deal with very large vocabularies and is fairly speaker independent, speech translation is currently still effective only in limited, albeit large, domains. The issue of domain portability is thus of significant importance, with several current research efforts designed to develop speech-translation systems that can be ported to new domains with significantly less time and effort than is currently possible."
2001.mtsummit-road.7,Design and implementation of controlled elicitation for machine translation of low-density languages,2001,10,17,4,0.714286,48480,katharina probst,Workshop on MT2010: Towards a Road Map for MT,0,"NICE is a machine translation project for low-density languages. We are building a tool that will elicit a controlled corpus from a bilingual speaker who is not an expert in linguistics. The corpus is intended to cover major typological phenomena, as it is designed to work for any language. Using implicational universals, we strive to minimize the number of sentences that each informant has to translate. From the elicited sentences, we learn transfer rules with a version space algorithm. Our vision for MT in the future is one in which systems can be quickly trained for new languages by native speakers, so that speakers of minor languages can participate in education, health care, government, and internet without having to give up their languages."
2001.mtsummit-papers.69,Pre-processing of bilingual corpora for {M}andarin-{E}nglish {EBMT},2001,5,3,4,0,7842,ying zhang,Proceedings of Machine Translation Summit VIII,0,"Pre-processing of bilingual corpora plays an important role in Example-Based Machine Translation (EBMT) and Statistical-Based Machine Translation (SBMT). For our Mandarin-English EBMT system, pre-processing includes segmentation for Mandarin, bracketing for English and building a statistical dictionary from the corpora. We used the Mandarin segmenter from the Linguistic Data Consortium (LDC). It uses dynamic programming with a frequency dictionary to segment the text. Although the frequency dictionary is large, it does not completely cover the corpora. In this paper, we describe the work we have done to improve the segmentation for Mandarin and the bracketing process for English to increase the length of English phrases. A statistical dictionary is built from the aligned bilingual corpus. It is used as feedback to segmentation and bracketing to re-segment / re-bracket the corpus. The process iterates several times to achieve better results. The final results of the corpus pre-processing are a segmented/bracketed aligned bilingual corpus and a statistical dictionary. We achieved positive results by increasing the average length of Chinese terms about 60{\%} and 10{\%} for English. The statistical dictionary gained about a 30{\%} increase in coverage."
W00-0203,Evaluation of a Practical Interlingua for Task-Oriented Dialogue,2000,3,28,3,0.895219,17380,lori levin,{NAACL}-{ANLP} 2000 Workshop: Applied Interlinguas: Practical Applications of Interlingual Approaches to {NLP},0,"IF (Interchange Format), the interlingua used by the C-STAR consortium, is a speech-act based interlingua for task-oriented dialogue. IF was designed as a practical interlingua that could strike a balance between expressivity and simplicity. If it is too simple, components of meaning will be lost and coverage of unseen data will be low. On the other hand, if it is too complex, it cannot be used with a high degree of consistency by collaborators on different continents. In this paper, we suggest methods for evaluating the coverage of IF and the consistency with which it was used in the C-STAR consortium."
levin-etal-2000-lessons,Lessons Learned from a Task-based Evaluation of Speech-to-Speech Machine Translation,2000,6,3,5,0.895219,17380,lori levin,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"For several years we have been conducting Accuracy Based Evaluations (ABE) of the JANUS speech-to-speech MT system (Gates et al., 1997) which measure quality and delity of translation. Recently we have begun to design a Task Based Evaluation for JANUS (Thomas, 1999) which measures goal completion. This paper describes what we have learned by comparing the two types of evaluation. Both evaluations (ABE and TBE) were conducted on a common set of user studies in the semantic domain of travel planning."
ries-etal-2000-shallow,Shallow Discourse Genre Annotation in {C}all{H}ome {S}panish,2000,14,10,4,1,53920,klaus ries,Proceedings of the Second International Conference on Language Resources and Evaluation ({LREC}{'}00),0,"The classification of speech genre is not yet an established task in language technologies. However we believe that it is a task that will become fairly important as large amounts of audio (and video) data become widely available. The technological cability to easily transmit and store all human interactions in audio and video could have a radical impact on our social structure. The major open question is how this information can be used in practical and beneficial ways. As a first approach to this question we are looking at issues involving information access to databases of human-human interactions. Classification by genre is a first step in the process of retrieving a document out of a large collection. In this paper we introduce a local notion of speech activities that are exist side-by-side in conversations that belong to speech-genre: While the genre of CallHome Spanish is personal telephone calls between family members the actual instances of these calls contain activities such as storytelling, advising, interrogation and so forth. We are presenting experimental work on the detection of those activities using a variety of features. We have also observed that a limited number of distinguised activities can be defined that describes most of the activities in this database in a precise way. Proceedings of the Second International Conference On Language Ressources And Evaluation, LREC 2000, Athens, Greece, 31st May-2nd June 2000"
2000.iwpt-1.16,Optimal Ambiguity Packing in Context-free Parsers with Interleaved Unification,2000,-1,-1,1,1,13539,alon lavie,Proceedings of the Sixth International Workshop on Parsing Technologies,0,"Ambiguity packing is a well known technique for enhancing the efficiency of context-free parsers. However, in the case of unification-augmented context-free parsers where parsing is interleaved with feature unification, the propagation of feature structures imposes difficulties on the ability of the parser to effectively perform ambiguity packing. We demonstrate that a clever heuristic for prioritizing the execution order of grammar rules and parsing actions can achieve a high level of ambiguity packing that is provably optimal. We present empirical evaluations of the proposed technique, performed with both a Generalized LR parser and a chart parser, that demonstrate its effectiveness."
W99-0306,Tagging of Speech Acts and Dialogue Games in {S}panish Call Home,1999,5,24,4,0.895219,17380,lori levin,Towards Standards and Tools for Discourse Tagging,0,"The Clarity project is devoted to automatic detection and classification of discourse structures in casual, non-task-oriented conversation using shallow, corpus-based methods of analysis. For the Clarity project, we have tagged speech acts and dialogue games in the Call Home Spanish corpus. We have done preliminary cross-level experiments on the relationship of word and speech act n-grams to dialogue games. Our results show that the label of a game cannot be predicted from n-grams of words it contains. We get better than baseline results for predicting the label of a game from the sequence of speech acts it contains, but only when the speech acts are hand tagged, and not when they are automatically detected. Our future research will focus on finding linguistic cues that are more predictive of game labels. The automatic classification of speech acts and games is carried out in a multi-level architecture that integrates classification at multiple discourse levels instead of performing them sequentially."
woszczcyna-etal-1998-modular,A modular approach to spoken language translation for large domains,1998,8,18,5,0,55460,monika woszczcyna,Proceedings of the Third Conference of the Association for Machine Translation in the Americas: Technical Papers,0,"The MT engine of the JANUS speech-to-speech translation system is designed around four main principles: 1) an interlingua approach that allows the efficient addition of new languages, 2) the use of semantic grammars that yield low cost high quality translations for limited domains, 3) modular grammars that support easy expansion into new domains, and 4) efficient integration of multiple grammars using multi-domain parse lattices and domain re-scoring. Within the framework of the C-STAR-II speech-to-speech translation effort, these principles are tested against the challenge of providing translation for a number of domains and language pairs with the additional restriction of a common interchange format."
W97-0410,Expanding the Domain of a Multi-lingual Speech-to-Speech Translation System,1997,6,7,1,1,13539,alon lavie,Spoken Language Translation,0,"JANUS is a multi-lingual speech-to-speech translation system, which has been designed to translate spontaneous spoken language in a limited domain. In this paper, we describe our recent preliminary efforts to expand the domain of coverage of the system from the rather limited Appointment Scheduling domain, to the much richer Travel Planning domain. We compare the two domains in terms of out-of-vocabulary rates and linguistic complexity. We discuss the challenges that these differences impose on our translation system and some planned changes in the design of the system. Initial evaluations on Travel Planning data are also presented."
W97-0303,An Efficient Distribution of Labor in a Two Stage Robust Interpretation Process,1997,8,9,2,0,2584,carolyn rose,Second Conference on Empirical Methods in Natural Language Processing,0,None
C96-1075,Multi-lingual Translation of Spontaneously Spoken Language in a Limited Domain,1996,9,11,1,1,13539,alon lavie,{COLING} 1996 Volume 1: The 16th International Conference on Computational Linguistics,0,"JANUS is a multi-lingual speech-to-speech translation system designed to facilitate communication between two parties engaged in a spontaneous conversation in a limited domain. In an attempt to achieve both robustness and translation accuracy we use two different translation components: the GLR module, designed to be more accurate, and the Phoenix module, designed to be more robust. We analyze the strengths and weaknesses of each of the approaches and describe our work on combining them. Another recent focus has been on developing a detailed end-to-end evaluation procedure to measure the performance and effectiveness of the system. We present our most recent Spanish-to-English performance evaluation results."
1996.amta-1.30,{JANUS}: multi-lingual translation of spontaneous speech in limited domain,1996,-1,-1,1,1,13539,alon lavie,Conference of the Association for Machine Translation in the Americas,0,None
1995.tmi-1.13,Using Context in Machine Translation of Spoken Language,1995,-1,-1,6,0.702429,17380,lori levin,Proceedings of the Sixth Conference on Theoretical and Methodological Issues in Machine Translation of Natural Languages,0,None
P94-1045,An Integrated Heuristic Scheme for Partial Parse Evaluation,1994,2,21,1,1,13539,alon lavie,32nd Annual Meeting of the Association for Computational Linguistics,1,"GLR is a recently developed robust version of the Generalized LR Parser [Tomita, 1986], that can parse almost any input sentence by ignoring unrecognizable parts of the sentence. On a given input sentence, the parser returns a collection of parses that correspond to maximal, or close to maximal, parsable subjsets of the original input. This paper describes recent work on developing an integrated heuristic scheme for selecting the parse that is deemed best from such a collection. We describe the heuristic measures used and their combination scheme. Preliminary results from experiments conducted on parsing speech recognized spontaneous speech are also reported."
1993.iwpt-1.12,{GLR}* {--} An Efficient Noise-skipping Parsing Algorithm For Context Free Grammars,1993,7,0,1,1,13539,alon lavie,Proceedings of the Third International Workshop on Parsing Technologies,0,"This paper describes GLR*, a parser that can parse any input sentence by ignoring unrecognizable parts of the sentence. In case the standard parsing procedure fails to parse an input sentence, the parser nondeterministically skips some word(s) in the sentence, and returns the parse with fewest skipped words. Therefore, the parser will return some parse(s) with any input sentence, unless no part of the sentence can be recognized at all. The problem can be defined in the following way: Given a context-free grammar $G$ and a sentence $S$, find and parse $S'$ {--} the largest subset of words of $S$, such that $S' \in L(G)$. The algorithm described in this paper is a modification of the Generalized LR (Tomita) parsing algorithm [Tomita, 1986] . The parser accommodates the skipping of words by allowing shift operations to be performed from inactive state nodes of the Graph Structured Stack. A heuristic similar to beam search makes the algorithm computationally tractable. There have been several other approaches to the problem of robust parsing, most of which are special purpose algorithms [Carbonell and Hayes, 1984] , [Ward, 1991] and others. Because our approach is a modification to a standard context-free parsing algorithm, all the techniques and grammars developed for the standard parser can be applied as they are. Also, in case the input sentence is by itself grammatical, our parser behaves exactly as the standard GLR parser. The modified parser, GLR*, has been implemented and integrated with the latest version of the Generalized LR Parser/Compiler [Tomita et al , 1988], [Tomita, 1990]. We discuss an application of the GLR* parser to spontaneous speech understanding and present some preliminary tests on the utility of the GLR* parser in such settings."
