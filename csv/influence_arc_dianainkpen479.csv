2007.jeptalnrecital-long.8,P02-1033,0,0.0514306,"Missing"
2007.jeptalnrecital-long.8,P06-1056,1,0.763884,"Missing"
2007.jeptalnrecital-long.8,N01-1014,0,0.0443316,"Missing"
2007.jeptalnrecital-long.8,J04-1001,0,0.0422686,"Missing"
2007.jeptalnrecital-long.8,N03-2016,0,0.181407,"Missing"
2007.jeptalnrecital-long.8,J99-1003,0,0.504387,"Missing"
2007.jeptalnrecital-long.8,1992.tmi-1.7,0,0.362359,"Missing"
2007.jeptalnrecital-long.8,P95-1026,0,0.170375,"Missing"
2009.mtsummit-papers.21,P91-1022,0,0.33147,"to have a proliferation of such HTML markups. Usually, if a text in French contains a markup for a section in italics, then the corresponding section in English is likely to have the same markup. We did some HTML style unification formatting so that some parts of the HTML codes are highlighted, while some are ignored. For example, the code &lt;a …&gt; becomes &lt;a alink&gt; after the unification formatting, because otherwise the link to an English webpage and the link to a French webpage might have been different. Lexical units: Specific lexical units such as words or phrases can serve as anchor points (Brown et al., 1991). In SDTES, we mainly use cognate words as anchors to help locate traces of alignment deviations. Identification of cognates in SDTES is a two-step operation. The first step is to produce candidate cognate lists using the K-vec algorithm (Fung and Church, 1994). The main objective is to find cognate candidates within an acceptable text-region range and limit the number of words to be considered as cognate pairs. The K-vec method was developed as a means of generating “a quick-and-dirty estimate of a bilingual lexicon” that “could be used as a starting point for a more detailed alignment algori"
2009.mtsummit-papers.21,2005.eamt-1.9,0,0.0232648,"luation results show that SDTES is a very effective system for identifying and extracting sentences that are translation pairs from most of the federal government web pages which are currently under the CLF2 (Common Look and Feel for the Internet 2.0) framework. 1 Introduction Well-aligned bilingual materials are a useful source of translations that can be used for translation studies, translation recycling, training data in machine learning, translation memories, information retrieval, machine translation, machine aided translation, and natural language processing (Jutras, 2000; Moore, 2002; Callison-Burch et al., 2005; Hutchins, 2005; Deng et al., 2006). In the past decade or so, an emerging trend in the use of bilingual texts has become rather noticeable: use of the web as a bilingual corpus (Resnik, 1999; Chen and Nie, 2000). With the increasingly widespread availability of vast quantities of web-based bilingual texts, more and more researchers are exploring ways to collect, from the web, bilingual texts of such language pairs as English-French, EnglishGerman, English-Italian, English-Chinese, English-Arabic and others (Kraaij et al., 2003; Resnik and Smith, 2003). Web-based materials that are officially"
2009.mtsummit-papers.21,C94-2178,0,0.115271,"ts of the HTML codes are highlighted, while some are ignored. For example, the code &lt;a …&gt; becomes &lt;a alink&gt; after the unification formatting, because otherwise the link to an English webpage and the link to a French webpage might have been different. Lexical units: Specific lexical units such as words or phrases can serve as anchor points (Brown et al., 1991). In SDTES, we mainly use cognate words as anchors to help locate traces of alignment deviations. Identification of cognates in SDTES is a two-step operation. The first step is to produce candidate cognate lists using the K-vec algorithm (Fung and Church, 1994). The main objective is to find cognate candidates within an acceptable text-region range and limit the number of words to be considered as cognate pairs. The K-vec method was developed as a means of generating “a quick-and-dirty estimate of a bilingual lexicon” that “could be used as a starting point for a more detailed alignment algorithm …” (Fung and Church, 1994). We use the K-vec++ package (Pedersen and Varma, 2002) for the implementation of the K-vec algorithm. The package is called the K-vec++ package because it extends the K-vec algorithm in a number of ways. Using the Perl programs in"
2009.mtsummit-papers.21,J93-1004,0,0.65017,"r text alignment. We used these observations as basic assumptions about the web pages of government websites in Canada, and designed algorithms and procedures to align the web pages and to detect potential errors in translation and in alignment. 3 Algorithms and procedures SDTES contains two major components: one for bilingual text alignment and the other for misalignment detection. In this part, we present a set of protocols and algorithms that are designed for the two components. 3.1 Text mapping using the Gale-Church statistical model The alignment component of SDTES is mostly based on the Gale and Church (1993) length model. The basic assumption of this model is that there is a strong likelihood that, for example, a long sentence in English will correspond to a long sentence in French; similarly a short sentence in one language will correspond to a short sentence in the other. Roughly speaking, if the average lengths of sentences in French and English are known, it is possible to set up a distribution of alignment possibilities from the sentence length information. Suppose we have parallel texts S and T which can be split into n segments each. Each segment in S (si) is the translation of a segment i"
2009.mtsummit-papers.21,2005.mtsummit-ebmt.9,0,0.0291002,"ES is a very effective system for identifying and extracting sentences that are translation pairs from most of the federal government web pages which are currently under the CLF2 (Common Look and Feel for the Internet 2.0) framework. 1 Introduction Well-aligned bilingual materials are a useful source of translations that can be used for translation studies, translation recycling, training data in machine learning, translation memories, information retrieval, machine translation, machine aided translation, and natural language processing (Jutras, 2000; Moore, 2002; Callison-Burch et al., 2005; Hutchins, 2005; Deng et al., 2006). In the past decade or so, an emerging trend in the use of bilingual texts has become rather noticeable: use of the web as a bilingual corpus (Resnik, 1999; Chen and Nie, 2000). With the increasingly widespread availability of vast quantities of web-based bilingual texts, more and more researchers are exploring ways to collect, from the web, bilingual texts of such language pairs as English-French, EnglishGerman, English-Italian, English-Chinese, English-Arabic and others (Kraaij et al., 2003; Resnik and Smith, 2003). Web-based materials that are officially-published bilin"
2009.mtsummit-papers.21,A00-1018,0,0.0225484,"websites in Canada. New evaluation results show that SDTES is a very effective system for identifying and extracting sentences that are translation pairs from most of the federal government web pages which are currently under the CLF2 (Common Look and Feel for the Internet 2.0) framework. 1 Introduction Well-aligned bilingual materials are a useful source of translations that can be used for translation studies, translation recycling, training data in machine learning, translation memories, information retrieval, machine translation, machine aided translation, and natural language processing (Jutras, 2000; Moore, 2002; Callison-Burch et al., 2005; Hutchins, 2005; Deng et al., 2006). In the past decade or so, an emerging trend in the use of bilingual texts has become rather noticeable: use of the web as a bilingual corpus (Resnik, 1999; Chen and Nie, 2000). With the increasingly widespread availability of vast quantities of web-based bilingual texts, more and more researchers are exploring ways to collect, from the web, bilingual texts of such language pairs as English-French, EnglishGerman, English-Italian, English-Chinese, English-Arabic and others (Kraaij et al., 2003; Resnik and Smith, 2003"
2009.mtsummit-papers.21,C04-1137,0,0.011938,"f the naïve matching algorithm of Simard et al. (1992), but avoids problems caused by common prefixes or by the requirement that the first four letters have to match. This improvement can increase the number of correct cognate pairs identified. At the same time, AMS inherits the strength of no-crossinglinks constraint in the Longest Common Subsequence Ratio (LCSR) algorithm by Melamed (1999). Also, in limiting the number of substrings to be matched, AMS overcomes the inherent weakness of LCSR in positing non-intuitive links because of lack of context sensitivity, as noted in a recent study by Kondrak and Dorr (2004). This can help reduce the number of false positives for SDTES such as courtiers/computers, mensuels/results, and parution/starting. We found that the K-vec technique together with AMS algorithm is a good fit for cognate identification for the misalignment detection purpose in SDTES. Numbers and punctuations: Similarly, numbers in texts can serve as anchors in alignment. They are good indications of correspondence because a number in one language is usually interpreted as a number in the other language. Some punctuation marks can also be anchor points. For example, if there is a question mark"
2009.mtsummit-papers.21,J03-3003,0,0.0366082,"Missing"
2009.mtsummit-papers.21,J99-1003,0,0.17851,"some cases these elements can be the source of massive misalignment for the Gale-Church algorithm. In the first parse, SDTES counts the few main HTML elements such as h2, title, and others to see if the pair of text files has the same number Misalignment detection using anchor information Anchor information includes positions or properties in one text which seem to match up with those in a parallel text. The information can be about structural features in the text or delimiters and markers that indicate “hard and soft boundaries” (Gale and Church, 1993:89), or “true points of correspondence” (Melamed, 1999:107) in alignment. Using anchor information, regions of text can be identified and alignments of text segments can be sought. Most alignment methods make use of anchor information in one way or another. In STDES, we use the following structural features as anchors to assist misalignment identification. HTML markups: Anchor information can be markups that go with the text and that reveal metainformation or style information about the text. For example some commonly used HTML tags such as h1, h2, h3, br, p, hr, table, i, pre, form, img, and a can be good anchors in bilingual text alignment. For"
2009.mtsummit-papers.21,moore-2002-fast,0,0.0296385,"nada. New evaluation results show that SDTES is a very effective system for identifying and extracting sentences that are translation pairs from most of the federal government web pages which are currently under the CLF2 (Common Look and Feel for the Internet 2.0) framework. 1 Introduction Well-aligned bilingual materials are a useful source of translations that can be used for translation studies, translation recycling, training data in machine learning, translation memories, information retrieval, machine translation, machine aided translation, and natural language processing (Jutras, 2000; Moore, 2002; Callison-Burch et al., 2005; Hutchins, 2005; Deng et al., 2006). In the past decade or so, an emerging trend in the use of bilingual texts has become rather noticeable: use of the web as a bilingual corpus (Resnik, 1999; Chen and Nie, 2000). With the increasingly widespread availability of vast quantities of web-based bilingual texts, more and more researchers are exploring ways to collect, from the web, bilingual texts of such language pairs as English-French, EnglishGerman, English-Italian, English-Chinese, English-Arabic and others (Kraaij et al., 2003; Resnik and Smith, 2003). Web-based"
2009.mtsummit-papers.21,P99-1068,0,0.0426968,"(Common Look and Feel for the Internet 2.0) framework. 1 Introduction Well-aligned bilingual materials are a useful source of translations that can be used for translation studies, translation recycling, training data in machine learning, translation memories, information retrieval, machine translation, machine aided translation, and natural language processing (Jutras, 2000; Moore, 2002; Callison-Burch et al., 2005; Hutchins, 2005; Deng et al., 2006). In the past decade or so, an emerging trend in the use of bilingual texts has become rather noticeable: use of the web as a bilingual corpus (Resnik, 1999; Chen and Nie, 2000). With the increasingly widespread availability of vast quantities of web-based bilingual texts, more and more researchers are exploring ways to collect, from the web, bilingual texts of such language pairs as English-French, EnglishGerman, English-Italian, English-Chinese, English-Arabic and others (Kraaij et al., 2003; Resnik and Smith, 2003). Web-based materials that are officially-published bilingual materials on Canadian government websites contain exemplary translations, and mining these web-based bilingual materials can be beneficial to translators, linguists and re"
2009.mtsummit-papers.21,J03-3002,0,0.0164732,"ocessing (Jutras, 2000; Moore, 2002; Callison-Burch et al., 2005; Hutchins, 2005; Deng et al., 2006). In the past decade or so, an emerging trend in the use of bilingual texts has become rather noticeable: use of the web as a bilingual corpus (Resnik, 1999; Chen and Nie, 2000). With the increasingly widespread availability of vast quantities of web-based bilingual texts, more and more researchers are exploring ways to collect, from the web, bilingual texts of such language pairs as English-French, EnglishGerman, English-Italian, English-Chinese, English-Arabic and others (Kraaij et al., 2003; Resnik and Smith, 2003). Web-based materials that are officially-published bilingual materials on Canadian government websites contain exemplary translations, and mining these web-based bilingual materials can be beneficial to translators, linguists and researchers in many fields. The StatCan Daily Translation Extraction System (SDTES) is a system that automatically extracts translations from the Daily news release texts of Statistics Canada (Zhu et al., 2007). In this paper, we will describe the algorithms and procedures of SDTES and present new results that show that SDTES can be effectively used to induce transla"
2009.mtsummit-papers.21,1992.tmi-1.7,0,0.129916,"trings θa and βb in W2. If a match is found for θa and βb in W2 in the right order, W1 and W2 are a cognate pair, if not, one substring θa is increased in length (a=a+1) while the other substring βb gets decreased (b=b-1). The search continues till a two-substring match is found or a&gt;T or b&lt;0. The main advantage of using K-vec with AMS for cognate identification is that K-vec does not rely on sentence boundary information. In this way, it can help detect errors in alignment that depend on sentence boundaries. At the same time, AMS has the straightforwardness of the naïve matching algorithm of Simard et al. (1992), but avoids problems caused by common prefixes or by the requirement that the first four letters have to match. This improvement can increase the number of correct cognate pairs identified. At the same time, AMS inherits the strength of no-crossinglinks constraint in the Longest Common Subsequence Ratio (LCSR) algorithm by Melamed (1999). Also, in limiting the number of substrings to be matched, AMS overcomes the inherent weakness of LCSR in positing non-intuitive links because of lack of context sensitivity, as noted in a recent study by Kondrak and Dorr (2004). This can help reduce the numb"
2021.adaptnlp-1.3,D08-1072,0,0.0574243,"veness of CAN on two MDTC benchmarks. The experimental results show that CAN yields state-of-the-art results. Moreover, further experiments on unsupervised multi-source domain adaptation demonstrate that CAN has a good capacity to generalize to unseen domains. 2 Related Work Multi-domain text classification (MDTC) was first proposed by (Li and Zong, 2008), aiming to simultaneously leverage all existing resources across different domains to improve system performance. Currently, there are two main streams for MDTC: one strand exploits covariance matrix to model the relationship across domains (Dredze and Crammer, 2008; Saha et al., 2011; Zhang and Yeung, 2012); the other strand is based on neural networks, sharing the first several layers for each domain to extract low-level features and generating outputs with domain-specific parameters. The multi-task convolutional neural network (MT-CNN) utilizes a convolutional layer in which only the lookup table is 17 shared for better word embeddings (Collobert and Weston, 2008). The collaborative multi-domain sentiment classification (CMSC) combines a classifier that learns common knowledge among domains with a set of classifiers, one per domain, each of which capt"
2021.adaptnlp-1.3,P07-1056,0,0.262304,"only if all M joint distributions are identical. Lemma 1. For any given Fs , {Fdi }M i=1 and C, the optimum conditional domain discriminator D∗ is: Di∗ ([f, c]) Pi (f, c) = PM j=1 Pj (f, c) M X Test 400 400 400 400 400 400 400 400 400 400 400 400 400 400 400 400 Unlabeled 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 2000 475 2000 2000 2000 Avg. L 159 101 173 89 57 130 81 136 90 156 104 117 129 94 269 21 Vocab. 62K 30K 69K 28K 21K 26K 26K 60K 28K 57K 26K 30K 26K 30K 44K 12K (8) 4.1 Experimental Settings Dataset We conduct experiments on two MDTC benchmarks: the Amazon review dataset (Blitzer et al., 2007) and the FDU-MTL dataset (Liu et al., 2017). The Amazon review dataset consists of four domains: books, DVDs, electronics, and kitchen. For each domain, there exist 2,000 instances: 1,000 positive ones and 1,000 negative ones. All data was pre-processed into a bag of features (unigrams and bigrams), losing all word order information. In our experiments, the 5,000 most frequent features are used, representing each review as a 5,000dimensional vector. The FDU-MTL dataset is a more complicated dataset, which contains 16 domains: books, electronics, DVDs, kitchen, apparel, camera, health, music, t"
2021.adaptnlp-1.3,N18-1111,0,0.306016,"text classification which incorporate conditional domain discriminator and entropy conditioning to perform alignment on the joint distributions of shared features and label predictions to improve the system performance. against a shared feature extractor to minimize the divergences across different domains. When the domain discriminator and the shared feature extractor reach equilibrium, the learned shared features can be regarded as domain-invariant and used for the subsequent classification. The adversarial training-based MDTC approaches yield the stateof-the-art results (Liu et al., 2017; Chen and Cardie, 2018). However, these methods still have a significant limitation: when the data distributions present complex structures, adversarial training may fail to perform global alignment among domains. Such a risk comes from the challenge that in adversarial training, only aligning the marginal distributions can not sufficiently guarantee the discriminability of the learned features. The features with different labels may be aligned, as shown in Figure 1. The critical mismatch can lead to weak discriminability of the learned features. In this paper, motivated by the conditional generative adversarial net"
2021.adaptnlp-1.3,D08-1083,0,0.00786627,"ons of shared features and label predictions. Therefore, CAN is a theoretically sound adversarial network that discriminates over multiple distributions. Evaluation results on two MDTC benchmarks show that CAN outperforms prior methods. Further experiments demonstrate that CAN has a good ability to generalize learned knowledge to unseen domains. 1 Introduction Text classification is a fundamental task in Natural Language Processing (NLP) and has received constant attention due to its wide applications, ranging from spam detection to social media analytics (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Socher et al., 2012; Vo and Zhang, 2015). Over the past couple of decades, supervised machine learning methods have shown dominant performance for text classification, such as Naive Bayes Classifiers (Troussas et al., 2013), Support Vector Machines (Li et al., 2018) and Neural Networks (Wu et al., 2020). In particular, with the advent of deep learning, neural network-based text classification models have gained impressive 16 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 16–27 April 20, 2021. ©2021 Association for Computational Linguistics (a) tion with low certainty."
2021.adaptnlp-1.3,P08-2065,0,0.216087,"negative polarity (e.g., This book is infantile and boring). Thus a text classifier trained on one domain is likely to make spurious predictions on another domain whose distribution is different from the training data distribution. In addition, it is always difficult to collect sufficient labeled data for all interested domains. Therefore, it is of great significance to explore how to leverage available resources from related domains to improve the classification accuracy on the target domain. The major line of approaches to tackle the above problem is multi-domain text classification (MDTC) (Li and Zong, 2008), which can handle the scenario where labeled data exist for multiple domains, but in insufficient amounts to training an effective classifier. Deep learning models have yielded impressive performance in MDTC (Wu and Guo, 2020; Wu et al., 2021). Most recent MDTC methods adopt the shared-private paradigm, which divides the latent space into two types: one is the shared feature space for all domains with the aim of capturing domain-invariant knowledge, the other one is the private feature space for each domain which extracts domain-specific knowledge. To explicitly ensure the optimum separations"
2021.adaptnlp-1.3,2020.emnlp-main.639,0,0.0362128,"t clasAll the comparison methods use the standard partitions of the datasets. Thus, we directly cite 22 Method CAN (full) CAN w/o C CAN w/o E CAN w/o CE Books 83.76 82.45 83.60 82.98 DVD 84.68 84.45 84.80 84.03 Electr. 88.34 87.30 87.70 87.06 Kit. 90.03 89.65 89.40 88.57 AVG 86.70 85.96 86.38 85.66 4.3 Unsupervised Multi-Source Domain Adaptation In the MDTC scenario, the model requires labeled training data from each domain. However, in realworld applications, many domains may have no labeled data at all. Therefore, it is important to evaluate the performance of MDTC models on unseen domains (Wright and Augenstein, 2020). In the unsupervised multi-source domain adaptation setting, we have multiple source domains with both labeled and unlabeled data and one target domain with only unlabeled data. The CAN has the ability to learn domain-invariant representations on unlabeled data, and thus it can be generalized to unseen domains. Since the target domain has no labeled data at all, the domain discriminator is updated only on unlabeled data in this setting. When conducting text classification on the target domain, we only feed the shared feature to C and set the domain-specific feature vector to 0. We conduct the"
2021.adaptnlp-1.3,P17-1001,0,0.268833,"s the latent space into two types: one is the shared feature space for all domains with the aim of capturing domain-invariant knowledge, the other one is the private feature space for each domain which extracts domain-specific knowledge. To explicitly ensure the optimum separations among the shared latent space and multiple domain-specific feature spaces, the adversarial training (Goodfellow et al., 2014) is introduced in MDTC. By employing the adversarial training, the domain-specific features can be prevented from creeping into the shared latent space, which will lead to feature redundancy (Liu et al., 2017). In adversarial training, a multinomial domain discriminator is trained In this paper, we propose conditional adversarial networks (CANs), a framework that explores the relationship between the shared features and the label predictions to impose stronger discriminability to the learned features, for multi-domain text classification (MDTC). The proposed CAN introduces a conditional domain discriminator to model the domain variance in both the shared feature representations and the class-aware information simultaneously, and adopts entropy conditioning to guarantee the transferability of the sh"
2021.adaptnlp-1.3,N15-1092,0,0.19645,"ch only the lookup table is 17 shared for better word embeddings (Collobert and Weston, 2008). The collaborative multi-domain sentiment classification (CMSC) combines a classifier that learns common knowledge among domains with a set of classifiers, one per domain, each of which captures domain-dependent features to make the final predictions (Wu and Huang, 2015). The multi-task deep neural network (MT-DNN) maps arbitrary text queries and documents into semantic vector representations in a low dimensional latent space and combines tasks as disparate as operations necessary for classification (Liu et al., 2015). Adversarial learning has several advantages, such as Markov chains are not needed and no inference is required during learning (Mirza and Osindero, 2014). However, there still exists an issue in adversarial learning. When data distributions embody complex structures, adversarial learning can fail in performing the global alignment. The conditional generative adversarial network (CGAN) is proposed to address this problem (Mirza and Osindero, 2014). In CGAN, both the generator and discriminator are conditioned on some extra information, such as labels or data from other modalities, to yield be"
2021.adaptnlp-1.3,W02-1011,0,0.0268232,"gence among multiple joint distributions of shared features and label predictions. Therefore, CAN is a theoretically sound adversarial network that discriminates over multiple distributions. Evaluation results on two MDTC benchmarks show that CAN outperforms prior methods. Further experiments demonstrate that CAN has a good ability to generalize learned knowledge to unseen domains. 1 Introduction Text classification is a fundamental task in Natural Language Processing (NLP) and has received constant attention due to its wide applications, ranging from spam detection to social media analytics (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Socher et al., 2012; Vo and Zhang, 2015). Over the past couple of decades, supervised machine learning methods have shown dominant performance for text classification, such as Naive Bayes Classifiers (Troussas et al., 2013), Support Vector Machines (Li et al., 2018) and Neural Networks (Wu et al., 2020). In particular, with the advent of deep learning, neural network-based text classification models have gained impressive 16 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 16–27 April 20, 2021. ©2021 Association for Computational"
2021.adaptnlp-1.3,D12-1110,0,0.0644577,"and label predictions. Therefore, CAN is a theoretically sound adversarial network that discriminates over multiple distributions. Evaluation results on two MDTC benchmarks show that CAN outperforms prior methods. Further experiments demonstrate that CAN has a good ability to generalize learned knowledge to unseen domains. 1 Introduction Text classification is a fundamental task in Natural Language Processing (NLP) and has received constant attention due to its wide applications, ranging from spam detection to social media analytics (Pang et al., 2002; Hu and Liu, 2004; Choi and Cardie, 2008; Socher et al., 2012; Vo and Zhang, 2015). Over the past couple of decades, supervised machine learning methods have shown dominant performance for text classification, such as Naive Bayes Classifiers (Troussas et al., 2013), Support Vector Machines (Li et al., 2018) and Neural Networks (Wu et al., 2020). In particular, with the advent of deep learning, neural network-based text classification models have gained impressive 16 Proceedings of the Second Workshop on Domain Adaptation for NLP, pages 16–27 April 20, 2021. ©2021 Association for Computational Linguistics (a) tion with low certainty. The entropy conditio"
2021.findings-acl.208,D16-1084,0,0.0556009,"-target and cross-target stance detection by the training setting. Most previous works focused on the in-target stance detection where a classifier is trained and validated on the same target (Mohammad et al., 2016b; Zarrella and Marsh, 2016; Wei et al., 2016; Vijayaraghavan et al., 2016; Du et al., 2017; Sun et al., 2018; Wei et al., 2018; Li and Caragea, 2019, 2021b). However, sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets, which motivates the studies of cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020). Most previous studies evaluated the cross-target models on the SemEval-2016 dataset (Mohammad et al., 2016a), which is a small dataset and thus may make the conclusions less convincing. In this paper, we show that our P-S TANCE dataset can be also used to evaluate the model performance of cross-target stance detection and provides opportunities for exploring more crosstarget tasks by interacting with previous SemEval2016 (Mohammad et al., 2016a) and Multi-Target stance datasets (Sobhani et al., 2017). In addition, P-S TANCE enables the"
2021.findings-acl.208,E17-1024,0,0.0232703,"a is target-specific stance detection (ALDayel and Magdy, 2021) which aims to identify the stance toward a set of figures or topics (Hasan and Ng, 2014; Mohammad et al., 2016a; Xu et al., 2016; Taulé et al., 2017; Swami et al., 2018; Zotova et al., 2020; Conforti et al., 2020b; Lai et al., 2020; Vamvas and Sennrich, 2020; Conforti et al., 2020a). Besides target-specific stance detection, multi-target stance detection (Sobhani et al., 2017; Darwish et al., 2017; Li and Caragea, 2021a), and claimbased stance detection (Qazvinian et al., 2011; Derczynski et al., 2015; Ferreira and Vlachos, 2016; Bar-Haim et al., 2017; Rao and Pomerleau, 2017; Derczynski et al., 2017; Gorrell et al., 2019) are other popular trends of stance detection. Multitarget stance detection aims to jointly identify the stance toward two or more targets in the same text. Unlike the target-specific stance detection and multi-target stance detection where the target is usually a prominent figure or topic, in claimbased stance detection the target is a claim, which could be an article headline or a rumor’s post. Interestingly, despite substantial progress on stance detection, large-scale annotated datasets are limited. We compare our P-S"
2021.findings-acl.208,N19-1423,0,0.174153,"s-target models on the SemEval-2016 dataset (Mohammad et al., 2016a), which is a small dataset and thus may make the conclusions less convincing. In this paper, we show that our P-S TANCE dataset can be also used to evaluate the model performance of cross-target stance detection and provides opportunities for exploring more crosstarget tasks by interacting with previous SemEval2016 (Mohammad et al., 2016a) and Multi-Target stance datasets (Sobhani et al., 2017). In addition, P-S TANCE enables the exploration of largescale deep learning models including pre-trained language models, e.g., BERT (Devlin et al., 2019) and BERTweet (Nguyen et al., 2020). We fine-tune the BERT and BERTweet models on our dataset and compare them with other strong baselines. 3 Building the Dataset In this section, we detail the creation and the particularities of P-S TANCE, our large political stance detection dataset composed of 21,574 tweets collected during the 2020 U.S. presidential election. 3.1 Data Collection We collected tweets using the Twitter streaming API. Similar to prior works (Mohammad et al., 2016a; Sobhani et al., 2017) that target presidential candidates, we focus our attention on three political figures2 in"
2021.findings-acl.208,N16-1138,0,0.0251918,"etection task on social media is target-specific stance detection (ALDayel and Magdy, 2021) which aims to identify the stance toward a set of figures or topics (Hasan and Ng, 2014; Mohammad et al., 2016a; Xu et al., 2016; Taulé et al., 2017; Swami et al., 2018; Zotova et al., 2020; Conforti et al., 2020b; Lai et al., 2020; Vamvas and Sennrich, 2020; Conforti et al., 2020a). Besides target-specific stance detection, multi-target stance detection (Sobhani et al., 2017; Darwish et al., 2017; Li and Caragea, 2021a), and claimbased stance detection (Qazvinian et al., 2011; Derczynski et al., 2015; Ferreira and Vlachos, 2016; Bar-Haim et al., 2017; Rao and Pomerleau, 2017; Derczynski et al., 2017; Gorrell et al., 2019) are other popular trends of stance detection. Multitarget stance detection aims to jointly identify the stance toward two or more targets in the same text. Unlike the target-specific stance detection and multi-target stance detection where the target is usually a prominent figure or topic, in claimbased stance detection the target is a claim, which could be an article headline or a rumor’s post. Interestingly, despite substantial progress on stance detection, large-scale annotated datasets are limi"
2021.findings-acl.208,D19-1657,1,0.817979,"arget-specific 21,574 Table 2: Comparison of English stance detection datasets. Different from classifying the stance detection tasks by target type (i.e., one specific target, multiple targets, or a claim), we can also categorize the stance detection as in-target and cross-target stance detection by the training setting. Most previous works focused on the in-target stance detection where a classifier is trained and validated on the same target (Mohammad et al., 2016b; Zarrella and Marsh, 2016; Wei et al., 2016; Vijayaraghavan et al., 2016; Du et al., 2017; Sun et al., 2018; Wei et al., 2018; Li and Caragea, 2019, 2021b). However, sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets, which motivates the studies of cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020). Most previous studies evaluated the cross-target models on the SemEval-2016 dataset (Mohammad et al., 2016a), which is a small dataset and thus may make the conclusions less convincing. In this paper, we show that our P-S TANCE dataset can be also used to evaluate the model perfor"
2021.findings-acl.208,2021.findings-acl.204,1,0.77849,"g., cross-target stance detection and crosstopic stance detection. 2 Related Work The most common stance detection task on social media is target-specific stance detection (ALDayel and Magdy, 2021) which aims to identify the stance toward a set of figures or topics (Hasan and Ng, 2014; Mohammad et al., 2016a; Xu et al., 2016; Taulé et al., 2017; Swami et al., 2018; Zotova et al., 2020; Conforti et al., 2020b; Lai et al., 2020; Vamvas and Sennrich, 2020; Conforti et al., 2020a). Besides target-specific stance detection, multi-target stance detection (Sobhani et al., 2017; Darwish et al., 2017; Li and Caragea, 2021a), and claimbased stance detection (Qazvinian et al., 2011; Derczynski et al., 2015; Ferreira and Vlachos, 2016; Bar-Haim et al., 2017; Rao and Pomerleau, 2017; Derczynski et al., 2017; Gorrell et al., 2019) are other popular trends of stance detection. Multitarget stance detection aims to jointly identify the stance toward two or more targets in the same text. Unlike the target-specific stance detection and multi-target stance detection where the target is usually a prominent figure or topic, in claimbased stance detection the target is a claim, which could be an article headline or a rumor’"
2021.findings-acl.208,2021.naacl-main.148,1,0.715108,"g., cross-target stance detection and crosstopic stance detection. 2 Related Work The most common stance detection task on social media is target-specific stance detection (ALDayel and Magdy, 2021) which aims to identify the stance toward a set of figures or topics (Hasan and Ng, 2014; Mohammad et al., 2016a; Xu et al., 2016; Taulé et al., 2017; Swami et al., 2018; Zotova et al., 2020; Conforti et al., 2020b; Lai et al., 2020; Vamvas and Sennrich, 2020; Conforti et al., 2020a). Besides target-specific stance detection, multi-target stance detection (Sobhani et al., 2017; Darwish et al., 2017; Li and Caragea, 2021a), and claimbased stance detection (Qazvinian et al., 2011; Derczynski et al., 2015; Ferreira and Vlachos, 2016; Bar-Haim et al., 2017; Rao and Pomerleau, 2017; Derczynski et al., 2017; Gorrell et al., 2019) are other popular trends of stance detection. Multitarget stance detection aims to jointly identify the stance toward two or more targets in the same text. Unlike the target-specific stance detection and multi-target stance detection where the target is usually a prominent figure or topic, in claimbased stance detection the target is a claim, which could be an article headline or a rumor’"
2021.findings-acl.208,2021.ccl-1.108,0,0.0753271,"Missing"
2021.findings-acl.208,S19-2147,0,0.0134874,"s to identify the stance toward a set of figures or topics (Hasan and Ng, 2014; Mohammad et al., 2016a; Xu et al., 2016; Taulé et al., 2017; Swami et al., 2018; Zotova et al., 2020; Conforti et al., 2020b; Lai et al., 2020; Vamvas and Sennrich, 2020; Conforti et al., 2020a). Besides target-specific stance detection, multi-target stance detection (Sobhani et al., 2017; Darwish et al., 2017; Li and Caragea, 2021a), and claimbased stance detection (Qazvinian et al., 2011; Derczynski et al., 2015; Ferreira and Vlachos, 2016; Bar-Haim et al., 2017; Rao and Pomerleau, 2017; Derczynski et al., 2017; Gorrell et al., 2019) are other popular trends of stance detection. Multitarget stance detection aims to jointly identify the stance toward two or more targets in the same text. Unlike the target-specific stance detection and multi-target stance detection where the target is usually a prominent figure or topic, in claimbased stance detection the target is a claim, which could be an article headline or a rumor’s post. Interestingly, despite substantial progress on stance detection, large-scale annotated datasets are limited. We compare our P-S TANCE dataset with some existing stance detection datasets in Table 2. W"
2021.findings-acl.208,D14-1083,0,0.0796845,"Missing"
2021.findings-acl.208,S16-1003,0,0.0688359,"Missing"
2021.findings-acl.208,D18-1136,0,0.0276816,"uts. BiCE is also a strong baseline for crosstarget stance detection. 5 CrossNet (Xu et al., 2018): CrossNet is another model for cross-target stance detection. It encodes the target and the tweet by using the same approach with BiCE and add an aspect attention layer to signal the core part of a stance-bearing input. CrossNet improves BiCE in many cross-target settings. 5.1 GCAE (Xue and Li, 2018): A CNN model that utilizes a gating mechanism to block targetunrelated information. GCAE is a strong baseline for aspect-based sentiment analysis and we apply it to our stance detection task. PGCNN (Huang and Carley, 2018): Similar to GCAE, PGCNN is based on gated convolutional networks and encodes target information by generating target-sensitive filters. BERT (Devlin et al., 2019): A pre-trained language model that predicts the stance by appending Results In this section, we present the set of experiments performed on various stance detection tasks on our dataset and show the results obtained by using the aforementioned baselines. Each result is the average of seven runs with different initializations. In-Target Stance Detection In-target stance detection is a stance detection task where a classifier is train"
2021.findings-acl.208,D14-1181,0,0.00459799,"Missing"
2021.findings-acl.208,L16-1623,0,0.244374,"fields of cross-domain stance detection such as cross-target stance detection where a classifier is adapted from a different but related target. We publicly release our dataset and code.1 1 Introduction Nowadays, people often express their stances toward specific targets (e.g., political events or figures, religion, or abortion) on social media. These opinions can provide valuable insights into important events, e.g., presidential election. The goal of the stance detection task is to determine whether the author of a piece of text is in favor of, against, or neutral toward a specific target (Mohammad et al., 2016b; Küçük and Can, 2020; ALDayel and Magdy, 2021). Twitter as a social platform has produced a large quantity of user-generated content, which has become a rich source for mining useful information about various topics such as presidential election. Political figures, who usually receive considerable attention and involve themselves in a large number of political events, are great targets to study stance detection. Therefore, detecting the 1 https://github.com/chuchun8/PStance stance expressed toward political figures on Twitter has drawn a lot of attention in the NLP community (Mohammad et al."
2021.findings-acl.208,2020.emnlp-demos.2,0,0.062135,"dataset (Mohammad et al., 2016a), which is a small dataset and thus may make the conclusions less convincing. In this paper, we show that our P-S TANCE dataset can be also used to evaluate the model performance of cross-target stance detection and provides opportunities for exploring more crosstarget tasks by interacting with previous SemEval2016 (Mohammad et al., 2016a) and Multi-Target stance datasets (Sobhani et al., 2017). In addition, P-S TANCE enables the exploration of largescale deep learning models including pre-trained language models, e.g., BERT (Devlin et al., 2019) and BERTweet (Nguyen et al., 2020). We fine-tune the BERT and BERTweet models on our dataset and compare them with other strong baselines. 3 Building the Dataset In this section, we detail the creation and the particularities of P-S TANCE, our large political stance detection dataset composed of 21,574 tweets collected during the 2020 U.S. presidential election. 3.1 Data Collection We collected tweets using the Twitter streaming API. Similar to prior works (Mohammad et al., 2016a; Sobhani et al., 2017) that target presidential candidates, we focus our attention on three political figures2 in the presidential race of 2020: “Don"
2021.findings-acl.208,D11-1147,0,0.112901,"Missing"
2021.findings-acl.208,P18-2123,0,0.10711,"stance detection by the training setting. Most previous works focused on the in-target stance detection where a classifier is trained and validated on the same target (Mohammad et al., 2016b; Zarrella and Marsh, 2016; Wei et al., 2016; Vijayaraghavan et al., 2016; Du et al., 2017; Sun et al., 2018; Wei et al., 2018; Li and Caragea, 2019, 2021b). However, sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets, which motivates the studies of cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020). Most previous studies evaluated the cross-target models on the SemEval-2016 dataset (Mohammad et al., 2016a), which is a small dataset and thus may make the conclusions less convincing. In this paper, we show that our P-S TANCE dataset can be also used to evaluate the model performance of cross-target stance detection and provides opportunities for exploring more crosstarget tasks by interacting with previous SemEval2016 (Mohammad et al., 2016a) and Multi-Target stance datasets (Sobhani et al., 2017). In addition, P-S TANCE enables the exploration of l"
2021.findings-acl.208,E17-2088,1,0.479906,"and Can, 2020; ALDayel and Magdy, 2021). Twitter as a social platform has produced a large quantity of user-generated content, which has become a rich source for mining useful information about various topics such as presidential election. Political figures, who usually receive considerable attention and involve themselves in a large number of political events, are great targets to study stance detection. Therefore, detecting the 1 https://github.com/chuchun8/PStance stance expressed toward political figures on Twitter has drawn a lot of attention in the NLP community (Mohammad et al., 2016a; Sobhani et al., 2017; Darwish et al., 2017). Even though stance detection has received a lot of attention, the annotated data are usually limited, which poses strong challenges to supervised models. Moreover, a limitation of existing datasets is that explicit mentions of targets and surface-level lexical cues that may expose the stance can be widely observed in the data (Mohammad et al., 2016a; Sobhani et al., 2017; Swami et al., 2018; Darwish et al., 2018; Conforti et al., 2020b; Lai et al., 2020), which means a model can detect the stance without extracting effective representations for the meanings of sentence"
2021.findings-acl.208,C18-1203,0,0.0118148,"cles Target-specific 3,291 Twitter Target-specific 21,574 Table 2: Comparison of English stance detection datasets. Different from classifying the stance detection tasks by target type (i.e., one specific target, multiple targets, or a claim), we can also categorize the stance detection as in-target and cross-target stance detection by the training setting. Most previous works focused on the in-target stance detection where a classifier is trained and validated on the same target (Mohammad et al., 2016b; Zarrella and Marsh, 2016; Wei et al., 2016; Vijayaraghavan et al., 2016; Du et al., 2017; Sun et al., 2018; Wei et al., 2018; Li and Caragea, 2019, 2021b). However, sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets, which motivates the studies of cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020). Most previous studies evaluated the cross-target models on the SemEval-2016 dataset (Mohammad et al., 2016a), which is a small dataset and thus may make the conclusions less convincing. In this paper, we show that our P-S TANCE dataset can b"
2021.findings-acl.208,P18-1234,0,0.0192522,"al., 2016): A BiLSTM that uses conditional encoding for stance detection. The target information is first encoded by a BiLSTM, whose hidden representations are then used to initialize another BiLSTM with tweets as inputs. BiCE is also a strong baseline for crosstarget stance detection. 5 CrossNet (Xu et al., 2018): CrossNet is another model for cross-target stance detection. It encodes the target and the tweet by using the same approach with BiCE and add an aspect attention layer to signal the core part of a stance-bearing input. CrossNet improves BiCE in many cross-target settings. 5.1 GCAE (Xue and Li, 2018): A CNN model that utilizes a gating mechanism to block targetunrelated information. GCAE is a strong baseline for aspect-based sentiment analysis and we apply it to our stance detection task. PGCNN (Huang and Carley, 2018): Similar to GCAE, PGCNN is based on gated convolutional networks and encodes target information by generating target-sensitive filters. BERT (Devlin et al., 2019): A pre-trained language model that predicts the stance by appending Results In this section, we present the set of experiments performed on various stance detection tasks on our dataset and show the results obtain"
2021.findings-acl.208,S16-1074,0,0.0251911,"r, Reddit Twitter Target-specific Claim-based Target-specific 3,545 8,574 51,284 News articles Target-specific 3,291 Twitter Target-specific 21,574 Table 2: Comparison of English stance detection datasets. Different from classifying the stance detection tasks by target type (i.e., one specific target, multiple targets, or a claim), we can also categorize the stance detection as in-target and cross-target stance detection by the training setting. Most previous works focused on the in-target stance detection where a classifier is trained and validated on the same target (Mohammad et al., 2016b; Zarrella and Marsh, 2016; Wei et al., 2016; Vijayaraghavan et al., 2016; Du et al., 2017; Sun et al., 2018; Wei et al., 2018; Li and Caragea, 2019, 2021b). However, sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets, which motivates the studies of cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020). Most previous studies evaluated the cross-target models on the SemEval-2016 dataset (Mohammad et al., 2016a), which is a small dataset and thus may make the co"
2021.findings-acl.208,2020.acl-main.291,0,0.0604491,"ting. Most previous works focused on the in-target stance detection where a classifier is trained and validated on the same target (Mohammad et al., 2016b; Zarrella and Marsh, 2016; Wei et al., 2016; Vijayaraghavan et al., 2016; Du et al., 2017; Sun et al., 2018; Wei et al., 2018; Li and Caragea, 2019, 2021b). However, sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets, which motivates the studies of cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020). Most previous studies evaluated the cross-target models on the SemEval-2016 dataset (Mohammad et al., 2016a), which is a small dataset and thus may make the conclusions less convincing. In this paper, we show that our P-S TANCE dataset can be also used to evaluate the model performance of cross-target stance detection and provides opportunities for exploring more crosstarget tasks by interacting with previous SemEval2016 (Mohammad et al., 2016a) and Multi-Target stance datasets (Sobhani et al., 2017). In addition, P-S TANCE enables the exploration of largescale deep learning models including"
2021.findings-acl.208,2020.lrec-1.171,0,0.0270879,"Missing"
2021.findings-acl.208,S16-1067,0,0.0204484,"d Target-specific 3,545 8,574 51,284 News articles Target-specific 3,291 Twitter Target-specific 21,574 Table 2: Comparison of English stance detection datasets. Different from classifying the stance detection tasks by target type (i.e., one specific target, multiple targets, or a claim), we can also categorize the stance detection as in-target and cross-target stance detection by the training setting. Most previous works focused on the in-target stance detection where a classifier is trained and validated on the same target (Mohammad et al., 2016b; Zarrella and Marsh, 2016; Wei et al., 2016; Vijayaraghavan et al., 2016; Du et al., 2017; Sun et al., 2018; Wei et al., 2018; Li and Caragea, 2019, 2021b). However, sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets, which motivates the studies of cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020). Most previous studies evaluated the cross-target models on the SemEval-2016 dataset (Mohammad et al., 2016a), which is a small dataset and thus may make the conclusions less convincing. In this paper, we sh"
2021.findings-acl.208,S16-1062,0,0.0208535,"pecific Claim-based Target-specific 3,545 8,574 51,284 News articles Target-specific 3,291 Twitter Target-specific 21,574 Table 2: Comparison of English stance detection datasets. Different from classifying the stance detection tasks by target type (i.e., one specific target, multiple targets, or a claim), we can also categorize the stance detection as in-target and cross-target stance detection by the training setting. Most previous works focused on the in-target stance detection where a classifier is trained and validated on the same target (Mohammad et al., 2016b; Zarrella and Marsh, 2016; Wei et al., 2016; Vijayaraghavan et al., 2016; Du et al., 2017; Sun et al., 2018; Wei et al., 2018; Li and Caragea, 2019, 2021b). However, sufficient annotated data are usually hard to obtain and conventional models on stance detection perform poorly on generalizing to the data of new targets, which motivates the studies of cross-target stance detection (Augenstein et al., 2016; Xu et al., 2018; Wei and Mao, 2019; Zhang et al., 2020). Most previous studies evaluated the cross-target models on the SemEval-2016 dataset (Mohammad et al., 2016a), which is a small dataset and thus may make the conclusions less con"
C18-1033,P10-1015,0,0.0293545,"ness, overspecialization, occurs when the system makes non-diverse recommendations (Alharthi et al., 2017). Hybrid RSs can combine CF and CB to tackle such issues and enhance the recommendation accuracy. A content-based RS is proposed that analyzes the texts of books to learn users’ reading interests. A survey on book recommender systems (Alharthi et al., 2017) describes only a few RSs that take the actual text of books into account. There is a lot of work that explores literary works, including automatic genre identification (Ardanuy and Sporleder, 2016) and learning the narrative structure (Elson et al., 2010). It is quite surprising, then, that the analysis of the textual content of books to improve their recommendations is still quite limited. A user’s reading preferences might be influenced by many elements specific to books. For example, recommendations given by the readers’ advisory (a library service that suggests books to patrons) rely on multiple appeal factors. The factors are characterization, frame, language and writing style, pacing, special topics, storyline, and tone. To obtain information about books’ appeal factors, librarians can subscribe (at a fee) to reader-advisory databases su"
C18-1033,D14-1181,0,0.0151007,"etwork adopted in (Macke and Hirshman, 2015) showed high accuracy only when the number of classes was small: 10 authors. Our preliminary experiments show that RNN-based author identification models have poor accuracy, so we do not use such models in this work. 3 Methodology This section describes the two components of the system. It first illustrates and explains the author identification system, which was proposed by (Solorio et al., 2017), and our modifications to the system. Next, the recommendation procedure is described. 3.1 Author Identification As shown in Figure 1, drawn similarly to (Kim, 2014), the neural network has the following layers. Embedding layer (also called lookup table). It takes a sequence of words or character bigrams as input, and maps each word or bigram to an embedding of size k. The embedding of the ith word/character is a dense k-dimensional vector xi ∈ Rk . Each book is represented as a matrix, with one word/character embedding per row. A book has sequence length n (n is the number of words/characters) where a sequence x1:n = x1 ⊕ x2 ... ⊕ xn is a concatenation of all words from x1 to xn . Convolution layer. This layer consists of one or more filters (also called"
C18-1033,E14-3011,0,0.0271692,"Missing"
C18-1033,E17-2106,0,0.0607163,"Missing"
D09-1129,J99-1003,0,0.00843564,"robable candidates along with the procedure to sort the candidates. 3.1 Similarity between Two Strings We use the longest common subsequence (LCS) (Allison and Dix, 1986) measure with some normalization and small modifications for our string similarity measure. We use the same three different modified versions of LCS that we (Islam and Inkpen, 2008) used, along with another modified version of LCS, and then take a weighted sum of these2 . Kondrak (2005) showed that edit distance and the length of the longest common subsequence are special cases of n-gram distance and similarity, respectively. Melamed (1999) normalized LCS by dividing the length of the longest common subsequence by the length of the longer string and called it longest common subsequence ratio (LCSR). But LCSR does not take into account the length of the shorter string which sometimes has a significant impact on the similarity score. Islam and Inkpen (2008) normalized the longest common subsequence so that it takes into account the length of both the shorter and the longer string and called it normalized longest common subse2 We (Islam and Inkpen, 2008) use modified versions because in our experiments we obtained better results (p"
D19-6208,W17-1612,0,0.120559,"r) as multiple inputs to enhance the model predictability. 2 Ethical Considerations It is of greater importance to follow strict guidelines on ethical research conduct when the research data resembles vulnerable users who could be compromised. The researchers working with data that could be used to single out individuals must take adequate precautions to avoid further psychological distress. During our research, we have given thorough considerations to these ethical facets and have adopted strict guidelines to ensure the anonymity and privacy of the data. Similar to the guidelines proposed by Benton et al. (2017a), we have exercised strict hardware and software security measures. Our research does not involve any intervention and has focused mainly on the applicability of machine learning models in determining users susceptible to mental disorders. 3 Related work As social media has become an integral part of ones’ day-to-day-life, it will be insightful to identify to what extent an individual has disclosed her/his personal information and whether accurate and sufficient information is being published to determine whether or not a person has a mental disorder. Considering the Twitter platform, rather"
D19-6208,Q17-1010,0,0.0199634,"e notable features that were identified over the years by researchers on detecting mental illnesses. Even though our best results were obtained by instantiating the embedding layer weights with random numbers (refer Table 4), we conducted several preliminary experiments using word embeddings trained on the fastText (Joulin et al., 2017) algorithm. We decided to use fastText because given the unstructured nature of the twitter messages we could obtain a more meaningful representation by expressing a word as a vector constructed out of the sum of several vectors for different parts of the word (Bojanowski et al., 2017). One of the reasons for the low measurements could be due to the reason that we used fewer data to train our embeddings. from one another (Caruana, 1997) can be considered as one of the most appropriate architectures when trying to detect multiple mental illnesses. Benton et al. (2017b) demonstrated the successful use of multi-task learning to recognize mental illnesses and suicide ideation. Different from their approach, we add multiple features discovered by researchers in the fields of computational linguistics and psychology, to enhance the model performances. We consider that it is impor"
D19-6208,C18-1126,0,0.0249682,"Missing"
D19-6208,W14-3207,0,0.0228001,"are commonly used with sequence data. Text extracted from social media platforms such as Twitter, Facebook, Reddit, and other similar forums has been successfully used in various natural language processing (NLP) tasks to identify users with different mental disorders and suicide ideation. Social media text was used to classify users with insomnia and distress (Jamison-Powell et al., 2012; Lehrman et al., 2012), postpartum depression (De Choudhury et al., 2013a,b, 2014), depression (Resnik et al., 2015a, 2013, 2015b; Schwartz et al., 2014; Tsugawa et al., 2015), PostTraumatic Stress Disorder (Coppersmith et al., 2014a,b), schizophrenia (Loveys et al., 2017) and many other mental illnesses such as Attention Deficit Hyperactivity Disorder (ADHD), Generalized Anxiety Disorder, Bipolar Disorder, Eating Disorders and obsessive-compulsive disorder (OCD) (Coppersmith et al., 2015a). 4.1 Data Emotion Classification We use the data from the 8th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA-2017). The data was used in the shared task to identify emotion intensity (Mohammad and Bravo-Marquez, 2017). The tweets in the dataset were assigned with the labels: anger, fear,"
D19-6208,W15-1201,0,0.100584,"sorders and suicide ideation. Social media text was used to classify users with insomnia and distress (Jamison-Powell et al., 2012; Lehrman et al., 2012), postpartum depression (De Choudhury et al., 2013a,b, 2014), depression (Resnik et al., 2015a, 2013, 2015b; Schwartz et al., 2014; Tsugawa et al., 2015), PostTraumatic Stress Disorder (Coppersmith et al., 2014a,b), schizophrenia (Loveys et al., 2017) and many other mental illnesses such as Attention Deficit Hyperactivity Disorder (ADHD), Generalized Anxiety Disorder, Bipolar Disorder, Eating Disorders and obsessive-compulsive disorder (OCD) (Coppersmith et al., 2015a). 4.1 Data Emotion Classification We use the data from the 8th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA-2017). The data was used in the shared task to identify emotion intensity (Mohammad and Bravo-Marquez, 2017). The tweets in the dataset were assigned with the labels: anger, fear, joy and sadness, and their associated intensities. Table 1 presents the detailed statistics of the dataset. Emotion Anger Fear Joy Sadness Total With the advancements in neural network-based algorithms, more research has been conducted successfully in detectin"
D19-6208,W15-1204,0,0.330474,"Missing"
D19-6208,W17-3110,0,0.0234233,"tracted from social media platforms such as Twitter, Facebook, Reddit, and other similar forums has been successfully used in various natural language processing (NLP) tasks to identify users with different mental disorders and suicide ideation. Social media text was used to classify users with insomnia and distress (Jamison-Powell et al., 2012; Lehrman et al., 2012), postpartum depression (De Choudhury et al., 2013a,b, 2014), depression (Resnik et al., 2015a, 2013, 2015b; Schwartz et al., 2014; Tsugawa et al., 2015), PostTraumatic Stress Disorder (Coppersmith et al., 2014a,b), schizophrenia (Loveys et al., 2017) and many other mental illnesses such as Attention Deficit Hyperactivity Disorder (ADHD), Generalized Anxiety Disorder, Bipolar Disorder, Eating Disorders and obsessive-compulsive disorder (OCD) (Coppersmith et al., 2015a). 4.1 Data Emotion Classification We use the data from the 8th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA-2017). The data was used in the shared task to identify emotion intensity (Mohammad and Bravo-Marquez, 2017). The tweets in the dataset were assigned with the labels: anger, fear, joy and sadness, and their associated in"
D19-6208,W18-0609,1,0.83913,"he shared task to identify emotion intensity (Mohammad and Bravo-Marquez, 2017). The tweets in the dataset were assigned with the labels: anger, fear, joy and sadness, and their associated intensities. Table 1 presents the detailed statistics of the dataset. Emotion Anger Fear Joy Sadness Total With the advancements in neural network-based algorithms, more research has been conducted successfully in detecting mental disorders, despite the limited amount of data. Kshirsagar et al. (2017) have used recurrent neural networks with attention to detect social media posts resembling crisis. Husseini Orabi et al. (2018) demonstrated that using convolution neural network-based architectures produces better results compared to recurrent neural network-based architectures when detecting users susceptible to depression. Even though our experiments are to categorize users into three classes (i.e., control, depression, PTSD), the proposed multi-channel architecture have produced comparable results to the ones presented by Husseini Orabi et al. (2018) using binary classification to distinguish users susceptible to depression. Train 857 1147 823 786 3613 Test 760 995 714 673 3142 Dev 84 110 79 74 347 Total 1701 2252"
D19-6208,W15-1203,0,0.128869,"er platform, rather than just sharing depressed feelings, users 55 4 are more likely to self-disclose to the extent where they reveal detailed information about their treatment history (Park et al., 2013). The same level of self-disclosure can be identified in the Reddit forums (Balani and De Choudhury, 2015) and specifically by users with anonymous accounts (Pavalanathan and De Choudhury, 2015). Also, it was identified that personality traits and metafeatures such as age and gender could have a positive impact on the model performances when detecting users susceptible to PTSD and depression (Preot et al., 2015). Similarly, we have also identified that age and gender as multiple inputs have positively impacted model predictability when used with multi-task learning. Proposed solution The proposed solution consists of two key components. The first identifies the type of emotion expressed by each user using the model trained on the WASSA 2017 shared task dataset. The identified emotion categories are used as multiple inputs within the multi-task learning environment. The second component is the model that predicts users susceptible to PTSD or depression. When structuring the two neural network models ("
D19-6208,E17-2068,0,0.0177451,"ks can benefit https://keras.io/preprocessing/text/ 57 the mental illness detection task. Throughout our research, we did not emphasize much on word embeddings as our primary objective was to identify the impact of merging features derived using deep learning methods with few of the notable features that were identified over the years by researchers on detecting mental illnesses. Even though our best results were obtained by instantiating the embedding layer weights with random numbers (refer Table 4), we conducted several preliminary experiments using word embeddings trained on the fastText (Joulin et al., 2017) algorithm. We decided to use fastText because given the unstructured nature of the twitter messages we could obtain a more meaningful representation by expressing a word as a vector constructed out of the sum of several vectors for different parts of the word (Bojanowski et al., 2017). One of the reasons for the low measurements could be due to the reason that we used fewer data to train our embeddings. from one another (Caruana, 1997) can be considered as one of the most appropriate architectures when trying to detect multiple mental illnesses. Benton et al. (2017b) demonstrated the successf"
D19-6208,W15-1205,0,0.039264,"Missing"
D19-6208,W17-3108,0,0.0233086,"the 8th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA-2017). The data was used in the shared task to identify emotion intensity (Mohammad and Bravo-Marquez, 2017). The tweets in the dataset were assigned with the labels: anger, fear, joy and sadness, and their associated intensities. Table 1 presents the detailed statistics of the dataset. Emotion Anger Fear Joy Sadness Total With the advancements in neural network-based algorithms, more research has been conducted successfully in detecting mental disorders, despite the limited amount of data. Kshirsagar et al. (2017) have used recurrent neural networks with attention to detect social media posts resembling crisis. Husseini Orabi et al. (2018) demonstrated that using convolution neural network-based architectures produces better results compared to recurrent neural network-based architectures when detecting users susceptible to depression. Even though our experiments are to categorize users into three classes (i.e., control, depression, PTSD), the proposed multi-channel architecture have produced comparable results to the ones presented by Husseini Orabi et al. (2018) using binary classification to disting"
D19-6208,W15-1207,0,0.0294212,"Missing"
D19-6208,W12-2102,0,0.119422,"Missing"
D19-6208,W15-1212,0,0.0153264,"validation accuracies compared to the accuracies produced by Recurrent Neural Network (RNN) based models, which are commonly used with sequence data. Text extracted from social media platforms such as Twitter, Facebook, Reddit, and other similar forums has been successfully used in various natural language processing (NLP) tasks to identify users with different mental disorders and suicide ideation. Social media text was used to classify users with insomnia and distress (Jamison-Powell et al., 2012; Lehrman et al., 2012), postpartum depression (De Choudhury et al., 2013a,b, 2014), depression (Resnik et al., 2015a, 2013, 2015b; Schwartz et al., 2014; Tsugawa et al., 2015), PostTraumatic Stress Disorder (Coppersmith et al., 2014a,b), schizophrenia (Loveys et al., 2017) and many other mental illnesses such as Attention Deficit Hyperactivity Disorder (ADHD), Generalized Anxiety Disorder, Bipolar Disorder, Eating Disorders and obsessive-compulsive disorder (OCD) (Coppersmith et al., 2015a). 4.1 Data Emotion Classification We use the data from the 8th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA-2017). The data was used in the shared task to identify emotio"
D19-6208,D13-1133,0,0.0788866,"Missing"
D19-6208,W14-3214,0,0.0148385,"he accuracies produced by Recurrent Neural Network (RNN) based models, which are commonly used with sequence data. Text extracted from social media platforms such as Twitter, Facebook, Reddit, and other similar forums has been successfully used in various natural language processing (NLP) tasks to identify users with different mental disorders and suicide ideation. Social media text was used to classify users with insomnia and distress (Jamison-Powell et al., 2012; Lehrman et al., 2012), postpartum depression (De Choudhury et al., 2013a,b, 2014), depression (Resnik et al., 2015a, 2013, 2015b; Schwartz et al., 2014; Tsugawa et al., 2015), PostTraumatic Stress Disorder (Coppersmith et al., 2014a,b), schizophrenia (Loveys et al., 2017) and many other mental illnesses such as Attention Deficit Hyperactivity Disorder (ADHD), Generalized Anxiety Disorder, Bipolar Disorder, Eating Disorders and obsessive-compulsive disorder (OCD) (Coppersmith et al., 2015a). 4.1 Data Emotion Classification We use the data from the 8th Workshop on Computational Approaches to Subjectivity, Sentiment & Social Media Analysis (WASSA-2017). The data was used in the shared task to identify emotion intensity (Mohammad and Bravo-Marqu"
E17-2088,W11-1701,0,0.0450617,"Macro-averaged F-scores of different models on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most o"
E17-2088,I13-1191,0,0.0226907,"Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing datasets for stance detection were created from o"
E17-2088,S14-2145,0,0.0368761,"Missing"
E17-2088,W14-4012,0,0.0207149,"Missing"
E17-2088,D14-1179,0,0.0080364,"Missing"
E17-2088,D14-1080,0,0.0238292,"As RNN unit, we used a Gated Recurrent Unit (Cho et al., 2014a) with 128 cells. The word vectors at the embedding layer have 100 554 Classifier Baselines i. random ii. majority One Classifier per Target a. Independent SVMs b. Window-based SVMs c. Cascading SVMs Single Model A. 9-Class SVM B. Seq2Seq F-macro positions towards them. Deep Recurrent Neural Models Different structures of deep RNNs have recently shown to be very effective in a wide range of sequence modeling problems, particularly for opinion mining and sentiment analysis (Zhu et al., 2015a; Socher et al., 2013; Zhu et al., 2015b; Irsoy and Cardie, 2014; Zhu et al., 2016). These neural models were extended for tasks with variable input and output sequence length including: end-toend neural machine translation (Sutskever et al., 2014; Cho et al., 2014b), image-to-text conversion (Vinyals et al., 2015b), syntactic constituency parsing (Vinyals et al., 2015a) and question answering (Hermann et al., 2015). Subsequently, the attention mechanism allowed the models to learn alignments between different parts of the source and the target such as between speech frames and the text in speech recognition (Chorowski et al., 2014) or between image frames"
E17-2088,D15-1166,0,0.00920779,"Missing"
E17-2088,D15-1018,0,0.0122887,"na Inkpen1 and Xiaodan Zhu2 1 EECS, University of Ottawa National Research Council Canada {psobh090,diana.inkpen}@uottawa.ca {xiaodan.zhu}@nrc-cnrc.gc.ca 2 Abstract sponding to related targets. Then, we investigate the problem of jointly predicting the stance expressed towards multiple targets (two at a time), in order to demonstrate the utility of the dataset. The closest work related to our work is Deng and Wiebe (2015a), where sentiment toward different entities and events is jointly modeled using a rule-based probabilistic soft logic approach. The authors also made their dataset MPQA 3.0 (Deng and Wiebe, 2015b) available, However, this dataset is relatively small (it contains 70 documents) and has a potentially infinite number of targets (the target sets depend on the context), which makes it hard to train a system. Instead, we provide a reasonably large dataset for training and evaluation. Our dataset contains 4,455 tweets manually annotated for stance towards more than one target simultaneously. We will refer to this data as the Multi-Target Stance Dataset. Moreover, we make available a much larger unlabeled dataset providing more choices for users to further investigate the multi-target stance"
E17-2088,N15-1146,0,0.0171662,"na Inkpen1 and Xiaodan Zhu2 1 EECS, University of Ottawa National Research Council Canada {psobh090,diana.inkpen}@uottawa.ca {xiaodan.zhu}@nrc-cnrc.gc.ca 2 Abstract sponding to related targets. Then, we investigate the problem of jointly predicting the stance expressed towards multiple targets (two at a time), in order to demonstrate the utility of the dataset. The closest work related to our work is Deng and Wiebe (2015a), where sentiment toward different entities and events is jointly modeled using a rule-based probabilistic soft logic approach. The authors also made their dataset MPQA 3.0 (Deng and Wiebe, 2015b) available, However, this dataset is relatively small (it contains 70 documents) and has a potentially infinite number of targets (the target sets depend on the context), which makes it hard to train a system. Instead, we provide a reasonably large dataset for training and evaluation. Our dataset contains 4,455 tweets manually annotated for stance towards more than one target simultaneously. We will refer to this data as the Multi-Target Stance Dataset. Moreover, we make available a much larger unlabeled dataset providing more choices for users to further investigate the multi-target stance"
E17-2088,S16-1003,1,0.596818,"Missing"
E17-2088,P11-2008,0,0.0247663,"Missing"
E17-2088,N12-1072,0,0.0322812,"s on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing datasets for stance detectio"
E17-2088,W15-0509,1,0.56123,"res of different models on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing dataset"
E17-2088,walker-etal-2012-annotated,0,0.011822,"s on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing datasets for stance detectio"
E17-2088,S16-2021,1,0.614302,"er deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing datasets for stance detection were created from online debate forums lik"
E17-2088,D13-1170,0,0.00153005,"learning rate (Adadelta (Zeiler, 2012)). As RNN unit, we used a Gated Recurrent Unit (Cho et al., 2014a) with 128 cells. The word vectors at the embedding layer have 100 554 Classifier Baselines i. random ii. majority One Classifier per Target a. Independent SVMs b. Window-based SVMs c. Cascading SVMs Single Model A. 9-Class SVM B. Seq2Seq F-macro positions towards them. Deep Recurrent Neural Models Different structures of deep RNNs have recently shown to be very effective in a wide range of sequence modeling problems, particularly for opinion mining and sentiment analysis (Zhu et al., 2015a; Socher et al., 2013; Zhu et al., 2015b; Irsoy and Cardie, 2014; Zhu et al., 2016). These neural models were extended for tasks with variable input and output sequence length including: end-toend neural machine translation (Sutskever et al., 2014; Cho et al., 2014b), image-to-text conversion (Vinyals et al., 2015b), syntactic constituency parsing (Vinyals et al., 2015a) and question answering (Hermann et al., 2015). Subsequently, the attention mechanism allowed the models to learn alignments between different parts of the source and the target such as between speech frames and the text in speech recognition (Chor"
E17-2088,S15-1001,1,0.782275,"ithm with adaptive learning rate (Adadelta (Zeiler, 2012)). As RNN unit, we used a Gated Recurrent Unit (Cho et al., 2014a) with 128 cells. The word vectors at the embedding layer have 100 554 Classifier Baselines i. random ii. majority One Classifier per Target a. Independent SVMs b. Window-based SVMs c. Cascading SVMs Single Model A. 9-Class SVM B. Seq2Seq F-macro positions towards them. Deep Recurrent Neural Models Different structures of deep RNNs have recently shown to be very effective in a wide range of sequence modeling problems, particularly for opinion mining and sentiment analysis (Zhu et al., 2015a; Socher et al., 2013; Zhu et al., 2015b; Irsoy and Cardie, 2014; Zhu et al., 2016). These neural models were extended for tasks with variable input and output sequence length including: end-toend neural machine translation (Sutskever et al., 2014; Cho et al., 2014b), image-to-text conversion (Vinyals et al., 2015b), syntactic constituency parsing (Vinyals et al., 2015a) and question answering (Hermann et al., 2015). Subsequently, the attention mechanism allowed the models to learn alignments between different parts of the source and the target such as between speech frames and the text in sp"
E17-2088,P09-1026,0,0.0147156,"32 52.05 50.63 54.81 Table 5: Macro-averaged F-scores of different models on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional"
E17-2088,W10-0214,0,0.0322195,"s treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models such as RNNs and convolutional neural nets. Most of the existing datasets for stance detection were created from online debate forums like 4forums.com and createdebates.com (Somasundaran and Wiebe, 2010; Walker et al., 2012b; Hasan and Ng, 2013). The majority of these debates are two-sided, and the data labels are often provided by the authors of the posts. Recently, Mohammad et al. (2016b) created a dataset of tweets labeled for both stance and sentiment. None of the prior work has created a dataset annotated for more than one target simultaneously, neither has explored the dependencies and relationships between targets when predicting overall 6 Conclusions and Future Work We presented the first multi-target stance dataset of a reasonable size from social media, to help further exploration"
E17-2088,N16-1106,1,0.62421,"Gated Recurrent Unit (Cho et al., 2014a) with 128 cells. The word vectors at the embedding layer have 100 554 Classifier Baselines i. random ii. majority One Classifier per Target a. Independent SVMs b. Window-based SVMs c. Cascading SVMs Single Model A. 9-Class SVM B. Seq2Seq F-macro positions towards them. Deep Recurrent Neural Models Different structures of deep RNNs have recently shown to be very effective in a wide range of sequence modeling problems, particularly for opinion mining and sentiment analysis (Zhu et al., 2015a; Socher et al., 2013; Zhu et al., 2015b; Irsoy and Cardie, 2014; Zhu et al., 2016). These neural models were extended for tasks with variable input and output sequence length including: end-toend neural machine translation (Sutskever et al., 2014; Cho et al., 2014b), image-to-text conversion (Vinyals et al., 2015b), syntactic constituency parsing (Vinyals et al., 2015a) and question answering (Hermann et al., 2015). Subsequently, the attention mechanism allowed the models to learn alignments between different parts of the source and the target such as between speech frames and the text in speech recognition (Chorowski et al., 2014) or between image frames and the agent’s ac"
E17-2088,W06-1639,0,0.0184189,"34.26 32.11 51.37 48.32 52.05 50.63 54.81 Table 5: Macro-averaged F-scores of different models on the Multi-Target Stance dataset decoder deep neural model on our dataset. This model has both the advantages of windows-based and cascading classification, and it has the best performance compared to all other models and baselines. By applying paired t-test on these results, we concluded that the differences between sequence-to-sequence model and all other models are statistically significant. 5 Related Work Stance Detection Over the last decade, there has been active research in modeling stance (Thomas et al., 2006; Somasundaran and Wiebe, 2009; Anand et al., 2011; Sobhani et al., 2015; Walker et al., 2012a; Hasan and Ng, 2013; Sobhani et al., 2016). However, all of these previous works treat each target independently, ignoring the potential dependencies that could exist among related targets. Stance detection was one of the tasks in the SemEval-2016 shared task competition (Mohammad et al., 2016a). Out of 19 participant teams, most used standard text classification features such as n-grams and word embedding vectors, as well as standard sentiment analysis features, while others used deep neural models"
E17-2088,L16-1623,1,\N,Missing
H05-1007,O97-1002,0,0.0110908,"hree types of semantic similarity measures: dictionary-based (lexical taxonomy structure), corpus-based, and hybrid. Most of the dictionary-based measures use path length in WordNet – for example (Leacock and Chodorow, 1998), (Hirst and St-Onge, 1998). The corpusbased measures use some form of vector similarity. The cosine measure uses frequency counts in its vectors and cosine to compute similarity; the simpler methods use binary vectors and compute coefficients such as: Matching, Dice, Jaccard, and Overlap. Examples of hybrid measures, based on WordNet and small corpora, are: Resnik (1995), Jiang and Conrath (1997), Lin (1998). All dictionary-based measures have the disadvantage of limited coverage: they cannot deal with many proper names and new words that are not in the dictionary. For WordNet-based approaches, there is the additional issue that they tend to work well only for nouns because the noun hierarchy in WordNet is the most developed. Also, most of the WordNetbased measures do not work for words with different part-of-speech, with small exceptions such as the extended Lesk measure (Banerjee and Pedersen, 2003). We did a pre-screening of the various semantic similarity measures in order to choo"
I13-1077,baccianella-etal-2010-sentiwordnet,0,0.0106765,"Missing"
I13-1077,N09-2061,0,0.0275424,"Missing"
I13-1077,P09-1026,0,0.0459836,"Missing"
I13-1077,J11-2001,0,0.0271791,"Missing"
I13-1077,H05-1044,0,\N,Missing
I13-1077,R11-1019,1,\N,Missing
islam-inkpen-2006-second,J90-1003,0,\N,Missing
islam-inkpen-2006-second,C04-1146,0,\N,Missing
islam-inkpen-2006-second,C92-2070,0,\N,Missing
islam-inkpen-2006-second,H05-1007,1,\N,Missing
islam-inkpen-2006-second,J92-4003,0,\N,Missing
islam-inkpen-2006-second,J06-1003,0,\N,Missing
islam-inkpen-2006-second,N03-1020,0,\N,Missing
islam-inkpen-2006-second,P98-2127,0,\N,Missing
islam-inkpen-2006-second,C98-2122,0,\N,Missing
islam-inkpen-2006-second,P98-2124,0,\N,Missing
islam-inkpen-2006-second,C98-2119,0,\N,Missing
islam-inkpen-2006-second,P99-1004,0,\N,Missing
J06-2003,C96-1013,0,0.0603358,"Missing"
J06-2003,P99-1016,0,0.0167551,"Missing"
J06-2003,A00-2018,0,0.0424908,"different because of translation divergences between languages (Dorr 1993). For the sentences in our test data, a quick manual inspection shows that this happens very rarely or not at all. This simplification eliminates the need to parse the French sentence and the need to build a tool to extract its semantics. As depicted in Figure 20, the interlingual representation is produced with a preexisting input construction tool that was previously used by Langkilde-Geary (2002a) in her HALogen evaluation experiments. In order to use this tool, we parsed the English sentences with Charniak’s parser (Charniak 2000).22 The tool was designed to work on parse trees from the Penn TreeBank, which have some extra annotations; it worked on parse trees produced by Charniak’s parser, but it failed on some parse trees probably more often than it 21 The sentences were obtained from USC/ISI (http://www.isi.edu/natural-language/download/hansard/) (approximately one million pairs of sentences). Other sources of parallel text, such as parallel translations of the Bible (http://benjamin.umd.edu/parallel/) (Resnik 1999) and a collection of Web pages (Resnik, Olsen, and Diab 1999), happened to contain very few occurrence"
J06-2003,P96-1025,0,0.0190393,"Missing"
J06-2003,W99-0613,0,0.056278,"extraction phase, for each sentence (or fragment of a sentence) in CTRW the program will decide which leaf class is expressed, with what strength, and what frequency. We use a decision-list algorithm to learn sets of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and ATTITUDE - STYLE DISTINCTIONS . These are split further for each leaf class, as explained in Section 2.3. The algorithm we implemented is inspired by the work of Yarowsky (1995) on word sense disambiguation. He classified the senses of a word on the basis of other words that the given word co-occurs with. Collins and Singer (1999) classified proper names 1 We are grateful to HarperCollins Publishers, Inc. for permission to use CTRW in this project. 228 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Figure 4 Example of distinctions extracted from CTRW. as P ERSON, O RGANIZATION, or L OCATION using contextual rules (that rely on other words appearing in the context of the proper names) and spelling rules (that rely on words in the proper names). Starting with a few spelling rules (using some proper-name features) in the decision list, their algorithm learns new contextual rules; using these rules,"
J06-2003,J02-2001,1,0.51899,"orce, floridity, and familiarity (Hovy 1990). Only the first three of these occur in CTRW. A sentence in CTRW expressing stylistic distinctions is this: Assistant and helper are nearly identical except for the latter’s greater informality. Words that signal the degree of formality include formal, informal, formality, and slang. The degree of concreteness is signaled by words such as abstract, concrete, and concretely. Force can be signaled by words such as emphatic and intensification. 1.1.1 The Class Hierarchy of Distinctions. Following the analysis of the distinctions among near-synonyms of Edmonds and Hirst (2002), we derived the class hierarchy of 225 Computational Linguistics Volume 32, Number 2 distinctions presented in Figure 2. The top-level class DISTINCTIONS consists of DENO TATIONAL DISTINCTIONS , ATTITUDE , and STYLE . The last two are grouped together in a class ATTITUDE - STYLE DISTINCTIONS because they are expressed by similar syntactic constructions in the text of CTRW. Therefore the algorithm to be described in Section 2.2 will treat them together. The leaf classes of DENOTATIONAL DISTINCTIONS are S UGGESTION, I MPLICATION, and D ENOTATION; those of ATTITUDE are FAVORABLE, N EUTRAL, and P"
J06-2003,C92-2082,0,0.0306758,"lexical knowledge base from a machine-readable dictionary (MRD) because the information it contains may be incomplete, or it may contain circularities. It is possible to combine information from multiple MRDs or to enhance an existing LKB, they say, although human supervision may be needed. Automatically extracting world knowledge from MRDs was attempted by projects such as MindNet at Microsoft Research (Richardson, Dolan, and Vanderwende 1998), and Barri`erre and Popowich’s (1996) project, which learns from children’s dictionaries. IS-A hierarchies have been learned automatically from MRDs (Hearst 1992) and from corpora (Caraballo [1999] among others). 14 http://www.cl.cam.ac.uk/Research/NL/acquilex/acqhome.html 240 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Research on merging information from various lexical resources is related to the present work in the sense that the consistency issues to be resolved are similar. One example is the construction of Unified Medical Language System (UMLS)15 (Lindberg, Humphreys, and McCray 1993), in the medical domain. UMLS takes a wide range of lexical and ontological resources and brings them together as a single resource. Most"
J06-2003,W02-0909,1,0.41021,"nd familiarity (Hovy 1990). Only the first three of these occur in CTRW. A sentence in CTRW expressing stylistic distinctions is this: Assistant and helper are nearly identical except for the latter’s greater informality. Words that signal the degree of formality include formal, informal, formality, and slang. The degree of concreteness is signaled by words such as abstract, concrete, and concretely. Force can be signaled by words such as emphatic and intensification. 1.1.1 The Class Hierarchy of Distinctions. Following the analysis of the distinctions among near-synonyms of Edmonds and Hirst (2002), we derived the class hierarchy of 225 Computational Linguistics Volume 32, Number 2 distinctions presented in Figure 2. The top-level class DISTINCTIONS consists of DENO TATIONAL DISTINCTIONS , ATTITUDE , and STYLE . The last two are grouped together in a class ATTITUDE - STYLE DISTINCTIONS because they are expressed by similar syntactic constructions in the text of CTRW. Therefore the algorithm to be described in Section 2.2 will treat them together. The leaf classes of DENOTATIONAL DISTINCTIONS are S UGGESTION, I MPLICATION, and D ENOTATION; those of ATTITUDE are FAVORABLE, N EUTRAL, and P"
J06-2003,N01-1001,0,0.0194005,"Missing"
J06-2003,A00-2023,0,0.0574166,"ts our LKB of NS. To implement Xenon, 242 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Figure 9 Lexical analysis and choice in machine translation; adapted from Edmonds and Hirst (2002). The solid lines show the flow of data: input, intermediate representations, and output; the dashed lines show the flow of knowledge from the knowledge sources to the analysis and the generation module. The rectangles denote the main processing modules; the rest of the boxes denote data or knowledge sources. we modified the lexical-choice component of a preexisting NLG system, HALogen (Langkilde 2000; Langkilde and Knight 1998), to handle knowledge about the nearsynonym differences. (Xenon will be described in detail in Section 7.) This required customization of the LKB to the Sensus ontology (Knight and Luk 1994) that HALogen uses as its representation. Customization of the core denotations for Xenon was straightforward. The core denotation of a cluster is a metaconcept representing the disjunction of all the Sensus concepts that could correspond to the near-synonyms in a cluster. The names of metaconcepts, which must be distinct, are formed by the prefix generic, followed by the name of"
J06-2003,W98-1426,0,0.176952,"nly one (or several) of the synonyms of the other word. For example, emotional baggage is a good collocation because baggage and luggage are in the same synset and ∗emotional luggage is not a collocation. Unlike Pearce, we use a combination of t-test and MI, not just frequency counts, to classify collocations. There are two typical approaches to collocations in previous NLG systems: the use of phrasal templates in the form of canned phrases, and the use of automatically extracted collocations for unification-based generation (McKeown and Radev 2000). Statistical NLG systems (such as Nitrogen [Langkilde and Knight 1998]) make good use of the most frequent words and their collocations, but such systems cannot choose a less-frequent synonym that may be more appropriate for conveying desired nuances of meaning if the synonym is not a frequent word. Turney (2001) used mutual information to choose the best answer to questions about near-synonyms in the Test of English as a Foreign Language (TOEFL) and English as a Second Language (ESL). Given a problem word (with or without context) and four alternative words, the question is to choose the alternative most similar in meaning to the problem word (the problem here"
J06-2003,W02-2103,0,0.0403331,"is a semantic representation and a set of preferences to be satisfied. The final output is a set of sentences and their scores. A concrete example of input and output is shown in Figure 13. Note that HALogen may generate some ungrammatical constructs, but they are (usually) assigned lower scores. The first sentence (the highest ranked) is considered to be the solution. 7.1 Metaconcepts The semantic representation input to Xenon is represented, like the input to HALogen, in an interlingua developed at University of Southern California/Information Sciences Institute (USC/ISI).18 As described by Langkilde-Geary (2002b), this language contains a specified set of 40 roles, whose fillers can be either words, concepts from Sensus (Knight and Luk 1994), or complex interlingual representations. The interlingual representations may be underspecified: If some information needed by HALogen is not present, it will use its corpus-derived statistical information to 16 We found that sometimes a rule would extract only a fragment of the expected configuration of concepts but still provided useful knowledge; however, such cases were not considered to be correct in this evaluation, which did not allow credit for partial"
J06-2003,2001.mtsummit-papers.68,0,0.0499307,"Missing"
J06-2003,J03-2001,0,0.00904644,"y Apresjan (2000). An entry includes the following types of differences: semantic, evaluative, associative and connotational, and differences in emphasis or logical stress. These differences are similar to the ones used in our work. Gao (2001) studied the distinctions between near-synonym verbs, more specifically Chinese physical action verbs such as verbs of cutting, putting, throwing, touching, and lying. Her dissertation presents an analysis of the types of semantic distinctions relevant to these verbs, and how they can be arranged into hierarchies on the basis of their semantic closeness. Ploux and Ji (2003) investigated the question of which words should be considered near-synonyms, without interest in their nuances of meaning. They merged clusters of near-synonyms from several dictionaries in English and French and represented them in a geometric space. In our work, the words that are considered near-synonyms are taken from CTRW; a different dictionary of synonyms may present slightly different views. For example, a cluster may contain some extra words, some missing words, or sometimes the clustering could be done in a different way. A different approach is to automatically acquire near-synonym"
J06-2003,P99-1068,0,0.0118209,"ion experiments. In order to use this tool, we parsed the English sentences with Charniak’s parser (Charniak 2000).22 The tool was designed to work on parse trees from the Penn TreeBank, which have some extra annotations; it worked on parse trees produced by Charniak’s parser, but it failed on some parse trees probably more often than it 21 The sentences were obtained from USC/ISI (http://www.isi.edu/natural-language/download/hansard/) (approximately one million pairs of sentences). Other sources of parallel text, such as parallel translations of the Bible (http://benjamin.umd.edu/parallel/) (Resnik 1999) and a collection of Web pages (Resnik, Olsen, and Diab 1999), happened to contain very few occurrences of the near-synonyms of interest. 22 ftp://ftp.cs.brown.edu/pub/nlparser/ 251 Computational Linguistics Volume 32, Number 2 Figure 19 Examples of parallel sentences used in Experiment 2. Figure 20 The architecture of Experiment 2 (French to English). did in HALogen’s evaluation experiments. We replaced each near-synonym with the metaconcept that is the core meaning of its cluster. The interlingual representation for the English sentence is semantically shallow; it does not reflect the meanin"
J06-2003,P98-2180,0,0.0747484,"Missing"
J06-2003,P95-1026,0,0.0787679,"is to learn a set of words and expressions from CTRW—that is, extraction patterns—that characterizes descriptions of the class. Then, during the extraction phase, for each sentence (or fragment of a sentence) in CTRW the program will decide which leaf class is expressed, with what strength, and what frequency. We use a decision-list algorithm to learn sets of words and extraction patterns for the classes DENOTATIONAL DISTINCTIONS and ATTITUDE - STYLE DISTINCTIONS . These are split further for each leaf class, as explained in Section 2.3. The algorithm we implemented is inspired by the work of Yarowsky (1995) on word sense disambiguation. He classified the senses of a word on the basis of other words that the given word co-occurs with. Collins and Singer (1999) classified proper names 1 We are grateful to HarperCollins Publishers, Inc. for permission to use CTRW in this project. 228 Inkpen and Hirst A Lexical Knowledge Base of Near-Synonym Differences Figure 4 Example of distinctions extracted from CTRW. as P ERSON, O RGANIZATION, or L OCATION using contextual rules (that rely on other words appearing in the context of the proper names) and spelling rules (that rely on words in the proper names)."
J06-2003,1993.eamt-1.2,0,\N,Missing
J06-2003,J98-3003,0,\N,Missing
J06-2003,J90-1003,0,\N,Missing
J06-2003,J93-1007,0,\N,Missing
J06-2003,W02-0907,0,\N,Missing
J06-2003,C94-1049,0,\N,Missing
J06-2003,P02-1040,0,\N,Missing
J06-2003,P01-1025,0,\N,Missing
J06-2003,C98-2175,0,\N,Missing
J06-2003,P98-2181,0,\N,Missing
J06-2003,C98-2176,0,\N,Missing
J18-4006,D14-1124,0,0.0735886,"Missing"
J18-4006,P06-2042,0,0.015628,"Missing"
J18-4006,J08-1001,0,0.0623289,"Missing"
J18-4006,J17-1006,1,0.895954,"Missing"
J18-4006,D11-1120,0,0.0169394,"Missing"
J18-4006,J15-3002,0,0.0485541,"Missing"
J18-4006,P16-1165,0,0.0400509,"Missing"
J18-4006,P18-1052,0,0.038273,"Missing"
J18-4006,P15-2106,0,0.0226009,"uses only textual information. For example, user profiles like age, gender, and location can be used to enhance subjectivity detection (including sentiment and emotion) (Volkova, Coppersmith, and Van Durme 2014; Volkova and Bachrach 2016), vote predictions (Persing and Ng 2014), or language identification (Saloot et al. 2016). Also, information from the conversational thread structure (e.g., links between previous posts) or valuable external sources can serve as contextual constraints to better capture the sentiment or the figurative reading of an utterance (Mukherjee and Bhattacharyya 2012; Karoui et al. 2015; Wallace, Choe, and Charniak 2015)2 . Finally, the social network, like social relationships, can enable grouping users according to specific communities regarding the topics or the sentiments they share (Deitrick and Hu 2013; West et al. 2014). Besides social media processing, the interaction of contextual information derived from sentences, discourse, and other forms of linguistic and extra-linguistic information have shown their effectiveness in language technology in general (Taboada and Mann 2006; Webber, Egg, and Kordoni 2012). This shows that computational linguistics is 1 See Farzinda"
J18-4006,D09-1036,0,0.106432,"Missing"
J18-4006,C16-1214,0,0.031032,"Missing"
J18-4006,C12-1113,0,0.033088,"s performance over a baseline that uses only textual information. For example, user profiles like age, gender, and location can be used to enhance subjectivity detection (including sentiment and emotion) (Volkova, Coppersmith, and Van Durme 2014; Volkova and Bachrach 2016), vote predictions (Persing and Ng 2014), or language identification (Saloot et al. 2016). Also, information from the conversational thread structure (e.g., links between previous posts) or valuable external sources can serve as contextual constraints to better capture the sentiment or the figurative reading of an utterance (Mukherjee and Bhattacharyya 2012; Karoui et al. 2015; Wallace, Choe, and Charniak 2015)2 . Finally, the social network, like social relationships, can enable grouping users according to specific communities regarding the topics or the sentiments they share (Deitrick and Hu 2013; West et al. 2014). Besides social media processing, the interaction of contextual information derived from sentences, discourse, and other forms of linguistic and extra-linguistic information have shown their effectiveness in language technology in general (Taboada and Mann 2006; Webber, Egg, and Kordoni 2012). This shows that computational linguisti"
J18-4006,N16-1013,0,0.0526016,"Missing"
J18-4006,D14-1119,0,0.0151401,"ocation detection) (Aiello et al. 2013; Ghosh et al. 2015; Inkpen et al. 2015; Londhe, Srihari, and Gopalakrishnan 2016).1 Other research explores the interactions between content and extra-linguistic or extra-textual features, showing that combining linguistic data with network and/or user context improves performance over a baseline that uses only textual information. For example, user profiles like age, gender, and location can be used to enhance subjectivity detection (including sentiment and emotion) (Volkova, Coppersmith, and Van Durme 2014; Volkova and Bachrach 2016), vote predictions (Persing and Ng 2014), or language identification (Saloot et al. 2016). Also, information from the conversational thread structure (e.g., links between previous posts) or valuable external sources can serve as contextual constraints to better capture the sentiment or the figurative reading of an utterance (Mukherjee and Bhattacharyya 2012; Karoui et al. 2015; Wallace, Choe, and Charniak 2015)2 . Finally, the social network, like social relationships, can enable grouping users according to specific communities regarding the topics or the sentiments they share (Deitrick and Hu 2013; West et al. 2014). Besides social"
J18-4006,J14-4007,0,0.0527101,"Missing"
J18-4006,D13-1170,0,0.00265532,"llows the syntactic tree up to the main clause by combining pairs of sister nodes by means of a set of sentiment composition rules. In Example (1), sentiment calculation has first to deal with the composition good enough that softens the positivity of the evaluation, which in turn has to be composed with the negation (not) that makes the overall opinion negative. In Example (2), the sentence’s syntactic structure indicates that the atmosphere and the cuisine have both a positive evaluation. For more discussions on sentiment composition, the reader can refer to the Stanford Sentiment Treebank (Socher et al. 2013). The composition process assumes that the interpretation of a given word within a sentence is fixed or disambiguated before being combined, which makes it restrictive in that it “precludes nonlinguistic information to go into the computation of meaning” (Bunt 2001).3 Indeed, the meaning of a sentence is closely tied to the pragmatics of how language is used, and thus to the meaning of the words themselves, which can be assigned different possible readings in different situations (Pustejovsky 1995; Lenci 2006). Consider the problem of lexical ambiguity. For example, A sad movie expresses a sen"
J18-4006,P17-2103,0,0.0541065,"Missing"
J18-4006,C96-2162,0,0.242394,"Missing"
J18-4006,C14-1221,0,0.0673781,"Missing"
J18-4006,P16-1148,0,0.0254568,"Missing"
J18-4006,P14-1018,0,0.0499274,"Missing"
J18-4006,P15-1100,0,0.0555802,"Missing"
J18-4006,E17-2108,0,0.0560014,"Missing"
J18-4006,Q14-1024,0,0.0209316,"predictions (Persing and Ng 2014), or language identification (Saloot et al. 2016). Also, information from the conversational thread structure (e.g., links between previous posts) or valuable external sources can serve as contextual constraints to better capture the sentiment or the figurative reading of an utterance (Mukherjee and Bhattacharyya 2012; Karoui et al. 2015; Wallace, Choe, and Charniak 2015)2 . Finally, the social network, like social relationships, can enable grouping users according to specific communities regarding the topics or the sentiments they share (Deitrick and Hu 2013; West et al. 2014). Besides social media processing, the interaction of contextual information derived from sentences, discourse, and other forms of linguistic and extra-linguistic information have shown their effectiveness in language technology in general (Taboada and Mann 2006; Webber, Egg, and Kordoni 2012). This shows that computational linguistics is 1 See Farzindar and Inkpen (2017) for an overview of the main NLP approaches for social media. 2 See Benamara, Taboada, and Mathieu (2017) for a recent overview of context-based approaches to evaluative language processing. 664 Benamara, Inkpen, and Taboada S"
J18-4006,W15-4614,0,0.0273634,"Missing"
J18-4006,C16-1230,0,0.0559677,"Missing"
J18-4006,S15-2080,0,\N,Missing
J18-4006,J95-2003,0,\N,Missing
J18-4006,J86-3001,0,\N,Missing
J18-4006,W15-1527,1,\N,Missing
J18-4006,P14-1048,0,\N,Missing
J18-4006,W16-2801,0,\N,Missing
J18-4006,D17-1245,0,\N,Missing
mihaila-etal-2010-romanian,cristea-etal-2002-ar,0,\N,Missing
mihaila-etal-2010-romanian,P00-1022,0,\N,Missing
mihaila-etal-2010-romanian,D07-1057,0,\N,Missing
mihaila-etal-2010-romanian,A00-1020,0,\N,Missing
mihaila-etal-2010-romanian,P06-1079,0,\N,Missing
mihaila-etal-2010-romanian,R09-1044,1,\N,Missing
mihaila-etal-2010-romanian,R09-2011,1,\N,Missing
N07-1045,W00-1702,0,0.0253599,"timate the counts by using the Waterloo MultiText1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1 We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 357 2003), Keller and Lapata (2003) show that Web counts correlate well with counts collected fr"
N07-1045,N03-1032,0,0.103043,"foe, rival Figure 1: Examples of sentences with a lexical gap, and candidate near-synonyms to fill the gap. solution. Moreover, what we consider to be the best choice is the typical usage in the corpus, but it may vary from writer to writer. Nonetheless, it is a convenient way of producing test data. The statistical method that we propose here is based on semantic coherence scores (based on mutual information) of each candidate with the words in the context. We explore how far such a method can go when using the Web as a corpus. We estimate the counts by using the Waterloo MultiText1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disam"
N07-1045,P97-1067,0,0.216992,"a corpus (Kilgarriff and Grefenstette, 1 We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 357 2003), Keller and Lapata (2003) show that Web counts correlate well with counts collected from a balanced corpus: the size of the Web compensates for the noise in the data. In this paper we are using a very large corpus of Web pages to address a problem that has not been successfully solved before. In fact, the only work that addresses exactly the same task is that of Edmonds (1997), as far as we are aware. Edmonds gives a solution based on a lexical co-occurrence network that included secondorder co-occurrences. We use a much larger corpus and a simpler method, and we obtain much better results. Our task has similarities to the word sense disambiguation task. Our near-synonyms have senses that are very close to each other. In Senseval, some of the fine-grained senses are also close to each other, so they might occur in similar contexts, while the coarse-grained senses are expected to occur in distinct contexts. In our case, the near-synonyms are different words to choos"
N07-1045,1999.tc-1.8,0,0.04016,"nce scores (based on mutual information) of each candidate with the words in the context. We explore how far such a method can go when using the Web as a corpus. We estimate the counts by using the Waterloo MultiText1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1 We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Pet"
N07-1045,J06-2003,1,0.754873,"faux pas ∗ghastly blooper ∗ghastly solecism ∗ghastly goof ∗ghastly contretemps ∗ghastly boner ∗ghastly slip spelling mistake spelling error ∗spelling blunder ∗spelling faux pas ∗spelling blooper ∗spelling solecism ∗spelling goof ∗spelling contretemps ∗spelling boner ∗spelling slip Table 1: Examples of collocations and anticollocations. The ∗ indicates the anti-collocations. tion of words that a native speaker would not use and therefore should not be used when automatically generating text. This method uses a knowledgebase of collocational behavior of near-synonyms acquired in previous work (Inkpen and Hirst, 2006). A fragment of the knowledge-base is presented in Table 1, for the near-synonyms of the word error and two collocate words ghastly and spelling. The lines marked by ∗ represent anti-collocations and the rest represent strong collocations. The anti-collocations method simply ranks the strong collocations higher than the anti-collocations. In case of ties it chooses the most frequent nearsynonym. In Section 6.2 we present the results of comparing this method to the method from the previous section. 5 A supervised learning method We can also apply supervised learning techniques to our task. It i"
N07-1045,J03-3005,0,0.0735483,"ype of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1 We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 357 2003), Keller and Lapata (2003) show that Web counts correlate well with counts collected from a balanced corpus: the size of the Web compensates for the noise in the data. In this paper we are using a very large corpus of Web pages to address a problem that has not been successfully solved before. In fact, the only work that addresses exactly the same task is that of Edmonds (1997), as far as we are aware. Edmonds gives a solution based on a lexical co-occurrence network that included secondorder co-occurrences. We use a much larger corpus and a simpler method, and we obtain much better results. Our task has similarities t"
N07-1045,J03-3001,0,0.281644,"Missing"
N07-1045,P98-2127,0,0.270441,"experiments show that this method performs better than a previous method on the same task. We also propose and evaluate two more methods, one that uses anticollocations, and one that uses supervised learning. To asses the difficulty of the task, we present results obtained by human judges. 1 Our thesaurus brings up only alternatives that have the same part-of-speech with the target word. The choices could come from various inventories of near-synonyms or similar words, for example the Roget thesaurus (Roget, 1852), dictionaries of synonyms (Hayakawa, 1994), or clusters acquired from corpora (Lin, 1998). Introduction When composing a text, a writer can access a thesaurus to retrieve words that are similar to a given target word, when there is a need to avoid repeating the same word, or when the word does not seem to be the best choice in the context. Our intelligent thesaurus is an interactive application that presents the user with a list of alternative words (near-synonyms), and, unlike standard thesauri, it orders the choices by their suitability to the writing context. We investigate how the collocational properties of near-synonyms can help with choosing the best words. This problem is"
N07-1045,P99-1020,0,0.0287828,"using the Web as a corpus. We estimate the counts by using the Waterloo MultiText1 system (Clarke and Terra, 2003b) with a corpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1 We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 357 2003), Keller and Lapata (2003) show that Web counts correlat"
N07-1045,P99-1068,0,0.0129932,"orpus of about one terabyte of text collected by a Web crawler. We also propose a method that uses collocations and anti-collocations, and a supervised method that uses words and mutual information scores as featured for machine learning. 2 Related work The idea of using the Web as a corpus of texts has been exploited by many researchers. Grefenstette (1999) used the Web for example-based machine translation; Kilgarriff (2001) investigated the type of noise in Web data; Mihalcea and Moldovan (1999) and Agirre and Martinez (2000) used it as an additional resource for word sense disambiguation; Resnik (1999) mined the Web for bilingual texts; Turney (2001) used Web frequency counts to compute information retrieval-based mutual-information scores. In a Computational Linguistics special issue on the Web as a corpus (Kilgarriff and Grefenstette, 1 We thank Egidio Terra, Charlie Clarke, and Univ. of Waterloo for allowing us to use MultiText, and to Peter Turney and IIT/NRC for giving us access to their local copy of the corpus. 357 2003), Keller and Lapata (2003) show that Web counts correlate well with counts collected from a balanced corpus: the size of the Web compensates for the noise in the data"
N07-1045,J90-1003,0,\N,Missing
N07-1045,C98-2122,0,\N,Missing
N12-1016,J08-4004,0,0.365664,"7 coders while developing the automatic segmenter TextTiling, and reported mean κ (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean κ scores were calculated by comparing a coder’s segmentation against a reference segmentation formulated by the majority opinion strategy used in Passonneau and Litman (1993, p. 150) (Hearst, 1997, pp. 53–54). Although mean κ scores attempt to take into account chance agreement, near misses are still unaccounted for, and use of Siegel and Castellan’s (1988) κ has declined in favour of other coefficients (Artstein and Poesio, 2008, pp. 555–556). Artstein and Poesio (2008) briefly touch upon recommendations for coefficients for segmentation evaluation, and though they do not propose a measure, they do conjecture that a modification of a weighted form of α (Krippendorff, 1980; Krippendorff, 2004) using unification and WindowDiff may suffice (Artstein and Poesio, 2008, pp. 580–582). 3 Segmentation Similarity For discussing segmentation, a segment’s size (or mass) is measured in units, the error is quantified in potential boundaries (PBs), and we have adopted a modified form of the notation used by Artstein and Poesio (200"
N12-1016,W97-0304,0,0.363231,"t (2002, pp. 5–10) highlighted a number of issues with Pk , specifically that: i) False negatives (FNs) are penalized more than false positives (FPs); ii) It does not penalize FPs that fall within k units of a reference boundary; iii) Its sensitivity to variations in segment size can cause it to linearly decrease the penalty for FPs if the size of any segments fall below k; and iv) Near-miss errors are too harshly penalized. To attempt to mitigate the shortcomings of Pk , Pevzner and Hearst (2002, p. 10) proposed a modified metric which changed how penalties were 2 Pk is a modification of Pµ (Beeferman et al., 1997, p. 43). Other modifications such as TDT Cseg (Doddington, 1998, pp. 5–6) have been proposed, but Pk has seen greater usage. 153 counted, named WindowDiff (W D). A window of size k is still slid across the text, but now penalties are attributed to windows where the number of boundaries in each segmentation differs (see Equation 1, where b(Rij ) and b(Hij ) represents the number of boundaries within the segments in a window of size k from position i to j, and N the number of sentences plus one), with the same normalization. WD(R, H) = 1 N −k N −k X (|b(Rij ) − b(Hij ) |&gt; 0) (1) i=1,j=i+k Windo"
N12-1016,P92-1032,0,0.397372,"t for near misses. 2.2 Inter-Annotator Agreement The need to ascertain the agreement and reliability between coders for segmentation was recognized 3 Georgescul et al. (2006, p. 48) note that both FPs and FNs are weighted by 1 /N −k , and although there are “equiprobable possibilities to have a [FP] in an interval of k units”, “the total number of equiprobable possibilities to have a [FN] in an interval of k units is smaller than (N −k)”, making the interpretation of a full miss as a FN less probable than as a FP. by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses. Hearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean κ (Siegel and Castellan, 1988) values for coders and automatic segmenters (Hearst, 1997, p. 56). Pairwise mean κ scores were calculat"
N12-1016,W06-1320,0,0.336966,"of word positions present (see Equation 2). RF N = 1 X F N (w), N w RF P = 1 X F P (w) N w (2) RF N and RF P have the advantage that they take into account the severity of an error in terms of segment size, allowing them to reflect the effects of erroneously missing, or added, words in a segment better than window based metrics. Unfortunately, RF N and RF P suffer from the same flaw as precision, recall, and Fβ -measure in that they do not account for near misses. 2.2 Inter-Annotator Agreement The need to ascertain the agreement and reliability between coders for segmentation was recognized 3 Georgescul et al. (2006, p. 48) note that both FPs and FNs are weighted by 1 /N −k , and although there are “equiprobable possibilities to have a [FP] in an interval of k units”, “the total number of equiprobable possibilities to have a [FN] in an interval of k units is smaller than (N −k)”, making the interpretation of a full miss as a FN less probable than as a FP. by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of"
N12-1016,J97-1003,0,0.948111,"istic tasks which depend upon the identification of segment boundaries in text. To evaluate automatic segmentation methods, a method of comparing an automatic segmenter’s performance against the segmentations produced by human judges (coders) is required. Current methods of performing this comparison designate only one coder’s segmentation as a reference to compare against. A single “true” reference segmentation from a coder should not be trusted, given that interannotator agreement is often reported to be rather Diana Inkpen University of Ottawa Ottawa, ON, Canada diana@eecs.uottawa.ca poor (Hearst, 1997, p. 54). Additionally, to ensure that an automatic segmenter does not over-fit to the preference and bias of one particular coder, an automatic segmenter should be compared directly against multiple coders. The state of the art segmentation evaluation metrics (Pk and WindowDiff) slide a window across a designated reference and hypothesis segmentation, and count the number of windows where the number of boundaries differ. Window-based methods suffer from a variety of problems, including: i) unequal penalization of error types; ii) an arbitrarily defined window size parameter (whose choice grea"
N12-1016,D09-1137,0,0.0141261,"fficients would signify that the automatic segmenter does not perform as reliably as the group of human coders.7 This can be performed independently for multiple automatic segmenters to compare them to each other– assuming that the coefficients model chance agreement appropriately–because agreement is calculated (and quantifies reliability) over all segmentations. 3.5 Inter-Annotator Agreement Similarity alone is not a sufficiently insightful measure of reliability, or agreement, between coders. with t boundary types is t · mass(i) − t. 7 Similar to how human competitiveness is ascertained by Medelyan et al. (2009, pp. 1324–1325) and Medelyan (2009, pp. 143–145) by comparing drops in inter-indexer consistency. 156 Chance agreement occurs in segmentation when coders operating at slightly different granularities agree due to their codings, and not their own innate segmentation heuristics. Inter-annotator agreement coefficients have been developed that assume a variety of prior distributions to characterize chance agreement, and to attempt to offer a way to identify whether agreement is primarily due to chance, or not, and to quantify reliability. Artstein and Poesio (2008) note that most of a coder’s jud"
N12-1016,P93-1020,0,0.477332,"he same flaw as precision, recall, and Fβ -measure in that they do not account for near misses. 2.2 Inter-Annotator Agreement The need to ascertain the agreement and reliability between coders for segmentation was recognized 3 Georgescul et al. (2006, p. 48) note that both FPs and FNs are weighted by 1 /N −k , and although there are “equiprobable possibilities to have a [FP] in an interval of k units”, “the total number of equiprobable possibilities to have a [FN] in an interval of k units is smaller than (N −k)”, making the interpretation of a full miss as a FN less probable than as a FP. by Passonneau and Litman (1993), who adapted the percentage agreement metric by Gale et al. (1992, p. 254) for usage in segmentation. This percentage agreement metric (Passonneau and Litman, 1993, p. 150) is the ratio of the total observed agreement of a coder with the majority opinion for each boundary over the total possible agreements. This measure failed to take into account chance agreement, or to less harshly penalize near-misses. Hearst (1997) collected segmentations from 7 coders while developing the automatic segmenter TextTiling, and reported mean κ (Siegel and Castellan, 1988) values for coders and automatic segm"
N12-1016,J02-1002,0,0.920902,"2. 2012 Association for Computational Linguistics inter-annotator agreement coefficient adaptations. In Section 4, we evaluate S and WindowDiff in various scenarios and simulations, and upon a multiplycoded corpus. 2 2.1 Related Work Segmentation Evaluation Precision, recall, and their mean (Fβ -measure) have been previously applied to segmentation evaluation. Precision is the proportion of boundaries chosen that agree with a reference segmentation, and recall is the proportion of boundaries chosen that agree with a reference segmentation out of all boundaries in the reference and hypothesis (Pevzner and Hearst, 2002, p. 3). For segmentation, these metrics are unsuitable because they penalize near-misses of boundaries as full-misses, causing them to drastically overestimate the error. Near-misses are prevalent in segmentation and can account for a large proportion of the errors produced by a coder, and as inter-annotator agreement often shows, they do not reflect coder error, but the difficulty of the task. Pk (Beeferman and Berger, 1999, pp. 198–200)2 is a window-based metric which attempts to solve the harsh near-miss penalization of precision, recall, and Fβ -measure. In Pk , a window of size k, where"
N12-1016,N12-1022,0,\N,Missing
N12-1038,D08-1001,0,0.0139009,"produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 1 Introduction WindowDiff (Pevzner and Hearst, 2002) has become the most frequently used measure to evaluate segmentation. Segmentation is the task of dividing a stream of data (text or other media) into coherent units. These units may be motivated topically (Malioutov and Barzilay, 2006), structurally (Stokes, 2003) (Malioutov et al., 2007) (Jancsary et al., 2008), or visually (Chen et al., 2008), depending on the domain and task. Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries. 2 WindowDiff “The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally.” (Malioutov et al., 2007). WindowDiff uses a sliding window over the segmentation; each window is evaluated as correct or incorrect. WindowDiff is effectively 1 − accuracy for all windows, but accuracy is sensitive to the balance of positive and negative data bein"
N12-1038,P06-1004,0,0.0773761,"of WindowDiff. WinPR distinguishes between false positive and false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 1 Introduction WindowDiff (Pevzner and Hearst, 2002) has become the most frequently used measure to evaluate segmentation. Segmentation is the task of dividing a stream of data (text or other media) into coherent units. These units may be motivated topically (Malioutov and Barzilay, 2006), structurally (Stokes, 2003) (Malioutov et al., 2007) (Jancsary et al., 2008), or visually (Chen et al., 2008), depending on the domain and task. Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries. 2 WindowDiff “The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally.” (Malioutov et al., 2007). WindowDiff uses a sliding window over the segmentation; each window is evaluated as correct or incorrect. WindowDiff is effectively 1 − accuracy for all window"
N12-1038,J02-1002,0,0.244354,"ON, K1N 6N5, Canada mscai056@uottawa.ca Diana Inkpen University of Ottawa Ottawa, ON, K1N 6N5, Canada diana@eecs.uottawa.com Abstract We introduce a new segmentation evaluation measure, WinPR, which resolves some of the limitations of WindowDiff. WinPR distinguishes between false positive and false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 1 Introduction WindowDiff (Pevzner and Hearst, 2002) has become the most frequently used measure to evaluate segmentation. Segmentation is the task of dividing a stream of data (text or other media) into coherent units. These units may be motivated topically (Malioutov and Barzilay, 2006), structurally (Stokes, 2003) (Malioutov et al., 2007) (Jancsary et al., 2008), or visually (Chen et al., 2008), depending on the domain and task. Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries. 2 WindowDiff “The WindowDiff metric is a variant of the Pk measure, which"
N12-1038,N03-3009,0,0.0185819,"lse positive and false negative errors; produces more intuitive measures, such as precision, recall, and F-measure; is insensitive to window size, which allows us to customize near miss sensitivity; and is based on counting errors not windows, but still provides partial reward for near misses. 1 Introduction WindowDiff (Pevzner and Hearst, 2002) has become the most frequently used measure to evaluate segmentation. Segmentation is the task of dividing a stream of data (text or other media) into coherent units. These units may be motivated topically (Malioutov and Barzilay, 2006), structurally (Stokes, 2003) (Malioutov et al., 2007) (Jancsary et al., 2008), or visually (Chen et al., 2008), depending on the domain and task. Segmentation evaluation is difficult because exact comparison of boundaries is too strict; a partial reward is required for close boundaries. 2 WindowDiff “The WindowDiff metric is a variant of the Pk measure, which penalizes false positives and near misses equally.” (Malioutov et al., 2007). WindowDiff uses a sliding window over the segmentation; each window is evaluated as correct or incorrect. WindowDiff is effectively 1 − accuracy for all windows, but accuracy is sensitive"
N12-1038,P07-1064,0,\N,Missing
P06-1056,N01-1014,0,0.584894,"Missing"
P06-1056,J04-1001,0,0.313312,"identified letter correspondence between words and estimates the likelihood of relatedness. No semantic component is present in the system, the words are assumed to be already matched by their meanings. Hewson (1993), Lowe and Mazadon (1994) used systematic sound correspondences to determine protoprojections for identifying cognate sets. WSD is a task that has attracted researchers since 1950 and it is still a topic of high interest. Determining the sense of an ambiguous word, using bootstrapping and texts from a different language was done by Yarowsky (1995), Hearst (1991), Diab (2002), and Li and Li (2004). Yarowsky (1995) has used a few seeds and untagged sentences in a bootstrapping algorithm based on decision lists. He added two constrains – words tend to have one sense per discourse and one sense per collocation. He reported high accuracy scores for a set of 10 words. The monolingual bootstrapping approach was also used by Hearst (1991), who used a small set of handlabeled data to bootstrap from a larger corpus for training a noun disambiguation system for English. Unlike Yarowsky (1995), we use automatic collection of seeds. Besides our monolingual bootstrapping technique, we also use bili"
P06-1056,J94-3004,0,0.110898,"Missing"
P06-1056,C04-1192,0,0.0613576,"Missing"
P06-1056,P95-1026,0,0.304869,"Missing"
P06-1056,P02-1033,0,\N,Missing
P17-1152,D15-1075,0,0.927613,"g for inflation. h: Some of the companies in the poll reported cost increases. Introduction Reasoning and inference are central to both human and artificial intelligence. Modeling inference in human language is notoriously challenging but is a basic problem towards true natural language understanding, as pointed out by MacCartney and Manning (2008), “a necessary (if not sufficient) The most recent years have seen advances in modeling natural language inference. An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects. This makes it feasible to train more complex inference models. Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016). While some previous top-performing models use rather complicated network architectures to achieve the state-of-the-art results (Munkhdalai and Yu, 2016b), we demon"
P17-1152,P16-1139,0,0.349126,". Rocktäschel et al. (2015) proposed neural attention-based models for NLI, which captured the attention information. In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a; Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Among them, more relevant to ours are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best performing models. Parikh et al. (2016) propose a relatively simple but very effective decomposable model. The model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, Munkhda"
P17-1152,P15-1011,1,0.299019,"Missing"
P17-1152,D16-1053,0,0.0771153,"Missing"
P17-1152,W14-4012,0,0.0198449,"Missing"
P17-1152,C14-1068,0,0.022914,"Missing"
P17-1152,P03-1054,0,0.0491768,"(SNLI) corpus (Bowman et al., 2015) focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral). The original SNLI corpus contains also “the other” category, which includes the sentence pairs lacking consensus among multiple human annotators. As in the related work, we remove this category. We used the same split as in Bowman et al. (2015) and other previous work. The parse trees used in this paper are produced by the Stanford PCFG Parser 3.5.3 (Klein and Manning, 2003) and they are delivered as part of the SNLI corpus. We use classification accuracy as the evaluation metric, as in related work. Training We use the development set to select models for testing. To help replicate our results, we publish our code1 . Below, we list our training details. We use the Adam method (Kingma and Ba, 2014) for optimization. The first momentum is set to be 0.9 and the second 0.999. The initial learning rate is 0.0004 and the batch size is 32. All hidden states of LSTMs, tree-LSTMs, and word embeddings have 300 dimensions. We use dropout with a rate of 0.5, which is applie"
P17-1152,S15-1002,0,0.0646178,"present that time step and its context. Note that we used LSTM memory blocks in our models. We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) (Cho et al., 2014) and they are inferior to LSTMs on the heldout set for our NLI task. As discussed above, it is intriguing to explore the effectiveness of syntax for natural language inference; for example, whether it is useful even when incorporated into the best-performing models. To this end, we will also encode syntactic parse trees of a premise and hypothesis through treeLSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al., 2011). Figure 1: A high-level view of our hybrid neural inference networks. to encode the input premise and hypothesis (Equation (1) and (2)). Here BiLSTM learns to represent a word (e.g., ai ) and its context. Later we will also use BiLSTM to perform inference composition to construct the final prediction, where BiLSTM encodes local inference information and its interaction. To bookkeep the notations for later use, we ¯i the hidden (output) state generated by write as a the BiLSTM at time i over the input sequence a. ¯j: Th"
P17-1152,C08-1066,0,0.0129228,"itional improvement. Particularly, incorporating syntactic parsing information contributes to our best result—it further improves the performance even when added to the already very strong model. 1 p: Several airlines polled saw costs grow more than expected, even after adjusting for inflation. h: Some of the companies in the poll reported cost increases. Introduction Reasoning and inference are central to both human and artificial intelligence. Modeling inference in human language is notoriously challenging but is a basic problem towards true natural language understanding, as pointed out by MacCartney and Manning (2008), “a necessary (if not sufficient) The most recent years have seen advances in modeling natural language inference. An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects. This makes it feasible to train more complex inference models. Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman"
P17-1152,N10-1146,0,0.0231776,"Missing"
P17-1152,P16-2022,0,0.4001,"attention-based models for NLI, which captured the attention information. In general, attention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a; Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Among them, more relevant to ours are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best performing models. Parikh et al. (2016) propose a relatively simple but very effective decomposable model. The model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, Munkhdalai and Yu (2016b) propose much more com"
P17-1152,D16-1244,0,0.640143,"ent years have seen advances in modeling natural language inference. An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects. This makes it feasible to train more complex inference models. Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016). While some previous top-performing models use rather complicated network architectures to achieve the state-of-the-art results (Munkhdalai and Yu, 2016b), we demonstrate in this paper that enhancing sequential inference models based on chain 1657 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1657–1668 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1152 models can outperform all previous results, suggesting that the potentials o"
P17-1152,D14-1162,0,0.123262,"Missing"
P17-1152,D15-1044,0,0.0690079,"Missing"
P17-1152,C16-1270,0,0.0980079,"dvances in modeling natural language inference. An important contribution is the creation of a much larger annotated dataset, the Stanford Natural Language Inference (SNLI) dataset (Bowman et al., 2015). The corpus has 570,000 human-written English sentence pairs manually labeled by multiple human subjects. This makes it feasible to train more complex inference models. Neural network models, which often need relatively large annotated data to estimate their parameters, have shown to achieve the state of the art on SNLI (Bowman et al., 2015, 2016; Munkhdalai and Yu, 2016b; Parikh et al., 2016; Sha et al., 2016; Paria et al., 2016). While some previous top-performing models use rather complicated network architectures to achieve the state-of-the-art results (Munkhdalai and Yu, 2016b), we demonstrate in this paper that enhancing sequential inference models based on chain 1657 Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 1657–1668 c Vancouver, Canada, July 30 - August 4, 2017. 2017 Association for Computational Linguistics https://doi.org/10.18653/v1/P17-1152 models can outperform all previous results, suggesting that the potentials of such sequential"
P17-1152,D13-1170,0,0.0469839,"Missing"
P17-1152,P15-1150,0,0.20089,"ep are concatenated to represent that time step and its context. Note that we used LSTM memory blocks in our models. We examined other recurrent memory blocks such as GRUs (Gated Recurrent Units) (Cho et al., 2014) and they are inferior to LSTMs on the heldout set for our NLI task. As discussed above, it is intriguing to explore the effectiveness of syntax for natural language inference; for example, whether it is useful even when incorporated into the best-performing models. To this end, we will also encode syntactic parse trees of a premise and hypothesis through treeLSTM (Zhu et al., 2015; Tai et al., 2015; Le and Zuidema, 2015), which extends the chain LSTM to a recursive network (Socher et al., 2011). Figure 1: A high-level view of our hybrid neural inference networks. to encode the input premise and hypothesis (Equation (1) and (2)). Here BiLSTM learns to represent a word (e.g., ai ) and its context. Later we will also use BiLSTM to perform inference composition to construct the final prediction, where BiLSTM encodes local inference information and its interaction. To bookkeep the notations for later use, we ¯i the hidden (output) state generated by write as a the BiLSTM at time i over the i"
P17-1152,N16-1170,0,0.201624,"ttention based models have been shown to be effective in a wide range of tasks, including machine translation (Bahdanau et al., 2014), speech recognition (Chorowski et al., 2015; Chan et al., 2016), image caption (Xu et al., 2015), and text summarization (Rush et al., 2015; Chen et al., 2016), among others. For NLI, the idea allows neural models to pay attention to specific areas of the sentences. A variety of more advanced networks have been developed since then (Bowman et al., 2016; Vendrov et al., 2015; Mou et al., 2016; Liu et al., 2016; Munkhdalai and Yu, 2016a; Rocktäschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Munkhdalai and Yu, 2016b; Sha et al., 2016; Paria et al., 2016). Among them, more relevant to ours are the approaches proposed by Parikh et al. (2016) and Munkhdalai and Yu (2016b), which are among the best performing models. Parikh et al. (2016) propose a relatively simple but very effective decomposable model. The model decomposes the NLI problem into subproblems that can be solved separately. On the other hand, Munkhdalai and Yu (2016b) propose much more complicated networks that consider sequential LSTM-based encoding, recursive networks, and comp"
P17-1152,P14-1029,1,0.288524,"Missing"
P18-1224,D15-1075,0,0.473188,"an Zhu ECE, Queen’s University xiaodan.zhu@queensu.ca Zhen-Hua Ling University of Science and Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently be"
P18-1224,P16-1139,0,0.0147052,"g-based models use Siamese architecture (Bromley et al., 1993). The parametertied neural networks are applied to encode both the premise and the hypothesis. Then a neural network classifier is applied to decide relationship between the two sentences. Different neural networks have been utilized for sentence encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rockt¨aschel et al. (2015) were among the first to propose neural attention-based models for NLI. Chen et al. (2017a) proposed an enhanced sequential inference model (ESIM), which is one of the best models so far and i"
P18-1224,P15-1011,1,0.45444,"oves the state-of-the-art neural NLI models to achieve better performances on the SNLI and MultiNLI datasets. The advantage of using external knowledge is more significant when the size of training data is restricted, suggesting that if more knowledge can be obtained, it may have more benefit. In addition to attaining the state-of-the-art performance, we are also interested in understanding how external knowledge affect major components of neural-network-based NLI models. In general, external knowledge has shown to be effective in neural networks for other NLP tasks, including word embedding (Chen et al., 2015; Faruqui et al., 2015; Liu et al., 2015; Wieting et al., 2015; Mrksic et al., 2017), machine translation (Shi et al., 2016; Zhang et al., 2017b), language modeling (Ahn et al., 2016), and dialogue systems (Chen et al., 2016b). 3 Neural-Network-Based NLI Models with External Knowledge In this section we propose neural-network-based NLI models to incorporate external inference knowledge, which, as we will show later in Section 5, achieve the state-of-the-art performance. In addition to attaining the leading performance we are also interested in investigating the effects of external knowledge on"
P18-1224,D16-1053,0,0.00874405,"Missing"
P18-1224,D17-1070,0,0.0157156,"to decide relationship between the two sentences. Different neural networks have been utilized for sentence encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rockt¨aschel et al. (2015) were among the first to propose neural attention-based models for NLI. Chen et al. (2017a) proposed an enhanced sequential inference model (ESIM), which is one of the best models so far and is used as one of our baselines in this paper. In this paper we enrich neural-network-based NLI models with external knowledge. Unlike early work on NLI (Jijkoun and de Rijke, 2005; MacCartney et al., 20"
P18-1224,P17-1152,1,0.37788,"d Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which"
P18-1224,N15-1184,0,0.00489591,"he-art neural NLI models to achieve better performances on the SNLI and MultiNLI datasets. The advantage of using external knowledge is more significant when the size of training data is restricted, suggesting that if more knowledge can be obtained, it may have more benefit. In addition to attaining the state-of-the-art performance, we are also interested in understanding how external knowledge affect major components of neural-network-based NLI models. In general, external knowledge has shown to be effective in neural networks for other NLP tasks, including word embedding (Chen et al., 2015; Faruqui et al., 2015; Liu et al., 2015; Wieting et al., 2015; Mrksic et al., 2017), machine translation (Shi et al., 2016; Zhang et al., 2017b), language modeling (Ahn et al., 2016), and dialogue systems (Chen et al., 2016b). 3 Neural-Network-Based NLI Models with External Knowledge In this section we propose neural-network-based NLI models to incorporate external inference knowledge, which, as we will show later in Section 5, achieve the state-of-the-art performance. In addition to attaining the leading performance we are also interested in investigating the effects of external knowledge on major components of n"
P18-1224,W17-5307,1,0.712456,"d Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based inference models, which"
P18-1224,P82-1020,0,0.845753,"Missing"
P18-1224,O97-1002,0,0.0637707,"H( j=1 αij rij )) w Pm Pn a = avi , exp(H( α r )) ij ij i=1 j=1 i=1 Pm n X exp(H( i=1 βij rji )) Pn Pm bvj . bw = exp(H( β r )) j=1 i=1 ij ji j=1 m X Representation of External Knowledge Lexical Semantic Relations As described in Section 3.1, to incorporate external knowledge (as a knowledge vector rij ) to the state-of-theart neural network-based NLI models, we first explore semantic relations in WordNet (Miller, 1995), motivated by MacCartney (2009). Specifically, the relations of lexical pairs are derived as described in (1)-(4) below. Instead of using JiangConrath WordNet distance metric (Jiang and Conrath, 1997), which does not improve the performance of our models on the development sets, we add a new feature, i.e., co-hyponyms, which consistently benefit our models. (1) Synonymy: It takes the value 1 if the words in the pair are synonyms in WordNet (i.e., belong to the same synset), and 0 otherwise. For example, [felicitous, good] = 1, [dog, wolf] = 0. (2) Antonymy: It takes the value 1 if the words in the pair are antonyms in WordNet, and 0 otherwise. For example, [wet, dry] = 1. (3) Hypernymy: It takes the value 1 − n/8 if one word is a (direct or indirect) hypernym of the other word in WordNet,"
P18-1224,P16-1098,0,0.0126473,"e complex models. These models mainly fall into two types of approaches: sentence-encoding-based models and models using also inter-sentence attention. Sentence-encoding-based models use Siamese architecture (Bromley et al., 1993). The parametertied neural networks are applied to encode both the premise and the hypothesis. Then a neural network classifier is applied to decide relationship between the two sentences. Different neural networks have been utilized for sentence encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rockt¨aschel et al. (2015) were among the first"
P18-1224,D16-1176,0,0.014176,"e complex models. These models mainly fall into two types of approaches: sentence-encoding-based models and models using also inter-sentence attention. Sentence-encoding-based models use Siamese architecture (Bromley et al., 1993). The parametertied neural networks are applied to encode both the premise and the hypothesis. Then a neural network classifier is applied to decide relationship between the two sentences. Different neural networks have been utilized for sentence encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rockt¨aschel et al. (2015) were among the first"
P18-1224,P15-1145,1,0.706034,"ls to achieve better performances on the SNLI and MultiNLI datasets. The advantage of using external knowledge is more significant when the size of training data is restricted, suggesting that if more knowledge can be obtained, it may have more benefit. In addition to attaining the state-of-the-art performance, we are also interested in understanding how external knowledge affect major components of neural-network-based NLI models. In general, external knowledge has shown to be effective in neural networks for other NLP tasks, including word embedding (Chen et al., 2015; Faruqui et al., 2015; Liu et al., 2015; Wieting et al., 2015; Mrksic et al., 2017), machine translation (Shi et al., 2016; Zhang et al., 2017b), language modeling (Ahn et al., 2016), and dialogue systems (Chen et al., 2016b). 3 Neural-Network-Based NLI Models with External Knowledge In this section we propose neural-network-based NLI models to incorporate external inference knowledge, which, as we will show later in Section 5, achieve the state-of-the-art performance. In addition to attaining the leading performance we are also interested in investigating the effects of external knowledge on major components of neural-network-base"
P18-1224,D08-1084,0,0.0476054,"Missing"
P18-1224,P14-5010,0,0.00165768,"the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral). We use the same data split as in previous work (Bowman et al., 2015; Williams et al., 2017) and classification accuracy as the evaluation metric. In addition, we test our models (trained on the SNLI training set) on a new test set (Glockner et al., 2018), which assesses the lexical inference abilities of NLI systems and consists of 8,193 samples. WordNet 3.0 (Miller, 1995) is used to extract semantic relation features between words. The words are lemmatized using Stanford CoreNLP 3.7.0 (Manning et al., 2014). The premise and the hypothesis sentences fed into the input encoding layer are tokenized. 4.3 Training Details For duplicability, we release our code1 . All our models were strictly selected on the development set of the SNLI data and the in-domain development set of MultiNLI and were then tested on the corresponding test set. The main training details are as follows: the dimension of the hidden states of LSTMs and word embeddings are 300. The word embeddings are initialized by 300D GloVe 840B (Pennington et al., 2014), and out-of-vocabulary words among them are initialized randomly. All wor"
P18-1224,P16-2022,0,0.146841,"l., 2017), has made it possible to train more complex models. These models mainly fall into two types of approaches: sentence-encoding-based models and models using also inter-sentence attention. Sentence-encoding-based models use Siamese architecture (Bromley et al., 1993). The parametertied neural networks are applied to encode both the premise and the hypothesis. Then a neural network classifier is applied to decide relationship between the two sentences. Different neural networks have been utilized for sentence encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rock"
P18-1224,Q17-1022,0,0.010362,"Missing"
P18-1224,W17-5308,0,0.106918,"Missing"
P18-1224,D16-1244,0,0.0911062,"Missing"
P18-1224,D14-1162,0,0.114445,"atures between words. The words are lemmatized using Stanford CoreNLP 3.7.0 (Manning et al., 2014). The premise and the hypothesis sentences fed into the input encoding layer are tokenized. 4.3 Training Details For duplicability, we release our code1 . All our models were strictly selected on the development set of the SNLI data and the in-domain development set of MultiNLI and were then tested on the corresponding test set. The main training details are as follows: the dimension of the hidden states of LSTMs and word embeddings are 300. The word embeddings are initialized by 300D GloVe 840B (Pennington et al., 2014), and out-of-vocabulary words among them are initialized randomly. All word embeddings are updated during training. Adam (Kingma and Ba, 2014) is used for optimization with an initial learning rate of 0.0004. The mini-batch size is set to 32. Note that the above hyperparameter settings are same as those used in the baseline ESIM (Chen et al., 2017a) model. ESIM is a strong NLI baseline framework with the source code made available at https://github.com/lukecq1231/nli (the ESIM core code has also been adapted to summarization (Chen et al., 2016a) and questionanswering tasks (Zhang et al., 2017a"
P18-1224,C16-1270,0,0.0789147,"sity of Science and Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex models such as neural-network-based infe"
P18-1224,P16-1212,0,0.0135296,"Missing"
P18-1224,E17-1038,0,0.27173,"ity xiaodan.zhu@queensu.ca Zhen-Hua Ling University of Science and Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex"
P18-1224,E17-1002,0,0.246186,"ity xiaodan.zhu@queensu.ca Zhen-Hua Ling University of Science and Technology of China zhling@ustc.edu.cn Diana Inkpen University of Ottawa diana@site.uottawa.ca Si Wei iFLYTEK Research siwei@iflytek.com Abstract In the last several years, larger annotated datasets were made available, e.g., the SNLI (Bowman et al., 2015) and MultiNLI datasets (Williams et al., 2017), which made it feasible to train rather complicated neuralnetwork-based models that fit a large set of parameters to better model NLI. Such models have shown to achieve the state-of-the-art performance (Bowman et al., 2015, 2016; Yu and Munkhdalai, 2017b; Parikh et al., 2016; Sha et al., 2016; Chen et al., 2017a,b; Tay et al., 2018). While neural networks have been shown to be very effective in modeling NLI with large training data, they have often focused on end-to-end training by assuming that all inference knowledge is learnable from the provided training data. In this paper, we relax this assumption and explore whether external knowledge can further help NLI. Consider an example: Modeling natural language inference is a very challenging task. With the availability of large annotated data, it has recently become feasible to train complex"
P18-1224,D17-1146,0,0.0419347,"Missing"
P18-1224,N16-1170,0,0.0335271,"nce encoding, such as LSTM (Bowman et al., 2015), GRU (Vendrov et al., 2015), CNN (Mou et al., 2016), BiLSTM and its variants (Liu et al., 2016c; Lin et al., 2017; Chen et al., 2017b; Nie and Bansal, 2017), self-attention network (Shen et al., 2017, 2018), and more complicated neural networks (Bowman et al., 2016; Yu and Munkhdalai, 2017a,b; Choi et al., 2017). Sentence-encoding-based models transform sentences into fixed-length vector representations, which may help a wide range of tasks (Conneau et al., 2017). The second set of models use inter-sentence attention (Rockt¨aschel et al., 2015; Wang and Jiang, 2016; Cheng et al., 2016; Parikh et al., 2016; Chen et al., 2017a). Among them, Rockt¨aschel et al. (2015) were among the first to propose neural attention-based models for NLI. Chen et al. (2017a) proposed an enhanced sequential inference model (ESIM), which is one of the best models so far and is used as one of our baselines in this paper. In this paper we enrich neural-network-based NLI models with external knowledge. Unlike early work on NLI (Jijkoun and de Rijke, 2005; MacCartney et al., 2008; MacCartney, 2009) that explores external knowledge in conventional NLI models on relatively small NL"
P18-1224,Q15-1025,0,\N,Missing
P18-1224,N18-1101,0,\N,Missing
R11-1093,N07-1025,0,\N,Missing
R13-1003,baccianella-etal-2010-sentiwordnet,0,0.0298118,"Missing"
R13-1003,C12-1019,0,0.106577,"Missing"
R13-1003,J11-2001,0,0.0490166,"Missing"
R13-1003,N09-2061,0,0.0361051,"Missing"
R13-1003,P09-1026,0,0.0280493,"Missing"
R13-1003,H05-1044,0,\N,Missing
R13-1003,R11-1019,1,\N,Missing
S13-2062,N03-1033,0,0.0106222,"he target word’s definition and synonyms were then concatenated to the original tweet. No additional changes were made to either the original tweet or the features that were added from SentiWordNet. An example follows in Table 1. The test data (Twitter and SMS) was not expanded, because there are no labels in the test data to be able to choose the sense with corresponding sentiment. 2.2 Task B For this task, we used the following resources: SentiwordNet (Baccianella et al, 2010), the Polarity Lexicon (Wilson et al., 2005), the General Inquirer (Stone et al., 1966), and the Stanford NLP tools (Toutanova et al., 2003) for preprocessing and feature selection. The preprocessing of Twitter messages is implemented in three steps namely, stop-word removal, stemming, and removal of words with occurrence frequency of one. Several extra features will be used: the number of positive words and negative words identified by three lexical resources mentioned above, the number of emoticons, the number of elongated words, and the number of punctuation tokens (single or repeated exclamation marks, etc.). As for SentiWordNet, for each word a score is calculated that shows the positive or negative weight of that word. No se"
S13-2062,S13-2052,0,0.0825451,"Missing"
S13-2062,H05-1044,0,0.0635892,"Missing"
S18-1027,P14-1062,0,0.0952575,"Missing"
S18-1027,L18-1030,0,0.0274617,"Missing"
S18-1027,S13-2053,0,0.0361263,"el Convolutional Neural Network (CNN), as well as a self-attentive word-level encoder. The key advantage of our model is its ability to signify the relevant and important information that enables self-optimization. Results are reported on the valence intensity regression task. 1 Introduction Affect analysis is one of the main topics of natural language processing (NLP). It involves many sub-tasks such as sentiment and valence analyses expressed in text. We focus on the task of determining valence intensity. Hand-crafted features and/or sentiment lexicons are commonly used for affect analysis (Mohammad, Kiritchenko, & Zhu, 2013; Taboada, Brooke, Tofiloski, Voll, & Stede, 2011) with classifiers such as random forest and support vector machines (SVM). Affect in tweets (AIT) is a challenging task as it requires handling an informal writing style, which typically has many grammar mistakes, slangs, and misspellings. In this paper, we present a self-attentive hybrid GRU-based network (SAHGN) that competed at SemEval-2018 Task 1 (Mohammad, BravoMarquez, Salameh, & Kiritchenko, 2018; Mohammad & Kiritchenko, 2018). Our contributions can be summarized as below. • 2 The implementation of a social media text processor: A librar"
S18-1027,D14-1162,0,0.0873733,"Missing"
S18-1027,J11-2001,0,0.0146871,"), as well as a self-attentive word-level encoder. The key advantage of our model is its ability to signify the relevant and important information that enables self-optimization. Results are reported on the valence intensity regression task. 1 Introduction Affect analysis is one of the main topics of natural language processing (NLP). It involves many sub-tasks such as sentiment and valence analyses expressed in text. We focus on the task of determining valence intensity. Hand-crafted features and/or sentiment lexicons are commonly used for affect analysis (Mohammad, Kiritchenko, & Zhu, 2013; Taboada, Brooke, Tofiloski, Voll, & Stede, 2011) with classifiers such as random forest and support vector machines (SVM). Affect in tweets (AIT) is a challenging task as it requires handling an informal writing style, which typically has many grammar mistakes, slangs, and misspellings. In this paper, we present a self-attentive hybrid GRU-based network (SAHGN) that competed at SemEval-2018 Task 1 (Mohammad, BravoMarquez, Salameh, & Kiritchenko, 2018; Mohammad & Kiritchenko, 2018). Our contributions can be summarized as below. • 2 The implementation of a social media text processor: A library to help process social media text such as short"
spracklin-etal-2008-using,I05-1084,0,\N,Missing
spracklin-etal-2008-using,J95-4004,0,\N,Missing
spracklin-etal-2008-using,J00-4001,0,\N,Missing
W02-0909,J93-1003,0,0.178753,"Missing"
W02-0909,J02-2001,1,0.822493,"lexical choice between nearsynonyms. We acquire collocations for the near-synonyms of interest from a corpus (only collocations with the appropriate sense and part-of-speech). For each word that collocates with a near-synonym we use a differential test to learn whether the word forms a less-preferred collocation or an anti-collocation with other near-synonyms in the same cluster. For this task we use a much larger corpus (the Web). We also look at associations (longer-distance co-occurrences) as a possible source of learning more about nuances that the near-synonyms may carry. 1 Introduction Edmonds and Hirst (2002 to appear) developed a lexical choice process for natural language generation (NLG) or machine translation (MT) that can decide which near-synonyms are most appropriate in a particular situation. The lexical choice process has to choose between clusters of near-synonyms (to convey the basic meaning), and then to choose between the near-synonyms in each cluster. To group near-synonyms in clusters we trust lexicographers’ judgment in dictionaries of synonym differences. For example task, job, duty, assignment, chore, stint, hitch all refer to a one-time piece of work, but which one to choose de"
W02-0909,P01-1025,0,0.0389636,"As in our work, three types of collocations are distinguished: words that collocate well; words that tend to not occur together, but if they do the reading is acceptable; and words that must not be used together because the reading will be unnatural (anti-collocations). In a similar manner with (Pearce, 2001), in section 3, we don’t record collocations in our lexical knowledge-base if they don’t help discriminate between near-synonyms. A difference is that we use more than frequency counts to classify collocations (we use a combination of t-test and MI). Our evaluation was partly inspired by Evert and Krenn (2001). They collect collocations of the form noun-adjective and verb-prepositional phrase. They build a solution using two human judges, and use the solution to decide what is the best threshold for taking the N highest-ranked pairs as true collocations. In their experiment MI behaves worse that other measures (LL, t-test), but in our experiment MI on the Web achieves good results. 8 Conclusions and Future Work We presented an unsupervised method to acquire knowledge about the collocational behaviour of near-synonyms. Our future work includes improving the way we combine the five measures for ranki"
W02-0909,W98-1426,0,0.011249,"e the Web as a corpus for this task (and a modified form of mutual information), and we distinguish three types of collocations (preferred, less-preferred, and anticollocations). We are concerned with extracting collocations for use in lexical choice. There is a lot of work on using collocations in NLG (but not in the lexical choice sub-component). There are two typical approaches: the use of phrasal templates in the form of canned phrases, and the use of automatically extracted collocations for unification-based generation (McKeown and Radev, 2000). Statistical NLG systems (such as Nitrogen (Langkilde and Knight, 1998)) make good use of the most frequent words and their collocations. But such a system cannot choose a less-frequent synonym that may be more appropriate for conveying desired nuances of meaning, if the synonym is not a frequent word. Finally, there is work related to ours from the point of view of the synonymy relation. Turney (2001) used mutual information to detect the best answer to questions about synonyms from Test of English as a Foreign Language (TOEFL) and English as a Second Language (ESL). Given a problem word (with or without context), and four alternative words, the question is to c"
W02-0909,J93-1007,0,0.485363,"ur purposes, such as proper names (tagged NP0). If we keep the proper names, they are likely to be among the highest-ranked collocations. There are many statistical methods that can be used to identify collocations. Four general methods are presented by Manning and Sch¨utze (1999). The first one, based on frequency of co-occurrence, 3 http://www.hcu.ox.ac.uk/BNC/ does not consider the length of the corpus. Part-ofspeech filtering is needed to obtain useful collocations. The second method considers the means and variance of the distance between two words, and can compute flexible collocations (Smadja, 1993). The third method is hypothesis testing, which uses statistical tests to decide if the words occur together with probability higher than chance (it tests whether we can reject the null hypothesis that the two words occurred together by chance). The fourth method is (pointwise) mutual information, an informationtheoretical measure. We use Ted Pedersen’s Bigram Statistics Package4 . BSP is a suite of programs to aid in analyzing bigrams in a corpus (newer versions allow Ngrams). The package can compute bigram frequencies and various statistics to measure the degree of association between two wo"
W02-0909,J90-1003,0,\N,Missing
W09-2811,J05-1002,0,0.028718,"Missing"
W10-0205,H05-1073,0,0.215559,"he blog. We selected only the texts corresponding to the six emotions that we mentioned. 3.2 Text Affect Dataset This dataset (Strapparava and Mihalcea, 2007) consists of newspaper headlines that were used in the SemEval 2007-Task 14. It includes a development dataset of 250 annotated headlines, and a test dataset of 1000 news headlines. We use all of them. The annotations were made with the six basic emotions on intensity scales of [-100, 100], therefore a threshold is used to choose the main emotion of each sentence. 3.3 Fairy Tales Dataset This dataset consists in 1580 annotated sentences (Alm et al., 2005), from tales by the Grimm brothers, H.C. Andersen, and B. Potter. The annotations used the extended set of nine basic emotions of Izard (1971). We selected only those marked with the six emotions that we focus on. 3.4 Annotated Blog Dataset We also used the dataset provided by Aman and Szpakowicz (2007). Emotion-rich sentences were selected from personal blogs, and annotated with the six emotions (as well as a non-emotion class, that we ignore here). They worked with blog posts and collected directly from the Web. First, they prepared 1 http://www.livejournalinc.com Dataset LiveJournal TextAff"
W10-0205,P01-1008,0,0.703219,"shop on Computational Approaches to Analysis and Generation of Emotion in Text, pages 35–44, c Los Angeles, California, June 2010. 2010 Association for Computational Linguistics his little boy was so happy to see him princess and she were very glad to visit him Table 1: Two sentence fragments (candidate contexts) from the emotion class happy, from the blog corpus. 2 Related Work Three main approaches for collecting paraphrases were proposed in the literature: manual collection, utilization of existing lexical resources, and corpusbased extraction of expressions that occur in similar contexts (Barzilay and McKeown, 2001). Manuallycollected paraphrases were used in natural language generation (NLG) (Iordanskaja et al., 1991). Langkilde et al. (1998) used lexical resources in statistical sentence generation, summarization, and questionanswering. Barzilay and McKeown (2001) used a corpus-based method to identify paraphrases from a corpus of multiple English translations of the same source text. Our method is similar to this method, but it extracts paraphrases only for a particular emotion, and it needs only a regular corpus, not a parallel corpus of multiple translations. Some research has been done in paraphras"
W10-0205,P09-2063,0,0.0250514,"a paraphrase relationship. They applied a generative model that generates a paraphrase of a given sentence, then used probabilistic inference to reason about whether two sentences share the paraphrase relationship. In another research, Wang et. al (2009) studied the problem of extracting technical paraphrases from a parallel software corpus. Their aim was to report duplicate bugs. In their method for paraphrase extraction, they used: sentence selection, global context-based and cooccurrence-based scoring. Also, some studies have been done in paraphrase generation in NLG (Zhao et al., 2009), (Chevelu et al., 2009). Bootstrapping methods have been applied to various natural language applications, for example to word sense disambiguation (Yarowsky, 1995), lexicon construction for information extraction (Riloff and Jones, 1999), and named entity classification (Collins and Singer, 1999). In our research, we use the bootstrapping approach to learn paraphrases for emotions. 36 3 Data The text data from which we will extract paraphrases is composed of four concatenated datasets. They contain sentences annotated with the six basic emotions. The number of sentences in each dataset is presented in Table 2. We b"
W10-0205,W99-0613,0,0.238345,"blem of extracting technical paraphrases from a parallel software corpus. Their aim was to report duplicate bugs. In their method for paraphrase extraction, they used: sentence selection, global context-based and cooccurrence-based scoring. Also, some studies have been done in paraphrase generation in NLG (Zhao et al., 2009), (Chevelu et al., 2009). Bootstrapping methods have been applied to various natural language applications, for example to word sense disambiguation (Yarowsky, 1995), lexicon construction for information extraction (Riloff and Jones, 1999), and named entity classification (Collins and Singer, 1999). In our research, we use the bootstrapping approach to learn paraphrases for emotions. 36 3 Data The text data from which we will extract paraphrases is composed of four concatenated datasets. They contain sentences annotated with the six basic emotions. The number of sentences in each dataset is presented in Table 2. We briefly describe the datasets, as follows. 3.1 LiveJournal blog dataset We used the blog corpus that Mishne collected for his research (Mishne, 2005). The corpus contains 815,494 blog posts from Livejournal 1 , a free weblog service used by millions of people to create weblog"
W10-0205,P09-1053,0,0.0210332,"aja et al., 1991). Langkilde et al. (1998) used lexical resources in statistical sentence generation, summarization, and questionanswering. Barzilay and McKeown (2001) used a corpus-based method to identify paraphrases from a corpus of multiple English translations of the same source text. Our method is similar to this method, but it extracts paraphrases only for a particular emotion, and it needs only a regular corpus, not a parallel corpus of multiple translations. Some research has been done in paraphrase extraction for natural language processing and generation for different applications. Das and Smith (2009) presented a approach to decide whether two sentences hold a paraphrase relationship. They applied a generative model that generates a paraphrase of a given sentence, then used probabilistic inference to reason about whether two sentences share the paraphrase relationship. In another research, Wang et. al (2009) studied the problem of extracting technical paraphrases from a parallel software corpus. Their aim was to report duplicate bugs. In their method for paraphrase extraction, they used: sentence selection, global context-based and cooccurrence-based scoring. Also, some studies have been d"
W10-0205,P98-1116,0,0.104402,"Missing"
W10-0205,W04-2405,0,0.0120717,"each candidate context, to allow us to determine similar contexts. 4.3 Candidate context: He was further annoyed by the jay bird ’PRP VBD RB VBN IN DT NN NN’,65,8,’VBD RB’,?,was, ?,?,?,He/PRP,was/VBD,further/RB,annoyed,by/IN,the/DT, jay/NN,bird/NN,?,?,jay,?,’IN DT NN’,2,2,0,1 Table 4: An example of extracted features. Feature Extraction Previous research on word sense disambiguation on contextual analysis has acknowledged several local and topical features as good indicators of word properties. These include surrounding words and their part of speech tags, collocations, keywords in contexts (Mihalcea, 2004). Also recently, other features have been proposed: bigrams, named entities, syntactic features, and semantic relations with other words in the context. We transfer the candidate phrases extracted by the sliding k-window into the vector space of features. We consider features that include both lexical and syntactic descriptions of the paraphrases for all pairs of two candidates. The lexical features include the sequence of tokens for each phrase in the paraphrase pair; the syntactic feature consists of a sequence of part-of-speech (PoS) tags where equal words and words with the same root and P"
W10-0205,S07-1013,0,0.0634538,"atasets, as follows. 3.1 LiveJournal blog dataset We used the blog corpus that Mishne collected for his research (Mishne, 2005). The corpus contains 815,494 blog posts from Livejournal 1 , a free weblog service used by millions of people to create weblogs. In Livejournal, users are able to optionally specify their current emotion or mood. To select their emotion/mood users can choose from a list of 132 provided moods. So, the data is annotated by the user who created the blog. We selected only the texts corresponding to the six emotions that we mentioned. 3.2 Text Affect Dataset This dataset (Strapparava and Mihalcea, 2007) consists of newspaper headlines that were used in the SemEval 2007-Task 14. It includes a development dataset of 250 annotated headlines, and a test dataset of 1000 news headlines. We use all of them. The annotations were made with the six basic emotions on intensity scales of [-100, 100], therefore a threshold is used to choose the main emotion of each sentence. 3.3 Fairy Tales Dataset This dataset consists in 1580 annotated sentences (Alm et al., 2005), from tales by the Grimm brothers, H.C. Andersen, and B. Potter. The annotations used the extended set of nine basic emotions of Izard (1971"
W10-0205,strapparava-valitutti-2004-wordnet,0,0.0273606,"Missing"
W10-0205,N03-1033,0,0.00361277,"tion 4.5 we explain how the bootstrapping algorithm processes and selects the paraphrases based on strong surrounding contexts. As it is shown in Figure 1, our method has several stages: extracting candidate contexts, using them to extract patterns, selecting the best patterns, extracting potential paraphrases, and filtering them to obtain the final paraphrases. 4.1 Preprocessing During preprocessing, HTML and XML tags are eliminated from the blogs data and other datasets, then the text is tokenized and annotated with part of speech tags. We use the Stanford part-of-speech tagger and chunker (Toutanova et al., 2003) to identify noun and verb phrases in the sentences. In the next step, we use a sliding window based on the k-window approach, to identify candidate contexts that contain the target seeds. 4.2 The k-window Algorithm We use the k-window algorithm introduced by Bostad (2003) in order to identify all the tokens surrounding a specific term in a window with size of ±k. Here, we use this approach to extract candidate patterns for each seed, from the sentences. We start with one seed and truncate all contexts around the seed within a window of ±k words before and ±k words after the seed, until all th"
W10-0205,P09-2050,0,0.0255268,"Missing"
W10-0205,P95-1026,0,0.0159302,"eason about whether two sentences share the paraphrase relationship. In another research, Wang et. al (2009) studied the problem of extracting technical paraphrases from a parallel software corpus. Their aim was to report duplicate bugs. In their method for paraphrase extraction, they used: sentence selection, global context-based and cooccurrence-based scoring. Also, some studies have been done in paraphrase generation in NLG (Zhao et al., 2009), (Chevelu et al., 2009). Bootstrapping methods have been applied to various natural language applications, for example to word sense disambiguation (Yarowsky, 1995), lexicon construction for information extraction (Riloff and Jones, 1999), and named entity classification (Collins and Singer, 1999). In our research, we use the bootstrapping approach to learn paraphrases for emotions. 36 3 Data The text data from which we will extract paraphrases is composed of four concatenated datasets. They contain sentences annotated with the six basic emotions. The number of sentences in each dataset is presented in Table 2. We briefly describe the datasets, as follows. 3.1 LiveJournal blog dataset We used the blog corpus that Mishne collected for his research (Mishne"
W10-0205,P09-1094,0,0.0237707,"er two sentences hold a paraphrase relationship. They applied a generative model that generates a paraphrase of a given sentence, then used probabilistic inference to reason about whether two sentences share the paraphrase relationship. In another research, Wang et. al (2009) studied the problem of extracting technical paraphrases from a parallel software corpus. Their aim was to report duplicate bugs. In their method for paraphrase extraction, they used: sentence selection, global context-based and cooccurrence-based scoring. Also, some studies have been done in paraphrase generation in NLG (Zhao et al., 2009), (Chevelu et al., 2009). Bootstrapping methods have been applied to various natural language applications, for example to word sense disambiguation (Yarowsky, 1995), lexicon construction for information extraction (Riloff and Jones, 1999), and named entity classification (Collins and Singer, 1999). In our research, we use the bootstrapping approach to learn paraphrases for emotions. 36 3 Data The text data from which we will extract paraphrases is composed of four concatenated datasets. They contain sentences annotated with the six basic emotions. The number of sentences in each dataset is pr"
W10-0205,C98-1112,0,\N,Missing
W10-0217,S07-1094,0,\N,Missing
W10-0217,S07-1067,0,\N,Missing
W10-0217,S07-1013,0,\N,Missing
W10-0217,S07-1072,0,\N,Missing
W10-0217,J09-3003,0,\N,Missing
W10-0217,I08-1041,1,\N,Missing
W10-1912,P04-1055,0,\N,Missing
W11-2826,W09-0613,0,0.236171,"hich helped us to choose parameters that can produce sentences in one of the two styles. We collected some ready-made parallel list of formal and informal words and phrases, from different sources. In addition, we added two more parallel lists: one that contains most of the contractions in English (short forms) and their full forms, and another one that consists in some common abbreviations and their full forms. These parallel lists might help to generate sentences in the preferred style, by changing words or expressions for that style. Our NLG system is built on top of the SimpleNLG package (Gatt and Reiter, 2009). We used templates from which we generated valid English texts with formal or informal style. In order to evaluate the quality of the generated sentences and their level of formality, we used human judges. The evaluation results show that our system can generate formal and informal style successfully, with high accuracy. The main contribution of our work consists in designing a set of parameters that led to good results for the task of generating texts with different formality levels. 1 2 Introduction In this paper, we introduce an important technique that takes into account the differences b"
W11-2826,J09-4008,0,0.0133487,"if the chosen style is Formal, then the system will choose to generate a sentence in passive voice. e. After the sentence is constructed, the system will search for any word, phrase, or expression from the formal/informal list, the abbreviations list, and the contractions list, in order to replace it with a synonym, based on the preferred style. f. Lastly, our system will generate a natural language sentence according to the preferred style, using SimpleNLG for surface realization. 6 Results and Evaluation Natural language generation is most often evaluated using scores given by human judges (Reiter and Belz, 2009). Our evaluation target was to measure the degree of formality (Formal / Informal) of the generated sentences. We asked two human judges (graduate students in computational linguistics, native speakers of English) to annotate 100 generated sentences as having formal or informal style. Table 4 shows samples4 of the generated sentences with the We will make the test set of annotated sentence available, on our website, in case other researchers need them for testing, as well as the three word lists used by our system. 4 judges’ annotations. We estimate the correctness of our system by comparing t"
W12-3711,H05-1073,0,0.145956,"table in this regard are two classes, anger and disgust, which human annotators often find hard to distinguish (Aman and Szpakowicz, 2007). In order to recognize and analyze affect in written text – seldom explicitly marked for emotions – NLP researchers have come up with a variety of techniques, including the use of machine learning, rule-based methods and the lexical approach (Neviarouskaya, Prendinger, and Ishizuka, 2011). There has been previous work using statistical methods and supervised machine learning applied to corpus-based features, mainly unigrams, combined with lexical features (Alm, Roth, and Sproat, 2005; Aman and Szpakowicz, 2007; Katz, Singleton, and Wicentowski, 2007). The weakness of such methods Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 70–78, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics is that they neglect negation, syntactic relations and semantic dependencies. They also require large (annotated) corpora for meaningful statistics and good performance. Processing may take time, and annotation effort is inevitably high. Rule-based methods (Chaumartin, 2007; Neviarouskaya, Prendinge"
W12-3711,S07-1094,0,0.0240853,"ical features (Alm, Roth, and Sproat, 2005; Aman and Szpakowicz, 2007; Katz, Singleton, and Wicentowski, 2007). The weakness of such methods Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 70–78, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics is that they neglect negation, syntactic relations and semantic dependencies. They also require large (annotated) corpora for meaningful statistics and good performance. Processing may take time, and annotation effort is inevitably high. Rule-based methods (Chaumartin, 2007; Neviarouskaya, Prendinger, and Ishizuka, 2011) require manual creation of rules. That is an expensive process with weak guarantee of consistency and coverage, and likely very task-dependent; the set of rules of rule-based affect analysis task (Neviarouskaya, Prendinger, and Ishizuka, 2011) can differ drastically from what underlies other tasks such as rule-based part-of-speech tagger, discourse parsers, word sense disambiguation and machine translation. The study of emotions in lexical semantics was the theme of a SemEval 2007 task (Strapparava and Mihalcea, 2007), carried out in an unsuperv"
W12-3711,H05-1045,0,0.0919912,"Hoffmann, 2009). The prior-polarity subjectivity lexicon contains over 8000 subjectivity clues collected from a number of sources. To create this lexicon, the authors began with the list of subjectivity clues extracted by Riloff (2003). The list was expanded using a dictionary and a thesaurus, and adding positive and negative word lists from the General Inquirer.1 Words are grouped into strong subjective and weak subjective clues; Table 2 presents the distribution of their polarity. The features used in our experiments were motivated both by the literature (Wilson, Wiebe, and Hoffmann, 2009; Choi et al., 2005) and by the exploration of contextual emotion of words in the annotated data. All of the features are counted based on the emotional word from the lexicon which occurs in the sentence. For ease of description, we group the features into four distinct sets: emotion-word features, part-of-speech features, sentence features and dependency-tree features. Emotion-word features. This set of features are based on the emotion-word itself. Intensifier Lexicon (Neviarouskaya, Prendinger, and Ishizuka, 2010). It is a list of 112 modifiers (adverbs). Two annotators gave coefficients for intensity degree –"
W12-3711,esuli-sebastiani-2006-sentiwordnet,0,0.0431294,"ask (Neviarouskaya, Prendinger, and Ishizuka, 2011) can differ drastically from what underlies other tasks such as rule-based part-of-speech tagger, discourse parsers, word sense disambiguation and machine translation. The study of emotions in lexical semantics was the theme of a SemEval 2007 task (Strapparava and Mihalcea, 2007), carried out in an unsupervised setting (Strapparava and Mihalcea, 2008; Chaumartin, 2007; Kozareva et al., 2007; Katz, Singleton, and Wicentowski, 2007). The participants were encouraged to work with WordNet-Affect (Strapparava and Valitutti, 2004) and SentiWordNet (Esuli and Sebastiani, 2006). Word-level analysis, however, will not suffice when affect is expressed by phrases which require complex phrase- and sentence-level analyses: words are interrelated and they mutually influence their affect-related interpretation. On the other hand, words can have more than one sense, and they can only be disambiguated in context. Consequently, the emotion conveyed by a word in a sentence can differ drastically from the emotion of the word on its own. For example, according to the WordNet-Affect lexicon, the word ”afraid” is listed in the ”fear” category, but in the sentence “I am afraid it i"
W12-3711,S07-1067,0,0.0290197,"which human annotators often find hard to distinguish (Aman and Szpakowicz, 2007). In order to recognize and analyze affect in written text – seldom explicitly marked for emotions – NLP researchers have come up with a variety of techniques, including the use of machine learning, rule-based methods and the lexical approach (Neviarouskaya, Prendinger, and Ishizuka, 2011). There has been previous work using statistical methods and supervised machine learning applied to corpus-based features, mainly unigrams, combined with lexical features (Alm, Roth, and Sproat, 2005; Aman and Szpakowicz, 2007; Katz, Singleton, and Wicentowski, 2007). The weakness of such methods Proceedings of the 3rd Workshop on Computational Approaches to Subjectivity and Sentiment Analysis, pages 70–78, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics is that they neglect negation, syntactic relations and semantic dependencies. They also require large (annotated) corpora for meaningful statistics and good performance. Processing may take time, and annotation effort is inevitably high. Rule-based methods (Chaumartin, 2007; Neviarouskaya, Prendinger, and Ishizuka, 2011) require manual creation of rules. That is an"
W12-3711,S07-1072,0,0.021395,"creation of rules. That is an expensive process with weak guarantee of consistency and coverage, and likely very task-dependent; the set of rules of rule-based affect analysis task (Neviarouskaya, Prendinger, and Ishizuka, 2011) can differ drastically from what underlies other tasks such as rule-based part-of-speech tagger, discourse parsers, word sense disambiguation and machine translation. The study of emotions in lexical semantics was the theme of a SemEval 2007 task (Strapparava and Mihalcea, 2007), carried out in an unsupervised setting (Strapparava and Mihalcea, 2008; Chaumartin, 2007; Kozareva et al., 2007; Katz, Singleton, and Wicentowski, 2007). The participants were encouraged to work with WordNet-Affect (Strapparava and Valitutti, 2004) and SentiWordNet (Esuli and Sebastiani, 2006). Word-level analysis, however, will not suffice when affect is expressed by phrases which require complex phrase- and sentence-level analyses: words are interrelated and they mutually influence their affect-related interpretation. On the other hand, words can have more than one sense, and they can only be disambiguated in context. Consequently, the emotion conveyed by a word in a sentence can differ drastically f"
W12-3711,de-marneffe-etal-2006-generating,0,0.00637076,"tanford tagger’s output (Toutanova et al., 2003), every word in a sentence gets one of the Penn Treebank tags. • The part-of-speech of the emotional word itself, both according to the emotion lexicon and Stanford tagger. • The POS of neighbouring words in the same sentence. We choose a window of [-2,2], as it is usually suggested by the literature (Choi et al., 2005). Sentence features. For now we only consider the number of words in the sentence. Dependency-tree features. For each emotional word, we create features based on the parse tree and its dependencies produced by the Stanford parser (Marneffe, Maccartney, and Manning, 2006). The dependencies are all binary relations: a grammatical relation holds between a governor (head) and a dependent (modifier). According to Mohammad and Turney (2010),2 adverbs and adjectives are some of the most emotion-inspiring terms. This is not surprising considering that they are used to qualify a noun or a verb; therefore to keep the number of features small, among all the 52 different type of dependencies, we only chose the negation, adverb and adjective modifier dependencies. After parsing the sentence and getting the dependencies, we count the following dependency-tree Boolean feat"
W12-3711,W10-0204,0,0.0234347,"he emotion lexicon and Stanford tagger. • The POS of neighbouring words in the same sentence. We choose a window of [-2,2], as it is usually suggested by the literature (Choi et al., 2005). Sentence features. For now we only consider the number of words in the sentence. Dependency-tree features. For each emotional word, we create features based on the parse tree and its dependencies produced by the Stanford parser (Marneffe, Maccartney, and Manning, 2006). The dependencies are all binary relations: a grammatical relation holds between a governor (head) and a dependent (modifier). According to Mohammad and Turney (2010),2 adverbs and adjectives are some of the most emotion-inspiring terms. This is not surprising considering that they are used to qualify a noun or a verb; therefore to keep the number of features small, among all the 52 different type of dependencies, we only chose the negation, adverb and adjective modifier dependencies. After parsing the sentence and getting the dependencies, we count the following dependency-tree Boolean features for the emotional word. • Whether the word is in a “neg” dependency (negation modifier): true when there is a negation word which modifies the emotional word. • Wh"
W12-3711,W02-1011,0,0.0277354,"iological activity. It is only recently that there has been a growing interest in automatic identification and extraction of sentiment, opinions and emotions in text. Sentiment analysis is the task of identifying positive and negative opinions, emotions and evaluations (Wilson, Wiebe, and Hoffmann, 2005). Most of the current work in sentiment analysis has focused on 70 determining the presence of sentiment in the given text, and on determining its polarity – the positive or negative orientation. The applications of sentiment analysis range from classifying positive and negative movie reviews (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002) to opinion question-answering (Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie, and Wiebe, 2005). The analysis of sentiment must, however, go beyond differentiating positive from negative emotions to give a systematic account of the qualitative differences among individual emotion (Ortony, Collins, and Clore, 1988). In this work, we deal with assigning fine-grained emotion classes to sentences in text. It might seem that these two tasks are strongly tied, but the higher level of classification in emotion recognition task and the presence of certain degrees of similarities betwe"
W12-3711,W03-1014,0,0.20239,"Missing"
W12-3711,H05-1116,0,0.0181966,"xtraction of sentiment, opinions and emotions in text. Sentiment analysis is the task of identifying positive and negative opinions, emotions and evaluations (Wilson, Wiebe, and Hoffmann, 2005). Most of the current work in sentiment analysis has focused on 70 determining the presence of sentiment in the given text, and on determining its polarity – the positive or negative orientation. The applications of sentiment analysis range from classifying positive and negative movie reviews (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002) to opinion question-answering (Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie, and Wiebe, 2005). The analysis of sentiment must, however, go beyond differentiating positive from negative emotions to give a systematic account of the qualitative differences among individual emotion (Ortony, Collins, and Clore, 1988). In this work, we deal with assigning fine-grained emotion classes to sentences in text. It might seem that these two tasks are strongly tied, but the higher level of classification in emotion recognition task and the presence of certain degrees of similarities between some emotion labels make categorization into distinct emotion classes more challenging and difficult. Partic"
W12-3711,S07-1013,0,0.0352476,"rt is inevitably high. Rule-based methods (Chaumartin, 2007; Neviarouskaya, Prendinger, and Ishizuka, 2011) require manual creation of rules. That is an expensive process with weak guarantee of consistency and coverage, and likely very task-dependent; the set of rules of rule-based affect analysis task (Neviarouskaya, Prendinger, and Ishizuka, 2011) can differ drastically from what underlies other tasks such as rule-based part-of-speech tagger, discourse parsers, word sense disambiguation and machine translation. The study of emotions in lexical semantics was the theme of a SemEval 2007 task (Strapparava and Mihalcea, 2007), carried out in an unsupervised setting (Strapparava and Mihalcea, 2008; Chaumartin, 2007; Kozareva et al., 2007; Katz, Singleton, and Wicentowski, 2007). The participants were encouraged to work with WordNet-Affect (Strapparava and Valitutti, 2004) and SentiWordNet (Esuli and Sebastiani, 2006). Word-level analysis, however, will not suffice when affect is expressed by phrases which require complex phrase- and sentence-level analyses: words are interrelated and they mutually influence their affect-related interpretation. On the other hand, words can have more than one sense, and they can only"
W12-3711,strapparava-valitutti-2004-wordnet,0,0.475248,"Missing"
W12-3711,N03-1033,0,0.0404741,"Missing"
W12-3711,P02-1053,0,0.00534146,"tly that there has been a growing interest in automatic identification and extraction of sentiment, opinions and emotions in text. Sentiment analysis is the task of identifying positive and negative opinions, emotions and evaluations (Wilson, Wiebe, and Hoffmann, 2005). Most of the current work in sentiment analysis has focused on 70 determining the presence of sentiment in the given text, and on determining its polarity – the positive or negative orientation. The applications of sentiment analysis range from classifying positive and negative movie reviews (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002) to opinion question-answering (Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie, and Wiebe, 2005). The analysis of sentiment must, however, go beyond differentiating positive from negative emotions to give a systematic account of the qualitative differences among individual emotion (Ortony, Collins, and Clore, 1988). In this work, we deal with assigning fine-grained emotion classes to sentences in text. It might seem that these two tasks are strongly tied, but the higher level of classification in emotion recognition task and the presence of certain degrees of similarities between some emotion"
W12-3711,H05-1044,0,0.0403733,"emotion, with a focus on exploring features important for this task. 1 Introduction Recognition, interpretation and representation of affect have been investigated by researchers in the field of affective computing (Picard 1997). They consider a wide range of modalities such as affect in speech, facial display, posture and physiological activity. It is only recently that there has been a growing interest in automatic identification and extraction of sentiment, opinions and emotions in text. Sentiment analysis is the task of identifying positive and negative opinions, emotions and evaluations (Wilson, Wiebe, and Hoffmann, 2005). Most of the current work in sentiment analysis has focused on 70 determining the presence of sentiment in the given text, and on determining its polarity – the positive or negative orientation. The applications of sentiment analysis range from classifying positive and negative movie reviews (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002) to opinion question-answering (Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie, and Wiebe, 2005). The analysis of sentiment must, however, go beyond differentiating positive from negative emotions to give a systematic account of the qualitative differenc"
W12-3711,J09-3003,0,0.0606315,"Missing"
W12-3711,W03-1017,0,0.0953362,"automatic identification and extraction of sentiment, opinions and emotions in text. Sentiment analysis is the task of identifying positive and negative opinions, emotions and evaluations (Wilson, Wiebe, and Hoffmann, 2005). Most of the current work in sentiment analysis has focused on 70 determining the presence of sentiment in the given text, and on determining its polarity – the positive or negative orientation. The applications of sentiment analysis range from classifying positive and negative movie reviews (Pang, Lee, and Vaithyanathan, 2002; Turney, 2002) to opinion question-answering (Yu and Hatzivassiloglou, 2003; Stoyanov, Cardie, and Wiebe, 2005). The analysis of sentiment must, however, go beyond differentiating positive from negative emotions to give a systematic account of the qualitative differences among individual emotion (Ortony, Collins, and Clore, 1988). In this work, we deal with assigning fine-grained emotion classes to sentences in text. It might seem that these two tasks are strongly tied, but the higher level of classification in emotion recognition task and the presence of certain degrees of similarities between some emotion labels make categorization into distinct emotion classes mor"
W12-3711,baccianella-etal-2010-sentiwordnet,0,\N,Missing
W12-3711,W10-0210,0,\N,Missing
W15-0509,W11-1701,0,0.0355377,"rvised learning has been used in almost all of the current approaches for stance classification, in which a large set of data has been collected and annotated in order to be used as training data for classifiers. In (Somasundaran and Wiebe, 2010), a lexicon for detecting argument trigger expressions was created and subsequently leveraged to identify arguments. 1 https://github.com/parinaz1366/News-Comments-BreastCancer-Screening-v1 68 These extracted arguments together with sentiment expressions and their targets were employed in a supervised learner as features for stance classification. In (Anand et al., 2011), several features were deployed in their rule-based classifier, such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreement and disagreements between posts were exploited in (Walker et al., 2012b),(Ghosh et al., 2014), likewise; while in this paper our aim is to investigate stance without considering the conversational structure which is not always available. Argument Tagging In (Albert et al., 2011), argument mining for reviews was introduced in order to extract the reasons for positive or negative opinions"
W15-0509,W14-2107,0,0.654578,"and Wiebe, 2005). Argumentation analysis is more focused on the reason for author’s overall position. Stance has been defined as the overall position toward an idea, object or proposition (Somasundaran and Wiebe, 2010). There has been growing interest in stance classification particularly for online debates (Walker et al., 2012a), (Hasan and Ng, 2013). To the best of our knowledge, our paper is the first work for stance classification of the news comments considering particular news as target to investigate the overall position toward it. Argument tagging was first introduced as a task in ˇ (Boltuzic and Snajder, 2014) in which the arguments were identified from a domain-dependent predefined list of arguments. An argument tag is a controversial aspect in the domain that is abstracted by a representative phrase/sentence (Conrad et al., 2012). In our paper, a new framework for argument tag67 Proceedings of the 2nd Workshop on Argumentation Mining, pages 67–77, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ging at document-level based on topic modeling, mainly Non-Negative Matrix Factorization, is proposed. The main advantage of this framework is that it is minimally supervis"
W15-0509,W12-3810,0,0.0689037,"Missing"
W15-0509,W14-2106,0,0.0518432,"created and subsequently leveraged to identify arguments. 1 https://github.com/parinaz1366/News-Comments-BreastCancer-Screening-v1 68 These extracted arguments together with sentiment expressions and their targets were employed in a supervised learner as features for stance classification. In (Anand et al., 2011), several features were deployed in their rule-based classifier, such as unigrams, bigrams, punctuation marks, syntactic dependencies and the dialogic structure of the posts. The dialogic relations of agreement and disagreements between posts were exploited in (Walker et al., 2012b),(Ghosh et al., 2014), likewise; while in this paper our aim is to investigate stance without considering the conversational structure which is not always available. Argument Tagging In (Albert et al., 2011), argument mining for reviews was introduced in order to extract the reasons for positive or negative opinions. Argumentation analysis can be applied at different text granularities. In (Conrad et al., 2012), a model for argument detection and tagging at sentence-level was suggested. In our research, argument tags were organized in a hierarchical structure inspired by a related field in political science “Argui"
W15-0509,W13-3514,0,0.0173016,"online discussions opinion mining where comments not only contain the sentiment/stance of the commenter toward the target, but also convey personal beliefs about what is true or what action should be taken. This kind of subjectivity is called argumentation (Wilson and Wiebe, 2005). Argumentation analysis is more focused on the reason for author’s overall position. Stance has been defined as the overall position toward an idea, object or proposition (Somasundaran and Wiebe, 2010). There has been growing interest in stance classification particularly for online debates (Walker et al., 2012a), (Hasan and Ng, 2013). To the best of our knowledge, our paper is the first work for stance classification of the news comments considering particular news as target to investigate the overall position toward it. Argument tagging was first introduced as a task in ˇ (Boltuzic and Snajder, 2014) in which the arguments were identified from a domain-dependent predefined list of arguments. An argument tag is a controversial aspect in the domain that is abstracted by a representative phrase/sentence (Conrad et al., 2012). In our paper, a new framework for argument tag67 Proceedings of the 2nd Workshop on Argumentation M"
W15-0509,D14-1083,0,0.0997999,"cted by a representative phrase/sentence (Conrad et al., 2012). In our paper, a new framework for argument tag67 Proceedings of the 2nd Workshop on Argumentation Mining, pages 67–77, c Denver, Colorado, June 4, 2015. 2015 Association for Computational Linguistics ging at document-level based on topic modeling, mainly Non-Negative Matrix Factorization, is proposed. The main advantage of this framework is that it is minimally supervised and no labeled data is required. The correlation between stance labels and argument tags has been addressed in different studies ˇ (Boltuzic and Snajder, 2014) (Hasan and Ng, 2014). In our research, a statistical model for stance classification based on the extracted arguments is suggested, while in previous research stance labels were exploited for argument tagging. Nowadays, several popular news websites like CNN and BBC allow their readers to express their opinion by commenting; these kinds of commentspheres can be considered as type of social media. Consequently, visualizing and summarizing the content of these data can play a significant role in public opinion mining and decision making. Considering the huge volume of the news comments that are generated every day,"
W15-0509,W14-2111,0,0.0280534,"Missing"
W15-0509,W06-2915,0,0.0124641,"e unfeasible. In our research, a corpus of news comments is collected and annotated and is made available to be deployed as a benchmark in this field 1 . Hence, it provides opportunities to further investigate automatic analysis of such types of UGC. 2 Related Work In (Somasundaran et al., 2007), two types of opinions are considered: sentiment and arguments. While sentiment mainly includes emotions, evaluations, feelings and stances, arguments are focused on convictions and persuasion. Stance Classification One of the first works related to stance classification is perspective identification (Lin et al., 2006), where this task was defined as subjective evaluation of points of view. Supervised learning has been used in almost all of the current approaches for stance classification, in which a large set of data has been collected and annotated in order to be used as training data for classifiers. In (Somasundaran and Wiebe, 2010), a lexicon for detecting argument trigger expressions was created and subsequently leveraged to identify arguments. 1 https://github.com/parinaz1366/News-Comments-BreastCancer-Screening-v1 68 These extracted arguments together with sentiment expressions and their targets wer"
W15-0509,P05-1015,0,0.061276,"is generate every day on the web. As the volume of this unstructured data increases, the request for automatically processing UGC grows significantly. Moreover, this new source of information and opinions contains valuable feedback about products, services, policies, and news and can play an important role in decision making for marketers, politicians, policy makers and even for ordinary people. So far, there has been a great effort toward subjectivity analysis of sentiment and opinion mining of reviews on concrete entities such as product or movies (Pang et al., 2002), (Dave et al., 2003), (Pang and Lee, 2005); however, this line of research does not fit online discussions opinion mining where comments not only contain the sentiment/stance of the commenter toward the target, but also convey personal beliefs about what is true or what action should be taken. This kind of subjectivity is called argumentation (Wilson and Wiebe, 2005). Argumentation analysis is more focused on the reason for author’s overall position. Stance has been defined as the overall position toward an idea, object or proposition (Somasundaran and Wiebe, 2010). There has been growing interest in stance classification particularly"
W15-0509,W02-1011,0,0.0158878,"d. Consequently, huge amounts of text data is generate every day on the web. As the volume of this unstructured data increases, the request for automatically processing UGC grows significantly. Moreover, this new source of information and opinions contains valuable feedback about products, services, policies, and news and can play an important role in decision making for marketers, politicians, policy makers and even for ordinary people. So far, there has been a great effort toward subjectivity analysis of sentiment and opinion mining of reviews on concrete entities such as product or movies (Pang et al., 2002), (Dave et al., 2003), (Pang and Lee, 2005); however, this line of research does not fit online discussions opinion mining where comments not only contain the sentiment/stance of the commenter toward the target, but also convey personal beliefs about what is true or what action should be taken. This kind of subjectivity is called argumentation (Wilson and Wiebe, 2005). Argumentation analysis is more focused on the reason for author’s overall position. Stance has been defined as the overall position toward an idea, object or proposition (Somasundaran and Wiebe, 2010). There has been growing int"
W15-0509,W10-0214,0,0.755066,"te entities such as product or movies (Pang et al., 2002), (Dave et al., 2003), (Pang and Lee, 2005); however, this line of research does not fit online discussions opinion mining where comments not only contain the sentiment/stance of the commenter toward the target, but also convey personal beliefs about what is true or what action should be taken. This kind of subjectivity is called argumentation (Wilson and Wiebe, 2005). Argumentation analysis is more focused on the reason for author’s overall position. Stance has been defined as the overall position toward an idea, object or proposition (Somasundaran and Wiebe, 2010). There has been growing interest in stance classification particularly for online debates (Walker et al., 2012a), (Hasan and Ng, 2013). To the best of our knowledge, our paper is the first work for stance classification of the news comments considering particular news as target to investigate the overall position toward it. Argument tagging was first introduced as a task in ˇ (Boltuzic and Snajder, 2014) in which the arguments were identified from a domain-dependent predefined list of arguments. An argument tag is a controversial aspect in the domain that is abstracted by a representative phr"
W15-0509,2007.sigdial-1.5,0,0.00832406,"these kinds of commentspheres can be considered as type of social media. Consequently, visualizing and summarizing the content of these data can play a significant role in public opinion mining and decision making. Considering the huge volume of the news comments that are generated every day, manual analysis of these data may be unfeasible. In our research, a corpus of news comments is collected and annotated and is made available to be deployed as a benchmark in this field 1 . Hence, it provides opportunities to further investigate automatic analysis of such types of UGC. 2 Related Work In (Somasundaran et al., 2007), two types of opinions are considered: sentiment and arguments. While sentiment mainly includes emotions, evaluations, feelings and stances, arguments are focused on convictions and persuasion. Stance Classification One of the first works related to stance classification is perspective identification (Lin et al., 2006), where this task was defined as subjective evaluation of points of view. Supervised learning has been used in almost all of the current approaches for stance classification, in which a large set of data has been collected and annotated in order to be used as training data for c"
W15-0509,P08-1036,0,0.0477567,"2008). In (Hasan and Ng, 2014), a reason classifier for online ideological debates is proposed. In this method documentlevel reason classification is leveraged by aggregating all sentence-level reasons of a post. Our proposed method tags arguments at document-level and unlike previous works is minimally supervised. Topic Modeling Topic modeling in more informal documents is more challenging due to the less organized and unedited style of these documents. Topic-modeling has been used in sentimental analysis and opinion mining to simultaneous investigate the topics and the sentiments in a text (Titov and McDonald, 2008a), (Mei et al., 2007). One of the most popular approaches for topic modeling is Latent Dirichlet allocation (LDA) (Blei et al., 2003). This probabilistic model has been extended in (Titov and McDonald, 2008b) to jointly model sentiments and topics in an unsupervised approach. LDA topic modeling was also employed for automatic identification of argument structure in formal documents of 19th century philosophical texts (Lawrence et al., 2014). LDA was applied on the target corpus and the resulting topics were exploited to find similarities between the different propositions. Non-Negative Matrix"
W15-0509,N12-1072,0,0.698248,"of research does not fit online discussions opinion mining where comments not only contain the sentiment/stance of the commenter toward the target, but also convey personal beliefs about what is true or what action should be taken. This kind of subjectivity is called argumentation (Wilson and Wiebe, 2005). Argumentation analysis is more focused on the reason for author’s overall position. Stance has been defined as the overall position toward an idea, object or proposition (Somasundaran and Wiebe, 2010). There has been growing interest in stance classification particularly for online debates (Walker et al., 2012a), (Hasan and Ng, 2013). To the best of our knowledge, our paper is the first work for stance classification of the news comments considering particular news as target to investigate the overall position toward it. Argument tagging was first introduced as a task in ˇ (Boltuzic and Snajder, 2014) in which the arguments were identified from a domain-dependent predefined list of arguments. An argument tag is a controversial aspect in the domain that is abstracted by a representative phrase/sentence (Conrad et al., 2012). In our paper, a new framework for argument tag67 Proceedings of the 2nd Wor"
W15-0509,walker-etal-2012-corpus,0,0.481545,"of research does not fit online discussions opinion mining where comments not only contain the sentiment/stance of the commenter toward the target, but also convey personal beliefs about what is true or what action should be taken. This kind of subjectivity is called argumentation (Wilson and Wiebe, 2005). Argumentation analysis is more focused on the reason for author’s overall position. Stance has been defined as the overall position toward an idea, object or proposition (Somasundaran and Wiebe, 2010). There has been growing interest in stance classification particularly for online debates (Walker et al., 2012a), (Hasan and Ng, 2013). To the best of our knowledge, our paper is the first work for stance classification of the news comments considering particular news as target to investigate the overall position toward it. Argument tagging was first introduced as a task in ˇ (Boltuzic and Snajder, 2014) in which the arguments were identified from a domain-dependent predefined list of arguments. An argument tag is a controversial aspect in the domain that is abstracted by a representative phrase/sentence (Conrad et al., 2012). In our paper, a new framework for argument tag67 Proceedings of the 2nd Wor"
W15-0509,W05-0308,0,0.0321935,"king for marketers, politicians, policy makers and even for ordinary people. So far, there has been a great effort toward subjectivity analysis of sentiment and opinion mining of reviews on concrete entities such as product or movies (Pang et al., 2002), (Dave et al., 2003), (Pang and Lee, 2005); however, this line of research does not fit online discussions opinion mining where comments not only contain the sentiment/stance of the commenter toward the target, but also convey personal beliefs about what is true or what action should be taken. This kind of subjectivity is called argumentation (Wilson and Wiebe, 2005). Argumentation analysis is more focused on the reason for author’s overall position. Stance has been defined as the overall position toward an idea, object or proposition (Somasundaran and Wiebe, 2010). There has been growing interest in stance classification particularly for online debates (Walker et al., 2012a), (Hasan and Ng, 2013). To the best of our knowledge, our paper is the first work for stance classification of the news comments considering particular news as target to investigate the overall position toward it. Argument tagging was first introduced as a task in ˇ (Boltuzic and Snaj"
W15-1527,I08-1041,0,0.018264,"ers can refer to the work of Hinton et al. (2006), Bengio and Lamblin (2007), Vincent et al. (2008) and the introduction by Bengio et al. (2013). 2.3 Deep Neural Networks Applied to NLP Data representation is important for machine learning (Domingos, 2012). Many statistical NLP tasks use hand-crafted features to represent language units such as words and documents; these features are fed as the input to machine learning models. One such example is emotion or sentiment classification which uses external lexicons that contain words with emotion or sentiment prior polarities (Ghazi et al., 2014; Aman and Szpakowicz, 2008; Melville et al., 2009; Li et al., 2009). Despite the usefulness of these hand-crafted features, designing them is timeconsuming and requires expertise. A number of researchers have implemented DNNs in the NLP domain, achieving state-of-the-art performance without having to manually design any features. The most relevant to ours is the work of Glorot et al. (2011), who developed a deep learning architecture that consists of stacked denoising autoencoders (SDA) and apply it to sentiment classification of Amazon reviews. Their stacked denoising auto-encoders can capture meaningful representatio"
W15-1527,D10-1124,0,0.633517,"Missing"
W15-1527,W10-2604,0,0.0742246,"Missing"
W15-1527,P09-1028,0,0.0301067,"Bengio and Lamblin (2007), Vincent et al. (2008) and the introduction by Bengio et al. (2013). 2.3 Deep Neural Networks Applied to NLP Data representation is important for machine learning (Domingos, 2012). Many statistical NLP tasks use hand-crafted features to represent language units such as words and documents; these features are fed as the input to machine learning models. One such example is emotion or sentiment classification which uses external lexicons that contain words with emotion or sentiment prior polarities (Ghazi et al., 2014; Aman and Szpakowicz, 2008; Melville et al., 2009; Li et al., 2009). Despite the usefulness of these hand-crafted features, designing them is timeconsuming and requires expertise. A number of researchers have implemented DNNs in the NLP domain, achieving state-of-the-art performance without having to manually design any features. The most relevant to ours is the work of Glorot et al. (2011), who developed a deep learning architecture that consists of stacked denoising autoencoders (SDA) and apply it to sentiment classification of Amazon reviews. Their stacked denoising auto-encoders can capture meaningful representations from reviews and outperform state-of-t"
W15-1527,N13-1039,0,0.0371511,"f Bag-of-N-grams frequency feature vectors. We did not use binary feature vectors because we believe the frequency of ngrams is relevant to the task at hand. For example, a user who tweets Senators 10 times is more likely to be from Ottawa than another user who tweets it just once. (The latter is more likely to be someone from Montreal who tweets Senators simply because the Canadiens happen to be defeated by the Senators that time.) Due to computational limitations, we consider only the 5000 most frequent unigrams, bigrams and trigrams4 . We tokenized the tweets using the Twokenizer tool from Owoputi et al. (2013). 3.4 Statistical Noises for Denoising Auto-encoders An essential component of a DA is its statistical noise. Following Glorot et al. (2011), the statistical noise we incorporate for the first layer of DA is the masking noise, i.e., each active element has a probability to become inactive. For the remaining layers, we apply Gaussian noise to each of them, i.e., a number independently sampled from the Gaussian distribution N (0, σ 2 ) is added to each element of the input vector to get the corrupted input vector. Note that the Gaussian distribution has a 0 mean. The standard deviation of the Ga"
W15-1527,D12-1137,0,0.723328,"stein et al. (2010) adopted a topic model approach. They treated tweets as documents generated by two latent variables, i.e., topic and region, and train a system they call geographic topic model, which could predict authors’ locations based on text alone. Like Cheng et al. (2010), their model also relied on learning regional word distributions. The average distance from the model’s prediction to the actual location is 900 kilometres. By comparison, their dataset is much smaller, containing 380,000 tweets from 9,500 users. This dataset is made available and has been used by a number of works. Roller et al. (2012) used a variant of K-Nearest Neighbours (kNN); they divided the geographic surface of the Earth into grids and then constructed a pseudo-document for each grid; a location for a test document was chosen based on the most similar pseudo-document. Another type of model is a variant of Gaussian mixture models (GMMs) proposed by Priedhorsky et al. (2014). Their approach 202 resembles that of Cheng et al. (2010) in constructing location-sensitive n-grams; besides tweets, they also used information such as users’ self-reported locations and time zones for prediction. 2.2 Deep Neural Networks In this"
W15-1527,P11-1096,0,0.481768,"Missing"
W15-2916,D09-1020,0,0.0945782,"different. The meaning of a word in a particular usage can only be determined by examining its context. Word Sense Disambiguation (WSD) is the process of identifying the sense of a polysemic word1 . Different approaches to WSD (Mihalcea, 2010) include knowledge-based systems such as Lesk algorithm and adapted Lesk algorithm (Banerjee and Pederson, 2002), unsupervised corpus-based systems (Schutze, 1998; Ng, Wang, and Chan, 2003), and supervised corpusbased systems (Chklovski and Mihalcea, 2002). Subjectivity Word Sense Disambiguation (SWSD) was shown to improve contextual opinion analysis by Akkaya et al. (2009). The authors state that SWSD is midway between pure dictionary classification and pure contextual interpretation. For SWSD, the context of the word is considered in order to perform the task, but the subjectivity is determined solely by the dictionary. A supervised learning approach was used, in which a different classifier was trained for each lexicon entry for which training data was present. Thus, they described their work as similar to targeted WSD, with two labels Subjective (S) and Objective (O). By applying SWSD to contextual polarity classification (positive/negative/neutral), This sh"
W15-2916,baccianella-etal-2010-sentiwordnet,0,0.0402091,"dard datasets show the state-of-the-art performance of Babelfy, as well as its robustness across languages. Its evaluation also demonstrates that Babelfy fares well both on long texts, such as those of the WSD tasks, and short and highly-ambiguous sentences, such as the ones in KORE50.3 3 Positive Table 1: Dataset Class Distribution. • Given an input text, it extracts all the linkable fragments from this text and, for each of them, lists the possible meanings according to the semantic network. 2 Dataset Lexicons Our system made use of a single lexical resource described below: • SentiWordNet (Baccianella et al., 2010) is a lexical resource for opinion mining. SentiWordNet assigns to each synset of WordNet three sentiment scores: positivity, negativity, objectivity; that is, SentiWordNet contains positivity, negativity, and objectivity scores for each sense of a word, totally adding up to 1.0 for every sense of the word. http://babelfy.org/about http://www.yovisto.com/labs/ner-benchmarks/ 4 116 http://www.cs.york.ac.uk/semeval-2013/task2 3.2 Features • Matching the disambiguated word senses for each term with the Positive (P), Negative (N) and Objective/Neutral (O) scores from the matching sense of that ter"
W15-2916,W02-0109,0,0.00869612,"meval-2013/task2 3.2 Features • Matching the disambiguated word senses for each term with the Positive (P), Negative (N) and Objective/Neutral (O) scores from the matching sense of that term, using SentiWordNet. The total P, N and O scores for the text are calculated as described in the previous section. We used the tokenizer of the Carnegie Mellon University (CMU) Twitter NLP tool (Gimpel et al., 2011) to tokenize the training and testing data. We also performed more pre-processing such as stop-word removal and word stemming using the tools provided by the NLTK: the Natural Language Toolkit (Loper and Bird, 2002). Additionally, we used word segmentation for hashtags (starting with #) and user-ids (starting with @) reinserted them after segmentation. Each tweet or SMS text was represented as a vector made up of three features: The output of the above phase is the three-featured vector representation of each tweet or SMS text message. We subsequently use supervision to make the system learn how to combine these three numeric features, representing each text, and reach a decision on the sentiment of that text. Thus, we repeat the above process and construct a three-featured vector (P, N and O scores) rep"
W15-2916,W12-2104,0,0.110587,"Missing"
W15-2916,S13-2053,0,0.0780751,"orrect POS) NRC-Canada (All unigram features) Our System F-Score 57.30 41.90 77.10 Table 4: Results for SMS test data, for each class. Tweets 29.19 SMS 19.03 34.65 29.75 39.61 39.29 50.75 49.60 Table 5: Comparison of Average F-scores for positive/negative classes. All scores reported are for the test datasets Our main focus is to show whether Word Sense Disambiguation helps improve sentiment analysis of micropost data. Therefore, we have evaluated our system using only unigram lexicons and compared our results with that of the allunigram-features results of the system developed by NRC-Canada (Mohammad et al., 2013), that was ranked first in the same task in the SemEval 2013 competition6 . These unigram features included punctuation, upper-case words, POS tags, Table 5 also shows baseline results (Baseline 1) obtained by a majority classifier that always predicts the most frequent class as output. Since the final Average F-score is based only on the F-scores of positive and negative classes and not on neutral, the majority baseline shown, chose the most frequent class among positive and negative, which in this case was the positive class. The results shown in Baseline 2 are obtained for an similar system"
W15-2916,W02-0817,0,0.027134,"levels of accuracy. The problem is that words often have more than one meaning, sometimes fairly similar and sometimes completely different. The meaning of a word in a particular usage can only be determined by examining its context. Word Sense Disambiguation (WSD) is the process of identifying the sense of a polysemic word1 . Different approaches to WSD (Mihalcea, 2010) include knowledge-based systems such as Lesk algorithm and adapted Lesk algorithm (Banerjee and Pederson, 2002), unsupervised corpus-based systems (Schutze, 1998; Ng, Wang, and Chan, 2003), and supervised corpusbased systems (Chklovski and Mihalcea, 2002). Subjectivity Word Sense Disambiguation (SWSD) was shown to improve contextual opinion analysis by Akkaya et al. (2009). The authors state that SWSD is midway between pure dictionary classification and pure contextual interpretation. For SWSD, the context of the word is considered in order to perform the task, but the subjectivity is determined solely by the dictionary. A supervised learning approach was used, in which a different classifier was trained for each lexicon entry for which training data was present. Thus, they described their work as similar to targeted WSD, with two labels Subje"
W15-2916,Q14-1019,0,0.014625,"Sentiment and Social Media Analysis (WASSA 2015), pages 115–121, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. 2 they observed an accuracy improvement of 3 percentage points over the original classifier (Wilson et al., 2005a) calculated on the SenMPQA dataset. Additionally, Rentoumi et al. (2009) showed that WSD is valuable in polarity classification of sentences containing figurative expressions. It should be noted that the above work did not focus on using WSD for social-media or micropost data, which is the primary focus area of our work. Babelfy (Moro et al., 2014)2 is a unified, multilingual, graph-based approach to Entity Linking and Word Sense Disambiguation based on a loose identification of candidate meanings coupled with a densest sub-graph heuristic which selects highcoherence semantic interpretations. We have used Babelfy for WSD in our work. Babelfy is based on the BabelNet 3.0 multilingual semantic network (Navigli and Ponzetto, 2012), and jointly performs WSD and entity linking in three steps: We used the Dataset from Conference on Semantic Evaluation Exercises (SemEval-2013) (Wilson et al., 2013)4 for Task 2: Sentiment Analysis in Twitter an"
W15-2916,P10-1023,0,0.0601554,"Missing"
W15-2916,P11-2008,0,0.106203,"Missing"
W15-2916,P03-1058,0,0.0379827,"hich predict the syntactic category of words in text with high levels of accuracy. The problem is that words often have more than one meaning, sometimes fairly similar and sometimes completely different. The meaning of a word in a particular usage can only be determined by examining its context. Word Sense Disambiguation (WSD) is the process of identifying the sense of a polysemic word1 . Different approaches to WSD (Mihalcea, 2010) include knowledge-based systems such as Lesk algorithm and adapted Lesk algorithm (Banerjee and Pederson, 2002), unsupervised corpus-based systems (Schutze, 1998; Ng, Wang, and Chan, 2003), and supervised corpusbased systems (Chklovski and Mihalcea, 2002). Subjectivity Word Sense Disambiguation (SWSD) was shown to improve contextual opinion analysis by Akkaya et al. (2009). The authors state that SWSD is midway between pure dictionary classification and pure contextual interpretation. For SWSD, the context of the word is considered in order to perform the task, but the subjectivity is determined solely by the dictionary. A supervised learning approach was used, in which a different classifier was trained for each lexicon entry for which training data was present. Thus, they de"
W15-2916,R09-1067,0,0.0896672,"entiment analysis finds applications in various domains such as marketing, business and commerce (Jansen et al., 2009), healthcare (Chew and Eysenbach, 2010; Salathe 1 As described in http://aclweb.org 115 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 115–121, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. 2 they observed an accuracy improvement of 3 percentage points over the original classifier (Wilson et al., 2005a) calculated on the SenMPQA dataset. Additionally, Rentoumi et al. (2009) showed that WSD is valuable in polarity classification of sentences containing figurative expressions. It should be noted that the above work did not focus on using WSD for social-media or micropost data, which is the primary focus area of our work. Babelfy (Moro et al., 2014)2 is a unified, multilingual, graph-based approach to Entity Linking and Word Sense Disambiguation based on a loose identification of candidate meanings coupled with a densest sub-graph heuristic which selects highcoherence semantic interpretations. We have used Babelfy for WSD in our work. Babelfy is based on the BabelN"
W15-2916,J98-1004,0,0.0549213,"Missing"
W15-2916,H05-1044,0,0.0329247,"ng to determine and dig subjective information from source materials. Sentiment analysis finds applications in various domains such as marketing, business and commerce (Jansen et al., 2009), healthcare (Chew and Eysenbach, 2010; Salathe 1 As described in http://aclweb.org 115 Proceedings of the 6th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA 2015), pages 115–121, c Lisboa, Portugal, 17 September, 2015. 2015 Association for Computational Linguistics. 2 they observed an accuracy improvement of 3 percentage points over the original classifier (Wilson et al., 2005a) calculated on the SenMPQA dataset. Additionally, Rentoumi et al. (2009) showed that WSD is valuable in polarity classification of sentences containing figurative expressions. It should be noted that the above work did not focus on using WSD for social-media or micropost data, which is the primary focus area of our work. Babelfy (Moro et al., 2014)2 is a unified, multilingual, graph-based approach to Entity Linking and Word Sense Disambiguation based on a loose identification of candidate meanings coupled with a densest sub-graph heuristic which selects highcoherence semantic interpretations"
W15-2916,P06-4018,0,\N,Missing
W16-0207,W15-0708,0,0.0314646,"In search of an efficient text classification method and following the related works mentioned above, we decided to use SVM (Cortes and Vapnik, 1995), because it is a state-ofthe-art classification algorithm (Joachims, 1998). Orthographic, syntactic and phonemic features were used to classify poems by style (Kaplan and Blei, 2007). In analyzing poems and their aesthetics to reach the semantics of imagery, other researchers employ sound devices such as alliteration, consonance and rhyme (Kao and Jurafsky, 2015). More work uses NER and POS taggers to create features to classify poems by style (Delmonte, 2015). Lou et al. (2015) classified poems into nine classes (Love, Nature, Religion and other), allowing a poem to be in more than one class. Unlike the previous work on poetry classification, we classify the poems by one poet alone – Hafez – in chronological order, and the poems contain many symbols and hidden semantics that we captured by LDA-driven cosine similarities in vector space. 56 4 Proposed Methodology As shown in Figure 2, we used feature-engineering techniques based on Bag-Of-Words4 and Term Frequency-Inverse Document Frequency (TF-IDF) that we transformed into the vector space of LSI"
W16-0207,P06-1070,0,0.0435444,"hammad Ghazvini (1874-1949), an Iranian scholar, corrected and prepared today’s most reliable prints of Hafez ghazals. 3 For example, d“aneS-“amuz ‘student’ is one word, but we write it as two in Persian. 55 Six Classes Youth = 38 After Youth = 25 Maturity = 79 Middle Age = 66 Before Senectitude = 28 Senectitude = 13 Three Classes a b c d e f a0 b0 c0 to classify texts. Researchers obtained varied high accuracies in text classification depending on the task, context and corpus size. One source of differences is in how features are developed and weighted; another is in the learning algorithms. Gliozzo and Strapparava (2006) built common etymological ancestry attributes of words between Italian and English, which were used to train an SVM model in one language to classify text in the other. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) was used to create a deep vector representation of the word-document co-occurrences of shared lexical and etymological attributes. Dumais et al. (1997) found semantic correspondences between languages by using LSA and SVM to create multilingual domain models. Languages adopt words from each other and adjust them for their purposes, yet maintain their common roots. For ex"
W16-0207,W98-0706,0,\N,Missing
W16-4702,W13-3524,0,0.0207722,"ventionally classified as linguistic, statistical, and hybrid (Cabre-Castellvi et al., 2001; Chung, 2003). Linguistic methods implement formal rules to detect terms; statistical methods exploit some measures based on relative frequency of terms in general and target corpora by means of which they can tell apart a term from a word in its generic sense; and, the hybrid methods combine the advantages of both of these techniques (Frantzi et al., 1998; Park et al., 2002; Drouin, 2003; Chung and Nation, 2004; Yoshida and Nakagawa, 2005; Vu et al., 2008; VRL, 2009; Yang et al., 2010; Zervanou, 2010; Broß and Ehrig, 2013; Conrado et al., 2013). These methods often regard words in a document as atomic elements; that is, they are manifested as their symbolic alphabetical form in the algorithm (such as in ’a’ below) and/or as some measure of their relative frequency (as in ’b’ below). But, in a distributed approach (as in ’c’ below) each word has tens or hundreds of realvalued components, as opposed to a single linguistic form or a termhood score1 . The idea is that such finer-granularity may grant more access to the information that a word contains, potentially resulting in a better detection of terms in a docu"
W16-4702,N13-2003,0,0.275018,"as linguistic, statistical, and hybrid (Cabre-Castellvi et al., 2001; Chung, 2003). Linguistic methods implement formal rules to detect terms; statistical methods exploit some measures based on relative frequency of terms in general and target corpora by means of which they can tell apart a term from a word in its generic sense; and, the hybrid methods combine the advantages of both of these techniques (Frantzi et al., 1998; Park et al., 2002; Drouin, 2003; Chung and Nation, 2004; Yoshida and Nakagawa, 2005; Vu et al., 2008; VRL, 2009; Yang et al., 2010; Zervanou, 2010; Broß and Ehrig, 2013; Conrado et al., 2013). These methods often regard words in a document as atomic elements; that is, they are manifested as their symbolic alphabetical form in the algorithm (such as in ’a’ below) and/or as some measure of their relative frequency (as in ’b’ below). But, in a distributed approach (as in ’c’ below) each word has tens or hundreds of realvalued components, as opposed to a single linguistic form or a termhood score1 . The idea is that such finer-granularity may grant more access to the information that a word contains, potentially resulting in a better detection of terms in a document. (a) apple (b) 0.0"
W16-4702,C10-2055,0,0.0300498,"al., 2012). Linguistic methods apply hand-coded rules to the target corpus to extract technical terms. Statistical methods are often unsupervised and apply some measure of relative frequency to a technical target corpus, a reference (general) corpus and sometimes a (contrastive) corpus from another domain, to identify the existing terms in the technical corpus (Frantzi et al., 1998; Chung, 2003; Vu et al., 2008; Conrado et al., 2013). Hybrid methods combine statistical and linguistic methods to extract terminology from a target corpus and often perform well (Drouin, 2003; Serrec et al., 2010; Ismail and Manandhar, 2010; Vintar, 2010). The above-mentioned approaches, in contrast to the method put forth in this paper, regard words as atomic units represented by their linguistic forms and their statistical scores that indicate their likelihood to be terms. They may, however, implement rules associated with some linguistic features (e.g., their POS tags, their position in the POS sequence, their position in the parse tree, phrase, and/or in the sentence). These linguistic rules make an algorithm language-dependent and even sometimes to some degree domain-dependent. On the contrary, our method, if used independe"
W16-4702,C02-1142,0,0.0785759,"its terminology. This makes Automatic Terminology Extraction (ATE) an important task in Natural Language Processing (NLP). ATE methods have been conventionally classified as linguistic, statistical, and hybrid (Cabre-Castellvi et al., 2001; Chung, 2003). Linguistic methods implement formal rules to detect terms; statistical methods exploit some measures based on relative frequency of terms in general and target corpora by means of which they can tell apart a term from a word in its generic sense; and, the hybrid methods combine the advantages of both of these techniques (Frantzi et al., 1998; Park et al., 2002; Drouin, 2003; Chung and Nation, 2004; Yoshida and Nakagawa, 2005; Vu et al., 2008; VRL, 2009; Yang et al., 2010; Zervanou, 2010; Broß and Ehrig, 2013; Conrado et al., 2013). These methods often regard words in a document as atomic elements; that is, they are manifested as their symbolic alphabetical form in the algorithm (such as in ’a’ below) and/or as some measure of their relative frequency (as in ’b’ below). But, in a distributed approach (as in ’c’ below) each word has tens or hundreds of realvalued components, as opposed to a single linguistic form or a termhood score1 . The idea is th"
W16-4702,D14-1162,0,0.12164,"is the degree of a linguistic unit being related to a domain-specific concept (Kageura and Umino, 1996) 2 Proceedings of the 5th International Workshop on Computational Terminology, pages 2–11, Osaka, Japan, December 12 2016. automatic terminology extraction picture. However, the fact that makes them particularly appealing is their computational efficiency and scalability as compared to the available alternatives, including LSA and LDA (Mikolov et al., 2013). We present a simple method that harnesses the rich distributed representation acquired by a log-bilinear regression model called GloVe (Pennington et al., 2014), as well as the efficiency of log-linear models with CBOW2 and skip-gram architectures (Mikolov et al., 2013), which is a step forward towards language-independent ATE. In our method, the GloVe model is used to preserve the global3 scope of a word in a general corpus (i.e., its general sense(s)) and the CBOW or the skip-gram model is used to capture the local scope for a word in a technical corpus (i.e., its technical usage). Currently, we use our method as a filter on top of a previously-developed hybrid term extraction algorithm, namely, TermoStat (Drouin, 2003; Serrec et al., 2010) along w"
W16-4702,S14-2004,0,0.0129138,"ression model. More specifically: J= V X i,j=1 f (Xij )(wTi w˜j + bi + b˜j − log Xij )2 7 (1) There has, however, been earlier supervised work in keyword/keyphrase extraction such as Turney (2000), as opposed to terminology extraction which is the topic of this paper. While Keyword extraction is the task of extracting only a few keywords in a text, terminology extraction needs to detect all the terms, usually from a large domain corpus. 8 Aspect term extraction is the task to identify the aspect expressions which refer to a products or services properties or attributes, from customer reviews (Pontiki et al., 2014; Pontiki et al., 2015; Yin et al., 2016). 9 This is similar to the premise of traditional statistical ATE methods except that those models carry less local information such as syntactic behavior. 10 Available at: http://nlp.stanford.edu/projects/glove/ 4 where V is the size of the vocabulary, f (Xij ) is the weighting function, wi and wj are two separate context word vectors and their sum constructs the final GloVe vector, and finally bi and bj are biases for their corresponding word vectors. GloVe has been shown to adequately reflect both semantic and syntactic regularities in the data (Penn"
W16-4702,S15-2082,0,0.0222922,"Missing"
W16-4702,I08-2084,0,0.396347,"in Natural Language Processing (NLP). ATE methods have been conventionally classified as linguistic, statistical, and hybrid (Cabre-Castellvi et al., 2001; Chung, 2003). Linguistic methods implement formal rules to detect terms; statistical methods exploit some measures based on relative frequency of terms in general and target corpora by means of which they can tell apart a term from a word in its generic sense; and, the hybrid methods combine the advantages of both of these techniques (Frantzi et al., 1998; Park et al., 2002; Drouin, 2003; Chung and Nation, 2004; Yoshida and Nakagawa, 2005; Vu et al., 2008; VRL, 2009; Yang et al., 2010; Zervanou, 2010; Broß and Ehrig, 2013; Conrado et al., 2013). These methods often regard words in a document as atomic elements; that is, they are manifested as their symbolic alphabetical form in the algorithm (such as in ’a’ below) and/or as some measure of their relative frequency (as in ’b’ below). But, in a distributed approach (as in ’c’ below) each word has tens or hundreds of realvalued components, as opposed to a single linguistic form or a termhood score1 . The idea is that such finer-granularity may grant more access to the information that a word cont"
W16-4702,Y10-1036,0,0.0191314,"ng (NLP). ATE methods have been conventionally classified as linguistic, statistical, and hybrid (Cabre-Castellvi et al., 2001; Chung, 2003). Linguistic methods implement formal rules to detect terms; statistical methods exploit some measures based on relative frequency of terms in general and target corpora by means of which they can tell apart a term from a word in its generic sense; and, the hybrid methods combine the advantages of both of these techniques (Frantzi et al., 1998; Park et al., 2002; Drouin, 2003; Chung and Nation, 2004; Yoshida and Nakagawa, 2005; Vu et al., 2008; VRL, 2009; Yang et al., 2010; Zervanou, 2010; Broß and Ehrig, 2013; Conrado et al., 2013). These methods often regard words in a document as atomic elements; that is, they are manifested as their symbolic alphabetical form in the algorithm (such as in ’a’ below) and/or as some measure of their relative frequency (as in ’b’ below). But, in a distributed approach (as in ’c’ below) each word has tens or hundreds of realvalued components, as opposed to a single linguistic form or a termhood score1 . The idea is that such finer-granularity may grant more access to the information that a word contains, potentially resulting in"
W16-4702,S10-1042,0,0.0310598,"ds have been conventionally classified as linguistic, statistical, and hybrid (Cabre-Castellvi et al., 2001; Chung, 2003). Linguistic methods implement formal rules to detect terms; statistical methods exploit some measures based on relative frequency of terms in general and target corpora by means of which they can tell apart a term from a word in its generic sense; and, the hybrid methods combine the advantages of both of these techniques (Frantzi et al., 1998; Park et al., 2002; Drouin, 2003; Chung and Nation, 2004; Yoshida and Nakagawa, 2005; Vu et al., 2008; VRL, 2009; Yang et al., 2010; Zervanou, 2010; Broß and Ehrig, 2013; Conrado et al., 2013). These methods often regard words in a document as atomic elements; that is, they are manifested as their symbolic alphabetical form in the algorithm (such as in ’a’ below) and/or as some measure of their relative frequency (as in ’b’ below). But, in a distributed approach (as in ’c’ below) each word has tens or hundreds of realvalued components, as opposed to a single linguistic form or a termhood score1 . The idea is that such finer-granularity may grant more access to the information that a word contains, potentially resulting in a better detect"
W17-2201,S16-2003,0,0.0375765,"Missing"
W17-2201,E06-1042,0,0.0607526,"e sentences in the poems. We call this method ConceptNet Overlap. We assigned true if there was an overlap and false otherwise. This was used as one of the features in our rule-based model. 2.4 C(x,y).N ln C(x)C(y) N is the size of the corpus, C(x,y) is the frequency of x and y together, C(x) and C(y) are the frequencies of x and y in corpus, respectively. 3 Statistical-based Metaphor Detection The Results We applied our method to the sentences extracted from the 12,830 PoFo poems and annotated manually (see section 2.2). For training data, we used a combination of the datasets such as TroFi (Birke and Sarkar, 2006) and Shutova (Mohammad et al., 2016) with our own poetry dataset. We included other datasets annotated for metaphor, in addition to poetry, in order to increase the training set and thus get better classification predictions. We report all results explicitly for the test set throughout this paper. Table 1 shows the results for the class To capture the distortion of the context that a metaphor causes to a sentence, we computed the vector difference between the vectors for the head words. The underlying idea is this: the smaller the difference, the more connected the words would be. Conversely,"
W17-2201,D14-1162,0,0.0797271,"(Neuman et al., 2013). Type III has a tag sequence of Adjective-Noun (Neuman et al., 2013). We also propose two more metaphor types that we noticed in our poetry data: Type IV with a tag sequence of Noun-Verb, and Type V with a tag sequence of Verb-Verb. Here are examples: Neuman et al. (2013) propose to categorize metaphor by part-of-speech (POS) tag sequences such as Noun-Verb-Noun, Adjective-Noun, and so on. We follow the same methodology to extract the set of sentences that can be metaphorical in nature. Our method differs because we use word embeddings pre-trained on the Gigaword corpus (Pennington et al., 2014) to get word vector representations (vector difference and cosine similarity) of possible metaphorical word pairs. Another difference is the addition of two more types of POS sequences, which we have found to be metaphorical in our Poetry Foundation poetry corpus.1 We explain the types in section 2.1. Neuman et al. (2013) describe a statistical model based on Mutual Information and selectional preferences. They suggest using a largescale corpus to find the concrete nouns which most frequently occur with a specific word. Any word outside this small set denotes a metaphor. Our experiments do not"
W17-2201,de-marneffe-etal-2006-generating,0,0.00997025,"Missing"
W17-2201,N16-1020,0,0.0342629,"y occur with a specific word. Any word outside this small set denotes a metaphor. Our experiments do not involve finding selectional preference sets directly. Instead, we use word embeddings. We have found the selectional preference sets too limiting. The word span is to be set before the experiments. Some sentences exceed that limit, so the contextual meaning is lost. • As if the world were a taxi, you enter it [Type 1] (Koch, 1962) • I counted the echoes assembling, thumbing the midnight on the piers. [Type 2] (Crane, 2006) • The moving waters at their priestlike task [Type 3] (Keats, 2009) Shutova et al. (2016) introduce a statistical model which detects metaphor. So does our method, but their work is more verb-centered, in that verbs are a seed set for training data. Our work looks more into the possible applications for poetry, not generically. We also concentrate on nouns, because our initial experiments concerned Type I metaphor: a copular verb plays only an auxiliary role, so the focus is on the two nouns. • The yellow smoke slipped by the terrace, made a sudden leap [Type 4] (Eliot, 1915) • To die – to sleep [Type 5] (Shakespeare, 1904) In this paper, we focus on Type I metaphor. We will work"
W17-2201,speer-havasi-2012-representing,0,0.0148111,"e based on Pointwise Mutual Information in order to measure if a word pair is a collocation: Rule-based Metaphor Detection Firstly, we applied rule-based methods to our poetry dataset. We used the Abstract-Concrete (Turney et al., 2011) and Concrete Category Overlap rules (Assaf et al., 2013). The Abstract-Concrete rule needs the hypernym class of each noun; we find that in WordNet (Miller, 1995). We got all hypernyms of head nouns and checked for each parent till we reached the hypernym “abstract entity” or “physical entity”. Apart from the above rules, we used a feature based on ConceptNet (Speer and Havasi, 2012). For each noun in our sentence, we extracted the corresponding SurfaceText from ConceptNet. A SurfaceText contains some associations between the specific word and real-world knowledge. For example, “car” gives the following associations: • “drive” is related to “car” • You are likely to find “a car” in “the city” and so on. The entities are already highlighted in the SurfaceTexts. We parsed these associations and extracted all the entities. There can be action associations as well: • “a car” can “crash” • “a car” can “slow down” and so on. These entities and actions were used to establish an"
W17-2201,D11-1063,0,0.0193812,"etaphor instances commonly occurring in poetry. In this task, we were more concerned with the detection of all types of metaphor, not just poetic metaphor. In effect, distinguishing between common-speech and poetic metaphor has been left for our future work. We computed the cosine similarity for all word vector pairs, and made it another feature of our model. We also added a feature based on Pointwise Mutual Information in order to measure if a word pair is a collocation: Rule-based Metaphor Detection Firstly, we applied rule-based methods to our poetry dataset. We used the Abstract-Concrete (Turney et al., 2011) and Concrete Category Overlap rules (Assaf et al., 2013). The Abstract-Concrete rule needs the hypernym class of each noun; we find that in WordNet (Miller, 1995). We got all hypernyms of head nouns and checked for each parent till we reached the hypernym “abstract entity” or “physical entity”. Apart from the above rules, we used a feature based on ConceptNet (Speer and Havasi, 2012). For each noun in our sentence, we extracted the corresponding SurfaceText from ConceptNet. A SurfaceText contains some associations between the specific word and real-world knowledge. For example, “car” gives th"
W17-3104,W16-0307,0,0.0204423,"t than using linguistic methods such as the use of n-grams and other lexicon based approaches (Resnik et al., 2015b). Resnik et al. (2015a) proved that such approaches can be successfully used in identifying users with depression, who have self-disclosed their mental illnesses on Twitter. In general, a clear distinction in the lexical and syntactic structure of the language used by individuals with different mental disorders, as well as between individuals within a control group, can be identified throughout the literature mentioned above, as well as from the explorative analysis conducted by Gkotsis et al. (2016). Due to the reliability of the lexical and behavioral features used in many of the models mentioned above, our proposed solution also focused on these feature categories. Even though the dataset we have used is relatively smaller than the ones used by most of the experiments mentioned above, we managed to obtain reliable results in identifying users with mental disorders. 3 Datasets For this research, we prepared a dataset consisting of tweets from users who participated in #BellLetsTalk 2015 campaign. #BellLetsTalk is a campaign created by Bell Canada to help reduce stigma and promote awaren"
W17-3104,W14-3207,0,0.414124,"a data and therefore do not use any user identification information in our research. 2 De Choudhury et al. (2013b) and Schwartz et al. (2014) proposed methods to identify the level of depression among social media users (SMDI: Social Media Depression Index). Schwartz et al. (2014) used a classification model trained with ngrams, linguistic behavior and Latent Dirichlet Allocation (LDA) topics as features for predicting the individuals who are susceptible to having depression. In addition to open-vocabulary analysis and lexicon-based approaches such as Linguistic Inquiry and Word Count (LIWC), Coppersmith et al. (2014a) suggested language models, primarily based on unigrams and character 5-grams to determine the existence of mental disorders. Related Work With the gradual increase in social media usage and the extensive level of self-disclosure within such platforms (Park et al., 2012), research has been conducted to identify mental disorders at an individual as well as at a society level. Researchers have used features such as behavioural characteristics, depression language, emotion and linguistic style, reduced social activity, increased negative affect, clustered social network, raised interpersonal an"
W17-3104,W15-1204,0,0.494915,"elf-disclosure within such platforms (Park et al., 2012), research has been conducted to identify mental disorders at an individual as well as at a society level. Researchers have used features such as behavioural characteristics, depression language, emotion and linguistic style, reduced social activity, increased negative affect, clustered social network, raised interpersonal and medical fears and increased expression in religious involvement, use of negative words, in order to determine the cues of major depressive disorder (De Choudhury et al., 2013a; Tsugawa et al., 2015). Tsugawa et al. (2015), also used syntactical features such as bag of words (BOW) and word frequencies to identify the ratio of tweet topics and managed to conclude that topic modeling also adds a positive contribution to the predictive model compared to the use of the bagof-words model, which could also result in overfitting. The successful use of computational linguistics techniques in identifying the progress and level of depression of individuals in online therapy could bring greater insights to clinicians, to apply interventions effectively and efficiently. Howes et al. (2014) used 882 transcripts gathered fro"
W17-3104,W16-0303,0,0.0415674,"Missing"
W17-3104,W15-1205,0,0.0891748,"on. For each user in the dataset, nearly 3,200 most recent posts were collected using the Twitter API. Resnik et al. (2015a), whose system ranked first in the CLPsych 2015 Shared Task, created 16 systems based on features derived using supervised LDA, supervised anchors (for topic modeling), lexical TF-IDF, and a combination of all. An SVM classifier with a linear kernel obtained an average precision above 0.80 for all the three tasks (i.e., depression vs. control, PTSD vs. control and depression vs. PTSD) and a maximum precision of 0.893 for differentiating PTSD users from the control group. Preotiuc-Pietro et al. (2015) employed user metadata and textual features from the corpus provided by the CLPsych 2015 Shared Task to develop a linear classifier to predict users 33 tives.3 We collected data for the year 2015 and we limited it to Canadian users. 156,612 tweets were obtained from 25,362 users. Only data made public by users was collected for this task. To clean the dataset, we used LDA (Gr¨un and Hornik (2011)), to obtain topics from tweets. Prominent topics included “campaign publicity”, “mental health awareness”, “raising donations”, “facts about mental health”. If a tweet contained two or more keywords"
W17-3104,W15-1207,0,0.163933,"Missing"
W17-3104,W15-1212,0,0.167522,"ly. Howes et al. (2014) used 882 transcripts gathered from an online psychological therapy provider to determined The Computational Linguistics and Clinical Psychology (CLPsych) 2015 shared task (Coppersmith et al., 2015) used self-reported data on Twitter about Post Traumatic Stress Disorder (PTSD) and depression, collected according to the procedure introduced by Coppersmith et al. (2014b). The shared task participants were provided with a dataset of self-reported users on PTSD and depression. For each user in the dataset, nearly 3,200 most recent posts were collected using the Twitter API. Resnik et al. (2015a), whose system ranked first in the CLPsych 2015 Shared Task, created 16 systems based on features derived using supervised LDA, supervised anchors (for topic modeling), lexical TF-IDF, and a combination of all. An SVM classifier with a linear kernel obtained an average precision above 0.80 for all the three tasks (i.e., depression vs. control, PTSD vs. control and depression vs. PTSD) and a maximum precision of 0.893 for differentiating PTSD users from the control group. Preotiuc-Pietro et al. (2015) employed user metadata and textual features from the corpus provided by the CLPsych 2015 Sha"
W17-3104,W14-3214,0,0.119618,"d of time. This paper describes experiments on both classifiers. Our system can be used by authorities to find a focused group of at-risk users. It is not a platform for labeling an individual as a patient with depression, but only a platform for raising an alarm so that the relevant authorities could take necessary interventions to further analyze the predicted user to confirm his/her state of mental health. We respect the ethical boundaries relating to the use of social media data and therefore do not use any user identification information in our research. 2 De Choudhury et al. (2013b) and Schwartz et al. (2014) proposed methods to identify the level of depression among social media users (SMDI: Social Media Depression Index). Schwartz et al. (2014) used a classification model trained with ngrams, linguistic behavior and Latent Dirichlet Allocation (LDA) topics as features for predicting the individuals who are susceptible to having depression. In addition to open-vocabulary analysis and lexicon-based approaches such as Linguistic Inquiry and Word Count (LIWC), Coppersmith et al. (2014a) suggested language models, primarily based on unigrams and character 5-grams to determine the existence of mental"
W17-5307,D15-1075,0,0.0228619,"Pn t 2 hpt ki k j 2 j=1 (6) k1 − ft k2 Pn hpt k1 − f k j 2 j=1 (7) ko k Pn t 2 hpt ko k j 2 j=1 (8) 4 Data RepEval 2017 use Multi-Genre NLI corpus (MultiNLI) (Williams et al., 2017), which focuses on three basic relationships between a premise and a potential hypothesis: the premise entails the hypothesis (entailment), they contradict each other (contradiction), or they are not related (neutral). The corpus has ten genres, such as fiction, letters, telephone speech and so on. Training set only has five genres of them, therefore there are in-domain and cross-domain development/test sets. SNLI (Bowman et al., 2015) corpus can be used as an additional training/development set, which includes content from the single genre of image captions. However, we don’t use SNLI as an additional training/development data in our experiments. where it , ft , ot are the input gate, forget gate, and output gate in the BiLSTM of the top layer. Note that the gates are concatenated by forward → − ← − and backward LSTM, i.e., it = [ it ; it ], ft = → − ← − − [ ft ; ft ], ot = [→ ot ; ← o−t ]. k∗k2 indicates l2 -norm, which converts vectors to scalars. The idea of gated-attention is inspired by the fact that human only rememb"
W17-5307,P16-2022,0,0.208419,"Eval 2017 Shared Task aims to evaluate language understanding models for sentence representation with natural language inference (NLI) tasks, where a sentence is represented as a fixedlength vector. Modeling inference in human language is very 36 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 36–40, c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics sentence encoder-based model. Many researchers have studied sentence encoder-based model for natural language inference (Bowman et al., 2015; Vendrov et al., 2015; Mou et al., 2016; Bowman et al., 2016; Munkhdalai and Yu, 2016a,b; Liu et al., 2016; Lin et al., 2017). It is, however, not very clear if the potential of the sentence encoderbased model has been well exploited. In this paper, we demonstrate that proposed models based on gated-attention can achieve a new state-of-theart performance for natural language inference. 3 character-composition vector and word-level embedding e = ([c1 ; w1 ], . . . , [cl ; wl ]). This is performed on both the premise and hypothesis, resulting into two matrices: the ep ∈ Rn×dw for a premise and the eh ∈ Rm×dw for a hypothesis, where n"
W17-5307,P16-1139,0,0.0714274,"ask aims to evaluate language understanding models for sentence representation with natural language inference (NLI) tasks, where a sentence is represented as a fixedlength vector. Modeling inference in human language is very 36 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 36–40, c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics sentence encoder-based model. Many researchers have studied sentence encoder-based model for natural language inference (Bowman et al., 2015; Vendrov et al., 2015; Mou et al., 2016; Bowman et al., 2016; Munkhdalai and Yu, 2016a,b; Liu et al., 2016; Lin et al., 2017). It is, however, not very clear if the potential of the sentence encoderbased model has been well exploited. In this paper, we demonstrate that proposed models based on gated-attention can achieve a new state-of-theart performance for natural language inference. 3 character-composition vector and word-level embedding e = ([c1 ; w1 ], . . . , [cl ; wl ]). This is performed on both the premise and hypothesis, resulting into two matrices: the ep ∈ Rn×dw for a premise and the eh ∈ Rm×dw for a hypothesis, where n and m are the length"
W17-5307,W17-5301,0,0.100549,"Missing"
W17-5307,P15-1011,1,0.817366,"Missing"
W17-5307,D14-1162,0,0.0893348,"Missing"
W17-5307,D14-1181,0,0.0210378,"Missing"
W17-5307,N18-1101,0,\N,Missing
W18-0609,W14-3207,0,0.500336,"eature engineering. Throughout the literature, it could be identified that the most widely adopted feature engineering method is to extract lexical features using the Linguistic inquiry word count (LIWC) lexicon, which contains more than 32 categories of psychological constructs (Pennebaker et al., 2007). The lexicons have been used as one of the key feature extraction mechanisms in identifying insomnia (Jamison-Powell et al., 2012), distress (Lehrman et al., 2012), postpartum depression (De Choudhury et al., 2013), depression (Schwartz et al., 2014) and post-traumatic stress disorder (PTSD) (Coppersmith et al., 2014a). For each of these mental disorders to be identified, researchers had to extract features that overlap with each other, and are unique to a particular disorder. For example, the use of first-person pronouns (Lehrman et al., 2012) compared to the lesser use of second and third person pronouns (De Choudhury, 2013) are being used to detect users susceptible to disMental illness detection Identifying the treatment requirement for a mental disorder is a complicated clinical decision, which involves several factors such as the severity of symptoms, patients’ suffering associated with the symptoms"
W18-0609,W16-0313,0,0.0609629,"Missing"
W18-0609,W15-1201,0,0.417564,"a sufficient amount of knowledge related to the particular area of research. Even if such features were extracted, this does not assure that those features are the key contributors to obtaining improved accuracies. Due to these reasons, we investigate the possibility of using deep neural architectures because the features are learned within the architecture itself. Here, we explore a few selected deep neural network architectures to detect mental disorders, specifically depression. We used the data released for the Computational Linguistics and Clinical Psychology (CLPsych) 2015 shared task (Coppersmith et al., 2015b). Even though the task is comprised of three subtasks: detecting Post-Traumatic Mental illness detection in social media can be considered a complex task, mainly due to the complicated nature of mental disorders. In recent years, this research area has started to evolve with the continuous increase in popularity of social media platforms that became an integral part of people’s life. This close relationship between social media platforms and their users has made these platforms to reflect the users’ personal life on many levels. In such an environment, researchers are presented with a wealth"
W18-0609,W15-1204,0,0.68944,"Missing"
W18-0609,W17-3108,0,0.173408,"Missing"
W18-0609,W12-2102,0,0.619833,"Missing"
W18-0609,W16-0312,0,0.0394465,"Missing"
W18-0609,N15-1184,0,0.0187785,"Missing"
W18-0609,W15-1202,0,0.0148682,"(2014a,b) has used unigram and character n-gram language models to extract features in the process of identifying users suspicious of having PTSD and several other mental illnesses such as bipolar disorder, depression, and seasonal affective disorder (SAD). Similarly, character n-grams can be identified as the key feature extraction mechanism in detecting mental illnesses such as attention deficit hyperactivity disorder (ADHD), generalized anxiety disorder, and eight other mental illnesses (Coppersmith et al., 2015a) as well as in detecting rare mental health conditions such as schizophrenia (Mitchell et al., 2015). Even though topic modelling techniques such as latent Dirichlet allocation (LDA) are being used to enhance the classifier predictability (Mitchell et al., 2015), researchers have identified supervised topic modeling methods (Resnik et al., 2015) and topics derived from clustering methods such as Word2Vec and GloVe Word Clusters (Preotiuc-Pietro et al., 2015b) to be more reliable in identifying users susceptible to having a mental illness. Further advancements in detecting mental health conditions were identified in the Computational Linguistics and Clinical Psychology (CLPsych) 2016 shared t"
W18-0609,N16-1162,0,0.0559337,"Missing"
W18-0609,W17-3104,1,0.938367,"ecurrent Neural Networks (RNNs), given the limited amount (i.e., in comparison to most of the deep neural network architectures) of unstructured data. Our approach and key contributions can be summarized as follows. • Word embedding optimization: we propose a novel approach to optimize word-embedding for classification with a focus on identifying users suffering from depression based on their social posts such as tweets. We use our approach to improve the performance of two tasks: depression detection on the CLPsych2015 dataset and test generalization capability on the Bell Lets Talk dataset (Jamil et al., 2017). As social media interactions reside in a more naturalistic setting, it is important to identify to what extent an individual has disclosed their personal information, and whether the accurate and sufficient information is being published to determine whether a person has a mental disorder. The longitudinal data published on social media platforms have been identified as valuable (De Choudhury, 2013, 2014, 2015) with an extensive level of self-disclosure (Balani and De Choudhury, 2015; Park et al., 2012). • Comparative evaluation: we investigate and report the performance of several deep lear"
W18-0609,P14-1062,0,0.00844508,"using L2 regularization penalty (Cortes, Mohri, & Rostamizadeh, 2012). Hyperparameters: we use an embedding layer of the size 300, and an LSTM layer of size 50, which increases to be 100 for the bidirectional LSTM. We apply a dropout of 0.4 and 0.2 on the recurrent connections. Finally, an L2 regularization of 0.0001 is applied at the loss function. c mation, wif = max(Cif ). Finally, we concatenate feature representations into a single output. Conversely to recurrent layers, convolutional operations are helpful with max-pooling to extract word features without considering the sequence order (Kalchbrenner et al., 2014). Such features can be used with recurrent features in order to improve the model performance. MultiChannelPoolingCNN: We extend the previous model to apply two different max-pooling sizes, 2 and 5. 5.2 Models Training Recurrent Neural Network (RNN) It is commonly used in NLP as it allows for remembering values over different time durations. In RNN, each element of an input embedding xi is processed sequentially. ht = tanh(Wxi + Wht−1 ) and W represent the weight matrix between an input and hidden states (ht ) of the recurrent connection at timestep (t). RNN allows for variable length processi"
W18-0609,W15-1203,0,0.384519,"a highly trained professional with the use of different techniques such as text descriptions and clinical interviews, as well as their judgments (American Psychiatric Association, 2013). Considering the complexity of the procedures and level of skills involved in identifying mental disorder and the necessary treatments, detecting mental illness within social media using web mining and emotion analysis techniques could be considered a preliminary step that could be used to generate awareness. 89 tress and depression. To distinguish depression from PTSD, age is identified as a distinct feature (Preotiuc-Pietro et al., 2015a). Number of users Number of tweets in each category Average age Gender (female) distribution per class We found that working with the data extracted from the Twitter social media platform is challenging due to the unstructured nature of the text posted by users. The Twitter posts are introduced with new terms, misspelled words, syntactic errors, and character limitations when composing a message. Character n-gram models could be considered as an intuitive approach to overcome challenges imposed by unstructured data. Considering the effectiveness of such language models in classification task"
W18-0609,W15-1205,0,0.601843,"a highly trained professional with the use of different techniques such as text descriptions and clinical interviews, as well as their judgments (American Psychiatric Association, 2013). Considering the complexity of the procedures and level of skills involved in identifying mental disorder and the necessary treatments, detecting mental illness within social media using web mining and emotion analysis techniques could be considered a preliminary step that could be used to generate awareness. 89 tress and depression. To distinguish depression from PTSD, age is identified as a distinct feature (Preotiuc-Pietro et al., 2015a). Number of users Number of tweets in each category Average age Gender (female) distribution per class We found that working with the data extracted from the Twitter social media platform is challenging due to the unstructured nature of the text posted by users. The Twitter posts are introduced with new terms, misspelled words, syntactic errors, and character limitations when composing a message. Character n-gram models could be considered as an intuitive approach to overcome challenges imposed by unstructured data. Considering the effectiveness of such language models in classification task"
W18-0609,W14-3214,0,0.340541,"ental illnesses in social media platforms has focused heavily on feature engineering. Throughout the literature, it could be identified that the most widely adopted feature engineering method is to extract lexical features using the Linguistic inquiry word count (LIWC) lexicon, which contains more than 32 categories of psychological constructs (Pennebaker et al., 2007). The lexicons have been used as one of the key feature extraction mechanisms in identifying insomnia (Jamison-Powell et al., 2012), distress (Lehrman et al., 2012), postpartum depression (De Choudhury et al., 2013), depression (Schwartz et al., 2014) and post-traumatic stress disorder (PTSD) (Coppersmith et al., 2014a). For each of these mental disorders to be identified, researchers had to extract features that overlap with each other, and are unique to a particular disorder. For example, the use of first-person pronouns (Lehrman et al., 2012) compared to the lesser use of second and third person pronouns (De Choudhury, 2013) are being used to detect users susceptible to disMental illness detection Identifying the treatment requirement for a mental disorder is a complicated clinical decision, which involves several factors such as the se"
W18-0609,N16-1174,0,0.0189045,"entation of learning from sliding w-grams for an input sequence of d entries, e1 , e2 , . . . , et . A vector ci Red is the concatenated embedding of f entries, such that xi−f +1 , . . . , xi where f is the filter length. For w-gram, we generate a representation pi Rd using convolution weights W Rd×wd where a bias bRd and pi = tanh(Wxi +b ). CNNWithMax: We apply a one-dimensional con92 weight values, as they are generally not equal. Thus, we use an attention mechanism (Bahdanau et al., 2014; Vaswani et al., 2017) to focus on the important words. We use a context-aware attention mechanism (Yang et al., 2016), which is the weighted summation of all words in a given sequence (r = ΣTi=1 ai hi ). We use this representation as a classification feature vector. volution operation with 250 filters and a kernel of size 3, where wif = conv1d(si ) and f is the filter length. After that, a global maxpooling layer is applied on the feature map to excf = tract global abstract information, such that w f globalmax(wi ), which results in an abstract feature representation of length 250. MultiChannelCNN: We apply 3 convolutions, each of which has 128 features and filters of the lengths 3, 4, and 5. A one-dimension"
W19-2511,J92-4003,0,0.67734,"termine semantic-role labels. Herbelot (2014) used similarity of word distributions, in pursuit of detecting semantic coherence in modern and contemporary poetry. Figure 3: Top 30 Most Relevant Terms which were then used in clustering. Rahgozar and Inkpen (2016a) used supervised learning to classify Hafez. We tried an unsupervised method and did not use master-labels by Houman (1938) as training, but we used his labels to evaluate our clusters. For a long time, researchers tried to extract what was implied in the context, by applying generative models and collocation of the words. For example Brown et al. (1992) assumed word clustering carried semantic groupings. Our corpus was considerably smaller than those in the literature, none-the-less, hand-labeling or human annotation is an expensive, rare and slow process. Therefore, similar to many NLP researchers, we used clustering to augment annotated data based on the assumption that word clusters contained specific semantic information (Miller et al., 2004). Capturing semantic latent properties has been a long and continuous effort in Computational Linguistics. (Deerwester et al., 1990) used singularvalue decomposition as pseudo-document vectors to det"
W19-2511,N04-1043,0,0.133146,"but we used his labels to evaluate our clusters. For a long time, researchers tried to extract what was implied in the context, by applying generative models and collocation of the words. For example Brown et al. (1992) assumed word clustering carried semantic groupings. Our corpus was considerably smaller than those in the literature, none-the-less, hand-labeling or human annotation is an expensive, rare and slow process. Therefore, similar to many NLP researchers, we used clustering to augment annotated data based on the assumption that word clusters contained specific semantic information (Miller et al., 2004). Capturing semantic latent properties has been a long and continuous effort in Computational Linguistics. (Deerwester et al., 1990) used singularvalue decomposition as pseudo-document vectors to detect implicit semantic properties, referred to as latent semantic analysis (LSA) in text. This was what we intended to do but in poetic text. In the continuation of semantic endeavour, (Blei et al., 2003) later developed latent Dirichlet allocation (LDA), an unsupervised generative probabilistic model to extract topics and their important associated terms. We used LDA driven features, before passing"
W19-2511,P09-1068,0,0.01536,"he context. The concept of similarity, mostly translated to distance in mathematics, is inherent and fundamental, especially in clustering and unsupervised learning algorithms. Kaplan and Blei (2007) for example, used vector space and principal components analysis (PCA), to depict style similarities in American poetry. Correlation was also used as a similarity measure to detect topics in poetry (Asgari and Chappelier, 2013). Lee et al. (2005) concluded that measures such as correlation, Jaccard and Cosine similarities performed almost the same in clustering documents. Similar to our research, Chambers and Jurafsky (2009) used but chain-similarities in an unsupervised learning algorithm, to determine narrative schemas and participants of semantic roles, instead of relying on any hand-built classes or knowledgebase. Their similarity definition was based on a pairwise summation of PMI and Log-Frequency of their narrative schema’s vector representations. Then they maximized those similarities to score and determine semantic-role labels. Herbelot (2014) used similarity of word distributions, in pursuit of detecting semantic coherence in modern and contemporary poetry. Figure 3: Top 30 Most Relevant Terms which wer"
W19-2511,W16-0207,1,0.898733,"is, partly discussed in Section 4, using a topicmodelling visualization interactive tool. Although the fundamental question was to find out how consistent our semantic-based clustering would be with Houman’s chronological classification, and to establish a verification experiment Introduction Chronological classification of Hafez poetry was done by Houman, in his book (Houman, 1938). He partly hand-classified Hafez’s poems in 1938, based on the semantic attributes engraved and encrypted in the ghazals. Houman’s labeling has been the gold-standard of chronological classification for Hafez, and Rahgozar and Inkpen (2016b) used them as training data for supervised learning to predict the rest of the ghazals. We used similar semantic features, but instead we conducted unsupervised learning (clustering experiments) to 1 Persian philosopher and poet. Popular form of Persian poetry with specific rhyme and rhythm, consisting of about ten, seemingly independent couplets; Ghazal is interchangably used with the word poem here. 2 3 Our Hafez corpus will be available, alternative sources for Hafez corpus are https://ganjoor.net/hafez/, http://www.nosokhan.com/ and https://www.hafizonlove.com/ 82 Proc. of the 3rd Joint"
W19-2511,W14-3110,0,0.0157564,"peripheries of the chronological Hafez corpus. This simple example, therefore, was consistent with the assumption that similarity measures transferred the information to the clustering and guided it as per the semantics of the anchored poems. 4.2 Semantic Analysis Figure 2: Intertopic Distance Map Each poem’s new label provided new perspective and insights, to enable us interpret Hafez’s poem better, by investigating the semantic characteristics of its associated cluster, in conjunction with its Houman classification. We could visualize the corresponding cluster, using LDAvis topic modelling (Sievert and Shirley, 2014) who introduced and used Relevance measure. (2012) defined and developed Saliency as part of Termite visualization tool. For example, we selected to analyze a poem, number 230 from the Houman labeled portion of the corpus, which was the number 143 in Ganjour19 . On the one hand, we saw that this poem belonged to class 5 or before-senectitude of Houman’s classification. On the other hand, we looked at the top 30 terms of the topic 3 which was central in PCA depiction of 5 LDA topics, Figure 2, which corresponded with our new label 1 cluster poems generated by Sim2 clusterer. The words old (pIr)"
W19-2511,D14-1074,0,0.0252471,"/ganjoor.net/hafez/ghazal/sh143/ 87 used topic models (unsupervised learning) to cluster Persian poetry by genre and then compared the results with SVM (supervised learning) classifications. Similarly, we used latent semantic indexing (LSI) and LDA-driven features for clustering. Saeedi et al. (2014) also used unsupervised semantic role labeling in Persian, but used different clustering scores than ours, such as purity and inverse-purity. We also used word embedding as features (Mikolov et al., 2011), which was the basis of our challenger model, against the top champion, the homothetic model. Zhang and Lapata (2014) used word embedding in poetry generation task and found it an effective feature for capturing the context. The concept of similarity, mostly translated to distance in mathematics, is inherent and fundamental, especially in clustering and unsupervised learning algorithms. Kaplan and Blei (2007) for example, used vector space and principal components analysis (PCA), to depict style similarities in American poetry. Correlation was also used as a similarity measure to detect topics in poetry (Asgari and Chappelier, 2013). Lee et al. (2005) concluded that measures such as correlation, Jaccard and"
