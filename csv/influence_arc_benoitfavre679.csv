2011.jeptalnrecital-long.26,W10-1408,1,0.877026,"Missing"
2011.jeptalnrecital-long.26,C10-1011,0,0.0269882,"Missing"
2011.jeptalnrecital-long.26,candito-etal-2010-statistical,0,0.0312059,"Missing"
2011.jeptalnrecital-long.26,W09-3821,0,0.0436454,"Missing"
2011.jeptalnrecital-long.26,2009.jeptalnrecital-long.4,0,0.0966482,"Missing"
2011.jeptalnrecital-long.26,C10-2013,0,0.0302856,"Missing"
2011.jeptalnrecital-long.26,P05-1022,0,0.145446,"Missing"
2011.jeptalnrecital-long.26,P97-1003,0,0.220523,"Missing"
2011.jeptalnrecital-long.26,2010.jeptalnrecital-long.3,0,0.0504872,"Missing"
2011.jeptalnrecital-long.26,N10-1095,0,0.0377103,"Missing"
2011.jeptalnrecital-long.26,P05-1010,0,0.0473543,"Missing"
2011.jeptalnrecital-long.26,C08-1071,0,0.0408272,"Missing"
2011.jeptalnrecital-long.26,P05-1012,0,0.152191,"Missing"
2011.jeptalnrecital-long.26,P06-1055,0,0.0763483,"Missing"
2011.jeptalnrecital-long.26,N07-1051,0,0.0986944,"Missing"
2011.jeptalnrecital-long.26,N10-1049,0,0.0288354,"Missing"
2016.jeptalnrecital-jep.41,giraudel-etal-2012-repere,0,0.0607418,"Missing"
2016.jeptalnrecital-jep.41,N06-2021,0,0.0794578,"Missing"
2016.jeptalnrecital-jep.41,N10-1108,0,0.0380926,"Missing"
2016.jeptalnrecital-long.5,P98-1013,0,0.0775484,"Missing"
2016.jeptalnrecital-long.5,bazillon-etal-2012-syntactic,1,0.846389,"Missing"
2016.jeptalnrecital-long.5,bechet-etal-2012-decoda,1,0.891737,"Missing"
2016.jeptalnrecital-long.5,C10-2046,0,0.0684206,"Missing"
2016.jeptalnrecital-long.5,W04-1013,0,0.0239437,"Missing"
2016.jeptalnrecital-long.5,P14-1115,0,0.0641249,"Missing"
2016.jeptalnrecital-long.5,P11-5003,0,0.0889028,"Missing"
2016.jeptalnrecital-long.5,H01-1054,0,0.136681,"Missing"
2017.jeptalnrecital-court.7,bechet-etal-2012-decoda,0,0.0515768,"Missing"
2017.jeptalnrecital-court.7,Y09-1024,0,0.0839578,"Missing"
2017.jeptalnrecital-court.7,J94-4002,0,0.203005,"Missing"
2017.jeptalnrecital-court.7,J13-4004,0,0.0793042,"Missing"
2017.jeptalnrecital-court.7,sagot-2010-lefff,0,0.0533437,"Missing"
2018.jeptalnrecital-court.4,hara-etal-2010-estimation,0,0.0818392,"Missing"
2018.jeptalnrecital-court.4,D14-1181,0,0.00513954,"Missing"
2018.jeptalnrecital-court.4,N13-1064,0,0.0306075,"Missing"
2018.jeptalnrecital-court.5,W17-4777,0,0.0420788,"Missing"
2018.jeptalnrecital-court.5,D14-1179,0,0.0452025,"Missing"
2018.jeptalnrecital-court.5,W16-2361,0,0.0638141,"Missing"
2018.jeptalnrecital-court.5,D17-1288,0,0.0613478,"Missing"
2018.jeptalnrecital-long.13,P08-1037,0,0.0567262,"Missing"
2018.jeptalnrecital-long.13,P15-1006,0,0.0589067,"Missing"
2018.jeptalnrecital-long.13,D16-1156,0,0.0311152,"Missing"
2018.jeptalnrecital-long.13,P17-1191,0,0.0198975,"Missing"
2018.jeptalnrecital-long.13,E17-2050,0,0.0304591,"Missing"
2018.jeptalnrecital-long.13,W17-6311,1,0.661114,"Missing"
2018.jeptalnrecital-long.13,J93-2004,0,0.0615618,"Missing"
2018.jeptalnrecital-long.13,J16-1002,1,0.87007,"Missing"
2018.jeptalnrecital-long.13,Q14-1006,0,0.100825,"Missing"
2020.cmcl-1.7,P16-1050,0,0.0134674,"ch as the choice of words, syntactic structures, and semantic topics. Linguistic alignment is considered an important mechanism for establishing common ground and rapport, fostering successful communicative interactions (Clark, 1996) . In addition, understanding this coordination in its natural context is crucial for the design of conversational systems that interact with people in a natural and effective fashion (Zhao et al., 2016; Loth et al., 2015; Park et al., 2017). While alignment has been largely studied with adults (Pickering and Garrod, 2004; Fusaroli et al., 2012; Dale et al., 2013; Doyle and Frank, 2016; Dideriksen et al., 2019), little has been done to investigate how it manifests in the context of childadult early communication and how it evolves across development. This is a significant gap in Related Work and Novelty of the Current Study Our study investigates children’s interactive alignment in natural conversations with adults. Previously, Dale and Spivey (2006) used recurrence analysis to investigate child-caregiver syntactic alignment (operationalized as sequences of parts of speech) and found evidence for syntactic coordination. Using a similar computational framework, Fern´andez an"
2020.cmcl-1.7,W16-3647,0,0.0228319,"hat interlocutors have to change the way they talk to accommodate their conversational partners. This can happen through mirroring the partner’s linguistic behavior on many levels such as the choice of words, syntactic structures, and semantic topics. Linguistic alignment is considered an important mechanism for establishing common ground and rapport, fostering successful communicative interactions (Clark, 1996) . In addition, understanding this coordination in its natural context is crucial for the design of conversational systems that interact with people in a natural and effective fashion (Zhao et al., 2016; Loth et al., 2015; Park et al., 2017). While alignment has been largely studied with adults (Pickering and Garrod, 2004; Fusaroli et al., 2012; Dale et al., 2013; Doyle and Frank, 2016; Dideriksen et al., 2019), little has been done to investigate how it manifests in the context of childadult early communication and how it evolves across development. This is a significant gap in Related Work and Novelty of the Current Study Our study investigates children’s interactive alignment in natural conversations with adults. Previously, Dale and Spivey (2006) used recurrence analysis to investigate c"
2020.cmcl-1.7,W19-2914,1,0.825736,"Missing"
2020.sigdial-1.25,N16-1174,0,0.0273755,"Missing"
bazillon-etal-2012-syntactic,N07-1051,0,\N,Missing
bazillon-etal-2012-syntactic,C10-1011,0,\N,Missing
bazillon-etal-2012-syntactic,P05-1012,0,\N,Missing
bazillon-etal-2012-syntactic,D07-1101,0,\N,Missing
bazillon-etal-2012-syntactic,W03-3017,0,\N,Missing
D15-1220,P11-1049,0,0.309005,"Missing"
D15-1220,W09-1802,1,0.585305,"and eliminate the need for concept pruning using an approximation algorithm that achieves comparable performance to exact inference. 1 Introduction Recent years have witnessed increased interest in global inference methods for extractive summarization. These methods formulate summarization as a combinatorial optimization problem, i.e. selecting a subset of sentences that maximizes an objective function under a length constraint, and use Integer Linear Programming (ILP) to solve it exactly (McDonald, 2007). In this work, we focus on the concept-based ILP model for summarization introduced by (Gillick and Favre, 2009). In their model, a summary is generated by assembling the subset of sentences that maximizes a function of the unique concepts it covers. Selecting the optimal subset of sentences is then cast as an instance of the budgeted maximum coverage problem1 . As this problem is NP-hard, pruning low-weight concepts is required for the ILP solver to find optimal solutions efficiently (Gillick and Favre, 2009; 1 Given a collection S of sets with associated costs and a budget L, find a subset S 0 ⊆ S such that the total cost of sets in S 0 does not exceed L, and the total weight of elements covered by S"
D15-1220,N09-2061,0,0.00953015,"role in the presence of multiple optimal solutions. Limited-range functions, such as frequency-based ones, yield many ties and increase the likelihood that different sentences have the same score. Redundancy within the set of input sentences exacerbate this problem, since highly similar sentences are likely to contain the same concepts. For comparison purposes, we use the same system pipeline as in (Gillick et al., 2009), which is described below. Step 1: clean input documents; a set of rules is used to remove bylines and format markup. Step 2: split the text into sentences; we use splitta2 (Gillick, 2009) and re-attach multisentence quotations. Step 3: compute parameters needed by the model; we extract and weight the concepts. Step 4: prune sentences shorter than 10 words, duplicate sentences and those that begin and end with a quotation mark. Step 5: map to ILP format and solve; we use an off-the-shelf ILP solver3 . Pruning to reduce complexity The concept-level formulation of (Gillick and Favre, 2009) is an instance of the budgeted maximum coverage problem, and solving such a problem is NP-hard (Khuller et al., 1999). Keeping the number of variables and constraints small is then critical to"
D15-1220,P13-1099,0,0.644153,"the solution: selecting a sentence leads to the selection of all the concepts it contains, and selecting a concept is only possible if it is present in at least one selected sentence. Choosing a suitable definition for concepts and a method to estimate their weights are the two key factors that affect the performance of this model. Bigrams of words are usually used as a proxy for concepts (Gillick and Favre, 2009; BergKirkpatrick et al., 2011). Concept weights are either estimated by heuristic counting, e.g. document frequency in (Gillick and Favre, 2009), or obtained by supervised learning (Li et al., 2013). 2.2 Pruning concepts to reduce complexity also cuts down the number of items from which summary scores are derived. As we will see in Section 3.2, this results in a lower ROUGE scores and leads to the production of multiple optimal summaries. The concept weighting function also plays an important role in the presence of multiple optimal solutions. Limited-range functions, such as frequency-based ones, yield many ties and increase the likelihood that different sentences have the same score. Redundancy within the set of input sentences exacerbate this problem, since highly similar sentences ar"
D15-1220,N10-1134,0,0.107173,"Missing"
D15-1220,W04-1013,0,0.0492157,"en the value of the objective function returned by the solver changes. 3 3.1 Experiments Datasets and evaluation measures Experiments are conducted on the DUC’04 and TAC’08 datasets. For DUC’04, we use the 50 topics from the generic multi-document summarization task (Task 2). For TAC’08, we focus only on the 48 topics from the non-update summarization task. Each topic contains 10 newswire articles for which the task is to generate a summary no longer than 100 words (whitespace-delimited tokens). Summaries are evaluated against reference summaries using the ROUGE automatic evaluation measures (Lin, 2004). We set the ROUGE parameters to those6 that lead to highest agreement with manual evaluation (Owczarzak et al., 2012), that is, with stemming and stopwords not removed. 3.2 Results Table 1 presents the average number of optimal solutions at different levels of concept pruning. Overall, the average number of optimal solutions increases along with the minimum document frequency, reaching 4.8 for TAC’08 at DF = 4. Prun6 We use ROUGE-1.5.5 with the parameters: n 4 -m -a -l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0 ing concepts also greatly reduces the number of variables in the ILP formulation, and c"
D15-1220,W12-2601,0,0.355246,"luation measures Experiments are conducted on the DUC’04 and TAC’08 datasets. For DUC’04, we use the 50 topics from the generic multi-document summarization task (Task 2). For TAC’08, we focus only on the 48 topics from the non-update summarization task. Each topic contains 10 newswire articles for which the task is to generate a summary no longer than 100 words (whitespace-delimited tokens). Summaries are evaluated against reference summaries using the ROUGE automatic evaluation measures (Lin, 2004). We set the ROUGE parameters to those6 that lead to highest agreement with manual evaluation (Owczarzak et al., 2012), that is, with stemming and stopwords not removed. 3.2 Results Table 1 presents the average number of optimal solutions at different levels of concept pruning. Overall, the average number of optimal solutions increases along with the minimum document frequency, reaching 4.8 for TAC’08 at DF = 4. Prun6 We use ROUGE-1.5.5 with the parameters: n 4 -m -a -l 100 -x -c 95 -r 1000 -f A -p 0.5 -t 0 ing concepts also greatly reduces the number of variables in the ILP formulation, and consequently improves the run-time for solving the problem. Interestingly, we note that, even without any pruning, the"
D15-1220,D13-1156,0,0.0194008,"orter stemmer in nltk. 1915 DUC’04 DF # solutions # concepts # sentences Avg. time (sec) TAC’08 1 2 3 4 1 2 3 4 1.3 2 955 184 22.3 1.3 676 175 1.7 1.5 247 159 0.5 1.5 107 139 0.3 1.2 2 909 174 21.5 1.3 393 167 0.8 1.8 127 149 0.3 4.8 56 129 0.2 Table 1: Average number of optimal solutions, concepts and sentences for different minimum document frequencies. The average time in seconds for finding the first optimal solution is also reported. where the concept was seen. Document frequency is a simple, yet effective approach to concept weighting (Gillick and Favre, 2009; Woodsend and Lapata, 2012; Qian and Liu, 2013). Reducing the number of concepts in the ILP model is then performed by pruning those concepts that occur in fewer than a given number of documents. ILP solvers usually provide only one solution. To generate alternate optimal solutions, we iteratively add new constraints to the problem that eliminate already found optimal solutions and rerun the solver. We stop the iterations when the value of the objective function returned by the solver changes. 3 3.1 Experiments Datasets and evaluation measures Experiments are conducted on the DUC’04 and TAC’08 datasets. For DUC’04, we use the 50 topics fro"
D15-1220,E09-1089,0,0.0302742,"Missing"
D15-1220,D12-1022,0,0.102345,"ww.nltk.org/ 5 We use the Porter stemmer in nltk. 1915 DUC’04 DF # solutions # concepts # sentences Avg. time (sec) TAC’08 1 2 3 4 1 2 3 4 1.3 2 955 184 22.3 1.3 676 175 1.7 1.5 247 159 0.5 1.5 107 139 0.3 1.2 2 909 174 21.5 1.3 393 167 0.8 1.8 127 149 0.3 4.8 56 129 0.2 Table 1: Average number of optimal solutions, concepts and sentences for different minimum document frequencies. The average time in seconds for finding the first optimal solution is also reported. where the concept was seen. Document frequency is a simple, yet effective approach to concept weighting (Gillick and Favre, 2009; Woodsend and Lapata, 2012; Qian and Liu, 2013). Reducing the number of concepts in the ILP model is then performed by pruning those concepts that occur in fewer than a given number of documents. ILP solvers usually provide only one solution. To generate alternate optimal solutions, we iteratively add new constraints to the problem that eliminate already found optimal solutions and rerun the solver. We stop the iterations when the value of the objective function returned by the solver changes. 3 3.1 Experiments Datasets and evaluation measures Experiments are conducted on the DUC’04 and TAC’08 datasets. For DUC’04, we"
F12-1098,devillers-etal-2004-french,0,0.0831867,"Missing"
F12-1098,esteve-etal-2010-epac,1,0.884492,"Missing"
F12-1098,gravier-etal-2004-ester,0,0.0432159,"Missing"
F12-1098,W11-0146,1,0.886156,"Missing"
F12-1098,W11-2039,1,0.883942,"Missing"
H05-1062,P03-1006,0,0.00756198,"ke the one observed when processing ASR output. In order to reduce and control the insertion rate of our NER system, we implemented a two level approach: the first level is made of NE grammars coded as Finite State Machine (FSM) transducers and the second level is a statistical HMM-based tagger. 5.2.1 NE transducers To each NE category is attached a set of regular grammars, extracted from the ESTER training corpus and generalized thanks to the annotation guidelines and web-gathered word lists. Theses grammars are represented by Finite State Machines (FSMs) (thanks to the AT&T GRM/FSM toolkit (Allauzen et al., 2003)). These FSMs are transducers that accept word sequences on the input symbols and output NE labels on the output symbols. They are all grouped together in a single transducer, called Tgram , with a filler model that accepts any string of words. Because these FSMs are lexicalized with the words of the ASR lexicon, one can control the generalization capabilities of the grammars thanks to the occurrence contexts of these words in the training corpus. During the NER process, the first step is to compose the FSM representing the NE transducer and the output of the ASR module (either a 1-best word s"
H05-1062,W98-1118,0,0.0234423,"Rtext in the experiment section. The second NER system has been developed for this study and is specifically built for being tightly integrated with the ASR processes. The two main features of this system, called NERasr in the following, are its ability to process word lattices and the fact that the NER models are trained for a specific ASR lexicon. These two systems are going to be presented in the next sections. 5.1 Text-based NER system: NERtext Among all the different methods that have been proposed for NER, one can find rule based models (Cunningham et al., 2002), Maximum Entropy models (Brothwick et al., 1998), Conditionnal Random Fields or probabilistic HMM-based models (Bikel et al., 1999). Lingpipe implements an HMM-based model. It maximizes the probability of a tag sequence Ti over 1 Lingpipe: http://alias-i.com/lingpipe/ 494 a word sequence Wi . A context of two preceding words and one preceding tag is used to approximate this probability. Generalization is done through a simple process: words occurring with low frequency are replaced by feature based categories (capitalized, contains digits, . . . ). In this approach, there must be one tag per word. Words starting and ending entities are labe"
H05-1062,A00-1044,0,0.0400847,"Missing"
H05-1062,H01-1034,0,0.498172,"nderstanding Conferences (MUC), the Conferences on Natural Language Learning (CoNLL), the DARPA HUB-5 program or more recently the French ESTER Rich Transcription program on Broadcast News data. Most of these conferences have studied the impact of using transcripts generated by an Automatic Speech Recognition (ASR) system rather than written texts. It appears from these studies that unlike other IE tasks, NER performance is greatly affected by the Word Error Rate (WER) of the transcripts processed. To tackle this problem, different ideas have been proposed: modeling explicitly the ASR errors (Palmer and Ostendorf, 2001) or using the ASR system alternate hypotheses found in word lattices (Saraclar and Sproat, 2004). However performance in NER decreases dramatically when processing high WER transcripts like the ones that are obtained with unmatched conditions between the ASR training model and the data to process. This paper investigates this phenomenon in the framework of the NER task of the French Rich Transcription program of Broadcast News ESTER (Gravier et al., 2004). Several issues are addressed: Traditional approaches to Information Extraction (IE) from speech input simply consist in applying text based"
H05-1062,N04-1017,0,0.143584,"UB-5 program or more recently the French ESTER Rich Transcription program on Broadcast News data. Most of these conferences have studied the impact of using transcripts generated by an Automatic Speech Recognition (ASR) system rather than written texts. It appears from these studies that unlike other IE tasks, NER performance is greatly affected by the Word Error Rate (WER) of the transcripts processed. To tackle this problem, different ideas have been proposed: modeling explicitly the ASR errors (Palmer and Ostendorf, 2001) or using the ASR system alternate hypotheses found in word lattices (Saraclar and Sproat, 2004). However performance in NER decreases dramatically when processing high WER transcripts like the ones that are obtained with unmatched conditions between the ASR training model and the data to process. This paper investigates this phenomenon in the framework of the NER task of the French Rich Transcription program of Broadcast News ESTER (Gravier et al., 2004). Several issues are addressed: Traditional approaches to Information Extraction (IE) from speech input simply consist in applying text based methods to the output of an Automatic Speech Recognition (ASR) system. If it gives satisfaction"
H05-1062,N04-4010,0,0.056164,"y language models (built from textual data) or artificially introduced. One should remove these from the transcript to improve the quality of the labeling. In order to deal with ASR errors two approaches have been proposed: • modeling explicitly the ASR errors, thanks to a development corpus and a set of confidence measures, in order to detect the possible errors of the 1-best word string hypothesis (with the type of errors) before extracting the NEs (Palmer and Ostendorf, 2001); • exploiting a search space bigger than the 1-best hypothesis alone, either by taking into account an n-best list (Zhai et al., 2004) or the whole word lattice (Saraclar and Sproat, 2004). The method proposed in this paper is close to this second approach where the whole word lattice output by the ASR system is used in order to increase NER performance from noisy input. We will present also in the next section a new strategy for adapting NER models to ASR transcripts, based on one of the main characteristics of such transcripts: a closed vocabulary is used by the ASR system. To our knowledge this has never been fully exploited by NER systems. Indeed while the key point of NER systems on written text is their generalization"
hong-etal-2014-repository,W12-2601,1,\N,Missing
hong-etal-2014-repository,W04-3252,0,\N,Missing
hong-etal-2014-repository,N09-1041,0,\N,Missing
hong-etal-2014-repository,C00-1072,0,\N,Missing
hong-etal-2014-repository,P06-2020,1,\N,Missing
hong-etal-2014-repository,J13-2002,1,\N,Missing
hong-etal-2014-repository,D11-1043,1,\N,Missing
hong-etal-2014-repository,W04-1013,0,\N,Missing
hong-etal-2014-repository,E14-1075,1,\N,Missing
hong-etal-2014-repository,C12-2078,0,\N,Missing
hong-etal-2014-repository,P13-2024,1,\N,Missing
hong-etal-2014-repository,P08-1094,1,\N,Missing
hong-etal-2014-repository,radev-etal-2004-mead,0,\N,Missing
hong-etal-2014-repository,E12-1023,0,\N,Missing
hong-etal-2014-repository,P11-1052,1,\N,Missing
hong-etal-2014-repository,W09-1802,1,\N,Missing
hong-etal-2014-repository,P13-1100,0,\N,Missing
hong-etal-2014-repository,P13-1101,0,\N,Missing
hong-etal-2014-repository,W13-3108,1,\N,Missing
L16-1046,P14-2131,0,0.135761,"qualities of the famous word embedding families, which can be different from the ones provided by works previously published in the scientific literature. Keywords: Word embeddings, benchmarking, speech processing, natural language processing 1. Introduction Word embeddings are projections in a continuous space of words supposed to preserve the semantic and syntactic similarities between them. They have been shown to be a great asset for several Natural Language Processing (NLP) tasks, like part-of-speech tagging, chunking, named entity recognition, semantic role labeling, syntactic parsing (Bansal et al., 2014a; Turian et al., 2010; Collobert et al., 2011), and also for speech processing: for instance, word embeddings were recently involved in spoken language understanding (Mesnil et al., 2015), in detection of errors in automatic transcriptions, and in calibration of confidence measures provided by an automatic speech recognition system (Ghannay et al., 2015). These word representations were introduced through the construction of neural language models (Bengio et al., 2003; Schwenk, 2013). Different approaches have been proposed to compute them from large corpora. They include neural networks (Col"
L16-1046,P12-1015,0,0.0430995,"ic questions such as adjective-to-adverb (amazing:amazingly → calm:?) and comparative (bad:worse → big:?). Overall, there are 8,869 semantic and 10,675 syntactic questions. A question is correctly answered if the proposed word is exactly the same as the correct one. The question is answered using Mikolov (Mikolov et al., 2013a) approach named 3CosAdd (addition and subtruction) in the literature. Finally, we want to evaluate the different word embeddings on a variety of word similarity tasks, based on corpora WordSim353 (Finkelstein et al., 2001), rare words (RW) (Luong et al., 2013) and, MEN (Bruni et al., 2012). These datasets contain word pairs with human similarity ratings. The evaluation of the word representations is performed by ranking the pairs according to their cosine similarities and measuring the Spearman’s rank correlation coefficient with the human judgment. 4. 4.1. Experiments Experimental setup The word embeddings described in section 2. are estimated on the annotated Gigaword corpus, which is composed of over 4 billion words. It contains dependency parses used for training w2vf-deps embeddings, and the unlabeled version is used to train the other embeddings. Note that words occurring"
L16-1046,N06-2015,0,0.0138471,"s, locations and organizations. There are 21 begin-inside-outside encoded word-level labels. The system is evaluated on the CoNLL 2003 benchmark (Tjong Kim Sang and De Meulder, 2003). • GloVe: This approach is introduced by (Pennington et al., 2014), and relies on constructing a global co-occurrence matrix of words in the corpus. The embedding vectors are based on the analysis of cooccurrences of words in a window. • Mention detection (MENT): recognizing mentions of entities for coreference resolution. There are 3 labels (begin, inside, outside). The task is performed on the Ontonotes corpus (Hovy et al., 2006) with the CoNLL 2012 split. CSLM word embeddings CSLM word embeddings are computed from unlabeled data by the CSLM toolkit (Schwenk, 2013), which estimates a feedforward neural language model. This approach projects the n−1 word indexes onto a continuous space and, from these word embeddings representations, computes the n-gram probabilities of each word in a short-list of the most Benchmark tasks NLP tasks In this sub-section, we briefly introduce the NLP tasks on which we evaluate the performance of the different word embeddings: part-of-speech tagging (POS), syntactic chunking (CHK), named"
L16-1046,P14-2050,0,0.740915,"iptions, and in calibration of confidence measures provided by an automatic speech recognition system (Ghannay et al., 2015). These word representations were introduced through the construction of neural language models (Bengio et al., 2003; Schwenk, 2013). Different approaches have been proposed to compute them from large corpora. They include neural networks (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014), dimensionality reduction on the word co-occurrence matrix (Lebret and Collobert, 2013), and explicit representation in terms of the context in which words appear (Levy and Goldberg, 2014). One particular hypothesis behind word embeddings is that they are generic representations that shall suit most applications. Many studies have focused on the evaluation of word embeddings intrinsic quality, as well as their impact when they are used as input of systems. Turian et al. (Turian et al., 2010) evaluate different types of word representations and their concatenation on the chunking and named entity recognition tasks. This work was partially funded by the European Commission through the EUMSSI project, under the contract number 611057, in the framework of the FP7-ICT-2013-10 call,"
L16-1046,Q15-1016,0,0.375486,"different types of word representations and their concatenation on the chunking and named entity recognition tasks. This work was partially funded by the European Commission through the EUMSSI project, under the contract number 611057, in the framework of the FP7-ICT-2013-10 call, by the French National Research Agency (ANR) through the VERA project, under the contract number ANR-12-BS02-006-01, and by the R´egion Pays de la Loire. The evaluation can be performed as well on the word similarity and analogical reasoning tasks, like in (Levy and Goldberg, 2014; Ji et al., 2015; Gao et al., 2014; Levy et al., 2015). Recently, the study proposed by (Levy et al., 2015), focuses on the evaluation of neural-network-inspired word embedding models (Skip-gram and GloVe) and traditional counted-based distributional models - pointwise mutual information (PMI) and Singular Value Decomposition (SVD) models-. This study reveals that the hyperparameter optimizations and certain system design choices have a considerable impact on the performance of word embeddings, rather than the embedding algorithms themselves. Moreover, it shows that, by adapting and transferring the hyperparameters into the traditional distributi"
L16-1046,W13-3512,0,0.0499144,":?), and nine types of syntactic questions such as adjective-to-adverb (amazing:amazingly → calm:?) and comparative (bad:worse → big:?). Overall, there are 8,869 semantic and 10,675 syntactic questions. A question is correctly answered if the proposed word is exactly the same as the correct one. The question is answered using Mikolov (Mikolov et al., 2013a) approach named 3CosAdd (addition and subtruction) in the literature. Finally, we want to evaluate the different word embeddings on a variety of word similarity tasks, based on corpora WordSim353 (Finkelstein et al., 2001), rare words (RW) (Luong et al., 2013) and, MEN (Bruni et al., 2012). These datasets contain word pairs with human similarity ratings. The evaluation of the word representations is performed by ranking the pairs according to their cosine similarities and measuring the Spearman’s rank correlation coefficient with the human judgment. 4. 4.1. Experiments Experimental setup The word embeddings described in section 2. are estimated on the annotated Gigaword corpus, which is composed of over 4 billion words. It contains dependency parses used for training w2vf-deps embeddings, and the unlabeled version is used to train the other embeddi"
L16-1046,J93-2004,0,0.0543816,"ith arbitrary features. This model is a generalization of the skip-gram model with negative sampling introduced by (Mikolov et al., 2013a), and it needs labeled data for training. As in (Levy and Goldberg, 2014), we derive contexts from dependency trees: a word is used to predict its governor and dependents, jointly with their dependency labels. This effectively allows for variable-size. 3. 3.1. • Part-Of-Speech Tagging (POS): categorizing words among 48 morpho-syntactic labels (noun, verb, adjective, etc.). The system is evaluated on the standard Penn Treebank benchmark train/dev/test split (Marcus et al., 1993). • Chunking (CHK): segmenting sentences in protosyntactic constituents. There are 22 begin-insideoutside encoded word-level labels. The system is evaluated on the CoNLL 2000 benchmark (Tjong Kim Sang and Buchholz, 2000). • Named Entity Recognition (NER): recognizing named entities in the text, such as persons, locations and organizations. There are 21 begin-inside-outside encoded word-level labels. The system is evaluated on the CoNLL 2003 benchmark (Tjong Kim Sang and De Meulder, 2003). • GloVe: This approach is introduced by (Pennington et al., 2014), and relies on constructing a global co-"
L16-1046,D14-1162,0,0.109932,"11), and also for speech processing: for instance, word embeddings were recently involved in spoken language understanding (Mesnil et al., 2015), in detection of errors in automatic transcriptions, and in calibration of confidence measures provided by an automatic speech recognition system (Ghannay et al., 2015). These word representations were introduced through the construction of neural language models (Bengio et al., 2003; Schwenk, 2013). Different approaches have been proposed to compute them from large corpora. They include neural networks (Collobert et al., 2011; Mikolov et al., 2013a; Pennington et al., 2014), dimensionality reduction on the word co-occurrence matrix (Lebret and Collobert, 2013), and explicit representation in terms of the context in which words appear (Levy and Goldberg, 2014). One particular hypothesis behind word embeddings is that they are generic representations that shall suit most applications. Many studies have focused on the evaluation of word embeddings intrinsic quality, as well as their impact when they are used as input of systems. Turian et al. (Turian et al., 2010) evaluate different types of word representations and their concatenation on the chunking and named ent"
L16-1046,W00-0726,0,0.167206,"derive contexts from dependency trees: a word is used to predict its governor and dependents, jointly with their dependency labels. This effectively allows for variable-size. 3. 3.1. • Part-Of-Speech Tagging (POS): categorizing words among 48 morpho-syntactic labels (noun, verb, adjective, etc.). The system is evaluated on the standard Penn Treebank benchmark train/dev/test split (Marcus et al., 1993). • Chunking (CHK): segmenting sentences in protosyntactic constituents. There are 22 begin-insideoutside encoded word-level labels. The system is evaluated on the CoNLL 2000 benchmark (Tjong Kim Sang and Buchholz, 2000). • Named Entity Recognition (NER): recognizing named entities in the text, such as persons, locations and organizations. There are 21 begin-inside-outside encoded word-level labels. The system is evaluated on the CoNLL 2003 benchmark (Tjong Kim Sang and De Meulder, 2003). • GloVe: This approach is introduced by (Pennington et al., 2014), and relies on constructing a global co-occurrence matrix of words in the corpus. The embedding vectors are based on the analysis of cooccurrences of words in a window. • Mention detection (MENT): recognizing mentions of entities for coreference resolution. Th"
L16-1046,W03-0419,0,0.231767,"Missing"
L16-1046,P10-1040,0,0.119677,"us word embedding families, which can be different from the ones provided by works previously published in the scientific literature. Keywords: Word embeddings, benchmarking, speech processing, natural language processing 1. Introduction Word embeddings are projections in a continuous space of words supposed to preserve the semantic and syntactic similarities between them. They have been shown to be a great asset for several Natural Language Processing (NLP) tasks, like part-of-speech tagging, chunking, named entity recognition, semantic role labeling, syntactic parsing (Bansal et al., 2014a; Turian et al., 2010; Collobert et al., 2011), and also for speech processing: for instance, word embeddings were recently involved in spoken language understanding (Mesnil et al., 2015), in detection of errors in automatic transcriptions, and in calibration of confidence measures provided by an automatic speech recognition system (Ghannay et al., 2015). These word representations were introduced through the construction of neural language models (Bengio et al., 2003; Schwenk, 2013). Different approaches have been proposed to compute them from large corpora. They include neural networks (Collobert et al., 2011; M"
L16-1070,L16-1494,1,0.823648,"a summary, and comments grouped appropriately and in thread context. The page is dynamically generated on the web server starting from a master document (an example of a metadocument) for a particular summarization of an article’s set of comments; the master document’s features include the information used to put the rest of the page together, in particular identifiers for other documents retrieved to generate the pie chart with its labels, the summary paragraphs, and the groups of comments. This UI prototype is described in more detail in our separate paper here on the task-based evaluation (Barker et al., 2016). 6. Conclusion We have presented a novel approach to storing, processing, and using conversational data of various kinds, which has been proven in use with a successful software deployment, with which several working input and output models are already interacting. We are currently adding asynchronous response handling through Spring’s ThreadPoolTaskExecutor (Johnson et al., 2014, §33) to the REST service, but otherwise the repository software is complete and we expect it to require no further essential work except for debugging as necessary and handling any additional features that might be"
L16-1070,S10-1021,0,0.0250374,"Missing"
L16-1070,A97-2017,1,0.369274,"d REST service with JSON for data interchange. This offers a number of advantages for easy interaction, testing, and debugging, and JSON avoids the verbosity and high-level parsing hassles associated with XML (e.g., walking a DOM tree or constructing one with SAX); it is also trivially easy to process with Python’s standard library and fairly easy to process in Java with the Jackson library (MongoLab, 2011; Saloranta, 2013; Ecma International, 2013). 2. Document model The main unit of data storage in the repository is a document object similar to the GATE or TIPSTER model (Wilks et al., 2000; Cunningham et al., 1997), which allows arbitrary data to be stored as stand-off annotations (grouped in named sets), annotation features, and document-wide features. The document is represented in the repository input and output as a JSON object1 containing an unique integer 1 The JSON object corresponds to other languages’ map, dictionary, or associative array. (Ecma International, 2013) id assigned by the repository, an optional name string, a content string representing the plain text content of the document (e.g., of a web page with the HTML tags stripped), a features object, and an annotations object. The featur"
L16-1070,P11-4015,1,0.817324,"Missing"
L16-1701,bechet-etal-2012-decoda,1,0.828923,"Missing"
L16-1701,W09-0505,1,0.873958,"Missing"
L18-1716,D11-1113,0,0.0240604,"Missing"
L18-1716,N07-1049,0,0.0621939,"Missing"
L18-1716,W16-3210,0,0.0493476,"Missing"
L18-1716,W05-1505,0,0.0831543,"Missing"
L18-1716,P11-4015,1,0.873913,"Missing"
L18-1716,Q14-1006,0,0.0432872,"age is described with five captions, each annotated with entities. Entities that corefer with a visual element in the image are linked to the corresponding bounding box. Finally, for every preposition manually attached, a set of possible attachment alternatives for use in a reranking system is produced. 2. Enriching the Flickr30k Entities Corpus with PP-Attachment Annotations Corpora with joint annotation of image and text has recently become widely available. The corpus used in this work is the Flickr30k Entities (F30kE) (Plummer et al., 2017), an extension of the original Flickr30k dataset (Young et al., 2014). This corpus is composed of almost 32K images and, for each image, five captions describing the image have been produced. Besides, every object in the image that corresponds to a mention in the captions has been manually identified with a bounding box. 4520 Preposition in with for near through on next to from into over by at of around in front of under behind along during across down against outside towards out of beside above in the middle of onto outside of inside between past toward on top of like among after away from off up up to before atop about along with underneath without out at the"
lefevre-etal-2012-leveraging,esteve-etal-2010-epac,1,\N,Missing
lefevre-etal-2012-leveraging,W11-0146,1,\N,Missing
lefevre-etal-2012-leveraging,W11-2039,1,\N,Missing
lefevre-etal-2012-leveraging,devillers-etal-2004-french,0,\N,Missing
N19-1393,J19-2006,0,0.0420386,"Missing"
N19-1393,D14-1082,0,0.0401592,"that are difficult to assess and would have prevented 1 2 https://wals.info/ http://universaldependencies.org 3919 Proceedings of NAACL-HLT 2019, pages 3919–3930 c Minneapolis, Minnesota, June 2 - June 7, 2019. 2019 Association for Computational Linguistics to measure the precise influence of the typological features on the behaviour of the parser. The fourth issue concerns the parser, which must be language independent and produce syntactic trees based on combinations of parameter values and sentential configurations. We use a transition-based parser with a multi-layer perceptron classifier (Chen and Manning, 2014), responsible for proposing how parameter values match observable patterns in the data. Our research hypotheses are: (a) features derived from the WALS enable cross-lingual sharing in multilingual parsing, and (b) these features do more than acting as mere language identifiers. Our main contributions are to reassess the utility of the WALS as informant of typological features of parsed languages, to evaluate their benefit in a controlled multilingual setting with full supervision, and to perform a set of analyses to better understand how they interact with the parser model. In addition to mult"
N19-1393,C12-1059,0,0.0406222,"Missing"
N19-1393,2016.jeptalnrecital-invite.2,0,0.0666192,"for multilingual dependency parsing. Our experiments on multilingual parsing for 40 languages show that typological information can indeed guide parsers to share information between similar languages beyond simple language identification. 1 Introduction Human languages may share some syntactic features, but differ on others. For example, some languages tend to place the subject before the verb (e.g., English) whereas others favour the reverse order (e.g., Arabic), and some do not exhibit a clear preference (e.g., Polish). These features can be viewed as the parameters of a language’s syntax (Greenberg, 1963; Chomsky, 1995). When training a multilingual parser, it could be interesting to explicitly represent these parameters, and to integrate them into the parsing model. If a successful strategy to do so was found, then, a parser could be trained simultaneously on several languages whose syntactic parameters have been explicitly represented. Such parser could then use a single model to parse texts in any language with known syntactic parameters. In theory, if we had at our disposal a set of parameters that completely describes the syntax of languages as well as treebanks that explore the whole sp"
N19-1393,P15-1119,0,0.0526325,": (a) features derived from the WALS enable cross-lingual sharing in multilingual parsing, and (b) these features do more than acting as mere language identifiers. Our main contributions are to reassess the utility of the WALS as informant of typological features of parsed languages, to evaluate their benefit in a controlled multilingual setting with full supervision, and to perform a set of analyses to better understand how they interact with the parser model. In addition to multilingual parsing, our method is suitable for zero-shot learning for under-resourced languages (Ammar et al., 2016; Guo et al., 2015). After discussing related work (Sec. 2), we describe UD (Sec. 3), the WALS (Sec. 4) and our parser (Sec. 5). The experimental setup (Sec. 6) precedes our results (Sec. 7), analyses (Sec. 8) and conclusions (Sec. 9). 2 Related Work Our work is at the intersection of three trends in the multilingual dependency parsing literature. The first is transfer parsing, when a parser is trained on a language (or a collection of languages) and tested on another one. The second is delexicalised parsing, which aims at abstracting away from the lexicon in order to neutralise genre, domain and topic biases wh"
N19-1393,N16-1121,0,0.0901739,"’ vocabulary. The third trend is the use of a handcrafted typological resources, such as the WALS, in multilingual NLP methods. Transfer parsing is often a suitable solution when dealing with low-resource languages (McDonald et al., 2011). Projected transfer relies on parallel corpora in which one of the languages does not have labelled training data to learn a parser, but the other does. One commonly employed solution is to use word alignments to project parsed sentences from one side onto the low-resource side of the parallel text, using heuristics (Hwa et al., 2005) or partial annotations (Lacroix et al., 2016). Agi´c et al. (2016) parse the resource-rich languages in a multi-parallel corpus, proposing a projection method to obtain POS tags and dependency trees for low-resource languages from multiple-language word alignments. The parsing model for the target language can also be obtained in an unsupervised fashion, by optimising a function that combines the likelihood of parallel data and the likelihood of the transferred model on non-annotated data in the low-resource language (Ma and Xia, 2014). Instead of assuming the availability of parallel corpora, direct transfer approaches capitalize on lan"
N19-1393,W14-4606,0,0.0214781,"source-rich languages in a multi-parallel corpus, proposing a projection method to obtain POS tags and dependency trees for low-resource languages from multiple-language word alignments. The parsing model for the target language can also be obtained in an unsupervised fashion, by optimising a function that combines the likelihood of parallel data and the likelihood of the transferred model on non-annotated data in the low-resource language (Ma and Xia, 2014). Instead of assuming the availability of parallel corpora, direct transfer approaches capitalize on language similarities. For instance, Lynn et al. (2014) build parser for Irish by first training a delexicalised parser on another language, and then applying it on Irish. They surprisingly found out that Indonesian was the language providing the best parsing results for Irish, even if they do not belong to the same language family, because longdistance dependencies are better represented in Indonesian than in the other languages tested. Low-resource languages may have some (insufficient) amount of training material available. One can employ bilingual parsing, concatenating training corpora in two languages, to verify if there is an improvement in"
N19-1393,P14-1126,0,0.0930726,"low-resource side of the parallel text, using heuristics (Hwa et al., 2005) or partial annotations (Lacroix et al., 2016). Agi´c et al. (2016) parse the resource-rich languages in a multi-parallel corpus, proposing a projection method to obtain POS tags and dependency trees for low-resource languages from multiple-language word alignments. The parsing model for the target language can also be obtained in an unsupervised fashion, by optimising a function that combines the likelihood of parallel data and the likelihood of the transferred model on non-annotated data in the low-resource language (Ma and Xia, 2014). Instead of assuming the availability of parallel corpora, direct transfer approaches capitalize on language similarities. For instance, Lynn et al. (2014) build parser for Irish by first training a delexicalised parser on another language, and then applying it on Irish. They surprisingly found out that Indonesian was the language providing the best parsing results for Irish, even if they do not belong to the same language family, because longdistance dependencies are better represented in Indonesian than in the other languages tested. Low-resource languages may have some (insufficient) amoun"
N19-1393,D11-1006,0,0.166397,"Missing"
N19-1393,P12-1066,0,0.589557,"then use a single model to parse texts in any language with known syntactic parameters. In theory, if we had at our disposal a set of parameters that completely describes the syntax of languages as well as treebanks that explore the whole space of parameters and their values, then such a universal parser could be designed. To make such a program realistic, though, several issues have to be addressed. In this paper, we propose to study the feasibility of learning such multilingual parser by addressing some of these issues. The first one is the choice of syntactic parameters that will be used (Naseem et al., 2012; T¨ackstr¨om et al., 2013; Zhang and Barzilay, 2015). In our work, we approximate these parameters by extracting syntactic information from the World Atlas of Language Structures (WALS) (Dryer and Haspelmath, 2013). 1 A language is represented by a vector containing the values it selects in the WALS. This vector plays the role of the parameters mentioned above. The second issue is the design of a unified scheme for representing syntax. Our natural choice is the Universal Dependencies (UD) initiative. 2 UD specifically proposes a set of universal dependency relations, part-of-speech tags and m"
N19-1393,J08-4003,0,0.0903223,"Missing"
N19-1393,P05-1013,0,0.103528,"Missing"
N19-1393,E17-2102,0,0.0315184,"the WALS provide information about the structure of languages (Dryer and Haspelmath, 2013). These could be useful to guide multilingual parsers, informing them about the model parameters that can be shared among languages with similar characteristics. Naseem et al. (2012) and Zhang and Barzilay (2015) use word-order features available for all their languages, while Ponti et al. (2018) used features they judged relevant in many categories (not only word order). The parameters proposed in the WALS are not the only way to represent properties of languages. Meth¨ ods based on language embeddings (Ostling and Tiedemann, 2017; Bjerva et al., 2019) also constitute interesting language representation. T¨ackstr¨om et al. (2013) use a multilingual delexicalised transfer method, showing how selective parameter sharing, based on typological features and language family membership, can be incorporated in a discriminative graph-based dependency parser. They select the typological features based on those used by Naseem et al. (2012), removing two features not considered useful. The work closest to ours experimented with concatenating treebanks to train a multilingual parser (Ammar et al., 2016). The authors use an S-LSTM t"
N19-1393,P18-1142,0,0.133767,"Missing"
N19-1393,P17-1049,0,0.0583782,"Missing"
N19-1393,N13-1126,0,0.187912,"Missing"
N19-1393,W15-2137,0,0.094949,"te training corpora. However, in our case, we combine treebanks from many more sources (around 40 languages) and include typological features. The combination of corpora in multiple languages for parser training is facilitated by the recent advent of multilingual standards and resources, in particular in Universal Dependencies for dependency syntax (Nivre et al., 2016). This initiative enables the annotation of POS, morphology and syntactic dependencies for all languages with the same guidelines and label sets. The availability of such corpora favours the development of cross-lingual methods (Tiedemann, 2015). Multilingual parsing research is also encouraged by initiatives such as the CoNLL 2017 and 2018 shared tasks, on highly multilingual dependency parsing from raw text (Zeman et al., 2017, 2018). Delexicalised parsers ignore the word forms and lemmas when analysing a sentence, usually relying on more abstract features such as word classes 3920 and POS tags. The use of delexicalised parsers is especially relevant when learning multilingual parsers, since languages generally share only a limited amount of lexical units. The approach proposed by Zeman and Resnik (2008) consists in adapting a pars"
N19-1393,K17-3001,0,0.157189,"guages for parser training is facilitated by the recent advent of multilingual standards and resources, in particular in Universal Dependencies for dependency syntax (Nivre et al., 2016). This initiative enables the annotation of POS, morphology and syntactic dependencies for all languages with the same guidelines and label sets. The availability of such corpora favours the development of cross-lingual methods (Tiedemann, 2015). Multilingual parsing research is also encouraged by initiatives such as the CoNLL 2017 and 2018 shared tasks, on highly multilingual dependency parsing from raw text (Zeman et al., 2017, 2018). Delexicalised parsers ignore the word forms and lemmas when analysing a sentence, usually relying on more abstract features such as word classes 3920 and POS tags. The use of delexicalised parsers is especially relevant when learning multilingual parsers, since languages generally share only a limited amount of lexical units. The approach proposed by Zeman and Resnik (2008) consists in adapting a parser for a new related language using either parallel corpora or delexicalised parsing. This method can be used to quickly construct a parser if the source and target languages are sufficie"
N19-2021,P98-1013,0,0.0871379,"Missing"
N19-2021,D18-1002,0,0.0267427,"nguage classifiers with an adversarial objective to train task-specific but language agnostic representations. Besides the cross-lingual transfer problem, there are few studies of the impact of domain-adversarial training in a monolingual setup. For instance, (Liu et al., 2017) successfully uses this technique to improve generalization in a document classification task. It has also been used recently for varied tasks such as transfer learning on Q&A systems (Yu et al., 2018) or duplicate question detection (Shah et al., 2018) and removal of protected attributes from social media textual data (Elazar and Goldberg, 2018). 2.2 Semantic parsing model with an adversarial training scheme 3.2 Sequence encoding/decoding For all experiments we use a BIO label encoding. To ensure that output sequences respect the BIO constrains we implement an A∗ decoding strategy similar to the one proposed by (He et al., 2017). We further apply a coherence filter to the output of the tagging process. This filter ensures that the predicted semantic structure is acceptable. Given a sentence and a word w that is a Lexical Unit (LU) trigger, we select the frame F as being the most probable frame among the ones that can have w as a trig"
N19-2021,D15-1112,0,0.0293163,"Missing"
N19-2021,passonneau-etal-2012-masc,0,0.0148032,"null hypothesis is selected if its probability is higher than P (yt = O). Varying δ > 0 (resp. δ < 0) is equivalent to being more strict (resp. less strict) on the highest non-null hypothesis. By doing so we can study the precision/recall (P/R) trade-off of our models. This δ parameter is tuned on a validation set and we either provide the P/R curve or report scores for the F max setting. Robustness in Semantic Frame Parsing In Frame Semantic Parsing, data is scarce and classic evaluation settings seldom propose outof-domain test data. Despite the existence of out-of-domain corpora such MASC (Passonneau et al., 2012) and YAGS (Hartmann et al., 2017) the domain adaptation problem has been widely reported (Johansson and Nugues, 2008; Søgaard et al., 2015) but not extensively studied. Recently, (Hartmann et al., 2017) presented the first in depth study of the domain adaptation problem using the YAGS frame corpus. They show that the main problem in domain adaptation for frame semantic parsing is the frame identification step and propose a more robust classifier using predicate and context embeddings to perform frame identification. This approach is suitable for cascade systems such as SEMAFOR (Das et al., 201"
N19-2021,D18-1131,0,0.0229096,"et al., 2017) and sentiment analysis (Chen et al., 2016). These approaches introduce language classifiers with an adversarial objective to train task-specific but language agnostic representations. Besides the cross-lingual transfer problem, there are few studies of the impact of domain-adversarial training in a monolingual setup. For instance, (Liu et al., 2017) successfully uses this technique to improve generalization in a document classification task. It has also been used recently for varied tasks such as transfer learning on Q&A systems (Yu et al., 2018) or duplicate question detection (Shah et al., 2018) and removal of protected attributes from social media textual data (Elazar and Goldberg, 2018). 2.2 Semantic parsing model with an adversarial training scheme 3.2 Sequence encoding/decoding For all experiments we use a BIO label encoding. To ensure that output sequences respect the BIO constrains we implement an A∗ decoding strategy similar to the one proposed by (He et al., 2017). We further apply a coherence filter to the output of the tagging process. This filter ensures that the predicted semantic structure is acceptable. Given a sentence and a word w that is a Lexical Unit (LU) trigger,"
N19-2021,E17-1045,0,0.0149287,"probability is higher than P (yt = O). Varying δ > 0 (resp. δ < 0) is equivalent to being more strict (resp. less strict) on the highest non-null hypothesis. By doing so we can study the precision/recall (P/R) trade-off of our models. This δ parameter is tuned on a validation set and we either provide the P/R curve or report scores for the F max setting. Robustness in Semantic Frame Parsing In Frame Semantic Parsing, data is scarce and classic evaluation settings seldom propose outof-domain test data. Despite the existence of out-of-domain corpora such MASC (Passonneau et al., 2012) and YAGS (Hartmann et al., 2017) the domain adaptation problem has been widely reported (Johansson and Nugues, 2008; Søgaard et al., 2015) but not extensively studied. Recently, (Hartmann et al., 2017) presented the first in depth study of the domain adaptation problem using the YAGS frame corpus. They show that the main problem in domain adaptation for frame semantic parsing is the frame identification step and propose a more robust classifier using predicate and context embeddings to perform frame identification. This approach is suitable for cascade systems such as SEMAFOR (Das et al., 2014), (Hermann et al., 2014) and (Y"
N19-2021,D18-1548,0,0.0502383,"Missing"
N19-2021,P17-1044,0,0.254675,"e to obtain labels for the classification task. Firstly we perform experiments on a large multi-domain frame corpus (Marzinotto et al., 2018a) where only a relatively small number of frames where annotated, corresponding to possible targets in an Information Extraction applicative framework. We evaluate our adversarial framework with a semantic frame parser we developed on this corpus and presented in (Marzinotto et al., 2018b). Secondly we checked the genericity of our approach on the standard PropBank Semantic Role Labeling task on the CoNLL-2005 benchmark, with a tagging model proposed by (He et al., 2017). We show that in both cases adversarial learning increases all models generalization capabilities both on in and out-of-domain data. This paper addresses the issue of generalization for Semantic Parsing in an adversarial framework. Building models that are more robust to inter-document variability is crucial for the integration of Semantic Parsing technologies in real applications. The underlying question throughout this study is whether adversarial learning can be used to train models on a higher level of abstraction in order to increase their robustness to lexical and stylistic variations."
N19-2021,D17-1128,0,0.638184,"s searched where the taskspecific classifier is good and the domain classifier is bad. It has been shown in (Ganin and Lempitsky, 2015) that this implicitly optimizes the hidden representation towards domain independence. 3.1 Semantic parsing model: biGRU We use in this study a sequence tagging semantic frame parser that performs frame selection and argument classification in one step based on a deep bi-directional GRU tagger (biGRU ). The advantage of this architecture is its flexibility as it can be applied to several semantic parsing schemes such as PropBank (He et al., 2017) and FrameNet (Yang and Mitchell, 2017). More precisely, the model consists of a 4 layer bi-directional Gated Recurrent Unit (GRU) with highway connections (Srivastava et al., 2015). This model does not rely solely on word embeddings as input. Instead, it has a richer set of features including syntactic, morphological and surface features. (see (Marzinotto et al., 2018b) for more details). Except for words where we use pre-trained embeddings, we use randomly initialized embedding layers for categorical features. In NLP problems this approach has successfully been used to train cross-lingual word representations (Conneau et al., 201"
N19-2021,P14-1136,0,0.0166367,"YAGS (Hartmann et al., 2017) the domain adaptation problem has been widely reported (Johansson and Nugues, 2008; Søgaard et al., 2015) but not extensively studied. Recently, (Hartmann et al., 2017) presented the first in depth study of the domain adaptation problem using the YAGS frame corpus. They show that the main problem in domain adaptation for frame semantic parsing is the frame identification step and propose a more robust classifier using predicate and context embeddings to perform frame identification. This approach is suitable for cascade systems such as SEMAFOR (Das et al., 2014), (Hermann et al., 2014) and (Yang and Mitchell, 2017). In this paper we propose to study the generalization issue within the framework of a sequence tagging semantic frame parser that performs frame selection and argument classification in one step. 167 3.3 Adversarial Domain Classifier In order to design an efficient adversarial task, several criteria have to be met. The task has to be related to the biases it is supposed to alleviate. And furthermore, the adversarial task should not be correlated to the main task (i.e semantic parsing here), otherwise it may harm the model’s performances. Determining where these b"
N19-2021,Q15-1020,0,0.0307751,"words surrounding the LU. 6 Generalization to PropBank Parsing We further show that this adversarial learning technique can be used on other semantic frameworks such as Propbank. In PropBank Semantic Role Labeling, CoNLL-2005 uses Wall Street Journal (WSJ) for training and two test corpora. The in-domain (ID) test set is derived from WSJ and the out-of-domain (OOD) test set contains ’general fiction’ from the Brown corpus. In published works, there is always an important gap in performances between ID and OOD. Several studies have tried to develop models with better generalization capacities (Yang et al., 2015), (FitzGerald et al., 2015). In recent works, PropBank SRL systems have evolved and span classifier approaches have been replaced by current state of the art sequence tagging models that use recurrent neural networks (He et al., 2017) and neural atten171 biGRU biGRU +AC Target Identification in-domain out-of-domain D1 D2 D3 97.6 95.5 93.3 97.6 95.6 94.3 Frame Identification in-domain out-of-domain D1 D2 D3 93.8 93.4 90.9 95.3 94.5 91.2 Argument Identification in-domain out-of-domain D1 D2 D3 58.2 46.1 43.6 60.0 47.1 45.2 Table 3: Frame semantic parsing performances (Fmax). Models trained on D1"
N19-2021,C08-1050,0,0.0327132,"t to being more strict (resp. less strict) on the highest non-null hypothesis. By doing so we can study the precision/recall (P/R) trade-off of our models. This δ parameter is tuned on a validation set and we either provide the P/R curve or report scores for the F max setting. Robustness in Semantic Frame Parsing In Frame Semantic Parsing, data is scarce and classic evaluation settings seldom propose outof-domain test data. Despite the existence of out-of-domain corpora such MASC (Passonneau et al., 2012) and YAGS (Hartmann et al., 2017) the domain adaptation problem has been widely reported (Johansson and Nugues, 2008; Søgaard et al., 2015) but not extensively studied. Recently, (Hartmann et al., 2017) presented the first in depth study of the domain adaptation problem using the YAGS frame corpus. They show that the main problem in domain adaptation for frame semantic parsing is the frame identification step and propose a more robust classifier using predicate and context embeddings to perform frame identification. This approach is suitable for cascade systems such as SEMAFOR (Das et al., 2014), (Hermann et al., 2014) and (Yang and Mitchell, 2017). In this paper we propose to study the generalization issue"
N19-2021,D17-1302,0,0.0504865,"t (GRU) with highway connections (Srivastava et al., 2015). This model does not rely solely on word embeddings as input. Instead, it has a richer set of features including syntactic, morphological and surface features. (see (Marzinotto et al., 2018b) for more details). Except for words where we use pre-trained embeddings, we use randomly initialized embedding layers for categorical features. In NLP problems this approach has successfully been used to train cross-lingual word representations (Conneau et al., 2017) and to transfer learning from English to low resource languages for POS tagging (Kim et al., 2017) and sentiment analysis (Chen et al., 2016). These approaches introduce language classifiers with an adversarial objective to train task-specific but language agnostic representations. Besides the cross-lingual transfer problem, there are few studies of the impact of domain-adversarial training in a monolingual setup. For instance, (Liu et al., 2017) successfully uses this technique to improve generalization in a document classification task. It has also been used recently for varied tasks such as transfer learning on Q&A systems (Yu et al., 2018) or duplicate question detection (Shah et al.,"
N19-2021,P17-1001,0,0.06826,"Missing"
N19-2021,L18-1159,1,0.864948,"Missing"
N19-2021,C98-1013,0,\N,Missing
N19-2021,Q18-1039,0,\N,Missing
nasr-etal-2014-automatically,H05-1066,0,\N,Missing
nasr-etal-2014-automatically,bazillon-etal-2012-syntactic,1,\N,Missing
nasr-etal-2014-automatically,bechet-etal-2012-decoda,1,\N,Missing
P11-4015,W10-1408,1,0.44138,"Missing"
P11-4015,2006.jeptalnrecital-long.5,0,0.0461776,"rk has been funded by the French Agence Nationale pour la Recherche, through the projects SEQUOIA (ANR-08EMER-013) and DECODA (2009-CORD-005-01) 1 Annotation must be taken here in a general sense which includes tagging, segmentation or the construction of more complex objets as syntagmatic or dependencies trees. 86 Proceedings of the ACL-HLT 2011 System Demonstrations, pages 86–91, c Portland, Oregon, USA, 21 June 2011. 2011 Association for Computational Linguistics of processing. 2 Several processing tools suites alread exist for French among which SXPIPE (Sagot and Boullier, 2008), OUTILEX (Blanc et al., 2006), NOOJ2 or UNI TEX 3 . A general comparison of MACAON with these tools is beyond the scope of this paper. Let us just mention that MACAON shares with most of them the use of finite state machines as core data representation. Some modules are implemented as standard operations on finite state machines. The MACAON exchange format is based on four concepts: segment, attribute, annotation level and segmentation. A segment refers to a segment of the text or speech signal that is to be processed, as a sentence, a clause, a syntactic constituent, a lexical unit, a named entity . . . A segment can be"
P11-4015,P06-1055,0,0.0181302,"ace that allows to inspect MACAON XML files and run the components. 3.3 sentation of many NLP tools input and output in the MACAON format. MACAON has been interfaced with the SPEERAL Automatic Speech Recognition System (Nocera et al., 2006). The word lattices produced by SPEERAL can be converted to pre-lexical MACAON automata. MACAON does not provide any native module for parsing yet but it can be interfaced with any already existing parser. For the purpose of this demonstration we have chosen the LORG parser developed at NCLT, Dublin14 . This parser is based on PCFGs with latent annotations (Petrov et al., 2006), a formalism that showed state-of-the-art parsing accuracy for a wide range of languages. In addition it offers a sophisticated handling of unknown words relying on automatically learned morphological clues, especially for French (Attia et al., 2010). Moreover, this parser accepts input that can be tokenized, postagged or pre-bracketed. This possibility allows for different settings when interfacing it with MACAON. 4 Applications MACAON has been used in several projects, two of which are briefly described here, the D EFINIENS project and the L UNA project. D EFINIENS (Barque et al., 2010) is"
P11-4015,W05-0618,0,\N,Missing
P11-4015,J08-2002,0,\N,Missing
P11-4015,sagot-etal-2006-lefff,0,\N,Missing
S16-1030,W15-2915,0,0.0227871,"Missing"
S16-1030,P11-2008,0,0.043589,"Missing"
S16-1030,C14-1048,0,0.0121858,"entral word. For a given context wi−2 . . . wi+2 , the input to the model is wi , and the output could be wi−2 , wi−1 , wi+1 , wi+2 . This method typically extracts a representation which both covers syntax and semantics, to some extent. Part-of-speech embeddings: as stated earlier, the lexical model cannot distinguish between the senses of words and creates a single representation per word form. For example, the word “apple” receives an embedding that is a mixture of its different contextual senses: fruit, company... A lot of sophisticated approaches have been proposed to tackle the problem (Guo et al., 2014; Neelakantan et al., 2015; Huang et al., 2012), by considering senses as latent variables during training, or by conditionning the training documents on topic distributions. In our system we follow a very simple approach which creates joint embeddings for words and their part of speech. Thus, for con205 text wi−2 . . . wi+2 tagged with the part-of-speech sequence pi−2 . . . pi+2 the input to the model is (wi , pi ) and the output is (wi−2 , pi−2 ), (wi−1 , pi−1 ), (wi+1 : pi+1 ), (wi+2 , pi+2 ). Sentiment embeddings: another problem with the basic skipgram approach (lexical embeddings) is tha"
S16-1030,P12-1092,0,0.0540489,"wi+2 , the input to the model is wi , and the output could be wi−2 , wi−1 , wi+1 , wi+2 . This method typically extracts a representation which both covers syntax and semantics, to some extent. Part-of-speech embeddings: as stated earlier, the lexical model cannot distinguish between the senses of words and creates a single representation per word form. For example, the word “apple” receives an embedding that is a mixture of its different contextual senses: fruit, company... A lot of sophisticated approaches have been proposed to tackle the problem (Guo et al., 2014; Neelakantan et al., 2015; Huang et al., 2012), by considering senses as latent variables during training, or by conditionning the training documents on topic distributions. In our system we follow a very simple approach which creates joint embeddings for words and their part of speech. Thus, for con205 text wi−2 . . . wi+2 tagged with the part-of-speech sequence pi−2 . . . pi+2 the input to the model is (wi , pi ) and the output is (wi−2 , pi−2 ), (wi−1 , pi−1 ), (wi+1 : pi+1 ), (wi+2 , pi+2 ). Sentiment embeddings: another problem with the basic skipgram approach (lexical embeddings) is that the model ignores the sentiment polarity of t"
S16-1030,D14-1181,0,0.0172989,"e participated in Subtask A: sentiment analysis at the message level. It consists in determining the message polarity of each tweet in the test set. The sentiment polarity classification task is set as a three-class problem: positive, negative and neutral. The sentiment analysis task is often modeled as a classification problem which relies on features ex1 http://www.sensei-conversation.eu/ tracted from the text in order to feed a classifier. Recent work has shown that Convolutional Neural Networks (CNN) using word representations as input are well suited for sentence classification problems (Kim, 2014) and have been shown to produce state-of-the-art results for sentiment polarity classification (Tang et al., 2014a; Severyn and Moschitti, 2015). Pre-trained word embeddings are used to initialize the word representations, which are then taken as input of a text CNN. Our approach consists in learning polarity classifiers for three types of embeddings, based on the same CNN architecture. Each set of word embedding models the tweet according to a different point of view: lexical, part-of-speech and sentiment. A final fusion step is applied, based on concatenating the hidden layers of the CNNs an"
S16-1030,S16-1001,0,0.042913,"Missing"
S16-1030,P11-4015,1,0.519452,"Missing"
S16-1030,D14-1113,0,0.0671509,"Missing"
S16-1030,N13-1039,0,0.0298295,"Missing"
S16-1030,D15-1303,0,0.00672749,"tti, 2015). Pre-trained word embeddings are used to initialize the word representations, which are then taken as input of a text CNN. Our approach consists in learning polarity classifiers for three types of embeddings, based on the same CNN architecture. Each set of word embedding models the tweet according to a different point of view: lexical, part-of-speech and sentiment. A final fusion step is applied, based on concatenating the hidden layers of the CNNs and training a deep neural network for the fusion. Our contributions are as follows: • We extend the deep CNN architecture proposed in (Poria et al., 2015) and introduce lexical information similar to (Ebert et al., 2015). • We introduce polarity embeddings, tweet representations extracted from the hidden layer of CNNs with different word embeddings as input. • We fuse polarity embeddings by concatenating them and feeding them to a neural network trained on the final task. • The source code of our system, the models trained for the evaluation, and the corpus collected for creating word embeddings are made 202 Proceedings of SemEval-2016, pages 202–208, c San Diego, California, June 16-17, 2016. 2016 Association for Computational Linguistics avai"
S16-1030,S15-2079,0,0.0606385,"Missing"
S16-1030,S14-2033,0,0.227741,"ge polarity of each tweet in the test set. The sentiment polarity classification task is set as a three-class problem: positive, negative and neutral. The sentiment analysis task is often modeled as a classification problem which relies on features ex1 http://www.sensei-conversation.eu/ tracted from the text in order to feed a classifier. Recent work has shown that Convolutional Neural Networks (CNN) using word representations as input are well suited for sentence classification problems (Kim, 2014) and have been shown to produce state-of-the-art results for sentiment polarity classification (Tang et al., 2014a; Severyn and Moschitti, 2015). Pre-trained word embeddings are used to initialize the word representations, which are then taken as input of a text CNN. Our approach consists in learning polarity classifiers for three types of embeddings, based on the same CNN architecture. Each set of word embedding models the tweet according to a different point of view: lexical, part-of-speech and sentiment. A final fusion step is applied, based on concatenating the hidden layers of the CNNs and training a deep neural network for the fusion. Our contributions are as follows: • We extend the deep CNN archi"
S16-1030,P14-1146,0,0.365654,"ge polarity of each tweet in the test set. The sentiment polarity classification task is set as a three-class problem: positive, negative and neutral. The sentiment analysis task is often modeled as a classification problem which relies on features ex1 http://www.sensei-conversation.eu/ tracted from the text in order to feed a classifier. Recent work has shown that Convolutional Neural Networks (CNN) using word representations as input are well suited for sentence classification problems (Kim, 2014) and have been shown to produce state-of-the-art results for sentiment polarity classification (Tang et al., 2014a; Severyn and Moschitti, 2015). Pre-trained word embeddings are used to initialize the word representations, which are then taken as input of a text CNN. Our approach consists in learning polarity classifiers for three types of embeddings, based on the same CNN architecture. Each set of word embedding models the tweet according to a different point of view: lexical, part-of-speech and sentiment. A final fusion step is applied, based on concatenating the hidden layers of the CNNs and training a deep neural network for the fusion. Our contributions are as follows: • We extend the deep CNN archi"
W09-1802,P06-2020,0,0.00923621,"Missing"
W09-1802,W04-1017,0,0.0187456,"Missing"
W09-1802,W00-0405,0,0.0613133,"ion is quite hard, the most successful systems tested at the Text Analysis Conference (TAC) and Document Understanding Conference (DUC)1 , for example, are extractive. In particular, sentence selection represents a reasonable trade-off between linguistic quality, guaranteed by longer textual units, and summary content, often improved with shorter units. Whereas the majority of approaches employ a greedy search to find a set of sentences that is 1 TAC is a continuation of DUC, which ran from 2001-2007. Methods like McDonald’s, including the wellknown Maximal Marginal Relevance (MMR) algorithm (Goldstein et al., 2000), are subject to another problem: Summary-level redundancy is not always well modeled by pairwise sentence-level redundancy. Figure 1 shows an example where the combination of sentences (1) and (2) overlaps completely with sentence (3), a fact not captured by pairwise redundancy measures. Redundancy, like content selection, is a global problem. Here, we discuss a model for sentence selection with a globally optimal solution that also addresses redundancy globally. We choose to represent infor10 Proceedings of the NAACL HLT Workshop on Integer Linear Programming for Natural Language Processing,"
W09-1802,hassel-sjobergh-2006-towards,0,0.0371293,"Missing"
W09-1802,J06-4003,0,0.0154584,"is only possible if it is present in at least one selected sentence. Constraint (1) also prevents the inclusion of concept-less sentences. 4 Performance Here we compare both models on a common summarization task. The data is part of the Text Analysis Conference (TAC) multi-document summarization evaluation and involves generating 100-word summaries from 10 newswire documents, each on a given topic. While the 2008 edition of TAC also includes an update task—additional summaries assuming some prior knowledge—we focus only on 12 2. Split text into sentences. We use the unsupervised Punkt system (Kiss and Strunk, 2006). 3. Prune sentences shorter than 5 words. 4. Compute parameters needed by the models. 5. Map to ILP format and solve. We use an open source solver3 . 6. Order sentences picked by the ILP for inclusion in the summary. The specifics of step 4 are described in detail in (McDonald, 2007) and (Gillick et al., 2008). McDonald’s sentence relevance combines word-level cosine similarity with the source document and the inverse of its position (early sentences tend to be more important). Redundancy between a pair of sentences is their cosine similarity. For sentence i in document D, Reli = cosine(i, D)"
W09-1802,W04-1013,0,0.134932,"lationship between the document frequency of input bigrams and the fraction of those bigrams that appear in the human generated “gold” set: Let di be document frequency i and pi be the percent of input bigrams with di that are actually in the gold set. Then the correlation ρ(D, P ) = 0.95 for DUC 2007 and 0.97 for DUC 2006. Data here averaged over all problems in DUC 2007. The summaries produced by the two systems have been evaluated automatically with ROUGE and manually with the Pyramid metric. In particular, ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries (Lin, 2004). The Pyramid score arises from a manual alignment of basic facts from the reference summaries, called Summary Content Units (SCUs), in a hypothesis summary (Nenkova and Passonneau, 2004). We used the SCUs provided by the TAC evaluation. Table 2 compares these results, alongside a baseline that uses the first 100 words of the most recent document. All the scores are significantly different, showing that according to both human and automatic content evaluation, the conceptbased model outperforms McDonald’s sentencebased model, which in turn outperforms the baseline. Of course, the relevance and"
W09-1802,E06-1038,0,0.020745,"ncy heuristic by learning bigram values have not produced significant gains. Better features may be necessary. However, since the ILP gives optimal solutions so quickly, we are more interested in discriminative training where we learn weights for features that 17 push the resulting summaries in the right direction, as opposed to the individual concept values. Third, our rule-based sentence compression is more of a proof of concept, showing that joint compression and optimal selection is feasible. Better statistical methods have been developed for producing high quality compression candidates (McDonald, 2006), that maintain linguistic quality, some recent work even uses ILPs for exact inference (Clarke and Lapata, 2008). The addition of compressed sentences tends to yield less coherent summaries, making sentence ordering more important. We would like to add constraints on sentence ordering to the ILP formulation to address this issue. Acknowledgments This work is supported by the Defense Advanced Research Projects Agency (DARPA) GALE project, under Contract No. HR0011-06-C-0023. Any opinions, findings, conclusions or recommendations expressed in this material are those of the authors and do not ne"
W09-1802,N04-1019,0,0.0425243,"i and pi be the percent of input bigrams with di that are actually in the gold set. Then the correlation ρ(D, P ) = 0.95 for DUC 2007 and 0.97 for DUC 2006. Data here averaged over all problems in DUC 2007. The summaries produced by the two systems have been evaluated automatically with ROUGE and manually with the Pyramid metric. In particular, ROUGE-2 is the recall in bigrams with a set of human-written abstractive summaries (Lin, 2004). The Pyramid score arises from a manual alignment of basic facts from the reference summaries, called Summary Content Units (SCUs), in a hypothesis summary (Nenkova and Passonneau, 2004). We used the SCUs provided by the TAC evaluation. Table 2 compares these results, alongside a baseline that uses the first 100 words of the most recent document. All the scores are significantly different, showing that according to both human and automatic content evaluation, the conceptbased model outperforms McDonald’s sentencebased model, which in turn outperforms the baseline. Of course, the relevance and redundancy functions used for McDonald’s formulation in this experiment are rather primitive, and results would likely improve with better relevance features as used in many TAC systems."
W09-1802,I08-1016,0,0.00983791,"runing concepts may be more desirable: Pruned sentences are irretrievable, but pruned concepts may well appear in the selected sentences through co-occurrence. Figure 2 compares ILP run-times for the two 6 Extensions Here we describe how our ILP formulation can be extended with additional constraints to incorporate sentence compression. In particular, we are interested in creating compressed alternatives for the original sentence by manipulating its parse tree (Knight and Marcu, 2000). This idea has been applied with some success to summarization (Turner and Charniak, 2005; Hovy et al., 2005; Nenkova, 2008) with the goal of removing irrelevant or redundant details, thus freeing space for more relevant information. One way to achieve this end is to generate compressed candidates for each sentence, creating an expanded pool of input sentences, and em14 8 Average Time per Problem (seconds) formulations, using a set of 25 topics from DUC 2007, each of which have at least 500 input sentences. These are very similar to the TAC 2008 topics, but more input documents are provided for each topic, which allowed us to extend the analysis to larger problems. While the ILP solver finds optimal solutions effic"
W09-1802,P05-1036,0,0.00592003,"lowweight concepts in the latter. Note that pruning concepts may be more desirable: Pruned sentences are irretrievable, but pruned concepts may well appear in the selected sentences through co-occurrence. Figure 2 compares ILP run-times for the two 6 Extensions Here we describe how our ILP formulation can be extended with additional constraints to incorporate sentence compression. In particular, we are interested in creating compressed alternatives for the original sentence by manipulating its parse tree (Knight and Marcu, 2000). This idea has been applied with some success to summarization (Turner and Charniak, 2005; Hovy et al., 2005; Nenkova, 2008) with the goal of removing irrelevant or redundant details, thus freeing space for more relevant information. One way to achieve this end is to generate compressed candidates for each sentence, creating an expanded pool of input sentences, and em14 8 Average Time per Problem (seconds) formulations, using a set of 25 topics from DUC 2007, each of which have at least 500 input sentences. These are very similar to the TAC 2008 topics, but more input documents are provided for each topic, which allowed us to extend the analysis to larger problems. While the ILP s"
W09-2818,W07-2302,0,\N,Missing
W10-4230,W09-2818,1,0.789022,"as to choose. This task is challenging because of the following aspects: 1. The data is imperfect as it is a patchwork of multiple authors’ writing. 2. The problem is hard to handle with a classifier because text is predicted, not classes. 3. The problem has a complex graph structure. 4. Some decisions are recursive for embedded references, i.e. “his father”. 5. Syntactic/semantic features cannot be extracted with a classical parser because the word sequence is latent. We do not deal with all of these challenges but we try to mitigate their impact. Our system extends our approach for GREC’09 (Favre and Bonhet, 2009). We use a sequence classifier to predict generic labels for the possible expressions. Labels for classification • “short” if the expression is a one-word long name or common name. • “nesting” if the expression is recursive. For recursive expressions, a special handling is applied: All possible assignments of the embedded entities are generated with labels corresponding to the concatenation of the involved entities’ labels. If the embedding is on the right (left) side of the expression, “right” (“left”) is added to the label. Non-sensical labels (i.e. “he father”) are not seen in the training"
W11-2835,C10-1012,1,0.800448,"atical function relation labels (as SSyntR).1 The system thus realizes the following steps: 1. Semantic graph → Deep-syntactic tree 2. Deep-syntactic tree → Surface-syntactic tree 3. Surface-syntactic tree → Linearized structure 4. Linearized structure → Surface In addition, two auxiliary steps are carried out. The first one is part-of-speech tagging; it is carried out after step 3. The second one is introduction of commata; it is done after step 4. Each step is implemented as a decoder that uses a classifier to select the appropriate operations. For the realization of the classifiers, we use Bohnet et al. (2010)’s implementation of MIRA (Margin Infused Relaxed Algorithm) (Crammer et al., 2006). 2 Sentence Realization Sentence generation consists in the application of the previously trained decoders in sequence 1.–4., plus the two auxiliary steps. 1 The DSyntR is inspired by the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x)"
W11-2835,P04-1015,0,0.0509894,"the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x) = argmax Score(y), where y ∈ M AP (x) (with M AP (x) as the set of all trees spanning over the nodes of the Sem-graph x). As in (Bohnet et al., 2011), the search is a beam search which creates a maximum spanning tree using “early update” as introduced for parsing by Collins and Roark (2004): when the correct beam element drops out of the beam, we stop and update the model using the best partial solution. The idea is that when all items in the current beam are incorrect, further processing is obsolete since the correct solution cannot be reached extending any elements of the beam. When we reach a final state, i.e. a tree spanning over all words and the correct solution is in the beam but not ranked first, we perform an update as well, since the correct element should have ranked first in the beam. Algorithm 1 displays the algorithm for the generation of the DSyntR from the SemR."
W11-2835,C96-1058,0,0.0241739,"uses a classifier to select the appropriate operations. For the realization of the classifiers, we use Bohnet et al. (2010)’s implementation of MIRA (Margin Infused Relaxed Algorithm) (Crammer et al., 2006). 2 Sentence Realization Sentence generation consists in the application of the previously trained decoders in sequence 1.–4., plus the two auxiliary steps. 1 The DSyntR is inspired by the DSynt structures in (Mel’ˇcuk, 1988), only that the latter are still “deeper”. Semantic Generation Our derivation of the DSynt-tree from an input Sem-graph is analogous to graph-based parsing algorithms (Eisner, 1996). It is defined as search for the highest scoring tree y from all possible trees given an input graph x: F (x) = argmax Score(y), where y ∈ M AP (x) (with M AP (x) as the set of all trees spanning over the nodes of the Sem-graph x). As in (Bohnet et al., 2011), the search is a beam search which creates a maximum spanning tree using “early update” as introduced for parsing by Collins and Roark (2004): when the correct beam element drops out of the beam, we stop and update the model using the best partial solution. The idea is that when all items in the current beam are incorrect, further proces"
W12-3412,W09-3821,0,0.0545879,"Missing"
W12-3412,C10-2013,0,0.219842,"ith a linear model (5). The parse with the best score is considered as final. The structure of the paper is the following: in Section 2 we describe the details of our generative parser and in Section 3 our reranking model together with the features templates. Section 4 reports the results of the experiments conducted on the Penn Treebank (Marcus et al., 1994) as well as on the Paris 7 Treebank (Abeillé et al., 2003) and Section 5 concludes the paper. 2 Generative Model The first part of our system, the syntactic analysis itself, generates surface dependency structures in a sequential fashion (Candito et al., 2010b; Candito et al., 2010a). A phrase structure parser based on Latent Variable PCFGs (P CFG -L As) produces tree structures that are enriched with functions and then converted to labelled dependency structures, which will be processed by the parse reranker. 2.1 P CFG -L As Probabilistic Context Free Grammars with Latent Annotations, introduced in (Matsuzaki et al., 2005) can be seen as automatically specialised PCFGs learnt from treebanks. Each symbol of the grammar is enriched with annotation symbols behaving as subclasses of this symbol. More formally, the probability of an unannotated tree i"
W12-3412,candito-etal-2010-statistical,0,0.196867,"ith a linear model (5). The parse with the best score is considered as final. The structure of the paper is the following: in Section 2 we describe the details of our generative parser and in Section 3 our reranking model together with the features templates. Section 4 reports the results of the experiments conducted on the Penn Treebank (Marcus et al., 1994) as well as on the Paris 7 Treebank (Abeillé et al., 2003) and Section 5 concludes the paper. 2 Generative Model The first part of our system, the syntactic analysis itself, generates surface dependency structures in a sequential fashion (Candito et al., 2010b; Candito et al., 2010a). A phrase structure parser based on Latent Variable PCFGs (P CFG -L As) produces tree structures that are enriched with functions and then converted to labelled dependency structures, which will be processed by the parse reranker. 2.1 P CFG -L As Probabilistic Context Free Grammars with Latent Annotations, introduced in (Matsuzaki et al., 2005) can be seen as automatically specialised PCFGs learnt from treebanks. Each symbol of the grammar is enriched with annotation symbols behaving as subclasses of this symbol. More formally, the probability of an unannotated tree i"
W12-3412,W10-1408,1,0.890534,"Missing"
W12-3412,W08-2102,0,0.107256,"for Computational Linguistics, pages 89–99, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics pendents of a predicate. On the other hand, dependencies extracted from constituent parses are known to be more accurate than dependencies obtained from dependency parsers. Therefore the solution we choose is an indirect one: we use a phrase-based parser to generate n-best lists and convert them to lists of dependency structures that are reranked. This approach can be seen as trade-off between phrasebased reranking experiments (Collins, 2000) and the approach of Carreras et al. (2008) where a discriminative model is used to score lexical features representing unlabelled dependencies in the Tree Adjoining Grammar formalism. Our architecture, illustrated in Figure 1, is based on two steps. During the first step, a syntagmatic parser processes the input sentence and produces nbest parses as well as their probabilities. They are annotated with a functional tagger which tags syntagms with standard syntactic functions subject, object, indirect object . . . and converted to dependency structures by application of percolation rules. In the second step, we extract a set of features"
W12-3412,P05-1022,0,0.610797,"ect of some recent work thanks to progresses achieved in the field of Machine Learning. A parse tree is represented as a vector of features and its accuracy is measured as the distance between this vector and the reference. One way to take advantage of both approaches is to combine them sequentially, as initially proposed by Collins (2000). A generative parser produces a set of candidates structures that constitute the input of a second, discriminative module, whose search space is limited to this set of candidates. Such an approach, parsing followed by reranking, is used in the Brown parser (Charniak and Johnson, 2005). The approach can be extended in order to feed the reranker with the output of different parsers, as shown by (Johnson and Ural, 2010; Zhang et al., 2009). In this paper we are interested in applying reranking to dependency structures. The main reason is that many linguistic constraints are straightforward to implement on dependency structures, as, for example, subcategorization frames or selectional constraints that are closely linked to the notion of deProceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 89–99, c Jeju, Republic of Korea, 12 July 201"
W12-3412,P97-1003,0,0.249871,"his can be implemented efficiently using two weight vectors as for the averaged perceptron. parses as possible, but specific enough to characterize good and bad configurations in the parse tree. We extended the feature set from (McDonald, 2006) which showed to be effective for a range of languages. Our feature templates can be categorized in 5 classes according to their domain of locality. In the following, we describe and exemplify these templates on the following sentence from the Penn treebank, in which we target the PMOD dependency between “at” and “watch.” like bilexical dependencies in (Collins, 1997). l Given dependency xi → xj , the feature created is (word xi , lemma xi , pos-tag xi , word xj , lemma xj , pos-tag xj , distance3 from i to j, direction, type). The previous example generates the following feature: [at, at, IN, watch, watch, NN, 2, R, PMOD] Where 2 is the distance between “at” and “watch”. Probability Three features are derived from the P CFG -L A parser, namely the posterior probability of the parse (eq. 1), its normalized probability relative to the 1-best, and its rank in the n-best list. Unigram Unigram features are the most simple as they only involve one word. Given a"
W12-3412,Y09-1013,0,0.0423233,"odel, namely Tree Adjoining Grammars, and the system of Suzuki et al. (2009) that makes use of external data (unannotated text). Huang, 2008 Bohnet, 2010 Zhang et al, 2008 Huang and Sagae, 2010 Charniak et al, 2005 Carreras et al. 2008 Suzuki et al. 2009 This work F 91.7 – 91.4 – 91.5 – – 91.1 LAS – 90.3 – – 90.0 – – 89.8 UAS – – 93.2 92.1 94.0 93.5 93.8 93.9 Table 3: Comparison on PTB Test set For French, see Table 4, we compare our system with the M ATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al., 2005) with hash kernels, using the ME LT part-of-speech tagger (Denis and Sagot, 2009) and our own lemmatiser. We also compare the French system with results drawn from the benchmark performed by Candito et al. (2010a). The first system (BKY-FR) is close to ours without the reranking module, using the Berkeley parser adapted to French. The second (MSTFR) is based on MSTParser (McDonald et al., 2005). These two system use word clusters as well. The next section takes a close look at the models 96 This work M ATE + ME LT B KY-F R M ST-F R F < 40 89.2 – 88.2 – LAS 89.2 89.2 86.8 88.2 UAS 92.1 91.8 91.0 90.9 Table 4: Comparison on FTB Test set 4.3.4 Model Analysis It is interesting"
W12-3412,P08-1067,0,0.164776,"k + MElt 87.4 89.2 89.2 92.1 7 The model is always trained with 100 candidates. F < 40 is the parseval F-score for sentences with less than 40 words. 8 95 Table 2: System results on FTB Test set 4.3.3 Comparison with Related Work of the reranker and its impact on performance. We compare our results with related parsing results on English and French. For English, the main results are shown in Table 3. From the presented data, we can see that indirect reranking on LAS may not seem as good as direct reranking on phrase-structures compared to F-scores obtained in (Charniak and Johnson, 2005) and (Huang, 2008) with one parser or (Zhang et al., 2009) with several parsers. However, our system does not rely on any language specific feature and can be applied to other languages/treebanks. It is difficult to compare our system for LAS because most systems evaluate on gold data (part-of-speech, lemmas and morphological information) like Bohnet (2010). Our system also compares favourably with the system of Carreras et al. (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al. (2009) that makes use of external data (unannotated text). Huang, 2"
W12-3412,W07-2416,0,0.0538469,"Missing"
W12-3412,H94-1020,0,0.383534,"tion w (5) MIRA reranking Final constituency & dependency parse Figure 1: The parsing architecture: production of the nbest syntagmatic trees (1) tagged with functional labels (2), conversion to a dependency structure (3) and feature extraction (4), scoring with a linear model (5). The parse with the best score is considered as final. The structure of the paper is the following: in Section 2 we describe the details of our generative parser and in Section 3 our reranking model together with the features templates. Section 4 reports the results of the experiments conducted on the Penn Treebank (Marcus et al., 1994) as well as on the Paris 7 Treebank (Abeillé et al., 2003) and Section 5 concludes the paper. 2 Generative Model The first part of our system, the syntactic analysis itself, generates surface dependency structures in a sequential fashion (Candito et al., 2010b; Candito et al., 2010a). A phrase structure parser based on Latent Variable PCFGs (P CFG -L As) produces tree structures that are enriched with functions and then converted to labelled dependency structures, which will be processed by the parse reranker. 2.1 P CFG -L As Probabilistic Context Free Grammars with Latent Annotations, introdu"
W12-3412,P05-1010,0,0.0421253,"as on the Paris 7 Treebank (Abeillé et al., 2003) and Section 5 concludes the paper. 2 Generative Model The first part of our system, the syntactic analysis itself, generates surface dependency structures in a sequential fashion (Candito et al., 2010b; Candito et al., 2010a). A phrase structure parser based on Latent Variable PCFGs (P CFG -L As) produces tree structures that are enriched with functions and then converted to labelled dependency structures, which will be processed by the parse reranker. 2.1 P CFG -L As Probabilistic Context Free Grammars with Latent Annotations, introduced in (Matsuzaki et al., 2005) can be seen as automatically specialised PCFGs learnt from treebanks. Each symbol of the grammar is enriched with annotation symbols behaving as subclasses of this symbol. More formally, the probability of an unannotated tree is the sum of the probabilities of its annotated counterparts. For a P CFG -L A G, R is the set of annotated rules, D(t) is the set of (annotated) derivations of an unannotated tree t, and R(d) is the set of rules used in a derivation d. Then the probability assigned by G to t is: PG (t) = X d∈D(t) PG (d) = X Y PG (r) (1) 3 Discriminative model d∈D(t) r∈R(d) Because of t"
W12-3412,P05-1012,0,0.0505025,"h the system of Carreras et al. (2008) that relies on a more complex generative model, namely Tree Adjoining Grammars, and the system of Suzuki et al. (2009) that makes use of external data (unannotated text). Huang, 2008 Bohnet, 2010 Zhang et al, 2008 Huang and Sagae, 2010 Charniak et al, 2005 Carreras et al. 2008 Suzuki et al. 2009 This work F 91.7 – 91.4 – 91.5 – – 91.1 LAS – 90.3 – – 90.0 – – 89.8 UAS – – 93.2 92.1 94.0 93.5 93.8 93.9 Table 3: Comparison on PTB Test set For French, see Table 4, we compare our system with the M ATE parser (Bohnet, 2010), an improvement over the MST parser (McDonald et al., 2005) with hash kernels, using the ME LT part-of-speech tagger (Denis and Sagot, 2009) and our own lemmatiser. We also compare the French system with results drawn from the benchmark performed by Candito et al. (2010a). The first system (BKY-FR) is close to ours without the reranking module, using the Berkeley parser adapted to French. The second (MSTFR) is based on MSTParser (McDonald et al., 2005). These two system use word clusters as well. The next section takes a close look at the models 96 This work M ATE + ME LT B KY-F R M ST-F R F < 40 89.2 – 88.2 – LAS 89.2 89.2 86.8 88.2 UAS 92.1 91.8 91."
W12-3412,P08-1108,0,0.0159804,"al linguistic knowledge (lexical preferences, subcategorisation frames, copula verbs, coordination symmetry . . . ). Integrating features from the phrase-structure trees is also an option that needs to be explored. Third this architecture enables the integration of several systems. We experimented on French using a part-of-speech tagger but we could also use another parser and either use the methodology of (Johnson and Ural, 2010) or (Zhang et al., 2009) which fusion n-best lists form different parsers, or use stacking methods where an additional parser is used as a guide for the main parser (Nivre and McDonald, 2008). Finally it should be noted that this system does not rely on any language specific feature, and thus can be applied to languages other that French or English S NP VP NP syntagmatic parses S NP PP QP VP NP PP NP PP QP NP NNP May PP NP NP CD 31 NNS VBD CD CD IN NNS Stocks were 698 million bushels on IN DT NN . of this year . NNP May CD 31 IN DT NN . of this year . dependency parses IN NNS VBD CD CD NNS Stocks were 698 million bushels on NP Before reranking After reranking Figure 3: English sentence from the WSJ test set for which the reranker selected the correct tree while the first candidate"
W12-3412,N07-1051,0,0.0129372,"ore formally, the probability of an unannotated tree is the sum of the probabilities of its annotated counterparts. For a P CFG -L A G, R is the set of annotated rules, D(t) is the set of (annotated) derivations of an unannotated tree t, and R(d) is the set of rules used in a derivation d. Then the probability assigned by G to t is: PG (t) = X d∈D(t) PG (d) = X Y PG (r) (1) 3 Discriminative model d∈D(t) r∈R(d) Because of this alternation of sums and products that cannot be optimally factorised, there is no exact polynomial dynamic programming algorithm for parsing. Matsuzaki et al. (2005) and Petrov and Klein (2007) discuss approximations of the decoding step based on a Bayesian variational approach. This enables cubic time decoding that can be further enhanced with coarse-to-fine methods (Charniak and Johnson, 2005). This type of grammars has already been tested on a variety of languages, in particular English and French, giving state-of-the-art results. Let us stress that this phrase-structure formalism is not lexicalised as opposed to grammars previously used in reranking experiments (Collins, 2000; Charniak and Johnson, 2005). The notion of lexical head is therefore absent at parsing time and will be"
W12-3412,P06-1055,0,0.0101291,"entence and produces nbest parses as well as their probabilities. They are annotated with a functional tagger which tags syntagms with standard syntactic functions subject, object, indirect object . . . and converted to dependency structures by application of percolation rules. In the second step, we extract a set of features from the dependency parses and the associated probabilities. These features are used to reorder the n-best list and select a potentially more accurate parse. Syntagmatic parses are produced by the implementation of a P CFG - LA parser of (Attia et al., 2010), similar to (Petrov et al., 2006), a functional tagger and dependency converter for the target language. The reranking model is a linear model trained with an implementation of the M IRA algorithm (Crammer et al., 2006)1 . Charniak and Johnson (2005) and Collins (2000) rerank phrase-structure parses and they also include head-dependent information, in other words unlabelled dependencies. In our approach we take into account grammatical functions or labelled dependencies. It should be noted that the features we use are very generic and do not depend on the linguistic knowledge of the authors. We applied our method to English,"
W12-3412,N10-1049,0,0.020729,"(Candito et al., 2010b) showed that a sequential approach is better than an integrated one for contextfree grammars, because the strong independence hypothesis of this formalism implies a restricted domain of locality which cannot express the context needed to properly assign functions. Most functional taggers, such as the ones used in the following experiments, rely on classifiers whose feature sets can describe the whole context of a node in order to make a decision. Dependency Structures A syntactic theory can either be expressed with phrase structures or dependencies, as advocated for in (Rambow, 2010). However, some information may be simpler to describe in one of the representations. This equivalence between the modes of representations only stands if the informational contents are the same. Unfortunately, this is not the case here because the phrase structures that we use do not contain functional annotations and lexical heads, whereas labelled dependencies do. 91 Our discriminative model is a linear model trained with the Margin-Infused Relaxed Algorithm (M IRA) (Crammer et al., 2006). This model computes the score of a parse tree as the inner product of a feature vector and a weight ve"
W12-3412,sagot-2010-lefff,0,0.0216271,"r as in (Candito and Crabbé, 2009). For both languages we constructed 10-fold training data from train sets in order to avoid overfitting the training data. The trees from training sets were divided into 10 subsets and the parses for each subset were generated by a parser trained on the other 4 Functions are omitted. 94 9 subsets. Development and test parses are given by a parser using the whole training set. Development sets were used to choose the best reranking model. For lemmatisation, we use the MATE lemmatiser for English and a home-made lemmatiser for French based on the lefff lexicon (Sagot, 2010). 4.2 Generative Model The performances of our parser are summarised in Figure 2, (a) and (b), where F-score denotes the Parseval F-score5 , and LAS and UAS are respectively the Labelled and Unlabelled Attachment Score of the converted dependency structures6 . We give oracle scores (the score that our system would get if it selected the best parse from the n-best lists) when the parser generates n-best lists of depth 10, 20, 50 and 100 in order to get an idea of the effectiveness of the reranking process. One of the issues we face with this approach is the use of an imperfect functional annota"
W12-3412,D09-1058,0,0.0661606,"Missing"
W12-3412,D09-1161,0,0.19433,"sured as the distance between this vector and the reference. One way to take advantage of both approaches is to combine them sequentially, as initially proposed by Collins (2000). A generative parser produces a set of candidates structures that constitute the input of a second, discriminative module, whose search space is limited to this set of candidates. Such an approach, parsing followed by reranking, is used in the Brown parser (Charniak and Johnson, 2005). The approach can be extended in order to feed the reranker with the output of different parsers, as shown by (Johnson and Ural, 2010; Zhang et al., 2009). In this paper we are interested in applying reranking to dependency structures. The main reason is that many linguistic constraints are straightforward to implement on dependency structures, as, for example, subcategorization frames or selectional constraints that are closely linked to the notion of deProceedings of the 50th Annual Meeting of the Association for Computational Linguistics, pages 89–99, c Jeju, Republic of Korea, 12 July 2012. 2012 Association for Computational Linguistics pendents of a predicate. On the other hand, dependencies extracted from constituent parses are known to b"
W12-3412,C10-1011,0,\N,Missing
W12-3412,N10-1095,0,\N,Missing
W15-0212,N09-2022,0,\N,Missing
W15-0212,J14-1002,0,\N,Missing
W15-0212,P11-4015,1,\N,Missing
W15-0212,J02-3001,0,\N,Missing
W15-0212,bazillon-etal-2012-syntactic,1,\N,Missing
W15-0212,bechet-etal-2012-decoda,1,\N,Missing
W15-4633,W12-1642,0,0.0704904,"Missing"
W15-4633,W09-0505,1,0.806537,"sk, manual transcripts were provided to the participants. While the original language of the conversations is French, the SENSEI project provided man• A man is calling cause he got a fine. He is waiting for a new card so he used his wife’s card. He must now write a letter asking for clemency. • A user wants to go to the Ambroise Par´e clinic but the employee misunderstands and gives her the wrong itinerary. Luckily the employee realises her mistake and gives the passenger the right information in the end. • School bag lost on line 4, not found. Luna corpus The Italian human-human Luna corpus (Dinarelli et al., 2009) consists of 572 dialogs (≈ 26.5K turns & 30 hours of speech) in the hardware/software help desk domain, where a 233 4 client and an agent are engaged in a problem solving task over the phone. The dialogs are organised in transcriptions and annotations created within the FP6 LUNA project. For the CCCS shared task, manual transcriptions were used. Metric Evaluation is performed with the ROUGE-2 metric (Lin, 2004). ROUGE-2 is the recall in term of word bigrams between a set of reference synopses and a system submission. The ROUGE 1.5.5 toolkit was adapted to deal with a conversation-dependent le"
W15-4633,W04-1013,0,0.0698795,"Missing"
W15-4633,W13-2117,0,0.0992767,"Missing"
W15-4633,stepanov-etal-2014-development,1,0.892092,"Missing"
W15-4633,bechet-etal-2012-decoda,1,\N,Missing
W15-4638,D09-1030,0,0.0138446,".g. polytope model optimization, genetic algorithms) on rich feature spaces to either maximize coverage of the output summaries, or train models for sentence scoring. The feature spaces went beyond words 4.2 Participation, Evaluation and Results Four research groups participated in the OnForumS, each submitting two runs. In addition, two baseline system runs were included making a total of ten different system runs. 1 272 http://www.sensei-conversation.eu/ Submissions were evaluated via crowdsourcing on Crowd Flowerwhich is a commonly used method for evaluating HLT systems (Snow et al., 2008; Callison-Burch, 2009). The crowdsourcing HIT was designed as a validation task (as opposed to annotation), where each system proposed link and labels are presented to a contributor for their validation. The approach used for the OnForumS evaluation is IR-inspired and based on the concept of pooling used in TREC (Soboroff, 2010), where the assumption is that possible links that were not proposed by any system are deemed irrelevant. Then from those links proposed by systems, four categories are formed as follows: (a) (b) (c) (d) phone. This task is different from news summarization in that dialogues need to be analy"
W15-4638,W13-3101,1,0.876356,"ional Linguistics independent summarization algorithms. Each system participating in the task was called upon to provide summaries for a range of different languages, based on corresponding language-specific corpora. Systems were to summarize texts in at least two of the ten different languages: Arabic, Chinese, Czech, English, French, Greek, Hebrew, Hindi, Romanian, Spanish. The task aims at the real problem of summarizing news topics, parts of which may be described or may happen in different moments in time. We consider, similarly to previous MultiLing efforts (Giannakopoulos et al., 2011; Li et al., 2013) that news topics can be seen as event sequences: style. Such articles make an excellent source of test data for single document summarization methods since they each have a well written summary (one of the style criterion), cover many languages, and have a diverse range of topics. 2.2 Participation, Evaluation, and Results Participation in the 2015 MSS task was excellent, 23 summarization systems were submitted by seven teams. Four of the teams submitted summaries for all 38 languages and the remaining three submitted summaries covering four languages. English was the only language for which"
W15-4638,W04-1013,0,0.148645,"summary score that participating systems should be able to exceed. An oracle summary was computed for each article using a covering algorithm (Davis et al., 2012) that selected sentences from the body text that covers the words in the summary using a minimal number of sentences until their aggregate size exceeds the summary. The oracle summary scores provide an approximate upper bound on the achievable summary scores and were, as expected, much higher than any submitted systems score. The baseline, oracle, and submitted summaries were scored against the human summaries using ROUGE-2, -3, -4 (Lin, 2004) and MeMoG (Giannakopoulos et al., 2008). Details of the preprocessing applied to the text and the performance of each submitted system are in (Kubina and Conroy, 2015b), but overall 14 of the 23 systems did better than the baseline summary for at least half of the languages they partook in. The ROUGE and MeMog scoring methods provide an automatic measure of summaries, which are good predictors of human judgements. A human evaluation of the summaries, that is currently underway, will measure the responsiveness and readability of each teams best performing system. 3 3.1 Definition 1. An event s"
W15-4638,W13-3102,1,0.925116,"Missing"
W15-4638,W15-4633,1,0.853959,"Missing"
W15-4638,W13-2323,0,0.0244471,"xcellent with 23 participants submitting two or more system runs across the four tasks that the campaign comprises. The next steps for the classical tasks MSS and MMS is to continue expanding the corpora in size and across languages, whereas for the pilot tasks is to further precise the boundaries of the new tasks and bridge the gaps in the evaluation methodologies by overcoming the limitations of ROUGE in order to assess abstractiveness and minimizing the effect of ‘cheating’ workers in crowdsourcing (e.g., by incorporating a probabilistic model of annotation, such as the one put forward by (Passonneau and Carpenter, 2013) to filter better noisy crowdsourcing data). The next MultiLing is planned for 2017. CCCS Task 5.1 Participation, evaluation and results Task description The call-center conversation summarization pilot task consists in automatically generating abstractive summaries of spoken conversations between a customer and an agent solving a problem over the 2 The popular links (a and b) were not that many, hence, we chose to include all. 3 Based on CrowdFlower’s aggregated judgements. 273 Acknowledgements G. Giannakopoulos, J. Kubina, J. Conroy, J. Steinberger, B. Favre, M. Kabadjov, U. Kruschwitz, and"
W15-4638,D08-1027,0,0.0276498,"Missing"
W15-4638,Q14-1025,0,\N,Missing
W17-1001,W15-4633,1,0.893661,"Missing"
W17-1001,W15-4638,1,0.928294,"mmarization by providing data sets and fostering further research and development of summarization systems. This year the scope of the workshop was widened, bringing together researchers that work on summarization across sources, languages and genres. We summarize the main tasks planned and implemented this year, also providing insights on next steps. 1 LIF, France The spectrum of the tasks covers a variety of real settings, identifying individual requirements and intricacies, similarly to previous MultiLing endeavours (Giannakopoulos et al., 2011a; Giannakopoulos, 2013; Elhadad et al., 2013; Giannakopoulos et al., 2015). • Multilingual summary evaluation: Summary evaluation has been an open question for several years, even though there exist methods that correlate well to human judgement, when called upon to compare systems. In the multilingual setting, it is not obvious that these methods will perform equally well to the English language setting. In fact, some preliminary results have shown that several problems may arise in the multilingual setting (Giannakopoulos et al., 2011a). The same challenges arise across different source types and genres. This aspect of the workshop aims to cover and discuss these"
W17-1001,W13-3103,1,0.825568,"pushes the state-of-the-art in Automatic Summarization by providing data sets and fostering further research and development of summarization systems. This year the scope of the workshop was widened, bringing together researchers that work on summarization across sources, languages and genres. We summarize the main tasks planned and implemented this year, also providing insights on next steps. 1 LIF, France The spectrum of the tasks covers a variety of real settings, identifying individual requirements and intricacies, similarly to previous MultiLing endeavours (Giannakopoulos et al., 2011a; Giannakopoulos, 2013; Elhadad et al., 2013; Giannakopoulos et al., 2015). • Multilingual summary evaluation: Summary evaluation has been an open question for several years, even though there exist methods that correlate well to human judgement, when called upon to compare systems. In the multilingual setting, it is not obvious that these methods will perform equally well to the English language setting. In fact, some preliminary results have shown that several problems may arise in the multilingual setting (Giannakopoulos et al., 2011a). The same challenges arise across different source types and genres. This asp"
W17-1001,W04-1013,0,0.0206207,"(16851) 36638 (18062) 17772 (9073) 37128 (22024) 31460 (16319) 37189 (16777) 38973 (20349) 59337 (27360) 9793 (7027) 26102 (11024) 30549 (15203) 32240 (23667) 23648 (14139) 35552 (32014) 10614 (6338) Table 1: The table lists the languages in the dataset with the first column containing the ISO code for each the language, the second column the name of the language, and the remaining columns containing the mean size, in characters, and standard deviation, in parentheses, of the summary and body of the article. For example, for English the mean size of the human summaries is 1,857 characters. 3 (Lin, 2004) and MeMoG (Gia, ) metrics, there exists a big gap between automatic methods and manual annotations, especially in non-English settings (Giannakopoulos et al., 2011b). This year’s task reuses the MultiLing 2013 and 2015 single-document and multi-document summarization corpora and evalautions. Furthermore, we generate summary variations (often through inducing “noise”), which the evaluation systems will be asked to grade. These variations include: As of the time of publication of the proceedings, three teams have participated and automatic methods of scoring the subumissions, using ROUGE (Lin,"
W17-1001,W13-3102,1,\N,Missing
W17-5304,N09-1003,0,0.44403,"Missing"
W17-5304,D14-1034,0,0.0204257,"not be used because differences between correlations are not significant due to their small size (Faruqui et al., 2016). While early datasets contained judgments collected in-house, such as WS-353 (Finkelstein et al., 2001; Agirre et al., 2009), most recent ones are created through crowd sourcing, with 10 to 20 annotations per wordpair, filtered for outliers: MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), MEN (Bruni et al., 2012), RW (Luong et al., 2013). Another trend is to address different syntactic categories than noun, such as YP-130 (Yang and Powers, 2006) and Verb (Baker et al., 2014) which focus on verbs. While past studies did not differentiate relatedness from semantic similarity, SimLex (Hill et al., 2016) and SimVerb (Gerz et al., 2016) explicitly promote the later. Embeddings can also be compared to lexical resources (Tsvetkov et al., 2015), but it is hard to balance the contribution of each linguistic phenomenon. Existing similarity ratings are subjective because they are the result of a conscious process while it seems desirable to directly evaluate embeddings against basic processes that support language in the brain. 3 Semantic priming There is an extensive psych"
W17-5304,P12-1015,0,0.0190747,"an be used to evaluate word similarity based on human judgment. Older datasets, such as RG (Rubenstein and Goodenough, 1965) and MC (Miller and Charles, 1991) shall not be used because differences between correlations are not significant due to their small size (Faruqui et al., 2016). While early datasets contained judgments collected in-house, such as WS-353 (Finkelstein et al., 2001; Agirre et al., 2009), most recent ones are created through crowd sourcing, with 10 to 20 annotations per wordpair, filtered for outliers: MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), MEN (Bruni et al., 2012), RW (Luong et al., 2013). Another trend is to address different syntactic categories than noun, such as YP-130 (Yang and Powers, 2006) and Verb (Baker et al., 2014) which focus on verbs. While past studies did not differentiate relatedness from semantic similarity, SimLex (Hill et al., 2016) and SimVerb (Gerz et al., 2016) explicitly promote the later. Embeddings can also be compared to lexical resources (Tsvetkov et al., 2015), but it is hard to balance the contribution of each linguistic phenomenon. Existing similarity ratings are subjective because they are the result of a conscious proces"
W17-5304,P14-2050,0,0.0489916,"we benchmark a range of embeddings on the SPP data. Four conditions are considered: LDT-200, LDT-1200, NT-200, NT-1200 (for lexical decision task, and naming task, both with an onset of 200 ms and 1,200 ms). We compare two categories of embeddings: a controlled setting for which algorithms are trained on the same dataset, and a selection of pretrained embeddings available to the community. Due to space constraints, the pretrained embeddings considered2 are limited to: W2V Skip-gram (Mikolov et al., 2013), GloVe (Pennington et al., 2014), Multilingual (Faruqui and Dyer, 2014), Dependencybased (Levy and Goldberg, 2014), and FastText (Bojanowski et al., 2016). For the embeddings with controlled settings, we used two algorithms: W2V Skip-grams and GloVe. We used three different corpora to train these embeddings: Wikipedia 2013 (Wiki), Gigaword3 (GW) and OpenSubtitles 2016 (OS). We used a centered window of size 10 and generated vectors with 100 dimensions for all 6 models. From the experiment detailed in Table 1, it appears that GloVe leads to significantly larger negative correlation compared to other approaches, both on the controlled and pretrained settings. On the controlled setting, we notice that the ch"
W17-5304,W16-2513,0,0.237933,"ance {firstname.lastname}@univ-amu.fr Abstract In this paper, we describe an evaluation framework based on comparing word embedding similarity against reaction times from the Semantic Priming Project (Hutchison et al., 2013). A set of word embeddings is evaluated by computing its Spearman rank correlation with average reaction times obtained by submitting a set of subjects to a prime (one word from the pair) and then perform one of two tasks: lexical decision (decide whether the second word is an existing word or not), and naming (read aloud the second word). Extending the ideas developed in (Ettinger and Linzen, 2016), this paper describes the following contributions: This work presents a framework for word similarity evaluation grounded on cognitive sciences experimental data. Word pair similarities are compared to reaction times of subjects in large scale lexical decision and naming tasks under semantic priming. Results show that GloVe embeddings lead to significantly higher correlation with experimental measurements than other controlled and off-the-shelf embeddings, and that the choice of a training corpus is less important than that of the algorithm. Comparison of rankings with other datasets shows th"
W17-5304,W14-1618,0,0.0271596,"s. • we also look at the correlation between SPP reaction times and subjective similarity and relatedness ratings from existing datasets. 2 Related Work Intrinsic and extrinsic approaches have been proposed for word embedding evaluation. The former typically consist in collecting human judgment of word similarity on a range of word pairs, and computing the rank correlation of their averaged value with the cosine similarity between the embeddings of the words in the pair. Word analogy is also evaluated but it has been proven to be equivalent to a linear combination between cosine similarities (Levy et al., 2014). In this work, we focus on 1 Available at https://github.com/JomnTAL/ spp-wordsim 21 Proceedings of the 2nd Workshop on Evaluating Vector-Space Representations for NLP, pages 21–26, c Copenhagen, Denmark, September 7–11, 2017. 2017 Association for Computational Linguistics ticularly relevant to our study in that the authors also propose to evaluate embeddings against reaction time data from the SPP dataset. Our work differs in that we provide an evaluation setup which can be reused by other researchers, we propose a different evaluation metric based on rank correlation, and we perform an anal"
W17-5304,E14-1049,0,0.0355159,"xperiments In a first set of experiments, we benchmark a range of embeddings on the SPP data. Four conditions are considered: LDT-200, LDT-1200, NT-200, NT-1200 (for lexical decision task, and naming task, both with an onset of 200 ms and 1,200 ms). We compare two categories of embeddings: a controlled setting for which algorithms are trained on the same dataset, and a selection of pretrained embeddings available to the community. Due to space constraints, the pretrained embeddings considered2 are limited to: W2V Skip-gram (Mikolov et al., 2013), GloVe (Pennington et al., 2014), Multilingual (Faruqui and Dyer, 2014), Dependencybased (Levy and Goldberg, 2014), and FastText (Bojanowski et al., 2016). For the embeddings with controlled settings, we used two algorithms: W2V Skip-grams and GloVe. We used three different corpora to train these embeddings: Wikipedia 2013 (Wiki), Gigaword3 (GW) and OpenSubtitles 2016 (OS). We used a centered window of size 10 and generated vectors with 100 dimensions for all 6 models. From the experiment detailed in Table 1, it appears that GloVe leads to significantly larger negative correlation compared to other approaches, both on the controlled and pretrained settings. On th"
W17-5304,W13-3512,0,0.053424,"rd similarity based on human judgment. Older datasets, such as RG (Rubenstein and Goodenough, 1965) and MC (Miller and Charles, 1991) shall not be used because differences between correlations are not significant due to their small size (Faruqui et al., 2016). While early datasets contained judgments collected in-house, such as WS-353 (Finkelstein et al., 2001; Agirre et al., 2009), most recent ones are created through crowd sourcing, with 10 to 20 annotations per wordpair, filtered for outliers: MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), MEN (Bruni et al., 2012), RW (Luong et al., 2013). Another trend is to address different syntactic categories than noun, such as YP-130 (Yang and Powers, 2006) and Verb (Baker et al., 2014) which focus on verbs. While past studies did not differentiate relatedness from semantic similarity, SimLex (Hill et al., 2016) and SimVerb (Gerz et al., 2016) explicitly promote the later. Embeddings can also be compared to lexical resources (Tsvetkov et al., 2015), but it is hard to balance the contribution of each linguistic phenomenon. Existing similarity ratings are subjective because they are the result of a conscious process while it seems desirabl"
W17-5304,W16-2506,0,0.359258,"on, and we perform an analysis in regard to different parameters, in particular the corpus used to train embeddings. intrinsic evaluation, and therefore do not detail efforts for extrinsic evaluation of word embeddings. More pointers can be found in (Schnabel et al., 2015; Lai et al., 2016; Ghannay et al., 2016). A number of datasets can be used to evaluate word similarity based on human judgment. Older datasets, such as RG (Rubenstein and Goodenough, 1965) and MC (Miller and Charles, 1991) shall not be used because differences between correlations are not significant due to their small size (Faruqui et al., 2016). While early datasets contained judgments collected in-house, such as WS-353 (Finkelstein et al., 2001; Agirre et al., 2009), most recent ones are created through crowd sourcing, with 10 to 20 annotations per wordpair, filtered for outliers: MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), MEN (Bruni et al., 2012), RW (Luong et al., 2013). Another trend is to address different syntactic categories than noun, such as YP-130 (Yang and Powers, 2006) and Verb (Baker et al., 2014) which focus on verbs. While past studies did not differentiate relatedness from semantic similarity"
W17-5304,D16-1235,0,0.0273087,"Missing"
W17-5304,L16-1046,1,0.924292,"eate and distribute a package1 for word embedding evaluation based on the SPP primed reaction time data; • in order to calibrate results from that evaluation framework, we look at the effect of training corpus on a set of word embeddings; Introduction Word representations have attracted a lot of interest in the community and led to very useful applications in a range of domains of natural language processing. Such representations are typically evaluated intrinsically on word similarity tasks and extrinsically on their impact on NLP systems performance (Schnabel et al., 2015; Lai et al., 2016; Ghannay et al., 2016). A recent trend towards building more general representations has looked at how similarities in the representation space can predict the outcome of cognitive experiments, such as human reaction time in semantic priming experiments (Ettinger and Linzen, 2016) or relying on eye tracking and brain imaging data (Søgaard, 2016; Ruan et al., 2016). The idea is that ground truth from unconscious phenomena might be less prone to subjective factors of more traditional word similarity and relatedness datasets. • we also look at the correlation between SPP reaction times and subjective similarity and re"
W17-5304,D14-1162,0,0.0943347,"iciency and attention tests results. 4 Experiments In a first set of experiments, we benchmark a range of embeddings on the SPP data. Four conditions are considered: LDT-200, LDT-1200, NT-200, NT-1200 (for lexical decision task, and naming task, both with an onset of 200 ms and 1,200 ms). We compare two categories of embeddings: a controlled setting for which algorithms are trained on the same dataset, and a selection of pretrained embeddings available to the community. Due to space constraints, the pretrained embeddings considered2 are limited to: W2V Skip-gram (Mikolov et al., 2013), GloVe (Pennington et al., 2014), Multilingual (Faruqui and Dyer, 2014), Dependencybased (Levy and Goldberg, 2014), and FastText (Bojanowski et al., 2016). For the embeddings with controlled settings, we used two algorithms: W2V Skip-grams and GloVe. We used three different corpora to train these embeddings: Wikipedia 2013 (Wiki), Gigaword3 (GW) and OpenSubtitles 2016 (OS). We used a centered window of size 10 and generated vectors with 100 dimensions for all 6 models. From the experiment detailed in Table 1, it appears that GloVe leads to significantly larger negative correlation compared to other approaches, both on the co"
W17-5304,D16-1064,0,0.0225867,"pplications in a range of domains of natural language processing. Such representations are typically evaluated intrinsically on word similarity tasks and extrinsically on their impact on NLP systems performance (Schnabel et al., 2015; Lai et al., 2016; Ghannay et al., 2016). A recent trend towards building more general representations has looked at how similarities in the representation space can predict the outcome of cognitive experiments, such as human reaction time in semantic priming experiments (Ettinger and Linzen, 2016) or relying on eye tracking and brain imaging data (Søgaard, 2016; Ruan et al., 2016). The idea is that ground truth from unconscious phenomena might be less prone to subjective factors of more traditional word similarity and relatedness datasets. • we also look at the correlation between SPP reaction times and subjective similarity and relatedness ratings from existing datasets. 2 Related Work Intrinsic and extrinsic approaches have been proposed for word embedding evaluation. The former typically consist in collecting human judgment of word similarity on a range of word pairs, and computing the rank correlation of their averaged value with the cosine similarity between the e"
W17-5304,D15-1036,0,0.239748,"word relatedness or similarity. 1 • we create and distribute a package1 for word embedding evaluation based on the SPP primed reaction time data; • in order to calibrate results from that evaluation framework, we look at the effect of training corpus on a set of word embeddings; Introduction Word representations have attracted a lot of interest in the community and led to very useful applications in a range of domains of natural language processing. Such representations are typically evaluated intrinsically on word similarity tasks and extrinsically on their impact on NLP systems performance (Schnabel et al., 2015; Lai et al., 2016; Ghannay et al., 2016). A recent trend towards building more general representations has looked at how similarities in the representation space can predict the outcome of cognitive experiments, such as human reaction time in semantic priming experiments (Ettinger and Linzen, 2016) or relying on eye tracking and brain imaging data (Søgaard, 2016; Ruan et al., 2016). The idea is that ground truth from unconscious phenomena might be less prone to subjective factors of more traditional word similarity and relatedness datasets. • we also look at the correlation between SPP reacti"
W17-5304,W16-2521,0,0.133014,"o very useful applications in a range of domains of natural language processing. Such representations are typically evaluated intrinsically on word similarity tasks and extrinsically on their impact on NLP systems performance (Schnabel et al., 2015; Lai et al., 2016; Ghannay et al., 2016). A recent trend towards building more general representations has looked at how similarities in the representation space can predict the outcome of cognitive experiments, such as human reaction time in semantic priming experiments (Ettinger and Linzen, 2016) or relying on eye tracking and brain imaging data (Søgaard, 2016; Ruan et al., 2016). The idea is that ground truth from unconscious phenomena might be less prone to subjective factors of more traditional word similarity and relatedness datasets. • we also look at the correlation between SPP reaction times and subjective similarity and relatedness ratings from existing datasets. 2 Related Work Intrinsic and extrinsic approaches have been proposed for word embedding evaluation. The former typically consist in collecting human judgment of word similarity on a range of word pairs, and computing the rank correlation of their averaged value with the cosine simi"
W17-5304,D15-1243,0,0.0372249,"re created through crowd sourcing, with 10 to 20 annotations per wordpair, filtered for outliers: MTurk-287 (Radinsky et al., 2011), MTurk-771 (Halawi et al., 2012), MEN (Bruni et al., 2012), RW (Luong et al., 2013). Another trend is to address different syntactic categories than noun, such as YP-130 (Yang and Powers, 2006) and Verb (Baker et al., 2014) which focus on verbs. While past studies did not differentiate relatedness from semantic similarity, SimLex (Hill et al., 2016) and SimVerb (Gerz et al., 2016) explicitly promote the later. Embeddings can also be compared to lexical resources (Tsvetkov et al., 2015), but it is hard to balance the contribution of each linguistic phenomenon. Existing similarity ratings are subjective because they are the result of a conscious process while it seems desirable to directly evaluate embeddings against basic processes that support language in the brain. 3 Semantic priming There is an extensive psychological literature concerning the nature of semantic representations and the influence of semantic or associative context on word processing (McNamara, 2005). In this domain, the semantic priming paradigm is one of the most popular experimental tool to study these c"
W17-6311,J16-1002,1,0.539863,"-end process a text directly from an image, without an explicit representation (syntactic or semantic) of the text generated. For syntactic parsing, the problem of PPattachment has a long history in Natural Language Processing and a wealth of different methods and sources of information have been used to alleviate it. Giving a overview of this vast body of literature is well beyond the scope of this paper. Traditionally, two types of resources have been used to help resolving PP-attachment, semantic knowledge bases (Agirre et al., 2008; Dasigi et al., 2017), and corpora (Rakshit et al., 2016; Mirroshandel and Nasr, 2016; Belinkov et al., 2014; de Kok et al., 2017). We are not aware of much work using multimodal information for PP-attachment. In the most relevant work that we have found (Christie et al., 2016), a parser is used to predict the k best parses for a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271 captions (2907 annotated prepositions) and a test set, made of 2288 captions (2907 pr"
W17-6311,P11-4015,1,0.883412,"Missing"
W17-6311,P08-1037,0,0.100252,"representation for both image and language features and generate in an end-to-end process a text directly from an image, without an explicit representation (syntactic or semantic) of the text generated. For syntactic parsing, the problem of PPattachment has a long history in Natural Language Processing and a wealth of different methods and sources of information have been used to alleviate it. Giving a overview of this vast body of literature is well beyond the scope of this paper. Traditionally, two types of resources have been used to help resolving PP-attachment, semantic knowledge bases (Agirre et al., 2008; Dasigi et al., 2017), and corpora (Rakshit et al., 2016; Mirroshandel and Nasr, 2016; Belinkov et al., 2014; de Kok et al., 2017). We are not aware of much work using multimodal information for PP-attachment. In the most relevant work that we have found (Christie et al., 2016), a parser is used to predict the k best parses for a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271"
W17-6311,W03-3017,0,0.11765,"or a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271 captions (2907 annotated prepositions) and a test set, made of 2288 captions (2907 prepositions). 4 Error Prediction The train part of the PP-corpus has been used to train a classifier that predicts whether a PPattachment proposed by a parser is correct or not. The parser used is a standard arc-eager transition based parser (Nivre, 2003), trained on sections 0 − 18 of the Penn Treebank (Marcus et al., 1993). The parser was run on the train set of the corpus and, for each occurrence of a manually attached preposition, a negative or a positive example has been produced depending on whether the parser has predicted the correct attachment or not. This data set is composed of 17643 positive and 5611 negative examples. It has been used to train a classifier that predicts whether the attachment made by the parser is correct or not. The classifier used for this task is the Icsiboost classifier (Favre et al., 2007). This Adaboost clas"
W17-6311,D11-1113,0,0.0220038,"f these prepositions, column two displays its number of occurrences, column three (BL) shows the attachment accuracy for this preposition in the output the parser. Columns four (T), five (C), six (V) and seven (TCV) show the attachment accuracy for the corrected output for four different configurations The classifier developed in the previous section only checked if a PP-attachment proposed by the parser is correct or not. In this section we integrate this classifier in a correction strategy in order to improve the accuracy of our parser. This correction strategy is inspired from the ideas of Anguiano and Candito (2011); Attardi and Ciaramita (2007); Hall and Nov´ak (2005): given a sentence S, a parse T for S and a target preposition p, a set 75 Prep into with through behind under down in in front of outside on around for at along across against near towards next to by of over during from TOTAL Occ 116 310 145 35 58 41 369 51 35 143 59 168 63 50 49 31 159 30 137 76 72 111 41 140 2907 BL 0.89 0.65 0.95 0.74 0.84 0.63 0.76 0.90 0.63 0.85 0.73 0.73 0.84 0.52 0.88 0.77 0.33 0.90 0.89 0.84 0.93 0.66 0.71 0.76 0.75 T 0.93 0.78 0.96 0.86 0.84 0.73 0.84 0.88 0.74 0.90 0.81 0.82 0.86 0.86 0.96 0.94 0.84 0.93 0.89 0.8"
W17-6311,N07-1049,0,0.0243527,"two displays its number of occurrences, column three (BL) shows the attachment accuracy for this preposition in the output the parser. Columns four (T), five (C), six (V) and seven (TCV) show the attachment accuracy for the corrected output for four different configurations The classifier developed in the previous section only checked if a PP-attachment proposed by the parser is correct or not. In this section we integrate this classifier in a correction strategy in order to improve the accuracy of our parser. This correction strategy is inspired from the ideas of Anguiano and Candito (2011); Attardi and Ciaramita (2007); Hall and Nov´ak (2005): given a sentence S, a parse T for S and a target preposition p, a set 75 Prep into with through behind under down in in front of outside on around for at along across against near towards next to by of over during from TOTAL Occ 116 310 145 35 58 41 369 51 35 143 59 168 63 50 49 31 159 30 137 76 72 111 41 140 2907 BL 0.89 0.65 0.95 0.74 0.84 0.63 0.76 0.90 0.63 0.85 0.73 0.73 0.84 0.52 0.88 0.77 0.33 0.90 0.89 0.84 0.93 0.66 0.71 0.76 0.75 T 0.93 0.78 0.96 0.86 0.84 0.73 0.84 0.88 0.74 0.90 0.81 0.82 0.86 0.86 0.96 0.94 0.84 0.93 0.89 0.86 0.93 0.85 0.76 0.86 0.85 C 0"
W17-6311,Q14-1043,0,0.021789,"from an image, without an explicit representation (syntactic or semantic) of the text generated. For syntactic parsing, the problem of PPattachment has a long history in Natural Language Processing and a wealth of different methods and sources of information have been used to alleviate it. Giving a overview of this vast body of literature is well beyond the scope of this paper. Traditionally, two types of resources have been used to help resolving PP-attachment, semantic knowledge bases (Agirre et al., 2008; Dasigi et al., 2017), and corpora (Rakshit et al., 2016; Mirroshandel and Nasr, 2016; Belinkov et al., 2014; de Kok et al., 2017). We are not aware of much work using multimodal information for PP-attachment. In the most relevant work that we have found (Christie et al., 2016), a parser is used to predict the k best parses for a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271 captions (2907 annotated prepositions) and a test set, made of 2288 captions (2907 prepositions). 4 Error Pr"
W17-6311,D16-1156,0,0.234932,"Natural Language Processing and a wealth of different methods and sources of information have been used to alleviate it. Giving a overview of this vast body of literature is well beyond the scope of this paper. Traditionally, two types of resources have been used to help resolving PP-attachment, semantic knowledge bases (Agirre et al., 2008; Dasigi et al., 2017), and corpora (Rakshit et al., 2016; Mirroshandel and Nasr, 2016; Belinkov et al., 2014; de Kok et al., 2017). We are not aware of much work using multimodal information for PP-attachment. In the most relevant work that we have found (Christie et al., 2016), a parser is used to predict the k best parses for a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271 captions (2907 annotated prepositions) and a test set, made of 2288 captions (2907 prepositions). 4 Error Prediction The train part of the PP-corpus has been used to train a classifier that predicts whether a PPattachment proposed by a parser is correct or not. The parser used"
W17-6311,Q14-1006,0,0.0483338,"ki, 1994) computes first spatial relations among objects detected in images with knowledge-based language generation model in order to generate short descriptions of videos in limited domains (traffic scenes, soccer matches). Recently open-domain language generation from 72 Proceedings of the 15th International Conference on Parsing Technologies, pages 72–77, c Pisa, Italy; September 20–22, 2017. 2017 Association for Computational Linguistics 3 Data The multimodal corpus used in this work is the Flickr30k Entities (F30kE) (Plummer et al., 2017), an extension of the original Flickr30k dataset (Young et al., 2014). This corpus is composed of almost 32K images and, for each image, five captions describing the image have been produced. Besides, every object in the image that corresponds to a mention in the captions has been manually identified with a bounding box. Bounding boxes and the mentions in the captions have been paired together via co-reference links. A total of 244K such links have been annotated. Furthermore, each mention in the captions has been categorized into eight coarse-grained conceptual types using manually constructed dictionaries. The types are: people, body parts, animals, clothing,"
W17-6311,P17-1191,0,0.150617,"oth image and language features and generate in an end-to-end process a text directly from an image, without an explicit representation (syntactic or semantic) of the text generated. For syntactic parsing, the problem of PPattachment has a long history in Natural Language Processing and a wealth of different methods and sources of information have been used to alleviate it. Giving a overview of this vast body of literature is well beyond the scope of this paper. Traditionally, two types of resources have been used to help resolving PP-attachment, semantic knowledge bases (Agirre et al., 2008; Dasigi et al., 2017), and corpora (Rakshit et al., 2016; Mirroshandel and Nasr, 2016; Belinkov et al., 2014; de Kok et al., 2017). We are not aware of much work using multimodal information for PP-attachment. In the most relevant work that we have found (Christie et al., 2016), a parser is used to predict the k best parses for a sentence and this set is re-ranked using visual information. The main difference with their work is, in our case, the combined use of lexical, semantic and visual cues as well as the method used (k best parses v/s parse correction). 73 a development set, made of 2271 captions (2907 annota"
W17-6311,E17-2050,0,0.162099,"Missing"
W17-6311,W05-1505,0,0.122245,"Missing"
W17-6311,J93-2004,0,\N,Missing
W18-4933,W11-0809,0,0.0257245,"he RNN is to predict the correct BIO+category tag for each token. We use no external corpus or word embeddings to train our system, hence we participated in the closed track. Veyn is freely available,2 and covers 19 of the 20 languages of the shared task (all except Arabic, which required a special license). Sequence taggers were successfully employed by many systems for MWE identification in the past. Most existing models and systems, however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for"
W18-4933,J17-4005,1,0.905563,"Missing"
W18-4933,W17-1707,0,0.0758703,", however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for MUMULS, BIO for Veyn), the use of pre-initialized embeddings (not used by MUMULS) and the number of recurrent layers (1 in MUMULS, 2 in Veyn). In the remainder of this paper, we describe the system This work is licensed under a Creative Commons Attribution 4.0 International License. License details: http:// creativecommons.org/licenses/by/4.0/ 1 http://multiword.sourceforge.net/sharedtask2018 2 https://github.com/zamp13/Veyn. Th"
W18-4933,W17-1715,0,0.0268949,"each token. We use no external corpus or word embeddings to train our system, hence we participated in the closed track. Veyn is freely available,2 and covers 19 of the 20 languages of the shared task (all except Arabic, which required a special license). Sequence taggers were successfully employed by many systems for MWE identification in the past. Most existing models and systems, however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for MUMULS, BIO for Veyn), the use of pre-initialize"
W18-4933,W95-0107,0,0.559976,"Missing"
W18-4933,W16-1816,0,0.301187,"rect BIO+category tag for each token. We use no external corpus or word embeddings to train our system, hence we participated in the closed track. Veyn is freely available,2 and covers 19 of the 20 languages of the shared task (all except Arabic, which required a special license). Sequence taggers were successfully employed by many systems for MWE identification in the past. Most existing models and systems, however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for MUMULS, BIO for Veyn), t"
W18-4933,Q14-1016,0,0.122787,"ence we participated in the closed track. Veyn is freely available,2 and covers 19 of the 20 languages of the shared task (all except Arabic, which required a special license). Sequence taggers were successfully employed by many systems for MWE identification in the past. Most existing models and systems, however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for MUMULS, BIO for Veyn), the use of pre-initialized embeddings (not used by MUMULS) and the number of recurrent layers (1 in MUMULS"
W18-4933,W17-1723,1,0.844061,"xternal corpus or word embeddings to train our system, hence we participated in the closed track. Veyn is freely available,2 and covers 19 of the 20 languages of the shared task (all except Arabic, which required a special license). Sequence taggers were successfully employed by many systems for MWE identification in the past. Most existing models and systems, however, represent features as discrete values taken from finite sets instead of continuous vectors. Examples of such systems employ conditional random fields (Constant and Sigogne, 2011; Riedl and Biemann, 2016; Maldonado et al., 2017; Scholivet and Ramisch, 2017) and structured perceptron (Schneider et al., 2014). Most recent NLP systems for sequence tagging, however, are based on RNNs. Our system follows this trend by adapting an RNN model successful in other tagging tasks to VMWE identification. Our system is similar to MUMULS, submitted to the previous PARSEME shared task, edition 1.0 (Klyueva et al., 2017). MUMULS was evaluated on fifteen languages with variable results. Our system differs from MUMULS in the hyper-parameter configuration, the tag encoding scheme (IO for MUMULS, BIO for Veyn), the use of pre-initialized embeddings (not used by MUMU"
